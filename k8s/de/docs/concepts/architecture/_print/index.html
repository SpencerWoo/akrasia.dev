<!doctype html><html lang=de class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/architecture/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/architecture/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/architecture/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/architecture/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/architecture/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/architecture/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/architecture/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/architecture/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/architecture/><link rel=alternate hreflang=vi href=https://kubernetes.io/vi/docs/concepts/architecture/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/architecture/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/de/docs/concepts/architecture/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Kubernetes Architekur | Kubernetes</title><meta property="og:title" content="Kubernetes Architekur"><meta property="og:description" content="Produktionsreife Container-Orchestrierung"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/de/docs/concepts/architecture/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Kubernetes Architekur"><meta itemprop=description content="Produktionsreife Container-Orchestrierung"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes Architekur"><meta name=twitter:description content="Produktionsreife Container-Orchestrierung"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/de/docs/concepts/architecture/"><meta property="og:title" content="Kubernetes Architekur"><meta name=twitter:title content="Kubernetes Architekur"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/de/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/de/docs/>Dokumentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/de/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/de/training/>Schulungen</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/de/partners/>Partner</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/de/community/>Community</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/de/case-studies/>Fallstudien</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/de/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/de/docs/concepts/architecture/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/de/docs/concepts/architecture/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/de/docs/concepts/architecture/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/de/docs/concepts/architecture/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/de/docs/concepts/architecture/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Deutsch (German)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/architecture/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/architecture/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/architecture/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/architecture/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/architecture/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/architecture/>Italiano (Italian)</a>
<a class=dropdown-item href=/es/docs/concepts/architecture/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/architecture/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/architecture/>Bahasa Indonesia</a>
<a class=dropdown-item href=/vi/docs/concepts/architecture/>Tiếng Việt (Vietnamese)</a>
<a class=dropdown-item href=/ru/docs/concepts/architecture/>Русский (Russian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Das ist eine für den Ausdruck optimierte Ansicht des gesamten Kapitels inkl. Unterseiten.
<a href=# onclick="return print(),!1">Druckvorgang starten</a>.</p><p><a href=/de/docs/concepts/architecture/>Zur Standardansicht zurückkehren</a>.</p></div><h1 class=title>Kubernetes Architekur</h1><ul><li>1: <a href=#pg-9ef2890698e773b6c0d24fd2c20146f5>Nodes</a></li><li>2: <a href=#pg-63e7fdf87ba61eb2586bb8c625c23506>Master-Node Kommunikation</a></li><li>3: <a href=#pg-bc804b02614d67025b4c788f1ca87fbc>Zugrunde liegende Konzepte des Cloud Controller Manager</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-9ef2890698e773b6c0d24fd2c20146f5>1 - Nodes</h1><p>Ein Knoten (Node in Englisch) ist eine Arbeitsmaschine in Kubernetes. Ein Node
kann je nach Cluster eine VM oder eine physische Maschine sein. Jeder Node enthält
die für den Betrieb von <a href=/docs/concepts/workloads/pods/pod/>Pods</a> notwendigen Dienste
und wird von den Master-Komponenten verwaltet.
Die Dienste auf einem Node umfassen die <a href=/docs/concepts/overview/components/#node-components>Container Runtime</a>, das Kubelet und den Kube-Proxy.
Weitere Informationen finden Sie im Abschnitt Kubernetes Node in der Architekturdesign-Dokumentation.</p><h2 id=node-status>Node Status</h2><p>Der Status eines Nodes enthält folgende Informationen:</p><ul><li><a href=#adressen>Adressen</a></li><li><a href=#zustand>Zustand</a></li><li><a href=#kapazit%C3%A4t>Kapazität</a></li><li><a href=#info>Info</a></li></ul><p>Jeder Abschnitt wird folgend detailliert beschrieben.</p><h3 id=adressen>Adressen</h3><p>Die Verwendung dieser Felder hängt von Ihrem Cloud-Anbieter oder der Bare-Metal-Konfiguration ab.</p><ul><li>HostName: Der vom Kernel des Nodes gemeldete Hostname. Kann mit dem kubelet-Parameter <code>--hostname-override</code> überschrieben werden.</li><li>ExternalIP: In der Regel die IP-Adresse des Nodes, die extern geroutet werden kann (von außerhalb des Clusters verfügbar).</li><li>InternalIP: In der Regel die IP-Adresse des Nodes, die nur innerhalb des Clusters routbar ist.</li></ul><h3 id=zustand>Zustand</h3><p>Das <code>conditions</code> Feld beschreibt den Zustand, aller <code>Running</code> Nodes.</p><table><thead><tr><th>Node Condition</th><th>Beschreibung</th></tr></thead><tbody><tr><td><code>OutOfDisk</code></td><td><code>True</code> wenn auf dem Node nicht genügend freier Speicherplatz zum Hinzufügen neuer Pods vorhanden ist, andernfalls <code>False</code></td></tr><tr><td><code>Ready</code></td><td><code>True</code> wenn der Node in einem guten Zustand und bereit ist Pods aufzunehmen, <code>False</code> wenn der Node nicht in einem guten Zustand ist und nicht bereit ist Pods aufzunehmeb, und <code>Unknown</code> wenn der Node-Controller seit der letzten <code>node-monitor-grace-period</code> nichts von dem Node gehört hat (Die Standardeinstellung beträgt 40 Sekunden)</td></tr><tr><td><code>MemoryPressure</code></td><td><code>True</code> wenn der verfügbare Speicher des Nodes niedrig ist; Andernfalls<code>False</code></td></tr><tr><td><code>PIDPressure</code></td><td><code>True</code> wenn zu viele Prozesse auf dem Node vorhanden sind; Andernfalls<code>False</code></td></tr><tr><td><code>DiskPressure</code></td><td><code>True</code> wenn die Festplattenkapazität niedrig ist. Andernfalls <code>False</code></td></tr><tr><td><code>NetworkUnavailable</code></td><td><code>True</code> wenn das Netzwerk für den Node nicht korrekt konfiguriert ist, andernfalls <code>False</code></td></tr></tbody></table><p>Der Zustand eines Nodes wird als JSON-Objekt dargestellt. Die folgende Antwort beschreibt beispielsweise einen fehlerfreien Node.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;conditions&#34;</span><span>:</span> [
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;Ready&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;True&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>Wenn der Status der <code>Ready</code>-Bedingung <code>Unknown</code> oder <code>False</code> länger als der <code>pod-eviction-timeout</code> bleibt, wird ein Parameter an den <a href=/docs/admin/kube-controller-manager/>kube-controller-manager</a> übergeben und alle Pods auf dem Node werden vom Node Controller gelöscht.</p><p>Die voreingestellte Zeit vor der Entfernung beträgt <strong>fünf Minuten</strong>.
In einigen Fällen, in denen der Node nicht erreichbar ist, kann der Apiserver nicht mit dem Kubelet auf dem Node kommunizieren.
Die Entscheidung, die Pods zu löschen, kann dem Kublet erst mitgeteilt werden, wenn die Kommunikation mit dem Apiserver wiederhergestellt ist.
In der Zwischenzeit können Pods, deren Löschen geplant ist, weiterhin auf dem unzugänglichen Node laufen.</p><p>In Versionen von Kubernetes vor 1.5 würde der Node Controller das Löschen dieser unerreichbaren Pods vom Apiserver <a href=/docs/concepts/workloads/pods/pod/#force-deletion-of-pods>erzwingen</a>. In Version 1.5 und höher erzwingt der Node Controller jedoch keine Pod Löschung, bis bestätigt wird, dass sie nicht mehr im Cluster ausgeführt werden. Pods die auf einem unzugänglichen Node laufen sind eventuell in einem einem <code>Terminating</code> oder <code>Unkown</code> Status. In Fällen, in denen Kubernetes nicht aus der zugrunde liegenden Infrastruktur schließen kann, ob ein Node einen Cluster dauerhaft verlassen hat, muss der Clusteradministrator den Node möglicherweise manuell löschen.
Das Löschen des Kubernetes-Nodeobjekts bewirkt, dass alle auf dem Node ausgeführten Pod-Objekte gelöscht und deren Namen freigegeben werden.</p><p>In Version 1.12 wurde die Funktion <code>TaintNodesByCondition</code> als Beta-Version eingeführt, die es dem Node-Lebenszyklus-Controller ermöglicht, automatisch <a href=/docs/concepts/configuration/taint-and-toleration/>Markierungen</a> (<em>taints</em> in Englisch) zu erstellen, die Bedingungen darstellen.</p><p>Ebenso ignoriert der Scheduler die Bedingungen, wenn er einen Node berücksichtigt; stattdessen betrachtet er die Markierungen (taints) des Nodes und die Toleranzen eines Pod.</p><p>Anwender können jetzt zwischen dem alten Scheduling-Modell und einem neuen, flexibleren Scheduling-Modell wählen.</p><p>Ein Pod, der keine Toleranzen aufweist, wird gemäß dem alten Modell geplant.
Aber ein Pod, die die Taints eines bestimmten Node toleriert, kann auf diesem Node geplant werden.</p><div class="alert alert-warning caution callout" role=alert><strong>Achtung:</strong> Wenn Sie diese Funktion aktivieren, entsteht eine kleine Verzögerung zwischen der Zeit,
in der eine Bedingung beobachtet wird, und der Zeit, in der ein Taint entsteht.
Diese Verzögerung ist in der Regel kürzer als eine Sekunde, aber sie kann die Anzahl
der Pods erhöhen, die erfolgreich geplant, aber vom Kubelet abgelehnt werden.</div><h3 id=kapazität>Kapazität</h3><p>Beschreibt die auf dem Node verfügbaren Ressourcen: CPU, Speicher und die maximale
Anzahl der Pods, die auf dem Node ausgeführt werden können.</p><h3 id=info>Info</h3><p>Allgemeine Informationen zum Node, z. B. Kernelversion, Kubernetes-Version
(kubelet- und kube-Proxy-Version), Docker-Version (falls verwendet), Betriebssystemname.
Die Informationen werden von Kubelet vom Node gesammelt.</p><h2 id=management>Management</h2><p>Im Gegensatz zu <a href=/docs/concepts/workloads/pods/pod/>Pods</a> und <a href=/docs/concepts/services-networking/service/>Services</a>,
ein Node wird nicht von Kubernetes erstellt: Er wird extern von Cloud-Anbietern wie Google Compute Engine erstellt oder ist in Ihrem Pool physischer oder virtueller Maschinen vorhanden.
Wenn Kubernetes also einen Node erstellt, wird ein Objekt erstellt, das den Node darstellt.
Nach der Erstellung überprüft Kubernetes, ob der Node gültig ist oder nicht.</p><p>Wenn Sie beispielsweise versuchen, einen Node aus folgendem Inhalt zu erstellen:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Node&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;10.240.79.157&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;my-first-k8s-node&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Kubernetes erstellt intern ein Node-Objekt (die Darstellung) und validiert den Node durch Zustandsprüfung basierend auf dem Feld <code>metadata.name</code>.
Wenn der Node gültig ist, d.h. wenn alle notwendigen Dienste ausgeführt werden, ist er berechtigt, einen Pod auszuführen.
Andernfalls wird er für alle Clusteraktivitäten ignoriert, bis er gültig wird.</p><div class="alert alert-info note callout" role=alert><strong>Hinweis:</strong> Kubernetes behält das Objekt für den ungültigen Node und prüft ständig seine Gültigkeit.
Sie müssen das Node-Objekt explizit löschen, um diesen Prozess zu stoppen.</div><p>Aktuell gibt es drei Komponenten, die mit dem Kubernetes Node-Interface interagieren: Node Controller, Kubelet und Kubectl.</p><h3 id=node-controller>Node Controller</h3><p>Der Node Controller ist eine Kubernetes-Master-Komponente, die verschiedene Aspekte von Nodes verwaltet.</p><p>Der Node Controller hat mehrere Rollen im Leben eines Nodes.
Der erste ist die Zuordnung eines CIDR-Blocks zu dem Node, wenn er registriert ist (sofern die CIDR-Zuweisung aktiviert ist).</p><p>Die zweite ist, die interne Node-Liste des Node Controllers mit der Liste der verfügbaren Computer des Cloud-Anbieters auf dem neuesten Stand zu halten.
Wenn ein Node in einer Cloud-Umgebung ausgeführt wird und sich in einem schlechten Zustand befindet, fragt der Node Controller den Cloud-Anbieter, ob die virtuelle Maschine für diesen Node noch verfügbar ist. Wenn nicht, löscht der Node Controller den Node aus seiner Node-Liste.</p><p>Der dritte ist die Überwachung des Zustands der Nodes. Der Node Controller ist dafür verantwortlich,
die NodeReady-Bedingung von NodeStatus auf ConditionUnknown zu aktualisieren, wenn ein Node unerreichbar wird (der Node Controller empfängt aus irgendeinem Grund keine Herzschläge mehr, z.B. weil der Node heruntergefahren ist) und später alle Pods aus dem Node zu entfernen (und diese ordnungsgemäss zu beenden), wenn der Node weiterhin unzugänglich ist. (Die Standard-Timeouts sind 40s, um ConditionUnknown zu melden und 5 Minuten, um mit der Evakuierung der Pods zu beginnen).</p><p>Der Node Controller überprüft den Zustand jedes Nodes alle <code>--node-monitor-period</code> Sekunden.</p><p>In Versionen von Kubernetes vor 1.13 ist NodeStatus der Herzschlag des Nodes.
Ab Kubernetes 1.13 wird das Node-Lease-Feature als Alpha-Feature eingeführt (Feature-Gate <code>NodeLease</code>, <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/0009-node-heartbeat.md>KEP-0009</a>).</p><p>Wenn die Node Lease Funktion aktiviert ist, hat jeder Node ein zugeordnetes <code>Lease</code>-Objekt im <code>kube-node-lease</code>-Namespace, das vom Node regelmäßig erneuert wird.
Sowohl NodeStatus als auch Node Lease werden als Herzschläge vom Node aus behandelt.
Node Leases werden häufig erneuert, während NodeStatus nur dann vom Node zu Master gemeldet wird, wenn sich etwas ändert oder genügend Zeit vergangen ist (Standard ist 1 Minute, was länger ist als der Standard-Timeout von 40 Sekunden für unerreichbare Nodes).
Da Node Leases viel lastärmer sind als NodeStatus, macht diese Funktion den Node Herzschlag sowohl in Bezug auf Skalierbarkeit als auch auf die Leistung deutlich effizienter.</p><p>In Kubernetes 1.4 haben wir die Logik der Node-Steuerung aktualisiert, um Fälle besser zu handhaben, in denen eine große Anzahl von Nodes Probleme hat, den Master zu erreichen (z.B. weil der Master Netzwerkprobleme hat).
Ab 1.4 betrachtet der Node-Controller den Zustand aller Nodes im Cluster, wenn er eine Entscheidung über die Enterfung eines Pods trifft.</p><p>In den meisten Fällen begrenzt der Node-Controller die Entfernungsrate auf <code>--node-eviction-rate</code> (Standard 0,1) pro Sekunde, was bedeutet, dass er die Pods nicht von mehr als einem Node pro 10 Sekunden entfernt.</p><p>Das Entfernungsverhalten von Nodes ändert sich, wenn ein Node in einer bestimmten Verfügbarkeitszone ungesund wird.
Der Node-Controller überprüft gleichzeitig, wie viel Prozent der Nodes in der Zone ungesund sind (NodeReady-Bedingung ist ConditionUnknown oder ConditionFalse).
Wenn der Anteil der ungesunden Nodes mindestens <code>--unhealthy-zone-threshold</code> (Standard 0,55) beträgt, wird die Entfernungsrate reduziert:</p><p>Wenn der Cluster klein ist (d.h. weniger als oder gleich <code>--large-cluster-size-threshold</code> Node - Standard 50), werden die Entfernungen gestoppt. Andernfalls wird die Entfernungsrate auf <code>--secondary-node-eviction-rate</code> (Standard 0,01) pro Sekunde reduziert.</p><p>Der Grund, warum diese Richtlinien pro Verfügbarkeitszone implementiert werden, liegt darin, dass eine Verfügbarkeitszone vom Master unerreichbar werden könnte, während die anderen verbunden bleiben. Wenn Ihr Cluster nicht mehrere Verfügbarkeitszonen von Cloud-Anbietern umfasst, gibt es nur eine Verfügbarkeitszone (den gesamten Cluster).</p><p>Ein wichtiger Grund für die Verteilung Ihrer Nodes auf Verfügbarkeitszonen ist, dass die Arbeitsbelastung auf gesunde Zonen verlagert werden kann, wenn eine ganze Zone ausfällt.
Wenn also alle Nodes in einer Zone ungesund sind, entfernt Node Controller mit der normalen <code>--node-eviction-rate</code> Geschwindigkeit.
Der Ausnahmefall ist, wenn alle Zonen völlig ungesund sind (d.h. es gibt keine gesunden Node im Cluster).
In diesem Fall geht der Node-Controller davon aus, dass es ein Problem mit der Master-Konnektivität gibt und stoppt alle Entfernungen, bis die Verbindung wiederhergestellt ist.</p><p>Ab Kubernetes 1.6 ist der Node-Controller auch für die Entfernung von Pods zuständig, die auf Nodes mit <code>NoExecute</code>-Taints laufen, wenn die Pods die Markierungen nicht tolerieren.
Zusätzlich ist der NodeController als Alpha-Funktion, die standardmäßig deaktiviert ist, dafür verantwortlich, Taints hinzuzufügen, die Node Probleme, wie <code>Node unreachable</code> oder <code>not ready</code> entsprechen.
Siehe <a href=/docs/concepts/configuration/taint-and-toleration/>diese Dokumentation</a> für Details über <code>NoExecute</code> Taints und die Alpha-Funktion.</p><p>Ab Version 1.8 kann der Node-Controller für die Erzeugung von Taints, die Node Bedingungen darstellen, verantwortlich gemacht werden. Dies ist eine Alpha-Funktion der Version 1.8.</p><h3 id=selbstregistrierung-von-nodes>Selbstregistrierung von Nodes</h3><p>Wenn das Kubelet-Flag <code>--register-node</code> aktiv ist (Standard), versucht das Kubelet, sich beim API-Server zu registrieren. Dies ist das bevorzugte Muster, das von den meisten Distributionen verwendet wird.</p><p>Zur Selbstregistrierung wird das kubelet mit den folgenden Optionen gestartet:</p><ul><li><code>--kubeconfig</code> - Pfad zu Anmeldeinformationen, um sich beim Apiserver zu authentifizieren.</li><li><code>--cloud-provider</code> - Wie man sich mit einem Cloud-Anbieter unterhält, um Metadaten über sich selbst zu lesen.</li><li><code>--register-node</code> - Automatisch beim API-Server registrieren.</li><li><code>--register-with-taints</code> - Registrieren Sie den Node mit der angegebenen Taints-Liste (Kommagetrennt <code>&lt;key>=&lt;value>:&lt;effect></code>). No-op wenn <code>register-node</code> false ist.</li><li><code>--node-ip</code> - IP-Adresse des Nodes.</li><li><code>--node-labels</code> - Labels, die bei der Registrierung des Nodes im Cluster hinzugefügt werden sollen (Beachten Sie die Richlinien des <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction admission plugin</a> in 1.13+).</li><li><code>--node-status-update-frequency</code> - Gibt an, wie oft kubelet den Nodestatus an den Master übermittelt.</li></ul><p>Wenn der <a href=/docs/reference/access-authn-authz/node/>Node authorization mode</a> und
<a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction admission plugin</a> aktiviert sind,
dürfen kubelets nur ihre eigene Node-Ressource erstellen / ändern.</p><h4 id=manuelle-nodeverwaltung>Manuelle Nodeverwaltung</h4><p>Ein Cluster-Administrator kann Nodeobjekte erstellen und ändern.</p><p>Wenn der Administrator Nodeobjekte manuell erstellen möchte, setzen Sie das kubelet Flag <code>--register-node=false</code>.</p><p>Der Administrator kann Node-Ressourcen ändern (unabhängig von der Einstellung von <code>--register-node</code>).
Zu den Änderungen gehören das Setzen von Labels und das Markieren des Nodes.</p><p>Labels auf Nodes können in Verbindung mit node selectors auf Pods verwendet werden, um die Planung zu steuern, z.B. um einen Pod so zu beschränken, dass er nur auf einer Teilmenge der Nodes ausgeführt werden darf.</p><p>Das Markieren eines Nodes als nicht geplant, verhindert, dass neue Pods für diesen Node geplant werden. Dies hat jedoch keine Auswirkungen auf vorhandene Pods auf dem Node.
Dies ist nützlich als vorbereitender Schritt vor einem Neustart eines Nodes usw.
Um beispielsweise einen Node als nicht geplant zu markieren, führen Sie den folgenden Befehl aus:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl cordon <span style=color:#b8860b>$NODENAME</span>
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Hinweis:</strong> Pods, die von einem DaemonSet-Controller erstellt wurden, umgehen den Kubernetes-Scheduler und respektieren nicht das <em>unschedulable</em> Attribut auf einem Node.
Dies setzt voraus, dass Daemons auf dem Computer verbleiben, auch wenn während der Vorbereitung eines Neustarts keine Anwendungen mehr vorhanden sind.</div><h3 id=node-kapazität>Node Kapazität</h3><p>Die Kapazität des Nodes (Anzahl der CPU und Speichermenge) ist Teil des Nodeobjekts.
Normalerweise registrieren sich Nodes selbst und melden ihre Kapazität beim Erstellen des Nodeobjekts.
Sofern Sie <a href=#Manuelle-Nodeverwaltung>Manuelle Nodeverwaltung</a> betreiben, müssen Sie die Node Kapazität setzen, wenn Sie einen Node hinzufügen.</p><p>Der Kubernetes-Scheduler stellt sicher, dass für alle Pods auf einem Nodes genügend Ressourcen vorhanden sind.
Er prüft, dass die Summe der Requests von Containern auf dem Node nicht größer ist als die Kapazität des Nodes.
Er beinhaltet alle Container die vom kubelet gestarted worden, aber keine Container die direkt von der <a href=/docs/concepts/overview/components/#node-components>container runtime</a> gestartet wurden, noch jegleiche Prozesse die ausserhalb von Containern laufen.</p><p>Wenn Sie Ressourcen explizit für Nicht-Pod-Prozesse reservieren möchten, folgen Sie diesem Lernprogramm um <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved>Ressourcen für Systemdaemons zu reservieren</a>.</p><h2 id=api-objekt>API-Objekt</h2><p>Node ist eine Top-Level-Ressource in der Kubernetes-REST-API. Weitere Details zum API-Objekt finden Sie unter:
<a href=/docs/reference/generated/kubernetes-api/v1.25/#node-v1-core>Node API object</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-63e7fdf87ba61eb2586bb8c625c23506>2 - Master-Node Kommunikation</h1><p>Dieses Dokument katalogisiert die Kommunikationspfade zwischen dem Master (eigentlich dem Apiserver) und des Kubernetes-Clusters.
Die Absicht besteht darin, Benutzern die Möglichkeit zu geben, ihre Installation so anzupassen, dass die Netzwerkkonfiguration so abgesichert wird, dass der Cluster in einem nicht vertrauenswürdigen Netzwerk (oder mit vollständig öffentlichen IP-Adressen eines Cloud-Providers) ausgeführt werden kann.</p><h2 id=cluster-zum-master>Cluster zum Master</h2><p>Alle Kommunikationspfade vom Cluster zum Master enden beim Apiserver (keine der anderen Master-Komponenten ist dafür ausgelegt, Remote-Services verfügbar zu machen).
In einem typischen Setup ist der Apiserver so konfiguriert, dass er Remote-Verbindungen an einem sicheren HTTPS-Port (443) mit einer oder mehreren Formen der <a href=/docs/reference/access-authn-authz/authentication/>Clientauthentifizierung</a> überwacht.
Eine oder mehrere Formene von <a href=/docs/reference/access-authn-authz/authorization/>Autorisierung</a> sollte aktiviert sein, insbesondere wenn <a href=/docs/reference/access-authn-authz/authentication/#anonymous-requests>anonyme Anfragen</a> oder <a href=/docs/reference/access-authn-authz/authentication/#service-account-tokens>Service Account Tokens</a> aktiviert sind.</p><p>Nodes sollten mit dem öffentlichen Stammzertifikat für den Cluster konfiguriert werden, sodass sie eine sichere Verbindung zum Apiserver mit gültigen Client-Anmeldeinformationen herstellen können.
Beispielsweise bei einer gewöhnlichen GKE-Konfiguration enstprechen die dem kubelet zur Verfügung gestellten Client-Anmeldeinformationen eines Client-Zertifikats.
Lesen Sie über <a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>kubelet TLS bootstrapping</a> zur automatisierten Bereitstellung von kubelet-Client-Zertifikaten.</p><p>Pods, die eine Verbindung zum Apiserver herstellen möchten, können dies auf sichere Weise tun, indem sie ein Dienstkonto verwenden, sodass Kubernetes das öffentliche Stammzertifikat und ein gültiges Trägertoken automatisch in den Pod einfügt, wenn er instanziiert wird.
Der <code>kubernetes</code>-Dienst (in allen Namespaces) ist mit einer virtuellen IP-Adresse konfiguriert, die (über den Kube-Proxy) an den HTTPS-Endpunkt auf dem Apiserver umgeleitet wird.</p><p>Die Master-Komponenten kommunizieren auch über den sicheren Port mit dem Cluster-Apiserver.</p><p>Der Standardbetriebsmodus für Verbindungen vom Cluster (Knoten und Pods, die auf den Knoten ausgeführt werden) zum Master ist daher standardmäßig gesichert und kann über nicht vertrauenswürdige und/oder öffentliche Netzwerke laufen.</p><h2 id=master-zum-cluster>Master zum Cluster</h2><p>Es gibt zwei primäre Kommunikationspfade vom Master (Apiserver) zum Cluster.
Der Erste ist vom Apiserver hin zum Kubelet-Prozess, der auf jedem Node im Cluster ausgeführt wird.
Der Zweite ist vom Apiserver zu einem beliebigen Node, Pod oder Dienst über die Proxy-Funktionalität des Apiservers.</p><h3 id=apiserver-zum-kubelet>Apiserver zum kubelet</h3><p>Die Verbindungen vom Apiserver zum Kubelet werden verwendet für:</p><ul><li>Das Abrufen von Protokollen für Pods.</li><li>Das Verbinden (durch kubectl) mit laufenden Pods.</li><li>Die Bereitstellung der Portweiterleitungsfunktion des kubelet.</li></ul><p>Diese Verbindungen enden am HTTPS-Endpunkt des kubelet.
Standardmäßig überprüft der Apiserver das Serverzertifikat des Kubelets nicht, was die Verbindung angreifbar für Man-in-the-Middle-Angriffe macht. Die Kommunikation ist daher <strong>unsicher</strong>, wenn die Verbindungen über nicht vertrauenswürdige und/oder öffentliche Netzwerke laufen.</p><p>Um diese Verbindung zu überprüfen, verwenden Sie das Flag <code>--kubelet-certificate-authority</code>, um dem Apiserver ein Stammzertifikatbündel bereitzustellen, das zur Überprüfung des Server-Zertifikats des kubelets verwendet wird.</p><p>Wenn dies nicht möglich ist, verwenden Sie <a href=/docs/concepts/architecture/master-node-communication/#ssh-tunnels>SSH tunneling</a>
zwischen dem Apiserver und dem kubelet, falls es erforderlich ist eine Verbindung über ein nicht vertrauenswürdiges oder öffentliches Netz zu vermeiden.</p><p>Außerdem sollte <a href=/docs/admin/kubelet-authentication-authorization/>Kubelet Authentifizierung und/oder Autorisierung</a> aktiviert sein, um die kubelet-API abzusichern.</p><h3 id=apiserver-zu-nodes-pods-und-services>Apiserver zu Nodes, Pods und Services</h3><p>Die Verbindungen vom Apiserver zu einem Node, Pod oder Dienst verwenden standardmäßig einfache HTTP-Verbindungen und werden daher weder authentifiziert noch verschlüsselt.
Sie können über eine sichere HTTPS-Verbindung ausgeführt werden, indem dem Node, dem Pod oder dem Servicenamen in der API-URL "https:" vorangestellt wird. Das vom HTTPS-Endpunkt bereitgestellte Zertifikat wird jedoch nicht überprüft, und es werden keine Clientanmeldeinformationen bereitgestellt. Die Verbindung wird zwar verschlüsselt, garantiert jedoch keine Integrität.
Diese Verbindungen <strong>sind derzeit nicht sicher</strong> innerhalb von nicht vertrauenswürdigen und/oder öffentlichen Netzen.</p><h3 id=ssh-tunnels>SSH Tunnels</h3><p>Kubernetes unterstützt SSH-Tunnel zum Schutz der Master -> Cluster Kommunikationspfade.
In dieser Konfiguration initiiert der Apiserver einen SSH-Tunnel zu jedem Node im Cluster (Verbindung mit dem SSH-Server, der mit Port 22 läuft), und leitet den gesamten Datenverkehr für ein kubelet, einen Node, einen Pod oder einen Dienst durch den Tunnel.
Dieser Tunnel stellt sicher, dass der Datenverkehr nicht außerhalb des Netzwerks sichtbar ist, in dem die Knoten ausgeführt werden.</p><p>SSH-Tunnel werden zur Zeit nicht unterstützt. Sie sollten also nicht verwendet werden, sei denn, man weiß, was man tut. Ein Ersatz für diesen Kommunikationskanal wird entwickelt.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bc804b02614d67025b4c788f1ca87fbc>3 - Zugrunde liegende Konzepte des Cloud Controller Manager</h1><p>Das Konzept des Cloud Controller Managers (CCM) (nicht zu verwechseln mit der Binärdatei) wurde ursprünglich entwickelt, um Cloud-spezifischen Anbieter Code und den Kubernetes Kern unabhängig voneinander entwickeln zu können. Der Cloud Controller Manager läuft zusammen mit anderen Master Komponenten wie dem Kubernetes Controller Manager, dem API-Server und dem Scheduler auf dem Host. Es kann auch als Kubernetes Addon gestartet werden, in diesem Fall läuft er auf Kubernetes.</p><p>Das Design des Cloud Controller Managers basiert auf einem Plugin Mechanismus, der es neuen Cloud Anbietern ermöglicht, sich mit Kubernetes einfach über Plugins zu integrieren. Es gibt Pläne für die Einbindung neuer Cloud Anbieter auf Kubernetes und für die Migration von Cloud Anbietern vom alten Modell auf das neue CCM-Modell.</p><p>Dieses Dokument beschreibt die Konzepte hinter dem Cloud Controller Manager und gibt Details zu den damit verbundenen Funktionen.</p><p>Die Architektur eines Kubernetes Clusters ohne den Cloud Controller Manager sieht wie folgt aus:</p><p><img src=/images/docs/pre-ccm-arch.png alt="Pre CCM Kube Arch"></p><h2 id=design>Design</h2><p>Im vorhergehenden Diagramm sind Kubernetes und der Cloud-Provider über mehrere verschiedene Komponenten integriert:</p><ul><li>Kubelet</li><li>Kubernetes Controller Manager</li><li>Kubernetes API Server</li></ul><p>CCM konsolidiert die gesamte Abhängigkeit der Cloud Logik von den drei vorhergehenden Komponenten zu einem einzigen Integrationspunkt mit der Cloud. So sieht die neue Architektur mit dem CCM aus:</p><p><img src=/images/docs/post-ccm-arch.png alt="CCM Kube Arch"></p><h2 id=komponenten-des-ccm>Komponenten des CCM</h2><p>Der CCM löst einen Teil der Funktionalität des Kubernetes Controller Managers (KCM) ab und führt ihn als separaten Prozess aus. Konkret trennt es die Cloud abhängigen Controller im KCM. Der KCM verfügt über die folgenden Cloud abhängigen Steuerungsschleifen:</p><ul><li>Node Controller</li><li>Volume Controller</li><li>Route Controller</li><li>Service Controller</li></ul><p>In der Version 1.9 führt der CCM die folgenden Controller aus der vorhergehenden Liste aus:</p><ul><li>Node Controller</li><li>Route Controller</li><li>Service Controller</li></ul><div class="alert alert-info note callout" role=alert><strong>Hinweis:</strong> Der Volume Controller wurde bewusst nicht als Teil des CCM gewählt. Aufgrund der Komplexität und der bestehenden Bemühungen, herstellerspezifische Volume Logik zu abstrahieren, wurde entschieden, dass der Volume Controller nicht zum CCM verschoben wird.</div><p>Der ursprüngliche Plan, Volumes mit CCM zu integrieren, sah die Verwendung von Flex-Volumes vor welche austauschbare Volumes unterstützt. Allerdings ist eine konkurrierende Initiative namens CSI geplant, um Flex zu ersetzen.</p><p>In Anbetracht dieser Dynamik haben wir uns entschieden, eine Zwischenstopp durchzuführen um die Unterschiede zu beobachten , bis das CSI bereit ist.</p><h2 id=funktionen-des-ccm>Funktionen des CCM</h2><p>Der CCM erbt seine Funktionen von Komponenten des Kubernetes, die von einem Cloud Provider abhängig sind. Dieser Abschnitt ist auf der Grundlage dieser Komponenten strukturiert.</p><h3 id=1-kubernetes-controller-manager>1. Kubernetes Controller Manager</h3><p>Die meisten Funktionen des CCM stammen aus dem KCM. Wie im vorherigen Abschnitt erwähnt, führt das CCM die folgenden Steuerschleifen durch:</p><ul><li>Node Controller</li><li>Route Controller</li><li>Service Controller</li></ul><h4 id=node-controller>Node Controller</h4><p>Der Node Controller ist für die Initialisierung eines Knotens verantwortlich, indem er Informationen über die im Cluster laufenden Knoten vom Cloud Provider erhält. Der Node Controller führt die folgenden Funktionen aus:</p><ol><li>Initialisierung eines Knoten mit Cloud-spezifischen Zonen-/Regionen Labels.</li><li>Initialisieren von Knoten mit Cloud-spezifischen Instanzdetails, z.B. Typ und Größe.</li><li>Ermitteln der Netzwerkadressen und des Hostnamen des Knotens.</li><li>Falls ein Knoten nicht mehr reagiert, überprüft der Controller die Cloud, um festzustellen, ob der Knoten aus der Cloud gelöscht wurde.
Wenn der Knoten aus der Cloud gelöscht wurde, löscht der Controller das Kubernetes Node Objekt.</li></ol><h4 id=route-controller>Route Controller</h4><p>Der Route Controller ist dafür verantwortlich, Routen in der Cloud so zu konfigurieren, dass Container auf verschiedenen Knoten im Kubernetes Cluster miteinander kommunizieren können. Der Route Controller ist nur auf einem Google Compute Engine Cluster anwendbar.</p><h4 id=service-controller>Service Controller</h4><p>Der Service Controller ist verantwortlich für das Abhören von Ereignissen zum Erstellen, Aktualisieren und Löschen von Diensten. Basierend auf dem aktuellen Stand der Services in Kubernetes konfiguriert es Cloud Load Balancer (wie ELB, Google LB oder Oracle Cloud Infrastructure LB), um den Zustand der Services in Kubernetes abzubilden. Darüber hinaus wird sichergestellt, dass die Service Backends für Cloud Loadbalancer auf dem neuesten Stand sind.</p><h3 id=2-kubelet>2. Kubelet</h3><p>Der Node Controller enthält die Cloud-abhängige Funktionalität des Kubelets. Vor der Einführung des CCM war das Kubelet für die Initialisierung eines Knotens mit cloudspezifischen Details wie IP-Adressen, Regions-/Zonenbezeichnungen und Instanztypinformationen verantwortlich. Mit der Einführung des CCM wurde diese Initialisierungsoperation aus dem Kubelet in das CCM verschoben.</p><p>In diesem neuen Modell initialisiert das Kubelet einen Knoten ohne cloudspezifische Informationen. Es fügt jedoch dem neu angelegten Knoten einen Taint hinzu, der den Knoten nicht verplanbar macht, bis der CCM den Knoten mit cloudspezifischen Informationen initialisiert. Dann wird der Taint entfernt.</p><h2 id=plugin-mechanismus>Plugin Mechanismus</h2><p>Der Cloud Controller Manager verwendet die Go Schnittstellen, um Implementierungen aus jeder Cloud einzubinden. Konkret verwendet dieser das CloudProvider Interface, das <a href=https://github.com/kubernetes/cloud-provider/blob/9b77dc1c384685cb732b3025ed5689dd597a5971/cloud.go#L42-L62>hier</a> definiert ist.</p><p>Die Implementierung der vier oben genannten geteiltent Controllern und einigen Scaffolding sowie die gemeinsame CloudProvider Schnittstelle bleiben im Kubernetes Kern. Cloud Provider spezifische Implementierungen werden außerhalb des Kerns aufgebaut und implementieren im Kern definierte Schnittstellen.</p><p>Weitere Informationen zur Entwicklung von Plugins findest du im Bereich <a href=/docs/tasks/administer-cluster/developing-cloud-controller-manager/>Entwickeln von Cloud Controller Manager</a>.</p><h2 id=autorisierung>Autorisierung</h2><p>Dieser Abschnitt beschreibt den Zugriff, den der CCM für die Ausführung seiner Operationen auf verschiedene API Objekte benötigt.</p><h3 id=node-controller-1>Node Controller</h3><p>Der Node Controller funktioniert nur mit Node Objekten. Es benötigt vollen Zugang zu get, list, create, update, patch, watch, und delete Node Objekten.</p><p>v1/Node:</p><ul><li>Get</li><li>List</li><li>Create</li><li>Update</li><li>Patch</li><li>Watch</li><li>Delete</li></ul><h3 id=route-controller-1>Route Controller</h3><p>Der Route Controller horcht auf die Erstellung von Node Objekten und konfiguriert die Routen entsprechend. Es erfordert get Zugriff auf die Node Objekte.</p><p>v1/Node:</p><ul><li>Get</li></ul><h3 id=service-controller-1>Service Controller</h3><p>Der Service Controller hört auf die Service Objekt Events create, update und delete und konfiguriert dann die Endpunkte für diese Services entsprechend.</p><p>Um auf Services zuzugreifen, benötigt man list und watch Berechtigung. Um die Services zu aktualisieren, sind patch und update Zugriffsrechte erforderlich.</p><p>Um die Endpunkte für die Dienste einzurichten, benötigt der Controller Zugriff auf create, list, get, watch und update.</p><p>v1/Service:</p><ul><li>List</li><li>Get</li><li>Watch</li><li>Patch</li><li>Update</li></ul><h3 id=sonstiges>Sonstiges</h3><p>Die Implementierung des Kerns des CCM erfordert den Zugriff auf die Erstellung von Ereignissen und zur Gewährleistung eines sicheren Betriebs den Zugriff auf die Erstellung von ServiceAccounts.</p><p>v1/Event:</p><ul><li>Create</li><li>Patch</li><li>Update</li></ul><p>v1/ServiceAccount:</p><ul><li>Create</li></ul><p>Die RBAC ClusterRole für CCM sieht wie folgt aus:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- events<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#39;*&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes/status<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- services<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- serviceaccounts<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- persistentvolumes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- endpoints<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=anbieter-implementierung>Anbieter Implementierung</h2><p>Die folgenden Cloud Anbieter haben CCMs implementiert:</p><ul><li><a href=https://github.com/digitalocean/digitalocean-cloud-controller-manager>Digital Ocean</a></li><li><a href=https://github.com/oracle/oci-cloud-controller-manager>Oracle</a></li><li><a href=https://github.com/kubernetes/cloud-provider-azure>Azure</a></li><li><a href=https://github.com/kubernetes/cloud-provider-gcp>GCP</a></li><li><a href=https://github.com/kubernetes/cloud-provider-aws>AWS</a></li><li><a href=https://github.com/baidu/cloud-provider-baiducloud>BaiduCloud</a></li><li><a href=https://github.com/linode/linode-cloud-controller-manager>Linode</a></li></ul><h2 id=cluster-administration>Cluster Administration</h2><p>Eine vollständige Anleitung zur Konfiguration und zum Betrieb des CCM findest du <a href=/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager>hier</a>.</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/de/docs/home/>Home</a>
<a class=text-white href=/de/blog/>Blog</a>
<a class=text-white href=/de/training/>Schulungen</a>
<a class=text-white href=/de/partners/>Partner</a>
<a class=text-white href=/de/community/>Community</a>
<a class=text-white href=/de/case-studies/>Fallstudien</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>