<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – 生产级别的容器编排系统</title><link>https://kubernetes.io/zh-cn/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/zh-cn/</link></image><atom:link href="https://kubernetes.io/zh-cn/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: “Kubernetes 1.25：对使用用户名字空间运行 Pod 提供 Alpha 支持”</title><link>https://kubernetes.io/zh-cn/blog/2022/10/03/userns-alpha/</link><pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/10/03/userns-alpha/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes 1.25: alpha support for running Pods with user namespaces"
date: 2022-10-03
slug: userns-alpha
-->
&lt;!--
**Authors:** Rodrigo Campos (Microsoft), Giuseppe Scrivano (Red Hat)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Rodrigo Campos（Microsoft）、Giuseppe Scrivano（Red Hat）&lt;/p>
&lt;!--
Kubernetes v1.25 introduces the support for user namespaces.
-->
&lt;p>Kubernetes v1.25 引入了对用户名字空间的支持。&lt;/p>
&lt;!--
This is a major improvement for running secure workloads in
Kubernetes. Each pod will have access only to a limited subset of the
available UIDs and GIDs on the system, thus adding a new security
layer to protect from other pods running on the same system.
-->
&lt;p>这是在 Kubernetes 中运行安全工作负载的一项重大改进。
每个 Pod 只能访问系统上可用 UID 和 GID 的有限子集，
因此添加了一个新的安全层来保护 Pod 免受运行在同一系统上的其他 Pod 的影响。&lt;/p>
&lt;!--
## How does it work?
A process running on Linux can use up to 4294967296 different UIDs and
GIDs.
User namespaces is a Linux feature that allows mapping a set of users
in the container to different users in the host, thus restricting what
IDs a process can effectively use.
Furthermore, the capabilities granted in a new user namespace do not
apply in the host initial namespaces.
-->
&lt;h2 id="how-does-it-work">它是如何工作的？ &lt;/h2>
&lt;p>在 Linux 上运行的进程最多可以使用 4294967296 个不同的 UID 和 GID。&lt;/p>
&lt;p>用户名字空间是 Linux 的一项特性，它允许将容器中的一组用户映射到主机中的不同用户，
从而限制进程可以实际使用的 ID。
此外，在新用户名字空间中授予的权能不适用于主机初始名字空间。&lt;/p>
&lt;!--
## Why is it important?
There are mainly two reasons why user namespaces are important:
- improve security since they restrict the IDs a pod can use, so each
pod can run in its own separate environment with unique IDs.
- enable running workloads as root in a safer manner.
In a user namespace we can map the root user inside the pod to a
non-zero ID outside the container, containers believe in running as
root while they are a regular unprivileged ID from the host point of
view.
The process can keep capabilities that are usually restricted to
privileged pods and do it in a safe way since the capabilities granted
in a new user namespace do not apply in the host initial namespaces.
-->
&lt;h2 id="why-is-it-important">它为什么如此重要？ &lt;/h2>
&lt;p>用户名字空间之所以重要，主要有两个原因：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>提高安全性。因为它们限制了 Pod 可以使用的 ID，
因此每个 Pod 都可以在其自己的具有唯一 ID 的单独环境中运行。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>以更安全的方式使用 root 身份运行工作负载。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>在用户名字空间中，我们可以将 Pod 内的 root 用户映射到容器外的非零 ID，
容器将认为是 root 身份在运行，而从主机的角度来看，它们是常规的非特权 ID。&lt;/p>
&lt;p>该进程可以保留通常仅限于特权 Pod 的功能，并以安全的方式执行这类操作，
因为在新用户名字空间中授予的功能不适用于主机初始名字空间。&lt;/p>
&lt;!--
## How do I enable user namespaces?
At the moment, user namespaces support is opt-in, so you must enable
it for a pod setting `hostUsers` to `false` under the pod spec stanza:
-->
&lt;h2 id="how-do-i-enable-user-namespaces">如何启用用户名字空间&lt;/h2>
&lt;p>目前，对用户名字空间的支持是可选的，因此你必须在 Pod 规约部分将
&lt;code>hostUsers&lt;/code> 设置为 &lt;code>false&lt;/code> 以启用用户名字空间：&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Pod
spec:
hostUsers: false
containers:
- name: nginx
image: docker.io/nginx
&lt;/code>&lt;/pre>&lt;!--
The feature is behind a feature gate, so make sure to enable
the `UserNamespacesStatelessPodsSupport` gate before you can use
the new feature.
-->
&lt;p>该特性目前还处于 Alpha 阶段，默认是禁用的，因此在使用此新特性之前，
请确保启用了 &lt;code>UserNamespacesStatelessPodsSupport&lt;/code> 特性门控。&lt;/p>
&lt;!--
The runtime must also support user namespaces:
* containerd: support is planned for the 1.7 release. See containerd
issue [#7063][containerd-userns-issue] for more details.
* CRI-O: v1.25 has support for user namespaces.
Support for this in `cri-dockerd` is [not planned][CRI-dockerd-issue] yet.
-->
&lt;p>此外，运行时也必须支持用户名字空间：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Containerd：计划在 1.7 版本中提供支持。
进一步了解，请参阅 Containerd issue &lt;a href="https://github.com/containerd/containerd/issues/7063">#7063&lt;/a>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CRI-O：v1.25 支持用户名字空间。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;code>cri-dockerd&lt;/code> 对用户名字空间的支持&lt;a href="https://github.com/Mirantis/cri-dockerd/issues/74">尚无计划&lt;/a>。&lt;/p>
&lt;!--
## How do I get involved?
You can reach SIG Node by several means:
- Slack: [#sig-node](https://kubernetes.slack.com/messages/sig-node)
- [Mailing list](https://groups.google.com/forum/#!forum/kubernetes-sig-node)
- [Open Community Issues/PRs](https://github.com/kubernetes/community/labels/sig%2Fnode)
You can also contact us directly:
- GitHub / Slack: @rata @giuseppe
-->
&lt;h2 id="how-do-i-get-involved">我如何参与？ &lt;/h2>
&lt;p>你可以通过多种方式联系 SIG Node：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Slack: &lt;a href="https://kubernetes.slack.com/messages/sig-node">#sig-node&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">邮件列表&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fnode">开源社区 Issue/PR&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>你也可以直接联系我们：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>GitHub / Slack: @rata @giuseppe&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fnode">开源社区 Issue/PR&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.25：应用滚动上线所用的两个特性进入稳定阶段</title><link>https://kubernetes.io/zh-cn/blog/2022/09/15/app-rollout-features-reach-stable/</link><pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/09/15/app-rollout-features-reach-stable/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes 1.25: Two Features for Apps Rollouts Graduate to Stable"
date: 2022-09-15
slug: "app-rollout-features-reach-stable"
-->
&lt;!--
**Authors:** Ravi Gudimetla (Apple), Filip Křepinský (Red Hat), Maciej Szulik (Red Hat)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Ravi Gudimetla (Apple)、Filip Křepinský (Red Hat)、Maciej Szulik (Red Hat)&lt;/p>
&lt;!--
This blog describes the two features namely `minReadySeconds` for StatefulSets and `maxSurge` for DaemonSets that SIG Apps is happy to graduate to stable in Kubernetes 1.25.
Specifying `minReadySeconds` slows down a rollout of a StatefulSet, when using a `RollingUpdate` value in `.spec.updateStrategy` field, by waiting for each pod for a desired time.
This time can be used for initializing the pod (e.g. warming up the cache) or as a delay before acknowledging the pod.
-->
&lt;p>这篇博客描述了两个特性，即用于 StatefulSet 的 &lt;code>minReadySeconds&lt;/code> 以及用于 DaemonSet 的 &lt;code>maxSurge&lt;/code>，
SIG Apps 很高兴宣布这两个特性在 Kubernetes 1.25 进入稳定阶段。&lt;/p>
&lt;p>当 &lt;code>.spec.updateStrategy&lt;/code> 字段设置为 &lt;code>RollingUpdate&lt;/code> 时，
你可以设置 &lt;code>minReadySeconds&lt;/code>， 通过让每个 Pod 等待一段预期时间来减缓 StatefulSet 的滚动上线。&lt;/p>
&lt;!--
`maxSurge` allows a DaemonSet workload to run multiple instances of the same pod on a node during a rollout when using a `RollingUpdate` value in `.spec.updateStrategy` field.
This helps to minimize the downtime of the DaemonSet for consumers.
These features were already available in a Deployment and other workloads. This graduation helps to align this functionality across the workloads.
-->
&lt;p>当 &lt;code>.spec.updateStrategy&lt;/code> 字段设置为 &lt;code>RollingUpdate&lt;/code> 时，
&lt;code>maxSurge&lt;/code> 允许 DaemonSet 工作负载在滚动上线期间在一个节点上运行同一 Pod 的多个实例。
这对于消费者而言有助于将 DaemonSet 的停机时间降到最低。&lt;/p>
&lt;p>这两个特性也可用于 Deployment 和其他工作负载。此功能的提级有助于将这一功能在所有工作负载上对齐。&lt;/p>
&lt;!--
## What problems do these features solve?
### minReadySeconds for StatefulSets {#solved-problem-statefulset-minreadyseconds}
-->
&lt;h2 id="what-problems-do-these-features-solve">这两个特性能解决什么问题？ &lt;/h2>
&lt;h3 id="solved-problem-statefulset-minreadyseconds">针对 StatefulSet 的 minReadySeconds &lt;/h3>
&lt;!--
`minReadySeconds` ensures that the StatefulSet workload is `Ready` for the given number of seconds before reporting the
pod as `Available`. The notion of being `Ready` and `Available` is quite important for workloads. For example, some workloads, like Prometheus with multiple instances of Alertmanager, should be considered `Available` only when the Alertmanager's state transfer is complete. `minReadySeconds` also helps when using loadbalancers with cloud providers. Since the pod should be `Ready` for the given number of seconds, it provides buffer time to prevent killing pods in rotation before new pods show up.
-->
&lt;p>&lt;code>minReadySeconds&lt;/code> 确保 StatefulSet 工作负载在给定的秒数内处于 &lt;code>Ready&lt;/code>，
然后才会将该 Pod 报告为 &lt;code>Available&lt;/code>。
处于 &lt;code>Ready&lt;/code> 和 &lt;code>Available&lt;/code> 状况的这种说法对工作负载相当重要。
例如 Prometheus 这些工作负载有多个 Alertmanager 实例，
只有 Alertmanager 的状态转换完成后才应该被视为 &lt;code>Available&lt;/code>。
&lt;code>minReadySeconds&lt;/code> 还有助于云驱动确定何时使用负载均衡器。
因为 Pod 应在给定的秒数内处于 &lt;code>Ready&lt;/code>，所以这就提供了一段缓冲时间，
防止新 Pod 还没起来之前就在轮转过程中杀死了旧 Pod。&lt;/p>
&lt;!--
### maxSurge for DaemonSets {#how-use-daemonset-maxsurge}
Kubernetes system-level components like CNI, CSI are typically run as DaemonSets. These components can have impact on the availability of the workloads if those DaemonSets go down momentarily during the upgrades. The feature allows DaemonSet pods to temporarily increase their number, thereby ensuring zero-downtime for the DaemonSets.
Please note that the usage of `hostPort` in conjunction with `maxSurge` in DaemonSets is not allowed as DaemonSet pods are tied to a single node and two active pods cannot share the same port on the same node.
-->
&lt;h3 id="how-use-daemonset-maxsurge">针对 DaemonSet 的 maxSurge &lt;/h3>
&lt;p>CNI、CSI 这类 Kubernetes 系统级别的组件通常以 DaemonSet 方式运行。如果这些 DaemonSet 在升级期间瞬间挂掉，
对应的组件可能会影响工作负载的可用性。此特性允许 DaemonSet Pod 临时增加数量，以此确保 DaemonSet 的停机时间为零。&lt;/p>
&lt;p>请注意在 DaemonSet 中不允许同时使用 &lt;code>hostPort&lt;/code> 和 &lt;code>maxSurge&lt;/code>，
因为 DaemonSet Pod 被捆绑到了一个节点，所以两个活跃的 Pod 无法共享同一节点上的相同端口。&lt;/p>
&lt;!--
## How does it work?
### minReadySeconds for StatefulSets {#how-does-statefulset-minreadyseconds-work}
The StatefulSet controller watches for the StatefulSet pods and counts how long a particular pod has been in the `Running` state, if this value is greater than or equal to the time specified in `.spec.minReadySeconds` field of the StatefulSet, the StatefulSet controller updates the `AvailableReplicas` field in the StatefulSet's status.
-->
&lt;h2 id="how-does-it-work">工作原理 &lt;/h2>
&lt;h3 id="how-does-statefulset-minreadyseconds-work">针对 StatefulSet 的 minReadySeconds &lt;/h3>
&lt;p>StatefulSet 控制器监视 StatefulSet Pod 并统计特定的 Pod 已处于 &lt;code>Running&lt;/code> 状态多长时间了，
如果这个值大于或等于 StatefulSet 的 &lt;code>.spec.minReadySeconds&lt;/code> 字段中指定的时间，
StatefulSet 控制器将更新 StatefulSet 的状态中的 &lt;code>AvailableReplicas&lt;/code> 字段。&lt;/p>
&lt;!--
### maxSurge for DaemonSets {#how-does-daemonset-maxsurge-work}
The DaemonSet controller creates the additional pods (above the desired number resulting from DaemonSet spec) based on the value given in `.spec.strategy.rollingUpdate.maxSurge`. The additional pods would run on the same node where the old DaemonSet pod is running till the old pod gets killed.
-->
&lt;h3 id="how-does-daemonset-maxsurge-work">针对 DaemonSet 的 maxSurge &lt;/h3>
&lt;p>DaemonSet 控制器根据 &lt;code>.spec.strategy.rollingUpdate.maxSurge&lt;/code> 中给出的值创建额外 Pod
（超出 DaemonSet 规约所设定的预期数量）。
这些 Pod 将运行在旧 DaemonSet Pod 运行所在的同一节点上，直到这个旧 Pod 被杀死为止。&lt;/p>
&lt;!--
- The default value is 0.
- The value cannot be `0` when `MaxUnavailable` is 0.
- The value can be specified either as an absolute number of pods, or a percentage (rounded up) of desired pods.
-->
&lt;ul>
&lt;li>默认值为 0。&lt;/li>
&lt;li>当 &lt;code>MaxUnavailable&lt;/code> 为 0 时此值不能为 &lt;code>0&lt;/code>。&lt;/li>
&lt;li>此值可以指定为一个绝对的 Pod 个数或预期 Pod 总数的百分比（向上取整）。&lt;/li>
&lt;/ul>
&lt;!--
## How do I use it?
### minReadySeconds for StatefulSets {#how-use-statefulset-minreadyseconds}
Specify a value for `minReadySeconds` for any StatefulSet and check if pods are available or not by inspecting
`AvailableReplicas` field using:
-->
&lt;h2 id="how-do-i-use-it">我如何使用它？ &lt;/h2>
&lt;h3 id="how-use-statefulset-minreadyseconds">针对 StatefulSet 的 minReadySeconds &lt;/h3>
&lt;p>执行以下命令为任意 StatefulSet 指定一个 &lt;code>minReadySeconds&lt;/code> 值，
通过检验 &lt;code>AvailableReplicas&lt;/code> 字段查看这些 Pod 是否可用：&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl get statefulset/&amp;lt;StatefulSet 名称&amp;gt; -o yaml
&lt;/code>&lt;/pre>&lt;!--
Please note that the default value of `minReadySeconds` is 0.
### maxSurge for DaemonSets {#how-use-daemonset-maxsurge}
Specify a value for `.spec.updateStrategy.rollingUpdate.maxSurge` and set `.spec.updateStrategy.rollingUpdate.maxUnavailable` to `0`.
Then observe a faster rollout and higher number of pods running at the same time in the next rollout.
-->
&lt;p>请注意 &lt;code>minReadySeconds&lt;/code> 的默认值为 0。&lt;/p>
&lt;h3 id="how-use-daemonset-maxsurge">针对 DaemonSet 的 maxSurge &lt;/h3>
&lt;p>为 &lt;code>.spec.updateStrategy.rollingUpdate.maxSurge&lt;/code> 指定一个值并将
&lt;code>.spec.updateStrategy.rollingUpdate.maxUnavailable&lt;/code> 设置为 &lt;code>0&lt;/code>。&lt;/p>
&lt;p>然后观察下一次滚动上线是不是更快，同时运行的 Pod 数量是不是更多。&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl rollout restart daemonset &amp;lt;name_of_the_daemonset&amp;gt;
kubectl get pods -w
&lt;/code>&lt;/pre>&lt;!--
## How can I learn more?
### minReadySeconds for StatefulSets {#learn-more-statefulset-minreadyseconds}
- Documentation: https://k8s.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds
- KEP: https://github.com/kubernetes/enhancements/issues/2599
- API Changes: https://github.com/kubernetes/kubernetes/pull/100842
-->
&lt;h2 id="how-can-i-learn-more">我如何才能了解更多？ &lt;/h2>
&lt;h3 id="learn-more-statefulset-minreadyseconds">针对 StatefulSet 的 minReadySeconds &lt;/h3>
&lt;ul>
&lt;li>文档： &lt;a href="https://k8s.io/zh-cn/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds">https://k8s.io/zh-cn/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds&lt;/a>&lt;/li>
&lt;li>KEP： &lt;a href="https://github.com/kubernetes/enhancements/issues/2599">https://github.com/kubernetes/enhancements/issues/2599&lt;/a>&lt;/li>
&lt;li>API 变更： &lt;a href="https://github.com/kubernetes/kubernetes/pull/100842">https://github.com/kubernetes/kubernetes/pull/100842&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### maxSurge for DaemonSets {#learn-more-daemonset-maxsurge}
- Documentation: https://k8s.io/docs/tasks/manage-daemon/update-daemon-set/
- KEP: https://github.com/kubernetes/enhancements/issues/1591
- API Changes: https://github.com/kubernetes/kubernetes/pull/96375
-->
&lt;h3 id="learn-more-daemonset-maxsurge">针对 DaemonSet 的 maxSurge &lt;/h3>
&lt;ul>
&lt;li>文档： &lt;a href="https://k8s.io/zh-cn/docs/tasks/manage-daemon/update-daemon-set/">https://k8s.io/zh-cn/docs/tasks/manage-daemon/update-daemon-set/&lt;/a>&lt;/li>
&lt;li>KEP： &lt;a href="https://github.com/kubernetes/enhancements/issues/1591">https://github.com/kubernetes/enhancements/issues/1591&lt;/a>&lt;/li>
&lt;li>API 变更： &lt;a href="https://github.com/kubernetes/kubernetes/pull/96375">https://github.com/kubernetes/kubernetes/pull/96375&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## How do I get involved?
Please reach out to us on [#sig-apps](https://kubernetes.slack.com/archives/C18NZM5K9) channel on Slack, or through the SIG Apps mailing list [kubernetes-sig-apps@googlegroups.com](https://groups.google.com/g/kubernetes-sig-apps).
-->
&lt;h2 id="how-do-i-get-involved">我如何参与？ &lt;/h2>
&lt;p>请通过 Slack &lt;a href="https://kubernetes.slack.com/archives/C18NZM5K9">#sig-apps&lt;/a> 频道或通过 SIG Apps
邮件列表 &lt;a href="https://groups.google.com/g/kubernetes-sig-apps">kubernetes-sig-apps@googlegroups.com&lt;/a> 联系我们。&lt;/p></description></item><item><title>Blog: Kubernetes 1.25：Pod 新增 PodHasNetwork 状况</title><link>https://kubernetes.io/zh-cn/blog/2022/09/14/pod-has-network-condition/</link><pubDate>Wed, 14 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/09/14/pod-has-network-condition/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.25: PodHasNetwork Condition for Pods'
date: 2022-09-14
slug: pod-has-network-condition
-->
&lt;!--
**Author:**
Deep Debroy (Apple)
-->
&lt;p>&lt;strong>作者：&lt;/strong>
Deep Debroy (Apple)&lt;/p>
&lt;!--
Kubernetes 1.25 introduces Alpha support for a new kubelet-managed pod condition
in the status field of a pod: `PodHasNetwork`. The kubelet, for a worker node,
will use the `PodHasNetwork` condition to accurately surface the initialization
state of a pod from the perspective of pod sandbox creation and network
configuration by a container runtime (typically in coordination with CNI
plugins). The kubelet starts to pull container images and start individual
containers (including init containers) after the status of the `PodHasNetwork`
condition is set to `"True"`. Metrics collection services that report latency of
pod initialization from a cluster infrastructural perspective (i.e. agnostic of
per container characteristics like image size or payload) can utilize the
`PodHasNetwork` condition to accurately generate Service Level Indicators
(SLIs). Certain operators or controllers that manage underlying pods may utilize
the `PodHasNetwork` condition to optimize the set of actions performed when pods
repeatedly fail to come up.
-->
&lt;p>Kubernetes 1.25 引入了对 kubelet 所管理的新的 Pod 状况 &lt;code>PodHasNetwork&lt;/code> 的 Alpha 支持，
该状况位于 Pod 的 status 字段中 。对于工作节点，kubelet 将使用 &lt;code>PodHasNetwork&lt;/code> 状况从容器运行时
（通常与 CNI 插件协作）创建 Pod 沙箱和网络配置的角度准确地了解 Pod 的初始化状态。
在 &lt;code>PodHasNetwork&lt;/code> 状况的 status 设置为 &lt;code>&amp;quot;True&amp;quot;&lt;/code> 后，kubelet 开始拉取容器镜像并启动独立的容器
（包括 Init 容器）。从集群基础设施的角度报告 Pod 初始化延迟的指标采集服务
（无需知道每个容器的镜像大小或有效负载等特征）就可以利用 &lt;code>PodHasNetwork&lt;/code>
状况来准确生成服务水平指标（Service Level Indicator，SLI）。
某些管理底层 Pod 的 Operator 或控制器可以利用 &lt;code>PodHasNetwork&lt;/code> 状况来优化 Pod 反复出现失败时要执行的操作。&lt;/p>
&lt;!--
### How is this different from the existing Initialized condition reported for pods?
The kubelet sets the status of the existing `Initialized` condition reported in
the status field of a pod depending on the presence of init containers in a pod.
-->
&lt;h3 id="这与现在为-pod-所报告的-intialized-状况有何不同">这与现在为 Pod 所报告的 Intialized 状况有何不同？&lt;/h3>
&lt;p>根据 Pod 中是否存在 Init 容器，kubelet 会设置在 Pod 的 status 字段中报告的 &lt;code>Initialized&lt;/code> 状况的状态。&lt;/p>
&lt;!--
If a pod specifies init containers, the status of the `Initialized` condition in
the pod status will not be set to `"True"` until all init containers for the pod
have succeeded. However, init containers, configured by users, may have errors
(payload crashing, invalid image, etc) and the number of init containers
configured in a pod may vary across different workloads. Therefore,
cluster-wide, infrastructural SLIs around pod initialization cannot depend on
the `Initialized` condition of pods.
-->
&lt;p>如果 Pod 指定了 Init 容器，则 Pod 状态中的 &lt;code>Initialized&lt;/code> 状况的 status 将不会设置为 &lt;code>&amp;quot;True&amp;quot;&lt;/code>，
直到该 Pod 的所有 Init 容器都成功为止。但是，用户配置的 Init 容器可能会出现错误（有效负载崩溃、无效镜像等），
并且 Pod 中配置的 Init 容器数量可能因工作负载不同而异。
因此，关于 Pod 初始化的集群范围基础设施 SLI 不能依赖于 Pod 的 &lt;code>Initialized&lt;/code> 状况。&lt;/p>
&lt;!--
If a pod does not specify init containers, the status of the `Initialized`
condition in the pod status is set to `"True"` very early in the lifecycle of
the pod. This occurs before the kubelet initiates any pod runtime sandbox
creation and network configuration steps. As a result, a pod without init
containers will report the status of the `Initialized` condition as `"True"`
even if the container runtime is not able to successfully initialize the pod
sandbox environment.
-->
&lt;p>如果 Pod 未指定 Init 容器，则在 Pod 生命周期的早期，
Pod 状态中的 &lt;code>Initialized&lt;/code> 状况的 status 会被设置为 &lt;code>&amp;quot;True&amp;quot;&lt;/code>。
这一设置发生在 kubelet 开始创建 Pod 运行时沙箱及配置网络之前。
因此，即使容器运行时未能成功初始化 Pod 沙箱环境，没有 Init 容器的
Pod 也会将 &lt;code>Initialized&lt;/code> 状况的 status 报告为 &lt;code>&amp;quot;True&amp;quot;&lt;/code>。&lt;/p>
&lt;!--
Relative to either situation above, the `PodHasNetwork` condition surfaces more
accurate data around when the pod runtime sandbox was initialized with
networking configured so that the kubelet can proceed to launch user-configured
containers (including init containers) in the pod.
-->
&lt;p>相对于上述任何一种情况，&lt;code>PodHasNetwork&lt;/code> 状况会在 Pod 运行时沙箱被初始化并配置了网络时能够提供更准确的数据，
这样 kubelet 可以继续在 Pod 中启动用户配置的容器（包括 Init 容器）。&lt;/p>
&lt;!--
### Special Cases
If a pod specifies `hostNetwork` as `"True"`, the `PodHasNetwork` condition is
set to `"True"` based on successful creation of the pod sandbox while the
network configuration state of the pod sandbox is ignored. This is because the
CRI implementation typically skips any pod sandbox network configuration when
`hostNetwork` is set to `"True"` for a pod.
-->
&lt;h3 id="特殊场景">特殊场景&lt;/h3>
&lt;p>如果一个 Pod 指定 &lt;code>hostNetwork&lt;/code> 为 &lt;code>&amp;quot;True&amp;quot;&lt;/code>，
系统会根据 Pod 沙箱创建操作是否成功来决定要不要将 &lt;code>PodHasNetwork&lt;/code> 状况设置为 &lt;code>&amp;quot;True&amp;quot;&lt;/code>，
设置此状况时会忽略 Pod 沙箱的网络配置状态。这是因为 Pod 的 &lt;code>hostNetwork&lt;/code> 被设置为
&lt;code>&amp;quot;True&amp;quot;&lt;/code> 时 CRI 实现通常会跳过所有 Pod 沙箱网络配置。&lt;/p>
&lt;!--
A node agent may dynamically re-configure network interface(s) for a pod by
watching changes in pod annotations that specify additional networking
configuration (e.g. `k8s.v1.cni.cncf.io/networks`). Dynamic updates of pod
networking configuration after the pod sandbox is initialized by Kubelet (in
coordination with a container runtime) are not reflected by the `PodHasNetwork`
condition.
-->
&lt;p>节点代理可以通过监视指定附加网络配置（例如 &lt;code>k8s.v1.cni.cncf.io/networks&lt;/code>）的 Pod 注解变化，
来动态地为 Pod 重新配置网络接口。Pod 沙箱被 Kubelet 初始化（结合容器运行时）之后
Pod 网络配置的动态更新不反映在 &lt;code>PodHasNetwork&lt;/code> 状况中。&lt;/p>
&lt;!--
### Try out the PodHasNetwork condition for pods
In order to have the kubelet report the `PodHasNetwork` condition in the status
field of a pod, please enable the `PodHasNetworkCondition` feature gate on the
kubelet.
For a pod whose runtime sandbox has been successfully created and has networking
configured, the kubelet will report the `PodHasNetwork` condition with status set to `"True"`:
-->
&lt;h3 id="试用-pod-的-podhasnetwork-状况">试用 Pod 的 &lt;code>PodHasNetwork&lt;/code> 状况&lt;/h3>
&lt;p>为了让 kubelet 在 Pod 的 status 字段中报告 &lt;code>PodHasNetwork&lt;/code> 状况，需在 kubelet 上启用
&lt;code>PodHasNetworkCondition&lt;/code> 特性门控。&lt;/p>
&lt;p>对于已成功创建运行时沙箱并已配置网络的 Pod，在 status 设置为 &lt;code>&amp;quot;True&amp;quot;&lt;/code> 后，
kubelet 将报告 &lt;code>PodHasNetwork&lt;/code> 状况：&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl describe pod nginx1
Name: nginx1
Namespace: default
...
Conditions:
Type Status
PodHasNetwork True
Initialized True
Ready True
ContainersReady True
PodScheduled True
&lt;/code>&lt;/pre>&lt;!--
For a pod whose runtime sandbox has not been created yet (and networking not
configured either), the kubelet will report the `PodHasNetwork` condition with
status set to `"False"`:
-->
&lt;p>对于尚未创建运行时沙箱（也未配置网络）的 Pod，在 status 设置为 &lt;code>&amp;quot;False&amp;quot;&lt;/code> 后，
kubelet 将报告 &lt;code>PodHasNetwork&lt;/code> 状况：&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl describe pod nginx2
Name: nginx2
Namespace: default
...
Conditions:
Type Status
PodHasNetwork False
Initialized True
Ready False
ContainersReady False
PodScheduled True
&lt;/code>&lt;/pre>&lt;!--
### What’s next?
Depending on feedback and adoption, the Kubernetes team plans to push the
reporting of the `PodHasNetwork` condition to Beta in 1.26 or 1.27.
-->
&lt;h3 id="下一步是什么">下一步是什么？&lt;/h3>
&lt;p>Kubernetes 团队根据反馈和采用情况，计划在 1.26 或 1.27 中将 &lt;code>PodHasNetwork&lt;/code> 状况的报告提升到 Beta 阶段。&lt;/p>
&lt;!--
### How can I learn more?
Please check out the
[documentation](/docs/concepts/workloads/pods/pod-lifecycle/) for the
`PodHasNetwork` condition to learn more about it and how it fits in relation to
other pod conditions.
-->
&lt;h3 id="我如何了解更多信息">我如何了解更多信息？&lt;/h3>
&lt;p>请查阅 &lt;code>PodHasNetwork&lt;/code> 状况有关的&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/">文档&lt;/a>，
以了解有关该状况的更多信息以及它与其他 Pod 状况的关系。&lt;/p>
&lt;!--
### How to get involved?
This feature is driven by the SIG Node community. Please join us to connect with
the community and share your ideas and feedback around the above feature and
beyond. We look forward to hearing from you!
-->
&lt;h3 id="如何参与">如何参与？&lt;/h3>
&lt;p>此特性由 SIG Node 社区驱动。请加入我们与社区建立联系，并就上述特性及其他问题分享你的想法和反馈。
我们期待你的回音！&lt;/p>
&lt;!--
### Acknowledgements
We want to thank the following people for their insightful and helpful reviews
of the KEP and PRs around this feature: Derek Carr (@derekwaynecarr), Mrunal
Patel (@mrunalp), Dawn Chen (@dchen1107), Qiutong Song (@qiutongs), Ruiwen Zhao
(@ruiwen-zhao), Tim Bannister (@sftim), Danielle Lancashire (@endocrimes) and
Agam Dua (@agamdua).
-->
&lt;h3 id="致谢">致谢&lt;/h3>
&lt;p>我们要感谢以下人员围绕此特性对 KEP 和 PR 进行了极具洞察力和相当有助益的评审工作：
Derek Carr (@derekwaynecarr)、Mrunal Patel (@mrunalp)、Dawn Chen (@dchen1107)、
Qiutong Song (@qiutongs)、Ruiwen Zhao (@ruiwen-zhao)、Tim Bannister (@sftim)、
Danielle Lancashire (@endocrimes) 和 Agam Dua (@agamdua)。&lt;/p></description></item><item><title>Blog: 宣布自动刷新官方 Kubernetes CVE 订阅源</title><link>https://kubernetes.io/zh-cn/blog/2022/09/12/k8s-cve-feed-alpha/</link><pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/09/12/k8s-cve-feed-alpha/</guid><description>
&lt;!--
layout: blog
title: Announcing the Auto-refreshing Official Kubernetes CVE Feed
date: 2022-09-12
slug: k8s-cve-feed-alpha
-->
&lt;!--
**Author**: Pushkar Joglekar (VMware)
A long-standing request from the Kubernetes community has been to have a
programmatic way for end users to keep track of Kubernetes security issues
(also called "CVEs", after the database that tracks public security issues across
different products and vendors). Accompanying the release of Kubernetes v1.25,
we are excited to announce availability of such
a [feed](/docs/reference/issues-security/official-cve-feed/) as an `alpha`
feature. This blog will cover the background and scope of this new service.
-->
&lt;p>&lt;strong>作者&lt;/strong>：Pushkar Joglekar (VMware)&lt;/p>
&lt;p>Kubernetes 社区有一个长时间未解决的需求，即为最终用户提供一种编程方式来跟踪
Kubernetes 安全问题（也称为 “CVE”，这来自于跟踪不同产品和供应商的公共安全问题的数据库）。
随着 Kubernetes v1.25 的发布，我们很高兴地宣布以 &lt;code>alpha&lt;/code>
特性的形式推出这样的&lt;a href="https://kubernetes.io/zh-cn/docs/reference/issues-security/official-cve-feed/">订阅源&lt;/a>。
在这篇博客中将介绍这项新服务的背景和范围。&lt;/p>
&lt;!--
## Motivation
With the growing number of eyes on Kubernetes, the number of CVEs related to
Kubernetes have increased. Although most CVEs that directly, indirectly, or
transitively impact Kubernetes are regularly fixed, there is no single place for
the end users of Kubernetes to programmatically subscribe or pull the data of
fixed CVEs. Current options are either broken or incomplete.
-->
&lt;h2 id="动机">动机&lt;/h2>
&lt;p>随着关注 Kubernetes 的人越来越多，与 Kubernetes 相关的 CVE 数量也在增加。
尽管大多数直接地、间接地或传递性地影响 Kubernetes 的 CVE 都被定期修复，
但 Kubernetes 的最终用户没有一个地方能够以编程方式来订阅或拉取固定的 CVE 数据。
目前的一些数据源要么已损坏，要么不完整。&lt;/p>
&lt;!--
## Scope
### What This Does
Create a periodically auto-refreshing, human and machine-readable list of
official Kubernetes CVEs
-->
&lt;h2 id="范围">范围&lt;/h2>
&lt;h3 id="能做什么">能做什么&lt;/h3>
&lt;p>创建一个定期自动刷新的、人和机器可读的官方 Kubernetes CVE 列表。&lt;/p>
&lt;!--
### What This Doesn't Do
* Triage and vulnerability disclosure will continue to be done by SRC (Security
Response Committee).
* Listing CVEs that are identified in build time dependencies and container
images are out of scope.
* Only official CVEs announced by the Kubernetes SRC will be published in the
feed.
-->
&lt;h3 id="不能做什么">不能做什么&lt;/h3>
&lt;ul>
&lt;li>漏洞的分类和披露将继续由 SRC（Security Response Committee，安全响应委员会）完成。&lt;/li>
&lt;li>不会列出在构建时依赖项和容器镜像中发现的 CVE。&lt;/li>
&lt;li>只有 Kubernetes SRC 公布的官方 CVE 才会在订阅源中发布。&lt;/li>
&lt;/ul>
&lt;!--
### Who It's For
* **End Users**: Persons or teams who _use_ Kubernetes to deploy applications
they own
* **Platform Providers**: Persons or teams who _manage_ Kubernetes clusters
* **Maintainers**: Persons or teams who _create_ and _support_ Kubernetes
releases through their work in Kubernetes Community - via various Special
Interest Groups and Committees.
-->
&lt;h3 id="针对的受众">针对的受众&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>最终用户&lt;/strong>：&lt;strong>使用&lt;/strong> Kubernetes 部署他们的应用程序的个人或团队。&lt;/li>
&lt;li>&lt;strong>平台提供商&lt;/strong>：&lt;strong>管理&lt;/strong> Kubernetes 集群的个人或团队。&lt;/li>
&lt;li>&lt;strong>维护人员&lt;/strong>：通过各种特别兴趣小组和委员会在 Kubernetes 社区中&lt;strong>创建&lt;/strong>和&lt;strong>支持&lt;/strong> Kubernetes
发布版本的个人或团队。&lt;/li>
&lt;/ul>
&lt;!--
## Implementation Details
A supporting
[contributor blog](https://kubernetes.dev/blog/2022/09/12/k8s-cve-feed-alpha/)
was published that describes in depth on how this CVE feed was implemented to
ensure the feed was reasonably protected against tampering and was automatically
updated after a new CVE was announced.
-->
&lt;h2 id="实现细节">实现细节&lt;/h2>
&lt;p>发布了一个支持性的&lt;a href="https://kubernetes.dev/blog/2022/09/12/k8s-cve-feed-alpha/">贡献者博客&lt;/a>,
深入讲述这个 CVE 订阅源是如何实现的，如何确保该订阅源得到合理的保护以免被篡改，
如何在一个新的 CVE 被公布后自动更新这个订阅源。&lt;/p>
&lt;!--
## What's Next?
In order to graduate this feature, SIG Security
is gathering feedback from end users who are using this alpha feed.
-->
&lt;h2 id="下一步工作">下一步工作&lt;/h2>
&lt;p>为了完善此功能，SIG Security 正在收集使用此 Alpha 订阅源的最终用户的反馈。&lt;/p>
&lt;!--
So in order to improve the feed in future Kubernetes Releases, if you have any
feedback, please let us know by adding a comment to
this [tracking issue](https://github.com/kubernetes/sig-security/issues/1) or
let us know on
[#sig-security-tooling](https://kubernetes.slack.com/archives/C01CUSVMHPY)
Kubernetes Slack channel.
(Join [Kubernetes Slack here](https://slack.k8s.io))
-->
&lt;p>因此，为了在未来的 Kubernetes 版本中改进订阅源，如果你有任何反馈，请通过添加评论至
&lt;a href="https://github.com/kubernetes/sig-security/issues/1">问题追踪&lt;/a>告诉我们，
或者在 &lt;a href="https://kubernetes.slack.com/archives/C01CUSVMHPY">#sig-security-tooling&lt;/a>
Kubernetes Slack 频道上告诉我们（从&lt;a href="https://slack.k8s.io">这里&lt;/a>加入 Kubernetes Slack) 。&lt;/p>
&lt;!--
_A special shout out and massive thanks to Neha Lohia
[(@nehalohia27)](https://github.com/nehalohia27) and Tim
Bannister [(@sftim)](https://github.com/sftim) for their stellar collaboration
for many months from "ideation to implementation" of this feature._
-->
&lt;p>&lt;strong>特别感谢 Neha Lohia
&lt;a href="https://github.com/nehalohia27">(@nehalohia27)&lt;/a>
和 Tim Bannister &lt;a href="https://github.com/sftim">(@sftim)&lt;/a>，
感谢他们几个月来从“构思到实现”此特性的出色合作。&lt;/strong>&lt;/p></description></item><item><title>Blog: COSI 简介：使用 Kubernetes API 管理对象存储</title><link>https://kubernetes.io/zh-cn/blog/2022/09/02/cosi-kubernetes-object-storage-management/</link><pubDate>Fri, 02 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/09/02/cosi-kubernetes-object-storage-management/</guid><description>
&lt;!--
layout: blog
title: "Introducing COSI: Object Storage Management using Kubernetes APIs"
date: 2022-09-02
slug: cosi-kubernetes-object-storage-management
-->
&lt;!--
**Authors:** Sidhartha Mani ([Minio, Inc](https://min.io))
-->
&lt;p>&lt;strong>作者：&lt;/strong> Sidhartha Mani (&lt;a href="https://min.io">Minio, Inc&lt;/a>)&lt;/p>
&lt;!--
This article introduces the Container Object Storage Interface (COSI), a standard for provisioning and consuming object storage in Kubernetes. It is an alpha feature in Kubernetes v1.25.
File and block storage are treated as first class citizens in the Kubernetes ecosystem via [Container Storage Interface](https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/) (CSI). Workloads using CSI volumes enjoy the benefits of portability across vendors and across Kubernetes clusters without the need to change application manifests. An equivalent standard does not exist for Object storage.
Object storage has been rising in popularity in recent years as an alternative form of storage to filesystems and block devices. Object storage paradigm promotes disaggregation of compute and storage. This is done by making data available over the network, rather than locally. Disaggregated architectures allow compute workloads to be stateless, which consequently makes them easier to manage, scale and automate.
-->
&lt;p>本文介绍了容器对象存储接口 (COSI)，它是在 Kubernetes 中制备和使用对象存储的一个标准。
它是 Kubernetes v1.25 中的一个 Alpha 功能。&lt;/p>
&lt;p>文件和块存储通过 &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface&lt;/a> (CSI)
被视为 Kubernetes 生态系统中的一等公民。
使用 CSI 卷的工作负载可以享受跨供应商和跨 Kubernetes 集群的可移植性优势，
而无需更改应用程序清单。对象存储不存在等效标准。&lt;/p>
&lt;p>近年来，对象存储作为文件系统和块设备的替代存储形式越来越受欢迎。
对象存储范式促进了计算和存储的分解，这是通过网络而不是本地提供数据来完成的。
分解的架构允许计算工作负载是无状态的，从而使它们更易于管理、扩展和自动化。&lt;/p>
&lt;h2 id="cosi">COSI&lt;/h2>
&lt;!--
COSI aims to standardize consumption of object storage to provide the following benefits:
* Kubernetes Native - Use the Kubernetes API to provision, configure and manage buckets
* Self Service - A clear delineation between administration and operations (DevOps) to enable self-service capability for DevOps personnel
* Portability - Vendor neutrality enabled through portability across Kubernetes Clusters and across Object Storage vendors
_Portability across vendors is only possible when both vendors support a common datapath-API. Eg. it is possible to port from AWS S3 to Ceph, or AWS S3 to MinIO and back as they all use S3 API. In contrast, it is not possible to port from AWS S3 and Google Cloud’s GCS or vice versa._
-->
&lt;p>COSI 旨在标准化对象存储的使用，以提供以下好处：&lt;/p>
&lt;ul>
&lt;li>Kubernetes 原生 - 使用 Kubernetes API 来制备、配置和管理 Bucket&lt;/li>
&lt;li>自助服务 - 明确划分管理和运营 (DevOps)，为 DevOps 人员赋予自助服务能力&lt;/li>
&lt;li>可移植性 - 通过跨 Kubernetes 集群和跨对象存储供应商的可移植性实现供应商中立性&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>跨供应商的可移植性只有在两家供应商都支持通用数据路径 API 时才有可能。
例如，可以从 AWS S3 移植到 Ceph，或从 AWS S3 移植到 MinIO 以及反向操作，因为它们都使用 S3 API。
但是无法从 AWS S3 和 Google Cloud 的 GCS 移植，反之亦然。&lt;/strong>&lt;/p>
&lt;!--
## Architecture
-->
&lt;h2 id="架构">架构&lt;/h2>
&lt;!--
COSI is made up of three components:
* COSI Controller Manager
* COSI Sidecar
* COSI Driver
-->
&lt;p>COSI 由三个部分组成：&lt;/p>
&lt;ul>
&lt;li>COSI 控制器管理器&lt;/li>
&lt;li>COSI 边车&lt;/li>
&lt;li>COSI 驱动程序&lt;/li>
&lt;/ul>
&lt;!--
The COSI Controller Manager acts as the main controller that processes changes to COSI API objects. It is responsible for fielding requests for bucket creation, updates, deletion and access management. One instance of the controller manager is required per kubernetes cluster. Only one is needed even if multiple object storage providers are used in the cluster.
The COSI Sidecar acts as a translator between COSI API requests and vendor-specific COSI Drivers. This component uses a standardized gRPC protocol that vendor drivers are expected to satisfy.
The COSI Driver is the vendor specific component that receives requests from the sidecar and calls the appropriate vendor APIs to create buckets, manage their lifecycle and manage access to them.
-->
&lt;p>COSI 控制器管理器充当处理 COSI API 对象更改的主控制器，它负责处理 Bucket 创建、更新、删除和访问管理的请求。
每个 Kubernetes 集群都需要一个控制器管理器实例。即使集群中使用了多个对象存储提供程序，也只需要一个。&lt;/p>
&lt;p>COSI 边车充当 COSI API 请求和供应商特定 COSI 驱动程序之间的转换器。
该组件使用供应商驱动程序应满足的标准化 gRPC 协议。&lt;/p>
&lt;p>COSI 驱动程序是供应商特定组件，它接收来自 sidecar 的请求并调用适当的供应商 API 以创建 Bucket、
管理其生命周期及对它们的访问。&lt;/p>
&lt;!--
## API
The COSI API is centered around buckets, since bucket is the unit abstraction for object storage. COSI defines three Kubernetes APIs aimed at managing them
* Bucket
* BucketClass
* BucketClaim
-->
&lt;h2 id="接口">接口&lt;/h2>
&lt;p>COSI 接口 以 Bucket 为中心，因为 Bucket 是对象存储的抽象单元。COSI 定义了三个旨在管理它们的 Kubernetes API&lt;/p>
&lt;ul>
&lt;li>Bucket&lt;/li>
&lt;li>BucketClass&lt;/li>
&lt;li>BucketClaim&lt;/li>
&lt;/ul>
&lt;!--
In addition, two more APIs for managing access to buckets are also defined:
* BucketAccess
* BucketAccessClass
-->
&lt;p>此外，还定义了另外两个用于管理对 Bucket 的访问的 API：&lt;/p>
&lt;ul>
&lt;li>BucketAccess&lt;/li>
&lt;li>BucketAccessClass&lt;/li>
&lt;/ul>
&lt;!--
In a nutshell, Bucket and BucketClaim can be considered to be similar to PersistentVolume and PersistentVolumeClaim respectively. The BucketClass’ counterpart in the file/block device world is StorageClass.
Since Object Storage is always authenticated, and over the network, access credentials are required to access buckets. The two APIs, namely, BucketAccess and BucketAccessClass are used to denote access credentials and policies for authentication. More info about these APIs can be found in the official COSI proposal - https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support
-->
&lt;p>简而言之，Bucket 和 BucketClaim 可以认为分别类似于 PersistentVolume 和 PersistentVolumeClaim。
BucketClass 在文件/块设备世界中对应的是 StorageClass。&lt;/p>
&lt;p>由于对象存储始终通过网络进行身份验证，因此需要访问凭证才能访问 Bucket。
BucketAccess 和 BucketAccessClass 这两个 API 用于表示访问凭证和身份验证策略。
有关这些 API 的更多信息可以在官方 COSI 提案中找到 - &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support">https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support&lt;/a>&lt;/p>
&lt;!--
## Self-Service
Other than providing kubernetes-API driven bucket management, COSI also aims to empower DevOps personnel to provision and manage buckets on their own, without admin intervention. This, further enabling dev teams to realize faster turn-around times and faster time-to-market.
COSI achieves this by dividing bucket provisioning steps among two different stakeholders, namely the administrator (admin), and the cluster operator. The administrator will be responsible for setting broad policies and limits on how buckets are provisioned, and how access is obtained for them. The cluster operator will be free to create and utilize buckets within the limits set by the admin.
For example, a cluster operator could use an admin policy could be used to restrict maximum provisioned capacity to 100GB, and developers would be allowed to create buckets and store data upto that limit. Similarly for access credentials, admins would be able to restrict who can access which buckets, and developers would be able to access all the buckets available to them.
-->
&lt;h2 id="自助服务">自助服务&lt;/h2>
&lt;p>除了提供 kubernetes-API 驱动的 Bucket 管理之外，COSI 还旨在使 DevOps 人员能够自行配置和管理 Bucket，
而无需管理员干预。这进一步使开发团队能够实现更快的周转时间和更快的上市时间。&lt;/p>
&lt;p>COSI 通过在两个不同的利益相关者（即管理员（admin）和集群操作员）之间划分 Bucket 配置步骤来实现这一点。
管理员将负责就如何配置 Bucket 以及如何获取 Bucket 的访问权限设置广泛的策略和限制。
集群操作员可以在管理员设置的限制内自由创建和使用 Bucket。&lt;/p>
&lt;p>例如，集群操作员可以使用管理策略将最大预置容量限制为 100GB，并且允许开发人员创建 Bucket 并将数据存储到该限制。
同样对于访问凭证，管理员将能够限制谁可以访问哪些 Bucket，并且开发人员将能够访问他们可用的所有 Bucket。&lt;/p>
&lt;!--
## Portability
The third goal of COSI is to achieve vendor neutrality for bucket management. COSI enables two kinds of portability:
* Cross Cluster
* Cross Provider
Cross Cluster portability is allowing buckets provisioned in one cluster to be available in another cluster. This is only valid when the object storage backend itself is accessible from both clusters.
Cross-provider portability is about allowing organizations or teams to move from one object storage provider to another seamlessly, and without requiring changes to application definitions (PodTemplates, StatefulSets, Deployment and so on). This is only possible if the source and destination providers use the same data.
_COSI does not handle data migration as it is outside of its scope. In case porting between providers requires data to be migrated as well, then other measures need to be taken to ensure data availability._
-->
&lt;h2 id="可移植性">可移植性&lt;/h2>
&lt;p>COSI 的第三个目标是实现 Bucket 管理的供应商中立性。COSI 支持两种可移植性：&lt;/p>
&lt;ul>
&lt;li>跨集群&lt;/li>
&lt;li>跨提供商&lt;/li>
&lt;/ul>
&lt;p>跨集群可移植性允许在一个集群中配置的 Bucket 在另一个集群中可用。这仅在对象存储后端本身可以从两个集群访问时才有效。&lt;/p>
&lt;p>跨提供商可移植性是指允许组织或团队无缝地从一个对象存储提供商迁移到另一个对象存储提供商，
而无需更改应用程序定义（PodTemplates、StatefulSets、Deployment 等）。这只有在源和目标提供者使用相同的数据时才有可能。&lt;/p>
&lt;p>&lt;strong>COSI 不处理数据迁移，因为它超出了其范围。如果提供者之间的移植也需要迁移数据，则需要采取其他措施来确保数据可用性。&lt;/strong>&lt;/p>
&lt;!--
## What’s next
The amazing sig-storage-cosi community has worked hard to bring the COSI standard to alpha status. We are looking forward to onboarding a lot of vendors to write COSI drivers and become COSI compatible!
We want to add more authentication mechanisms for COSI buckets, we are designing advanced bucket sharing primitives, multi-cluster bucket management and much more. Lots of great ideas and opportunities ahead!
Stay tuned for what comes next, and if you have any questions, comments or suggestions
* Chat with us on the Kubernetes [Slack:#sig-storage-cosi](https://kubernetes.slack.com/archives/C017EGC1C6N)
* Join our [Zoom meeting](https://zoom.us/j/614261834?pwd=Sk1USmtjR2t0MUdjTGVZeVVEV1BPQT09), every Thursday at 10:00 Pacific Time
* Participate in the [bucket API proposal PR](https://github.com/kubernetes/enhancements/pull/2813) to add your ideas, suggestions and more.
-->
&lt;h2 id="接下来">接下来&lt;/h2>
&lt;p>令人惊叹的 sig-storage-cosi 社区一直在努力将 COSI 标准带入 Alpha 状态。
我们期待很多供应商加入编写 COSI 驱动程序并与 COSI 兼容！&lt;/p>
&lt;p>我们希望为 COSI Bucket 添加更多身份验证机制，我们正在设计高级存储桶共享原语、多集群存储桶管理等等。
未来有很多伟大的想法和机会！&lt;/p>
&lt;p>请继续关注接下来的内容，如果你有任何问题、意见或建议分解的架构允许计算工作负载是无状态&lt;/p>
&lt;ul>
&lt;li>在 Kubernetes 上与我们讨论 &lt;a href="https://kubernetes.slack.com/archives/C017EGC1C6N">Slack:#sig-storage-cosi&lt;/a>&lt;/li>
&lt;li>参加我们的 &lt;a href="https://zoom.us/j/614261834?pwd=Sk1USmtjR2t0MUdjTGVZeVVEV1BPQT09">Zoom 会议&lt;/a>，每周四太平洋时间 10:00&lt;/li>
&lt;li>参与 &lt;a href="https://github.com/kubernetes/enhancements/pull/2813">bucket API 提案 PR&lt;/a> 提出你的想法、建议等。&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.25: cgroup v2 升级到 GA</title><link>https://kubernetes.io/zh-cn/blog/2022/08/31/cgroupv2-ga-1-25/</link><pubDate>Wed, 31 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/08/31/cgroupv2-ga-1-25/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes 1.25: cgroup v2 graduates to GA"
date: 2022-08-31
slug: cgroupv2-ga-1-25
-->
&lt;!--
**Authors:**: David Porter (Google), Mrunal Patel (Red Hat)
-->
&lt;p>&lt;strong>作者&lt;/strong>: David Porter (Google), Mrunal Patel (Red Hat)&lt;/p>
&lt;!--
Kubernetes 1.25 brings cgroup v2 to GA (general availability), letting the
[kubelet](/docs/concepts/overview/components/#kubelet) use the latest container resource
management capabilities.
-->
&lt;p>Kubernetes 1.25 将 cgroup v2 正式发布（GA），
让 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/overview/components/#kubelet">kubelet&lt;/a> 使用最新的容器资源管理能力。&lt;/p>
&lt;!--
## What are cgroups?
-->
&lt;h2 id="什么是-cgroup">什么是 cgroup？&lt;/h2>
&lt;!--
Effective [resource management](/docs/concepts/configuration/manage-resources-containers/) is a
critical aspect of Kubernetes. This involves managing the finite resources in
your nodes, such as CPU, memory, and storage.
*cgroups* are a Linux kernel capability that establish resource management
functionality like limiting CPU usage or setting memory limits for running
processes.
-->
&lt;p>有效的&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/">资源管理&lt;/a>是 Kubernetes 的一个关键方面。
这涉及管理节点中的有限资源，例如 CPU、内存和存储。&lt;/p>
&lt;p>&lt;strong>cgroups&lt;/strong> 是一种可建立资源管理功能的 Linux 内核能力，
例如为正在运行的进程限制 CPU 使用率或设置内存限制。&lt;/p>
&lt;!--
When you use the resource management capabilities in Kubernetes, such as configuring
[requests and limits for Pods and containers](/docs/concepts/configuration/manage-resources-containers/#requests-and-limits),
Kubernetes uses cgroups to enforce your resource requests and limits.
The Linux kernel offers two versions of cgroups: cgroup v1 and cgroup v2.
-->
&lt;p>当你使用 Kubernetes 中的资源管理能力时，例如配置
&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">Pod 和容器的请求和限制&lt;/a>，
Kubernetes 会使用 cgroups 来强制执行你的资源请求和限制。&lt;/p>
&lt;p>Linux 内核提供了两个版本的 cgroup：cgroup v1 和 cgroup v2。&lt;/p>
&lt;!--
## What is cgroup v2?
-->
&lt;h2 id="什么是-cgroup-v2">什么是 cgroup v2？&lt;/h2>
&lt;!--
cgroup v2 is the latest version of the Linux cgroup API. cgroup v2 provides a
unified control system with enhanced resource management capabilities.
cgroup v2 has been in development in the Linux Kernel since 2016 and in recent
years has matured across the container ecosystem. With Kubernetes 1.25, cgroup
v2 support has graduated to general availability.
-->
&lt;p>cgroup v2 是 Linux cgroup API 的最新版本,
提供了一个具有增强的资源管理能力的统一控制系统。&lt;/p>
&lt;p>自 2016 年以来，cgroup v2 一直在 Linux 内核中进行开发，
近年来在整个容器生态系统中已经成熟。在 Kubernetes 1.25 中，
对 cgroup v2 的支持已升级为正式发布。&lt;/p>
&lt;!--
Many recent releases of Linux distributions have switched over to cgroup v2 by
default so it's important that Kubernetes continues to work well on these new
updated distros.
cgroup v2 offers several improvements over cgroup v1, such as the following:
-->
&lt;p>默认情况下，许多最新版本的 Linux 发行版已切换到 cgroup v2，
因此 Kubernetes 继续在这些新更新的发行版上正常运行非常重要。&lt;/p>
&lt;p>cgroup v2 对 cgroup v1 进行了多项改进，例如：&lt;/p>
&lt;!--
* Single unified hierarchy design in API
* Safer sub-tree delegation to containers
* Newer features like [Pressure Stall Information](https://www.kernel.org/doc/html/latest/accounting/psi.html)
* Enhanced resource allocation management and isolation across multiple resources
* Unified accounting for different types of memory allocations (network and kernel memory, etc)
* Accounting for non-immediate resource changes such as page cache write backs
-->
&lt;ul>
&lt;li>API 中单个统一的层次结构设计&lt;/li>
&lt;li>为容器提供更安全的子树委派能力&lt;/li>
&lt;li>&lt;a href="https://www.kernel.org/doc/html/latest/accounting/psi.html">压力阻塞信息&lt;/a>等新功能&lt;/li>
&lt;li>增强的资源分配管理和跨多个资源的隔离
&lt;ul>
&lt;li>统一核算不同类型的内存分配（网络和内核内存等）&lt;/li>
&lt;li>考虑非即时资源更改，例如页面缓存回写&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
Some Kubernetes features exclusively use cgroup v2 for enhanced resource
management and isolation. For example,
the [MemoryQoS feature](/blog/2021/11/26/qos-memory-resources/) improves
memory utilization and relies on cgroup v2 functionality to enable it. New
resource management features in the kubelet will also take advantage of the new
cgroup v2 features moving forward.
-->
&lt;p>一些 Kubernetes 特性专门使用 cgroup v2 来增强资源管理和隔离。
例如，&lt;a href="https://kubernetes.io/blog/2021/11/26/qos-memory-resources/">MemoryQoS 特性&lt;/a>提高了内存利用率并依赖
cgroup v2 功能来启用它。kubelet 中的新资源管理特性也将利用新的 cgroup v2 特性向前发展。&lt;/p>
&lt;!--
## How do you use cgroup v2?
Many Linux distributions are switching to cgroup v2 by default; you might start
using it the next time you update the Linux version of your control plane and
nodes!
Using a Linux distribution that uses cgroup v2 by default is the recommended
method. Some of the popular Linux distributions that use cgroup v2 include the
following:
* Container Optimized OS (since M97)
* Ubuntu (since 21.10)
* Debian GNU/Linux (since Debian 11 Bullseye)
* Fedora (since 31)
* Arch Linux (since April 2021)
* RHEL and RHEL-like distributions (since 9)
-->
&lt;h2 id="如何使用-cgroup-v2">如何使用 cgroup v2?&lt;/h2>
&lt;p>许多 Linux 发行版默认切换到 cgroup v2；
你可能会在下次更新控制平面和节点的 Linux 版本时开始使用它！&lt;/p>
&lt;p>推荐使用默认使用 cgroup v2 的 Linux 发行版。
一些使用 cgroup v2 的流行 Linux 发行版包括：&lt;/p>
&lt;ul>
&lt;li>Container-Optimized OS（从 M97 开始）&lt;/li>
&lt;li>Ubuntu（从 21.10 开始，推荐 22.04+）&lt;/li>
&lt;li>Debian GNU/Linux（从 Debian 11 Bullseye 开始）&lt;/li>
&lt;li>Fedora（从 31 开始）&lt;/li>
&lt;li>Arch Linux（从 2021 年 4 月开始）&lt;/li>
&lt;li>RHEL 和类似 RHEL 的发行版（从 9 开始）&lt;/li>
&lt;/ul>
&lt;!--
To check if your distribution uses cgroup v2 by default,
refer to [Check your cgroup version](/docs/concepts/architecture/cgroups/#check-cgroup-version) or
consult your distribution's documentation.
If you're using a managed Kubernetes offering, consult your provider to
determine how they're adopting cgroup v2, and whether you need to take action.
To use cgroup v2 with Kubernetes, you must meet the following requirements:
-->
&lt;p>要检查你的发行版是否默认使用 cgroup v2，
请参阅你的发行版文档或遵循&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/cgroups/#check-cgroup-version">识别 Linux 节点上的 cgroup 版本&lt;/a>。&lt;/p>
&lt;p>如果你使用的是托管 Kubernetes 产品，请咨询你的提供商以确定他们如何采用 cgroup v2，
以及你是否需要采取行动。&lt;/p>
&lt;p>要将 cgroup v2 与 Kubernetes 一起使用，必须满足以下要求：&lt;/p>
&lt;!--
* Your Linux distribution enables cgroup v2 on kernel version 5.8 or later
* Your container runtime supports cgroup v2. For example:
* [containerd](https://containerd.io/) v1.4 or later
* [cri-o](https://cri-o.io/) v1.20 or later
* The kubelet and the container runtime are configured to use the [systemd cgroup driver](/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver)
-->
&lt;ul>
&lt;li>你的 Linux 发行版在内核版本 5.8 或更高版本上启用 cgroup v2&lt;/li>
&lt;li>你的容器运行时支持 cgroup v2。例如：
&lt;ul>
&lt;li>&lt;a href="https://containerd.io/">containerd&lt;/a> v1.4 或更高版本&lt;/li>
&lt;li>&lt;a href="https://cri-o.io/">cri-o&lt;/a> v1.20 或更高版本&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>kubelet 和容器运行时配置为使用 &lt;a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup 驱动程序&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
The kubelet and container runtime use a [cgroup driver](/docs/setup/production-environment/container-runtimes#cgroup-drivers)
to set cgroup paramaters. When using cgroup v2, it's strongly recommended that both
the kubelet and your container runtime use the
[systemd cgroup driver](/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver),
so that there's a single cgroup manager on the system. To configure the kubelet
and the container runtime to use the driver, refer to the
[systemd cgroup driver documentation](/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver).
-->
&lt;p>kubelet 和容器运行时使用 &lt;a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes#cgroup-drivers">cgroup 驱动&lt;/a>
来设置 cgroup 参数。使用 cgroup v2 时，强烈建议 kubelet 和你的容器运行时都使用
&lt;a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup 驱动程序&lt;/a>，
以便系统上只有一个 cgroup 管理员。要配置 kubelet 和容器运行时以使用该驱动程序，
请参阅 &lt;a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup 驱动程序文档&lt;/a>。&lt;/p>
&lt;!--
## Migrate to cgroup v2
When you run Kubernetes with a Linux distribution that enables cgroup v2, the
kubelet should automatically adapt without any additional configuration
required, as long as you meet the requirements.
In most cases, you won't see a difference in the user experience when you
switch to using cgroup v2 unless your users access the cgroup file system
directly.
-->
&lt;h2 id="迁移到-cgroup-v2">迁移到 cgroup v2&lt;/h2>
&lt;p>当你使用启用 cgroup v2 的 Linux 发行版运行 Kubernetes 时，只要你满足要求，
kubelet 应该会自动适应而无需任何额外的配置。&lt;/p>
&lt;p>在大多数情况下，除非你的用户直接访问 cgroup 文件系统，
否则当你切换到使用 cgroup v2 时，不会感知到用户体验有什么不同。&lt;/p>
&lt;!--
If you have applications that access the cgroup file system directly, either on
the node or from inside a container, you must update the applications to use
the cgroup v2 API instead of the cgroup v1 API.
Scenarios in which you might need to update to cgroup v2 include the following:
* If you run third-party monitoring and security agents that depend on the cgroup file system, update the
agents to versions that support cgroup v2.
* If you run [cAdvisor](https://github.com/google/cadvisor) as a stand-alone
DaemonSet for monitoring pods and containers, update it to v0.43.0 or later.
* If you deploy Java applications with the JDK, prefer to use JDK 11.0.16 and
later or JDK 15 and later, which [fully support cgroup v2](https://bugs.openjdk.org/browse/JDK-8230305).
-->
&lt;p>如果你在节点上或从容器内直接访问 cgroup 文件系统的应用程序，
你必须更新应用程序以使用 cgroup v2 API 而不是 cgroup v1 API。&lt;/p>
&lt;p>你可能需要更新到 cgroup v2 的场景包括：&lt;/p>
&lt;ul>
&lt;li>如果你运行依赖于 cgroup 文件系统的第三方监控和安全代理，请将代理更新到支持 cgroup v2 的版本。&lt;/li>
&lt;li>如果你将 &lt;a href="https://github.com/google/cadvisor">cAdvisor&lt;/a> 作为独立的 DaemonSet 运行以监控 Pod 和容器，
请将其更新到 v0.43.0 或更高版本。&lt;/li>
&lt;li>如果你使用 JDK 部署 Java 应用程序，首选使用&lt;a href="https://bugs.openjdk.org/browse/JDK-8230305">完全支持 cgroup v2&lt;/a>
的 JDK 11.0.16 及更高版本或 JDK 15 及更高版本。&lt;/li>
&lt;/ul>
&lt;!--
## Learn more
* Read the [Kubernetes cgroup v2 documentation](/docs/concepts/architecture/cgroups/)
* Read the enhancement proposal, [KEP 2254](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2254-cgroup-v2/README.md)
* Learn more about
[cgroups](https://man7.org/linux/man-pages/man7/cgroups.7.html) on Linux Manual Pages
and [cgroup v2](https://docs.kernel.org/admin-guide/cgroup-v2.html) on the Linux Kernel documentation
-->
&lt;h2 id="进一步了解">进一步了解&lt;/h2>
&lt;ul>
&lt;li>阅读 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/cgroups/">Kubernetes cgroup v2 文档&lt;/a>&lt;/li>
&lt;li>阅读增强提案 &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2254-cgroup-v2/README.md">KEP 2254&lt;/a>&lt;/li>
&lt;li>学习更多关于 Linux 手册页上的 &lt;a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups&lt;/a> 和 Linux 内核文档上的
&lt;a href="https://docs.kernel.org/admin-guide/cgroup-v2.html">cgroup v2&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Get involved
Your feedback is always welcome! SIG Node meets regularly and are available in
the `#sig-node` channel in the Kubernetes [Slack](https://slack.k8s.io/), or
using the SIG [mailing list](https://github.com/kubernetes/community/tree/master/sig-node#contact).
cgroup v2 has had a long journey and is a great example of open source
community collaboration across the industry because it required work across the
stack, from the Linux Kernel to systemd to various container runtimes, and (of
course) Kubernetes.
-->
&lt;h2 id="参与其中">参与其中&lt;/h2>
&lt;p>随时欢迎你的反馈！SIG Node 定期开会，可在 Kubernetes &lt;a href="https://slack.k8s.io/">Slack&lt;/a>的
&lt;code>#sig-node&lt;/code> 频道中获得，或使用 SIG &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">邮件列表&lt;/a>。&lt;/p>
&lt;p>cgroup v2 经历了漫长的旅程，是整个行业开源社区协作的一个很好的例子，
因为它需要跨堆栈的工作，从 Linux 内核到 systemd 到各种容器运行时，当然还有 Kubernetes。&lt;/p>
&lt;!--
## Acknowledgments
We would like to thank [Giuseppe Scrivano](https://github.com/giuseppe) who
initiated cgroup v2 support in Kubernetes, and reviews and leadership from the
SIG Node community including chairs [Dawn Chen](https://github.com/dchen1107)
and [Derek Carr](https://github.com/derekwaynecarr).
We'd also like to thank the maintainers of container runtimes like Docker,
containerd and CRI-O, and the maintainers of components like
[cAdvisor](https://github.com/google/cadvisor)
and [runc, libcontainer](https://github.com/opencontainers/runc),
which underpin many container runtimes. Finally, this wouldn't have been
possible without support from systemd and upstream Linux Kernel maintainers.
It's a team effort!
-->
&lt;h2 id="致谢">致谢&lt;/h2>
&lt;p>我们要感谢 &lt;a href="https://github.com/giuseppe">Giuseppe Scrivano&lt;/a> 在 Kubernetes 中发起对 cgroup v2 的支持，
还要感谢 SIG Node 社区主席 &lt;a href="https://github.com/dchen1107">Dawn Chen&lt;/a> 和
&lt;a href="https://github.com/derekwaynecarr">Derek Carr&lt;/a> 所作的审查和领导工作。&lt;/p>
&lt;p>我们还要感谢 Docker、containerd 和 CRI-O 等容器运行时的维护者，
以及支持多种容器运行时的 &lt;a href="https://github.com/google/cadvisor">cAdvisor&lt;/a> 和
&lt;a href="https://github.com/opencontainers/runc">runc, libcontainer&lt;/a> 等组件的维护者。
最后，如果没有 systemd 和上游 Linux 内核维护者的支持，这将是不可能的。&lt;/p></description></item><item><title>Blog: Kubernetes 1.25：CSI 内联存储卷正式发布</title><link>https://kubernetes.io/zh-cn/blog/2022/08/29/csi-inline-volumes-ga/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/08/29/csi-inline-volumes-ga/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes 1.25: CSI Inline Volumes have graduated to GA"
date: 2022-08-29
slug: csi-inline-volumes-ga
-->
&lt;!--
**Author:** Jonathan Dobson (Red Hat)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Jonathan Dobson (Red Hat)&lt;/p>
&lt;!--
CSI Inline Volumes were introduced as an alpha feature in Kubernetes 1.15 and have been beta since 1.16. We are happy to announce that this feature has graduated to General Availability (GA) status in Kubernetes 1.25.
CSI Inline Volumes are similar to other ephemeral volume types, such as `configMap`, `downwardAPI` and `secret`. The important difference is that the storage is provided by a CSI driver, which allows the use of ephemeral storage provided by third-party vendors. The volume is defined as part of the pod spec and follows the lifecycle of the pod, meaning the volume is created once the pod is scheduled and destroyed when the pod is destroyed.
-->
&lt;p>CSI 内联存储卷是在 Kubernetes 1.15 中作为 Alpha 功能推出的，并从 1.16 开始成为 Beta 版本。
我们很高兴地宣布，这项功能在 Kubernetes 1.25 版本中正式发布（GA）。&lt;/p>
&lt;p>CSI 内联存储卷与其他类型的临时卷相似，如 &lt;code>configMap&lt;/code>、&lt;code>downwardAPI&lt;/code> 和 &lt;code>secret&lt;/code>。
重要的区别是，存储是由 CSI 驱动提供的，它允许使用第三方供应商提供的临时存储。
卷被定义为 Pod 规约的一部分，并遵循 Pod 的生命周期，这意味着卷随着 Pod 的调度而创建，并随着 Pod 的销毁而销毁。&lt;/p>
&lt;!--
## What's new in 1.25?
There are a couple of new bug fixes related to this feature in 1.25, and the [CSIInlineVolume feature gate](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/) has been locked to `True` with the graduation to GA. There are no new API changes, so users of this feature during beta should not notice any significant changes aside from these bug fixes.
- [#89290 - CSI inline volumes should support fsGroup](https://github.com/kubernetes/kubernetes/issues/89290)
- [#79980 - CSI volume reconstruction does not work for ephemeral volumes](https://github.com/kubernetes/kubernetes/issues/79980)
-->
&lt;h2 id="1-25-版本有什么新内容">1.25 版本有什么新内容？&lt;/h2>
&lt;p>1.25 版本修复了几个与 CSI 内联存储卷相关的漏洞，
并且 &lt;a href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/feature-gates/">CSIInlineVolume 特性门控&lt;/a>
已正式发布，锁定为 &lt;code>True&lt;/code>。
因为没有新的 API 变化，所以除了这些错误修复外，使用该功能 Beta 版本的用户应该不会注意到任何重大变化。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/89290">#89290 - CSI inline volumes should support fsGroup&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/79980">#79980 - CSI volume reconstruction does not work for ephemeral volumes&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## When to use this feature
CSI inline volumes are meant for simple local volumes that should follow the lifecycle of the pod. They may be useful for providing secrets, configuration data, or other special-purpose storage to the pod from a CSI driver.
A CSI driver is not suitable for inline use when:
- The volume needs to persist longer than the lifecycle of a pod
- Volume snapshots, cloning, or volume expansion are required
- The CSI driver requires `volumeAttributes` that should be restricted to an administrator
-->
&lt;h2 id="何时使用此功能">何时使用此功能&lt;/h2>
&lt;p>CSI 内联存储卷是为简单的本地卷准备的，这种本地卷应该跟随 Pod 的生命周期。
它们对于使用 CSI 驱动为 Pod 提供 Secret、配置数据或其他特殊用途的存储可能很有用。&lt;/p>
&lt;p>在以下情况下，CSI 驱动不适合内联使用：&lt;/p>
&lt;ul>
&lt;li>卷需要持续的时间超过 Pod 的生命周期&lt;/li>
&lt;li>卷快照、克隆或卷扩展是必需的&lt;/li>
&lt;li>CSI 驱动需要 &lt;code>volumeAttributes&lt;/code> 字段，此字段应该限制给管理员使用&lt;/li>
&lt;/ul>
&lt;!--
## How to use this feature
In order to use this feature, the `CSIDriver` spec must explicitly list `Ephemeral` as one of the supported `volumeLifecycleModes`. Here is a simple example from the [Secrets Store CSI Driver](https://github.com/kubernetes-sigs/secrets-store-csi-driver).
-->
&lt;h2 id="如何使用此功能">如何使用此功能&lt;/h2>
&lt;p>为了使用这个功能，&lt;code>CSIDriver&lt;/code> 规约必须明确将 &lt;code>Ephemeral&lt;/code> 列举为 &lt;code>volumeLifecycleModes&lt;/code> 的参数之一。
下面是一个来自 &lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secrets Store CSI Driver&lt;/a> 的简单例子。&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
name: secrets-store.csi.k8s.io
spec:
podInfoOnMount: true
attachRequired: false
volumeLifecycleModes:
- Ephemeral
&lt;/code>&lt;/pre>&lt;!--
Any pod spec may then reference that CSI driver to create an inline volume, as in this example.
-->
&lt;p>所有 Pod 规约都可以引用该 CSI 驱动来创建一个内联卷，如下例所示。&lt;/p>
&lt;pre tabindex="0">&lt;code>kind: Pod
apiVersion: v1
metadata:
name: my-csi-app-inline
spec:
containers:
- name: my-frontend
image: busybox
volumeMounts:
- name: secrets-store-inline
mountPath: &amp;#34;/mnt/secrets-store&amp;#34;
readOnly: true
command: [ &amp;#34;sleep&amp;#34;, &amp;#34;1000000&amp;#34; ]
volumes:
- name: secrets-store-inline
csi:
driver: secrets-store.csi.k8s.io
readOnly: true
volumeAttributes:
secretProviderClass: &amp;#34;my-provider&amp;#34;
&lt;/code>&lt;/pre>&lt;!--
If the driver supports any volume attributes, you can provide these as part of the `spec` for the Pod as well:
-->
&lt;p>如果驱动程序支持一些卷属性，你也可以将这些属性作为 Pod &lt;code>spec&lt;/code> 的一部分。&lt;/p>
&lt;pre tabindex="0">&lt;code> csi:
driver: block.csi.vendor.example
volumeAttributes:
foo: bar
&lt;/code>&lt;/pre>&lt;!--
## Example Use Cases
Two existing CSI drivers that support the `Ephemeral` volume lifecycle mode are the Secrets Store CSI Driver and the Cert-Manager CSI Driver.
The [Secrets Store CSI Driver](https://github.com/kubernetes-sigs/secrets-store-csi-driver) allows users to mount secrets from external secret stores into a pod as an inline volume. This can be useful when the secrets are stored in an external managed service or Vault instance.
The [Cert-Manager CSI Driver](https://github.com/cert-manager/csi-driver) works along with [cert-manager](https://cert-manager.io/) to seamlessly request and mount certificate key pairs into a pod. This allows the certificates to be renewed and updated in the application pod automatically.
-->
&lt;h2 id="使用案例示例">使用案例示例&lt;/h2>
&lt;p>支持 &lt;code>Ephemeral&lt;/code> 卷生命周期模式的两个现有 CSI 驱动是 Secrets Store CSI 驱动和 Cert-Manager CSI 驱动。&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secrets Store CSI Driver&lt;/a>
允许用户将 Secret 作为内联卷从外部挂载到一个 Pod 中。
当密钥存储在外部管理服务或 Vault 实例中时，这可能很有用。&lt;/p>
&lt;p>&lt;a href="https://github.com/cert-manager/csi-driver">Cert-Manager CSI Driver&lt;/a> 与 &lt;a href="https://cert-manager.io/">cert-manager&lt;/a> 协同工作，
无缝地请求和挂载证书密钥对到一个 Pod 中。这使得证书可以在应用 Pod 中自动更新。&lt;/p>
&lt;!--
## Security Considerations
Special consideration should be given to which CSI drivers may be used as inline volumes. `volumeAttributes` are typically controlled through the `StorageClass`, and may contain attributes that should remain restricted to the cluster administrator. Allowing a CSI driver to be used for inline ephmeral volumes means that any user with permission to create pods may also provide `volumeAttributes` to the driver through a pod spec.
Cluster administrators may choose to omit (or remove) `Ephemeral` from `volumeLifecycleModes` in the CSIDriver spec to prevent the driver from being used as an inline ephemeral volume, or use an [admission webhook](/docs/reference/access-authn-authz/extensible-admission-controllers/) to restrict how the driver is used.
-->
&lt;h2 id="安全考虑因素">安全考虑因素&lt;/h2>
&lt;p>应特别考虑哪些 CSI 驱动可作为内联卷使用。
&lt;code>volumeAttributes&lt;/code> 通常通过 &lt;code>StorageClass&lt;/code> 控制，并可能包含应限制给集群管理员的属性。
允许 CSI 驱动用于内联临时卷意味着任何有权限创建 Pod 的用户也可以通过 Pod 规约向驱动提供 &lt;code>volumeAttributes&lt;/code> 字段。&lt;/p>
&lt;p>集群管理员可以选择从 CSIDriver 规约中的 &lt;code>volumeLifecycleModes&lt;/code> 中省略（或删除） &lt;code>Ephemeral&lt;/code>，
以防止驱动被用作内联临时卷，或者使用&lt;a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/">准入 Webhook&lt;/a> 来限制驱动的使用。&lt;/p>
&lt;!--
## References
For more information on this feature, see:
- [Kubernetes documentation](https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes)
- [CSI documentation](https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html)
- [KEP-596](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/596-csi-inline-volumes/README.md)
- [Beta blog post for CSI Inline Volumes](https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/)
-->
&lt;h2 id="参考资料">参考资料&lt;/h2>
&lt;p>关于此功能的更多信息，请参阅：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">Kubernetes 文档&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html">CSI 文档&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/596-csi-inline-volumes/README.md">KEP-596&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/">CSI 内联存储卷的 Beta 阶段博客文章&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: PodSecurityPolicy：历史背景</title><link>https://kubernetes.io/zh-cn/blog/2022/08/23/podsecuritypolicy-the-historical-context/</link><pubDate>Tue, 23 Aug 2022 15:00:00 -0800</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/08/23/podsecuritypolicy-the-historical-context/</guid><description>
&lt;!--
layout: blog
title: "PodSecurityPolicy: The Historical Context"
date: 2022-08-23T15:00:00-0800
slug: podsecuritypolicy-the-historical-context
evergreen: true
-->
&lt;!--
**Author:** Mahé Tardy (Quarkslab)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Mahé Tardy (Quarkslab)&lt;/p>
&lt;!--
The PodSecurityPolicy (PSP) admission controller has been removed, as of
Kubernetes v1.25. Its deprecation was announced and detailed in the blog post
[PodSecurityPolicy Deprecation: Past, Present, and Future](/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/),
published for the Kubernetes v1.21 release.
-->
&lt;p>从 Kubernetes v1.25 开始，PodSecurityPolicy (PSP) 准入控制器已被移除。
在为 Kubernetes v1.21 发布的博文 &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy 弃用：过去、现在和未来&lt;/a>
中，已经宣布并详细说明了它的弃用情况。&lt;/p>
&lt;!--
This article aims to provide historical context on the birth and evolution of
PSP, explain why the feature never made it to stable, and show why it was
removed and replaced by Pod Security admission control.
-->
&lt;p>本文旨在提供 PSP 诞生和演变的历史背景，解释为什么从未使该功能达到稳定状态，并说明为什么它被移除并被 Pod 安全准入控制取代。&lt;/p>
&lt;!--
PodSecurityPolicy, like other specialized admission control plugins, provided
fine-grained permissions on specific fields concerning the pod security settings
as a built-in policy API. It acknowledged that cluster administrators and
cluster users are usually not the same people, and that creating workloads in
the form of a Pod or any resource that will create a Pod should not equal being
"root on the cluster". It could also encourage best practices by configuring
more secure defaults through mutation and decoupling low-level Linux security
decisions from the deployment process.
-->
&lt;p>PodSecurityPolicy 与其他专门的准入控制插件一样，作为内置的策略 API，对有关 Pod 安全设置的特定字段提供细粒度的权限。
它承认集群管理员和集群用户通常不是同一个人，并且以 Pod 形式或任何将创建 Pod 的资源的形式创建工作负载的权限不应该等同于“集群上的 root 账户”。
它还可以通过变更配置来应用更安全的默认值，并将底层 Linux 安全决策与部署过程分离来促进最佳实践。&lt;/p>
&lt;!--
## The birth of PodSecurityPolicy
PodSecurityPolicy originated from OpenShift's SecurityContextConstraints
(SCC) that were in the very first release of the Red Hat OpenShift Container Platform,
even before Kubernetes 1.0. PSP was a stripped-down version of the SCC.
-->
&lt;h2 id="podsecuritypolicy-的诞生">PodSecurityPolicy 的诞生&lt;/h2>
&lt;p>PodSecurityPolicy 源自 OpenShift 的 SecurityContextConstraints (SCC)，
它出现在 Red Hat OpenShift 容器平台的第一个版本中，甚至在 Kubernetes 1.0 之前。PSP 是 SCC 的精简版。&lt;/p>
&lt;!--
The origin of the creation of PodSecurityPolicy is difficult to track, notably
because it was mainly added before Kubernetes Enhancements Proposal (KEP)
process, when design proposals were still a thing. Indeed, the archive of the final
[design proposal](https://github.com/kubernetes/design-proposals-archive/blob/main/auth/pod-security-policy.md)
is still available. Nevertheless, a [KEP issue number five](https://github.com/kubernetes/enhancements/issues/5)
was created after the first pull requests were merged.
-->
&lt;p>PodSecurityPolicy 的创建起源很难追踪，特别是因为它主要是在 Kubernetes 增强提案 (KEP) 流程之前添加的，
当时仍在使用设计提案（Design Proposal）。事实上，最终&lt;a href="https://github.com/kubernetes/design-proposals-archive/blob/main/auth/pod-security-policy.md">设计提案&lt;/a>的存档仍然可以找到。
尽管如此，&lt;a href="https://github.com/kubernetes/enhancements/issues/5">编号为 5 的 KEP&lt;/a>
是在合并第一个拉取请求后创建的。&lt;/p>
&lt;!--
Before adding the first piece of code that created PSP, two main pull
requests were merged into Kubernetes, a [`SecurityContext` subresource](https://github.com/kubernetes/kubernetes/pull/7343)
that defined new fields on pods' containers, and the first iteration of the [ServiceAccount](https://github.com/kubernetes/kubernetes/pull/7101)
API.
-->
&lt;p>在添加创建 PSP 的第一段代码之前，两个主要的拉取请求被合并到 Kubernetes 中，
&lt;a href="https://github.com/kubernetes/kubernetes/pull/7343">&lt;code>SecurityContext&lt;/code> 子资源&lt;/a>
定义了 Pod 容器上的新字段，以及 &lt;a href="https://github.com/kubernetes/kubernetes/pull/7101">ServiceAccount&lt;/a>
API 的第一次迭代。&lt;/p>
&lt;!--
Kubernetes 1.0 was released on 10 July 2015 without any mechanism to restrict the
security context and sensitive options of workloads, other than an alpha-quality
SecurityContextDeny admission plugin (then known as `scdeny`).
The [SecurityContextDeny plugin](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#securitycontextdeny)
is still in Kubernetes today (as an alpha feature) and creates an admission controller that
prevents the usage of some fields in the security context.
-->
&lt;p>Kubernetes 1.0 于 2015 年 7 月 10 日发布，除了 Alpha 阶段的 SecurityContextDeny 准入插件
（当时称为 &lt;code>scdeny&lt;/code>）之外，
没有任何机制来限制安全上下文和工作负载的敏感选项。
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#securitycontextdeny">SecurityContextDeny 插件&lt;/a>
今天仍存在于 Kubernetes 中（作为 Alpha 特性），负责创建一个准入控制器，以防止在安全上下文中使用某些字段。&lt;/p>
&lt;!--
The roots of the PodSecurityPolicy were added with
[the very first pull request on security policy](https://github.com/kubernetes/kubernetes/pull/7893),
which added the design proposal with the new PSP object, based on the SCC (Security Context Constraints). It
was a long discussion of nine months, with back and forth from OpenShift's SCC,
many rebases, and the rename to PodSecurityPolicy that finally made it to
upstream Kubernetes in February 2016. Now that the PSP object
had been created, the next step was to add an admission controller that could enforce
these policies. The first step was to add the admission
[without taking into account the users or groups](https://github.com/kubernetes/kubernetes/pull/7893#issuecomment-180410539).
A specific [issue to bring PodSecurityPolicy to a usable state](https://github.com/kubernetes/kubernetes/issues/23217)
was added to keep track of the progress and a first version of the admission
controller was merged in [pull request named PSP admission](https://github.com/kubernetes/kubernetes/pull/24600)
in May 2016. Then around two months later, Kubernetes 1.3 was released.
-->
&lt;p>PodSecurityPolicy 的根源是&lt;a href="https://github.com/kubernetes/kubernetes/pull/7893">早期关于安全策略的一个拉取请求&lt;/a>，
它以 SCC（安全上下文约束）为基础，增加了新的 PSP 对象的设计方案。这是一个长达 9 个月的漫长讨论，
基于 OpenShift 的 SCC 反复讨论，
多次变动，并重命名为 PodSecurityPolicy，最终在 2016 年 2 月进入上游 Kubernetes。
现在 PSP 对象已经创建，下一步是添加一个可以执行这些政策的准入控制器。
第一步是添加&lt;a href="https://github.com/kubernetes/kubernetes/pull/7893#issuecomment-180410539">不考虑用户或组&lt;/a>
的准入控制。
2016 年 5 月，一个特定的&lt;a href="https://github.com/kubernetes/kubernetes/issues/23217">使 PodSecurityPolicy 达到可用状态的问题&lt;/a>被添加进来，
以跟踪进展，并在&lt;a href="https://github.com/kubernetes/kubernetes/pull/24600">名为 PSP 准入的拉取请求&lt;/a>中合并了准入控制器的第一个版本。
然后大约两个月后，发布了 Kubernetes 1.3。&lt;/p>
&lt;!--
Here is a timeline that recaps the main pull requests of the birth of the
PodSecurityPolicy and its admission controller with 1.0 and 1.3 releases as
reference points.
-->
&lt;p>下面是一个时间表，它以 1.0 和 1.3 版本作为参考点，回顾了 PodSecurityPolicy 及其准入控制器诞生的主要拉取请求。&lt;/p>
&lt;figure>
&lt;img src="./timeline.svg"
alt="Timeline of the PodSecurityPolicy creation pull requests"/>
&lt;/figure>
&lt;!--
After that, the PSP admission controller was enhanced by adding what was initially
left aside. [The authorization mechanism](https://github.com/kubernetes/kubernetes/pull/33080),
merged in early November 2016 allowed administrators to use multiple policies
in a cluster to grant different levels of access for different types of users.
Later, a [pull request](https://github.com/kubernetes/kubernetes/pull/52849)
merged in October 2017 fixed [a design issue](https://github.com/kubernetes/kubernetes/issues/36184)
on ordering PodSecurityPolicies between mutating and alphabetical order, and continued to
build the PSP admission as we know it. After that, many improvements and fixes
followed to build the PodSecurityPolicy feature of recent Kubernetes releases.
-->
&lt;p>之后，PSP 准入控制器通过添加最初被搁置的内容进行了增强。
在 2016 年 11 月上旬合并&lt;a href="https://github.com/kubernetes/kubernetes/pull/33080">鉴权机制&lt;/a>，
允许管理员在集群中使用多个策略，为不同类型的用户授予不同级别的访问权限。
后来，2017 年 10 月合并的一个&lt;a href="https://github.com/kubernetes/kubernetes/pull/52849">拉取请求&lt;/a>
修复了 PodSecurityPolicies 在变更和字母顺序之间冲突的&lt;a href="https://github.com/kubernetes/kubernetes/issues/36184">设计问题&lt;/a>，
并继续构建我们所知道的 PSP 准入。之后，进行了许多改进和修复，以构建最近 Kubernetes 版本的 PodSecurityPolicy 功能。&lt;/p>
&lt;!--
## The rise of Pod Security Admission
Despite the crucial issue it was trying to solve, PodSecurityPolicy presented
some major flaws:
-->
&lt;h2 id="pod-安全准入的兴起">Pod 安全准入的兴起&lt;/h2>
&lt;p>尽管 PodSecurityPolicy 试图解决的是一个关键问题，但它却包含一些重大缺陷：&lt;/p>
&lt;!--
- **Flawed authorization model** - users can create a pod if they have the
**use** verb on the PSP that allows that pod or the pod's service account has
the **use** permission on the allowing PSP.
- **Difficult to roll out** - PSP fail-closed. That is, in the absence of a policy,
all pods are denied. It mostly means that it cannot be enabled by default and
that users have to add PSPs for all workloads before enabling the feature,
thus providing no audit mode to discover which pods would not be allowed by
the new policy. The opt-in model also leads to insufficient test coverage and
frequent breakage due to cross-feature incompatibility. And unlike RBAC,
there was no strong culture of shipping PSP manifests with projects.
- **Inconsistent unbounded API** - the API has grown with lots of
inconsistencies notably because of many requests for niche use cases: e.g.
labels, scheduling, fine-grained volume controls, etc. It has poor
composability with a weak prioritization model, leading to unexpected
mutation priority. It made it really difficult to combine PSP with other
third-party admission controllers.
- **Require security knowledge** - effective usage still requires an
understanding of Linux security primitives. e.g. MustRunAsNonRoot +
AllowPrivilegeEscalation.
-->
&lt;ul>
&lt;li>&lt;strong>有缺陷的鉴权模式&lt;/strong> - 如果用户针对 PSP 具有执行 &lt;strong>use&lt;/strong> 动作的权限，而此 PSP 准许该 Pod
或者该 Pod 的服务帐户对 PSP 执行 &lt;strong>use&lt;/strong> 操作，则用户可以创建一个 Pod。&lt;/li>
&lt;li>&lt;strong>难以推广&lt;/strong> - PSP 失败关闭。也就是说，在没有策略的情况下，所有 Pod 都会被拒绝。
这主要意味着默认情况下无法启用它，并且用户必须在启用该功能之前为所有工作负载添加 PSP，
因此没有提供审计模式来发现哪些 Pod 会不被新策略所允许。
这种采纳模式还导致测试覆盖率不足，并因跨特性不兼容而经常出现故障。
而且与 RBAC 不同的是，还不存在在项目中交付 PSP 清单的强大文化。&lt;/li>
&lt;li>&lt;strong>不一致的无边界 API&lt;/strong> - API 的发展有很多不一致的地方，特别是由于许多小众场景的请求：
如标签、调度、细粒度的卷控制等。它的可组合性很差，优先级模型较弱，会导致意外的变更优先级。
这使得 PSP 与其他第三方准入控制器的结合真的很困难。&lt;/li>
&lt;li>&lt;strong>需要安全知识&lt;/strong> - 有效使用 PSP 仍然需要了解 Linux 的安全原语。
例如：MustRunAsNonRoot + AllowPrivilegeEscalation。&lt;/li>
&lt;/ul>
&lt;!--
The experience with PodSecurityPolicy concluded that most users care for two or three
policies, which led to the creation of the [Pod Security Standards](/docs/concepts/security/pod-security-standards/),
that define three policies:
- **Privileged** - unrestricted policy.
- **Baseline** - minimally restrictive policy, allowing the default pod
configuration.
- **Restricted** - security best practice policy.
-->
&lt;p>PodSecurityPolicy 的经验得出的结论是，大多数用户关心两个或三个策略，这导致了
&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-standards/">Pod 安全标准&lt;/a>的创建，它定义了三个策略：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Privileged（特权的）&lt;/strong> - 策略不受限制。&lt;/li>
&lt;li>&lt;strong>Baseline（基线的）&lt;/strong> - 策略限制很少，允许默认 Pod 配置。&lt;/li>
&lt;li>&lt;strong>Restricted（受限的）&lt;/strong> - 安全最佳实践策略。&lt;/li>
&lt;/ul>
&lt;!--
The replacement for PSP, the new [Pod Security Admission](/docs/concepts/security/pod-security-admission/)
is an in-tree, stable for Kubernetes v1.25, admission plugin to enforce these
standards at the namespace level. It makes it easier to enforce basic pod
security without deep security knowledge. For more sophisticated use cases, you
might need a third-party solution that can be easily combined with Pod Security
Admission.
-->
&lt;p>作为 PSP 的替代品，新的 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-admission/">Pod 安全准入&lt;/a>是
Kubernetes v1.25 的树内稳定的准入插件，用于在命名空间级别强制执行这些标准。
无需深入的安全知识，就可以更轻松地实施基本的 Pod 安全性。
对于更复杂的用例，你可能需要一个可以轻松与 Pod 安全准入结合的第三方解决方案。&lt;/p>
&lt;!--
## What's next
For further details on the SIG Auth processes, covering PodSecurityPolicy removal and
creation of Pod Security admission, the
[SIG auth update at KubeCon NA 2019](https://www.youtube.com/watch?v=SFtHRmPuhEw)
and the [PodSecurityPolicy Replacement: Past, Present, and Future](https://www.youtube.com/watch?v=HsRRmlTJpls)
presentation at KubeCon NA 2021 records are available.
-->
&lt;h2 id="下一步是什么">下一步是什么&lt;/h2>
&lt;p>有关 SIG Auth 流程的更多详细信息，包括 PodSecurityPolicy 删除和 Pod 安全准入的创建，
请参阅在 KubeCon NA 2021 的
&lt;a href="https://www.youtube.com/watch?v=SFtHRmPuhEw">SIG auth update at KubeCon NA 2019&lt;/a> 和
&lt;a href="https://www.youtube.com/watch?v=HsRRmlTJpls">PodSecurityPolicy Replacement: Past, Present, and Future&lt;/a>
演示录像。&lt;/p>
&lt;!--
Particularly on the PSP removal, the
[PodSecurityPolicy Deprecation: Past, Present, and Future](/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/)
blog post is still accurate.
-->
&lt;p>特别是在 PSP 移除方面，&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy 弃用：过去、现在和未来&lt;/a>博客文章仍然是准确的。&lt;/p>
&lt;!--
And for the new Pod Security admission,
[documentation is available](/docs/concepts/security/pod-security-admission/).
In addition, the blog post
[Kubernetes 1.23: Pod Security Graduates to Beta](/blog/2021/12/09/pod-security-admission-beta/)
along with the KubeCon EU 2022 presentation
[The Hitchhiker's Guide to Pod Security](https://www.youtube.com/watch?v=gcz5VsvOYmI)
give great hands-on tutorials to learn.
-->
&lt;p>对于新的 Pod 安全许可，&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-admission/">可以访问文档&lt;/a>。
此外，博文 &lt;a href="https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/">Kubernetes 1.23: Pod Security Graduers to Beta&lt;/a>
以及 KubeCon EU 2022 演示文稿 &lt;a href="https://www.youtube.com/watch?v=gcz5VsvOYmI">the Hitchhicker’s Guide to Pod Security&lt;/a>
提供了很好的实践教程来学习。&lt;/p></description></item><item><title>Blog: Kubernetes v1.25: Combiner</title><link>https://kubernetes.io/zh-cn/blog/2022/08/23/kubernetes-v1-25-release/</link><pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/08/23/kubernetes-v1-25-release/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes v1.25: Combiner"
date: 2022-08-23
slug: kubernetes-v1-25-release
-->
&lt;!--
**Authors**: [Kubernetes 1.25 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md)
-->
&lt;p>&lt;strong>作者&lt;/strong>: &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md">Kubernetes 1.25 发布团队&lt;/a>&lt;/p>
&lt;!--
Announcing the release of Kubernetes v1.25!
-->
&lt;p>宣布 Kubernetes v1.25 的发版！&lt;/p>
&lt;!--
This release includes a total of 40 enhancements. Fifteen of those enhancements are entering Alpha, ten are graduating to Beta, and thirteen are graduating to Stable. We also have two features being deprecated or removed.
-->
&lt;p>这个版本总共包括 40 项增强功能。
其中 15 项增强功能进入 Alpha，10 项进入 Beta，13 项进入 Stable。
我们也废弃/移除了两个功能。&lt;/p>
&lt;!--
## Release theme and logo
**Kubernetes 1.25: Combiner**
&lt;figure class="release-logo">
&lt;img src="https://kubernetes.io/images/blog/2022-08-23-kubernetes-1.25-release/kubernetes-1.25.png"
alt="Combiner logo"/>
&lt;/figure>
The theme for Kubernetes v1.25 is _Combiner_.
The Kubernetes project itself is made up of many, many individual components that, when combined, take the form of the project you see today. It is also built and maintained by many individuals, all of them with different skills, experiences, histories, and interests, who join forces not just as the release team but as the many SIGs that support the project and the community year-round.
With this release, we wish to honor the collaborative, open spirit that takes us from isolated developers, writers, and users spread around the globe to a combined force capable of changing the world. Kubernetes v1.25 includes a staggering 40 enhancements, none of which would exist without the incredible power we have when we work together.
Inspired by our release lead's son, Albert Song, Kubernetes v1.25 is named for each and every one of you, no matter how you choose to contribute your unique power to the combined force that becomes Kubernetes.
-->
&lt;h2 id="版本主题和徽标">版本主题和徽标&lt;/h2>
&lt;p>&lt;strong>Kubernetes 1.25: Combiner&lt;/strong>&lt;/p>
&lt;figure class="release-logo">
&lt;img src="https://kubernetes.io/images/blog/2022-08-23-kubernetes-1.25-release/kubernetes-1.25.png"
alt="Combiner logo"/>
&lt;/figure>
&lt;p>Kubernetes v1.25 的主题是 &lt;strong>Combiner&lt;/strong>，即组合器。&lt;/p>
&lt;p>Kubernetes 项目本身是由特别多单独的组件组成的，这些组件组合起来就形成了你今天看到的这个项目。
同时它也是由许多个人建立和维护的，这些人拥有不同的技能、经验、历史和兴趣，
他们不仅作为发布团队成员，而且作为许多 SIG 成员，常年通力合作支持项目和社区。&lt;/p>
&lt;p>通过这次发版，我们希望向协作和开源的精神致敬，
这种精神使我们从分散在世界各地的独立开发者、作者和用户变成了能够改变世界的联合力量。
Kubernetes v1.25 包含了惊人的 40 项增强功能，
如果没有我们在一起工作时拥有的强大力量，这些增强功能都不会存在。&lt;/p>
&lt;p>受我们的发布负责人的儿子 Albert Song 的启发，Kubernetes v1.25 是以你们每一个人命名的，
无论你们选择如何作为 Kubernetes 的联合力量贡献自己的独有力量。&lt;/p>
&lt;!--
## What's New (Major Themes)
### PodSecurityPolicy is removed; Pod Security Admission graduates to Stable {#pod-security-changes}
PodSecurityPolicy was initially [deprecated in v1.21](/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/), and with the release of v1.25, it has been removed. The updates required to improve its usability would have introduced breaking changes, so it became necessary to remove it in favor of a more friendly replacement. That replacement is [Pod Security Admission](/docs/concepts/security/pod-security-admission/), which graduates to Stable with this release. If you are currently relying on PodSecurityPolicy, please follow the instructions for [migration to Pod Security Admission](/docs/tasks/configure-pod-container/migrate-from-psp/).
-->
&lt;h2 id="新增内容-主要主题">新增内容（主要主题）&lt;/h2>
&lt;h3 id="pod-security-changes">移除 PodSecurityPolicy；Pod Security Admission 成长为 Stable&lt;/h3>
&lt;p>PodSecurityPolicy 是在 &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">1.21 版本中被弃用&lt;/a>，到 1.25 版本被移除。
因为提升其可用性的变更会带来破坏性的变化，所以有必要将其删除，以支持一个更友好的替代品。
这个替代品就是 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a>，它在这个版本里成长为 Stable。
如果你最近依赖于 PodSecurityPolicy，请参考 &lt;a href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/migrate-from-psp/">Pod Security Admission 迁移说明&lt;/a>。&lt;/p>
&lt;!--
### Ephemeral Containers Graduate to Stable
[Ephemeral Containers](/docs/concepts/workloads/pods/ephemeral-containers/) are containers that exist for only a limited time within an existing pod. This is particularly useful for troubleshooting when you need to examine another container but cannot use `kubectl exec` because that container has crashed or its image lacks debugging utilities. Ephemeral containers graduated to Beta in Kubernetes v1.23, and with this release, the feature graduates to Stable.
-->
&lt;h3 id="ephemeral-containers-成长为-stable">Ephemeral Containers 成长为 Stable&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/pods/ephemeral-containers/">临时容器&lt;/a>是在现有的 Pod 中存在有限时间的容器。
当你需要检查另一个容器，但因为该容器已经崩溃或其镜像缺乏调试工具不能使用 &lt;code>kubectl exec&lt;/code> 时，它对故障排除特别有用。
临时容器在 Kubernetes v1.23 中成长为 Beta，并在这个版本中，该功能成长为 Stable。&lt;/p>
&lt;!--
### Support for cgroups v2 Graduates to Stable
It has been more than two years since the Linux kernel cgroups v2 API was declared stable. With some distributions now defaulting to this API, Kubernetes must support it to continue operating on those distributions. cgroups v2 offers several improvements over cgroups v1, for more information see the [cgroups v2](https://kubernetes.io/docs/concepts/architecture/cgroups/) documentation. While cgroups v1 will continue to be supported, this enhancement puts us in a position to be ready for its eventual deprecation and replacement.
-->
&lt;h3 id="对-cgroups-v2-的支持进入-stable-阶段">对 cgroups v2 的支持进入 Stable 阶段&lt;/h3>
&lt;p>自 Linux 内核 cgroups v2 API 宣布稳定以来，已经有两年多的时间了。
随着一些发行版现在默认使用该 API，Kubernetes 必须支持它以继续在这些发行版上运行。
cgroups v2 比 cgroups v1 提供了一些改进，更多信息参见 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/cgroups/">cgroups v2&lt;/a> 文档。
虽然 cgroups v1 将继续受到支持，但这一改进使我们能够为其最终的废弃和替代做好准备。&lt;/p>
&lt;!--
### Improved Windows support
- [Performance dashboards](http://perf-dash.k8s.io/#/?jobname=soak-tests-capz-windows-2019) added support for Windows
- [Unit tests](https://github.com/kubernetes/kubernetes/issues/51540) added support for Windows
- [Conformance tests](https://github.com/kubernetes/kubernetes/pull/108592) added support for Windows
- New GitHub repository created for [Windows Operational Readiness](https://github.com/kubernetes-sigs/windows-operational-readiness)
-->
&lt;h3 id="改善对-windows-系统的支持">改善对 Windows 系统的支持&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="http://perf-dash.k8s.io/#/?jobname=soak-tests-capz-windows-2019">性能仪表板&lt;/a>增加了对 Windows 系统的支持&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/51540">单元测试&lt;/a>增加了对 Windows 系统的支持&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/108592">一致性测试&lt;/a>增加了对 Windows 系统的支持&lt;/li>
&lt;li>为 &lt;a href="https://github.com/kubernetes-sigs/windows-operational-readiness">Windows Operational Readiness&lt;/a> 创建了新的 GitHub 仓库&lt;/li>
&lt;/ul>
&lt;!--
### Moved container registry service from k8s.gcr.io to registry.k8s.io
[Moving container registry from k8s.gcr.io to registry.k8s.io](https://github.com/kubernetes/kubernetes/pull/109938) got merged. For more details, see the [wiki page](https://github.com/kubernetes/k8s.io/wiki/New-Registry-url-for-Kubernetes-\(registry.k8s.io\)), [announcement](https://groups.google.com/a/kubernetes.io/g/dev/c/DYZYNQ_A6_c/m/oD9_Q8Q9AAAJ) was sent to the kubernetes development mailing list.
-->
&lt;h3 id="将容器注册服务从-k8s-gcr-io-迁移至-registry-k8s-io">将容器注册服务从 k8s.gcr.io 迁移至 registry.k8s.io&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/kubernetes/pull/109938">将容器注册服务从 k8s.gcr.io 迁移至 registry.k8s.io&lt;/a> 的 PR 已经被合并。
更多细节参考 &lt;a href="https://github.com/kubernetes/k8s.io/wiki/New-Registry-url-for-Kubernetes-(registry.k8s.io)">wiki 页面&lt;/a>，
同时&lt;a href="https://groups.google.com/a/kubernetes.io/g/dev/c/DYZYNQ_A6_c/m/oD9_Q8Q9AAAJ">公告&lt;/a>已发送到 kubernetes 开发邮件列表。&lt;/p>
&lt;!--
### Promoted SeccompDefault to Beta
SeccompDefault promoted to beta, see the tutorial [Restrict a Container's Syscalls with seccomp](https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads) for more details.
-->
&lt;h3 id="seccompdefault-升级为-beta">SeccompDefault 升级为 Beta&lt;/h3>
&lt;p>SeccompDefault 升级为 Beta，
更多细节参考教程&lt;a href="https://kubernetes.io/zh-cn/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads">用 seccomp 限制一个容器的系统调用&lt;/a>。&lt;/p>
&lt;!--
### Promoted endPort in Network Policy to Stable
Promoted `endPort` in [Network Policy](https://kubernetes.io/docs/concepts/services-networking/network-policies/#targeting-a-range-of-ports) to GA. Network Policy providers that support `endPort` field now can use it to specify a range of ports to apply a Network Policy. Previously, each Network Policy could only target a single port.
Please be aware that `endPort` field **must be supported** by the Network Policy provider. If your provider does not support `endPort`, and this field is specified in a Network Policy, the Network Policy will be created covering only the port field (single port).
-->
&lt;h3 id="网络策略中-endport-升级为-stable">网络策略中 endPort 升级为 Stable&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/network-policies/#targeting-a-range-of-ports">网络策略&lt;/a>中的
&lt;code>endPort&lt;/code> 已经迎来 GA 正式发布。
支持 &lt;code>endPort&lt;/code> 字段的网络策略提供程序现在可使用该字段来指定端口范围，应用网络策略。
在之前的版本中，每个网络策略只能指向单一端口。&lt;/p>
&lt;p>请注意，网络策略提供程序 &lt;strong>必须支持&lt;/strong> &lt;code>endPort&lt;/code> 字段。
如果提供程序不支持 &lt;code>endPort&lt;/code>，又在网络策略中指定了此字段，
则会创建出仅覆盖端口字段（单端口）的网络策略。&lt;/p>
&lt;!--
### Promoted Local Ephemeral Storage Capacity Isolation to Stable
The [Local Ephemeral Storage Capacity Isolation](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/361-local-ephemeral-storage-isolation) feature moved to GA. This was introduced as alpha in 1.8, moved to beta in 1.10, and it is now a stable feature. It provides support for capacity isolation of local ephemeral storage between pods, such as `EmptyDir`, so that a pod can be hard limited in its consumption of shared resources by evicting Pods if its consumption of local ephemeral storage exceeds that limit.
-->
&lt;h3 id="本地临时容器存储容量隔离升级为-stable">本地临时容器存储容量隔离升级为 Stable&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/361-local-ephemeral-storage-isolation">本地临时存储容量隔离功能&lt;/a>已经迎来 GA 正式发布版本。
该功能在 1.8 版中作为 alpha 版本引入，在 1.10 中升级为 beta，现在终于成为了稳定功能。
它提供了对 Pod 之间本地临时存储容量隔离的支持，如 &lt;code>EmptyDir&lt;/code>，
因此，如果一个 Pod 对本地临时存储容量的消耗超过该限制，就可以通过驱逐 Pod 来硬性限制其对共享资源的消耗。&lt;/p>
&lt;!--
### Promoted core CSI Migration to Stable
[CSI Migration](https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/#quick-recap-what-is-csi-migration-and-why-migrate) is an ongoing effort that SIG Storage has been working on for a few releases. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. The [core CSI Migration](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration) feature moved to GA. CSI Migration for GCE PD and AWS EBS also moved to GA. CSI Migration for vSphere remains in beta (but is on by default). CSI Migration for Portworx moved to Beta (but is off-by-default).
-->
&lt;h3 id="核心-csi-迁移为稳定版">核心 CSI 迁移为稳定版&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/#quick-recap-what-is-csi-migration-and-why-migrate">CSI 迁移&lt;/a>是 SIG Storage 在之前多个版本中做出的持续努力。
目标是将树内数据卷插件转移到树外 CSI 驱动程序并最终移除树内数据卷插件。
此次&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">核心 CSI 迁移&lt;/a>已迎来 GA。
同样，GCE PD 和 AWS EBS 的 CSI 迁移也进入 GA 阶段。
vSphere 的 CSI 迁移仍为 beta（但也默认启用）。
Portworx 的 CSI 迁移同样处于 beta 阶段（但默认不启用）。&lt;/p>
&lt;!--
### Promoted CSI Ephemeral Volume to Stable
The [CSI Ephemeral Volume](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/596-csi-inline-volumes) feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it moved to GA. This feature is used by some CSI drivers such as the [secret-store CSI driver](https://github.com/kubernetes-sigs/secrets-store-csi-driver).
-->
&lt;h3 id="csi-临时数据卷升级为稳定版">CSI 临时数据卷升级为稳定版&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/596-csi-inline-volumes">CSI 临时数据卷&lt;/a>
功能允许在临时使用的情况下在 Pod 里直接指定 CSI 数据卷。
因此可以直接用它们在使用挂载卷的 Pod 内注入任意状态，如配置、秘密、身份、变量或类似信息。
这个功能最初是作为 alpha 功能在 1.15 版本中引入，现在已升级为 GA 通用版。
某些 CSI 驱动程序会使用此功能，例如&lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">存储密码的 CSI 驱动程序&lt;/a>。&lt;/p>
&lt;!--
### Promoted CRD Validation Expression Language to Beta
[CRD Validation Expression Language](https://github.com/kubernetes/enhancements/blob/master/keps/sig-api-machinery/2876-crd-validation-expression-language/README.md) is promoted to beta, which makes it possible to declare how custom resources are validated using the [Common Expression Language (CEL)](https://github.com/google/cel-spec). Please see the [validation rules](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules) guide.
-->
&lt;h3 id="crd-验证表达式语言升级为-beta">CRD 验证表达式语言升级为 Beta&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-api-machinery/2876-crd-validation-expression-language/README.md">CRD 验证表达式语言&lt;/a>已升级为 beta 版本，
这使得声明如何使用&lt;a href="https://github.com/google/cel-spec">通用表达式语言（CEL）&lt;/a>验证自定义资源成为可能。
请参考&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">验证规则&lt;/a>指导。&lt;/p>
&lt;!--
### Promoted Server Side Unknown Field Validation to Beta
Promoted the `ServerSideFieldValidation` feature gate to beta (on by default). This allows optionally triggering schema validation on the API server that errors when unknown fields are detected. This allows the removal of client-side validation from kubectl while maintaining the same core functionality of erroring out on requests that contain unknown or invalid fields.
-->
&lt;h3 id="服务器端未知字段验证升级为-beta">服务器端未知字段验证升级为 Beta&lt;/h3>
&lt;p>&lt;code>ServerSideFieldValidation&lt;/code> 特性门控已升级为 beta（默认开启）。
它允许在检测到未知字段时，有选择地触发 API 服务器上的模式验证机制。
因此这允许从 kubectl 中移除客户端验证的同时保持相同的核心功能，即对包含未知或无效字段的请求进行错误处理。&lt;/p>
&lt;!--
### Introduced KMS v2 API
Introduce KMS v2alpha1 API to add performance, rotation, and observability improvements. Encrypt data at rest (ie Kubernetes `Secrets`) with DEK using AES-GCM instead of AES-CBC for kms data encryption. No user action is required. Reads with AES-GCM and AES-CBC will continue to be allowed. See the guide [Using a KMS provider for data encryption](https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/) for more information.
-->
&lt;h3 id="引入-kms-v2-api">引入 KMS v2 API&lt;/h3>
&lt;p>引入 KMS v2 alpha1 API 以提升性能，实现轮替与可观察性改进。
此 API 使用 AES-GCM 替代了 AES-CBC，通过 DEK 实现静态数据（即 Kubernetes Secrets）加密。
过程中无需额外用户操作，而且仍然支持通过 AES-GCM 和 AES-CBC 进行读取。
更多信息参考&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kms-provider/">使用 KMS provider 进行数据加密&lt;/a>指南。&lt;/p>
&lt;!--
### Kube-proxy images are now based on distroless images
In previous releases, kube-proxy container images were built using Debian as the base image. Starting with this release, the images are now built using [distroless](https://github.com/GoogleContainerTools/distroless). This change reduced image size by almost 50% and decreased the number of installed packages and files to only those strictly required for kube-proxy to do its job.
-->
&lt;h3 id="kube-proxy-镜像当前基于无发行版镜像">Kube-proxy 镜像当前基于无发行版镜像&lt;/h3>
&lt;p>在以前的版本中，kube-proxy 的容器镜像是以 Debian 作为基础镜像构建的。
从这个版本开始，其镜像现在使用 &lt;a href="https://github.com/GoogleContainerTools/distroless">distroless&lt;/a> 来构建。
这一改变将镜像的大小减少了近 50%，并将安装的软件包和文件的数量减少到只有 kube-proxy 工作所需的那些。&lt;/p>
&lt;!--
## Other Updates
### Graduations to Stable
This release includes a total of thirteen enhancements promoted to stable:
* [Ephemeral Containers](https://github.com/kubernetes/enhancements/issues/277)
* [Local Ephemeral Storage Resource Management](https://github.com/kubernetes/enhancements/issues/361)
* [CSI Ephemeral Volumes](https://github.com/kubernetes/enhancements/issues/596)
* [CSI Migration - Core](https://github.com/kubernetes/enhancements/issues/625)
* [Graduate the kube-scheduler ComponentConfig to GA](https://github.com/kubernetes/enhancements/issues/785)
* [CSI Migration - AWS](https://github.com/kubernetes/enhancements/issues/1487)
* [CSI Migration - GCE](https://github.com/kubernetes/enhancements/issues/1488)
* [DaemonSets Support MaxSurge](https://github.com/kubernetes/enhancements/issues/1591)
* [NetworkPolicy Port Range](https://github.com/kubernetes/enhancements/issues/2079)
* [cgroups v2](https://github.com/kubernetes/enhancements/issues/2254)
* [Pod Security Admission](https://github.com/kubernetes/enhancements/issues/2579)
* [Add `minReadySeconds` to Statefulsets](https://github.com/kubernetes/enhancements/issues/2599)
* [Identify Windows pods at API admission level authoritatively](https://github.com/kubernetes/enhancements/issues/2802)
-->
&lt;h2 id="其他更新">其他更新&lt;/h2>
&lt;h3 id="稳定版升级">稳定版升级&lt;/h3>
&lt;p>1.25 版本共包含 13 项升级至稳定版的增强功能：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/277">临时容器&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/361">本地临时存储资源管理&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/596">CSI 临时数据卷&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/625">CSI 迁移 -- 核心&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/785">kube-scheduler ComponentConfig 升级为 GA 通用版&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1487">CSI 迁移 -- AWS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1488">CSI 迁移 -- GCE&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1591">DaemonSets 支持 MaxSurge&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2079">网络策略端口范围&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2254">cgroups v2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2579">Pod Security Admission&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2599">Statefulsets 增加 &lt;code>minReadySeconds&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2802">在 API 准入层级权威识别 Windows Pod&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Deprecations and Removals
Two features were [deprecated or removed](/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/) from Kubernetes with this release.
* [PodSecurityPolicy is removed](https://github.com/kubernetes/enhancements/issues/5)
* [GlusterFS plugin deprecated from available in-tree drivers](https://github.com/kubernetes/enhancements/issues/3446)
-->
&lt;h3 id="弃用和移除">弃用和移除&lt;/h3>
&lt;p>1.25 版本&lt;a href="https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/">废弃/移除&lt;/a>两个功能。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/5">移除 PodSecurityPolicy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3446">从树内驱动程序移除 GlusterFS 插件&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Release Notes
The complete details of the Kubernetes v1.25 release are available in our [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md).
-->
&lt;h3 id="发行版说明">发行版说明&lt;/h3>
&lt;p>Kubernetes 1.25 版本的完整信息可参考&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md">发行版说明&lt;/a>。&lt;/p>
&lt;!--
### Availability
Kubernetes v1.25 is available for download on [GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.25.0).
To get started with Kubernetes, check out these [interactive tutorials](/docs/tutorials/) or run local
Kubernetes clusters using containers as “nodes”, with [kind](https://kind.sigs.k8s.io/).
You can also easily install 1.25 using [kubeadm](/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;h3 id="获取">获取&lt;/h3>
&lt;p>Kubernetes 1.25 版本可在 &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.25.0">GitHub&lt;/a> 下载获取。
开始使用 Kubernetes 请查看这些&lt;a href="https://kubernetes.io/zh-cn/docs/tutorials/">交互式教程&lt;/a>或者使用
&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> 把容器当作 “节点” 来运行本地 Kubernetes 集群。
你也可以使用 &lt;a href="https://kubernetes.io/zh-cn/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a> 来简单的安装 1.25 版本。&lt;/p>
&lt;!--
### Release Team
Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that, when combined, make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.
We would like to thank the entire release team for the hours spent hard at work to ensure we deliver a solid Kubernetes v1.25 release for our community. Every one of you had a part to play in building this, and you all executed beautifully. We would like to extend special thanks to our fearless release lead, Cici Huang, for all she did to guarantee we had what we needed to succeed.
-->
&lt;h3 id="发布团队">发布团队&lt;/h3>
&lt;p>Kubernetes 的发展离不开其社区的支持、承诺和辛勤工作。
每个发布团队都是由专门的社区志愿者组成的，他们共同努力，建立了许多模块，这些模块结合起来，就构成了你所依赖的 Kubernetes。
从代码本身到文档和项目管理，这需要我们社区每一个人的专业技能。&lt;/p>
&lt;!--
### User Highlights
* Finleap Connect operates in a highly regulated environment. [In 2019, they had five months to implement mutual TLS (mTLS) across all services in their clusters for their business code to comply with the new European PSD2 payment directive](https://www.cncf.io/case-studies/finleap-connect/).
* PNC sought to develop a way to ensure new code would meet security standards and audit compliance requirements automatically—replacing the cumbersome 30-day manual process they had in place. Using Knative, [PNC developed internal tools to automatically check new code and changes to existing code](https://www.cncf.io/case-studies/pnc-bank/).
* Nexxiot needed highly-reliable, secure, performant, and cost efficient Kubernetes clusters. [They turned to Cilium as the CNI to lock down their clusters and enable resilient networking with reliable day two operations](https://www.cncf.io/case-studies/nexxiot/).
* Because the process of creating cyber insurance policies is a complicated multi-step process, At-Bay sought to improve operations by using asynchronous message-based communication patterns/facilities. [They determined that Dapr fulfilled its desired list of requirements and much more](https://www.cncf.io/case-studies/at-bay/).
-->
&lt;h3 id="重要用户">重要用户&lt;/h3>
&lt;ul>
&lt;li>Finleap Connect 在一个高度规范的环境中运作。
&lt;a href="https://www.cncf.io/case-studies/finleap-connect/">2019年，他们有五个月的时间在其集群的所有服务中实施交互 TLS（mTLS），以使其业务代码符合新的欧洲 PSD2 支付指令&lt;/a>。&lt;/li>
&lt;li>PNC 试图开发一种方法，以确保新的代码能够自动满足安全标准和审计合规性要求--取代他们现有的 30 天的繁琐的人工流程。
使用 Knative，&lt;a href="https://www.cncf.io/case-studies/pnc-bank/">PNC 开发了内部工具来自动检查新代码和对修改现有代码&lt;/a>。&lt;/li>
&lt;li>Nexxiot 公司需要高可用、安全、高性能以及低成本的
Kubernetes 集群。&lt;a href="https://www.cncf.io/case-studies/nexxiot/">他们求助于 Cilium 作为 CNI 来锁定他们的集群，并通过可靠的 Day2 操作实现弹性网络&lt;/a>。&lt;/li>
&lt;li>因为创建网络安全策略的过程是一个复杂的多步骤过程，
At-Bay 试图通过使用基于异步消息的通信模式/设施来改善运营。&lt;a href="https://www.cncf.io/case-studies/at-bay/">他们确定 Dapr 满足了其所需的要求清单，且远超预期&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
### Ecosystem Updates
* KubeCon + CloudNativeCon North America 2022 will take place in Detroit, Michigan from 24 – 28 October 2022! You can find more information about the conference and registration on the [event site](https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/).
* KubeDay event series kicks off with KubeDay Japan on December 7! Register or submit a proposal on the [event site](https://events.linuxfoundation.org/kubeday-japan/)
* In the [2021 Cloud Native Survey](https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/), the CNCF saw record Kubernetes and container adoption. Take a look at the [results of the survey](https://www.cncf.io/reports/cncf-annual-survey-2021/).
-->
&lt;h3 id="生态系统更新">生态系统更新&lt;/h3>
&lt;ul>
&lt;li>2022 北美 KubeCon + CloudNativeCon 将于 2022 年 10 月 24 - 28 日在密歇根州的底特律举行!
你可以在&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">活动网站&lt;/a>找到更多关于会议和注册的信息。&lt;/li>
&lt;li>KubeDay 系列活动将于 12 月 7 日在日本 KubeDay 拉开帷幕!
在&lt;a href="https://events.linuxfoundation.org/kubeday-japan/">活动网站&lt;/a>上注册或提交提案。&lt;/li>
&lt;li>在 &lt;a href="https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/">2021 云原生调查&lt;/a>中，CNCF 看见了创纪录的 Kubernetes 和容器应用。
请参考&lt;a href="https://www.cncf.io/reports/cncf-annual-survey-2021/">调查结果&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
### Project Velocity
The [CNCF K8s DevStats](https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m) project
aggregates a number of interesting data points related to the velocity of Kubernetes and various
sub-projects. This includes everything from individual contributions to the number of companies that
are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.
In the v1.25 release cycle, which [ran for 14 weeks](https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25) (May 23 to August 23), we saw contributions from [1065 companies](https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;var-metric=contributions) and [1620 individuals](https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All&amp;var-repo_name=kubernetes%2Fkubernetes).
-->
&lt;h3 id="项目进度">项目进度&lt;/h3>
&lt;p>&lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;amp;refresh=15m">CNCF K8s DevStats&lt;/a> 项目汇集了大量关于
Kubernetes 和各种子项目研发进度相关性的有趣的数据点。
其中包括从个人贡献到参与贡献的公司数量的全面信息，
并证明了为发展 Kubernetes 生态系统所做努力的深度和广度。&lt;/p>
&lt;p>在 1.25 版本的发布周期中，
该周期&lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25">运行了 14 周&lt;/a> (May 23 to August 23)，
我们看到来着 &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;amp;var-metric=contributions">1065 家公司&lt;/a>
以及 &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&amp;amp;var-repo_name=kubernetes%2Fkubernetes">1620 位个人&lt;/a>所做出的贡献。&lt;/p>
&lt;!--
## Upcoming Release Webinar
Join members of the Kubernetes v1.25 release team on Thursday September 22, 2022 10am – 11am PT to learn about
the major features of this release, as well as deprecations and removals to help plan for upgrades.
For more information and registration, visit the [event page](https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v125-release/).
-->
&lt;h2 id="即将举行的网络发布研讨会">即将举行的网络发布研讨会&lt;/h2>
&lt;p>加入 Kubernetes 1.25 版本发布团队的成员，将于 2022 年 9 月 22 日星期四上午 10 点至 11 点(太平洋时间)了解该版本的主要功能，
以及弃用和删除的内容，以帮助制定升级计划。
欲了解更多信息和注册，请访问&lt;a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v125-release/">活动页面&lt;/a>。&lt;/p>
&lt;!--
## Get Involved
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests.
Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below:
* Find out more about contributing to Kubernetes at the [Kubernetes Contributors](https://www.kubernetes.dev/) website
* Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for the latest updates
* Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
* Join the community on [Slack](http://slack.k8s.io/)
* Post questions (or answer questions) on [Server Fault](https://serverfault.com/questions/tagged/kubernetes).
* Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
* Read more about what’s happening with Kubernetes on the [blog](https://kubernetes.io/blog/)
* Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)
-->
&lt;h2 id="参与其中">参与其中&lt;/h2>
&lt;p>参与 Kubernetes 最简单的方法就是加入众多&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">特殊兴趣小组&lt;/a>(SIGs) 中你感兴趣的一个。
你有什么东西想要跟 Kubernetes 社区沟通吗？
来我们每周的&lt;a href="https://github.com/kubernetes/community/tree/master/communication">社区会议&lt;/a>分享你的想法，并参考一下渠道：&lt;/p>
&lt;ul>
&lt;li>在 &lt;a href="https://www.kubernetes.dev/">Kubernetes 贡献者&lt;/a>网站了解更多关于为 Kubernetes 做贡献的信息。&lt;/li>
&lt;li>在 Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> 上关注我们，了解最新动态。&lt;/li>
&lt;li>在 &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a> 上加入社区讨论。&lt;/li>
&lt;li>在 &lt;a href="http://slack.k8s.io/">Slack&lt;/a> 上加入社区。&lt;/li>
&lt;li>在 &lt;a href="https://serverfault.com/questions/tagged/kubernetes">Server Fault&lt;/a> 上发布问题（或者回答问题）。&lt;/li>
&lt;li>分享你的 Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">故事&lt;/a>&lt;/li>
&lt;li>在&lt;a href="https://kubernetes.io/blog/">博客&lt;/a>上阅读更多关于 Kubernetes 的情况。&lt;/li>
&lt;li>了解更多关于 &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes 发布团队&lt;/a>的信息。&lt;/li>
&lt;/ul></description></item><item><title>Blog: 聚焦 SIG Storage</title><link>https://kubernetes.io/zh-cn/blog/2022/08/22/sig-storage-spotlight/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/08/22/sig-storage-spotlight/</guid><description>
&lt;!--
layout: blog
title: "Spotlight on SIG Storage"
slug: sig-storage-spotlight
date: 2022-08-22
canonicalUrl: https://www.kubernetes.dev/blog/2022/08/22/sig-storage-spotlight-2022/
-->
&lt;!--
**Author**: Frederico Muñoz (SAS)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Frederico Muñoz (SAS)&lt;/p>
&lt;!--
Since the very beginning of Kubernetes, the topic of persistent data and how to address the requirement of stateful applications has been an important topic. Support for stateless deployments was natural, present from the start, and garnered attention, becoming very well-known. Work on better support for stateful applications was also present from early on, with each release increasing the scope of what could be run on Kubernetes.
-->
&lt;p>自 Kubernetes 诞生之初，持久数据以及如何解决有状态应用程序的需求一直是一个重要的话题。
对无状态部署的支持是很自然的、从一开始就存在的，并引起了人们的关注，变得众所周知。
从早期开始，我们也致力于更好地支持有状态应用程序，每个版本都增加了可以在 Kubernetes 上运行的范围。&lt;/p>
&lt;!--
Message queues, databases, clustered filesystems: these are some examples of the solutions that have different storage requirements and that are, today, increasingly deployed in Kubernetes. Dealing with ephemeral and persistent storage, local or remote, file or block, from many different vendors, while considering how to provide the needed resiliency and data consistency that users expect, all of this is under SIG Storage's umbrella.
-->
&lt;p>消息队列、数据库、集群文件系统：这些是具有不同存储要求的解决方案的一些示例，
如今这些解决方案越来越多地部署在 Kubernetes 中。
处理来自许多不同供应商的临时和持久存储（本地或远程、文件或块），同时考虑如何提供用户期望的所需弹性和数据一致性，
所有这些都在 SIG Storage 的整体负责范围之内。&lt;/p>
&lt;!--
In this SIG Storage spotlight, [Frederico Muñoz](https://twitter.com/fredericomunoz) (Cloud &amp; Architecture Lead at SAS) talked with [Xing Yang](https://twitter.com/2000xyang), Tech Lead at VMware and co-chair of SIG Storage, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute.
-->
&lt;p>在这次 SIG Storage 采访报道中，&lt;a href="https://twitter.com/fredericomunoz">Frederico Muñoz&lt;/a>
（SAS 的云和架构负责人）与 VMware 技术负责人兼 SIG Storage 联合主席
&lt;a href="https://twitter.com/2000xyang">Xing Yang&lt;/a>，讨论了 SIG 的组织方式、当前的挑战是什么以及如何进行参与和贡献。&lt;/p>
&lt;!--
## About SIG Storage
**Frederico (FSM)**: Hello, thank you for the opportunity of learning more about SIG Storage. Could you tell us a bit about yourself, your role, and how you got involved in SIG Storage.
-->
&lt;h2 id="关于-sig-storage">关于 SIG Storage&lt;/h2>
&lt;p>&lt;strong>Frederico (FSM)&lt;/strong>：你好，感谢你给我这个机会了解更多关于 SIG Storage 的情况。
你能否介绍一下你自己、你的角色以及你是如何参与 SIG Storage 的。&lt;/p>
&lt;!--
**Xing Yang (XY)**: I am a Tech Lead at VMware, working on Cloud Native Storage. I am also a Co-Chair of SIG Storage. I started to get involved in K8s SIG Storage at the end of 2017, starting with contributing to the [VolumeSnapshot](https://kubernetes.io/docs/concepts/storage/volume-snapshots/) project. At that time, the VolumeSnapshot project was still in an experimental, pre-alpha stage. It needed contributors. So I volunteered to help. Then I worked with other community members to bring VolumeSnapshot to Alpha in K8s 1.12 release in 2018, Beta in K8s 1.17 in 2019, and eventually GA in 1.20 in 2020.
-->
&lt;p>&lt;strong>Xing Yang (XY)&lt;/strong>：我是 VMware 的技术主管，从事云原生存储方面的工作。我也是 SIG Storage 的联合主席。
我从 2017 年底开始参与 K8s SIG Storage，开始为
&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/volume-snapshots/">VolumeSnapshot&lt;/a> 项目做贡献。
那时，VolumeSnapshot 项目仍处于实验性的 pre-alpha 阶段。它需要贡献者。所以我自愿提供帮助。
然后我与其他社区成员合作，在 2018 年的 K8s 1.12 版本中将 VolumeSnapshot 带入 Alpha，
2019 年在 K8s 1.17 版本中带入 Beta，并最终在 2020 年在 1.20 版本中带入 GA。&lt;/p>
&lt;!--
**FSM**: Reading the [SIG Storage charter](https://github.com/kubernetes/community/blob/master/sig-storage/charter.md) alone it’s clear that SIG Storage covers a lot of ground, could you describe how the SIG is organised?
-->
&lt;p>&lt;strong>FSM&lt;/strong>：仅仅阅读 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/charter.md">SIG Storage 章程&lt;/a>
就可以看出，SIG Storage 涵盖了很多领域，你能描述一下 SIG 的组织方式吗？&lt;/p>
&lt;!--
**XY**: In SIG Storage, there are two Co-Chairs and two Tech Leads. Saad Ali from Google and myself are Co-Chairs. Michelle Au from Google and Jan Šafránek from Red Hat are Tech Leads.
-->
&lt;p>&lt;strong>XY&lt;/strong>：在 SIG Storage 中，有两位联合主席和两位技术主管。来自 Google 的 Saad Ali 和我是联合主席。
来自 Google 的 Michelle Au 和来自 Red Hat 的 Jan Šafránek 是技术主管。&lt;/p>
&lt;!--
We have bi-weekly meetings where we go through features we are working on for each particular release, getting the statuses, making sure each feature has dev owners and reviewers working on it, and reminding people about the release deadlines, etc. More information on the SIG is on the [community page](https://github.com/kubernetes/community/tree/master/sig-storage). People can also add PRs that need attention, design proposals that need discussion, and other topics to the meeting agenda doc. We will go over them after project tracking is done.
-->
&lt;p>我们每两周召开一次会议，讨论我们正在为每个特定版本开发的功能，获取状态，确保每个功能都有开发人员和审阅人员在处理它，
并提醒人们发布截止日期等。有关 SIG 的更多信息，请查阅&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">社区页面&lt;/a>。
人们还可以将需要关注的 PR、需要讨论的设计提案和其他议题添加到会议议程文档中。
我们将在项目跟踪完成后对其进行审查。&lt;/p>
&lt;!--
We also have other regular meetings, i.e., CSI Implementation meeting, Object Bucket API design meeting, and one-off meetings for specific topics if needed. There is also a [K8s Data Protection Workgroup](https://github.com/kubernetes/community/blob/master/wg-data-protection/README.md) that is sponsored by SIG Storage and SIG Apps. SIG Storage owns or co-owns features that are being discussed at the Data Protection WG.
-->
&lt;p>我们还举行其他的定期会议，如 CSI 实施会议，Object Bucket API 设计会议，以及在需要时针对特定议题的一次性会议。
还有一个由 SIG Storage 和 SIG Apps 赞助的
&lt;a href="https://github.com/kubernetes/community/blob/master/wg-data-protection/README.md">K8s 数据保护工作组&lt;/a>。
SIG Storage 拥有或共同拥有数据保护工作组正在讨论的功能特性。&lt;/p>
&lt;!--
## Storage and Kubernetes
**FSM**: Storage is such a foundational component in so many things, not least in Kubernetes: what do you think are the Kubernetes-specific challenges in terms of storage management?
-->
&lt;h2 id="存储和-kubernetes">存储和 Kubernetes&lt;/h2>
&lt;p>&lt;strong>FSM&lt;/strong>：存储是很多模块的基础组件，尤其是 Kubernetes：你认为 Kubernetes 在存储管理方面的具体挑战是什么?&lt;/p>
&lt;!--
**XY**: In Kubernetes, there are multiple components involved for a volume operation. For example, creating a Pod to use a PVC has multiple components involved. There are the Attach Detach Controller and the external-attacher working on attaching the PVC to the pod. There’s the Kubelet that works on mounting the PVC to the pod. Of course the CSI driver is involved as well. There could be race conditions sometimes when coordinating between multiple components.
-->
&lt;p>&lt;strong>XY&lt;/strong>：在 Kubernetes 中，卷操作涉及多个组件。例如，创建一个使用 PVC 的 Pod 涉及多个组件。
有 Attach Detach Controller 和 external-attacher 负责将 PVC 连接到 Pod。
还有 Kubelet 可以将 PVC 挂载到 Pod 上。当然，CSI 驱动程序也参与其中。
在多个组件之间进行协调时，有时可能会出现竞争状况。&lt;/p>
&lt;!--
Another challenge is regarding core vs [Custom Resource Definitions](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) (CRD), not really storage specific. CRD is a great way to extend Kubernetes capabilities while not adding too much code to the Kubernetes core itself. However, this also means there are many external components that are needed when running a Kubernetes cluster.
-->
&lt;p>另一个挑战是关于核心与 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions&lt;/a>（CRD），
这并不是特定于存储的。CRD 是一种扩展 Kubernetes 功能的好方法，同时又不会向 Kubernetes 核心本身添加太多代码。
然而，这也意味着运行 Kubernetes 集群时需要许多外部组件。&lt;/p>
&lt;!--
From the SIG Storage side, one most notable example is Volume Snapshot. Volume Snapshot APIs are defined as CRDs. API definitions and controllers are out-of-tree. There is a common snapshot controller and a snapshot validation webhook that should be deployed on the control plane, similar to how kube-controller-manager is deployed. Although Volume Snapshot is a CRD, it is a core feature of SIG Storage. It is recommended for the K8s cluster distros to deploy Volume Snapshot CRDs, the snapshot controller, and the snapshot validation webhook, however, most of the time we don’t see distros deploy them. So this becomes a problem for the storage vendors: now it becomes their responsibility to deploy these non-driver specific common components. This could cause conflicts if a customer wants to use more than one storage system and deploy more than one CSI driver.
-->
&lt;p>在 SIG Storage 方面，一个最好的例子是卷快照。卷快照 API 被定义为 CRD。
API 定义和控制器是 out-of-tree。有一个通用的快照控制器和一个快照验证 Webhook
应该部署在控制平面上，类似于 kube-controller-manager 的部署方式。
虽然 Volume Snapshot 是一个 CRD，但它是 SIG Storage 的核心特性。
建议 K8s 集群发行版部署卷快照 CRD、快照控制器和快照验证 Webhook，然而，大多数时候我们没有看到发行版部署它们。
因此，这对存储供应商来说就成了一个问题：现在部署这些非驱动程序特定的通用组件成为他们的责任。
如果客户需要使用多个存储系统，且部署多个 CSI 驱动，可能会导致冲突。&lt;/p>
&lt;!--
**FSM**: Not only the complexity of a single storage system, you have to consider how they will be used together in Kubernetes?
-->
&lt;p>&lt;strong>FSM&lt;/strong>：不仅要考虑单个存储系统的复杂性，还要考虑它们在 Kubernetes 中如何一起使用？&lt;/p>
&lt;!--
**XY**: Yes, there are many different storage systems that can provide storage to containers in Kubernetes. They don’t work the same way. It is challenging to find a solution that works for everyone.
-->
&lt;p>&lt;strong>XY&lt;/strong>：是的，有许多不同的存储系统可以为 Kubernetes 中的容器提供存储。它们的工作方式不同。找到适合所有人的解决方案是具有挑战性的。&lt;/p>
&lt;!--
**FSM**: Storage in Kubernetes also involves interacting with external solutions, perhaps more so than other parts of Kubernetes. Is this interaction with vendors and external providers challenging? Has it evolved with time in any way?
-->
&lt;p>&lt;strong>FSM&lt;/strong>：Kubernetes 中的存储还涉及与外部解决方案的交互，可能比 Kubernetes 的其他部分更多。
这种与供应商和外部供应商的互动是否具有挑战性？它是否以任何方式随着时间而演变？&lt;/p>
&lt;!--
**XY**: Yes, it is definitely challenging. Initially Kubernetes storage had in-tree volume plugin interfaces. Multiple storage vendors implemented in-tree interfaces and have volume plugins in the Kubernetes core code base. This caused lots of problems. If there is a bug in a volume plugin, it affects the entire Kubernetes code base. All volume plugins must be released together with Kubernetes. There was no flexibility if storage vendors need to fix a bug in their plugin or want to align with their own product release.
-->
&lt;p>&lt;strong>XY&lt;/strong>：是的，这绝对是具有挑战性的。最初 Kubernetes 存储具有 in-tree 卷插件接口。
多家存储供应商实现了 in-tree 接口，并在 Kubernetes 核心代码库中拥有卷插件。这引起了很多问题。
如果卷插件中存在错误，它会影响整个 Kubernetes 代码库。所有卷插件必须与 Kubernetes 一起发布。
如果存储供应商需要修复其插件中的错误或希望与他们自己的产品版本保持一致，这是不灵活的。&lt;/p>
&lt;!--
**FSM**: That’s where CSI enters the game?
-->
&lt;p>&lt;strong>FSM&lt;/strong>：这就是 CSI 加入的原因？&lt;/p>
&lt;!--
**XY**: Exactly, then there comes [Container Storage Interface](https://kubernetes-csi.github.io/docs/) (CSI). This is an industry standard trying to design common storage interfaces so that a storage vendor can write one plugin and have it work across a range of container orchestration systems (CO). Now Kubernetes is the main CO, but back when CSI just started, there were Docker, Mesos, Cloud Foundry, in addition to Kubernetes. CSI drivers are out-of-tree so bug fixes and releases can happen at their own pace.
-->
&lt;p>&lt;strong>XY&lt;/strong>：没错，接下来就是&lt;a href="https://kubernetes-csi.github.io/docs/">容器存储接口&lt;/a>（CSI）。
这是一个试图设计通用存储接口的行业标准，以便存储供应商可以编写一个插件并让它在一系列容器编排系统（CO）中工作。
现在 Kubernetes 是主要的 CO，但是在 CSI 刚开始的时候，除了 Kubernetes 之外，还有 Docker、Mesos、Cloud Foundry。
CSI 驱动程序是 out-of-tree 的，因此可以按照自己的节奏进行错误修复和发布。&lt;/p>
&lt;!--
CSI is definitely a big improvement compared to in-tree volume plugins. Kubernetes implementation of CSI has been GA [since the 1.13 release](https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/). It has come a long way. SIG Storage has been working on moving in-tree volume plugins to out-of-tree CSI drivers for several releases now.
-->
&lt;p>与 in-tree 卷插件相比，CSI 绝对是一个很大的改进。CSI 的 Kubernetes
实现&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">自 1.13 版本以来&lt;/a>就达到 GA。
它已经发展了很长时间。SIG Storage 一直致力于将 in-tree 卷插件迁移到 out-of-tree 的 CSI 驱动，已经有几个版本了。&lt;/p>
&lt;!--
**FSM**: Moving drivers away from the Kubernetes main tree and into CSI was an important improvement.
-->
&lt;p>&lt;strong>FSM&lt;/strong>：将驱动程序从 Kubernetes 主仓移到 CSI 中是一项重要的改进。&lt;/p>
&lt;!--
**XY**: CSI interface is an improvement over the in-tree volume plugin interface, however, there are still challenges. There are lots of storage systems. Currently [there are more than 100 CSI drivers listed in CSI driver docs](https://kubernetes-csi.github.io/docs/drivers.html). These storage systems are also very diverse. So it is difficult to design a common API that works for all. We introduced capabilities at CSI driver level, but we also have challenges when volumes provisioned by the same driver have different behaviors. The other day we just had a meeting discussing Per Volume CSI Driver Capabilities. We have a problem differentiating some CSI driver capabilities when the same driver supports both block and file volumes. We are going to have follow up meetings to discuss this problem.
-->
&lt;p>&lt;strong>XY&lt;/strong>： CSI 接口是对 in-tree 卷插件接口的改进，但是仍然存在挑战。有很多存储系统。
目前在 &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">CSI 驱动程序文档中列出了 100 多个 CSI 驱动程序&lt;/a>。
这些存储系统也非常多样化。因此，很难设计一个适用于所有人的通用 API。
我们在 CSI 驱动层面引入了功能，但当同一驱动配置的卷具有不同的行为时，我们也会面临挑战。
前几天我们刚刚开会讨论每种卷 CSI 驱动程序功能。
当同一个驱动程序同时支持块卷和文件卷时，我们在区分某些 CSI 驱动程序功能时遇到了问题。
我们将召开后续会议来讨论这个问题。&lt;/p>
&lt;!--
## Ongoing challenges
**FSM**: Specifically for the [1.25 release](https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25) we can see that there are a relevant number of storage-related [KEPs](https://bit.ly/k8s125-enhancements) in the pipeline, would you say that this release is particularly important for the SIG?
-->
&lt;h2 id="持续的挑战">持续的挑战&lt;/h2>
&lt;p>&lt;strong>FSM&lt;/strong>：具体来说，对于 &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25">1.25 版本&lt;/a>
们可以看到管道中有一些与存储相关的 &lt;a href="https://bit.ly/k8s125-enhancements">KEPs&lt;/a>。
你是否认为这个版本对 SIG 特别重要？&lt;/p>
&lt;!--
**XY**: I wouldn’t say one release is more important than other releases. In any given release, we are working on a few very important things.
-->
&lt;p>&lt;strong>XY&lt;/strong>：我不会说一个版本比其他版本更重要。在任何给定的版本中，我们都在做一些非常重要的事情。&lt;/p>
&lt;!--
**FSM**: Indeed, but are there any 1.25 specific specificities and highlights you would like to point out though?
-->
&lt;p>&lt;strong>FSM&lt;/strong>：确实如此，但你是否想指出 1.25 版本的特定特性和亮点呢？&lt;/p>
&lt;!--
**XY**: Yes. For the 1.25 release, I want to highlight the following:
-->
&lt;p>&lt;strong>XY&lt;/strong>：好的。对于 1.25 版本，我想强调以下几点：&lt;/p>
&lt;!--
* [CSI Migration](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration) is an on-going effort that SIG Storage has been working on for a few releases now. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. There are 7 KEPs that we are targeting in 1.25 are related to CSI migration. There is one core KEP for the general CSI Migration feature. That is targeting GA in 1.25. CSI Migration for GCE PD and AWS EBS are targeting GA. CSI Migration for vSphere is targeting to have the feature gate on by default while staying in 1.25 that are in Beta. Ceph RBD and PortWorx are targeting Beta, with feature gate off by default. Ceph FS is targeting Alpha.
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">CSI 迁移&lt;/a>
是一项持续的工作，SIG Storage 已经工作了几个版本了。目标是将 in-tree 卷插件移动到 out-of-tree 的
CSI 驱动程序，并最终删除 in-tree 卷插件。在 1.25 版本中，有 7 个 KEP 与 CSI 迁移有关。
有一个核心 KEP 用于通用的 CSI 迁移功能。它的目标是在 1.25 版本中达到 GA。
GCE PD 和 AWS EBS 的 CSI 迁移以 GA 为目标。vSphere 的 CSI 迁移的目标是在默认情况下启用特性门控，
在 1.25 版本中达到 Beta。Ceph RBD 和 PortWorx 的目标是达到 Beta，默认关闭特性门控。
Ceph FS 的目标是达到 Alpha。&lt;/li>
&lt;/ul>
&lt;!--
* The second one I want to highlight is [COSI, the Container Object Storage Interface](https://github.com/kubernetes-sigs/container-object-storage-interface-spec). This is a sub-project under SIG Storage. COSI proposes object storage Kubernetes APIs to support orchestration of object store operations for Kubernetes workloads. It also introduces gRPC interfaces for object storage providers to write drivers to provision buckets. The COSI team has been working on this project for more than two years now. The COSI feature is targeting Alpha in 1.25. The KEP just got merged. The COSI team is working on updating the implementation based on the updated KEP.
-->
&lt;ul>
&lt;li>我要强调的第二个是 &lt;a href="https://github.com/kubernetes-sigs/container-object-storage-interface-spec">COSI，容器对象存储接口&lt;/a>。
这是 SIG Storage 下的一个子项目。COSI 提出对象存储 Kubernetes API 来支持 Kubernetes 工作负载的对象存储操作的编排。
它还为对象存储提供商引入了 gRPC 接口，以编写驱动程序来配置存储桶。COSI 团队已经在这个项目上工作两年多了。
COSI 功能的目标是 1.25 版本中达到 Alpha。KEP 刚刚合入。COSI 团队正在根据更新后的 KEP 更新实现。&lt;/li>
&lt;/ul>
&lt;!--
* Another feature I want to mention is [CSI Ephemeral Volume](https://github.com/kubernetes/enhancements/issues/596) support. This feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it is now targeting GA in 1.25.
-->
&lt;ul>
&lt;li>我要提到的另一个功能是 &lt;a href="https://github.com/kubernetes/enhancements/issues/596">CSI 临时卷&lt;/a>支持。
此功能允许在临时用例的 Pod 规约中直接指定 CSI 卷。它们可用于使用已安装的卷直接在 Pod 内注入任意状态，
例如配置、Secrets、身份、变量或类似信息。这最初是在 1.15 版本中作为一个 Alpha 功能引入的，现在它的目标是在 1.25 版本中达到 GA。&lt;/li>
&lt;/ul>
&lt;!--
**FSM**: If you had to single something out, what would be the most pressing areas the SIG is working on?
-->
&lt;p>&lt;strong>FSM&lt;/strong>：如果你必须单独列出一些内容，那么 SIG 正在研究的最紧迫的领域是什么?&lt;/p>
&lt;!--
**XY**: CSI migration is definitely one area that the SIG has put in lots of effort and it has been on-going for multiple releases now. It involves work from multiple cloud providers and storage vendors as well.
-->
&lt;p>&lt;strong>XY&lt;/strong>：CSI 迁移绝对是 SIG 投入大量精力的领域之一，并且现在已经进行了多个版本。它还涉及来自多个云提供商和存储供应商的工作。&lt;/p>
&lt;!--
## Community involvement
**FSM**: Kubernetes is a community-driven project. Any recommendation for anyone looking into getting involved in SIG Storage work? Where should they start?
-->
&lt;h2 id="社区参与">社区参与&lt;/h2>
&lt;p>&lt;strong>FSM&lt;/strong>：Kubernetes 是一个社区驱动的项目。对任何希望参与 SIG Storage 工作的人有什么建议吗？他们应该从哪里开始？&lt;/p>
&lt;!--
**XY**: Take a look at the [SIG Storage community page](https://github.com/kubernetes/community/tree/master/sig-storage), it has lots of information on how to get started. There are [SIG annual reports](https://github.com/kubernetes/community/blob/master/sig-storage/annual-report-2021.md) that tell you what we did each year. Take a look at the Contributing guide. It has links to presentations that can help you get familiar with Kubernetes storage concepts.
-->
&lt;p>&lt;strong>XY&lt;/strong>：查看 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG Storage 社区页面&lt;/a>，
它有很多关于如何开始的信息。&lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/annual-report-2021.md">SIG 年度报告&lt;/a>告诉你我们每年做了什么。
查看贡献指南。它有一些演示的链接，可以帮助你熟悉 Kubernetes 存储概念。&lt;/p>
&lt;!--
Join our [bi-weekly meetings on Thursdays](https://github.com/kubernetes/community/tree/master/sig-storage#meetings). Learn how the SIG operates and what we are working on for each release. Find a project that you are interested in and help out. As I mentioned earlier, I got started in SIG Storage by contributing to the Volume Snapshot project.
-->
&lt;p>参加我们&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">在星期四举行的双周会议&lt;/a>。
了解 SIG 的运作方式以及我们为每个版本所做的工作。找到你感兴趣的项目并提供贡献。
正如我之前提到的，我通过参与 Volume Snapshot 项目开始了 SIG Storage。&lt;/p>
&lt;!--
**FSM**: Any closing thoughts you would like to add?
-->
&lt;p>&lt;strong>FSM&lt;/strong>：你有什么要补充的结束语吗？&lt;/p>
&lt;!--
**XY**: SIG Storage always welcomes new contributors. We need contributors to help with building new features, fixing bugs, doing code reviews, writing tests, monitoring test grid health, and improving documentation, etc.
-->
&lt;p>&lt;strong>XY&lt;/strong>：SIG Storage 总是欢迎新的贡献者。
我们需要贡献者来帮助构建新功能、修复错误、进行代码审查、编写测试、监控测试网格的健康状况以及改进文档等。&lt;/p>
&lt;!--
**FSM**: Thank you so much for your time and insights into the workings of SIG Storage!
-->
&lt;p>&lt;strong>FSM&lt;/strong>：非常感谢你抽出宝贵时间让我们深入了解 SIG Storage！&lt;/p></description></item><item><title>Blog: 认识我们的贡献者 - 亚太地区（中国地区）</title><link>https://kubernetes.io/zh-cn/blog/2022/08/15/meet-our-contributors-china-ep-03/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/08/15/meet-our-contributors-china-ep-03/</guid><description>
&lt;!--
layout: blog
title: "Meet Our Contributors - APAC (China region)"
date: 2022-08-15
slug: meet-our-contributors-china-ep-03
canonicalUrl: https://www.kubernetes.dev/blog/2022/08/15/meet-our-contributors-chn-ep-03/
-->
&lt;!--
**Authors &amp; Interviewers:** [Avinesh Tripathi](https://github.com/AvineshTripathi), [Debabrata Panigrahi](https://github.com/Debanitrkl), [Jayesh Srivastava](https://github.com/jayesh-srivastava), [Priyanka Saggu](https://github.com/Priyankasaggu11929/), [Purneswar Prasad](https://github.com/PurneswarPrasad), [Vedant Kakde](https://github.com/vedant-kakde)
-->
&lt;p>&lt;strong>作者和受访者：&lt;/strong> &lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>、
&lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>、
&lt;a href="https://github.com/jayesh-srivastava">Jayesh Srivastava&lt;/a>、
&lt;a href="https://github.com/Priyankasaggu11929/">Priyanka Saggu&lt;/a>、
&lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>、
&lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;hr>
&lt;!--
Hello, everyone 👋
Welcome back to the third edition of the "Meet Our Contributors" blog post series for APAC.
This post features four outstanding contributors from China, who have played diverse leadership and community roles in the upstream Kubernetes project.
So, without further ado, let's get straight to the article.
-->
&lt;p>大家好 👋&lt;/p>
&lt;p>欢迎来到亚太地区的 “认识我们的贡献者” 博文系列第三期。&lt;/p>
&lt;p>这篇博文介绍了四名来自中国的优秀贡献者，他们在上游 Kubernetes 项目中扮演了不同的领导角色和社区角色。&lt;/p>
&lt;p>闲话少说，让我们直接进入正文。&lt;/p>
&lt;h2 id="andy-zhang-https-github-com-andyzhangx">&lt;a href="https://github.com/andyzhangx">Andy Zhang&lt;/a>&lt;/h2>
&lt;!--
Andy Zhang currently works for Microsoft China at the Shanghai site. His main focus is on Kubernetes storage drivers. Andy started contributing to Kubernetes about 5 years ago.
He states that as he is working in Azure Kubernetes Service team and spends most of his time contributing to the Kubernetes community project. Now he is the main contributor of quite a lot Kubernetes subprojects such as Kubernetes cloud provider code.
-->
&lt;p>Andy Zhang 目前就职于微软中国上海办事处，他主要关注 Kubernetes 存储驱动。
Andy 大约在 5 年前开始为 Kubernetes 做贡献。&lt;/p>
&lt;p>他说由于自己为 Azure Kubernetes Service 团队工作，所以大部分时间都在为 Kubernetes 社区项目做贡献。
现在他是很多 Kubernetes 子项目的主要贡献者，例如 Kubernetes cloud-provider 仓库的代码。&lt;/p>
&lt;!--
His open source contributions are mainly self-motivated. In the last two years he has mentored a few students contributing to Kubernetes through the LFX Mentorship program, some of whom got jobs due to their expertise and contributions on Kubernetes projects.
Andy is an active member of the China Kubernetes community. He adds that the Kubernetes community has a good guide about how to become members, code reviewers, approvers and finally when he found out that some open source projects are in the very early stage, he actively contributed to those projects and became the project maintainer.
-->
&lt;p>他的开源贡献主要是出于自我激励。在过去的两年里，他通过 LFX Mentorship 计划指导了一些学生为 Kubernetes 做贡献，
其中一些学生凭借 Kubernetes 项目积累的专业知识和贡献经历而找到了工作。&lt;/p>
&lt;p>Andy 是 Kubernetes 中国社区的活跃成员。
他补充说，Kubernetes 社区对如何成为 Member、Code Reviewer、Approver 有完善的指导说明，
后来他发现一些开源项目还处于非常早期的阶段，他积极地为这些项目做了贡献并成为了项目维护者。&lt;/p>
&lt;h2 id="shiming-zhang-https-github-com-wzshiming">&lt;a href="https://github.com/wzshiming">Shiming Zhang&lt;/a>&lt;/h2>
&lt;!--
Shiming Zhang is a Software Engineer working on Kubernetes for DaoCloud in Shanghai, China.
He has mostly been involved with SIG Node as a reviewer. His major contributions have mainly been bug fixes and feature improvements in an ongoing [KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2712-pod-priority-based-graceful-node-shutdown), all revolving around SIG Node.
Some of his major PRs are [fixing watchForLockfileContention memory leak](https://github.com/kubernetes/kubernetes/pull/100326), [fixing startupProbe behaviour](https://github.com/kubernetes/kubernetes/pull/101093), [adding Field status.hostIPs for Pod](https://github.com/kubernetes/enhancements/pull/2661).
-->
&lt;p>Shiming Zhang 是一名软件工程师，供职于中国上海道客网络科技。&lt;/p>
&lt;p>他主要以 Reviewer 的身份参与 SIG Node。他的主要贡献集中在当下的
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2712-pod-priority-based-graceful-node-shutdown">KEP&lt;/a>
的漏洞修复和功能优化，这些工作全部围绕 SIG Node 展开。&lt;/p>
&lt;p>他发起的一些主要 PR 有
&lt;a href="https://github.com/kubernetes/kubernetes/pull/100326">fixing watchForLockfileContention memory leak&lt;/a>、
&lt;a href="https://github.com/kubernetes/kubernetes/pull/101093">fixing startupProbe behaviour&lt;/a>、
&lt;a href="https://github.com/kubernetes/enhancements/pull/2661">adding Field status.hostIPs for Pod&lt;/a>。&lt;/p>
&lt;h2 id="paco-xu-https-github-com-pacoxu">&lt;a href="https://github.com/pacoxu">Paco Xu&lt;/a>&lt;/h2>
&lt;!--
Paco Xu works at DaoCloud, a Shanghai-based cloud-native firm. He works with the infra and the open source team, focusing on enterprise cloud native platforms based on Kubernetes.
He started with Kubernetes in early 2017 and his first contribution was in March 2018. He started with a bug that he found, but his solution was not that graceful, hence wasn't accepted. He then started with some good first issues, which helped him to a great extent. In addition to this, from 2016 to 2017, he made some minor contributions to Docker.
-->
&lt;p>Paco Xu 就职于上海的一家云原生公司：道客网络科技。
他与基础设施和开源团队合作，专注于基于 Kubernetes 的企业云原生平台。&lt;/p>
&lt;p>他在 2017 年初开始使用 Kubernetes，他的第一个贡献是在 2018 年 3 月。
他的贡献始于自己发现的一个漏洞，但当时他的解决方案不是那么优雅，因此没有被接受。
然后他从一些 Good First Issue 开始做贡献，这在很大程度上帮助了他。
除此之外，在 2016 到 2017 年间，他还对 Docker 做出了一些小贡献。&lt;/p>
&lt;!--
Currently, Paco is a reviewer for `kubeadm` (a SIG Cluster Lifecycle product), and for SIG Node.
Paco says that you should contribute to open source projects you use. For him, an open source project is like a book to learn, getting inspired through discussions with the project maintainers.
> In my opinion, the best way for me is learning how owners work on the project.
-->
&lt;p>目前，Paco 是 &lt;code>kubeadm&lt;/code>（一个 SIG Cluster Lifecycle 产品）和 SIG Node 的 Reviewer。&lt;/p>
&lt;p>Paco 说大家应该为自己使用的开源项目做贡献。
对他来说，开源项目就像一本要学习的书，通过与项目维护者们讨论可以获得启发。&lt;/p>
&lt;blockquote>
&lt;p>在我看来，对我来说最好的方式是学习项目所有者如何处理项目。&lt;/p>
&lt;/blockquote>
&lt;h2 id="jintao-zhang-https-github-com-tao12345666333">&lt;a href="https://github.com/tao12345666333">Jintao Zhang&lt;/a>&lt;/h2>
&lt;!--
Jintao Zhang is presently employed at API7, where he focuses on ingress and service mesh.
In 2017, he encountered an issue which led to a community discussion and his contributions to Kubernetes started. Before contributing to Kubernetes, Jintao was a long-time contributor to Docker-related open source projects.
-->
&lt;p>Jintao Zhang 目前受聘于 API7，他专注于 Ingress 和服务网格。&lt;/p>
&lt;p>2017 年，他遇到了一个引发社区讨论的问题，并开始了对 Kubernetes 做贡献。
在为 Kubernetes 做贡献之前，Jintao 是 Docker 相关开源项目的长期贡献者。&lt;/p>
&lt;!--
Currently Jintao is a reviewer for the [ingress-nginx](https://kubernetes.github.io/ingress-nginx/) project.
He suggests keeping track of job opportunities at open source companies so that you can find one that allows you to contribute full time. For new contributors Jintao says that if anyone wants to make a significant contribution to an open source project, then they should choose the project based on their interests and should generously invest time.
-->
&lt;p>目前 Jintao 是 &lt;a href="https://kubernetes.github.io/ingress-nginx/">ingress-nginx&lt;/a> 项目的 Reviewer。&lt;/p>
&lt;p>他建议关注开源公司的工作机会，这样你就可以找到一个可以让你全职贡献的机会。
对于新的贡献者们，Jintao 表示如果有人想为一个开源项目做重大贡献，
那么应该根据自己的兴趣选择项目，然后应该慷慨地投入时间。&lt;/p>
&lt;hr>
&lt;!--
If you have any recommendations/suggestions for who we should interview next, please let us know in the [#sig-contribex channel](https://kubernetes.slack.com/archives/C1TU9EB9S) channel on the Kubernetes Slack. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.
We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋
-->
&lt;p>如果你对我们下一步应该采访谁有任何想法/建议，请在
&lt;a href="https://kubernetes.slack.com/archives/C1TU9EB9S">#sig-contribex 频道&lt;/a>中告知我们。
我们很高兴有其他人帮助我们接触社区中更优秀的人。我们将不胜感激。&lt;/p>
&lt;p>我们下期见。最后，祝大家都能快乐地为社区做贡献！👋&lt;/p></description></item><item><title>Blog: 逐个 KEP 地增强 Kubernetes</title><link>https://kubernetes.io/zh-cn/blog/2022/08/11/enhancing-kubernetes-one-kep-at-a-time/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/08/11/enhancing-kubernetes-one-kep-at-a-time/</guid><description>
&lt;!--
layout: blog
title: "Enhancing Kubernetes one KEP at a Time"
date: 2022-08-11
slug: enhancing-kubernetes-one-kep-at-a-time
canonicalUrl: https://www.k8s.dev/blog/2022/08/11/enhancing-kubernetes-one-kep-at-a-time/
-->
&lt;!--
**Author:** Ryler Hockenbury (Mastercard)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Ryler Hockenbury（Mastercard）&lt;/p>
&lt;!--
Did you know that Kubernetes v1.24 has [46 enhancements](https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/)? That's a lot of new functionality packed into a 4-month release cycle. The Kubernetes release team coordinates the logistics of the release, from remediating test flakes to publishing updated docs. It's a ton of work, but they always deliver.
The release team comprises around 30 people across six subteams - Bug Triage, CI Signal, Enhancements, Release Notes, Communications, and Docs.  Each of these subteams manages a component of the release. This post will focus on the role of the enhancements subteam and how you can get involved.
-->
&lt;p>你是否知道 Kubernetes v1.24 有
&lt;a href="https://kubernetes.io/zh-cn/blog/2022/05/03/kubernetes-1-24-release-announcement/">46 个增强特性&lt;/a>？
在为期 4 个月的发布周期内包含了大量新特性。
Kubernetes 发布团队协调发布的后勤工作，从修复测试问题到发布更新的文档。他们需要完成成吨的工作，但发布团队总是能按期交付。&lt;/p>
&lt;p>发布团队由大约 30 人组成，分布在六个子团队：Bug Triage、CI Signal、Enhancements、Release Notes、Communications 和 Docs。
每个子团队负责管理发布的一个组件。这篇博文将重点介绍增强子团队的角色以及你如何能够参与其中。&lt;/p>
&lt;!--
## What's the enhancements subteam?
Great question. We'll get to that in a second but first, let's talk about how features are managed in Kubernetes.
Each new feature requires a [Kubernetes Enhancement Proposal](https://github.com/kubernetes/enhancements/blob/master/keps/README.md) - KEP for short. KEPs are small structured design documents that provide a way to propose and coordinate new features. The KEP author describes the motivation, design (and alternatives), risks, and tests - then community members provide feedback to build consensus.
-->
&lt;h2 id="增强子团队是什么">增强子团队是什么？&lt;/h2>
&lt;p>好问题。我们稍后会讨论这个问题，但首先让我们谈谈 Kubernetes 中是如何管理功能特性的。&lt;/p>
&lt;p>每个新特性都需要一个 &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/README.md">Kubernetes 增强提案&lt;/a>，
简称为 KEP。KEP 是一些小型结构化设计文档，提供了一种提出和协调新特性的方法。
KEP 作者描述其提案动机、设计理念（和替代方案）、风险和测试，然后社区成员会提供反馈以达成共识。&lt;/p>
&lt;!--
KEPs are submitted and updated through a pull request (PR) workflow on the [k/enhancements repo](https://github.com/kubernetes/enhancements). Features start in alpha and move through a graduation process to beta and stable as they mature. For example, here's a cool KEP about [privileged container support on Windows Server](https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/1981-windows-privileged-container-support/kep.yaml).  It was introduced as alpha in Kubernetes v1.22 and graduated to beta in v1.23.
Now getting back to the question - the enhancements subteam coordinates the lifecycle tracking of the KEPs for each release. Each KEP is required to meet a set of requirements to be cleared for inclusion in a release. The enhancements subteam verifies each requirement for each KEP and tracks the status.
-->
&lt;p>你可以通过 &lt;a href="https://github.com/kubernetes/enhancements">Kubernetes/enhancements 仓库&lt;/a>的拉取请求（PR）工作流来提交和更新 KEP。
每个功能特性始于 Alpha 阶段，随着不断成熟，经由毕业流程进入 Beta 和 Stable 阶段。
这里有一个很酷的 KEP 例子，是关于 &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/1981-windows-privileged-container-support/kep.yaml">Windows Server 上的特权容器支持&lt;/a>。
这个 KEP 在 Kubernetes v1.22 中作为 Alpha 引入，并在 v1.23 中进入 Beta 阶段。&lt;/p>
&lt;p>现在回到上一个问题：增强子团队如何协调每个版本的 KEP 生命周期跟踪。
每个 KEP 都必须满足一组清晰具体的要求，才能被纳入一个发布版本中。
增强子团队负责验证每个 KEP 的要求并跟踪其状态。&lt;/p>
&lt;!--
At the start of a release, [Kubernetes Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) submit their enhancements to opt into a release. A typical release might have from 60 to 90 enhancements at the beginning.  During the release, many enhancements will drop out. Some do not quite meet the KEP requirements, and others do not complete their implementation in code. About 60%-70% of the opted-in KEPs will make it into the final release.
-->
&lt;p>在一个发行版本启动时，各个 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Kubernetes 特别兴趣小组&lt;/a> (SIG)
会提交各自的增强特性以进入某版本发布。通常一个版本最初可能有 60 到 90 个增强特性。随后，许多增强特性会被过滤掉。
这是因为有些不完全符合 KEP 要求，而另一些还未完成代码的实现。最初选择加入的 KEP 中大约有 60% - 70% 将进入最终发布。&lt;/p>
&lt;!--
## What does the enhancements subteam do?
Another great question, keep them coming! The enhancements team is involved in two crucial milestones during each release: enhancements freeze and code freeze.
-->
&lt;h2 id="增强子团队做什么">增强子团队做什么？&lt;/h2>
&lt;p>这是另一个很好的问题，切中了要点！增强特性的团队在每个版本中会涉及两个重要的里程碑：增强特性冻结和代码冻结。&lt;/p>
&lt;!--
#### Enhancements Freeze
Enhancements freeze is the deadline for a KEP to be complete in order for the enhancement to be included in a release. It's a quality gate to enforce alignment around maintaining and updating KEPs. The most notable requirements are a (1) [production readiness review ](https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md)(PRR) and a (2) [KEP file](https://github.com/kubernetes/enhancements/tree/master/keps/NNNN-kep-template) with a complete test plan and graduation criteria.
-->
&lt;h4 id="增强特性冻结">增强特性冻结&lt;/h4>
&lt;p>增强特性冻结是一个 KEP 按序完成增强特性并纳入一个发布版本的最后期限。
这是一个质量门控，用于强制对齐与 KEP 维护和更新相关的事项。
最值得注意的要求是
(1) &lt;a href="https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md">生产就绪审查&lt;/a>(PRR)
和 (2) 附带完整测试计划和毕业标准的 &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/NNNN-kep-template">KEP 文件&lt;/a>。&lt;/p>
&lt;!--
The enhancements subteam communicates to each KEP author through comments on the KEP issue on Github. As a first step, they'll verify the status and check if it meets the requirements.  The KEP gets marked as tracked after satisfying the requirements; otherwise, it's considered at risk. If a KEP is still at risk when enhancement freeze is in effect, the KEP is removed from the release.
This part of the cycle is typically the busiest for the enhancements subteam because of the large number of KEPs to groom, and each KEP might need to be visited multiple times to verify whether it meets requirements.
-->
&lt;p>增强子团队通过在 Github 上对 KEP 问题发表评论与每位 KEP 作者进行沟通。
作为第一步，子团队成员将检查 KEP 状态并确认其是否符合要求。
KEP 在满足要求后被标记为已被跟踪（Tracked）；否则，它会被认为有风险。
如果在增强特性冻结生效时 KEP 仍然存在风险，该 KEP 将被从发布版本中移除。&lt;/p>
&lt;p>在发布周期的这个阶段，增强子团队通常是最繁忙的，因为他们要梳理大量的 KEP，可能需要反复审查每个 KEP 才能验证某个 KEP 是否满足要求。&lt;/p>
&lt;!--
#### Code Freeze
Code freeze is the implementation deadline for all enhancements. The code must be implemented, reviewed, and merged by this point if a code change or update is needed for the enhancement. The latter third of the release is focused on stabilizing the codebase - fixing flaky tests, resolving various regressions, and preparing docs - and all the code needs to be in place before those steps can happen.
The enhancements subteam verifies that all PRs for an enhancement are merged into the [Kubernetes codebase](https://github.com/kubernetes/kubernetes) (k/k). During this period, the subteam reaches out to KEP authors to understand what PRs are part of the KEP, verifies that those PRs get merged, and then updates the status of the KEP. The enhancement is removed from the release if the code isn't all merged before the code freeze deadline.
-->
&lt;h4 id="代码冻结">代码冻结&lt;/h4>
&lt;p>代码冻结是从代码上实现所有增强特性的最后期限。
如果某增强特性的代码需要更改或更新，则必须在这个时间节点完成所有代码实现、代码审查和代码合并工作。
版本发布的最后三个工作专注于稳定代码库：修复测试问题，解决各种回归并准备文档。而在此之前，所有代码必须就位。&lt;/p>
&lt;p>增强子团队将验证某增强特性相关的所有 PR 均已合并到 &lt;a href="https://github.com/kubernetes/kubernetes">Kubernetes 代码库&lt;/a> (k/k)。
在此期间，子团队将联系 KEP 作者以了解哪些 PR 是 KEP 的一部分，检查这些 PR 是否已合并，然后更新 KEP 的状态。
如果在代码冻结的最后期限之前这些代码还未全部合并，该增强特性将从发布版本中移除。&lt;/p>
&lt;!--
## How can I get involved with the release team?
I'm glad you asked. The most direct way is to apply to be a [release team shadow](https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md). The shadow role is a hands-on apprenticeship intended to prepare individuals for leadership positions on the release team. Many shadow roles are non-technical and do not require prior contributions to the Kubernetes codebase.
With 3 Kubernetes releases every year and roughly 25 shadows per release, the release team is always in need of individuals wanting to contribute. Before each release cycle, the release team opens the application for the shadow program. When the application goes live, it's posted in the [Kubernetes Dev Mailing List](https://groups.google.com/a/kubernetes.io/g/dev).  You can subscribe to notifications from that list (or check it regularly!) to watch when the application opens. The announcement will typically go out in mid-April, mid-July, and mid-December - or roughly a month before the start of each release.
-->
&lt;h2 id="我如何才能参与发布团队">我如何才能参与发布团队？&lt;/h2>
&lt;p>很高兴你提出这个问题。
最直接的方式就是申请成为一名&lt;a href="https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md">发布团队影子&lt;/a>。
影子角色是一个见习职位，旨在帮助个人在发布团队中担任领导职位做好准备。许多影子角色是非技术性的，且不需要事先对 Kubernetes 代码库做出贡献。&lt;/p>
&lt;p>Kubernetes 每年发布 3 个版本，每个版本大约有 25 个影子，发布团队总是需要愿意做出贡献的人。
在每个发布周期之前，发布团队都会为影子计划打开申请渠道。当申请渠道上线时，
会公布在 &lt;a href="https://groups.google.com/a/kubernetes.io/g/dev">Kubernetes 开发邮件清单&lt;/a>中。
你可以订阅该列表中的通知（或定期查看！），以了解申请渠道何时开通。该公告通常会在 4 月中旬、7 月中旬和 12 月中旬发布，
或者在每个版本开始前大约一个月时发布。&lt;/p>
&lt;!--
## How can I find out more?
Check out the [role handbooks](https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks) if you're curious about the specifics of all the Kubernetes release subteams. The handbooks capture the logistics of each subteam, including a week-by-week breakdown of the subteam activities.  It's an excellent reference for getting to know each team better.
You can also check out the release-related Kubernetes slack channels - particularly #release, #sig-release, and #sig-arch. These channels have discussions and updates surrounding many aspects of the release.
-->
&lt;h2 id="我怎样才能找到更多信息">我怎样才能找到更多信息？&lt;/h2>
&lt;p>如果你对所有 Kubernetes 发布子团队的详情感到好奇，
请查阅&lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks">角色手册&lt;/a>。
这些手册记录了每个子团队的后勤工作，包括每周对子团队活动的细分任务。这是更好地了解每个团队的绝佳参考。&lt;/p>
&lt;p>你还可以查看与发布相关的 Kubernetes slack 频道，特别是 #release、#sig-release 和 #sig-arch。
这些频道围绕发布的许多方面进行了讨论和更新。&lt;/p></description></item><item><title>Blog: Kubernetes 1.25 的移除说明和主要变更</title><link>https://kubernetes.io/zh-cn/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes Removals and Major Changes In 1.25"
date: 2022-08-04
slug: upcoming-changes-in-kubernetes-1-25
-->
&lt;!--
**Authors**: Kat Cosgrove, Frederico Muñoz, Debabrata Panigrahi
-->
&lt;p>&lt;strong>作者&lt;/strong>：Kat Cosgrove、Frederico Muñoz、Debabrata Panigrahi&lt;/p>
&lt;!--
As Kubernetes grows and matures, features may be deprecated, removed, or replaced with improvements
for the health of the project. Kubernetes v1.25 includes several major changes and one major removal.
-->
&lt;p>随着 Kubernetes 成长和日趋成熟，为了此项目的健康发展，某些功能特性可能会被弃用、移除或替换为优化过的功能特性。
Kubernetes v1.25 包括几个主要变更和一个主要移除。&lt;/p>
&lt;!--
## The Kubernetes API Removal and Deprecation process
The Kubernetes project has a well-documented [deprecation policy](/docs/reference/using-api/deprecation-policy/) for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API is one that has been marked for removal in a future Kubernetes release; it will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.
-->
&lt;h2 id="the-kubernetes-api-removal-and-deprecation-process">Kubernetes API 移除和弃用流程&lt;/h2>
&lt;p>Kubernetes 项目对功能特性有一个文档完备的&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-policy/">弃用策略&lt;/a>。
该策略规定，只有当较新的、稳定的相同 API 可用时，原有的稳定 API 才可能被弃用，每个稳定级别的 API 都有一个最短的生命周期。
弃用的 API 指的是已标记为将在后续发行某个 Kubernetes 版本时移除的 API；
移除之前该 API 将继续发挥作用（从弃用起至少一年时间），但使用时会显示一条警告。
移除的 API 将在当前版本中不再可用，此时你必须迁移以使用替换的 API。&lt;/p>
&lt;!--
* Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.
* Beta or pre-release API versions must be supported for 3 releases after deprecation.
* Alpha or experimental API versions may be removed in any release without prior deprecation notice.
-->
&lt;ul>
&lt;li>正式发布（GA）或稳定的 API 版本可能被标记为已弃用，但只有在 Kubernetes 大版本更新时才会移除。&lt;/li>
&lt;li>测试版（Beta）或预发布 API 版本在弃用后必须支持 3 个版本。&lt;/li>
&lt;li>Alpha 或实验性 API 版本可能会在任何版本中被移除，恕不另行通知。&lt;/li>
&lt;/ul>
&lt;!--
Whether an API is removed as a result of a feature graduating from beta to stable or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the documentation.
-->
&lt;p>无论一个 API 是因为某功能特性从 Beta 进入稳定阶段而被移除，还是因为该 API 根本没有成功，
所有移除均遵从上述弃用策略。无论何时移除一个 API，文档中都会列出迁移选项。&lt;/p>
&lt;!--
## A Note About PodSecurityPolicy
In Kubernetes v1.25, we will be removing PodSecurityPolicy [after its deprecation in v1.21](/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/). PodSecurityPolicy has served us honorably, but its complex and often confusing usage necessitated changes, which unfortunately would have been breaking changes. To address this, it is being removed in favor of a replacement, Pod Security Admission, which is graduating to stable in this release as well. If you are currently relying on PodSecurityPolicy, follow the instructions for [migration to Pod Security Admission](/docs/tasks/configure-pod-container/migrate-from-psp/).
-->
&lt;h2 id="a-note-about-podsecuritypolicy">有关 PodSecurityPolicy 的说明&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">继 PodSecurityPolicy 在 v1.21 弃用后&lt;/a>，
Kubernetes v1.25 将移除 PodSecurityPolicy。PodSecurityPolicy 曾光荣地为我们服务，
但由于其复杂和经常令人困惑的使用方式，让大家觉得有必要进行修改，但很遗憾这种修改将会是破坏性的。
为此我们移除了 PodSecurityPolicy，取而代之的是 Pod Security Admission（即 PodSecurity 安全准入控制器），
后者在本次发行中也进入了稳定阶段。
如果你目前正依赖 PodSecurityPolicy，请遵循指示说明&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/migrate-from-psp/">迁移到 PodSecurity 准入控制器&lt;/a>。&lt;/p>
&lt;!--
## Major Changes for Kubernetes v1.25
Kubernetes v1.25 will include several major changes, in addition to the removal of PodSecurityPolicy.
### [CSI Migration](https://github.com/kubernetes/enhancements/issues/625)
The effort to move the in-tree volume plugins to out-of-tree CSI drivers continues, with the core CSI Migration feature going GA in v1.25. This is an important step towards removing the in-tree volume plugins entirely.
-->
&lt;h2 id="major-changes-for-kubernetes-v1.25">Kubernetes v1.25 的主要变更&lt;/h2>
&lt;p>Kubernetes v1.25 除了移除 PodSecurityPolicy 之外，还将包括以下几个主要变更。&lt;/p>
&lt;h3 id="csi-migration-https-github-com-kubernetes-enhancements-issues-625">&lt;a href="https://github.com/kubernetes/enhancements/issues/625">CSI Migration&lt;/a>&lt;/h3>
&lt;p>将树内卷插件迁移到树外 CSI 驱动的努力还在继续，核心的 CSI Migration 特性在 v1.25 进入 GA 阶段。
对于全面移除树内卷插件而言，这是重要的一步。&lt;/p>
&lt;!--
### Deprecations and removals for storage drivers
Several volume plugins are being deprecated or removed.
[GlusterFS will be deprecated in v1.25](https://github.com/kubernetes/enhancements/issues/3446). While a CSI driver was built for it, it has not been maintained. The possibility of migration to a compatible CSI driver [was discussed](https://github.com/kubernetes/kubernetes/issues/100897), but a decision was ultimately made to begin the deprecation of the GlusterFS plugin from in-tree drivers. The [Portworx in-tree volume plugin](https://github.com/kubernetes/enhancements/issues/2589) is also being deprecated with this release. The Flocker, Quobyte, and StorageOS in-tree volume plugins are being removed.
-->
&lt;h3 id="deprecations-and-removals-for-storage-drivers">存储驱动的弃用和移除&lt;/h3>
&lt;p>若干卷插件将被弃用或移除。&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/3446">GlusterFS 将在 v1.25&lt;/a> 中被弃用。
虽然为其构建了 CSI 驱动，但未曾得到维护。
社区&lt;a href="https://github.com/kubernetes/kubernetes/issues/100897">曾讨论&lt;/a>迁移到一个兼容 CSI 驱动的可能性，
但最终决定开始从树内驱动中弃用 GlusterFS 插件。
本次发行还会弃用 &lt;a href="https://github.com/kubernetes/enhancements/issues/2589">Portworx 树内卷插件&lt;/a>。
Flocker、Quobyte 和 StorageOS 树内卷插件将被移除。&lt;/p>
&lt;!--
[Flocker](https://github.com/kubernetes/kubernetes/pull/111618), [Quobyte](https://github.com/kubernetes/kubernetes/pull/111619), and [StorageOS](https://github.com/kubernetes/kubernetes/pull/111620) in-tree volume plugins will be removed in v1.25 as part of the [CSI Migration](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration).
-->
&lt;p>&lt;a href="https://github.com/kubernetes/kubernetes/pull/111618">Flocker&lt;/a>、
&lt;a href="https://github.com/kubernetes/kubernetes/pull/111619">Quobyte&lt;/a> 和
&lt;a href="https://github.com/kubernetes/kubernetes/pull/111620">StorageOS&lt;/a> 树内卷插件将作为
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">CSI Migration&lt;/a>
的一部分在 v1.25 中移除。&lt;/p>
&lt;!--
### [Change to vSphere version support](https://github.com/kubernetes/kubernetes/pull/111255)
From Kubernetes v1.25, the in-tree vSphere volume driver will not support any vSphere release before 7.0u2. Check the v1.25 detailed release notes for more advice on how to handle this.
-->
&lt;h3 id="对-vsphere-版本支持的变更-https-github-com-kubernetes-kubernetes-pull-111255">&lt;a href="https://github.com/kubernetes/kubernetes/pull/111255">对 vSphere 版本支持的变更&lt;/a>&lt;/h3>
&lt;p>从 Kubernetes v1.25 开始，树内 vSphere 卷驱动将不支持任何早于 7.0u2 的 vSphere 版本。
查阅 v1.25 详细发行说明，了解如何处理这种状况的更多建议。&lt;/p>
&lt;!--
### [Cleaning up IPTables Chain Ownership](https://github.com/kubernetes/enhancements/issues/3178)
On Linux, Kubernetes (usually) creates iptables chains to ensure that network packets reach
Although these chains and their names have been an internal implementation detail, some tooling
has relied upon that behavior.
will only support for internal Kubernetes use cases. Starting with v1.25, the Kubelet will gradually move towards not creating the following iptables chains in the `nat` table:
-->
&lt;h3 id="清理-iptables-链的所有权-https-github-com-kubernetes-enhancements-issues-3178">&lt;a href="https://github.com/kubernetes/enhancements/issues/3178">清理 IPTables 链的所有权&lt;/a>&lt;/h3>
&lt;p>在 Linux 上，Kubernetes（通常）创建 iptables 链来确保这些网络数据包到达，
尽管这些链及其名称已成为内部实现的细节，但某些工具已依赖于此行为。
将仅支持内部 Kubernetes 使用场景。
从 v1.25 开始，Kubelet 将逐渐迁移为不在 &lt;code>nat&lt;/code> 表中创建以下 iptables 链：&lt;/p>
&lt;ul>
&lt;li>&lt;code>KUBE-MARK-DROP&lt;/code>&lt;/li>
&lt;li>&lt;code>KUBE-MARK-MASQ&lt;/code>&lt;/li>
&lt;li>&lt;code>KUBE-POSTROUTING&lt;/code>&lt;/li>
&lt;/ul>
&lt;!--
This change will be phased in via the `IPTablesCleanup` feature gate. Although this is not formally a deprecation, some end users have come to rely on specific internal behavior of `kube-proxy`. The Kubernetes project overall wants to make it clear that depending on these internal details is not supported, and that future implementations will change their behavior here.
-->
&lt;p>此项变更将通过 &lt;code>IPTablesCleanup&lt;/code> 特性门控分阶段完成。
尽管这不是正式的弃用，但某些最终用户已开始依赖 &lt;code>kube-proxy&lt;/code> 特定的内部行为。
Kubernetes 项目总体上希望明确表示不支持依赖这些内部细节，并且未来的实现将更改它们在此处的行为。&lt;/p>
&lt;!--
## Looking ahead
The official [list of API removals planned for Kubernetes 1.26](/docs/reference/using-api/deprecation-guide/#v1-26) is:
* The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)
* The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)
-->
&lt;h2 id="looking-ahead">展望未来&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-guide/#v1-26">Kubernetes 1.26 计划移除的 API 的正式列表&lt;/a>为：&lt;/p>
&lt;ul>
&lt;li>Beta 版 FlowSchema 和 PriorityLevelConfiguration API（flowcontrol.apiserver.k8s.io/v1beta1）&lt;/li>
&lt;li>Beta 版 HorizontalPodAutoscaler API（autoscaling/v2beta2）&lt;/li>
&lt;/ul>
&lt;!--
### Want to know more?
Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:
-->
&lt;h3 id="want-to-know-more">了解更多&lt;/h3>
&lt;p>Kubernetes 发行说明公布了弃用信息。你可以在以下版本的发行说明中查看待弃用特性的公告：&lt;/p>
&lt;!--
* [Kubernetes 1.21](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation)
* [Kubernetes 1.22](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation)
* [Kubernetes 1.23](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation)
* [Kubernetes 1.24](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation)
* We will formally announce the deprecations that come with [Kubernetes 1.25](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation) as part of the CHANGELOG for that release.
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24&lt;/a>&lt;/li>
&lt;li>我们将正式宣布 &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation">Kubernetes 1.25&lt;/a>
的弃用信息，作为该版本 CHANGELOG 的一部分。&lt;/li>
&lt;/ul>
&lt;!--
For information on the process of deprecation and removal, check out the official Kubernetes [deprecation policy](/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api) document.
-->
&lt;p>有关弃用和移除流程的信息，请查阅 Kubernetes
官方&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">弃用策略&lt;/a>文档。&lt;/p></description></item><item><title>Blog: 聚光灯下的 SIG Docs</title><link>https://kubernetes.io/zh-cn/blog/2022/08/02/sig-docs-spotlight-2022/</link><pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/08/02/sig-docs-spotlight-2022/</guid><description>
&lt;!--
layout: blog
title: "Spotlight on SIG Docs"
date: 2022-08-02
slug: sig-docs-spotlight-2022
canonicalUrl: https://kubernetes.dev/blog/2022/08/02/sig-docs-spotlight-2022/
-->
&lt;!--
**Author:** Purneswar Prasad
-->
&lt;p>&lt;strong>作者：&lt;/strong> Purneswar Prasad&lt;/p>
&lt;!--
## Introduction
The official documentation is the go-to source for any open source project. For Kubernetes,
it's an ever-evolving Special Interest Group (SIG) with people constantly putting in their efforts
to make details about the project easier to consume for new contributors and users. SIG Docs publishes
the official documentation on [kubernetes.io](https://kubernetes.io) which includes,
but is not limited to, documentation of the core APIs, core architectural details, and CLI tools
shipped with the Kubernetes release.
To learn more about the work of SIG Docs and its future ahead in shaping the community, I have summarised
my conversation with the co-chairs, [Divya Mohan](https://twitter.com/Divya_Mohan02) (DM),
[Rey Lejano](https://twitter.com/reylejano) (RL) and Natali Vlatko (NV), who ran through the
SIG's goals and how fellow contributors can help.
-->
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>官方文档是所有开源项目的首选资料源。对于 Kubernetes，它是一个持续演进的特别兴趣小组 (SIG)，
人们持续不断努力制作详实的项目资料，让新贡献者和用户更容易取用这些文档。
SIG Docs 在 &lt;a href="https://kubernetes.io">kubernetes.io&lt;/a> 上发布官方文档，
包括但不限于 Kubernetes 版本发布时附带的核心 API 文档、核心架构细节和 CLI 工具文档。&lt;/p>
&lt;p>为了了解 SIG Docs 的工作及其在塑造社区未来方面的更多信息，我总结了自己与联合主席
&lt;a href="https://twitter.com/Divya_Mohan02">Divya Mohan&lt;/a>（下称 DM）、
&lt;a href="https://twitter.com/reylejano">Rey Lejano&lt;/a>（下称 RL）和 Natali Vlatko（下称 NV）的谈话，
他们讲述了 SIG 的目标以及其他贡献者们如何从旁协助。&lt;/p>
&lt;!--
## A summary of the conversation
### Could you tell us a little bit about what SIG Docs does?
SIG Docs is the special interest group for documentation for the Kubernetes project on kubernetes.io,
generating reference guides for the Kubernetes API, kubeadm and kubectl as well as maintaining the official
website’s infrastructure and analytics. The remit of their work also extends to docs releases, translation of docs,
improvement and adding new features to existing documentation, pushing and reviewing content for the official
Kubernetes blog and engaging with the Release Team for each cycle to get docs and blogs reviewed.
-->
&lt;h2 id="谈话汇总">谈话汇总&lt;/h2>
&lt;h3 id="你能告诉我们-sig-docs-具体做什么吗">你能告诉我们 SIG Docs 具体做什么吗？&lt;/h3>
&lt;p>SIG Docs 是 kubernetes.io 上针对 Kubernetes 项目文档的特别兴趣小组，
为 Kubernetes API、kubeadm 和 kubectl 制作参考指南，并维护官方网站的基础设施和数据分析。
他们的工作范围还包括文档发布、文档翻译、改进并向现有文档添加新功能特性、推送和审查官方 Kubernetes 博客的内容，
并在每个发布周期与发布团队合作以审查文档和博客。&lt;/p>
&lt;!--
### There are 2 subprojects under Docs: blogs and localization. How has the community benefited from it and are there some interesting contributions by those teams you want to highlight?
**Blogs**: This subproject highlights new or graduated Kubernetes enhancements, community reports, SIG updates
or any relevant news to the Kubernetes community such as thought leadership, tutorials and project updates,
such as the Dockershim removal and removal of PodSecurityPolicy, which is upcoming in the 1.25 release.
Tim Bannister, one of the SIG Docs tech leads, does awesome work and is a major force when pushing contributions
through to the docs and blogs.
-->
&lt;h3 id="docs-下有-2-个子项目-博客和本地化-社区如何从中受益-你想强调的这些团队是否侧重于某些贡献">Docs 下有 2 个子项目：博客和本地化。社区如何从中受益？你想强调的这些团队是否侧重于某些贡献？&lt;/h3>
&lt;p>&lt;strong>博客&lt;/strong>：这个子项目侧重于介绍新的或毕业的 Kubernetes 增强特性、社区报告、SIG 更新或任何与 Kubernetes
社区相关的新闻，例如思潮引领、教程和项目更新，例如即将在 1.25 版本中移除 Dockershim 和 PodSecurityPolicy。
Tim Bannister 是 SIG Docs 技术负责人之一，他做得工作非常出色，是推动文档和博客贡献的主力人物。&lt;/p>
&lt;!--
**Localization**: With this subproject, the Kubernetes community has been able to achieve greater inclusivity
and diversity among both users and contributors. This has also helped the project gain more contributors,
especially students, since a couple of years ago.
One of the major highlights and up-and-coming localizations are Hindi and Bengali. The efforts for Hindi
localization are currently being spearheaded by students in India.
In addition to that, there are two other subprojects: [reference-docs](https://github.com/kubernetes-sigs/reference-docs) and the [website](https://github.com/kubernetes/website), which is built with Hugo and is an important ownership area.
-->
&lt;p>&lt;strong>本地化&lt;/strong>：通过这个子项目，Kubernetes 社区能够在用户和贡献者之间实现更大的包容性和多样性。
自几年前以来，这也帮助该项目获得了更多的贡献者，尤其是学生们。
主要亮点之一是即将到来的本地化版本：印地语和孟加拉语。印地语的本地化工作目前由印度的学生们牵头。&lt;/p>
&lt;p>除此之外，还有另外两个子项目：&lt;a href="https://github.com/kubernetes-sigs/reference-docs">reference-docs&lt;/a> 和
&lt;a href="https://github.com/kubernetes/website">website&lt;/a>，后者采用 Hugo 构建，是 Kubernetes 拥有的一个重要阵地。&lt;/p>
&lt;!--
### Recently there has been a lot of buzz around the Kubernetes ecosystem as well as the industry regarding the removal of dockershim in the latest 1.24 release. How has SIG Docs helped the project to ensure a smooth change among the end-users? {#dockershim-removal}
-->
&lt;h3 id="dockershim-removal">最近有很多关于 Kubernetes 生态系统以及业界对最新 1.24 版本中移除 Dockershim 的讨论。SIG Docs 如何帮助该项目确保最终用户们平滑变更？&lt;/h3>
&lt;!--
Documenting the removal of Dockershim was a mammoth task, requiring the revamping of existing documentation
and communicating to the various stakeholders regarding the deprecation efforts. It needed a community effort,
so ahead of the 1.24 release, SIG Docs partnered with Docs and Comms verticals, the Release Lead from the
Release Team, and also the CNCF to help put the word out. Weekly meetings and a GitHub project board were
set up to track progress, review issues and approve PRs and keep the Kubernetes website updated. This has
also helped new contributors know about the depreciation, so that if any good-first-issue pops up, they could chip in.
A dedicated Slack channel was used to communicate meeting updates, invite feedback or to solicit help on
outstanding issues and PRs. The weekly meeting also continued for a month after the 1.24 release to review related issues and fix them.
A huge shoutout to [Celeste Horgan](https://twitter.com/celeste_horgan), who kept the ball rolling on this
conversation throughout the deprecation process.
-->
&lt;p>与 Dockershim 移除有关的文档工作是一项艰巨的任务，需要修改现有文档并就弃用工作与各种利益相关方进行沟通。
这需要社区的努力，因此在 1.24 版本发布之前，SIG Docs 与 Docs and Comms 垂直行业、来自发布团队的发布负责人以及
CNCF 建立合作关系，帮助在全网宣传。设立了每周例会和 GitHub 项目委员会，以跟踪进度、审查问题和批准 PR，
并保持更新 Kubernetes 网站。这也有助于新的贡献者们了解这次弃用，因此如果出现任何 good-first-issue，
新的贡献者也可以参与进来。开通了专用的 Slack 频道用于交流会议更新、邀请反馈或就悬而未决的问题和 PR 寻求帮助。
每周例会在 1.24 发布后也持续了一个月，以审查并修复相关问题。
非常感谢 &lt;a href="https://twitter.com/celeste_horgan">Celeste Horgan&lt;/a>，与他的顺畅交流贯穿了这个弃用过程的前前后后。&lt;/p>
&lt;!--
### Why should new and existing contributors consider joining this SIG?
Kubernetes is a vast project and can be intimidating at first for a lot of folks to find a place to start.
Any open source project is defined by its quality of documentation and SIG Docs aims to be a welcoming,
helpful place for new contributors to get onboard. One gets the perks of working with the project docs
as well as learning by reading it. They can also bring their own, new perspective to create and improve
the documentation. In the long run if they stick to SIG Docs, they can rise up the ladder to be maintainers.
This will help make a big project like Kubernetes easier to parse and navigate.
-->
&lt;h3 id="为什么新老贡献者都应该考虑加入这个-sig">为什么新老贡献者都应该考虑加入这个 SIG？&lt;/h3>
&lt;p>Kubernetes 是一个庞大的项目，起初可能会让很多人难以找到切入点。
任何开源项目的优劣总能从文档质量略窥一二，SIG Docs 的目标是建设一个欢迎新贡献者加入并对其有帮助的地方。
希望所有人可以轻松参与该项目的文档，并能从阅读中受益。他们还可以带来自己的新视角，以制作和改进文档。
从长远来看，如果他们坚持参与 SIG Docs，就可以拾阶而上晋升成为维护者。
这将有助于使 Kubernetes 这样的大型项目更易于解析和导航。&lt;/p>
&lt;!--
### How do you help new contributors get started? Are there any prerequisites to join?
There are no such prerequisites to get started with contributing to Docs. But there is certainly a fantastic
Contribution to Docs guide which is always kept as updated and relevant as possible and new contributors
are urged to read it and keep it handy. Also, there are a lot of useful pins and bookmarks in the
community Slack channel [#sig-docs](https://kubernetes.slack.com/archives/C1J0BPD2M). GitHub issues with
the good-first-issue labels in the kubernetes/website repo is a great place to create your first PR.
Now, SIG Docs has a monthly New Contributor Meet and Greet on the first Tuesday of the month with the
first occupant of the New Contributor Ambassador role, [Arsh Sharma](https://twitter.com/RinkiyaKeDad).
This has helped in making a more accessible point of contact within the SIG for new contributors.
-->
&lt;h3 id="你如何帮助新的贡献者入门-加入有什么前提条件吗">你如何帮助新的贡献者入门？加入有什么前提条件吗？&lt;/h3>
&lt;p>开始为 Docs 做贡献没有这样的前提条件。但肯定有一个很棒的对文档做贡献的指南，这个指南始终尽可能保持更新和贴合实际，
希望新手们多多阅读并将其放在趁手的地方。此外，社区 Slack 频道
&lt;a href="https://kubernetes.slack.com/archives/C1J0BPD2M">#sig-docs&lt;/a> 中有很多有用的便贴和书签。
kubernetes/website 仓库中带有 good-first-issue 标签的那些 GitHub 问题是创建你的第一个 PR 的好地方。
现在，SIG Docs 在每月的第一个星期二配合第一任 New Contributor Ambassador（新贡献者大使）角色
&lt;a href="https://twitter.com/RinkiyaKeDad">Arsh Sharma&lt;/a> 召开月度 New Contributor Meet and Greet（新贡献者见面会）。
这有助于在 SIG 内为新的贡献者建立一个更容易参与的联络形式。&lt;/p>
&lt;!--
### Any SIG related accomplishment that you’re really proud of?
**DM &amp; RL** : The formalization of the localization subproject in the last few months has been a big win
for SIG Docs, given all the great work put in by contributors from different countries. Earlier the
localization efforts didn’t have any streamlined process and focus was given to provide a structure by
drafting a KEP over the past couple of months for localization to be formalized as a subproject, which
is planned to be pushed through by the end of third quarter.
-->
&lt;h3 id="你是否有任何真正自豪的-sig-相关成绩">你是否有任何真正自豪的 SIG 相关成绩？&lt;/h3>
&lt;p>&lt;strong>DM &amp;amp; RL&lt;/strong> ：鉴于来自不同国家的贡献者们做出的所有出色工作，
过去几个月本地化子项目的正式推行对 SIG Docs 来说是一个巨大的胜利。
早些时候，本地化工作还没有任何流水线的流程，过去几个月的重点是通过起草一份 KEP 为本地化正式成为一个子项目提供一个框架，
这项工作计划在第三个季度结束时完成。&lt;/p>
&lt;!--
**DM** : Another area where there has been a lot of success is the New Contributor Ambassador role,
which has helped in making a more accessible point of contact for the onboarding of new contributors into the project.
**NV** : For each release cycle, SIG Docs have to review release docs and feature blogs highlighting
release updates within a short window. This is always a big effort for the docs and blogs reviewers.
-->
&lt;p>&lt;strong>DM&lt;/strong>：另一个取得很大成功的领域是 New Contributor Ambassador（新贡献者大使）角色，
这个角色有助于为新贡献者参与项目提供更便捷的联系形式。&lt;/p>
&lt;p>&lt;strong>NV&lt;/strong>：对于每个发布周期，SIG Docs 都必须在短时间内评审突出介绍发布更新的发布文档和功能特性博客。
这对于文档和博客审阅者来说，始终需要付出巨大的努力。&lt;/p>
&lt;!--
### Is there something exciting coming up for the future of SIG Docs that you want the community to know?
SIG Docs is now looking forward to establishing a roadmap, having a steady pipeline of folks being able
to push improvements to the documentation and streamlining community involvement in triaging issues and
reviewing PRs being filed. To build one such contributor and reviewership base, a mentorship program is
being set up to help current contributors become reviewers. This definitely is a space to watch out for more!
-->
&lt;h3 id="你是否有一些关于-sig-docs-未来令人兴奋的举措想让社区知道">你是否有一些关于 SIG Docs 未来令人兴奋的举措想让社区知道？&lt;/h3>
&lt;p>SIG Docs 现在期望设计一个路线图，建立稳定的人员流转机制以期推动对文档的改进，
简化社区参与 Issue 评判和已提交 PR 的评审工作。
为了建立一个这样由贡献者和 Reviewer 组成的群体，我们正在设立一项辅导计划帮助当前的贡献者们成为 Reviewer。
这绝对是一项值得关注的举措！&lt;/p>
&lt;!--
## Wrap Up
SIG Docs hosted a [deep dive talk](https://www.youtube.com/watch?v=GDfcBF5et3Q)
during on KubeCon + CloudNativeCon North America 2021, covering their awesome SIG.
They are very welcoming and have been the starting ground into Kubernetes
for a lot of new folks who want to contribute to the project.
Join the [SIG's meetings](https://github.com/kubernetes/community/blob/master/sig-docs/README.md) to find out
about the most recent research results, their plans for the forthcoming year, and how to get involved in the upstream Docs team as a contributor!
-->
&lt;h2 id="结束语">结束语&lt;/h2>
&lt;p>SIG Docs 在 KubeCon + CloudNativeCon North America 2021
期间举办了一次&lt;a href="https://www.youtube.com/watch?v=GDfcBF5et3Q">深度访谈&lt;/a>，涵盖了他们很棒的 SIG 主题。
他们非常欢迎想要为 Kubernetes 项目做贡献的新人，对这些新人而言 SIG Docs 已成为加入 Kubernetes 的起跳板。
欢迎加入 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-docs/README.md">SIG 会议&lt;/a>，
了解最新的研究成果、来年的计划以及如何作为贡献者参与上游 Docs 团队！&lt;/p></description></item><item><title>Blog: Kubernetes Gateway API 进入 Beta 阶段</title><link>https://kubernetes.io/zh-cn/blog/2022/07/13/gateway-api-graduates-to-beta/</link><pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/07/13/gateway-api-graduates-to-beta/</guid><description>
&lt;!--
layout: blog
title: Kubernetes Gateway API Graduates to Beta
date: 2022-07-13
slug: gateway-api-graduates-to-beta
canonicalUrl: https://gateway-api.sigs.k8s.io/blog/2022/graduating-to-beta/
-->
&lt;!--
**Authors:** Shane Utt (Kong), Rob Scott (Google), Nick Young (VMware), Jeff Apple (HashiCorp)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Shane Utt (Kong)、Rob Scott (Google)、Nick Young (VMware)、Jeff Apple (HashiCorp)&lt;/p>
&lt;!--
We are excited to announce the v0.5.0 release of Gateway API. For the first
time, several of our most important Gateway API resources are graduating to
beta. Additionally, we are starting a new initiative to explore how Gateway API
can be used for mesh and introducing new experimental concepts such as URL
rewrites. We'll cover all of this and more below.
-->
&lt;p>我们很高兴地宣布 Gateway API 的 v0.5.0 版本发布。
我们最重要的几个 Gateway API 资源首次进入 Beta 阶段。
此外，我们正在启动一项新的倡议，探索如何将 Gateway API 用于网格，还引入了 URL 重写等新的实验性概念。
下文涵盖了这部分内容和更多说明。&lt;/p>
&lt;!--
## What is Gateway API?
-->
&lt;h2 id="什么是-gateway-api">什么是 Gateway API？&lt;/h2>
&lt;!--
Gateway API is a collection of resources centered around [Gateway][gw] resources
(which represent the underlying network gateways / proxy servers) to enable
robust Kubernetes service networking through expressive, extensible and
role-oriented interfaces that are implemented by many vendors and have broad
industry support.
-->
&lt;p>Gateway API 是以 &lt;a href="https://gateway-api.sigs.k8s.io/api-types/gateway/">Gateway&lt;/a> 资源（代表底层网络网关/代理服务器）为中心的资源集合，
Kubernetes 服务网络的健壮性得益于众多供应商实现、得到广泛行业支持且极具表达力、可扩展和面向角色的各个接口。&lt;/p>
&lt;!--
Originally conceived as a successor to the well known [Ingress][ing] API, the
benefits of Gateway API include (but are not limited to) explicit support for
many commonly used networking protocols (e.g. `HTTP`, `TLS`, `TCP`, `UDP`) as
well as tightly integrated support for Transport Layer Security (TLS). The
`Gateway` resource in particular enables implementations to manage the lifecycle
of network gateways as a Kubernetes API.
-->
&lt;p>Gateway API 最初被认为是知名 &lt;a href="https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/service-resources/ingress-v1/">Ingress&lt;/a> API 的继任者，
Gateway API 的好处包括（但不限于）对许多常用网络协议的显式支持
（例如 &lt;code>HTTP&lt;/code>、&lt;code>TLS&lt;/code>、&lt;code>TCP &lt;/code>、&lt;code>UDP&lt;/code>) 以及对传输层安全 (TLS) 的紧密集成支持。
特别是 &lt;code>Gateway&lt;/code> 资源能够实现作为 Kubernetes API 来管理网络网关的生命周期。&lt;/p>
&lt;!--
If you're an end-user interested in some of the benefits of Gateway API we
invite you to jump in and find an implementation that suits you. At the time of
this release there are over a dozen [implementations][impl] for popular API
gateways and service meshes and guides are available to start exploring quickly.
-->
&lt;p>如果你是对 Gateway API 的某些优势感兴趣的终端用户，我们邀请你加入并找到适合你的实现方式。
值此版本发布之时，对于流行的 API 网关和服务网格有十多种&lt;a href="https://gateway-api.sigs.k8s.io/implementations/">实现&lt;/a>，还提供了操作指南便于快速开始探索。&lt;/p>
&lt;!--
[gw]:https://gateway-api.sigs.k8s.io/api-types/gateway/
[ing]:https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/
[impl]:https://gateway-api.sigs.k8s.io/implementations/
-->
&lt;!--
### Getting started
-->
&lt;h3 id="入门">入门&lt;/h3>
&lt;!--
Gateway API is an official Kubernetes API like
[Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/).
Gateway API represents a superset of Ingress functionality, enabling more
advanced concepts. Similar to Ingress, there is no default implementation of
Gateway API built into Kubernetes. Instead, there are many different
[implementations][impl] available, providing significant choice in terms of underlying
technologies while providing a consistent and portable experience.
-->
&lt;p>Gateway API 是一个类似 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/">Ingress&lt;/a>
的正式 Kubernetes API。Gateway API 代表了 Ingress 功能的一个父集，使得一些更高级的概念成为可能。
与 Ingress 类似，Kubernetes 中没有内置 Gateway API 的默认实现。
相反，有许多不同的&lt;a href="https://gateway-api.sigs.k8s.io/implementations/">实现&lt;/a>可用，在提供一致且可移植体验的同时，还在底层技术方面提供了重要的选择。&lt;/p>
&lt;!--
Take a look at the [API concepts documentation][concepts] and check out some of
the [Guides][guides] to start familiarizing yourself with the APIs and how they
work. When you're ready for a practical application open the [implementations
page][impl] and select an implementation that belongs to an existing technology
you may already be familiar with or the one your cluster provider uses as a
default (if applicable). Gateway API is a [Custom Resource Definition
(CRD)][crd] based API so you'll need to [install the CRDs][install-crds] onto a
cluster to use the API.
-->
&lt;p>查看 &lt;a href="https://gateway-api.sigs.k8s.io/concepts/api-overview/">API 概念文档&lt;/a> 并查阅一些&lt;a href="https://gateway-api.sigs.k8s.io/guides/getting-started/">指南&lt;/a>以开始熟悉这些 API 及其工作方式。
当你准备好一个实用的应用程序时，
请打开&lt;a href="https://gateway-api.sigs.k8s.io/implementations/">实现页面&lt;/a>并选择属于你可能已经熟悉的现有技术或集群提供商默认使用的技术（如果适用）的实现。
Gateway API 是一个基于 &lt;a href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">CRD&lt;/a> 的 API，因此你将需要&lt;a href="https://gateway-api.sigs.k8s.io/guides/getting-started/#install-the-crds">安装 CRD&lt;/a> 到集群上才能使用该 API。&lt;/p>
&lt;!--
If you're specifically interested in helping to contribute to Gateway API, we
would love to have you! Please feel free to [open a new issue][issue] on the
repository, or join in the [discussions][disc]. Also check out the [community
page][community] which includes links to the Slack channel and community meetings.
-->
&lt;p>如果你对 Gateway API 做贡献特别有兴趣，我们非常欢迎你的加入！
你可以随时在仓库上&lt;a href="https://github.com/kubernetes-sigs/gateway-api/issues/new/choose">提一个新的 issue&lt;/a>，或&lt;a href="https://github.com/kubernetes-sigs/gateway-api/discussions">加入讨论&lt;/a>。
另请查阅&lt;a href="https://gateway-api.sigs.k8s.io/contributing/community/">社区页面&lt;/a>以了解 Slack 频道和社区会议的链接。&lt;/p>
&lt;!--
[crd]:https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/
[concepts]:https://gateway-api.sigs.k8s.io/concepts/api-overview/
[guides]:https://gateway-api.sigs.k8s.io/guides/getting-started/
[impl]:https://gateway-api.sigs.k8s.io/implementations/
[install-crds]:https://gateway-api.sigs.k8s.io/guides/getting-started/#install-the-crds
[issue]:https://github.com/kubernetes-sigs/gateway-api/issues/new/choose
[disc]:https://github.com/kubernetes-sigs/gateway-api/discussions
[community]:https://gateway-api.sigs.k8s.io/contributing/community/
-->
&lt;!--
## Release highlights
### Graduation to beta
-->
&lt;h2 id="发布亮点">发布亮点&lt;/h2>
&lt;h3 id="进入-beta-阶段">进入 Beta 阶段&lt;/h3>
&lt;!--
The `v0.5.0` release is particularly historic because it marks the growth in
maturity to a beta API version (`v1beta1`) release for some of the key APIs:
-->
&lt;p>&lt;code>v0.5.0&lt;/code> 版本特别具有历史意义，因为它标志着一些关键 API 成长至 Beta API 版本（&lt;code>v1beta1&lt;/code>）：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/api-types/gatewayclass/">GatewayClass&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/api-types/gateway/">Gateway&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/api-types/httproute/">HTTPRoute&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
This achievement was marked by the completion of several graduation criteria:
- API has been [widely implemented][impl].
- Conformance tests provide basic coverage for all resources and have multiple implementations passing tests.
- Most of the API surface is actively being used.
- Kubernetes SIG Network API reviewers have approved graduation to beta.
-->
&lt;p>这一成就的标志是达到了以下几个进入标准：&lt;/p>
&lt;ul>
&lt;li>API 已&lt;a href="https://gateway-api.sigs.k8s.io/implementations/">广泛实现&lt;/a>。&lt;/li>
&lt;li>合规性测试基本覆盖了所有资源且可以让多种实现通过测试。&lt;/li>
&lt;li>大多数 API 接口正被积极地使用。&lt;/li>
&lt;li>Kubernetes SIG Network API 评审团队已批准其进入 Beta 阶段。&lt;/li>
&lt;/ul>
&lt;!--
For more information on Gateway API versioning, refer to the [official
documentation](https://gateway-api.sigs.k8s.io/concepts/versioning/). To see
what's in store for future releases check out the [next steps](#next-steps)
section.
-->
&lt;p>有关 Gateway API 版本控制的更多信息，请参阅&lt;a href="https://gateway-api.sigs.k8s.io/concepts/versioning/">官方文档&lt;/a>。
要查看未来版本的计划，请查看&lt;a href="#next-steps">下一步&lt;/a>。&lt;/p>
&lt;!--
### Release channels
This release introduces the `experimental` and `standard` [release channels][ch]
which enable a better balance of maintaining stability while still enabling
experimentation and iterative development.
-->
&lt;h3 id="发布渠道">发布渠道&lt;/h3>
&lt;p>此版本引入了 &lt;code>experimental&lt;/code> 和 &lt;code>standard&lt;/code> &lt;a href="https://gateway-api.sigs.k8s.io/concepts/versioning/#release-channels-eg-experimental-standard">发布渠道&lt;/a>，
这样能够更好地保持平衡，在确保稳定性的同时，还能支持实验和迭代开发。&lt;/p>
&lt;!--
The `standard` release channel includes:
- resources that have graduated to beta
- fields that have graduated to standard (no longer considered experimental)
-->
&lt;p>&lt;code>standard&lt;/code> 发布渠道包括：&lt;/p>
&lt;ul>
&lt;li>已进入 Beta 阶段的资源&lt;/li>
&lt;li>已进入 standard 的字段（不再被视为 experimental）&lt;/li>
&lt;/ul>
&lt;!--
The `experimental` release channel includes everything in the `standard` release
channel, plus:
- `alpha` API resources
- fields that are considered experimental and have not graduated to `standard` channel
-->
&lt;p>&lt;code>experimental&lt;/code> 发布渠道包括 &lt;code>standard&lt;/code> 发布渠道的所有内容，另外还有：&lt;/p>
&lt;ul>
&lt;li>&lt;code>alpha&lt;/code> API 资源&lt;/li>
&lt;li>视为 experimental 且还未进入 &lt;code>standard&lt;/code> 渠道的字段&lt;/li>
&lt;/ul>
&lt;!--
Release channels are used internally to enable iterative development with
quick turnaround, and externally to indicate feature stability to implementors
and end-users.
-->
&lt;p>使用发布渠道能让内部实现快速流转的迭代开发，且能让外部实现者和最终用户标示功能稳定性。&lt;/p>
&lt;!--
For this release we've added the following experimental features:
- [Routes can attach to Gateways by specifying port numbers](https://gateway-api.sigs.k8s.io/geps/gep-957/)
- [URL rewrites and path redirects](https://gateway-api.sigs.k8s.io/geps/gep-726/)
-->
&lt;p>本次发布新增了以下实验性的功能特性：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/geps/gep-957/">路由通过指定端口号可以挂接到 Gateway&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/geps/gep-726/">URL 重写和路径重定向&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Other improvements
For an exhaustive list of changes included in the `v0.5.0` release, please see
the [v0.5.0 release notes](https://github.com/kubernetes-sigs/gateway-api/releases/tag/v0.5.0).
-->
&lt;h3 id="其他改进">其他改进&lt;/h3>
&lt;p>有关 &lt;code>v0.5.0&lt;/code> 版本中包括的完整变更清单，请参阅
&lt;a href="https://github.com/kubernetes-sigs/gateway-api/releases/tag/v0.5.0">v0.5.0 发布说明&lt;/a>。&lt;/p>
&lt;!--
## Gateway API for service mesh: the GAMMA Initiative
Some service mesh projects have [already implemented support for the Gateway
API](https://gateway-api.sigs.k8s.io/implementations/). Significant overlap
between the Service Mesh Interface (SMI) APIs and the Gateway API has [inspired
discussion in the SMI
community](https://github.com/servicemeshinterface/smi-spec/issues/249) about
possible integration.
-->
&lt;h2 id="适用于服务网格的-gateway-api-gamma-倡议">适用于服务网格的 Gateway API：GAMMA 倡议&lt;/h2>
&lt;p>某些服务网格项目&lt;a href="https://gateway-api.sigs.k8s.io/implementations/">已实现对 Gateway API 的支持&lt;/a>。
服务网格接口 (Service Mesh Interface，SMI) API 和 Gateway API 之间的显著重叠
&lt;a href="https://github.com/servicemeshinterface/smi-spec/issues/249">已激发了 SMI 社区讨论&lt;/a>可能的集成方式。&lt;/p>
&lt;!--
We are pleased to announce that the service mesh community, including
representatives from Cilium Service Mesh, Consul, Istio, Kuma, Linkerd, NGINX
Service Mesh and Open Service Mesh, is coming together to form the [GAMMA
Initiative](https://gateway-api.sigs.k8s.io/contributing/gamma/), a dedicated
workstream within the Gateway API subproject focused on Gateway API for Mesh
Management and Administration.
-->
&lt;p>我们很高兴地宣布，来自 Cilium Service Mesh、Consul、Istio、Kuma、Linkerd、NGINX Service Mesh
和 Open Service Mesh 等服务网格社区的代表汇聚一堂组成
&lt;a href="https://gateway-api.sigs.k8s.io/contributing/gamma/">GAMMA 倡议小组&lt;/a>，
这是 Gateway API 子项目内一个专门的工作流，专注于网格管理所用的 Gateway API。&lt;/p>
&lt;!--
This group will deliver [enhancement
proposals](https://gateway-api.sigs.k8s.io/v1beta1/contributing/gep/) consisting
of resources, additions, and modifications to the Gateway API specification for
mesh and mesh-adjacent use-cases.
This work has begun with [an exploration of using Gateway API for
service-to-service
traffic](https://docs.google.com/document/d/1T_DtMQoq2tccLAtJTpo3c0ohjm25vRS35MsestSL9QU/edit#heading=h.jt37re3yi6k5)
and will continue with enhancement in areas such as authentication and
authorization policy.
-->
&lt;p>这个小组将交付&lt;a href="https://gateway-api.sigs.k8s.io/v1beta1/contributing/gep/">增强提案&lt;/a>，
包括对网格和网格相关用例适用的 Gateway API 规约的资源、添加和修改。&lt;/p>
&lt;p>这项工作已从
&lt;a href="https://docs.google.com/document/d/1T_DtMQoq2tccLAtJTpo3c0ohjm25vRS35MsestSL9QU/edit#heading=h.jt37re3yi6k5">探索针对服务间流量使用 Gateway API&lt;/a>
开始，并将继续增强身份验证和鉴权策略等领域。&lt;/p>
&lt;!--
## Next steps
As we continue to mature the API for production use cases, here are some of the highlights of what we'll be working on for the next Gateway API releases:
- [GRPCRoute][gep1016] for [gRPC][grpc] traffic routing
- [Route delegation][pr1085]
- Layer 4 API maturity: Graduating [TCPRoute][tcpr], [UDPRoute][udpr] and
[TLSRoute][tlsr] to beta
- [GAMMA Initiative](https://gateway-api.sigs.k8s.io/contributing/gamma/) - Gateway API for Service Mesh
-->
&lt;h2 id="下一步">下一步&lt;/h2>
&lt;p>随着我们不断完善用于生产用例的 API，以下是我们将为下一个 Gateway API 版本所做的一些重点工作：&lt;/p>
&lt;ul>
&lt;li>针对 &lt;a href="https://grpc.io/">gRPC&lt;/a> 流量路由的 &lt;a href="https://github.com/kubernetes-sigs/gateway-api/blob/master/site-src/geps/gep-1016.md">GRPCRoute&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/gateway-api/pull/1085">路由代理&lt;/a>&lt;/li>
&lt;li>4 层 API 成熟度：&lt;a href="https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/tcproute_types.go">TCPRoute&lt;/a>、&lt;a href="https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/udproute_types.go">UDPRoute&lt;/a> 和 &lt;a href="https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/tlsroute_types.go">TLSRoute&lt;/a> 正进入 Beta 阶段&lt;/li>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/contributing/gamma/">GAMMA 倡议&lt;/a> - 针对服务网格的 Gateway API&lt;/li>
&lt;/ul>
&lt;!--
If there's something on this list you want to get involved in, or there's
something not on this list that you want to advocate for to get on the roadmap
please join us in the #sig-network-gateway-api channel on Kubernetes Slack or our weekly [community calls](https://gateway-api.sigs.k8s.io/contributing/community/#meetings).
-->
&lt;p>如果你想参与此列表中的某些工作，或者你想倡导加入路线图的内容不在此列表中，
请通过 Kubernetes Slack 的 #sig-network-gateway-api 频道或我们每周的
&lt;a href="https://gateway-api.sigs.k8s.io/contributing/community/#meetings">社区电话会议&lt;/a>加入我们。&lt;/p></description></item><item><title>Blog: 2021 年度总结报告</title><link>https://kubernetes.io/zh-cn/blog/2022/06/01/annual-report-summary-2021/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/06/01/annual-report-summary-2021/</guid><description>
&lt;!--
layout: blog
title: "Annual Report Summary 2021"
date: 2022-06-01
slug: annual-report-summary-2021
-->
&lt;!--
**Author:** Paris Pittman (Steering Committee)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Paris Pittman（指导委员会）&lt;/p>
&lt;!--
Last year, we published our first [Annual Report Summary](/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/) for 2020 and it's already time for our second edition!
[2021 Annual Report Summary](https://www.cncf.io/reports/kubernetes-annual-report-2021/)
-->
&lt;p>去年，我们发布了第一期
&lt;a href="https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/">2020 年度总结报告&lt;/a>，
现在已经是时候发布第二期了！&lt;/p>
&lt;p>&lt;a href="https://www.cncf.io/reports/kubernetes-annual-report-2021/">2021 年度总结报告&lt;/a>&lt;/p>
&lt;!--
This summary reflects the work that has been done in 2021 and the initiatives on deck for the rest of 2022. Please forward to organizations and indidviduals participating in upstream activities, planning cloud native strategies, and/or those looking to help out. To find a specific community group's complete report, go to the [kubernetes/community repo](https://github.com/kubernetes/community) under the groups folder. Example: [sig-api-machinery/annual-report-2021.md](https://github.com/kubernetes/community/blob/master/sig-api-machinery/annual-report-2021.md)
-->
&lt;p>这份总结反映了 2021 年已完成的工作以及 2022 下半年置于台面上的倡议。
请将这份总结转发给正参与上游活动、计划云原生战略和寻求帮助的那些组织和个人。
若要查阅特定社区小组的完整报告，请访问
&lt;a href="https://github.com/kubernetes/community">kubernetes/community 仓库&lt;/a>查找各小组的文件夹。例如：
&lt;a href="https://github.com/kubernetes/community/blob/master/sig-api-machinery/annual-report-2021.md">sig-api-machinery/annual-report-2021.md&lt;/a>&lt;/p>
&lt;!--
You’ll see that this report summary is a growth area in itself. It takes us roughly 6 months to prepare and execute, which isn’t helpful or valuable to anyone as a fast moving project with short and long term needs. How can we make this better? Provide your feedback here: https://github.com/kubernetes/steering/issues/242
Reference:
[Annual Report Documentation](https://github.com/kubernetes/community/blob/master/committee-steering/governance/annual-reports.md)
-->
&lt;p>你将看到这份总结报告本身涵盖的领域在增长。我们准备和制作这份报告大约用了 6 个月的时间。
作为一个随着长短期需求而快速发展的项目，这么长的制作周期对任何人来说可能帮助都不大，
报告的价值也有所缩水。我等苦思无良策，请诸君不吝赐教：
&lt;a href="https://github.com/kubernetes/steering/issues/242">https://github.com/kubernetes/steering/issues/242&lt;/a>&lt;/p>
&lt;p>参考：
&lt;a href="https://github.com/kubernetes/community/blob/master/committee-steering/governance/annual-reports.md">年度报告文献&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: StatefulSet 的最大不可用副本数</title><link>https://kubernetes.io/zh-cn/blog/2022/05/27/maxunavailable-for-statefulset/</link><pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/27/maxunavailable-for-statefulset/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.24: Maximum Unavailable Replicas for StatefulSet'
date: 2022-05-27
slug: maxunavailable-for-statefulset
**Author:** Mayank Kumar (Salesforce)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Mayank Kumar (Salesforce)&lt;/p>
&lt;!--
Kubernetes [StatefulSets](/docs/concepts/workloads/controllers/statefulset/), since their introduction in
1.5 and becoming stable in 1.9, have been widely used to run stateful applications. They provide stable pod identity, persistent
per pod storage and ordered graceful deployment, scaling and rolling updates. You can think of StatefulSet as the atomic building
block for running complex stateful applications. As the use of Kubernetes has grown, so has the number of scenarios requiring
StatefulSets. Many of these scenarios, require faster rolling updates than the currently supported one-pod-at-a-time updates, in the
case where you're using the `OrderedReady` Pod management policy for a StatefulSet.
-->
&lt;p>Kubernetes &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/">StatefulSet&lt;/a>，
自 1.5 版本中引入并在 1.9 版本中变得稳定以来，已被广泛用于运行有状态应用。它提供固定的 Pod 身份标识、
每个 Pod 的持久存储以及 Pod 的有序部署、扩缩容和滚动更新功能。你可以将 StatefulSet
视为运行复杂有状态应用程序的原子构建块。随着 Kubernetes 的使用增多，需要 StatefulSet 的场景也越来越多。
当 StatefulSet 的 Pod 管理策略为 &lt;code>OrderedReady&lt;/code> 时，其中许多场景需要比当前所支持的一次一个 Pod
的更新更快的滚动更新。&lt;/p>
&lt;!--
Here are some examples:
- I am using a StatefulSet to orchestrate a multi-instance, cache based application where the size of the cache is large. The cache
starts cold and requires some siginificant amount of time before the container can start. There could be more initial startup tasks
that are required. A RollingUpdate on this StatefulSet would take a lot of time before the application is fully updated. If the
StatefulSet supported updating more than one pod at a time, it would result in a much faster update.
-->
&lt;p>这里有些例子：&lt;/p>
&lt;ul>
&lt;li>我使用 StatefulSet 来编排一个基于缓存的多实例应用程序，其中缓存的规格很大。
缓存冷启动，需要相当长的时间才能启动容器。所需要的初始启动任务有很多。在应用程序完全更新之前，
此 StatefulSet 上的 RollingUpdate 将花费大量时间。如果 StatefulSet 支持一次更新多个 Pod，
那么更新速度会快得多。&lt;/li>
&lt;/ul>
&lt;!--
- My stateful application is composed of leaders and followers or one writer and multiple readers. I have multiple readers or
followers and my application can tolerate multiple pods going down at the same time. I want to update this application more than
one pod at a time so that i get the new updates rolled out quickly, especially if the number of instances of my application are
large. Note that my application still requires unique identity per pod.
-->
&lt;ul>
&lt;li>我的有状态应用程序由 leader 和 follower 或者一个 writer 和多个 reader 组成。
我有多个 reader 或 follower，并且我的应用程序可以容忍多个 Pod 同时出现故障。
我想一次更新这个应用程序的多个 Pod，特别是当我的应用程序实例数量很多时，这样我就能快速推出新的更新。
注意，我的应用程序仍然需要每个 Pod 具有唯一标识。&lt;/li>
&lt;/ul>
&lt;!--
In order to support such scenarios, Kubernetes 1.24 includes a new alpha feature to help. Before you can use the new feature you must
enable the `MaxUnavailableStatefulSet` feature flag. Once you enable that, you can specify a new field called `maxUnavailable`, part
of the `spec` for a StatefulSet. For example:
-->
&lt;p>为了支持这样的场景，Kubernetes 1.24 提供了一个新的 alpha 特性。在使用新特性之前，必须启用
&lt;code>MaxUnavailableStatefulSet&lt;/code> 特性标志。一旦启用，就可以指定一个名为 &lt;code>maxUnavailable&lt;/code> 的新字段，
这是 StatefulSet &lt;code>spec&lt;/code> 的一部分。例如：&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: apps/v1
kind: StatefulSet
metadata:
name: web
namespace: default
spec:
podManagementPolicy: OrderedReady # 你必须设为 OrderedReady
replicas: 5
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
containers:
- image: k8s.gcr.io/nginx-slim:0.8
imagePullPolicy: IfNotPresent
name: nginx
updateStrategy:
rollingUpdate:
maxUnavailable: 2 # 这是 alpha 特性的字段，默认值是 1
partition: 0
type: RollingUpdate
&lt;/code>&lt;/pre>&lt;!--
If you enable the new feature and you don't specify a value for `maxUnavailable` in a StatefulSet, Kubernetes applies a default
`maxUnavailable: 1`. This matches the behavior you would see if you don't enable the new feature.
-->
&lt;p>如果你启用了新特性，但没有在 StatefulSet 中指定 &lt;code>maxUnavailable&lt;/code> 的值，Kubernetes
会默认设置 &lt;code>maxUnavailable: 1&lt;/code>。这与你不启用新特性时看到的行为是一致的。&lt;/p>
&lt;!--
I'll run through a scenario based on that example manifest to demonstrate how this feature works. I will deploy a StatefulSet that
has 5 replicas, with `maxUnavailable` set to 2 and `partition` set to 0.
-->
&lt;p>我将基于该示例清单做场景演练，以演示此特性是如何工作的。我将部署一个有 5 个副本的 StatefulSet，
&lt;code>maxUnavailable&lt;/code> 设置为 2 并将 &lt;code>partition&lt;/code> 设置为 0。&lt;/p>
&lt;!--
I can trigger a rolling update by changing the image to `k8s.gcr.io/nginx-slim:0.9`. Once I initiate the rolling update, I can
watch the pods update 2 at a time as the current value of maxUnavailable is 2. The below output shows a span of time and is not
complete. The maxUnavailable can be an absolute number (for example, 2) or a percentage of desired Pods (for example, 10%). The
absolute number is calculated from percentage by rounding down.
-->
&lt;p>我可以通过将镜像更改为 &lt;code>k8s.gcr.io/nginx-slim:0.9&lt;/code> 来触发滚动更新。一旦开始滚动更新，
就可以看到一次更新 2 个 Pod，因为 &lt;code>maxUnavailable&lt;/code> 的当前值是 2。
下面的输出显示了一个时间段内的结果，但并不是完整过程。&lt;code>maxUnavailable&lt;/code> 可以是绝对数值（例如 2）或所需 Pod
的百分比（例如 10%），绝对数是通过百分比计算结果进行四舍五入得出的。&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl get pods --watch
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>NAME READY STATUS RESTARTS AGE
web-0 1/1 Running 0 85s
web-1 1/1 Running 0 2m6s
web-2 1/1 Running 0 106s
web-3 1/1 Running 0 2m47s
web-4 1/1 Running 0 2m27s
web-4 1/1 Terminating 0 5m43s ----&amp;gt; start terminating 4
web-3 1/1 Terminating 0 6m3s ----&amp;gt; start terminating 3
web-3 0/1 Terminating 0 6m7s
web-3 0/1 Pending 0 0s
web-3 0/1 Pending 0 0s
web-4 0/1 Terminating 0 5m48s
web-4 0/1 Terminating 0 5m48s
web-3 0/1 ContainerCreating 0 2s
web-3 1/1 Running 0 2s
web-4 0/1 Pending 0 0s
web-4 0/1 Pending 0 0s
web-4 0/1 ContainerCreating 0 0s
web-4 1/1 Running 0 1s
web-2 1/1 Terminating 0 5m46s ----&amp;gt; start terminating 2 (only after both 4 and 3 are running)
web-1 1/1 Terminating 0 6m6s ----&amp;gt; start terminating 1
web-2 0/1 Terminating 0 5m47s
web-1 0/1 Terminating 0 6m7s
web-1 0/1 Pending 0 0s
web-1 0/1 Pending 0 0s
web-1 0/1 ContainerCreating 0 1s
web-1 1/1 Running 0 2s
web-2 0/1 Pending 0 0s
web-2 0/1 Pending 0 0s
web-2 0/1 ContainerCreating 0 0s
web-2 1/1 Running 0 1s
web-0 1/1 Terminating 0 6m6s ----&amp;gt; start terminating 0 (only after 2 and 1 are running)
web-0 0/1 Terminating 0 6m7s
web-0 0/1 Pending 0 0s
web-0 0/1 Pending 0 0s
web-0 0/1 ContainerCreating 0 0s
web-0 1/1 Running 0 1s
&lt;/code>&lt;/pre>&lt;!--
Note that as soon as the rolling update starts, both 4 and 3 (the two highest ordinal pods) start terminating at the same time. Pods
with ordinal 4 and 3 may become ready at their own pace. As soon as both pods 4 and 3 are ready, pods 2 and 1 start terminating at the
same time. When pods 2 and 1 are both running and ready, pod 0 starts terminating.
-->
&lt;p>注意，滚动更新一开始，4 和 3（两个最高序号的 Pod）同时开始进入 &lt;code>Terminating&lt;/code> 状态。
Pod 4 和 3 会按照自身节奏进行更新。一旦 Pod 4 和 3 更新完毕后，Pod 2 和 1 会同时进入
&lt;code>Terminating&lt;/code> 状态。当 Pod 2 和 1 都准备完毕处于 &lt;code>Running&lt;/code> 状态时，Pod 0 开始进入 &lt;code>Terminating&lt;/code> 状态&lt;/p>
&lt;!--
In Kubernetes, updates to StatefulSets follow a strict ordering when updating Pods. In this example, the update starts at replica 4, then
replica 3, then replica 2, and so on, one pod at a time. When going one pod at a time, its not possible for 3 to be running and ready
before 4. When `maxUnavailable` is more than 1 (in the example scenario I set `maxUnavailable` to 2), it is possible that replica 3 becomes
ready and running before replica 4 is ready&amp;mdash;and that is ok. If you're a developer and you set `maxUnavailable` to more than 1, you should
know that this outcome is possible and you must ensure that your application is able to handle such ordering issues that occur
if any. When you set `maxUnavailable` greater than 1, the ordering is guaranteed in between each batch of pods being updated. That guarantee
means that pods in update batch 2 (replicas 2 and 1) cannot start updating until the pods from batch 0 (replicas 4 and 3) are ready.
-->
&lt;p>在 Kubernetes 中，StatefulSet 更新 Pod 时遵循严格的顺序。在此示例中，更新从副本 4 开始，
然后是副本 3，然后是副本 2，以此类推，一次更新一个 Pod。当一次只更新一个 Pod 时，
副本 3 不可能在副本 4 之前准备好进入 &lt;code>Running&lt;/code> 状态。当 &lt;code>maxUnavailable&lt;/code> 值
大于 1 时（在示例场景中我设置 &lt;code>maxUnavailable&lt;/code> 值为 2），副本 3 可能在副本 4 之前准备好并运行，
这是没问题的。如果你是开发人员并且设置 &lt;code>maxUnavailable&lt;/code> 值大于 1，你应该知道可能出现这种情况，
并且如果有这种情况的话，你必须确保你的应用程序能够处理发生的此类顺序问题。当你设置 &lt;code>maxUnavailable&lt;/code>
值大于 1 时，更新 Pod 的批次之间会保证顺序。该保证意味着在批次 0（副本 4 和 3）中的 Pod
准备好之前，更新批次 2（副本 2 和 1）中的 Pod 无法开始更新。&lt;/p>
&lt;!--
Although Kubernetes refers to these as _replicas_, your stateful application may have a different view and each pod of the StatefulSet may
be holding completely different data than other pods. The important thing here is that updates to StatefulSets happen in batches, and you can
now have a batch size larger than 1 (as an alpha feature).
-->
&lt;p>尽管 Kubernetes 将这些称为&lt;strong>副本&lt;/strong>，但你的有状态应用程序可能不这样理解，StatefulSet 的每个
Pod 可能持有与其他 Pod 完全不同的数据。重要的是，StatefulSet 的更新是分批进行的，
你现在让批次大小大于 1（作为 alpha 特性）。&lt;/p>
&lt;!--
Also note, that the above behavior is with `podManagementPolicy: OrderedReady`. If you defined a StatefulSet as `podManagementPolicy: Parallel`,
not only `maxUnavailable` number of replicas are terminated at the same time; `maxUnavailable` number of replicas start in `ContainerCreating`
phase at the same time as well. This is called bursting.
-->
&lt;p>还要注意，上面的行为采用的 Pod 管理策略是 &lt;code>podManagementPolicy: OrderedReady&lt;/code>。
如果你的 StatefulSet 的 Pod 管理策略是 &lt;code>podManagementPolicy: Parallel&lt;/code>，
那么不仅是 &lt;code>maxUnavailable&lt;/code> 数量的副本同时被终止，还会导致 &lt;code>maxUnavailable&lt;/code> 数量的副本同时在
&lt;code>ContainerCreating&lt;/code> 阶段。这就是所谓的突发（Bursting）。&lt;/p>
&lt;!--
So, now you may have a lot of questions about:-
- What is the behavior when you set `podManagementPolicy: Parallel`?
- What is the behavior when `partition` to a value other than `0`?
-->
&lt;p>因此，现在你可能有很多关于以下方面的问题：&lt;/p>
&lt;ul>
&lt;li>当设置 &lt;code>podManagementPolicy:Parallel&lt;/code> 时，会产生什么行为？&lt;/li>
&lt;li>将 &lt;code>partition&lt;/code> 设置为非 &lt;code>0&lt;/code> 值时会发生什么？&lt;/li>
&lt;/ul>
&lt;!--
It might be better to try and see it for yourself. This is an alpha feature, and the Kubernetes contributors are looking for feedback on this feature. Did
this help you achieve your stateful scenarios Did you find a bug or do you think the behavior as implemented is not intuitive or can
break applications or catch them by surprise? Please [open an issue](https://github.com/kubernetes/kubernetes/issues) to let us know.
-->
&lt;p>自己试试看可能会更好。这是一个 alpha 特性，Kubernetes 贡献者正在寻找有关此特性的反馈。
这是否有助于你实现有状态的场景？你是否发现了一个 bug，或者你认为实现的行为不直观易懂，
或者它可能会破坏应用程序或让他们感到吃惊？请&lt;a href="https://github.com/kubernetes/kubernetes/issues">登记一个 issue&lt;/a>
告知我们。&lt;/p>
&lt;!--
## Further reading and next steps {#next-steps}
- [Maximum unavailable Pods](/docs/concepts/workloads/controllers/statefulset/#maximum-unavailable-pods)
- [KEP for MaxUnavailable for StatefulSet](https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/961-maxunavailable-for-statefulset)
- [Implementation](https://github.com/kubernetes/kubernetes/pull/82162/files)
- [Enhancement Tracking Issue](https://github.com/kubernetes/enhancements/issues/961)
-->
&lt;h2 id="next-steps">进一步阅读和后续步骤&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/#maximum-unavailable-pods">最多不可用 Pod 数&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/961-maxunavailable-for-statefulset">KEP for MaxUnavailable for StatefulSet&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/82162/files">代码实现&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/961">增强跟踪 Issue&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.24 中的上下文日志记录</title><link>https://kubernetes.io/zh-cn/blog/2022/05/25/contextual-logging/</link><pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/25/contextual-logging/</guid><description>
&lt;!--
layout: blog
title: "Contextual Logging in Kubernetes 1.24"
date: 2022-05-25
slug: contextual-logging
canonicalUrl: https://kubernetes.dev/blog/2022/05/25/contextual-logging/
-->
&lt;!--
**Authors:** Patrick Ohly (Intel)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;!--
The [Structured Logging Working
Group](https://github.com/kubernetes/community/blob/master/wg-structured-logging/README.md)
has added new capabilities to the logging infrastructure in Kubernetes
1.24. This blog post explains how developers can take advantage of those to
make log output more useful and how they can get involved with improving Kubernetes.
-->
&lt;p>&lt;a href="https://github.com/kubernetes/community/blob/master/wg-structured-logging/README.md">结构化日志工作组&lt;/a>
在 Kubernetes 1.24 中为日志基础设施添加了新功能。这篇博文解释了开发者如何利用这些功能使日志输出更有用，
以及他们如何参与改进 Kubernetes。&lt;/p>
&lt;!--
## Structured logging
-->
&lt;h2 id="结构化日志记录">结构化日志记录&lt;/h2>
&lt;!--
The goal of [structured
logging](https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/1602-structured-logging/README.md)
is to replace C-style formatting and the resulting opaque log strings with log
entries that have a well-defined syntax for storing message and parameters
separately, for example as a JSON struct.
-->
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/1602-structured-logging/README.md">结构化日志&lt;/a>
记录的目标是用具有明确定义的语法的日志条目来取代 C 风格的格式化和由此产生的不透明的日志字符串，用于分别存储消息和参数，例如，作为一个 JSON 结构。&lt;/p>
&lt;!--
When using the traditional klog text output format for structured log calls,
strings were originally printed with `\n` escape sequences, except when
embedded inside a struct. For structs, log entries could still span multiple
lines, with no clean way to split the log stream into individual entries:
-->
&lt;p>当使用传统的 klog 文本输出格式进行结构化日志调用时，字符串最初使用 &lt;code>\n&lt;/code> 转义序列打印，除非嵌入到结构中。
对于结构体，日志条目仍然可以跨越多行，没有干净的方法将日志流拆分为单独的条目：&lt;/p>
&lt;pre tabindex="0">&lt;code>I1112 14:06:35.783529 328441 structured_logging.go:51] &amp;#34;using InfoS&amp;#34; longData={Name:long Data:Multiple
lines
with quite a bit
of text. internal:0}
I1112 14:06:35.783549 328441 structured_logging.go:52] &amp;#34;using InfoS with\nthe message across multiple lines&amp;#34; int=1 stringData=&amp;#34;long: Multiple\nlines\nwith quite a bit\nof text.&amp;#34; str=&amp;#34;another value&amp;#34;
&lt;/code>&lt;/pre>&lt;!--
Now, the `&lt;` and `>` markers along with indentation are used to ensure that splitting at a
klog header at the start of a line is reliable and the resulting output is human-readable:
-->
&lt;p>现在，&lt;code>&amp;lt;&lt;/code> 和 &lt;code>&amp;gt;&lt;/code> 标记以及缩进用于确保在行首的 klog 标头处拆分是可靠的，并且生成的输出是人类可读的：&lt;/p>
&lt;pre tabindex="0">&lt;code>I1126 10:31:50.378204 121736 structured_logging.go:59] &amp;#34;using InfoS&amp;#34; longData=&amp;lt;
{Name:long Data:Multiple
lines
with quite a bit
of text. internal:0}
&amp;gt;
I1126 10:31:50.378228 121736 structured_logging.go:60] &amp;#34;using InfoS with\nthe message across multiple lines&amp;#34; int=1 stringData=&amp;lt;
long: Multiple
lines
with quite a bit
of text.
&amp;gt; str=&amp;#34;another value&amp;#34;
&lt;/code>&lt;/pre>&lt;!--
Note that the log message itself is printed with quoting. It is meant to be a
fixed string that identifies a log entry, so newlines should be avoided there.
-->
&lt;p>请注意，日志消息本身带有引号。它是一个用于标识日志条目的固定字符串，因此应避免使用换行符。&lt;/p>
&lt;!--
Before Kubernetes 1.24, some log calls in kube-scheduler still used `klog.Info`
for multi-line strings to avoid the unreadable output. Now all log calls have
been updated to support structured logging.
-->
&lt;p>在 Kubernetes 1.24 之前，kube-scheduler 中的一些日志调用仍然使用 &lt;code>klog.Info&lt;/code> 处理多行字符串，
以避免不可读的输出。现在所有日志调用都已更新以支持结构化日志记录。&lt;/p>
&lt;!--
## Contextual logging
-->
&lt;h2 id="上下文日志记录">上下文日志记录&lt;/h2>
&lt;!--
[Contextual logging](https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/3077-contextual-logging/README.md)
is based on the [go-logr API](https://github.com/go-logr/logr#a-minimal-logging-api-for-go). The key
idea is that libraries are passed a logger instance by their caller and use
that for logging instead of accessing a global logger. The binary decides about
the logging implementation, not the libraries. The go-logr API is designed
around structured logging and supports attaching additional information to a
logger.
-->
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/3077-contextual-logging/README.md">上下文日志&lt;/a>
基于 &lt;a href="https://github.com/go-logr/logr#a-minimal-logging-api-for-go">go-logr API&lt;/a>。
关键的想法是，库被其调用者传递给一个记录器实例，并使用它来记录，而不是访问一个全局记录器。
二进制文件决定了日志的实现，而不是库。go-logr API 是围绕着结构化的日志记录而设计的，并支持将额外的信息附加到一个记录器上。&lt;/p>
&lt;!--
This enables additional use cases:
-->
&lt;p>这使得以下用例成为可能：&lt;/p>
&lt;!--
- The caller can attach additional information to a logger:
- [`WithName`](https://pkg.go.dev/github.com/go-logr/logr#Logger.WithName) adds a prefix
- [`WithValues`](https://pkg.go.dev/github.com/go-logr/logr#Logger.WithValues) adds key/value pairs
-->
&lt;ul>
&lt;li>
&lt;p>调用者可以将附加信息附加到记录器：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://pkg.go.dev/github.com/go-logr/logr#Logger.WithName">&lt;code>WithName&lt;/code>&lt;/a> 添加前缀&lt;/li>
&lt;li>&lt;a href="https://pkg.go.dev/github.com/go-logr/logr#Logger.WithValues">&lt;code>WithValues&lt;/code>&lt;/a> 添加键/值对&lt;/li>
&lt;/ul>
&lt;!--
When passing this extended logger into a function and a function uses it
instead of the global logger, the additional information is
then included in all log entries, without having to modify the code that
generates the log entries. This is useful in highly parallel applications
where it can become hard to identify all log entries for a certain operation
because the output from different operations gets interleaved.
-->
&lt;p>当将此扩展记录器传递给函数并且函数使用它而不是全局记录器时，附加信息随后将包含在所有日志条目中，而无需修改生成日志条目的代码。
这在高度并行的应用程序中很有用，在这些应用程序中，由于不同操作的输出会交错，因此很难识别某个操作的所有日志条目。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
- When running unit tests, log output can be associated with the current test.
Then when a test fails, only the log output of the failed test gets shown
by `go test`. That output can also be more verbose by default because it
will not get shown for successful tests. Tests can be run in parallel
without interleaving their output.
-->
&lt;ul>
&lt;li>运行单元测试时，可以将日志输出与当前测试关联起来。当测试失败时，&lt;code>go test&lt;/code> 只显示失败测试的日志输出。
默认情况下，该输出也可以更详细，因为它不会显示成功的测试。这些测试可以在不交错输出的情况下并行运行。&lt;/li>
&lt;/ul>
&lt;!--
One of the design decisions for contextual logging was to allow attaching a
logger as value to a `context.Context`. Since the logger encapsulates all
aspects of the intended logging for the call, it is *part* of the context and
not just *using* it. A practical advantage is that many APIs already have a
`ctx` parameter or adding one has additional advantages, like being able to get
rid of `context.TODO()` calls inside the functions.
-->
&lt;p>上下文日志记录的设计决策之一是允许将记录器作为值附加到 &lt;code>context.Context&lt;/code>。
由于记录器封装了调用的预期记录的所有方面，它是上下文的&lt;strong>部分&lt;/strong>，而不仅仅是&lt;strong>使用&lt;/strong>它。
一个实际的优势是许多 API 已经有一个 &lt;code>ctx&lt;/code> 参数，或者添加一个具有其他优势，例如能够摆脱函数内部的 &lt;code>context.TODO()&lt;/code> 调用。&lt;/p>
&lt;!--
Another decision was to not break compatibility with klog v2:
-->
&lt;p>另一个决定是不破坏与 klog v2 的兼容性：&lt;/p>
&lt;!--
- Libraries that use the traditional klog logging calls in a binary that has
set up contextual logging will work and log through the logging backend
chosen by the binary. However, such log output will not include the
additional information and will not work well in unit tests, so libraries
should be modified to support contextual logging. The [migration guide](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md)
for structured logging has been extended to also cover contextual logging.
-->
&lt;ul>
&lt;li>在已设置上下文日志记录的二进制文件中使用传统 klog 日志记录调用的库将通过二进制文件选择的日志记录后端工作和记录。
但是，这样的日志输出不会包含额外的信息，并且在单元测试中不能很好地工作，因此应该修改库以支持上下文日志记录。
结构化日志记录的&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md">迁移指南&lt;/a>
已扩展为也涵盖上下文日志记录。&lt;/li>
&lt;/ul>
&lt;!--
- When a library supports contextual logging and retrieves a logger from its
context, it will still work in a binary that does not initialize contextual
logging because it will get a logger that logs through klog.
-->
&lt;ul>
&lt;li>当一个库支持上下文日志并从其上下文中检索一个记录器时，它仍将在不初始化上下文日志的二进制文件中工作，
因为它将获得一个通过 klog 记录的记录器。&lt;/li>
&lt;/ul>
&lt;!--
In Kubernetes 1.24, contextual logging is a new alpha feature with
`ContextualLogging` as feature gate. When disabled (the default), the new klog
API calls for contextual logging (see below) become no-ops to avoid performance
or functional regressions.
-->
&lt;p>在 Kubernetes 1.24 中，上下文日志是一个新的 Alpha 特性，以 &lt;code>ContextualLogging&lt;/code> 作为特性门控。
禁用时（默认），用于上下文日志记录的新 klog API 调用（见下文）变为无操作，以避免性能或功能回归。&lt;/p>
&lt;!--
No Kubernetes component has been converted yet. An [example program](https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go)
in the Kubernetes repository demonstrates how to enable contextual logging in a
binary and how the output depends on the binary's parameters:
-->
&lt;p>尚未转换任何 Kubernetes 组件。 Kubernetes 存储库中的&lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go">示例程序&lt;/a>
演示了如何在一个二进制文件中启用上下文日志记录，以及输出如何取决于该二进制文件的参数：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> &lt;span style="color:#a2f">cd&lt;/span> &lt;span style="color:#b8860b">$GOPATH&lt;/span>/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> go run . --help
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> --feature-gates mapStringBool A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> AllAlpha=true|false (ALPHA - default=false)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> AllBeta=true|false (BETA - default=false)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> ContextualLogging=true|false (ALPHA - default=false)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> go run . --feature-gates &lt;span style="color:#b8860b">ContextualLogging&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0404 18:00:02.916429 451895 logger.go:94] &amp;#34;example/myname: runtime&amp;#34; foo=&amp;#34;bar&amp;#34; duration=&amp;#34;1m0s&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0404 18:00:02.916447 451895 logger.go:95] &amp;#34;example: another runtime&amp;#34; foo=&amp;#34;bar&amp;#34; duration=&amp;#34;1m0s&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
The `example` prefix and `foo="bar"` were added by the caller of the function
which logs the `runtime` message and `duration="1m0s"` value.
-->
&lt;p>&lt;code>example&lt;/code> 前缀和 &lt;code>foo=&amp;quot;bar&amp;quot;&lt;/code> 是由记录 &lt;code>runtime&lt;/code> 消息和 &lt;code>duration=&amp;quot;1m0s&amp;quot;&lt;/code> 值的函数的调用者添加的。&lt;/p>
&lt;!--
The sample code for klog includes an
[example](https://github.com/kubernetes/klog/blob/v2.60.1/ktesting/example/example_test.go)
for a unit test with per-test output.
-->
&lt;p>针对 klog 的示例代码包括一个单元测试&lt;a href="https://github.com/kubernetes/klog/blob/v2.60.1/ktesting/example/example_test.go">示例&lt;/a>
以及每个测试的输出。&lt;/p>
&lt;!--
## klog enhancements
### Contextual logging API
The following calls manage the lookup of a logger:
-->
&lt;h2 id="klog-增强功能">klog 增强功能&lt;/h2>
&lt;h3 id="上下文日志-api">上下文日志 API&lt;/h3>
&lt;p>以下调用管理记录器的查找：&lt;/p>
&lt;!--
[`FromContext`](https://pkg.go.dev/k8s.io/klog/v2#FromContext)
: from a `context` parameter, with fallback to the global logger
-->
&lt;p>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#FromContext">&lt;code>FromContext&lt;/code>&lt;/a>
：来自 &lt;code>context&lt;/code> 参数，回退到全局记录器&lt;/p>
&lt;!--
[`Background`](https://pkg.go.dev/k8s.io/klog/v2#Background)
: the global fallback, with no intention to support contextual logging
[`TODO`](https://pkg.go.dev/k8s.io/klog/v2#TODO)
: the global fallback, but only as a temporary solution until the function gets extended to accept
a logger through its parameters
-->
&lt;p>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#Background">&lt;code>Background&lt;/code>&lt;/a>
：全局后备，无意支持上下文日志记录&lt;/p>
&lt;p>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#TODO">&lt;code>TODO&lt;/code>&lt;/a>
：全局回退，但仅作为一个临时解决方案，直到该函数得到扩展能够通过其参数接受一个记录器&lt;/p>
&lt;!--
[`SetLoggerWithOptions`](https://pkg.go.dev/k8s.io/klog/v2#SetLoggerWithOptions)
: changes the fallback logger; when called with [`ContextualLogger(true)`](https://pkg.go.dev/k8s.io/klog/v2#ContextualLogger),
the logger is ready to be called directly, in which case logging will be done
without going through klog
-->
&lt;p>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#SetLoggerWithOptions">&lt;code>SetLoggerWithOptions&lt;/code>&lt;/a>
：更改后备记录器；当使用&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#ContextualLogger">&lt;code>ContextualLogger(true)&lt;/code>&lt;/a> 调用时,
记录器已准备好被直接调用，在这种情况下，记录将无需执行通过 klog&lt;/p>
&lt;!--
To support the feature gate mechanism in Kubernetes, klog has wrapper calls for
the corresponding go-logr calls and a global boolean controlling their behavior:
-->
&lt;p>为了支持 Kubernetes 中的特性门控机制，klog 对相应的 go-logr 调用进行了包装调用，并使用了一个全局布尔值来控制它们的行为：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#LoggerWithName">&lt;code>LoggerWithName&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#LoggerWithValues">&lt;code>LoggerWithValues&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#NewContext">&lt;code>NewContext&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#EnableContextualLogging">&lt;code>EnableContextualLogging&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Usage of those functions in Kubernetes code is enforced with a linter
check. The klog default for contextual logging is to enable the functionality
because it is considered stable in klog. It is only in Kubernetes binaries
where that default gets overridden and (in some binaries) controlled via the
`--feature-gate` parameter.
-->
&lt;p>在 Kubernetes 代码中使用这些函数是通过 linter 检查强制执行的。
上下文日志的 klog 默认是启用该功能，因为它在 klog 中被认为是稳定的。
只有在 Kubernetes 二进制文件中，该默认值才会被覆盖，并且（在某些二进制文件中）通过 &lt;code>--feature-gate&lt;/code> 参数进行控制。&lt;/p>
&lt;!--
### ktesting logger
The new [ktesting](https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting) package
implements logging through `testing.T` using klog's text output format. It has
a [single API call](https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting#NewTestContext) for
instrumenting a test case and [support for command line flags](https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting/init).
-->
&lt;h3 id="ktesting-记录器">ktesting 记录器&lt;/h3>
&lt;p>新的 &lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting">ktesting&lt;/a>
包使用 klog 的文本输出格式通过 &lt;code>testing.T&lt;/code> 实现日志记录。它有一个 &lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting#NewTestContext">single API call&lt;/a>
用于检测测试用例和&lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting/init">支持命令行标志&lt;/a>。&lt;/p>
&lt;!--
### klogr
[`klog/klogr`](https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr) continues to be
supported and it's default behavior is unchanged: it formats structured log
entries using its own, custom format and prints the result via klog.
-->
&lt;h3 id="klogr">klogr&lt;/h3>
&lt;p>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr">&lt;code>klog/klogr&lt;/code>&lt;/a> 继续受支持，默认行为不变：
它使用其格式化结构化日志条目拥有自己的自定义格式并通过 klog 打印结果。&lt;/p>
&lt;!--
However, this usage is discouraged because that format is neither
machine-readable (in contrast to real JSON output as produced by zapr, the
go-logr implementation used by Kubernetes) nor human-friendly (in contrast to
the klog text format).
-->
&lt;p>但是，不鼓励这种用法，因为这种格式既不是机器可读的（与 zapr 生成的真实 JSON 输出相比，Kubernetes 使用的 go-logr 实现）也不是人类友好的（与 klog 文本格式相比）。&lt;/p>
&lt;!--
Instead, a klogr instance should be created with
[`WithFormat(FormatKlog)`](https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr#WithFormat)
which chooses the klog text format. A simpler construction method with the same
result is the new
[`klog.NewKlogr`](https://pkg.go.dev/k8s.io/klog/v2#NewKlogr). That is the
logger that klog returns as fallback when nothing else is configured.
-->
&lt;p>相反，应该使用选择 klog 文本格式的 &lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr#WithFormat">&lt;code>WithFormat(FormatKlog)&lt;/code>&lt;/a>
创建一个 klogr 实例。 一个更简单但结果相同的构造方法是新的 &lt;a href="https://pkg.go.dev/k8s.io/klog/v2#NewKlogr">&lt;code>klog.NewKlogr&lt;/code>&lt;/a>。
这是 klog 在未配置任何其他内容时作为后备返回的记录器。&lt;/p>
&lt;!--
### Reusable output test
-->
&lt;h3 id="可重用输出测试">可重用输出测试&lt;/h3>
&lt;!--
A lot of go-logr implementations have very similar unit tests where they check
the result of certain log calls. If a developer didn't know about certain
caveats like for example a `String` function that panics when called, then it
is likely that both the handling of such caveats and the unit test are missing.
-->
&lt;p>许多 go-logr 实现都有非常相似的单元测试，它们检查某些日志调用的结果。
如果开发人员不知道某些警告，例如调用时会出现恐慌的 &lt;code>String&lt;/code> 函数，那么很可能缺少对此类警告的处理和单元测试。&lt;/p>
&lt;!--
[`klog.test`](https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/test) is a reusable set
of test cases that can be applied to a go-logr implementation.
-->
&lt;p>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/test">&lt;code>klog.test&lt;/code>&lt;/a> 是一组可重用的测试用例，可应用于 go-logr 实现。&lt;/p>
&lt;!--
### Output flushing
klog used to start a goroutine unconditionally during `init` which flushed
buffered data at a hard-coded interval. Now that goroutine is only started on
demand (i.e. when writing to files with buffering) and can be controlled with
[`StopFlushDaemon`](https://pkg.go.dev/k8s.io/klog/v2#StopFlushDaemon) and
[`StartFlushDaemon`](https://pkg.go.dev/k8s.io/klog/v2#StartFlushDaemon).
-->
&lt;h3 id="输出刷新">输出刷新&lt;/h3>
&lt;p>klog 用于在 &lt;code>init&lt;/code> 期间无条件地启动一个 goroutine，它以硬编码的时间间隔刷新缓冲数据。
现在 goroutine 仅按需启动（即当写入具有缓冲的文件时）并且可以使用 &lt;a href="https://pkg.go.dev/k8s.io/klog/v2#StopFlushDaemon">&lt;code>StopFlushDaemon&lt;/code>&lt;/a>
和 &lt;a href="https://pkg.go.dev/k8s.io/klog/v2#StartFlushDaemon">&lt;code>StartFlushDaemon&lt;/code>&lt;/a>。&lt;/p>
&lt;!--
When a go-logr implementation buffers data, flushing that data can be
integrated into [`klog.Flush`](https://pkg.go.dev/k8s.io/klog/v2#Flush) by
registering the logger with the
[`FlushLogger`](https://pkg.go.dev/k8s.io/klog/v2#FlushLogger) option.
-->
&lt;p>当 go-logr 实现缓冲数据时，可以通过使用 &lt;a href="https://pkg.go.dev/k8s.io/klog/v2#FlushLogger">&lt;code>FlushLogger&lt;/code>&lt;/a>
选项注册记录器来将刷新该数据集成到 &lt;a href="https://pkg.go.dev/k8s.io/klog/v2#Flush">&lt;code>klog.Flush&lt;/code>&lt;/a> 中。&lt;/p>
&lt;!--
### Various other changes
For a description of all other enhancements see in the [release notes](https://github.com/kubernetes/klog/releases).
-->
&lt;h3 id="其他各种变化">其他各种变化&lt;/h3>
&lt;p>有关所有其他增强功能的描述，请参见 &lt;a href="https://github.com/kubernetes/klog/releases">发行说明&lt;/a>。&lt;/p>
&lt;!--
## logcheck
-->
&lt;h2 id="日志检查">日志检查&lt;/h2>
&lt;!--
Originally designed as a linter for structured log calls, the
[`logcheck`](https://github.com/kubernetes/klog/tree/788efcdee1e9be0bfbe5b076343d447314f2377e/hack/tools/logcheck)
tool has been enhanced to support also contextual logging and traditional klog
log calls. These enhanced checks already found bugs in Kubernetes, like calling
`klog.Info` instead of `klog.Infof` with a format string and parameters.
-->
&lt;p>最初设计为结构化日志调用的 linter，[&lt;code>logcheck&lt;/code>] 工具已得到增强，还支持上下文日志记录和传统的 klog 日志调用。
这些增强检查已经在 Kubernetes 中发现了错误，例如使用格式字符串和参数调用 &lt;code>klog.Info&lt;/code> 而不是 &lt;code>klog.Infof&lt;/code>。&lt;/p>
&lt;!--
It can be included as a plugin in a `golangci-lint` invocation, which is how
[Kubernetes uses it now](https://github.com/kubernetes/kubernetes/commit/17e3c555c5115f8c9176bae10ba45baa04d23a7b),
or get invoked stand-alone.
-->
&lt;p>它可以作为插件包含在 &lt;code>golangci-lint&lt;/code> 调用中，这就是
&lt;a href="https://github.com/kubernetes/kubernetes/commit/17e3c555c5115f8c9176bae10ba45baa04d23a7b">Kubernetes 现在使用它的方式&lt;/a>，或者单独调用。&lt;/p>
&lt;!--
We are in the process of [moving the tool](https://github.com/kubernetes/klog/issues/312) into a new repository because it isn't
really related to klog and its releases should be tracked and tagged properly.
-->
&lt;p>我们正在 &lt;a href="https://github.com/kubernetes/klog/issues/312">移动工具&lt;/a>
到一个新的存储库中，因为它与 klog 没有真正的关系，并且应该正确跟踪和标记它的发布。&lt;/p>
&lt;!--
## Next steps
The [Structured Logging WG](https://github.com/kubernetes/community/tree/master/wg-structured-logging)
is always looking for new contributors. The migration
away from C-style logging is now going to target structured, contextual logging
in one step to reduce the overall code churn and number of PRs. Changing log
calls is good first contribution to Kubernetes and an opportunity to get to
know code in various different areas.
-->
&lt;h2 id="下一步">下一步&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/wg-structured-logging">Structured Logging WG&lt;/a>
一直在寻找新的贡献者。 从 C 风格的日志记录迁移现在将一步一步地针对结构化的上下文日志记录，
以减少整体代码流失和 PR 数量。 更改日志调用是对 Kubernetes 的良好贡献，也是了解各个不同领域代码的机会。&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: 避免为 Services 分配 IP 地址时发生冲突</title><link>https://kubernetes.io/zh-cn/blog/2022/05/23/service-ip-dynamic-and-static-allocation/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/23/service-ip-dynamic-and-static-allocation/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes 1.24: Avoid Collisions Assigning IP Addresses to Services"
date: 2022-05-23
slug: service-ip-dynamic-and-static-allocation
-->
&lt;!--
**Author:** Antonio Ojea (Red Hat)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Antonio Ojea (Red Hat)&lt;/p>
&lt;!--
In Kubernetes, [Services](/docs/concepts/services-networking/service/) are an abstract way to expose
an application running on a set of Pods. Services
can have a cluster-scoped virtual IP address (using a Service of `type: ClusterIP`).
Clients can connect using that virtual IP address, and Kubernetes then load-balances traffic to that
Service across the different backing Pods.
-->
&lt;p>在 Kubernetes 中，&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/">Services&lt;/a>
是一种抽象，用来暴露运行在一组 Pod 上的应用。
Service 可以有一个集群范围的虚拟 IP 地址（使用 &lt;code>type: ClusterIP&lt;/code> 的 Service）。
客户端可以使用该虚拟 IP 地址进行连接， Kubernetes 为对该 Service 的访问流量提供负载均衡，以访问不同的后端 Pod。&lt;/p>
&lt;!--
## How Service ClusterIPs are allocated?
-->
&lt;h2 id="service-clusterip-是如何分配的">Service ClusterIP 是如何分配的？&lt;/h2>
&lt;!--
A Service `ClusterIP` can be assigned:
-->
&lt;p>Service &lt;code>ClusterIP&lt;/code> 有如下分配方式：&lt;/p>
&lt;!--
_dynamically_
: the cluster's control plane automatically picks a free IP address from within the configured IP range for `type: ClusterIP` Services.
-->
&lt;p>&lt;strong>动态&lt;/strong>
：群集的控制平面会自动从配置的 IP 范围内为 &lt;code>type:ClusterIP&lt;/code> 的 Service 选择一个空闲 IP 地址。&lt;/p>
&lt;!--
_statically_
: you specify an IP address of your choice, from within the configured IP range for Services.
-->
&lt;p>&lt;strong>静态&lt;/strong>
：你可以指定一个来自 Service 配置的 IP 范围内的 IP 地址。&lt;/p>
&lt;!--
Across your whole cluster, every Service `ClusterIP` must be unique.
Trying to create a Service with a specific `ClusterIP` that has already
been allocated will return an error.
-->
&lt;p>在整个集群中，每个 Service 的 &lt;code>ClusterIP&lt;/code> 必须是唯一的。
尝试创建一个已经被分配了的 &lt;code>ClusterIP&lt;/code> 的 Service 将会返回错误。&lt;/p>
&lt;!--
## Why do you need to reserve Service Cluster IPs?
-->
&lt;h2 id="为什么需要预留-service-cluster-ip">为什么需要预留 Service Cluster IP？&lt;/h2>
&lt;!--
Sometimes you may want to have Services running in well-known IP addresses, so other components and
users in the cluster can use them.
-->
&lt;p>有时，你可能希望让 Service 运行在众所周知的 IP 地址上，以便集群中的其他组件和用户可以使用它们。&lt;/p>
&lt;!--
The best example is the DNS Service for the cluster. Some Kubernetes installers assign the 10th address from
the Service IP range to the DNS service. Assuming you configured your cluster with Service IP range
10.96.0.0/16 and you want your DNS Service IP to be 10.96.0.10, you'd have to create a Service like
this:
-->
&lt;p>最好的例子是集群的 DNS Service。一些 Kubernetes 安装程序将 Service IP 范围中的第 10 个地址分配给 DNS Service。
假设你配置集群 Service IP 范围是 10.96.0.0/16，并且希望 DNS Service IP 为 10.96.0.10，
那么你必须创建一个如下所示的 Service：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-dns&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kubernetes.io/name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CoreDNS&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-dns&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">clusterIP&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10.96.0.10&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dns&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">53&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>UDP&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">53&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dns-tcp&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">53&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TCP&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">53&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-dns&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterIP&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
but as I explained before, the IP address 10.96.0.10 has not been reserved; if other Services are created
before or in parallel with dynamic allocation, there is a chance they can allocate this IP, hence,
you will not be able to create the DNS Service because it will fail with a conflict error.
-->
&lt;p>但正如我之前解释的，IP 地址 10.96.0.10 没有被保留；
如果其他 Service 在动态分配之前创建或与动态分配并行创建，则它们有可能分配此 IP 地址，
因此，你将无法创建 DNS Service，因为它将因冲突错误而失败。&lt;/p>
&lt;!--
## How can you avoid Service ClusterIP conflicts? {#avoid-ClusterIP-conflict}
-->
&lt;h2 id="avoid-ClusterIP-conflict">如何避免 Service ClusterIP 冲突？&lt;/h2>
&lt;!--
In Kubernetes 1.24, you can enable a new feature gate `ServiceIPStaticSubrange`.
Turning this on allows you to use a different IP
allocation strategy for Services, reducing the risk of collision.
-->
&lt;p>在 Kubernetes 1.24 中，你可以启用一个新的特性门控 &lt;code>ServiceIPStaticSubrange&lt;/code>。
启用此特性允许你为 Service 使用不同的 IP 分配策略，减少冲突的风险。&lt;/p>
&lt;!--
The `ClusterIP` range will be divided, based on the formula `min(max(16, cidrSize / 16), 256)`,
described as _never less than 16 or more than 256 with a graduated step between them_.
-->
&lt;p>&lt;code>ClusterIP&lt;/code> 范围将根据公式 &lt;code>min(max(16, cidrSize / 16), 256)&lt;/code> 进行划分，
该公式可描述为 “在不小于 16 且不大于 256 之间有一个步进量（Graduated Step）”。&lt;/p>
&lt;!--
Dynamic IP assignment will use the upper band by default, once this has been exhausted it will
use the lower range. This will allow users to use static allocations on the lower band with a low
risk of collision.
-->
&lt;p>分配默认使用上半段地址，当上半段地址耗尽后，将使用下半段地址范围。
这将允许用户使用下半段地址中静态分配的地址并且降低冲突的风险。&lt;/p>
&lt;!--
Examples:
-->
&lt;p>举例：&lt;/p>
&lt;!--
#### Service IP CIDR block: 10.96.0.0/24
-->
&lt;h4 id="service-ip-cidr-地址段-10-96-0-0-24">Service IP CIDR 地址段： 10.96.0.0/24&lt;/h4>
&lt;!--
Range Size: 2&lt;sup>8&lt;/sup> - 2 = 254
Band Offset: `min(max(16, 256/16), 256)` = `min(16, 256)` = 16
Static band start: 10.96.0.1
Static band end: 10.96.0.16
Range end: 10.96.0.254
-->
&lt;p>地址段大小：2&lt;sup>8&lt;/sup> - 2 = 254&lt;br>
地址段偏移：&lt;code>min(max(16, 256/16), 256)&lt;/code> = &lt;code>min(16, 256)&lt;/code> = 16&lt;br>
静态地址段起点：10.96.0.1&lt;br>
静态地址段终点：10.96.0.16&lt;br>
地址范围终点：10.96.0.254&lt;/p>
&lt;!--
&lt;figure>
&lt;div class="mermaid">
pie showData
title 10.96.0.0/24
"Static" : 16
"Dynamic" : 238
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">必须&lt;a href="https://www.enable-javascript.com/">启用&lt;/a> JavaScript 才能查看此页内容&lt;/em>
&lt;/div>
&lt;/noscript>
-->
&lt;figure>
&lt;div class="mermaid">
pie showData
title 10.96.0.0/24
"静态" : 16
"动态" : 238
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">必须&lt;a href="https://www.enable-javascript.com/">启用&lt;/a> JavaScript 才能查看此页内容&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;!--
#### Service IP CIDR block: 10.96.0.0/20
-->
&lt;h4 id="service-ip-cidr-地址段-10-96-0-0-20">Service IP CIDR 地址段： 10.96.0.0/20&lt;/h4>
&lt;!--
Range Size: 2&lt;sup>12&lt;/sup> - 2 = 4094
Band Offset: `min(max(16, 4096/16), 256)` = `min(256, 256)` = 256
Static band start: 10.96.0.1
Static band end: 10.96.1.0
Range end: 10.96.15.254
-->
&lt;p>地址段大小：2&lt;sup>12&lt;/sup> - 2 = 4094&lt;br>
地址段偏移：&lt;code>min(max(16, 4096/16), 256)&lt;/code> = &lt;code>min(256, 256)&lt;/code> = 256&lt;br>
静态地址段起点：10.96.0.1&lt;br>
静态地址段终点：10.96.1.0&lt;br>
地址范围终点：10.96.15.254&lt;/p>
&lt;!--
&lt;figure>
&lt;div class="mermaid">
pie showData
title 10.96.0.0/20
"Static" : 256
"Dynamic" : 3838
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">必须&lt;a href="https://www.enable-javascript.com/">启用&lt;/a> JavaScript 才能查看此页内容&lt;/em>
&lt;/div>
&lt;/noscript>
-->
&lt;figure>
&lt;div class="mermaid">
pie showData
title 10.96.0.0/20
"静态" : 256
"动态" : 3838
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">必须&lt;a href="https://www.enable-javascript.com/">启用&lt;/a> JavaScript 才能查看此页内容&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;!--
#### Service IP CIDR block: 10.96.0.0/16
-->
&lt;h4 id="service-ip-cidr-地址段-10-96-0-0-16">Service IP CIDR 地址段： 10.96.0.0/16&lt;/h4>
&lt;!--
Range Size: 2&lt;sup>16&lt;/sup> - 2 = 65534
Band Offset: `min(max(16, 65536/16), 256)` = `min(4096, 256)` = 256
Static band start: 10.96.0.1
Static band ends: 10.96.1.0
Range end: 10.96.255.254
-->
&lt;p>地址段大小：2&lt;sup>16&lt;/sup> - 2 = 65534&lt;br>
地址段偏移：&lt;code>min(max(16, 65536/16), 256)&lt;/code> = &lt;code>min(4096, 256)&lt;/code> = 256&lt;br>
静态地址段起点：10.96.0.1&lt;br>
静态地址段终点：10.96.1.0&lt;br>
地址范围终点：10.96.255.254&lt;/p>
&lt;!--
&lt;figure>
&lt;div class="mermaid">
pie showData
title 10.96.0.0/16
"Static" : 256
"Dynamic" : 65278
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">必须&lt;a href="https://www.enable-javascript.com/">启用&lt;/a> JavaScript 才能查看此页内容&lt;/em>
&lt;/div>
&lt;/noscript>
-->
&lt;figure>
&lt;div class="mermaid">
pie showData
title 10.96.0.0/16
"静态" : 256
"动态" : 65278
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">必须&lt;a href="https://www.enable-javascript.com/">启用&lt;/a> JavaScript 才能查看此页内容&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;!--
## Get involved with SIG Network
-->
&lt;h2 id="加入-sig-network">加入 SIG Network&lt;/h2>
&lt;!--
The current SIG-Network [KEPs](https://github.com/orgs/kubernetes/projects/10) and [issues](https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork) on GitHub illustrate the SIG’s areas of emphasis.
-->
&lt;p>当前 SIG-Network 在 GitHub 上的 &lt;a href="https://github.com/orgs/kubernetes/projects/10">KEPs&lt;/a> 和
&lt;a href="https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork">issues&lt;/a>
表明了该 SIG 的重点领域。&lt;/p>
&lt;!--
[SIG Network meetings](https://github.com/kubernetes/community/tree/master/sig-network) are a friendly, welcoming venue for you to connect with the community and share your ideas.
Looking forward to hearing from you!
-->
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network 会议&lt;/a>是一个友好、热情的场所，
你可以与社区联系并分享你的想法。期待你的回音！&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: 节点非体面关闭特性进入 Alpha 阶段</title><link>https://kubernetes.io/zh-cn/blog/2022/05/20/kubernetes-1-24-non-graceful-node-shutdown-alpha/</link><pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/20/kubernetes-1-24-non-graceful-node-shutdown-alpha/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes 1.24: Introducing Non-Graceful Node Shutdown Alpha"
date: 2022-05-20
slug: kubernetes-1-24-non-graceful-node-shutdown-alpha
-->
&lt;!--
**Authors** Xing Yang and Yassine Tijani (VMware)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Xing Yang 和 Yassine Tijani (VMware)&lt;/p>
&lt;!--
Kubernetes v1.24 introduces alpha support for [Non-Graceful Node Shutdown](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2268-non-graceful-shutdown).
This feature allows stateful workloads to failover to a different node after the original node is shutdown or in a non-recoverable state such as hardware failure or broken OS.
-->
&lt;p>Kubernetes v1.24 引入了对&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2268-non-graceful-shutdown">节点非体面关闭&lt;/a>
（Non-Graceful Node Shutdown）的 Alpha 支持。
此特性允许有状态工作负载在原节点关闭或处于不可恢复状态（如硬件故障或操作系统损坏）后，故障转移到不同的节点。&lt;/p>
&lt;!--
## How is this different from Graceful Node Shutdown
-->
&lt;h2 id="这与节点体面关闭有何不同">这与节点体面关闭有何不同&lt;/h2>
&lt;!--
You might have heard about the [Graceful Node Shutdown](/docs/concepts/architecture/nodes/#graceful-node-shutdown) capability of Kubernetes,
and are wondering how the Non-Graceful Node Shutdown feature is different from that. Graceful Node Shutdown
allows Kubernetes to detect when a node is shutting down cleanly, and handles that situation appropriately.
A Node Shutdown can be "graceful" only if the node shutdown action can be detected by the kubelet ahead
of the actual shutdown. However, there are cases where a node shutdown action may not be detected by
the kubelet. This could happen either because the shutdown command does not trigger the systemd inhibitor
locks mechanism that kubelet relies upon, or because of a configuration error
(the `ShutdownGracePeriod` and `ShutdownGracePeriodCriticalPods` are not configured properly).
-->
&lt;p>你可能听说过 Kubernetes 的&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/#graceful-node-shutdown">节点体面关闭&lt;/a>特性，
并且想知道节点非体面关闭特性与之有何不同。节点体面关闭允许 Kubernetes 检测节点何时完全关闭，并适当地处理这种情况。
只有当 kubelet 在实际关闭之前检测到节点关闭动作时，节点关闭才是“体面（graceful）”的。
但是，在某些情况下，kubelet 可能检测不到节点关闭操作。
这可能是因为 shutdown 命令没有触发 kubelet 所依赖的 systemd 抑制锁机制，
或者是因为配置错误（&lt;code>ShutdownGracePeriod&lt;/code> 和 &lt;code>ShutdownGracePeriodCriticalPods&lt;/code> 配置不正确）。&lt;/p>
&lt;!--
Graceful node shutdown relies on Linux-specific support. The kubelet does not watch for upcoming
shutdowns on Windows nodes (this may change in a future Kubernetes release).
-->
&lt;p>节点体面关闭依赖于特定 Linux 的支持。
kubelet 不监视 Windows 节点上即将关闭的情况（这可能在未来的 Kubernetes 版本中会有所改变）。&lt;/p>
&lt;!--
When a node is shutdown but without the kubelet detecting it, pods on that node
also shut down ungracefully. For stateless apps, that's often not a problem (a ReplicaSet adds a new pod once
the cluster detects that the affected node or pod has failed). For stateful apps, the story is more complicated.
If you use a StatefulSet and have a pod from that StatefulSet on a node that fails uncleanly, that affected pod
will be marked as terminating; the StatefulSet cannot create a replacement pod because the pod
still exists in the cluster.
As a result, the application running on the StatefulSet may be degraded or even offline. If the original, shut
down node comes up again, the kubelet on that original node reports in, deletes the existing pods, and
the control plane makes a replacement pod for that StatefulSet on a different running node.
If the original node has failed and does not come up, those stateful pods would be stuck in a
terminating status on that failed node indefinitely.
-->
&lt;p>当一个节点被关闭但 kubelet 没有检测到时，该节点上的 Pod 也会非体面地关闭。
对于无状态应用程序，这通常不是问题（一旦集群检测到受影响的节点或 Pod 出现故障，ReplicaSet 就会添加一个新的 Pod）。
对于有状态的应用程序，情况要复杂得多。如果你使用一个 StatefulSet，
并且该 StatefulSet 中的一个 Pod 在某个节点上发生了不干净故障，则该受影响的 Pod 将被标记为终止（Terminating）；
StatefulSet 无法创建替换 Pod，因为该 Pod 仍存在于集群中。
因此，在 StatefulSet 上运行的应用程序可能会降级甚至离线。
如果已关闭的原节点再次出现，该节点上的 kubelet 会执行报到操作，删除现有的 Pod，
并且控制平面会在不同的运行节点上为该 StatefulSet 生成一个替换 Pod。
如果原节点出现故障并且没有恢复，这些有状态的 Pod 将处于终止状态且无限期地停留在该故障节点上。&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get pod -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
web-0 1/1 Running 0 100m 10.244.2.4 k8s-node-876-1639279816 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
web-1 1/1 Terminating 0 100m 10.244.1.3 k8s-node-433-1639279804 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;!--
## Try out the new non-graceful shutdown handling
To use the non-graceful node shutdown handling, you must enable the `NodeOutOfServiceVolumeDetach`
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) for the `kube-controller-manager`
component.
-->
&lt;h2 id="尝试新的非体面关闭处理">尝试新的非体面关闭处理&lt;/h2>
&lt;p>要使用节点非体面关闭处理，你必须为 &lt;code>kube-controller-manager&lt;/code> 组件启用
&lt;code>NodeOutOfServiceVolumeDetach&lt;/code> &lt;a href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/feature-gates/">特性门控&lt;/a>。&lt;/p>
&lt;!--
In the case of a node shutdown, you can manually taint that node as out of service. You should make certain that
the node is truly shutdown (not in the middle of restarting) before you add that taint. You could add that
taint following a shutdown that the kubelet did not detect and handle in advance; another case where you
can use that taint is when the node is in a non-recoverable state due to a hardware failure or a broken OS.
The values you set for that taint can be `node.kubernetes.io/out-of-service=nodeshutdown: "NoExecute"`
or `node.kubernetes.io/out-of-service=nodeshutdown:" NoSchedule"`.
Provided you have enabled the feature gate mentioned earlier, setting the out-of-service taint on a Node
means that pods on the node will be deleted unless if there are matching tolerations on the pods.
Persistent volumes attached to the shutdown node will be detached, and for StatefulSets, replacement pods will
be created successfully on a different running node.
-->
&lt;p>在节点关闭的情况下，你可以手动为该节点标记污点，标示其已停止服务。
在添加污点之前，你应该确保节点确实关闭了（不是在重启过程中）。
你可以在发生了节点关闭事件，且该事件没有被 kubelet 提前检测和处理的情况下，在节点关闭之后添加污点；
你可以使用该污点的另一种情况是当节点由于硬件故障或操作系统损坏而处于不可恢复状态时。
你可以为该污点设置的值是 &lt;code>node.kubernetes.io/out-of-service=nodeshutdown: &amp;quot;NoExecute&amp;quot;&lt;/code> 或
&lt;code>node.kubernetes.io/out-of-service=nodeshutdown: &amp;quot;NoSchedule&amp;quot;&lt;/code>。
如果你已经启用了前面提到的特性门控，在节点上设置 out-of-service 污点意味着节点上的 Pod 将被删除，
除非 Pod 上设置有与之匹配的容忍度。原来挂接到已关闭节点的持久卷（Persistent volume）将被解除挂接，
对于 StatefulSet，系统将在不同的运行节点上成功创建替换 Pod。&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl taint nodes &amp;lt;node-name&amp;gt; node.kubernetes.io/out-of-service=nodeshutdown:NoExecute
$ kubectl get pod -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
web-0 1/1 Running 0 150m 10.244.2.4 k8s-node-876-1639279816 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
web-1 1/1 Running 0 10m 10.244.1.7 k8s-node-433-1639279804 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;!--
Note: Before applying the out-of-service taint, you **must** verify that a node is already in shutdown or power off state (not in the middle of restarting),
either because the user intentionally shut it down or the node is down due to hardware failures, OS issues, etc.
-->
&lt;p>注意：在应用 out-of-service 污点之前，你&lt;strong>必须&lt;/strong>确认节点是否已经处于关闭或断电状态（不是在重新启动过程中），
节点关闭的原因可能是用户有意将其关闭，也可能是节点由于硬件故障、操作系统问题等而关闭。&lt;/p>
&lt;!--
Once all the workload pods that are linked to the out-of-service node are moved to a new running node, and the shutdown node has been recovered, you should remove
that taint on the affected node after the node is recovered.
If you know that the node will not return to service, you could instead delete the node from the cluster.
-->
&lt;p>一旦关联到无法提供服务的节点的所有工作负载 Pod 都被移动到新的运行中的节点，并且关闭了的节点也已恢复，
你应该在节点恢复后删除受影响节点上的污点。如果你知道该节点不会恢复服务，则可以改为从集群中删除该节点。&lt;/p>
&lt;!--
## What’s next?
Depending on feedback and adoption, the Kubernetes team plans to push the Non-Graceful Node Shutdown implementation to Beta in either 1.25 or 1.26.
-->
&lt;h2 id="下一步是什么">下一步是什么？&lt;/h2>
&lt;p>根据反馈和采用情况，Kubernetes 团队计划在 1.25 或 1.26 版本中将节点非体面关闭实现推送到 Beta 阶段。&lt;/p>
&lt;!--
This feature requires a user to manually add a taint to the node to trigger workloads failover and remove the taint after the node is recovered.
In the future, we plan to find ways to automatically detect and fence nodes that are shutdown/failed and automatically failover workloads to another node.
-->
&lt;p>此功能需要用户手动向节点添加污点以触发工作负载故障转移，并要求用户在节点恢复后移除污点。
未来，我们计划寻找方法来自动检测和隔离已关闭的或已失败的节点，并自动将工作负载故障转移到另一个节点。&lt;/p>
&lt;!--
## How can I learn more?
Check out the [documentation](/docs/concepts/architecture/nodes/#non-graceful-node-shutdown)
for non-graceful node shutdown.
-->
&lt;h2 id="怎样才能了解更多">怎样才能了解更多？&lt;/h2>
&lt;p>查看节点非体面关闭相关&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/#non-graceful-node-shutdown">文档&lt;/a>。&lt;/p>
&lt;!--
## How to get involved?
-->
&lt;h2 id="如何参与">如何参与？&lt;/h2>
&lt;!--
This feature has a long story. Yassine Tijani ([yastij](https://github.com/yastij)) started the KEP more than two years ago.
Xing Yang ([xing-yang](https://github.com/xing-yang)) continued to drive the effort.
There were many discussions among SIG Storage, SIG Node, and API reviewers to nail down the design details.
Ashutosh Kumar ([sonasingh46](https://github.com/sonasingh46)) did most of the implementation and brought it to Alpha in Kubernetes 1.24.
-->
&lt;p>此功能特性由来已久。Yassine Tijani（&lt;a href="https://github.com/yastij">yastij&lt;/a>）在两年多前启动了这个 KEP。
Xing Yang（&lt;a href="https://github.com/xing-yang">xing-yang&lt;/a>）继续推动这项工作。
SIG-Storage、SIG-Node 和 API 评审人员们进行了多次讨论，以确定设计细节。
Ashutosh Kumar（&lt;a href="https://github.com/sonasingh46">sonasingh46&lt;/a>）
完成了大部分实现并在 Kubernetes 1.24 版本中将其引进到 Alpha 阶段。&lt;/p>
&lt;!--
We want to thank the following people for their insightful reviews: Tim Hockin ([thockin](https://github.com/thockin)) for his guidance on the design,
Jing Xu ([jingxu97](https://github.com/jingxu97)), Hemant Kumar ([gnufied](https://github.com/gnufied)), and Michelle Au ([msau42](https://github.com/msau42)) for reviews from SIG Storage side,
and Mrunal Patel ([mrunalp](https://github.com/mrunalp)), David Porter ([bobbypage](https://github.com/bobbypage)), Derek Carr ([derekwaynecarr](https://github.com/derekwaynecarr)),
and Danielle Endocrimes ([endocrimes](https://github.com/endocrimes)) for reviews from SIG Node side.
-->
&lt;p>我们要感谢以下人员的评审：Tim Hockin（&lt;a href="https://github.com/thockin">thockin&lt;/a>）对设计的指导；
来自 SIG-Storage 的 Jing Xu（&lt;a href="https://github.com/jingxu97">jingxu97&lt;/a>）、
Hemant Kumar（&lt;a href="https://github.com/gnufied">gnufied&lt;/a>）
和 Michelle Au（&lt;a href="https://github.com/msau42">msau42&lt;/a>）的评论；
以及 Mrunal Patel（&lt;a href="https://github.com/mrunalp">mrunalp&lt;/a>）、
David Porter（&lt;a href="https://github.com/bobbypage">bobbypage&lt;/a>）、
Derek Carr（&lt;a href="https://github.com/derekwaynecarr">derekwaynecarr&lt;/a>）
和 Danielle Endocrimes（&lt;a href="https://github.com/endocrimes">endocrimes&lt;/a>）来自 SIG-Node 方面的评论。&lt;/p>
&lt;!--
There are many people who have helped review the design and implementation along the way.
We want to thank everyone who has contributed to this effort including the about 30 people who have reviewed the [KEP](https://github.com/kubernetes/enhancements/pull/1116) and implementation over the last couple of years.
-->
&lt;p>在此过程中，有很多人帮助审查了设计和实现。我们要感谢所有为此做出贡献的人，
包括在过去几年中审核 &lt;a href="https://github.com/kubernetes/enhancements/pull/1116">KEP&lt;/a> 和实现的大约 30 人。&lt;/p>
&lt;!--
This feature is a collaboration between SIG Storage and SIG Node.
For those interested in getting involved with the design and development of any part of the Kubernetes Storage system,
join the [Kubernetes Storage Special Interest Group](https://github.com/kubernetes/community/tree/master/sig-storage) (SIG).
For those interested in getting involved with the design and development of the components that support the controlled interactions between pods and host resources,
join the [Kubernetes Node SIG](https://github.com/kubernetes/community/tree/master/sig-node).
-->
&lt;p>此特性是 SIG-Storage 和 SIG-Node 之间的协作。
对于那些有兴趣参与 Kubernetes Storage 系统任何部分的设计和开发的人，
请加入 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes 存储特别兴趣小组&lt;/a>（SIG-Storage）。
对于那些有兴趣参与支持 Pod 和主机资源之间受控交互的组件的设计和开发的人，
请加入 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">Kubernetes Node SIG&lt;/a>。&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: 防止未经授权的卷模式转换</title><link>https://kubernetes.io/zh-cn/blog/2022/05/18/prevent-unauthorised-volume-mode-conversion-alpha/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/18/prevent-unauthorised-volume-mode-conversion-alpha/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.24: Prevent unauthorised volume mode conversion'
date: 2022-05-18
slug: prevent-unauthorised-volume-mode-conversion-alpha
-->
&lt;!--
**Author:** Raunak Pradip Shah (Mirantis)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Raunak Pradip Shah (Mirantis)&lt;/p>
&lt;!--
Kubernetes v1.24 introduces a new alpha-level feature that prevents unauthorised users
from modifying the volume mode of a [`PersistentVolumeClaim`](/docs/concepts/storage/persistent-volumes/) created from an
existing [`VolumeSnapshot`](/docs/concepts/storage/volume-snapshots/) in the Kubernetes cluster.
-->
&lt;p>Kubernetes v1.24 引入了一个新的 alpha 级特性，可以防止未经授权的用户修改基于 Kubernetes
集群中已有的 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/volume-snapshots/">&lt;code>VolumeSnapshot&lt;/code>&lt;/a>
创建的 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/">&lt;code>PersistentVolumeClaim&lt;/code>&lt;/a> 的卷模式。&lt;/p>
&lt;!--
### The problem
-->
&lt;h3 id="问题">问题&lt;/h3>
&lt;!--
The [Volume Mode](/docs/concepts/storage/persistent-volumes/#volume-mode) determines whether a volume
is formatted into a filesystem or presented as a raw block device.
-->
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#volume-mode">卷模式&lt;/a>确定卷是格式化为文件系统还是显示为原始块设备。&lt;/p>
&lt;!--
Users can leverage the `VolumeSnapshot` feature, which has been stable since Kubernetes v1.20,
to create a `PersistentVolumeClaim` (shortened as PVC) from an existing `VolumeSnapshot` in
the Kubernetes cluster. The PVC spec includes a `dataSource` field, which can point to an
existing `VolumeSnapshot` instance.
Visit [Create a PersistentVolumeClaim from a Volume Snapshot](/docs/concepts/storage/persistent-volumes/#create-persistent-volume-claim-from-volume-snapshot) for more details.
-->
&lt;p>用户可以使用自 Kubernetes v1.20 以来就稳定的 &lt;code>VolumeSnapshot&lt;/code> 功能，
基于 Kubernetes 集群中的已有的 &lt;code>VolumeSnapshot&lt;/code> 创建一个 &lt;code>PersistentVolumeClaim&lt;/code> (简称 PVC )。
PVC 规约包括一个 &lt;code>dataSource&lt;/code> 字段，它可以指向一个已有的 &lt;code>VolumeSnapshot&lt;/code> 实例。
查阅&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#create-persistent-volume-claim-from-volume-snapshot">基于卷快照创建 PVC&lt;/a>
获取更多详细信息。&lt;/p>
&lt;!--
When leveraging the above capability, there is no logic that validates whether the mode of the
original volume, whose snapshot was taken, matches the mode of the newly created volume.
-->
&lt;p>当使用上述功能时，没有逻辑来验证快照所在的原始卷的模式是否与新创建的卷的模式匹配。&lt;/p>
&lt;!--
This presents a security gap that allows malicious users to potentially exploit an
as-yet-unknown vulnerability in the host operating system.
-->
&lt;p>这引起了一个安全漏洞，允许恶意用户潜在地利用主机操作系统中的未知漏洞。&lt;/p>
&lt;!--
Many popular storage backup vendors convert the volume mode during the course of a
backup operation, for efficiency purposes, which prevents Kubernetes from blocking
the operation completely and presents a challenge in distinguishing trusted
users from malicious ones.
-->
&lt;p>为了提高效率，许多流行的存储备份供应商在备份操作过程中转换卷模式，
这使得 Kubernetes 无法完全阻止该操作，并在区分受信任用户和恶意用户方面带来挑战。&lt;/p>
&lt;!--
### Preventing unauthorised users from converting the volume mode
-->
&lt;h3 id="防止未经授权的用户转换卷模式">防止未经授权的用户转换卷模式&lt;/h3>
&lt;!--
In this context, an authorised user is one who has access rights to perform `Update`
or `Patch` operations on `VolumeSnapshotContents`, which is a cluster-level resource.
It is upto the cluster administrator to provide these rights only to trusted users
or applications, like backup vendors.
-->
&lt;p>在这种情况下，授权用户是指有权对 &lt;code>VolumeSnapshotContents&lt;/code>（集群级资源）执行 &lt;code>Update&lt;/code>
或 &lt;code>Patch&lt;/code> 操作的用户。集群管理员只能向受信任的用户或应用程序（如备份供应商）提供这些权限。&lt;/p>
&lt;!--
If the alpha feature is [enabled](https://kubernetes-csi.github.io/docs/) in
`snapshot-controller`, `snapshot-validation-webhook` and `external-provisioner`,
then unauthorised users will not be allowed to modify the volume mode of a PVC
when it is being created from a `VolumeSnapshot`.
-->
&lt;p>如果在 &lt;code>snapshot-controller&lt;/code>、&lt;code>snapshot-validation-webhook&lt;/code> 和
&lt;code>external-provisioner&lt;/code> 中&lt;a href="https://kubernetes-csi.github.io/docs/">启用&lt;/a>了这个 alpha
特性，则基于 &lt;code>VolumeSnapshot&lt;/code> 创建 PVC 时，将不允许未经授权的用户修改其卷模式。&lt;/p>
&lt;!--
To convert the volume mode, an authorised user must do the following:
-->
&lt;p>如要转换卷模式，授权用户必须执行以下操作：&lt;/p>
&lt;!--
1. Identify the `VolumeSnapshot` that is to be used as the data source for a newly
created PVC in the given namespace.
2. Identify the `VolumeSnapshotContent` bound to the above `VolumeSnapshot`.
-->
&lt;ol>
&lt;li>
&lt;p>确定要用作给定命名空间中新创建 PVC 的数据源的 &lt;code>VolumeSnapshot&lt;/code>。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>确定绑定到上面 &lt;code>VolumeSnapshot&lt;/code> 的 &lt;code>VolumeSnapshotContent&lt;/code>。&lt;/p>
&lt;pre tabindex="0">&lt;code> kubectl get volumesnapshot -n &amp;lt;namespace&amp;gt;
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ol>
&lt;!--
3. Add the annotation [`snapshot.storage.kubernetes.io/allowVolumeModeChange`](/docs/reference/labels-annotations-taints/#snapshot-storage-kubernetes-io-allowvolumemodechange)
to the `VolumeSnapshotContent`.
-->
&lt;ol start="3">
&lt;li>给 &lt;code>VolumeSnapshotContent&lt;/code> 添加
&lt;a href="https://kubernetes.io/zh-cn/docs/reference/labels-annotations-taints/#snapshot-storage-kubernetes-io-allowvolumemodechange">&lt;code>snapshot.storage.kubernetes.io/allowVolumeModeChange&lt;/code>&lt;/a>
注解。&lt;/li>
&lt;/ol>
&lt;!--
4.This annotation can be added either via software or manually by the authorised
user. The `VolumeSnapshotContent` annotation must look like following manifest fragment:
-->
&lt;ol start="4">
&lt;li>
&lt;p>此注解可通过软件添加或由授权用户手动添加。&lt;code>VolumeSnapshotContent&lt;/code> 注解必须类似于以下清单片段：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">snapshot.storage.kubernetes.io/allowVolumeModeChange&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ol>
&lt;!--
**Note**: For pre-provisioned `VolumeSnapshotContents`, you must take an extra
step of setting `spec.sourceVolumeMode` field to either `Filesystem` or `Block`,
depending on the mode of the volume from which this snapshot was taken.
-->
&lt;p>&lt;strong>注意&lt;/strong>：对于预先制备的 &lt;code>VolumeSnapshotContents&lt;/code>，你必须采取额外的步骤设置 &lt;code>spec.sourceVolumeMode&lt;/code>
字段为 &lt;code>Filesystem&lt;/code> 或 &lt;code>Block&lt;/code>，这取决于快照所在卷的模式。&lt;/p>
&lt;!--
An example is shown below:
-->
&lt;p>如下为一个示例：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">snapshot.storage.kubernetes.io/allowVolumeModeChange&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>new-snapshot-content-test&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hostpath.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>7bdd0de3-aaeb-11e8-9aae-0242ac110002&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">sourceVolumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>new-snapshot-test&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Repeat steps 1 to 3 for all `VolumeSnapshotContents` whose volume mode needs to be
converted during a backup or restore operation.
-->
&lt;p>对于在备份或恢复操作期间需要转换卷模式的所有 &lt;code>VolumeSnapshotContents&lt;/code>，重复步骤 1 到 3。&lt;/p>
&lt;!--
If the annotation shown in step 4 above is present on a `VolumeSnapshotContent`
object, Kubernetes will not prevent the volume mode from being converted.
Users should keep this in mind before they attempt to add the annotation
to any `VolumeSnapshotContent`.
-->
&lt;p>如果 &lt;code>VolumeSnapshotContent&lt;/code> 对象上存在上面步骤 4 中显示的注解，Kubernetes 将不会阻止转换卷模式。
用户在尝试将注解添加到任何 &lt;code>VolumeSnapshotContent&lt;/code> 之前，应该记住这一点。&lt;/p>
&lt;!--
### What's next
-->
&lt;h3 id="接下来">接下来&lt;/h3>
&lt;!--
[Enable this feature](https://kubernetes-csi.github.io/docs/) and let us know
what you think!
-->
&lt;p>&lt;a href="https://kubernetes-csi.github.io/docs/">启用此特性&lt;/a>并让我们知道你的想法!&lt;/p>
&lt;!--
We hope this feature causes no disruption to existing workflows while preventing
malicious users from exploiting security vulnerabilities in their clusters.
-->
&lt;p>我们希望此功能不会中断现有工作流程，同时防止恶意用户利用集群中的安全漏洞。&lt;/p>
&lt;!--
For any issues, create a thread in the #sig-storage slack channel or an issue
in the CSI external-snapshotter [repository](https://github.com/kubernetes-csi/external-snapshotter).
-->
&lt;p>若有任何问题，请在 #sig-storage slack 频道中创建一个会话，
或在 CSI 外部快照存储&lt;a href="https://github.com/kubernetes-csi/external-snapshotter">仓库&lt;/a>中报告一个 issue。&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: 卷填充器功能进入 Beta 阶段</title><link>https://kubernetes.io/zh-cn/blog/2022/05/16/volume-populators-beta/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/16/volume-populators-beta/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes 1.24: Volume Populators Graduate to Beta"
date: 2022-05-16
slug: volume-populators-beta
-->
&lt;!--
**Author:**
Ben Swartzlander (NetApp)
-->
&lt;p>&lt;strong>作者：&lt;/strong>
Ben Swartzlander (NetApp)&lt;/p>
&lt;!--
The volume populators feature is now two releases old and entering beta! The `AnyVolumeDataSource` feature
gate defaults to enabled in Kubernetes v1.24, which means that users can specify any custom resource
as the data source of a PVC.
-->
&lt;p>卷填充器功能现在已经经历两个发行版本并进入 Beta 阶段！
在 Kubernetes v1.24 中 &lt;code>AnyVolumeDataSource&lt;/code> 特性门控默认被启用。
这意味着用户可以指定任何自定义资源作为 PVC 的数据源。&lt;/p>
&lt;!--
An [earlier blog article](/blog/2021/08/30/volume-populators-redesigned/) detailed how the
volume populators feature works. In short, a cluster administrator can install a CRD and
associated populator controller in the cluster, and any user who can create instances of
the CR can create pre-populated volumes by taking advantage of the populator.
-->
&lt;p>&lt;a href="https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/">之前的一篇博客&lt;/a>详细介绍了卷填充器功能的工作原理。
简而言之，集群管理员可以在集群中安装 CRD 和相关的填充器控制器，
任何可以创建 CR 实例的用户都可以利用填充器创建预填充卷。&lt;/p>
&lt;!--
Multiple populators can be installed side by side for different purposes. The SIG storage
community is already seeing some implementations in public, and more prototypes should
appear soon.
-->
&lt;p>出于不同的目的，可以一起安装多个填充器。存储 SIG 社区已经有了一些公开的实现，更多原型应该很快就会出现。&lt;/p>
&lt;!--
Cluster administrations are **strongly encouraged** to install the
volume-data-source-validator controller and associated `VolumePopulator` CRD before installing
any populators so that users can get feedback about invalid PVC data sources.
-->
&lt;p>&lt;strong>强烈建议&lt;/strong>集群管理人员在安装任何填充器之前安装 volume-data-source-validator 控制器和相关的
&lt;code>VolumePopulator&lt;/code> CRD，以便用户可以获得有关无效 PVC 数据源的反馈。&lt;/p>
&lt;!--
## New Features
-->
&lt;h2 id="新功能">新功能&lt;/h2>
&lt;!--
The [lib-volume-populator](https://github.com/kubernetes-csi/lib-volume-populator) library
on which populators are built now includes metrics to help operators monitor and detect
problems. This library is now beta and latest release is v1.0.1.
-->
&lt;p>构建填充器的 &lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a>
库现在包含可帮助操作员监控和检测问题的指标。这个库现在是 beta 阶段，最新版本是 v1.0.1。&lt;/p>
&lt;!--
The [volume data source validator](https://github.com/kubernetes-csi/volume-data-source-validator)
controller also has metrics support added, and is in beta. The `VolumePopulator` CRD is
beta and the latest release is v1.0.1.
-->
&lt;p>&lt;a href="https://github.com/kubernetes-csi/volume-data-source-validator">卷数据源校验器&lt;/a>控制器也添加了指标支持，
处于 beta 阶段。&lt;code>VolumePopulator&lt;/code> CRD 是 beta 阶段，最新版本是 v1.0.1。&lt;/p>
&lt;!--
## Trying it out
-->
&lt;h2 id="尝试一下">尝试一下&lt;/h2>
&lt;!--
To see how this works, you can install the sample "hello" populator and try it
out.
-->
&lt;p>要查看它是如何工作的，你可以安装 “hello” 示例填充器并尝试一下。&lt;/p>
&lt;!--
First install the volume-data-source-validator controller.
-->
&lt;p>首先安装 volume-data-source-validator 控制器。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/client/config/crd/populator.storage.k8s.io_volumepopulators.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/rbac-data-source-validator.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/setup-data-source-validator.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Next install the example populator.
-->
&lt;p>接下来安装 hello 示例填充器。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/v1.0.1/example/hello-populator/crd.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/87a47467b86052819e9ad13d15036d65b9a32fbb/example/hello-populator/deploy.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Your cluster now has a new CustomResourceDefinition that provides a test API named Hello.
Create an instance of the `Hello` custom resource, with some text:
-->
&lt;p>你的集群现在有一个新的 CustomResourceDefinition，它提供了一个名为 Hello 的测试 API。
创建一个 &lt;code>Hello&lt;/code> 自定义资源的实例，内容如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.example.com/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileContents&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello, world!&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Create a PVC that refers to that CR as its data source.
-->
&lt;p>创建一个将该 CR 引用为其数据源的 PVC。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10Mi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSourceRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.example.com&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Next, run a Job that reads the file in the PVC.
-->
&lt;p>接下来，运行一个读取 PVC 中文件的 Job。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-job&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-container&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>busybox:latest&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- cat&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- /mnt/example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/mnt&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeClaim&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Wait for the job to complete (including all of its dependencies).
-->
&lt;p>等待 Job 完成（包括其所有依赖项）。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl &lt;span style="color:#a2f">wait&lt;/span> --for&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b8860b">condition&lt;/span>&lt;span style="color:#666">=&lt;/span>Complete job/example-job
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
And last examine the log from the job.
-->
&lt;p>最后检查 Job 中的日志。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl logs job/example-job
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
The output should be:
-->
&lt;p>输出应该是：&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-terminal" data-lang="terminal">Hello, world!
&lt;/code>&lt;/pre>&lt;!--
Note that the volume already contained a text file with the string contents from
the CR. This is only the simplest example. Actual populators can set up the volume
to contain arbitrary contents.
-->
&lt;p>请注意，该卷已包含一个文本文件，其中包含来自 CR 的字符串内容。这只是最简单的例子。
实际填充器可以将卷设置为包含任意内容。&lt;/p>
&lt;!--
## How to write your own volume populator
-->
&lt;h2 id="如何编写自己的卷填充器">如何编写自己的卷填充器&lt;/h2>
&lt;!--
Developers interested in writing new poplators are encouraged to use the
[lib-volume-populator](https://github.com/kubernetes-csi/lib-volume-populator) library
and to only supply a small controller wrapper around the library, and a pod image
capable of attaching to volumes and writing the appropriate data to the volume.
-->
&lt;p>鼓励有兴趣编写新的填充器的开发人员使用
&lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a> 库，
只提供一个小型控制器，以及一个能够连接到卷并向卷写入适当数据的 Pod 镜像。&lt;/p>
&lt;!--
Individual populators can be extremely generic such that they work with every type
of PVC, or they can do vendor specific things to rapidly fill a volume with data
if the volume was provisioned by a specific CSI driver from the same vendor, for
example, by communicating directly with the storage for that volume.
-->
&lt;p>单个填充器非常通用，它们可以与所有类型的 PVC 一起使用，
或者如果卷是来自同一供应商的特定 CSI 驱动程序供应的，
它们可以执行供应商特定的的操作以快速用数据填充卷，例如，通过通信直接使用该卷的存储。&lt;/p>
&lt;!--
## How can I learn more?
-->
&lt;h2 id="我怎样才能了解更多">我怎样才能了解更多？&lt;/h2>
&lt;!--
The enhancement proposal,
[Volume Populators](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators), includes lots of detail about the history and technical implementation
of this feature.
-->
&lt;p>增强提案，
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators">卷填充器&lt;/a>，
包含有关此功能的历史和技术实现的许多详细信息。&lt;/p>
&lt;!--
[Volume populators and data sources](/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources), within the documentation topic about persistent volumes,
explains how to use this feature in your cluster.
-->
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources">卷填充器与数据源&lt;/a>,
在有关持久卷的文档主题中，解释了如何在集群中使用此功能。&lt;/p>
&lt;!--
Please get involved by joining the Kubernetes storage SIG to help us enhance this
feature. There are a lot of good ideas already and we'd be thrilled to have more!
-->
&lt;p>请加入 Kubernetes 的存储 SIG，帮助我们增强这一功能。这里已经有很多好的主意了，我们很高兴能有更多！&lt;/p></description></item><item><title>Blog: Kubernetes 1.24：gRPC 容器探针功能进入 Beta 阶段</title><link>https://kubernetes.io/zh-cn/blog/2022/05/13/grpc-probes-now-in-beta/</link><pubDate>Fri, 13 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/13/grpc-probes-now-in-beta/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes 1.24: gRPC container probes in beta"
date: 2022-05-13
slug: grpc-probes-now-in-beta
-->
&lt;!--
**Author**: Sergey Kanzhelev (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>：Sergey Kanzhelev (Google)&lt;/p>
&lt;!--
With Kubernetes 1.24 the gRPC probes functionality entered beta and is available by default.
Now you can configure startup, liveness, and readiness probes for your gRPC app
without exposing any HTTP endpoint, nor do you need an executable. Kubernetes can natively connect to your your workload via gRPC and query its status.
-->
&lt;p>在 Kubernetes 1.24 中，gRPC 探针（probe）功能进入了 beta 阶段，默认情况下可用。
现在，你可以为 gRPC 应用程序配置启动、活跃和就绪探测，而无需公开任何 HTTP 端点，
也不需要可执行文件。Kubernetes 可以通过 gRPC 直接连接到你的工作负载并查询其状态。&lt;/p>
&lt;!--
## Some history
It's useful to let the system managing your workload check that the app is
healthy, has started OK, and whether the app considers itself good to accept
traffic. Before the gRPC support was added, Kubernetes already allowed you to
check for health based on running an executable from inside the container image,
by making an HTTP request, or by checking whether a TCP connection succeeded.
-->
&lt;h2 id="一些历史">一些历史&lt;/h2>
&lt;p>让管理你的工作负载的系统检查应用程序是否健康、启动是否正常，以及应用程序是否认为自己可以接收流量，是很有用的。
在添加 gRPC 探针支持之前，Kubernetes 已经允许你通过从容器镜像内部运行可执行文件、发出 HTTP
请求或检查 TCP 连接是否成功来检查健康状况。&lt;/p>
&lt;!--
For most apps, those checks are enough. If your app provides a gRPC endpoint
for a health (or readiness) check, it is easy
to repurpose the `exec` probe to use it for gRPC health checking.
In the blog article [Health checking gRPC servers on Kubernetes](/blog/2018/10/01/health-checking-grpc-servers-on-kubernetes/),
Ahmet Alp Balkan described how you can do that — a mechanism that still works today.
-->
&lt;p>对于大多数应用程序来说，这些检查就足够了。如果你的应用程序提供了用于运行状况（或准备就绪）检查的
gRPC 端点，则很容易重新调整 &lt;code>exec&lt;/code> 探针的用途，将其用于 gRPC 运行状况检查。
在博文&lt;a href="https://kubernetes.io/zh-cn/blog/2018/10/01/health-checking-grpc-servers-on-kubernetes/">在 Kubernetes 上对 gRPC 服务器进行健康检查&lt;/a>中，
Ahmet Alp Balkan 描述了如何做到这一点 —— 这种机制至今仍在工作。&lt;/p>
&lt;!--
There is a commonly used tool to enable this that was [created](https://github.com/grpc-ecosystem/grpc-health-probe/commit/2df4478982e95c9a57d5fe3f555667f4365c025d)
on August 21, 2018, and with
the first release at [Sep 19, 2018](https://github.com/grpc-ecosystem/grpc-health-probe/releases/tag/v0.1.0-alpha.1).
-->
&lt;p>2018 年 8 月 21 日所&lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/commit/2df4478982e95c9a57d5fe3f555667f4365c025d">创建&lt;/a>的一种常用工具可以启用此功能，
工具于 &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/releases/tag/v0.1.0-alpha.1">2018 年 9 月 19 日&lt;/a>首次发布。&lt;/p>
&lt;!--
This approach for gRPC apps health checking is very popular. There are [3,626 Dockerfiles](https://github.com/search?l=Dockerfile&amp;q=grpc_health_probe&amp;type=code)
with the `grpc_health_probe` and [6,621 yaml](https://github.com/search?l=YAML&amp;q=grpc_health_probe&amp;type=Code) files that are discovered with the
basic search on GitHub (at the moment of writing). This is good indication of the tool popularity
and the need to support this natively.
-->
&lt;p>这种 gRPC 应用健康检查的方法非常受欢迎。使用 GitHub 上的基本搜索，发现了带有 &lt;code>grpc_health_probe&lt;/code>
的 &lt;a href="https://github.com/search?l=Dockerfile&amp;amp;q=grpc_health_probe&amp;amp;type=code">3,626 个 Dockerfile 文件&lt;/a>和
&lt;a href="https://github.com/search?l=YAML&amp;amp;q=grpc_health_probe&amp;amp;type=Code">6,621 个 yaml 文件&lt;/a>（在撰写本文时）。
这很好地表明了该工具的受欢迎程度，以及对其本地支持的需求。&lt;/p>
&lt;!--
Kubernetes v1.23 introduced an alpha-quality implementation of native support for
querying a workload status using gRPC. Because it was an alpha feature,
this was disabled by default for the v1.23 release.
-->
&lt;p>Kubernetes v1.23 引入了一个 alpha 质量的实现，原生支持使用 gRPC 查询工作负载状态。
因为这是一个 alpha 特性，所以在 1.23 版中默认是禁用的。&lt;/p>
&lt;!--
## Using the feature
We built gRPC health checking in similar way with other probes and believe
it will be [easy to use](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe)
if you are familiar with other probe types in Kubernetes.
The natively supported health probe has many benefits over the workaround involving `grpc_health_probe` executable.
-->
&lt;h2 id="使用该功能">使用该功能&lt;/h2>
&lt;p>我们用与其他探针类似的方式构建了 gRPC 健康检查，相信如果你熟悉 Kubernetes 中的其他探针类型，
它会&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe">很容易使用&lt;/a>。
与涉及 &lt;code>grpc_health_probe&lt;/code> 可执行文件的解决办法相比，原生支持的健康探针有许多好处。&lt;/p>
&lt;!--
With the native gRPC support you don't need to download and carry `10MB` of an additional executable with your image.
Exec probes are generally slower than a gRPC call as they require instantiating a new process to run an executable.
It also makes the checks less sensible for edge cases when the pod is running at maximum resources and has troubles
instantiating new processes.
-->
&lt;p>有了原生 gRPC 支持，你不需要在镜像中下载和携带 &lt;code>10MB&lt;/code> 的额外可执行文件。
Exec 探针通常比 gRPC 调用慢，因为它们需要实例化一个新进程来运行可执行文件。
当 Pod 在最大资源下运行并且在实例化新进程时遇到困难时，它还使得对边界情况的检查变得不那么智能。&lt;/p>
&lt;!--
There are a few limitations though. Since configuring a client certificate for probes is hard,
services that require client authentication are not supported. The built-in probes are also
not checking the server certificates and ignore related problems.
-->
&lt;p>不过有一些限制。由于为探针配置客户端证书很难，因此不支持依赖客户端身份验证的服务。
内置探针也不检查服务器证书，并忽略相关问题。&lt;/p>
&lt;!--
Built-in checks also cannot be configured to ignore certain types of errors
(`grpc_health_probe` returns different exit codes for different errors),
and cannot be "chained" to run the health check on multiple services in a single probe.
-->
&lt;p>内置检查也不能配置为忽略某些类型的错误（&lt;code>grpc_health_probe&lt;/code> 针对不同的错误返回不同的退出代码），
并且不能“串接”以在单个探测中对多个服务运行健康检查。&lt;/p>
&lt;!--
But all these limitations are quite standard for gRPC and there are easy workarounds
for those.
-->
&lt;p>但是所有这些限制对于 gRPC 来说都是相当标准的，并且有简单的解决方法。&lt;/p>
&lt;!--
## Try it for yourself
### Cluster-level setup
You can try this feature today. To try native gRPC probes, you can spin up a Kubernetes cluster
yourself with the `GRPCContainerProbe` feature gate enabled, there are many [tools available](/docs/tasks/tools/).
-->
&lt;h2 id="自己试试">自己试试&lt;/h2>
&lt;h3 id="集群级设置">集群级设置&lt;/h3>
&lt;p>你现在可以尝试这个功能。要尝试原生 gRPC 探针，你可以自己启动一个启用了
&lt;code>GRPCContainerProbe&lt;/code> 特性门控的 Kubernetes 集群，可用的&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/tools/">工具&lt;/a>有很多。&lt;/p>
&lt;!--
Since the feature gate `GRPCContainerProbe` is enabled by default in 1.24,
many vendors will have this functionality working out of the box.
So you may just create an 1.24 cluster on platform of your choice. Some vendors
allow to enable alpha features on 1.23 clusters.
-->
&lt;p>由于特性门控 &lt;code>GRPCContainerProbe&lt;/code> 在 1.24 版本中是默认启用的，因此许多供应商支持此功能开箱即用。
因此，你可以在自己选择的平台上创建 1.24 版本集群。一些供应商允许在 1.23 版本集群上启用 alpha 特性。&lt;/p>
&lt;!--
For example, at the moment of writing, you can spin up the test cluster on GKE for a quick test.
Other vendors may also have similar capabilities, especially if you
are reading this blog post long after the Kubernetes 1.24 release.
-->
&lt;p>例如，在编写本文时，你可以在 GKE 上运行测试集群来进行快速测试。
其他供应商可能也有类似的功能，尤其是当你在 Kubernetes 1.24 版本发布很久后才阅读这篇博客时。&lt;/p>
&lt;!--
On GKE use the following command (note, version is `1.23` and `enable-kubernetes-alpha` are specified).
-->
&lt;p>在 GKE 上使用以下命令（注意，版本是 &lt;code>1.23&lt;/code>，并且指定了 &lt;code>enable-kubernetes-alpha&lt;/code>）。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>gcloud container clusters create test-grpc &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --enable-kubernetes-alpha &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --no-enable-autorepair &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --no-enable-autoupgrade &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --release-channel&lt;span style="color:#666">=&lt;/span>rapid &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --cluster-version&lt;span style="color:#666">=&lt;/span>1.23
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
You will also need to configure `kubectl` to access the cluster:
-->
&lt;p>你还需要配置 kubectl 来访问集群：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>gcloud container clusters get-credentials test-grpc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Trying the feature out
Let's create the pod to test how gRPC probes work. For this test we will use the `agnhost` image.
This is a k8s maintained image with that can be used for all sorts of workload testing.
For example, it has a useful [grpc-health-checking](https://github.com/kubernetes/kubernetes/blob/b2c5bd2a278288b5ef19e25bf7413ecb872577a4/test/images/agnhost/README.md#grpc-health-checking) module
that exposes two ports - one is serving health checking service,
another - http port to react on commands `make-serving` and `make-not-serving`.
-->
&lt;h3 id="试用该功能">试用该功能&lt;/h3>
&lt;p>让我们创建 Pod 来测试 gRPC 探针是如何工作的。对于这个测试，我们将使用 &lt;code>agnhost&lt;/code> 镜像。
这是一个 k8s 维护的镜像，可用于各种工作负载测试。例如，它有一个有用的
&lt;a href="https://github.com/kubernetes/kubernetes/blob/b2c5bd2a278288b5ef19e25bf7413ecb872577a4/test/images/agnhost/README.md#grpc-health-checking">grpc-health-checking&lt;/a>
模块，该模块暴露了两个端口：一个是提供健康检查服务的端口，另一个是对 &lt;code>make-serving&lt;/code> 和
&lt;code>make-not-serving&lt;/code> 命令做出反应的 http 端口。&lt;/p>
&lt;!--
Here is an example pod definition. It starts the `grpc-health-checking` module,
exposes ports `5000` and `8080`, and configures gRPC readiness probe:
-->
&lt;p>下面是一个 Pod 定义示例。它启用 &lt;code>grpc-health-checking&lt;/code> 模块，暴露 5000 和 8080 端口，并配置 gRPC 就绪探针：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-grpc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>agnhost&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8s.gcr.io/e2e-test-images/agnhost:2.35&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;/agnhost&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;grpc-health-checking&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">5000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">readinessProbe&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">grpc&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">5000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
If the file called `test.yaml`, you can create the pod and check it's status.
The pod will be in ready state as indicated by the snippet of the output.
-->
&lt;p>如果文件名为 &lt;code>test.yaml&lt;/code>，你可以用以下命令创建 Pod，并检查它的状态。如输出片段所示，Pod 将处于就绪状态。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f test.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl describe test-grpc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
The output will contain something like this:
-->
&lt;p>输出将包含如下内容：&lt;/p>
&lt;pre tabindex="0">&lt;code>Conditions:
Type Status
Initialized True
Ready True
ContainersReady True
PodScheduled True
&lt;/code>&lt;/pre>&lt;!--
Now let's change the health checking endpoint status to NOT_SERVING.
In order to call the http port of the Pod, let's create a port forward:
-->
&lt;p>现在让我们将健康检查端点状态更改为 &lt;code>NOT_SERVING&lt;/code>。为了调用 Pod 的 http 端口，让我们创建一个端口转发：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl port-forward test-grpc 8080:8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
You can `curl` to call the command...
-->
&lt;p>你可以用 &lt;code>curl&lt;/code> 来调用这个命令。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>curl http://localhost:8080/make-not-serving
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
... and in a few seconds the port status will switch to not ready.
-->
&lt;p>几秒钟后，端口状态将切换到未就绪。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl describe pod test-grpc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
The output now will have:
-->
&lt;p>现在的输出将显示：&lt;/p>
&lt;pre tabindex="0">&lt;code>Conditions:
Type Status
Initialized True
Ready False
ContainersReady False
PodScheduled True
...
Warning Unhealthy 2s (x6 over 42s) kubelet Readiness probe failed: service unhealthy (responded with &amp;#34;NOT_SERVING&amp;#34;)
&lt;/code>&lt;/pre>&lt;!--
Once it is switched back, in about one second the Pod will get back to ready status:
-->
&lt;p>一旦切换回来，Pod 将在大约一秒钟后恢复到就绪状态：&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-bsh" data-lang="bsh">curl http://localhost:8080/make-serving
kubectl describe test-grpc
&lt;/code>&lt;/pre>&lt;!--
The output indicates that the Pod went back to being `Ready`:
-->
&lt;p>输出表明 Pod 恢复为 &lt;code>Ready&lt;/code>：&lt;/p>
&lt;pre tabindex="0">&lt;code>Conditions:
Type Status
Initialized True
Ready True
ContainersReady True
PodScheduled True
&lt;/code>&lt;/pre>&lt;!--
This new built-in gRPC health probing on Kubernetes makes implementing a health-check via gRPC
much easier than the older approach that relied on using a separate `exec` probe. Read through
the official
[documentation](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe)
to learn more and provide feedback before the feature will be promoted to GA.
-->
&lt;p>Kubernetes 上这种新的内置 gRPC 健康探测，使得通过 gRPC 实现健康检查比依赖使用额外的 &lt;code>exec&lt;/code>
探测的旧方法更容易。请阅读官方
&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe">文档&lt;/a>
了解更多信息并在该功能正式发布（GA）之前提供反馈。&lt;/p>
&lt;!--
## Summary
Kubernetes is a popular workload orchestration platform and we add features based on feedback and demand.
Features like gRPC probes support is a minor improvement that will make life of many app developers
easier and apps more resilient. Try it today and give feedback, before the feature went into GA.
-->
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>Kubernetes 是一个流行的工作负载编排平台，我们根据反馈和需求添加功能。
像 gRPC 探针支持这样的特性是一个小的改进，它将使许多应用程序开发人员的生活更容易，应用程序更有弹性。
在该功能 GA（正式发布）之前，现在就试试，并给出反馈。&lt;/p></description></item><item><title>Blog: Kubernetes 1.24 版本中存储容量跟踪特性进入 GA 阶段</title><link>https://kubernetes.io/zh-cn/blog/2022/05/06/storage-capacity-ga/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/06/storage-capacity-ga/</guid><description>
&lt;!--
layout: blog
title: "Storage Capacity Tracking reaches GA in Kubernetes 1.24"
date: 2022-05-06
slug: storage-capacity-ga
-->
&lt;!--
**Authors:** Patrick Ohly (Intel)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Patrick Ohly（Intel）&lt;/p>
&lt;!--
The v1.24 release of Kubernetes brings [storage capacity](/docs/concepts/storage/storage-capacity/)
tracking as a generally available feature.
-->
&lt;p>在 Kubernetes v1.24 版本中，&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/storage-capacity/">存储容量&lt;/a>跟踪已经成为一项正式发布的功能。&lt;/p>
&lt;!--
## Problems we have solved
-->
&lt;h2 id="已经解决的问题">已经解决的问题&lt;/h2>
&lt;!--
As explained in more detail in the [previous blog post about this
feature](/blog/2021/04/14/local-storage-features-go-beta/), storage capacity
tracking allows a CSI driver to publish information about remaining
capacity. The kube-scheduler then uses that information to pick suitable nodes
for a Pod when that Pod has volumes that still need to be provisioned.
-->
&lt;p>如&lt;a href="https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/">上一篇关于此功能的博文&lt;/a>中所详细介绍的，
存储容量跟踪允许 CSI 驱动程序发布有关剩余容量的信息。当 Pod 仍然有需要配置的卷时，
kube-scheduler 使用该信息为 Pod 选择合适的节点。&lt;/p>
&lt;!--
Without this information, a Pod may get stuck without ever being scheduled onto
a suitable node because kube-scheduler has to choose blindly and always ends up
picking a node for which the volume cannot be provisioned because the
underlying storage system managed by the CSI driver does not have sufficient
capacity left.
-->
&lt;p>如果没有这些信息，Pod 可能会被卡住，而不会被调度到合适节点，这是因为 kube-scheduler
只能盲目地选择节点。由于 CSI 驱动程序管理的下层存储系统没有足够的容量，
kube-scheduler 常常会选择一个无法为其配置卷的节点。&lt;/p>
&lt;!--
Because CSI drivers publish storage capacity information that gets used at a
later time when it might not be up-to-date anymore, it can still happen that a
node is picked that doesn't work out after all. Volume provisioning recovers
from that by informing the scheduler that it needs to try again with a
different node.
-->
&lt;p>因为 CSI 驱动程序发布的这些存储容量信息在被使用的时候可能已经不是最新的信息了，
所以最终选择的节点无法正常工作的情况仍然可能会发生。
卷配置通过通知调度程序需要在其他节点上重试来恢复。&lt;/p>
&lt;!--
[Load
tests](https://github.com/kubernetes-csi/csi-driver-host-path/blob/master/docs/storage-capacity-tracking.md)
that were done again for promotion to GA confirmed that all storage in a
cluster can be consumed by Pods with storage capacity tracking whereas Pods got
stuck without it.
-->
&lt;p>升级到 GA 版本后重新进行的&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/master/docs/storage-capacity-tracking.md">负载测试&lt;/a>证实，
集群中部署了存储容量跟踪功能的 Pod 可以使用所有的存储，而没有部署此功能的 Pod 就会被卡住。&lt;/p>
&lt;!--
## Problems we have *not* solved
-->
&lt;h2 id="尚未-解决的问题">&lt;em>尚未&lt;/em>解决的问题&lt;/h2>
&lt;!--
Recovery from a failed volume provisioning attempt has one known limitation: if a Pod
uses two volumes and only one of them could be provisioned, then all future
scheduling decisions are limited by the already provisioned volume. If that
volume is local to a node and the other volume cannot be provisioned there, the
Pod is stuck. This problem pre-dates storage capacity tracking and while the
additional information makes it less likely to occur, it cannot be avoided in
all cases, except of course by only using one volume per Pod.
-->
&lt;p>如果尝试恢复一个制备失败的卷，存在一个已知的限制：
如果 Pod 使用两个卷并且只能制备其中一个，那么所有将来的调度决策都受到已经制备的卷的限制。
如果该卷是节点的本地卷，并且另一个卷无法被制备，则 Pod 会卡住。
此问题早在存储容量跟踪功能之前就存在，虽然苛刻的附加条件使这种情况不太可能发生，
但是无法完全避免，当然每个 Pod 仅使用一个卷的情况除外。&lt;/p>
&lt;!--
An idea for solving this was proposed in a [KEP
draft](https://github.com/kubernetes/enhancements/pull/1703): volumes that were
provisioned and haven't been used yet cannot have any valuable data and
therefore could be freed and provisioned again elsewhere. SIG Storage is
looking for interested developers who want to continue working on this.
-->
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/pull/1703">KEP 草案&lt;/a>中提出了一个解决此问题的想法：
已制备但尚未被使用的卷不能包含任何有价值的数据，因此可以在其他地方释放并且再次被制备。
SIG Storage 正在寻找对此感兴趣并且愿意继续从事此工作的开发人员。&lt;/p>
&lt;!--
Also not solved is support in Cluster Autoscaler for Pods with volumes. For CSI
drivers with storage capacity tracking, a prototype was developed and discussed
in [a PR](https://github.com/kubernetes/autoscaler/pull/3887). It was meant to
work with arbitrary CSI drivers, but that flexibility made it hard to configure
and slowed down scale up operations: because autoscaler was unable to simulate
volume provisioning, it only scaled the cluster by one node at a time, which
was seen as insufficient.
-->
&lt;p>另一个没有解决的问题是 Cluster Autoscaler 对包含卷的 Pod 的支持。
对于具有存储容量跟踪功能的 CSI 驱动程序，我们开发了一个原型并在此
&lt;a href="https://github.com/kubernetes/autoscaler/pull/3887">PR&lt;/a> 中进行了讨论。
此原型旨在与任意 CSI 驱动程序协同工作，但这种灵活性使其难以配置并减慢了扩展操作：
因为自动扩展程序无法模拟卷制备操作，它一次只能将集群扩展一个节点，这是此方案的不足之处。&lt;/p>
&lt;!--
Therefore that PR was not merged and a different approach with tighter coupling
between autoscaler and CSI driver will be needed. For this a better
understanding is needed about which local storage CSI drivers are used in
combination with cluster autoscaling. Should this lead to a new KEP, then users
will have to try out an implementation in practice before it can move to beta
or GA. So please reach out to SIG Storage if you have an interest in this
topic.
-->
&lt;p>因此，这个 PR 没有被合入，需要另一种不同的方法，在自动缩放器和 CSI 驱动程序之间实现更紧密的耦合。
为此，需要更好地了解哪些本地存储 CSI 驱动程序与集群自动缩放结合使用。如果这会引出新的 KEP，
那么用户将不得不在实践中尝试实现，然后才能迁移到 beta 版本或 GA 版本中。
如果你对此主题感兴趣，请联系 SIG Storage。&lt;/p>
&lt;!--
## Acknowledgements
-->
&lt;h2 id="致谢">致谢&lt;/h2>
&lt;!--
Thanks a lot to the members of the community who have contributed to this
feature or given feedback including members of [SIG
Scheduling](https://github.com/kubernetes/community/tree/master/sig-scheduling),
[SIG
Autoscaling](https://github.com/kubernetes/community/tree/master/sig-autoscaling),
and of course [SIG
Storage](https://github.com/kubernetes/community/tree/master/sig-storage)!
-->
&lt;p>非常感谢为此功能做出贡献或提供反馈的 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-scheduling">SIG Scheduling&lt;/a>、
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">SIG Autoscaling&lt;/a>
和 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG Storage&lt;/a> 成员！&lt;/p></description></item><item><title>Blog: Kubernetes 1.24：卷扩充现在成为稳定功能</title><link>https://kubernetes.io/zh-cn/blog/2022/05/05/volume-expansion-ga/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/05/volume-expansion-ga/</guid><description>
&lt;!--
---
layout: blog
title: "Kubernetes 1.24: Volume Expansion Now A Stable Feature"
date: 2022-05-05
slug: volume-expansion-ga
---
-->
&lt;!--
**Author:** Hemant Kumar (Red Hat)
Volume expansion was introduced as a alpha feature in Kubernetes 1.8 and it went beta in 1.11 and with Kubernetes 1.24 we are excited to announce general availability(GA)
of volume expansion.
This feature allows Kubernetes users to simply edit their `PersistentVolumeClaim` objects and specify new size in PVC Spec and Kubernetes will automatically expand the volume
using storage backend and also expand the underlying file system in-use by the Pod without requiring any downtime at all if possible.
-->
&lt;p>&lt;strong>作者：&lt;/strong> Hemant Kumar (Red Hat)&lt;/p>
&lt;p>卷扩充在 Kubernetes 1.8 作为 Alpha 功能引入，
在 Kubernetes 1.11 进入了 Beta 阶段。
在 Kubernetes 1.24 中，我们很高兴地宣布卷扩充正式发布（GA）。&lt;/p>
&lt;p>此功能允许 Kubernetes 用户简单地编辑其 &lt;code>PersistentVolumeClaim&lt;/code> 对象，
并在 PVC Spec 中指定新的大小，Kubernetes 将使用存储后端自动扩充卷，
同时也会扩充 Pod 使用的底层文件系统，使得无需任何停机时间成为可能。&lt;/p>
&lt;!--
### How to use volume expansion
You can trigger expansion for a PersistentVolume by editing the `spec` field of a PVC, specifying a different
(and larger) storage request. For example, given following PVC:
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
name: myclaim
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1Gi # specify new size here
```
-->
&lt;h3 id="如何使用卷扩充">如何使用卷扩充&lt;/h3>
&lt;p>通过编辑 PVC 的 &lt;code>spec&lt;/code> 字段，指定不同的（和更大的）存储请求，
可以触发 PersistentVolume 的扩充。
例如，给定以下 PVC：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myclaim&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># 在此处指定新的大小&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
You can request expansion of the underlying PersistentVolume by specifying a new value instead of old `1Gi` size.
Once you've changed the requested size, watch the `status.conditions` field of the PVC to see if the
resize has completed.
When Kubernetes starts expanding the volume - it will add `Resizing` condition to the PVC, which will be removed once expansion completes. More information about progress of
expansion operation can also be obtained by monitoring events associated with the PVC:
```bash
kubectl describe pvc &lt;pvc>
```
-->
&lt;p>你可以指定新的值来替代旧的 &lt;code>1Gi&lt;/code> 大小来请求扩充下层 PersistentVolume。
一旦你更改了请求的大小，可以查看 PVC 的 &lt;code>status.conditions&lt;/code> 字段，
确认卷大小的调整是否已完成。&lt;/p>
&lt;p>当 Kubernetes 开始扩充卷时，它会给 PVC 添加 &lt;code>Resizing&lt;/code> 状况。
一旦扩充结束，这个状况会被移除。通过监控与 PVC 关联的事件，
还可以获得更多关于扩充操作进度的信息：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>kubectl describe pvc &amp;lt;pvc&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Storage driver support
Not every volume type however is expandable by default. Some volume types such as - intree hostpath volumes are not expandable at all. For CSI volumes - the CSI driver
must have capability `EXPAND_VOLUME` in controller or node service (or both if appropriate). Please refer to documentation of your CSI driver, to find out
if it supports volume expansion.
Please refer to volume expansion documentation for intree volume types which support volume expansion - [Expanding Persistent Volumes](/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims).
-->
&lt;h3 id="存储驱动支持">存储驱动支持&lt;/h3>
&lt;p>然而，并不是每种卷类型都默认支持扩充。
某些卷类型（如树内 hostpath 卷）不支持扩充。
对于 CSI 卷，
CSI 驱动必须在控制器或节点服务（如果合适，二者兼备）
中具有 &lt;code>EXPAND_VOLUME&lt;/code> 能力。
请参阅 CSI 驱动的文档，了解其是否支持卷扩充。&lt;/p>
&lt;p>有关支持卷扩充的树内（intree）卷类型，
请参阅卷扩充文档：&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">扩充 PVC 申领&lt;/a>。&lt;/p>
&lt;!--
In general to provide some degree of control over volumes that can be expanded, only dynamically provisioned PVCs whose storage class has `allowVolumeExpansion` parameter set to `true` are expandable.
A Kubernetes cluster administrator must edit the appropriate StorageClass object and set
the `allowVolumeExpansion` field to `true`. For example:
-->
&lt;p>通常，为了对可扩充的卷提供某种程度的控制，
只有在存储类将 &lt;code>allowVolumeExpansion&lt;/code> 参数设置为 &lt;code>true&lt;/code> 时，
动态供应的 PVC 才是可扩充的。&lt;/p>
&lt;p>Kubernetes 集群管理员必须编辑相应的 StorageClass 对象，
并将 &lt;code>allowVolumeExpansion&lt;/code> 字段设置为 &lt;code>true&lt;/code>。例如：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StorageClass&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gp2-default&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">provisioner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubernetes.io/aws-ebs&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">secretNamespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">allowVolumeExpansion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Online expansion compared to offline expansion
By default, Kubernetes attempts to expand volumes immediately after user requests a resize.
If one or more Pods are using the volume, Kubernetes tries to expands the volume using an online resize;
as a result volume expansion usually requires no application downtime.
Filesystem expansion on the node is also performed online and hence does not require shutting
down any Pod that was using the PVC.
If you expand a PersistentVolume that is not in use, Kubernetes does an offline resize (and,
because the volume isn't in use, there is again no workload disruption).
-->
&lt;h3 id="在线扩充与离线扩充比较">在线扩充与离线扩充比较&lt;/h3>
&lt;p>默认情况下，Kubernetes 会在用户请求调整大小后立即尝试扩充卷。
如果一个或多个 Pod 正在使用该卷，
Kubernetes 会尝试通过在线调整大小来扩充该卷；
因此，卷扩充通常不需要应用停机。
节点上的文件系统也可以在线扩充，因此不需要关闭任何正在使用 PVC 的 Pod。&lt;/p>
&lt;p>如果要扩充的 PersistentVolume 未被使用，Kubernetes 会用离线方式调整卷大小
（而且，由于该卷未使用，所以也不会造成工作负载中断）。&lt;/p>
&lt;!--
In some cases though - if underlying Storage Driver can only support offline expansion, users of the PVC must take down their Pod before expansion can succeed. Please refer to documentation of your storage
provider to find out - what mode of volume expansion it supports.
When volume expansion was introduced as an alpha feature, Kubernetes only supported offline filesystem
expansion on the node and hence required users to restart their pods for file system resizing to finish.
His behaviour has been changed and Kubernetes tries its best to fulfil any resize request regardless
of whether the underlying PersistentVolume volume is online or offline. If your storage provider supports
online expansion then no Pod restart should be necessary for volume expansion to finish.
-->
&lt;p>但在某些情况下，如果底层存储驱动只能支持离线扩充，
则 PVC 用户必须先停止 Pod，才能让扩充成功。
请参阅存储提供商的文档，了解其支持哪种模式的卷扩充。&lt;/p>
&lt;p>当卷扩充作为 Alpha 功能引入时，
Kubernetes 仅支持在节点上进行离线的文件系统扩充，
因此需要用户重新启动 Pod，才能完成文件系统的大小调整。
今天，用户的行为已经被改变，无论底层 PersistentVolume 是在线还是离线，
Kubernetes 都会尽最大努力满足任何调整大小的请求。
如果你的存储提供商支持在线扩充，则无需重启 Pod 即可完成卷扩充。&lt;/p>
&lt;!--
## Next steps
Although volume expansion is now stable as part of the recent v1.24 release,
SIG Storage are working to make it even simpler for users of Kubernetes to expand their persistent storage.
Kubernetes 1.23 introduced features for triggering recovery from failed volume expansion, allowing users
to attempt self-service healing after a failed resize.
See [Recovering from volume expansion failure](/docs/concepts/storage/persistent-volumes/#recovering-from-failure-when-expanding-volumes) for more details.
-->
&lt;h2 id="下一步">下一步&lt;/h2>
&lt;p>尽管卷扩充在最近的 v1.24 发行版中成为了稳定版本，
但 SIG Storage 团队仍然在努力让 Kubernetes 用户扩充其持久性存储变得更简单。
Kubernetes 1.23 引入了卷扩充失败后触发恢复机制的功能特性，
允许用户在大小调整失败后尝试自助修复。
更多详细信息，请参阅&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/#recovering-from-failure-when-expanding-volumes">处理扩充卷过程中的失败&lt;/a>。&lt;/p>
&lt;!--
The Kubernetes contributor community is also discussing the potential for StatefulSet-driven storage expansion. This proposed
feature would let you trigger expansion for all underlying PVs that are providing storage to a StatefulSet,
by directly editing the StatefulSet object.
See the [Support Volume Expansion Through StatefulSets](https://github.com/kubernetes/enhancements/issues/661) enhancement proposal for more details.
-->
&lt;p>Kubernetes 贡献者社区也在讨论有状态（StatefulSet）驱动的存储扩充的潜力。
这个提议的功能特性将允许用户通过直接编辑 StatefulSet 对象，
触发为 StatefulSet 提供存储的所有底层 PV 的扩充。
更多详细信息，请参阅&lt;a href="https://github.com/kubernetes/enhancements/issues/661">通过 StatefulSet 支持卷扩充&lt;/a>的改善提议。&lt;/p></description></item><item><title>Blog: Dockershim：历史背景</title><link>https://kubernetes.io/zh-cn/blog/2022/05/03/dockershim-historical-context/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/03/dockershim-historical-context/</guid><description>
&lt;!--
layout: blog
title: "Dockershim: The Historical Context"
date: 2022-05-03
slug: dockershim-historical-context
-->
&lt;!--
**Author:** Kat Cosgrove
Dockershim has been removed as of Kubernetes v1.24, and this is a positive move for the project. However, context is important for fully understanding something, be it socially or in software development, and this deserves a more in-depth review. Alongside the dockershim removal in Kubernetes v1.24, we’ve seen some confusion (sometimes at a panic level) and dissatisfaction with this decision in the community, largely due to a lack of context around this removal. The decision to deprecate and eventually remove dockershim from Kubernetes was not made quickly or lightly. Still, it’s been in the works for so long that many of today’s users are newer than that decision, and certainly newer than the choices that led to the dockershim being necessary in the first place.
So what is the dockershim, and why is it going away?
-->
&lt;p>&lt;strong>作者：&lt;/strong> Kat Cosgrove&lt;/p>
&lt;p>自 Kubernetes v1.24 起，Dockershim 已被删除，这对项目来说是一个积极的举措。
然而，背景对于充分理解某事很重要，无论是社交还是软件开发，这值得更深入的审查。
除了 Kubernetes v1.24 中的 dockershim 移除之外，
我们在社区中看到了一些混乱（有时处于恐慌级别）和对这一决定的不满，
主要是由于缺乏有关此删除背景的了解。弃用并最终从 Kubernetes 中删除
dockershim 的决定并不是迅速或轻率地做出的。
尽管如此，它已经工作了很长时间，以至于今天的许多用户都比这个决定更新，
更不用提当初为何引入 dockershim 了。&lt;/p>
&lt;p>那么 dockershim 是什么，为什么它会消失呢？&lt;/p>
&lt;!--
In the early days of Kubernetes, we only supported one container runtime. That runtime was Docker Engine. Back then, there weren’t really a lot of other options out there and Docker was the dominant tool for working with containers, so this was not a controversial choice. Eventually, we started adding more container runtimes, like rkt and hypernetes, and it became clear that Kubernetes users want a choice of runtimes working best for them. So Kubernetes needed a way to allow cluster operators the flexibility to use whatever runtime they choose.
-->
&lt;p>在 Kubernetes 的早期，我们只支持一个容器运行时，那个运行时就是 Docker Engine。
那时，并没有太多其他选择，而 Docker 是使用容器的主要工具，所以这不是一个有争议的选择。
最终，我们开始添加更多的容器运行时，比如 rkt 和 hypernetes，很明显 Kubernetes
用户希望选择最适合他们的运行时。因此，Kubernetes 需要一种方法来允许集群操作员灵活地使用他们选择的任何运行时。&lt;/p>
&lt;!--
The [Container Runtime Interface](/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) (CRI) was released to allow that flexibility. The introduction of CRI was great for the project and users alike, but it did introduce a problem: Docker Engine’s use as a container runtime predates CRI, and Docker Engine is not CRI-compatible. To solve this issue, a small software shim (dockershim) was introduced as part of the kubelet component specifically to fill in the gaps between Docker Engine and CRI, allowing cluster operators to continue using Docker Engine as their container runtime largely uninterrupted.
-->
&lt;p>&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">容器运行时接口&lt;/a> (CRI)
已发布以支持这种灵活性。 CRI 的引入对项目和用户来说都很棒，但它确实引入了一个问题：Docker Engine
作为容器运行时的使用早于 CRI，并且 Docker Engine 不兼容 CRI。 为了解决这个问题，在 kubelet
组件中引入了一个小型软件 shim (dockershim)，专门用于填补 Docker Engine 和 CRI 之间的空白，
允许集群操作员继续使用 Docker Engine 作为他们的容器运行时基本上不间断。&lt;/p>
&lt;!--
However, this little software shim was never intended to be a permanent solution. Over the course of years, its existence has introduced a lot of unnecessary complexity to the kubelet itself. Some integrations are inconsistently implemented for Docker because of this shim, resulting in an increased burden on maintainers, and maintaining vendor-specific code is not in line with our open source philosophy. To reduce this maintenance burden and move towards a more collaborative community in support of open standards, [KEP-2221 was introduced](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim), proposing the removal of the dockershim. With the release of Kubernetes v1.20, the deprecation was official.
-->
&lt;p>然而，这个小软件 shim 从来没有打算成为一个永久的解决方案。 多年来，它的存在给
kubelet 本身带来了许多不必要的复杂性。由于这个 shim，Docker
的一些集成实现不一致，导致维护人员的负担增加，并且维护特定于供应商的代码不符合我们的开源理念。
为了减少这种维护负担并朝着支持开放标准的更具协作性的社区迈进，
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">引入了 KEP-2221&lt;/a>，
建议移除 dockershim。随着 Kubernetes v1.20 的发布，正式弃用。&lt;/p>
&lt;!--
We didn’t do a great job communicating this, and unfortunately, the deprecation announcement led to some panic within the community. Confusion around what this meant for Docker as a company, if container images built by Docker would still run, and what Docker Engine actually is led to a conflagration on social media. This was our fault; we should have more clearly communicated what was happening and why at the time. To combat this, we released [a blog](/blog/2020/12/02/dont-panic-kubernetes-and-docker/) and [accompanying FAQ](/blog/2020/12/02/dockershim-faq/) to allay the community’s fears and correct some misconceptions about what Docker is and how containers work within Kubernetes. As a result of the community’s concerns, Docker and Mirantis jointly agreed to continue supporting the dockershim code in the form of [cri-dockerd](https://www.mirantis.com/blog/the-future-of-dockershim-is-cri-dockerd/), allowing you to continue using Docker Engine as your container runtime if need be. For the interest of users who want to try other runtimes, like containerd or cri-o, [migration documentation was written](/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/).
-->
&lt;p>我们没有很好地传达这一点，不幸的是，弃用公告在社区内引起了一些恐慌。关于这对
Docker作为一家公司意味着什么，Docker 构建的容器镜像是否仍然可以运行，以及
Docker Engine 究竟是什么导致了社交媒体上的一场大火，人们感到困惑。
这是我们的错；我们应该更清楚地传达当时发生的事情和原因。为了解决这个问题，
我们发布了&lt;a href="https://kubernetes.io/zh-cn/blog/2020/12/02/dont-panic-kubernetes-and-docker/">一篇博客&lt;/a>和&lt;a href="https://kubernetes.io/zh-cn/blog/2020/12/02/dockershim-faq/">相应的 FAQ&lt;/a>
以减轻社区的恐惧并纠正对 Docker 是什么以及容器如何在 Kubernetes 中工作的一些误解。
由于社区的关注，Docker 和 Mirantis 共同决定继续以
&lt;a href="https://www.mirantis.com/blog/the-future-of-dockershim-is-cri-dockerd/">cri-dockerd&lt;/a>
的形式支持 dockershim 代码，允许你在需要时继续使用 Docker Engine 作为容器运行时。
对于想要尝试其他运行时（如 containerd 或 cri-o）的用户，
&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">已编写迁移文档&lt;/a>。&lt;/p>
&lt;!--
We later [surveyed the community](https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/) and [discovered that there are still many users with questions and concerns](/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim). In response, Kubernetes maintainers and the CNCF committed to addressing these concerns by extending documentation and other programs. In fact, this blog post is a part of this program. With so many end users successfully migrated to other runtimes, and improved documentation, we believe that everyone has a paved way to migration now.
-->
&lt;p>我们后来&lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">调查了社区&lt;/a>&lt;a href="https://kubernetes.io/zh-cn/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim">发现还有很多用户有疑问和顾虑&lt;/a>。
作为回应，Kubernetes 维护人员和 CNCF 承诺通过扩展文档和其他程序来解决这些问题。
事实上，这篇博文是这个计划的一部分。随着如此多的最终用户成功迁移到其他运行时，以及改进的文档，
我们相信每个人现在都为迁移铺平了道路。&lt;/p>
&lt;!--
Docker is not going away, either as a tool or as a company. It’s an important part of the cloud native community and the history of the Kubernetes project. We wouldn’t be where we are without them. That said, removing dockershim from kubelet is ultimately good for the community, the ecosystem, the project, and open source at large. This is an opportunity for all of us to come together to support open standards, and we’re glad to be doing so with the help of Docker and the community.
-->
&lt;p>Docker 不会消失，无论是作为一种工具还是作为一家公司。它是云原生社区的重要组成部分，
也是 Kubernetes 项目的历史。没有他们，我们就不会是现在的样子。也就是说，从 kubelet
中删除 dockershim 最终对社区、生态系统、项目和整个开源都有好处。
这是我们所有人齐心协力支持开放标准的机会，我们很高兴在 Docker 和社区的帮助下这样做。&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: 观星者</title><link>https://kubernetes.io/zh-cn/blog/2022/05/03/kubernetes-1-24-release-announcement/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/05/03/kubernetes-1-24-release-announcement/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes 1.24: Stargazer"
date: 2022-05-03
slug: kubernetes-1-24-release-announcement
-->
&lt;!--
**Authors**: [Kubernetes 1.24 Release Team](https://git.k8s.io/sig-release/releases/release-1.24/release-team.md)
We are excited to announce the release of Kubernetes 1.24, the first release of 2022!
This release consists of 46 enhancements: fourteen enhancements have graduated to stable,
fifteen enhancements are moving to beta, and thirteen enhancements are entering alpha.
Also, two features have been deprecated, and two features have been removed.
-->
&lt;p>&lt;strong>作者&lt;/strong>: &lt;a href="https://git.k8s.io/sig-release/releases/release-1.24/release-team.md">Kubernetes 1.24 发布团队&lt;/a>&lt;/p>
&lt;p>我们很高兴地宣布 Kubernetes 1.24 的发布，这是 2022 年的第一个版本！&lt;/p>
&lt;p>这个版本包括 46 个增强功能：14 个增强功能已经升级到稳定版，15 个增强功能正在进入 Beta 版，
13 个增强功能正在进入 Alpha 阶段。另外，有两个功能被废弃了，还有两个功能被删除了。&lt;/p>
&lt;!--
## Major Themes
### Dockershim Removed from kubelet
After its deprecation in v1.20, the dockershim component has been removed from the kubelet in Kubernetes v1.24.
From v1.24 onwards, you will need to either use one of the other [supported runtimes](/docs/setup/production-environment/container-runtimes/) (such as containerd or CRI-O)
or use cri-dockerd if you are relying on Docker Engine as your container runtime.
For more information about ensuring your cluster is ready for this removal, please
see [this guide](/blog/2022/03/31/ready-for-dockershim-removal/).
-->
&lt;h2 id="主要议题">主要议题&lt;/h2>
&lt;h3 id="从-kubelet-中删除-dockershim">从 kubelet 中删除 Dockershim&lt;/h3>
&lt;p>在 v1.20 版本中被废弃后，dockershim 组件已被从 Kubernetes v1.24 版本的 kubelet 中移除。
从 v1.24 开始，如果你依赖 Docker Engine 作为容器运行时，
则需要使用其他&lt;a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/">受支持的运行时&lt;/a>之一
（如 containerd 或 CRI-O）或使用 CRI dockerd。
有关确保集群已准备好进行此删除的更多信息，请参阅&lt;a href="https://kubernetes.io/zh-cn/blog/2022/03/31/ready-for-dockershim-removal/">本指南&lt;/a>。&lt;/p>
&lt;!--
### Beta APIs Off by Default
[New beta APIs will not be enabled in clusters by default](https://github.com/kubernetes/enhancements/issues/3136).
Existing beta APIs and new versions of existing beta APIs will continue to be enabled by default.
-->
&lt;h3 id="默认情况下关闭-beta-api">默认情况下关闭 Beta API&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/3136">新的 beta API 默认不会在集群中启用&lt;/a>。
默认情况下，现有 Beta API 和及其更新版本将继续被启用。&lt;/p>
&lt;!--
### Signing Release Artifacts
Release artifacts are [signed](https://github.com/kubernetes/enhancements/issues/3031) using [cosign](https://github.com/sigstore/cosign)
signatures,
and there is experimental support for [verifying image signatures](/docs/tasks/administer-cluster/verify-signed-images/).
Signing and verification of release artifacts is part of [increasing software supply chain security for the Kubernetes release process](https://github.com/kubernetes/enhancements/issues/3027).
-->
&lt;h3 id="签署发布工件">签署发布工件&lt;/h3>
&lt;p>发布工件使用 &lt;a href="https://github.com/sigstore/cosign">cosign&lt;/a> 签名进行&lt;a href="https://github.com/kubernetes/enhancements/issues/3031">签名&lt;/a>，
并且有&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/verify-signed-images/">验证图像签名&lt;/a>的实验性支持。
发布工件的签名和验证是&lt;a href="https://github.com/kubernetes/enhancements/issues/3027">提高 Kubernetes 发布过程的软件供应链安全性&lt;/a>
的一部分。&lt;/p>
&lt;!--
### OpenAPI v3
Kubernetes 1.24 offers beta support for publishing its APIs in the [OpenAPI v3 format](https://github.com/kubernetes/enhancements/issues/2896).
-->
&lt;h3 id="openapi-v3">OpenAPI v3&lt;/h3>
&lt;p>Kubernetes 1.24 提供了以 &lt;a href="https://github.com/kubernetes/enhancements/issues/2896">OpenAPI v3 格式&lt;/a>发布其 API 的 Beta 支持。&lt;/p>
&lt;!--
### Storage Capacity and Volume Expansion Are Generally Available
[Storage capacity tracking](https://github.com/kubernetes/enhancements/issues/1472)
supports exposing currently available storage capacity via [CSIStorageCapacity objects](/docs/concepts/storage/storage-capacity/#api)
and enhances scheduling of pods that use CSI volumes with late binding.
[Volume expansion](https://github.com/kubernetes/enhancements/issues/284) adds support
for resizing existing persistent volumes.
-->
&lt;h3 id="存储容量和卷扩展普遍可用">存储容量和卷扩展普遍可用&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/1472">存储容量跟踪&lt;/a>支持通过
&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/storage-capacity/#api">CSIStorageCapacity 对象&lt;/a>公开当前可用的存储容量，
并增强使用具有后期绑定的 CSI 卷的 Pod 的调度。&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/284">卷的扩展&lt;/a>增加了对调整现有持久性卷大小的支持。&lt;/p>
&lt;!--
### NonPreemptingPriority to Stable
This feature adds [a new option to PriorityClasses](https://github.com/kubernetes/enhancements/issues/902),
which can enable or disable pod preemption.
-->
&lt;h3 id="nonpreemptingpriority-到稳定">NonPreemptingPriority 到稳定&lt;/h3>
&lt;p>此功能&lt;a href="https://github.com/kubernetes/enhancements/issues/902">为 PriorityClasses 添加了一个新选项&lt;/a>，可以启用或禁用 Pod 抢占。&lt;/p>
&lt;!--
### Storage Plugin Migration
Work is underway to [migrate the internals of in-tree storage plugins](https://github.com/kubernetes/enhancements/issues/625) to call out to CSI Plugins
while maintaining the original API.
The [Azure Disk](https://github.com/kubernetes/enhancements/issues/1490)
and [OpenStack Cinder](https://github.com/kubernetes/enhancements/issues/1489) plugins
have both been migrated.
-->
&lt;h3 id="存储插件迁移">存储插件迁移&lt;/h3>
&lt;p>目前正在进行&lt;a href="https://github.com/kubernetes/enhancements/issues/625">迁移树内存储插件的内部组件&lt;/a>工作，
以便在保持原有 API 的同时调用 CSI 插件。&lt;a href="https://github.com/kubernetes/enhancements/issues/1490">Azure Disk&lt;/a>
和 &lt;a href="https://github.com/kubernetes/enhancements/issues/1489">OpenStack Cinder&lt;/a> 插件都已迁移。&lt;/p>
&lt;!--
### gRPC Probes Graduate to Beta
With Kubernetes 1.24, the [gRPC probes functionality](https://github.com/kubernetes/enhancements/issues/2727)
has entered beta and is available by default. You can now [configure startup, liveness, and readiness probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes) for your gRPC app
natively within Kubernetes without exposing an HTTP endpoint or
using an extra executable.
-->
&lt;h3 id="grpc-探针升级到-beta">gRPC 探针升级到 Beta&lt;/h3>
&lt;p>在 Kubernetes 1.24 中，&lt;a href="https://github.com/kubernetes/enhancements/issues/2727">gRPC 探测功能&lt;/a>
已进入测试版，默认可用。现在，你可以在 Kubernetes 中为你的 gRPC
应用程序原生地&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes">配置启动、存活和就绪性探测&lt;/a>，
而无需暴露 HTTP 端点或使用额外的可执行文件。&lt;/p>
&lt;!--
### Kubelet Credential Provider Graduates to Beta
Originally released as Alpha in Kubernetes 1.20, the kubelet's support for
[image credential providers](/docs/tasks/kubelet-credential-provider/kubelet-credential-provider/)
has now graduated to Beta.
This allows the kubelet to dynamically retrieve credentials for a container image registry
using exec plugins rather than storing credentials on the node's filesystem.
-->
&lt;h3 id="kubelet-凭证提供者毕业至-beta">Kubelet 凭证提供者毕业至 Beta&lt;/h3>
&lt;p>kubelet 最初在 Kubernetes 1.20 中作为 Alpha 发布，现在它对&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/kubelet-credential-provider/kubelet-credential-provider/">镜像凭证提供者&lt;/a>
的支持已升级到 Beta。这允许 kubelet 使用 exec 插件动态检索容器镜像仓库的凭据，而不是将凭据存储在节点的文件系统上。&lt;/p>
&lt;!--
### Contextual Logging in Alpha
Kubernetes 1.24 has introduced [contextual logging](https://github.com/kubernetes/enhancements/issues/3077)
that enables the caller of a function to control all aspects of logging (output formatting, verbosity, additional values, and names).
-->
&lt;h3 id="alpha-中的上下文日志记录">Alpha 中的上下文日志记录&lt;/h3>
&lt;p>Kubernetes 1.24 引入了&lt;a href="https://github.com/kubernetes/enhancements/issues/3077">上下文日志&lt;/a>
这使函数的调用者能够控制日志记录的所有方面（输出格式、详细程度、附加值和名称）。&lt;/p>
&lt;!--
### Avoiding Collisions in IP allocation to Services
Kubernetes 1.24 introduces a new opt-in feature that allows you to
[soft-reserve a range for static IP address assignments](/docs/concepts/services-networking/service/#service-ip-static-sub-range)
to Services.
With the manual enablement of this feature, the cluster will prefer automatic assignment from
the pool of Service IP addresses, thereby reducing the risk of collision.
-->
&lt;h3 id="避免-ip-分配给服务的冲突">避免 IP 分配给服务的冲突&lt;/h3>
&lt;p>Kubernetes 1.24 引入了一项新的选择加入功能，
允许你&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/#service-ip-static-sub-range">为服务的静态 IP 地址分配软保留范围&lt;/a>。
通过手动启用此功能，集群将更喜欢从服务 IP 地址池中自动分配，从而降低冲突风险。&lt;/p>
&lt;!--
A Service `ClusterIP` can be assigned:
* dynamically, which means the cluster will automatically pick a free IP within the configured Service IP range.
* statically, which means the user will set one IP within the configured Service IP range.
Service `ClusterIP` are unique; hence, trying to create a Service with a `ClusterIP` that has already been allocated will return an error.
-->
&lt;p>服务的 &lt;code>ClusterIP&lt;/code> 可以按照以下两种方式分配：&lt;/p>
&lt;ul>
&lt;li>动态，这意味着集群将自动在配置的服务 IP 范围内选择一个空闲 IP。&lt;/li>
&lt;li>静态，这意味着用户将在配置的服务 IP 范围内设置一个 IP。&lt;/li>
&lt;/ul>
&lt;p>服务 &lt;code>ClusterIP&lt;/code> 是唯一的；因此，尝试使用已分配的 &lt;code>ClusterIP&lt;/code> 创建服务将返回错误。&lt;/p>
&lt;!--
### Dynamic Kubelet Configuration is Removed from the Kubelet
After being deprecated in Kubernetes 1.22, Dynamic Kubelet Configuration has been removed from the kubelet. The feature will be removed from the API server in Kubernetes 1.26.
-->
&lt;h3 id="从-kubelet-中移除动态-kubelet-配置">从 Kubelet 中移除动态 Kubelet 配置&lt;/h3>
&lt;p>在 Kubernetes 1.22 中被弃用后，动态 Kubelet 配置已从 kubelet 中移除。
该功能将从 Kubernetes 1.26 的 API 服务器中移除。&lt;/p>
&lt;!--
## CNI Version-Related Breaking Change
Before you upgrade to Kubernetes 1.24, please verify that you are using/upgrading to a container
runtime that has been tested to work correctly with this release.
For example, the following container runtimes are being prepared, or have already been prepared, for Kubernetes:
* containerd v1.6.4 and later, v1.5.11 and later
* CRI-O 1.24 and later
-->
&lt;h2 id="cni-版本相关的重大更改">CNI 版本相关的重大更改&lt;/h2>
&lt;p>在升级到 Kubernetes 1.24 之前，请确认你正在使用/升级到经过测试可以在此版本中正常工作的容器运行时。&lt;/p>
&lt;p>例如，以下容器运行时正在为 Kubernetes 准备，或者已经准备好了。&lt;/p>
&lt;ul>
&lt;li>containerd v1.6.4 及更高版本，v1.5.11 及更高版本&lt;/li>
&lt;li>CRI-O 1.24 及更高版本&lt;/li>
&lt;/ul>
&lt;!--
Service issues exist for pod CNI network setup and tear down in containerd
v1.6.0–v1.6.3 when the CNI plugins have not been upgraded and/or the CNI config
version is not declared in the CNI config files. The containerd team reports, "these issues are resolved in containerd v1.6.4."
With containerd v1.6.0–v1.6.3, if you do not upgrade the CNI plugins and/or
declare the CNI config version, you might encounter the following "Incompatible
CNI versions" or "Failed to destroy network for sandbox" error conditions.
-->
&lt;p>当 CNI 插件尚未升级和/或 CNI 配置版本未在 CNI 配置文件中声明时，在 containerd v1.6.0–v1.6.3
中存在 Pod CNI 网络设置和拆除的服务问题。containerd 团队报告说，“这些问题在 containerd v1.6.4 中得到解决。”&lt;/p>
&lt;p>在 containerd v1.6.0-v1.6.3 版本中，如果你不升级 CNI 插件和/或声明 CNI 配置版本，
你可能会遇到以下 “Incompatible CNI versions” 或 “Failed to destroy network for sandbox” 的错误情况。&lt;/p>
&lt;!--
## CSI Snapshot
_This information was added after initial publication._
[VolumeSnapshot v1beta1 CRD has been removed](https://github.com/kubernetes/enhancements/issues/177).
Volume snapshot and restore functionality for Kubernetes and the Container Storage Interface (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, moved to GA in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.20 and is now unsupported. Refer to [KEP-177: CSI Snapshot](https://git.k8s.io/enhancements/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot) and [Volume Snapshot GA blog](/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/) for more information.
-->
&lt;h2 id="csi-快照">CSI 快照&lt;/h2>
&lt;p>&lt;strong>此信息是在首次发布后添加的。&lt;/strong>&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/177">VolumeSnapshot v1beta1 CRD 已被移除&lt;/a>。
Kubernetes 和容器存储接口 (CSI) 的卷快照和恢复功能，提供标准化的 API 设计 (CRD) 并添加了对 CSI 卷驱动程序的
PV 快照/恢复支持，在 v1.20 中升级至 GA。VolumeSnapshot v1beta1 在 v1.20 中被弃用，现在不受支持。
有关详细信息，请参阅 &lt;a href="https://git.k8s.io/enhancements/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot">KEP-177: CSI 快照&lt;/a>
和&lt;a href="https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/">卷快照 GA 博客&lt;/a>。&lt;/p>
&lt;!--
## Other Updates
### Graduations to Stable
This release saw fourteen enhancements promoted to stable:
-->
&lt;h2 id="其他更新">其他更新&lt;/h2>
&lt;h3 id="毕业到稳定版">毕业到稳定版&lt;/h3>
&lt;p>在此版本中，有 14 项增强功能升级为稳定版：&lt;/p>
&lt;!--
* [Container Storage Interface (CSI) Volume Expansion](https://github.com/kubernetes/enhancements/issues/284)
* [Pod Overhead](https://github.com/kubernetes/enhancements/issues/688): Account for resources tied to the pod sandbox but not specific containers.
* [Add non-preempting option to PriorityClasses](https://github.com/kubernetes/enhancements/issues/902)
* [Storage Capacity Tracking](https://github.com/kubernetes/enhancements/issues/1472)
* [OpenStack Cinder In-Tree to CSI Driver Migration](https://github.com/kubernetes/enhancements/issues/1489)
* [Azure Disk In-Tree to CSI Driver Migration](https://github.com/kubernetes/enhancements/issues/1490)
* [Efficient Watch Resumption](https://github.com/kubernetes/enhancements/issues/1904): Watch can be efficiently resumed after kube-apiserver reboot.
* [Service Type=LoadBalancer Class Field](https://github.com/kubernetes/enhancements/issues/1959): Introduce a new Service annotation `service.kubernetes.io/load-balancer-class` that allows multiple implementations of `type: LoadBalancer` Services in the same cluster.
* [Indexed Job](https://github.com/kubernetes/enhancements/issues/2214): Add a completion index to Pods of Jobs with a fixed completion count.
* [Add Suspend Field to Jobs API](https://github.com/kubernetes/enhancements/issues/2232): Add a suspend field to the Jobs API to allow orchestrators to create jobs with more control over when pods are created.
* [Pod Affinity NamespaceSelector](https://github.com/kubernetes/enhancements/issues/2249): Add a `namespaceSelector` field for to pod affinity/anti-affinity spec.
* [Leader Migration for Controller Managers](https://github.com/kubernetes/enhancements/issues/2436): kube-controller-manager and cloud-controller-manager can apply new controller-to-controller-manager assignment in HA control plane without downtime.
* [CSR Duration](https://github.com/kubernetes/enhancements/issues/2784): Extend the CertificateSigningRequest API with a mechanism to allow clients to request a specific duration for the issued certificate.
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/284">容器存储接口（CSI）卷扩展&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/688">Pod 开销&lt;/a>: 核算与 Pod 沙箱绑定的资源，但不包括特定的容器。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/902">向 PriorityClass 添加非抢占选项&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1472">存储容量跟踪&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1489">OpenStack Cinder In-Tree 到 CSI 驱动程序迁移&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1490">Azure 磁盘树到 CSI 驱动程序迁移&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1904">高效的监视恢复&lt;/a>：
kube-apiserver 重新启动后，可以高效地恢复监视。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1959">Service Type=LoadBalancer 类字段&lt;/a>：
引入新的服务注解 &lt;code>service.kubernetes.io/load-balancer-class&lt;/code>，
允许在同一个集群中提供 &lt;code>type: LoadBalancer&lt;/code> 服务的多个实现。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2214">带索引的 Job&lt;/a>：为带有固定完成计数的 Job 的 Pod 添加完成索引。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2232">在 Job API 中增加 suspend 字段&lt;/a>：
在 Job API 中增加一个 suspend 字段，允许协调者在创建作业时对 Pod 的创建进行更多控制。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2249">Pod 亲和性 NamespaceSelector&lt;/a>：
为 Pod 亲和性/反亲和性规约添加一个 &lt;code>namespaceSelector&lt;/code> 字段。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2436">控制器管理器的领导者迁移&lt;/a>：
kube-controller-manager 和 cloud-controller-manager 可以在 HA 控制平面中重新分配新的控制器到控制器管理器，而无需停机。&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2784">CSR 期限&lt;/a>：
用一种机制来扩展证书签名请求 API，允许客户为签发的证书请求一个特定的期限。&lt;/li>
&lt;/ul>
&lt;!--
### Major Changes
This release saw two major changes:
* [Dockershim Removal](https://github.com/kubernetes/enhancements/issues/2221)
* [Beta APIs are off by Default](https://github.com/kubernetes/enhancements/issues/3136)
-->
&lt;h3 id="主要变更">主要变更&lt;/h3>
&lt;p>此版本有两个主要变更：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2221">移除 Dockershim&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3136">默认关闭 Beta API&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Release Notes
Check out the full details of the Kubernetes 1.24 release in our [release notes](https://git.k8s.io/kubernetes/CHANGELOG/CHANGELOG-1.24.md).
-->
&lt;h3 id="发行说明">发行说明&lt;/h3>
&lt;p>在我们的&lt;a href="https://git.k8s.io/kubernetes/CHANGELOG/CHANGELOG-1.24.md">发行说明&lt;/a> 中查看 Kubernetes 1.24 版本的完整详细信息。&lt;/p>
&lt;!--
### Availability
Kubernetes 1.24 is available for download on [GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.24.0).
To get started with Kubernetes, check out these [interactive tutorials](/docs/tutorials/) or run local
Kubernetes clusters using containers as “nodes”, with [kind](https://kind.sigs.k8s.io/).
You can also easily install 1.24 using [kubeadm](/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;h3 id="可用性">可用性&lt;/h3>
&lt;p>Kubernetes 1.24 可在 &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.24.0">GitHub&lt;/a> 上下载。
要开始使用 Kubernetes，请查看这些&lt;a href="https://kubernetes.io/zh-cn/docs/tutorials/">交互式教程&lt;/a>或在本地运行。
使用 &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a>，可以将容器作为 Kubernetes 集群的 “节点”。
你还可以使用 &lt;a href="https://kubernetes.io/zh-cn/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a> 轻松安装 1.24。&lt;/p>
&lt;!--
### Release Team
This release would not have been possible without the combined efforts of committed individuals
comprising the Kubernetes 1.24 release team. This team came together to deliver all of the components
that go into each Kubernetes release, including code, documentation, release notes, and more.
Special thanks to James Laverack, our release lead, for guiding us through a successful release cycle,
and to all of the release team members for the time and effort they put in to deliver the v1.24
release for the Kubernetes community.
-->
&lt;h3 id="发布团队">发布团队&lt;/h3>
&lt;p>如果没有 Kubernetes 1.24 发布团队每个人做出的共同努力，这个版本是不可能实现的。
该团队齐心协力交付每个 Kubernetes 版本中的所有组件，包括代码、文档、发行说明等。&lt;/p>
&lt;p>特别感谢我们的发布负责人 James Laverack 指导我们完成了一个成功的发布周期，
并感谢所有发布团队成员投入时间和精力为 Kubernetes 社区提供 v1.24 版本。&lt;/p>
&lt;!--
### Release Theme and Logo
**Kubernetes 1.24: Stargazer**
&lt;figure class="release-logo">
&lt;img src="https://kubernetes.io/images/blog/2022-05-03-kubernetes-release-1.24/kubernetes-1.24.png"/>
&lt;/figure>
The theme for Kubernetes 1.24 is _Stargazer_.
-->
&lt;h3 id="发布主题和徽标">发布主题和徽标&lt;/h3>
&lt;p>&lt;strong>Kubernetes 1.24: 观星者&lt;/strong>&lt;/p>
&lt;figure class="release-logo">
&lt;img src="https://kubernetes.io/images/blog/2022-05-03-kubernetes-release-1.24/kubernetes-1.24.png"/>
&lt;/figure>
&lt;p>Kubernetes 1.24 的主题是&lt;strong>观星者（Stargazer）&lt;/strong>。&lt;/p>
&lt;!--
Generations of people have looked to the stars in awe and wonder, from ancient astronomers to the
scientists who built the James Webb Space Telescope. The stars have inspired us, set our imagination
alight, and guided us through long nights on difficult seas.
With this release we gaze upwards, to what is possible when our community comes together. Kubernetes
is the work of hundreds of contributors across the globe and thousands of end-users supporting
applications that serve millions. Every one is a star in our sky, helping us chart our course.
-->
&lt;p>古代天文学家到建造 James Webb 太空望远镜的科学家，几代人都怀着敬畏和惊奇的心情仰望星空。
是这些星辰启发了我们，点燃了我们的想象力，引导我们在艰难的海上度过了漫长的夜晚。&lt;/p>
&lt;p>通过此版本，我们向上凝视，当我们的社区聚集在一起时可能发生的事情。
Kubernetes 是全球数百名贡献者和数千名最终用户支持的成果，
是一款为数百万人服务的应用程序。每个人都是我们天空中的一颗星星，帮助我们规划路线。&lt;/p>
&lt;!--
The release logo is made by [Britnee Laverack](https://www.instagram.com/artsyfie/), and depicts a telescope set upon starry skies and the
[Pleiades](https://en.wikipedia.org/wiki/Pleiades), often known in mythology as the “Seven Sisters”. The number seven is especially auspicious
for the Kubernetes project, and is a reference back to our original “Project Seven” name.
This release of Kubernetes is named for those that would look towards the night sky and wonder — for
all the stargazers out there. ✨
-->
&lt;p>发布标志由 &lt;a href="https://www.instagram.com/artsyfie/">Britnee Laverack&lt;/a> 制作，
描绘了一架位于星空和&lt;a href="https://en.wikipedia.org/wiki/Pleiades">昴星团&lt;/a>的望远镜，在神话中通常被称为“七姐妹”。
数字 7 对于 Kubernetes 项目特别吉祥，是对我们最初的“项目七”名称的引用。&lt;/p>
&lt;p>这个版本的 Kubernetes 为那些仰望夜空的人命名——为所有的观星者命名。 ✨&lt;/p>
&lt;!--
### User Highlights
* Check out how leading retail e-commerce company [La Redoute used Kubernetes, alongside other CNCF projects, to transform and streamline its software delivery lifecycle](https://www.cncf.io/case-studies/la-redoute/) - from development to operations.
* Trying to ensure no change to an API call would cause any breaks, [Salt Security built its microservices entirely on Kubernetes, and it communicates via gRPC while Linkerd ensures messages are encrypted](https://www.cncf.io/case-studies/salt-security/).
* In their effort to migrate from private to public cloud, [Allainz Direct engineers redesigned its CI/CD pipeline in just three months while managing to condense 200 workflows down to 10-15](https://www.cncf.io/case-studies/allianz/).
* Check out how [Bink, a UK based fintech company, updated its in-house Kubernetes distribution with Linkerd to build a cloud-agnostic platform that scales as needed whilst allowing them to keep a close eye on performance and stability](https://www.cncf.io/case-studies/bink/).
* Using Kubernetes, the Dutch organization [Stichting Open Nederland](http://www.stichtingopennederland.nl/) created a testing portal in just one-and-a-half months to help safely reopen events in the Netherlands. The [Testing for Entry (Testen voor Toegang)](https://www.testenvoortoegang.org/) platform [leveraged the performance and scalability of Kubernetes to help individuals book over 400,000 COVID-19 testing appointments per day. ](https://www.cncf.io/case-studies/true/)
* Working alongside SparkFabrik and utilizing Backstage, [Santagostino created the developer platform Samaritan to centralize services and documentation, manage the entire lifecycle of services, and simplify the work of Santagostino developers](https://www.cncf.io/case-studies/santagostino/).
-->
&lt;h3 id="用户亮点">用户亮点&lt;/h3>
&lt;ul>
&lt;li>了解领先的零售电子商务公司
&lt;a href="https://www.cncf.io/case-studies/la-redoute/">La Redoute 如何使用 Kubernetes 以及其他 CNCF 项目来转变和简化&lt;/a>
其从开发到运营的软件交付生命周期。&lt;/li>
&lt;li>为了确保对 API 调用的更改不会导致任何中断，&lt;a href="https://www.cncf.io/case-studies/salt-security/">Salt Security 完全在 Kubernetes 上构建了它的微服务，
它通过 gRPC 进行通信，而 Linkerd 确保消息是加密的&lt;/a>。&lt;/li>
&lt;li>为了从私有云迁移到公共云，&lt;a href="https://www.cncf.io/case-studies/allianz/">Alllainz Direct 工程师在短短三个月内重新设计了其 CI/CD 管道，
同时设法将 200 个工作流压缩到 10-15 个&lt;/a>。&lt;/li>
&lt;li>看看&lt;a href="https://www.cncf.io/case-studies/bink/">英国金融科技公司 Bink 是如何用 Linkerd 更新其内部的 Kubernetes 分布，以建立一个云端的平台，
根据需要进行扩展，同时允许他们密切关注性能和稳定性&lt;/a>。&lt;/li>
&lt;li>利用Kubernetes，荷兰组织 &lt;a href="http://www.stichtingopennederland.nl/">Stichting Open Nederland&lt;/a>
在短短一个半月内创建了一个测试门户网站，以帮助安全地重新开放荷兰的活动。
&lt;a href="https://www.testenvoortoegang.org/">入门测试 (Testen voor Toegang)&lt;/a>
平台&lt;a href="https://www.cncf.io/case-studies/true/">利用 Kubernetes 的性能和可扩展性来帮助个人每天预订超过 400,000 个 COVID-19 测试预约&lt;/a>。&lt;/li>
&lt;li>与 SparkFabrik 合作并利用 Backstage，&lt;a href="https://www.cncf.io/case-studies/santagostino/">Santagostino 创建了开发人员平台 Samaritan 来集中服务和文档，
管理服务的整个生命周期，并简化 Santagostino 开发人员的工作&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
### Ecosystem Updates
* KubeCon + CloudNativeCon Europe 2022 will take place in Valencia, Spain, from 16 – 20 May 2022! You can find more information about the conference and registration on the [event site](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/).
* In the [2021 Cloud Native Survey](https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/), the CNCF saw record Kubernetes and container adoption. Take a look at the [results of the survey](https://www.cncf.io/reports/cncf-annual-survey-2021/).
* The [Linux Foundation](https://www.linuxfoundation.org/) and [The Cloud Native Computing Foundation](https://www.cncf.io/) (CNCF) announced the availability of a new [Cloud Native Developer Bootcamp](https://training.linuxfoundation.org/training/cloudnativedev-bootcamp/?utm_source=lftraining&amp;utm_medium=pr&amp;utm_campaign=clouddevbc0322) to provide participants with the knowledge and skills to design, build, and deploy cloud native applications. Check out the [announcement](https://www.cncf.io/announcements/2022/03/15/new-cloud-native-developer-bootcamp-provides-a-clear-path-to-cloud-native-careers/) to learn more.
-->
&lt;h3 id="生态系统更新">生态系统更新&lt;/h3>
&lt;ul>
&lt;li>KubeCon + CloudNativeCon Europe 2022 于 2022 年 5 月 16 日至 20 日在西班牙巴伦西亚举行！
你可以在&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">活动网站&lt;/a>上找到有关会议和注册的更多信息。&lt;/li>
&lt;li>在 &lt;a href="https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/">2021 年云原生调查&lt;/a>
中，CNCF 看到了创纪录的 Kubernetes 和容器采用。参阅&lt;a href="https://www.cncf.io/reports/cncf-annual-survey-2021/">调查结果&lt;/a>。&lt;/li>
&lt;li>&lt;a href="https://www.linuxfoundation.org/">Linux 基金会&lt;/a>和&lt;a href="https://www.cncf.io/">云原生计算基金会&lt;/a> (CNCF)
宣布推出新的 &lt;a href="https://training.linuxfoundation.org/training/cloudnativedev-bootcamp/?utm_source=lftraining&amp;amp;utm_medium=pr&amp;amp;utm_campaign=clouddevbc0322">云原生开发者训练营&lt;/a>
为参与者提供设计、构建和部署云原生应用程序的知识和技能。查看&lt;a href="https://www.cncf.io/announcements/2022/03/15/new-cloud-native-developer-bootcamp-provides-a-clear-path-to-cloud-native-careers/">公告&lt;/a>以了解更多信息。&lt;/li>
&lt;/ul>
&lt;!--
### Project Velocity
The [CNCF K8s DevStats](https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;refresh=15m) project
aggregates a number of interesting data points related to the velocity of Kubernetes and various
sub-projects. This includes everything from individual contributions to the number of companies that
are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.
In the v1.24 release cycle, which [ran for 17 weeks](https://github.com/kubernetes/sig-release/tree/master/releases/release-1.24) (January 10 to May 3), we saw contributions from [1029 companies](https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;var-metric=contributions) and [1179 individuals](https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;var-metric=contributions&amp;var-repogroup_name=Kubernetes&amp;var-country_name=All&amp;var-companies=All&amp;var-repo_name=kubernetes%2Fkubernetes).
-->
&lt;h3 id="项目速度">项目速度&lt;/h3>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;amp;refresh=15m">CNCF K8s DevStats&lt;/a> 项目
汇总了许多与 Kubernetes 和各种子项目的速度相关的有趣数据点。这包括从个人贡献到做出贡献的公司数量的所有内容，
并且说明了为发展这个生态系统而付出的努力的深度和广度。&lt;/p>
&lt;p>在&lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.24">运行 17 周&lt;/a>
（ 1 月 10 日至 5 月 3 日）的 v1.24 发布周期中，我们看到 &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;amp;var-metric=contributions">1029 家公司&lt;/a>
和 &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&amp;amp;var-repo_name=kubernetes%2Fkubernetes">1179 人&lt;/a> 的贡献。&lt;/p>
&lt;!--
## Upcoming Release Webinar
Join members of the Kubernetes 1.24 release team on Tue May 24, 2022 9:45am – 11am PT to learn about
the major features of this release, as well as deprecations and removals to help plan for upgrades.
For more information and registration, visit the [event page](https://community.cncf.io/e/mck3kd/)
on the CNCF Online Programs site.
-->
&lt;h2 id="即将发布的网络研讨会">即将发布的网络研讨会&lt;/h2>
&lt;p>在太平洋时间 2022 年 5 月 24 日星期二上午 9:45 至上午 11 点加入 Kubernetes 1.24 发布团队的成员，
了解此版本的主要功能以及弃用和删除，以帮助规划升级。有关更多信息和注册，
请访问 CNCF 在线计划网站上的&lt;a href="https://community.cncf.io/e/mck3kd/">活动页面&lt;/a>。&lt;/p>
&lt;!--
## Get Involved
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://git.k8s.io/community/sig-list.md) (SIGs) that align with your interests.
Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://git.k8s.io/community/communication), and through the channels below:
* Find out more about contributing to Kubernetes at the [Kubernetes Contributors](https://www.kubernetes.dev/) website
* Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for the latest updates
* Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
* Join the community on [Slack](http://slack.k8s.io/)
* Post questions (or answer questions) on [Server Fault](https://serverfault.com/questions/tagged/kubernetes).
* Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
* Read more about what’s happening with Kubernetes on the [blog](https://kubernetes.io/blog/)
* Learn more about the [Kubernetes Release Team](https://git.k8s.io/sig-release/release-team)
-->
&lt;h2 id="参与进来">参与进来&lt;/h2>
&lt;p>参与 Kubernetes 的最简单方法是加入符合你兴趣的众多&lt;a href="https://git.k8s.io/community/sig-list.md">特别兴趣组&lt;/a>（SIG）之一。
你有什么想向 Kubernetes 社区广播的内容吗？
在我们的每周的&lt;a href="https://git.k8s.io/community/communication">社区会议&lt;/a>上分享你的声音，并通过以下渠道：&lt;/p>
&lt;ul>
&lt;li>在 &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributors&lt;/a> 网站上了解有关为 Kubernetes 做出贡献的更多信息&lt;/li>
&lt;li>在 Twitter 上关注我们 &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> 以获取最新更新&lt;/li>
&lt;li>加入社区讨论 &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>加入 &lt;a href="http://slack.k8s.io/">Slack&lt;/a> 社区&lt;/li>
&lt;li>在 &lt;a href="https://serverfault.com/questions/tagged/kubernetes">Server Fault&lt;/a> 上发布问题（或回答问题）。&lt;/li>
&lt;li>分享你的 Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">故事&lt;/a>&lt;/li>
&lt;li>在&lt;a href="https://kubernetes.io/zh-cn/blog/">博客&lt;/a>上阅读有关 Kubernetes 正在发生的事情的更多信息&lt;/li>
&lt;li>详细了解 &lt;a href="https://git.k8s.io/sig-release/release-team">Kubernetes 发布团队&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Frontiers, fsGroups and frogs: Kubernetes 1.23 发布采访</title><link>https://kubernetes.io/zh-cn/blog/2022/04/29/frontiers-fsgroups-and-frogs-kubernetes-1.23-%E5%8F%91%E5%B8%83%E9%87%87%E8%AE%BF/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/04/29/frontiers-fsgroups-and-frogs-kubernetes-1.23-%E5%8F%91%E5%B8%83%E9%87%87%E8%AE%BF/</guid><description>
&lt;!--
layout: blog
title: "Frontiers, fsGroups and frogs: the Kubernetes 1.23 release interview"
date: 2022-04-29
-->
&lt;!--
**Author**: Craig Box (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;!--
One of the highlights of hosting the weekly [Kubernetes Podcast from Google](https://kubernetespodcast.com/) is talking to the release managers for each new Kubernetes version. The release team is constantly refreshing. Many working their way from small documentation fixes, step up to shadow roles, and then eventually lead a release.
-->
&lt;p>举办每周一次的&lt;a href="https://kubernetespodcast.com/">来自 Google 的 Kubernetes 播客&lt;/a>
的亮点之一是与每个新 Kubernetes 版本的发布经理交谈。发布团队不断刷新。许多人从小型文档修复开始，逐步晋升为影子角色，然后最终领导发布。&lt;/p>
&lt;!--
As we prepare for the 1.24 release next week, [in accordance with long-standing tradition](https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog), I'm pleased to bring you a look back at the story of 1.23. The release was led by [Rey Lejano](https://twitter.com/reylejano), a Field Engineer at SUSE. [I spoke to Rey](https://kubernetespodcast.com/episode/167-kubernetes-1.23/) in December, as he was awaiting the birth of his first child.
-->
&lt;p>在我们为下周发布的 1.24 版本做准备时，&lt;a href="https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog">按照长期以来的传统&lt;/a>，
很高兴带大家回顾一下 1.23 的故事。该版本由 SUSE 的现场工程师 &lt;a href="https://twitter.com/reylejano">Rey Lejano&lt;/a> 领导。
在 12 月&lt;a href="https://kubernetespodcast.com/episode/167-kubernetes-1.23/">我与 Rey 交谈过&lt;/a>，当时他正在等待他的第一个孩子的出生。&lt;/p>
&lt;!--
Make sure you [subscribe, wherever you get your podcasts](https://kubernetespodcast.com/subscribe/), so you hear all our stories from the Cloud Native community, including the story of 1.24 next week.
-->
&lt;p>请确保你&lt;a href="https://kubernetespodcast.com/subscribe/">订阅，无论你在哪里获得你的播客&lt;/a>，
以便你听到我们所有来自云原生社区的故事，包括下周 1.24 的故事。&lt;/p>
&lt;!--
*This transcript has been lightly edited and condensed for clarity.*
-->
&lt;p>&lt;strong>为清晰起见本稿件经过了简单的编辑和浓缩。&lt;/strong>&lt;/p>
&lt;hr>
&lt;!--
**CRAIG BOX: I'd like to start with what is, of course, on top of everyone's mind at the moment. Let's talk African clawed frogs!**
-->
&lt;p>&lt;strong>CRAIG BOX：我想从现在每个人最关心的问题开始。让我们谈谈非洲爪蛙！&lt;/strong>&lt;/p>
&lt;!--
REY LEJANO: [CHUCKLES] Oh, you mean [Xenopus lavis](https://en.wikipedia.org/wiki/African_clawed_frog), the scientific name for the African clawed frog?
**CRAIG BOX: Of course.**
-->
&lt;p>REY LEJANO：[笑]哦，你是说 &lt;a href="https://en.wikipedia.org/wiki/African_clawed_frog">Xenopus lavis&lt;/a>，非洲爪蛙的学名？&lt;/p>
&lt;p>&lt;strong>CRAIG BOX：当然。&lt;/strong>&lt;/p>
&lt;!--
REY LEJANO: Not many people know, but my background and my degree is actually in microbiology, from the University of California Davis. I did some research for about four years in biochemistry, in a biochemistry lab, and I [do have a research paper published](https://www.sciencedirect.com/science/article/pii/). It's actually on glycoproteins, particularly something called "cortical granule lectin". We used frogs, because they generate lots and lots of eggs, from which we can extract the protein. That protein prevents polyspermy. When the sperm goes into the egg, the egg releases a glycoprotein, cortical granule lectin, to the membrane, and prevents any other sperm from going inside the egg.
-->
&lt;p>REY LEJANO：知道的人不多，但我曾就读于戴維斯加利福尼亚大学的微生物学专业。
我在生物化学实验室做了大约四年的生物化学研究，并且我&lt;a href="https://www.sciencedirect.com/science/article/pii/">确实发表了一篇研究论文&lt;/a>。
它实际上是在糖蛋白上，特别是一种叫做“皮质颗粒凝集素”的东西。我们使用青蛙，因为它们会产生大量的蛋，我们可以从中提取蛋白质。
这种蛋白质可以防止多精症。当精子进入卵子时，卵子会向细胞膜释放一种糖蛋白，即皮质颗粒凝集素，并阻止任何其他精子进入卵子。&lt;/p>
&lt;!--
**CRAIG BOX: Were you able to take anything from the testing that we did on frogs and generalize that to higher-order mammals, perhaps?**
REY LEJANO: Yes. Since mammals also have cortical granule lectin, we were able to analyze both the convergence and the evolutionary pattern, not just from multiple species of frogs, but also into mammals as well.
-->
&lt;p>&lt;strong>CRAIG BOX：你是否能够从我们对青蛙进行的测试中汲取任何东西并将其推广到更高阶的哺乳动物？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。由于哺乳动物也有皮质颗粒凝集素，我们能够分析收敛和进化模式，不仅来自多种青蛙，还包括哺乳动物。&lt;/p>
&lt;!--
**CRAIG BOX: Now, there's a couple of different threads to unravel here. When you were young, what led you into the fields of biology, and perhaps more the technical side of it?**
REY LEJANO: I think it was mostly from family, since I do have a family history in the medical field that goes back generations. So I kind of felt like that was the natural path going into college.
-->
&lt;p>&lt;strong>CRAIG BOX：现在，这里有几个不同的线索需要解开。当你年轻的时候，是什么引导你进入生物学领域，可以侧重介绍技术方面的内容吗？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：我认为这主要来自家庭，因为我在医学领域确实有可以追溯到几代人的家族史。所以我觉得那是进入大学的自然路径。&lt;/p>
&lt;!--
**CRAIG BOX: Now, of course, you're working in a more abstract tech field. What led you out of microbiology?**
REY LEJANO: [CHUCKLES] Well, I've always been interested in tech. Taught myself a little programming when I was younger, before high school, did some web dev stuff. Just kind of got burnt out being in a lab. I was literally in the basement. I had a great opportunity to join a consultancy that specialized in [ITIL](https://www.axelos.com/certifications/itil-service-management/what-is-itil). I actually started off with application performance management, went into monitoring, went into operation management and also ITIL, which is aligning your IT asset management and service managements with business services. Did that for a good number of years, actually.
-->
&lt;p>&lt;strong>CRAIG BOX：现在，你正在一个更抽象的技术领域工作。是什么让你离开了微生物学？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：[笑]嗯，我一直对科技很感兴趣。我年轻的时候自学了一点编程，在高中之前，做了一些网络开发的东西。
只是在实验室里有点焦头烂额了，实际上是在地下室。我有一个很好的机会加入了一家专门从事 &lt;a href="https://www.axelos.com/certifications/itil-service-management/what-is-itil">ITIL&lt;/a>
的咨询公司。实际上，我从应用性能管理开始，进入监控，进入运营管理和 ITIL，也就是把你的 IT 资产管理和服务管理与商业服务结合起来。实际上，我在这方面做了很多年。&lt;/p>
&lt;!--
**CRAIG BOX: It's very interesting, as people describe the things that they went through and perhaps the technologies that they worked on, you can pretty much pinpoint how old they might be. There's a lot of people who come into tech these days that have never heard of ITIL. They have no idea what it is. It's basically just SRE with more process.**
REY LEJANO: Yes, absolutely. It's not very cloud native. [CHUCKLES]
-->
&lt;p>&lt;strong>CRAIG BOX：这很有趣，当人们描述他们所经历的事情以及他们所从事的技术时，你几乎可以确定他们的年龄。
现在有很多人进入科技行业，但从未听说过 ITIL。他们不知道那是什么。它基本上和 SRE 类似，只是过程更加复杂。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的，一点没错。它不是非常云原生的。[笑]&lt;/p>
&lt;!--
**CRAIG BOX: Not at all.**
REY LEJANO: You don't really hear about it in the cloud native landscape. Definitely, you can tell someone's been in the field for a little bit, if they specialize or have worked with ITIL before.
-->
&lt;p>&lt;strong>CRAIG BOX：一点也不。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：在云原生环境中，你并没有真正听说过它。毫无疑问，如果有人专门从事过 ITIL 工作或之前曾与 ITIL 合作过，你肯定可以看出他们已经在该领域工作了一段时间。&lt;/p>
&lt;!--
**CRAIG BOX: You mentioned that you wanted to get out of the basement. That is quite often where people put the programmers. Did they just give you a bit of light in the new basement?**
REY LEJANO: [LAUGHS] They did give us much better lighting. Able to get some vitamin D sometimes, as well.
-->
&lt;p>&lt;strong>CRAIG BOX：你提到你想离开地下室。这的确是程序员常待的地方。他们只是在新的地下室里给了你一点光吗？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：[笑]他们确实给了我们更好的照明。有时也能获得一些维生素 D。&lt;/p>
&lt;!--
**CRAIG BOX: To wrap up the discussion about your previous career — over the course of the last year, with all of the things that have happened in the world, I could imagine that microbiology skills may be more in demand than perhaps they were when you studied them?**
REY LEJANO: Oh, absolutely. I could definitely see a big increase of numbers of people going into the field. Also, reading what's going on with the world currently kind of brings back all the education I've learned in the past, as well.
-->
&lt;p>&lt;strong>CRAIG BOX：总结一下你的过往职业经历：在过去的一年里，随着全球各地的发展变化，我认为如今微生物学技能可能比你在校时更受欢迎？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：哦，当然。我肯定能看到进入这个领域的人数大增。此外，阅读当前世界正在发生的事情也会带回我过去所学的所有教育。&lt;/p>
&lt;!--
**CRAIG BOX: Do you keep in touch with people you went through school with?**
REY LEJANO: Just some close friends, but not in the microbiology field.
-->
&lt;p>&lt;strong>CRAIG BOX：你和当时的同学还在保持联系吗？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：只是一些亲密的朋友，但不是在微生物学领域。&lt;/p>
&lt;!--
**CRAIG BOX: One thing that I think will probably happen as a result of the pandemic is a renewed interest in some of these STEM fields. It will be interesting to see what impact that has on society at large.**
REY LEJANO: Yeah. I think that'll be great.
-->
&lt;p>&lt;strong>CRAIG BOX：我认为，这次的全球疫情可能让人们对科学、技术、工程和数学领域重新产生兴趣。
看看这对整个社会有什么影响，将是很有趣的。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。我认为那会很棒。&lt;/p>
&lt;!--
**CRAIG BOX: You mentioned working at a consultancy doing IT management, application performance monitoring, and so on. When did Kubernetes come into your professional life?**
REY LEJANO: One of my good friends at the company I worked at, left in mid-2015. He went on to a company that was pretty heavily into Docker. He taught me a little bit. I did my first "docker run" around 2015, maybe 2016. Then, one of the applications we were using for the ITIL framework was containerized around 2018 or so, also in Kubernetes. At that time, it was pretty buggy. That was my initial introduction to Kubernetes and containerised applications.
-->
&lt;p>&lt;strong>CRAIG BOX：你提到在一家咨询公司工作，从事 IT 管理、应用程序性能监控等工作。Kubernetes 是什么时候进入你的职业生涯的？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：在我工作的公司，我的一位好朋友于 2015 年年中离职。他去了一家非常热衷于 Docker 的公司。
他教了我一点东西。我在 2015 年左右，也许是 2016 年，做了我的第一次 “docker run”。
然后，我们用于 ITIL 框架的一个应用程序在 2018 年左右被容器化了，也在 Kubernetes 中。
那个时候，它是有些问题的。那是我第一次接触 Kubernetes 和容器化应用程序。&lt;/p>
&lt;!--
Then I left that company, and I actually joined my friend over at [RX-M](https://rx-m.com/), which is a cloud native consultancy and training firm. They specialize in Docker and Kubernetes. I was able to get my feet wet. I got my CKD, got my CKA as well. And they were really, really great at encouraging us to learn more about Kubernetes and also to be involved in the community.
**CRAIG BOX: You will have seen, then, the life cycle of people adopting Kubernetes and containerization at large, through your own initial journey and then through helping customers. How would you characterize how that journey has changed from the early days to perhaps today?**
-->
&lt;p>然后我离开了那家公司，实际上我加入了我在 &lt;a href="https://rx-m.com/">RX-M&lt;/a> 的朋友，这是一家云原生咨询和培训公司。
他们专门从事 Docker 和 Kubernetes 的工作。我能够让我脚踏实地。我拿到了 CKD 和 CKA 证书。
他们在鼓励我们学习更多关于 Kubernetes 的知识和参与社区活动方面真的非常棒。&lt;/p>
&lt;p>&lt;strong>CRAIG BOX：然后，你将看到人们采用 Kubernetes 和容器化的整个生命周期，通过你自己的初始旅程，然后通过帮助客户。你如何描述这段旅程从早期到今天的变化？&lt;/strong>&lt;/p>
&lt;!--
REY LEJANO: I think the early days, there was a lot of questions of, why do I have to containerize? Why can't I just stay with virtual machines?
**CRAIG BOX: It's a line item on your CV.**
-->
&lt;p>REY LEJANO：我认为早期有很多问题，为什么我必须容器化？为什么我不能只使用虚拟机？&lt;/p>
&lt;p>&lt;strong>CRAIG BOX：这是你的简历上的一个条目。&lt;/strong>&lt;/p>
&lt;!--
REY LEJANO: [CHUCKLES] It is. And nowadays, I think people know the value of using containers, of orchestrating containers with Kubernetes. I don't want to say "jumping on the bandwagon", but it's become the de-facto standard to orchestrate containers.
**CRAIG BOX: It's not something that a consultancy needs to go out and pitch to customers that they should be doing. They're just taking it as, that will happen, and starting a bit further down the path, perhaps.**
REY LEJANO: Absolutely.
-->
&lt;p>REY LEJANO：[笑]是的。现在，我认为人们知道使用容器的价值，以及使用 Kubernetes 编排容器的价值。我不想说“赶上潮流”，但它已经成为编排容器的事实标准。&lt;/p>
&lt;p>&lt;strong>CRAIG BOX：这不是咨询公司需要走出去向客户推销他们应该做的事情。他们只是把它当作会发生的事情，并开始在这条路上走得更远一些，也许。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：当然。&lt;/p>
&lt;!--
**CRAIG BOX: Working at a consultancy like that, how much time do you get to work on improving process, perhaps for multiple customers, and then looking at how you can upstream that work, versus paid work that you do for just an individual customer at a time?**
-->
&lt;p>&lt;strong>CRAIG BOX：在这样的咨询公司工作，你有多少时间致力于改善流程，也许是为多个客户，然后研究如何将这项工作推向上游，而不是每次只为单个客户做有偿工作？&lt;/strong>&lt;/p>
&lt;!--
REY LEJANO: Back then, it would vary. They helped me introduce myself, and I learned a lot about the cloud native landscape and Kubernetes itself. They helped educate me as to how the cloud native landscape, and the tools around it, can be used together. My boss at that company, Randy, he actually encouraged us to start contributing upstream, and encouraged me to join the release team. He just said, this is a great opportunity. Definitely helped me with starting with the contributions early on.
-->
&lt;p>REY LEJANO：那时，情况会有所不同。他们帮我介绍了自己，我也了解了很多关于云原生环境和 Kubernetes 本身的情况。
他们帮助我了解如何将云原生环境及其周围的工具一起使用。我在那家公司的老板，Randy，实际上他鼓励我们开始向上游做贡献，
并鼓励我加入发布团队。他只是说，这是个很好的机会。这对我在早期就开始做贡献有很大的帮助。&lt;/p>
&lt;!--
**CRAIG BOX: Was the release team the way that you got involved with upstream Kubernetes contribution?**
REY LEJANO: Actually, no. My first contribution was with SIG Docs. I met Taylor Dolezal — he was the release team lead for 1.19, but he is involved with SIG Docs as well. I met him at KubeCon 2019, I sat at his table during a luncheon. I remember Paris Pittman was hosting this luncheon at the Marriott. Taylor says he was involved with SIG Docs. He encouraged me to join. I started joining into meetings, started doing a few drive-by PRs. That's what we call them — drive-by — little typo fixes. Then did a little bit more, started to send better or higher quality pull requests, and also reviewing PRs.
-->
&lt;p>&lt;strong>CRAIG BOX：发布团队是你参与上游 Kubernetes 贡献的方式吗？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：实际上，没有。我的第一个贡献是 SIG Docs。我认识了 Taylor Dolezal——他是 1.19 的发布团队负责人，但他也参与了 SIG Docs。
我在 KubeCon 2019 遇到了他，在午餐时我坐在他的桌子旁。我记得 Paris Pittman 在万豪酒店主持了这次午餐会。
Taylor 说他参与了 SIG Docs。他鼓励我加入。我开始参加会议，开始做一些路过式的 PR。
这就是我们所说的 - 驱动式 - 小错字修复。然后做更多的事情，开始发送更好或更高质量的拉取请求，并审查 PR。&lt;/p>
&lt;!--
**CRAIG BOX: When did you first formally take your release team role?**
REY LEJANO: That was in [1.18](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md), in December. My boss at the time encouraged me to apply. I did, was lucky enough to get accepted for the release notes shadow. Then from there, stayed in with release notes for a few cycles, then went into Docs, naturally then led Docs, then went to Enhancements, and now I'm the release lead for 1.23.
-->
&lt;p>&lt;strong>CRAIG BOX：你第一次正式担任发布团队的角色是什么时候？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：那是在 12月的 &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">1.18&lt;/a>。
当时我的老板鼓励我去申请。我申请了，很幸运地被录取了，成为发布说明的影子。然后从那里开始，我在发布说明中呆了几个周期，
然后去了文档，自然而然地领导了文档，然后去了增强版，现在我是 1.23 的发行负责人。&lt;/p>
&lt;!--
**CRAIG BOX: I don't know that a lot of people think about what goes into a good release note. What would you say does?**
REY LEJANO: [CHUCKLES] You have to tell the end user what has changed or what effect that they might see in the release notes. It doesn't have to be highly technical. It could just be a few lines, and just saying what has changed, what they have to do if they have to do anything as well.
-->
&lt;p>&lt;strong>CRAIG BOX：我不知道很多人都会考虑到一个好的发行说明需要什么。你说什么才是呢？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：[笑]你必须告诉最终用户发生了什么变化，或者他们在发行说明中可能看到什么效果。
它不必是高度技术性的。它可以只是几行字，只是说有什么变化，如果他们也必须做任何事情，他们必须做什么。&lt;/p>
&lt;!--
**CRAIG BOX: As you moved through the process of shadowing, how did you learn from the people who were leading those roles?**
REY LEJANO: I said this a few times when I was the release lead for this cycle. You get out of the release team as much as you put in, or it directly aligns to how much you put in. I learned a lot. I went into the release team having that mindset of learning from the role leads, learning from the other shadows, as well. That's actually a saying that my first role lead told me. I still carry it to heart, and that was back in 1.18. That was Eddie, in the very first meeting we had, and I still carry it to heart.
-->
&lt;p>&lt;strong>CRAIG BOX：我不知道很多人会考虑一个好的发布说明的内容。你会说什么？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：当我是这个周期的发布负责人时，我说过几次。你从发布团队得到的东西和你投入的东西一样多，或者说它直接与你投入的东西相一致。
我学到了很多东西。我在进入发布团队时就有这样的心态：向角色领导学习，也向其他影子学习。
这实际上是我的第一个角色负责人告诉我的一句话。我仍然铭记于心，那是在 1.18 中。那是 Eddie，在我们第一次见面时，我仍然牢记在心。&lt;/p>
&lt;!--
**CRAIG BOX: You, of course, were [the release lead for 1.23](https://github.com/kubernetes/sig-release/tree/master/releases/release-1.23). First of all, congratulations on the release.**
REY LEJANO: Thank you very much.
-->
&lt;p>&lt;strong>CRAIG BOX：当然，你是 &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.23">1.23 的发布负责人&lt;/a>。首先，祝贺发布。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：非常感谢。&lt;/p>
&lt;!--
**CRAIG BOX: The theme for this release is [The Next Frontier](https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/). Tell me the story of how we came to the theme and then the logo.**
REY LEJANO: The Next Frontier represents a few things. It not only represents the next enhancements in this release, but Kubernetes itself also has a history of Star Trek references. The original codename for Kubernetes was Project Seven, a reference to Seven of Nine, originally from Star Trek Voyager. Also the seven spokes in the helm in the logo of Kubernetes as well. And, of course, Borg, the predecessor to Kubernetes.
-->
&lt;p>&lt;strong>CRAIG BOX：这个版本的主题是&lt;a href="https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/">最后战线&lt;/a>。
请告诉我我们是如何确定主题和标志的故事。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：最后战线代表了几件事。它不仅代表了此版本的下一个增强功能，而且 Kubernetes 本身也有《星际迷航》的参考历史。
Kubernetes 的原始代号是 Project Seven，指的是最初来自《星际迷航》中的 Seven of Nine。
在 Kubernetes 的 logo 中掌舵的七根辐条也是如此。当然，还有 Kubernetes 的前身 Borg。&lt;/p>
&lt;!--
The Next Frontier continues that Star Trek reference. It's a fusion of two titles in the Star Trek universe. One is [Star Trek V, the Final Frontier](https://en.wikipedia.org/wiki/Star_Trek_V:_The_Final_Frontier), and the Star Trek: The Next Generation.
-->
&lt;p>最后战线继续星际迷航参考。这是星际迷航宇宙中两个标题的融合。一个是&lt;a href="https://en.wikipedia.org/wiki/Star_Trek_V:_The_Final_Frontier">星际迷航 5：最后战线&lt;/a>，还有星际迷航：下一代。&lt;/p>
&lt;!--
**CRAIG BOX: Do you have any opinion on the fact that Star Trek V was an odd-numbered movie, and they are [canonically referred to as being lesser than the even-numbered ones](https://screenrant.com/star-trek-movies-odd-number-curse-explained/)?**
REY LEJANO: I can't say, because I am such a sci-fi nerd that I love all of them even though they're bad. Even the post-Next Generation movies, after the series, I still liked all of them, even though I know some weren't that great.
-->
&lt;p>&lt;strong>CRAIG BOX：你对《星际迷航 5》是一部奇数电影有什么看法，而且它们&lt;a href="https://screenrant.com/star-trek-movies-odd-number-curse-explained/">通常被称为比偶数电影票房少&lt;/a>？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：我不能说，因为我是一个科幻书呆子，我喜欢他们所有的人，尽管他们很糟糕。即使是《下一代》系列之后的电影，我仍然喜欢所有的电影，尽管我知道有些并不那么好。&lt;/p>
&lt;!--
**CRAIG BOX: Am I right in remembering that Star Trek V was the one directed by William Shatner?**
REY LEJANO: Yes, that is correct.
-->
&lt;p>&lt;strong>CRAIG BOX：我记得星际迷航 5 是由 William Shatner 执导对吗？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的，对的。&lt;/p>
&lt;!--
**CRAIG BOX: I think that says it all.**
REY LEJANO: [CHUCKLES] Yes.
-->
&lt;p>&lt;strong>CRAIG BOX：我认为这说明了一切。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：[笑]是的。&lt;/p>
&lt;!--
**CRAIG BOX: Now, I understand that the theme comes from a part of the [SIG Release charter](https://github.com/kubernetes/community/blob/master/sig-release/charter.md)?**
REY LEJANO: Yes. There's a line in the SIG Release charter, "ensure there is a consistent group of community members in place to support the release process across time." With the release team, we have new shadows that join every single release cycle. With this, we're growing with this community. We're growing the release team members. We're growing SIG Release. We're growing the Kubernetes community itself. For a lot of people, this is their first time contributing to open source, so that's why I say it's their new open source frontier.
-->
&lt;p>&lt;strong>CRAIG BOX：现在，我明白了，主题来自于 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-release/charter.md">SIG 发布章程&lt;/a>？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。SIG 发布章程中有一句话，“确保有一个一致的社区成员小组来支持不同时期的发布过程。”
在发布团队中，我们每一个发布周期都有新的影子加入。有了这个，我们与这个社区一起成长。我们正在壮大发布团队的成员。
我们正在增加 SIG 版本。我们正在发展 Kubernetes 社区本身。对于很多人来说，这是他们第一次为开源做出贡献，所以我说这是他们新的开源前沿。&lt;/p>
&lt;!--
**CRAIG BOX: And the logo is obviously very Star Trek-inspired. It sort of surprised me that it took that long for someone to go this route.**
REY LEJANO: I was very surprised as well. I had to relearn Adobe Illustrator to create the logo.
-->
&lt;p>&lt;strong>CRAIG BOX：而这个标志显然是受《星际迷航》的启发。让我感到惊讶的是，花了那么长时间才有人走这条路&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：我也很惊讶。我不得不重新学习 Adobe Illustrator 来创建标志。&lt;/p>
&lt;!--
**CRAIG BOX: This your own work, is it?**
REY LEJANO: This is my own work.
-->
&lt;p>&lt;strong>CRAIG BOX：这是你自己的作品，是吗？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：这是我自己的作品。&lt;/p>
&lt;!--
**CRAIG BOX: It's very nice.**
REY LEJANO: Thank you very much. Funny, the galaxy actually took me the longest time versus the ship. Took me a few days to get that correct. I'm always fine-tuning it, so there might be a final change when this is actually released.
-->
&lt;p>&lt;strong>CRAIG BOX：非常好。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：谢谢。有趣的是，相对于飞船，银河系实际上花了我最长的时间。我花了几天时间才把它弄正确。
我一直在对它进行微调，所以在真正发布时可能会有最后的改变。&lt;/p>
&lt;!--
**CRAIG BOX: No frontier is ever truly final.**
REY LEJANO: True, very true.
-->
&lt;p>&lt;strong>CRAIG BOX：没有边界是真正的终结。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的，非常正确。&lt;/p>
&lt;!--
**CRAIG BOX: Moving now from the theme of the release to the substance, perhaps, what is new in 1.23?**
REY LEJANO: We have 47 enhancements. I'm going to run through most of the stable ones, if not all of them, some of the key Beta ones, and a few of the Alpha enhancements for 1.23.
-->
&lt;p>&lt;strong>CRAIG BOX：现在从发布的主题转到实质内容，也许，1.23 中有什么新内容？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：我们有 47 项增强功能。我将运行大部分稳定的，甚至全部的，一些关键的 Beta 版，以及一些 1.23 版的 Alpha 增强。&lt;/p>
&lt;!--
One of the key enhancements is [dual-stack IPv4/IPv6](https://github.com/kubernetes/enhancements/issues/563), which went GA in 1.23.
Some background info: dual-stack was introduced as Alpha in 1.15. You probably saw a keynote at KubeCon 2019. Back then, the way dual-stack worked was that you needed two services — you needed a service per IP family. You would need a service for IPv4 and a service for IPv6. It was refactored in 1.20. In 1.21, it was in Beta; clusters were enabled to be dual-stack by default.
-->
&lt;p>其中一个关键的改进是&lt;a href="https://github.com/kubernetes/enhancements/issues/563">双堆栈 IPv4/IPv6&lt;/a>，它在 1.23 版本中采用了 GA。&lt;/p>
&lt;p>一些背景信息：双堆栈在 1.15 中作为 Alpha 引入。你可能在 KubeCon 2019 上看到了一个主题演讲。
那时，双栈的工作方式是，你需要两个服务--你需要每个IP家族的服务。你需要一个用于 IPv4 的服务和一个用于 IPv6 的服务。
它在 1.20 版本中被重构了。在 1.21 版本中，它处于测试阶段；默认情况下，集群被启用为双堆栈。&lt;/p>
&lt;!--
And then in 1.23 we did remove the IPv6 dual-stack feature flag. It's not mandatory to use dual-stack. It's actually not "default" still. The pods, the services still default to single-stack. There are some requirements to be able to use dual-stack. The nodes have to be routable on IPv4 and IPv6 network interfaces. You need a CNI plugin that supports dual-stack. The pods themselves have to be configured to be dual-stack. And the services need the ipFamilyPolicy field to specify prefer dual-stack, or require dual-stack.
-->
&lt;p>然后在 1.23 版本中，我们确实删除了 IPv6 双栈功能标志。这不是强制性的使用双栈。它实际上仍然不是 &amp;quot;默认&amp;quot;的。
Pod，服务仍然默认为单栈。要使用双栈，有一些要求。节点必须可以在 IPv4 和 IPv6 网络接口上进行路由。
你需要一个支持双栈的 CNI 插件。Pod 本身必须被配置为双栈。而服务需要 ipFamilyPolicy 字段来指定喜欢双栈或要求双栈。&lt;/p>
&lt;!--
**CRAIG BOX: This sounds like there's an implication in this that v4 is still required. Do you see a world where we can actually move to v6-only clusters?**
REY LEJANO: I think we'll be talking about IPv4 and IPv6 for many, many years to come. I remember a long time ago, they kept saying "it's going to be all IPv6", and that was decades ago.
-->
&lt;p>&lt;strong>CRAIG BOX：这听起来暗示仍然需要 v4。你是否看到了一个我们实际上可以转移到仅有 v6 的集群的世界？？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：我认为在未来很多很多年里，我们都会谈论 IPv4 和 IPv6。我记得很久以前，他们一直在说 &amp;quot;这将全部是 IPv6&amp;quot;，而那是几十年前的事了。&lt;/p>
&lt;!--
**CRAIG BOX: I think I may have mentioned on the show before, but there was [a meeting in London that Vint Cerf attended](https://www.youtube.com/watch?v=AEaJtZVimqs), and he gave a public presentation at the time to say, now is the time of v6. And that was 10 years ago at least. It's still not the time of v6, and my desktop still doesn't have Linux on it. One day.**
REY LEJANO: [LAUGHS] In my opinion, that's one of the big key features that went stable for 1.23.
-->
&lt;p>&lt;strong>CRAIG BOX：我想我之前可能在节目中提到过，Vint Cerf &lt;a href="https://www.youtube.com/watch?v=AEaJtZVimqs">在伦敦参加了一个会议&lt;/a>，
他当时做了一个公开演讲说，现在是v6的时代了。那是至少 10 年前的事了。现在还不是 v6 的时代，我的电脑桌面上还没有一天拥有 Linux。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：[笑]在我看来，这是 1.23 版稳定的一大关键功能。&lt;/p>
&lt;!--
One of the other highlights of 1.23 is [pod security admission going to Beta](/blog/2021/12/09/pod-security-admission-beta/). I know this feature is going to Beta, but I highlight this because as some people might know, PodSecurityPolicy, which was deprecated in 1.21, is targeted to be removed in 1.25. Pod security admission replaces pod security policy. It's an admission controller. It evaluates the pods against a predefined set of pod security standards to either admit or deny the pod for running.
There's three levels of pod security standards. Privileged, that's totally open. Baseline, known privileges escalations are minimized. Or Restricted, which is hardened. And you could set pod security standards either to run in three modes, which is enforce: reject any pods that are in violation; to audit: pods are allowed to be created, but the violations are recorded; or warn: it will send a warning message to the user, and the pod is allowed.
-->
&lt;p>1.23 版的另一个亮点是 &lt;a href="https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/">Pod 安全许可进入 Beta 版&lt;/a>。
我知道这个功能将进入 Beta 版，但我强调这一点是因为有些人可能知道，PodSecurityPolicy 在 1.21 版本中被废弃，目标是在 1.25 版本中被移除。
Pod 安全接纳取代了 Pod 安全策略。它是一个准入控制器。它根据预定义的 Pod 安全标准集对 Pod 进行评估，以接纳或拒绝 Pod 的运行。&lt;/p>
&lt;p>Pod 安全标准分为三个级别。特权，这是完全开放的。基线，已知的特权升级被最小化。或者 限制级，这是强化的。而且你可以将 Pod 安全标准设置为以三种模式运行，
即强制：拒绝任何违规的 Pod；审计：允许创建 Pod，但记录违规行为；或警告：它会向用户发送警告消息，并且允许该 Pod。&lt;/p>
&lt;!--
**CRAIG BOX: You mentioned there that PodSecurityPolicy is due to be deprecated in two releases' time. Are we lining up these features so that pod security admission will be GA at that time?**
REY LEJANO: Yes. Absolutely. I'll talk about that for another feature in a little bit as well. There's also another feature that went to GA. It was an API that went to GA, and therefore the Beta API is now deprecated. I'll talk about that a little bit.
-->
&lt;p>&lt;strong>CRAIG BOX：你提到 PodSecurityPolicy 将在两个版本的时间内被弃用。我们是否对这些功能进行了排列，以便届时 Pod 安全接纳将成为 GA？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。当然可以。我稍后也会为另一个功能谈谈这个问题。还有另一个功能也进入了 GA。这是一个归入 GA 的 API，
因此 Beta 版的 API 现在被废弃了。我稍稍讲一下这个问题。&lt;/p>
&lt;!--
**CRAIG BOX: All right. Let's talk about what's next on the list.**
REY LEJANO: Let's move on to more stable enhancements. One is the [TTL controller](https://github.com/kubernetes/enhancements/issues/592). This cleans up jobs and pods after the jobs are finished. There is a TTL timer that starts when the job or pod is finished. This TTL controller watches all the jobs, and ttlSecondsAfterFinished needs to be set. The controller will see if the ttlSecondsAfterFinished, combined with the last transition time, if it's greater than now. If it is, then it will delete the job and the pods of that job.
-->
&lt;p>&lt;strong>CRAIG BOX：好吧。让我们来谈谈名单上的下一个问题。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：让我们继续讨论更稳定的增强功能。一种是 &lt;a href="https://github.com/kubernetes/enhancements/issues/592">TTL 控制器&lt;/a>。
它在作业完成后清理作业和 Pod。有一个 TTL 计时器在作业或 Pod 完成后开始计时。此 TTL 控制器监视所有作业，
并且需要设置 ttlSecondsAfterFinished。该控制器将查看 ttlSecondsAfterFinished，结合最后的过渡时间，如果它大于现在。
如果是，那么它将删除该作业和该作业的 Pod。&lt;/p>
&lt;!--
**CRAIG BOX: Loosely, it could be called a garbage collector?**
REY LEJANO: Yes. Garbage collector for pods and jobs, or jobs and pods.
-->
&lt;p>&lt;strong>CRAIG BOX：粗略地说，它可以称为垃圾收集器吗？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。用于 Pod 和作业，或作业和 Pod 的垃圾收集器。&lt;/p>
&lt;!--
**CRAIG BOX: If Kubernetes is truly becoming a programming language, it of course has to have a garbage collector implemented.**
REY LEJANO: Yeah. There's another one, too, coming in Alpha. [CHUCKLES]
-->
&lt;p>&lt;strong>CRAIG BOX：如果 Kubernetes 真正成为一种编程语言，它当然必须实现垃圾收集器。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。还有另一个，也将在 Alpha 中出现。[笑]&lt;/p>
&lt;!--
**CRAIG BOX: Tell me about that.**
REY LEJANO: That one is coming in in Alpha. It's actually one of my favorite features, because there's only a few that I'm going to highlight today. [PVCs for StafeulSet will be cleaned up](https://github.com/kubernetes/enhancements/issues/1847). It will auto-delete PVCs created by StatefulSets, when you delete that StatefulSet.
-->
&lt;p>&lt;strong>CRAIG BOX：告诉我。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO： 那个是在 Alpha 中出现的。这实际上是我最喜欢的功能之一，今天我只想强调几个。
&lt;a href="https://github.com/kubernetes/enhancements/issues/1847">StafeulSet 的 PVC 将被清理&lt;/a>。
当你删除那个 StatefulSet 时，它将自动删除由 StatefulSets 创建的 PVC。&lt;/p>
&lt;!--
**CRAIG BOX: What's next on our tour of stable features?**
REY LEJANO: Next one is, [skip volume ownership change goes to stable](https://github.com/kubernetes/enhancements/issues/695). This is from SIG Storage. There are times when you're running a stateful application, like many databases, they're sensitive to permission bits changing underneath. Currently, when a volume is bind mounted inside the container, the permissions of that volume will change recursively. It might take a really long time.
-->
&lt;p>&lt;strong>CRAIG BOX：我们的稳定功能之旅的下一步是什么？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：下一个是，&lt;a href="https://github.com/kubernetes/enhancements/issues/695">跳过卷所有权更改进入稳定状态&lt;/a>。
这是来自 SIG 存储。有的时候，当你运行一个有状态的应用程序时，就像许多数据库一样，它们对下面的权限位变化很敏感。
目前，当一个卷被绑定安装在容器内时，该卷的权限将递归更改。这可能需要很长时间。
--&amp;gt;&lt;/p>
&lt;!--
Now, there's a field, the fsGroupChangePolicy, which allows you, as a user, to tell Kubernetes how you want the permission and ownership change for that volume to happen. You can set it to always, to always change permissions, or just on mismatch, to only do it when the permission ownership changes at the top level is different from what is expected.
-->
&lt;p>现在，有一个字段，即 fsGroupChangePolicy，它允许你作为用户告诉 Kubernetes 你希望如何更改该卷的权限和所有权。
你可以将其设置为总是、始终更改权限，或者只是在不匹配的情况下，只在顶层的权限所有权变化与预期不同的情况下进行。&lt;/p>
&lt;!--
**CRAIG BOX: It does feel like a lot of these enhancements came from a very particular use case where someone said, "hey, this didn't work for me and I've plumbed in a feature that works with exactly the thing I need to have".**
REY LEJANO: Absolutely. People create issues for these, then create Kubernetes enhancement proposals, and then get targeted for releases.
-->
&lt;p>&lt;strong>CRAIG BOX：确实感觉很多这些增强功能都来自一个非常特殊的用例，有人说，“嘿，这对我来说不起作用，我已经研究了一个功能，它可以完全满足我需要的东西”&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：当然可以。人们为这些问题创建问题，然后创建 Kubernetes 增强提案，然后被列为发布目标。&lt;/p>
&lt;!--
**CRAIG BOX: Another GA feature in this release — ephemeral volumes.**
REY LEJANO: We've always been able to use empty dir for ephemeral volumes, but now we could actually have [ephemeral inline volumes](https://github.com/kubernetes/enhancements/issues/1698), meaning that you could take your standard CSI driver and be able to use ephemeral volumes with it.
-->
&lt;p>&lt;strong>CRAIG BOX：此版本中的另一个 GA 功能--临时卷。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：我们一直能够将空目录用于临时卷，但现在我们实际上可以拥有[临时内联卷] (&lt;a href="https://github.com/kubernetes/enhancements/issues/1698">https://github.com/kubernetes/enhancements/issues/1698&lt;/a>)，
这意味着你可以使用标准 CSI 驱动程序并能够与它一起使用临时卷。&lt;/p>
&lt;!--
**CRAIG BOX: And, a long time coming, [CronJobs](https://github.com/kubernetes/enhancements/issues/19).**
REY LEJANO: CronJobs is a funny one, because it was stable before 1.23. For 1.23, it was still tracked,but it was just cleaning up some of the old controller. With CronJobs, there's a v2 controller. What was cleaned up in 1.23 is just the old v1 controller.
-->
&lt;p>&lt;strong>CRAIG BOX：而且，很长一段时间，&lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs&lt;/a>。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：CronJobs 很有趣，因为它在 1.23 之前是稳定的。对于 1.23，它仍然被跟踪，但它只是清理了一些旧控制器。
使用 CronJobs，有一个 v2 控制器。1.23 中清理的只是旧的 v1 控制器。&lt;/p>
&lt;!--
**CRAIG BOX: Were there any other duplications or major cleanups of note in this release?**
REY LEJANO: Yeah. There were a few you might see in the major themes. One's a little tricky, around FlexVolumes. This is one of the efforts from SIG Storage. They have an effort to migrate in-tree plugins to CSI drivers. This is a little tricky, because FlexVolumes were actually deprecated in November 2020. We're [formally announcing it in 1.23](https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors).
-->
&lt;p>&lt;strong>CRAIG BOX：在这个版本中，是否有任何其他的重复或重大的清理工作值得注意？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。有几个你可能会在主要的主题中看到。其中一个有点棘手，围绕 FlexVolumes。这是 SIG 存储公司的努力之一。
他们正在努力将树内插件迁移到 CSI 驱动。这有点棘手，因为 FlexVolumes 实际上是在 2020 年 11 月被废弃的。我们
&lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors">在 1.23 中正式宣布&lt;/a>。&lt;/p>
&lt;!--
**CRAIG BOX: FlexVolumes, in my mind, predate CSI as a concept. So it's about time to get rid of them.**
REY LEJANO: Yes, it is. There's another deprecation, just some [klog specific flags](https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog), but other than that, there are no other big deprecations in 1.23.
-->
&lt;p>&lt;strong>CRAIG BOX：在我看来，FlexVolumes 比 CSI 这个概念还要早。所以现在是时候摆脱它们了。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。还有另一个弃用，只是一些 &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog">klog 特定标志&lt;/a>，但除此之外，1.23 中没有其他大的弃用。&lt;/p>
&lt;!--
**CRAIG BOX: The buzzword of the last KubeCon, and in some ways the theme of the last 12 months, has been secure software supply chain. What work is Kubernetes doing to improve in this area?**
REY LEJANO: For 1.23, Kubernetes is now SLSA compliant at Level 1, which means that provenance attestation files that describe the staging and release phases of the release process are satisfactory for the SLSA framework.
-->
&lt;p>&lt;strong>CRAIG BOX：上一届 KubeCon 的流行语，在某种程度上也是过去 12 个月的主题，是安全的软件供应链。Kubernetes 在这一领域做了哪些改进工作？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：对于 1.23 版本，Kubernetes 现在符合 SLSA 的 1 级标准，这意味着描述发布过程中分期和发布阶段的证明文件对于 SLSA 框架来说是令人满意的。&lt;/p>
&lt;!--
**CRAIG BOX: What needs to happen to step up to further levels?**
REY LEJANO: Level 1 means a few things — that the build is scripted; that the provenance is available, meaning that the artifacts are verified and they're handed over from one phase to the next; and describes how the artifact is produced. Level 2 means that the source is version-controlled, which it is, provenance is authenticated, provenance is service-generated, and there is a build service. There are four levels of SLSA compliance.
-->
&lt;p>&lt;strong>CRAIG BOX：需要做什么才能提升到更高的水平？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：级别 1 意味着一些事情——构建是脚本化的；出处是可用的，这意味着工件是经过验证，并且已从一个阶段移交到下一个阶段；
并描述了工件是如何产生的。级别 2 意味着源是受版本控制的，也就是说，源是经过身份验证的，源是服务生成的，并且存在构建服务。SLSA 的合规性分为四个级别。&lt;/p>
&lt;!--
**CRAIG BOX: It does seem like the levels were largely influenced by what it takes to build a big, secure project like this. It doesn't seem like it will take a lot of extra work to move up to verifiable provenance, for example. There's probably just a few lines of script required to meet many of those requirements.**
REY LEJANO: Absolutely. I feel like we're almost there; we'll see what will come out of 1.24. And I do want to give a big shout-out to SIG Release and Release Engineering, primarily to Adolfo García Veytia, who is aka Puerco on GitHub and on Slack. He's been driving this forward.
-->
&lt;p>&lt;strong>CRAIG BOX：看起来这些水平在很大程度上受到了建立这样一个大型安全项目的影响。例如，似乎不需要很多额外的工作来提升到可验证的出处。
可能只需要几行脚本即可满足其中许多要求。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：当然。我觉得我们就快成功了；我们会看到 1.24 版本会出现什么。我确实想对 SIG 发布和发布工程部大加赞赏，
主要是 Adolfo García Veytia，他在 GitHub 和 Slack 上又名 Puerco。 他一直在推动这一进程。&lt;/p>
&lt;!--
**CRAIG BOX: You've mentioned some APIs that are being graduated in time to replace their deprecated version. Tell me about the new HPA API.**
REY LEJANO: The [horizontal pod autoscaler v2 API](https://github.com/kubernetes/enhancements/issues/2702), is now stable, which means that the v2beta2 API is deprecated. Just for everyone's knowledge, the v1 API is not being deprecated. The difference is that v2 adds support for multiple and custom metrics to be used for HPA.
-->
&lt;p>&lt;strong>CRAIG BOX：你提到了一些 API 正在及时升级以替换其已弃用的版本。告诉我有关新 HPA API 的信息。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：&lt;a href="https://github.com/kubernetes/enhancements/issues/2702">horizontal pod autoscaler v2 API&lt;/a>，
现已稳定，这意味着 v2beta2 API 已弃用。众所周知，v1 API 并未被弃用。不同之处在于 v2 添加了对用于 HPA 的多个和自定义指标的支持。&lt;/p>
&lt;!--
**CRAIG BOX: There's also now a facility to validate my CRDs with an expression language.**
REY LEJANO: Yeah. You can use the [Common Expression Language, or CEL](https://github.com/google/cel-spec), to validate your CRDs, so you no longer need to use webhooks. This also makes the CRDs more self-contained and declarative, because the rules are now kept within the CRD object definition.
-->
&lt;p>&lt;strong>CRAIG BOX：现在还可以使用表达式语言验证我的 CRD。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。你可以使用 &lt;a href="https://github.com/google/cel-spec">通用表达式语言，或 CEL&lt;/a>
来验证你的 CRD，因此你不再需要使用 webhook。这也使 CRD 更加自包含和声明性，因为规则现在保存在 CRD 对象定义中。&lt;/p>
&lt;!--
**CRAIG BOX: What new features, perhaps coming in Alpha or Beta, have taken your interest?**
REY LEJANO: Aside from pod security policies, I really love [ephemeral containers](https://github.com/kubernetes/enhancements/issues/277) supporting kubectl debug. It launches an ephemeral container and a running pod, shares those pod namespaces, and you can do all your troubleshooting with just running kubectl debug.
-->
&lt;p>&lt;strong>CRAIG BOX：哪些新功能（可能是 Alpha 版或 Beta 版）引起了你的兴趣？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：除了 Pod 安全策略，我真的很喜欢支持 kubectl 调试的&lt;a href="https://github.com/kubernetes/enhancements/issues/277">临时容器&lt;/a>。
它启动一个临时容器和一个正在运行的 Pod，共享这些 Pod 命名空间，你只需运行 kubectl debug 即可完成所有故障排除。&lt;/p>
&lt;!--
**CRAIG BOX: There's also been some interesting changes in the way that events are handled with kubectl.**
REY LEJANO: Yeah. kubectl events has always had some issues, like how things weren't sorted. [kubectl events improved](https://github.com/kubernetes/enhancements/issues/1440) that so now you can do `--watch`, and it will also sort with the `--watch` option as well. That is something new. You can actually combine fields and custom columns. And also, you can list events in the timeline with doing the last N number of minutes. And you can also sort events using other criteria as well.
-->
&lt;p>&lt;strong>CRAIG BOX：使用 kubectl 处理事件的方式也发生了一些有趣的变化。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。kubectl events 总是有一些问题，比如事情没有排序。
&lt;a href="https://github.com/kubernetes/enhancements/issues/1440">kubectl 事件得到了改进&lt;/a>，
所以现在你可以使用 &lt;code>--watch&lt;/code>，它也可以使用 &lt;code>--watch&lt;/code> 选项进行排序。那是新事物。
你实际上可以组合字段和自定义列。此外，你可以在时间线中列出最后 N 分钟的事件。你还可以使用其他标准对事件进行排序。&lt;/p>
&lt;!--
**CRAIG BOX: You are a field engineer at SUSE. Are there any things that are coming in that your individual customers that you deal with are looking out for?**
REY LEJANO: More of what I look out for to help the customers.
-->
&lt;p>&lt;strong>CRAIG BOX：你是 SUSE 的一名现场工程师。有什么事情是你所处理的个别客户所要注意的吗？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：更多我期待帮助客户的东西。&lt;/p>
&lt;!--
**CRAIG BOX: Right.**
REY LEJANO: I really love kubectl events. Really love the PVCs being cleaned up with StatefulSets. Most of it's for selfish reasons that it will improve troubleshooting efforts. [CHUCKLES]
-->
&lt;p>&lt;strong>CRAIG BOX：好吧。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：我真的很喜欢 kubectl 事件。真的很喜欢用 StatefulSets 清理的 PVC。其中大部分是出于自私的原因，它将改进故障排除工作。[笑]&lt;/p>
&lt;!--
**CRAIG BOX: I have always hoped that a release team lead would say to me, "yes, I have selfish reasons. And I finally got something I wanted in."**
REY LEJANO: [LAUGHS]
-->
&lt;p>&lt;strong>CRAIG BOX：我一直希望发布团队负责人对我说：“是的，我有自私的理由。我终于得到了我想要的东西。”&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：[大笑]&lt;/p>
&lt;!--
**CRAIG BOX: Perhaps I should run to be release team lead, just so I can finally get init containers fixed once and for all.**
REY LEJANO: Oh, init containers, I've been looking for that for a while. I've actually created animated GIFs on how init containers will be run with that Kubernetes enhancement proposal, but it's halted currently.
-->
&lt;p>&lt;strong>CRAIG BOX：也许我应该竞选发布团队的负责人，这样我就可以最终让 Init 容器一劳永逸地得到修复。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：哦，Init 容器，我一直在寻找它。实际上，我已经制作了 GIF 动画，介绍了 Init 容器将如何与那个 Kubernetes 增强提案一起运行，但目前已经停止了。&lt;/p>
&lt;!--
**CRAIG BOX: One day.**
REY LEJANO: One day. Maybe I shouldn't stay halted.
-->
&lt;p>&lt;strong>CRAIG BOX：有一天。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：总有一天。也许我不应该停下来。&lt;/p>
&lt;!--
**CRAIG BOX: You mentioned there are obviously the things you look out for. Are there any things that are coming down the line, perhaps Alpha features or maybe even just proposals you've seen lately, that you're personally really looking forward to seeing which way they go?**
REY LEJANO: Yeah. Oone is a very interesting one, it affects the whole community, so it's not just for personal reasons. As you may have known, Dockershim is deprecated. And we did release a blog that it will be removed in 1.24.
-->
&lt;p>&lt;strong>CRAIG BOX：你提到的显然是你所关注的事情。是否有任何即将推出的东西，可能是 Alpha 功能，甚至可能只是你最近看到的建议，你个人真的很期待看到它们的发展方向？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。Oone 是一个非常有趣的问题，它影响了整个社区，所以这不仅仅是出于个人原因。
正如你可能已经知道的，Dockershim 已经被废弃了。而且我们确实发布了一篇博客，说它将在 1.24 中被删除。&lt;/p>
&lt;!--
**CRAIG BOX: Scared a bunch of people.**
REY LEJANO: Scared a bunch of people. From a survey, we saw that a lot of people are still using Docker and Dockershim. One of the enhancements for 1.23 is, [kubelet CRI goes to Beta](https://github.com/kubernetes/enhancements/issues/2040). This promotes the CRI API, which is required. This had to be in Beta for Dockershim to be removed in 1.24.
-->
&lt;p>&lt;strong>CRAIG BOX：吓坏了一群人。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：吓坏了一群人。从一项调查中，我们看到很多人仍在使用 Docker 和 Dockershim。
1.23 的增强功能之一是 &lt;a href="https://github.com/kubernetes/enhancements/issues/2040">kubelet CRI 进入 Beta 版&lt;/a>。
这促进了 CRI API 的发展，而这是必需的。 这必须是 Beta 版才能在 1.24 中删除 Dockershim。&lt;/p>
&lt;!--
**CRAIG BOX: Now, in the last release team lead interview, [we spoke with Savitha Raghunathan](https://kubernetespodcast.com/episode/157-kubernetes-1.22/), and she talked about what she would advise you as her successor. It was to look out for the mental health of the team members. How were you able to take that advice on board?**
REY LEJANO: That was great advice from Savitha. A few things I've made note of with each release team meeting. After each release team meeting, I stop the recording, because we do record all the meetings and post them on YouTube. And I open up the floor to anyone who wants to say anything that's not recorded, that's not going to be on the agenda. Also, I tell people not to work on weekends. I broke this rule once, but other than that, I told people it could wait. Just be mindful of your mental health.
-->
&lt;p>**CRAIG BOX：现在，在最后一次发布团队领导访谈中，&lt;a href="https://kubernetespodcast.com/episode/157-kubernetes-1.22/">我们与 Savitha Raghunathan 进行了交谈&lt;/a>，
她谈到了作为她的继任者她会给你什么建议。她说要关注团队成员的心理健康。你是如何采纳这个建议的？&lt;/p>
&lt;p>REY LEJANO：Savitha 的建议很好。我在每次发布团队会议上都记录了一些事情。
每次发布团队会议后，我都会停止录制，因为我们确实会录制所有会议并将其发布到 YouTube 上。
我向任何想要说任何未记录的内容的人开放发言，这不会出现在议程上。此外，我告诉人们不要在周末工作。
我曾经打破过这个规则，但除此之外，我告诉人们它可以等待。只要注意你的心理健康。&lt;/p>
&lt;!--
**CRAIG BOX: It's just been announced that [James Laverack from Jetstack](https://twitter.com/JamesLaverack/status/1466834312993644551) will be the release team lead for 1.24. James and I shared an interesting Mexican dinner at the last KubeCon in San Diego.**
REY LEJANO: Oh, nice. I didn't know you knew James.
-->
&lt;p>&lt;strong>CRAIG BOX：刚刚宣布&lt;a href="https://twitter.com/JamesLaverack/status/1466834312993644551">来自 Jetstack 的 James Laverack&lt;/a>
将成为 1.24 的发布团队负责人。James 和我在 San Diego 的最后一届 KubeCon 上分享了一顿有趣的墨西哥晚餐。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：哦，不错。我不知道你认识 James。&lt;/p>
&lt;!--
**CRAIG BOX: The British tech scene. We're a very small world. What will your advice to James be?**
REY LEJANO: What I would tell James for 1.24 is use teachable moments in the release team meetings. When you're a shadow for the first time, it's very daunting. It's very difficult, because you don't know the repos. You don't know the release process. Everyone around you seems like they know the release process, and very familiar with what the release process is. But as a first-time shadow, you don't know all the vernacular for the community. I just advise to use teachable moments. Take a few minutes in the release team meetings to make it a little easier for new shadows to ramp up and to be familiar with the release process.
-->
&lt;p>&lt;strong>CRAIG BOX：英国科技界。我们是一个非常小的世界。你对 James 的建议是什么？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：对于 1.24，我要告诉 James 的是在发布团队会议中使用教学时刻。当你第一次成为影子时，这是非常令人生畏的。
这非常困难，因为你不知道存储库。你不知道发布过程。周围的每个人似乎都知道发布过程，并且非常熟悉发布过程是什么。
但作为第一次出现的影子，你并不了解社区的所有白话。我只是建议使用教学时刻。在发布团队会议上花几分钟时间，让新影子更容易上手并熟悉发布过程。&lt;/p>
&lt;!--
**CRAIG BOX: Has there been major evolution in the process in the time that you've been involved? Or do you think that it's effectively doing what it needs to do?**
REY LEJANO: It's always evolving. I remember my first time in release notes, 1.18, we said that our goal was to automate and program our way out so that we don't have a release notes team anymore. That's changed [CHUCKLES] quite a bit. Although there's been significant advancements in the release notes process by Adolfo and also James, they've created a subcommand in krel to generate release notes.
-->
&lt;p>&lt;strong>CRAIG BOX：在你参与的这段时间里，这个过程是否有重大演变？或者你认为它正在有效地做它需要做的事情？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：它总是在不断发展。我记得我第一次做发布说明时，1.18，我们说我们的目标是自动化和编程，这样我们就不再有发行说明团队了。
这改变了很多[笑]。尽管 Adolfo 和 James 在发布说明过程中取得了重大进展，但他们在 krel 中创建了一个子命令来生成发行说明。&lt;/p>
&lt;!--
But nowadays, all their release notes are richer. Still not there at the automation process yet. Every release cycle, there is something a little bit different. For this release cycle, we had a production readiness review deadline. It was a soft deadline. A production readiness review is a review by several people in the community. It's actually been required since 1.21, and it ensures that the enhancements are observable, scalable, supportable, and it's safe to operate in production, and could also be disabled or rolled back. In 1.23, we had a deadline to have the production readiness review completed by a specific date.
-->
&lt;p>但如今，他们所有的发行说明都更加丰富了。在自动化过程中，仍然没有达到。每个发布周期，都有一点不同的东西。
对于这个发布周期，我们有一个生产就绪审查截止日期。这是一个软期限。生产就绪审查是社区中几个人的审查。
实际上从 1.21 开始就需要它，它确保增强是可观察的、可扩展的、可支持的，并且在生产中运行是安全的，也可以被禁用或回滚。
在 1.23 中，我们有一个截止日期，要求在特定日期之前完成生产就绪审查。&lt;/p>
&lt;!--
**CRAIG BOX: How have you found the change of schedule to three releases per year rather than four?**
REY LEJANO: Moving to three releases a year from four, in my opinion, has been an improvement, because we support the last three releases, and now we can actually support the last releases in a calendar year instead of having 9 months out of 12 months of the year.
-->
&lt;p>&lt;strong>CRAIG BOX：你如何发现每年发布三个版本，而不是四个版本？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：从一年四个版本转为三个版本，在我看来是一种进步，因为我们支持最后三个版本，
现在我们实际上可以支持在一个日历年内的最后一个版本，而不是在 12 个月中只有 9 个月。&lt;/p>
&lt;!--
**CRAIG BOX: The next event on the calendar is a [Kubernetes contributor celebration](https://www.kubernetes.dev/events/kcc2021/) starting next Monday. What can we expect from that event?**
REY LEJANO: This is our second time running this virtual event. It's a virtual celebration to recognize the whole community and all of our accomplishments of the year, and also contributors. There's a number of events during this week of celebration. It starts the week of December 13.
-->
&lt;p>&lt;strong>CRAIG BOX：日历上的下一个活动是下周一开始的 &lt;a href="https://www.kubernetes.dev/events/kcc2021/">Kubernetes 贡献者庆典&lt;/a>。我们可以从活动中期待什么？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：这是我们第二次举办这个虚拟活动。这是一个虚拟的庆祝活动，以表彰整个社区和我们今年的所有成就，以及贡献者。
在这周的庆典中有许多活动。它从 12 月 13 日的那一周开始。&lt;/p>
&lt;!--
There's events like the Kubernetes Contributor Awards, where SIGs honor and recognize the hard work of the community and contributors. There's also a DevOps party game as well. There is a cloud native bake-off. I do highly suggest people to go to [kubernetes.dev/celebration](https://www.kubernetes.dev/events/past-events/2021/kcc2021/) to learn more.
-->
&lt;p>有像 Kubernetes 贡献者奖这样的活动，SIG 对社区和贡献者的辛勤工作进行表彰和奖励。
也有一个 DevOps 聚会游戏。还有一个云原生的烘烤活动。我强烈建议人们去
&lt;a href="https://www.kubernetes.dev/events/past-events/2021/kcc2021/">kubernetes.dev/celebration&lt;/a>
了解更多。&lt;/p>
&lt;!--
**CRAIG BOX: How exactly does one judge a virtual bake-off?**
REY LEJANO: That I don't know. [CHUCKLES]
-->
&lt;p>&lt;strong>CRAIG BOX： 究竟如何评判一个虚拟的烘焙比赛呢？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：那我不知道。[笑]&lt;/p>
&lt;!--
**CRAIG BOX: I tasted my scones. I think they're the best. I rate them 10 out of 10.**
REY LEJANO: Yeah. That is very difficult to do virtually. I would have to say, probably what the dish is, how closely it is tied with Kubernetes or open source or to CNCF. There's a few judges. I know Josh Berkus and Rin Oliver are a few of the judges running the bake-off.
-->
&lt;p>&lt;strong>CRAIG BOX：我尝了尝我的烤饼。我认为他们是最好的。我给他们打了 10 分（满分 10 分）。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：是的。这是很难做到的。我不得不说，这道菜可能是什么，它与 Kubernetes 或开源或与 CNCF 的关系有多密切。
有几个评委。我知道 Josh Berkus 和 Rin Oliver 是主持烘焙比赛的几个评委。&lt;/p>
&lt;!--
**CRAIG BOX: Yes. We spoke with Josh about his love of the kitchen, and so he seems like a perfect fit for that role.**
REY LEJANO: He is.
-->
&lt;p>&lt;strong>CRAIG BOX：是的。我们与 Josh 谈到了他对厨房的热爱，因此他似乎非常适合这个角色。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：他是。&lt;/p>
&lt;!--
**CRAIG BOX: Finally, your wife and yourself are expecting your first child in January. Have you had a production readiness review for that?**
REY LEJANO: I think we failed that review. [CHUCKLES]
-->
&lt;p>&lt;strong>CRAIG BOX：最后，你的妻子和你自己将在一月份迎来你们的第一个孩子。你是否为此进行过生产准备审查？&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：我认为我们没有通过审查。[笑]&lt;/p>
&lt;!--
**CRAIG BOX: There's still time.**
REY LEJANO: We are working on refactoring. We're going to refactor a little bit in December, and `--apply` again.
-->
&lt;p>&lt;strong>CRAIG BOX：还有时间。&lt;/strong>&lt;/p>
&lt;p>REY LEJANO：我们正在努力重构。我们将在 12 月进行一些重构，然后再次使用 &lt;code>--apply&lt;/code>。&lt;/p>
&lt;hr>
&lt;!--
_[Rey Lejano](https://twitter.com/reylejano) is a field engineer at SUSE, by way of Rancher Labs, and was the release team lead for Kubernetes 1.23. He is now also a co-chair for SIG Docs. His son Liam is now 3 and a half months old._
_You can find the [Kubernetes Podcast from Google](http://www.kubernetespodcast.com/) at [@KubernetesPod](https://twitter.com/KubernetesPod) on Twitter, and you can [subscribe](https://kubernetespodcast.com/subscribe/) so you never miss an episode._
-->
&lt;p>&lt;strong>&lt;a href="https://twitter.com/reylejano">Rey Lejano&lt;/a> 是 SUSE 的一名现场工程师，来自 Rancher Labs，并且是 Kubernetes 1.23 的发布团队负责人。
他现在也是 SIG Docs 的联合主席。他的儿子 Liam 现在 3 个半月大。&lt;/strong>&lt;/p>
&lt;p>&lt;strong>你可以在 Twitter 上的 &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a>
找到&lt;a href="http://www.kubernetespodcast.com/">来自谷歌的 Kubernetes 播客&lt;/a>，
你也可以&lt;a href="https://kubernetespodcast.com/subscribe/">订阅&lt;/a>，这样你就不会错过任何一集。&lt;/strong>&lt;/p></description></item><item><title>Blog: 在 Ingress-NGINX v1.2.0 中提高安全标准</title><link>https://kubernetes.io/zh-cn/blog/2022/04/28/ingress-nginx-1-2-0/</link><pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/04/28/ingress-nginx-1-2-0/</guid><description>
&lt;!--
layout: blog
title: 'Increasing the security bar in Ingress-NGINX v1.2.0'
date: 2022-04-28
slug: ingress-nginx-1-2-0
-->
&lt;!--
**Authors:** Ricardo Katz (VMware), James Strong (Chainguard)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Ricardo Katz (VMware), James Strong (Chainguard)&lt;/p>
&lt;!--
The [Ingress](/docs/concepts/services-networking/ingress/) may be one of the most targeted components
of Kubernetes. An Ingress typically defines an HTTP reverse proxy, exposed to the Internet, containing
multiple websites, and with some privileged access to Kubernetes API (such as to read Secrets relating to
TLS certificates and their private keys).
-->
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/">Ingress&lt;/a> 可能是 Kubernetes 最容易受攻击的组件之一。
Ingress 通常定义一个 HTTP 反向代理，暴露在互联网上，包含多个网站，并具有对 Kubernetes API
的一些特权访问（例如读取与 TLS 证书及其私钥相关的 Secret）。&lt;/p>
&lt;!--
While it is a risky component in your architecture, it is still the most popular way to properly expose your services.
-->
&lt;p>虽然它是架构中的一个风险组件，但它仍然是正常公开服务的最流行方式。&lt;/p>
&lt;!--
Ingress-NGINX has been part of security assessments that figured out we have a big problem: we don't
do all proper sanitization before turning the configuration into an `nginx.conf` file, which may lead to information
disclosure risks.
-->
&lt;p>Ingress-NGINX 一直是安全评估的重头戏，这类评估会发现我们有着很大的问题：
在将配置转换为 &lt;code>nginx.conf&lt;/code> 文件之前，我们没有进行所有适当的清理，这可能会导致信息泄露风险。&lt;/p>
&lt;!--
While we understand this risk and the real need to fix this, it's not an easy process to do, so we took another approach to reduce (but not remove!) this risk in the current (v1.2.0) release.
-->
&lt;p>虽然我们了解此风险以及解决此问题的真正需求，但这并不是一个容易的过程，
因此我们在当前(v1.2.0)版本中采取了另一种方法来减少（但不是消除！）这种风险。&lt;/p>
&lt;!--
## Meet Ingress NGINX v1.2.0 and the chrooted NGINX process
-->
&lt;h2 id="了解-ingress-nginx-v1-2-0-和-chrooted-nginx-进程">了解 Ingress NGINX v1.2.0 和 chrooted NGINX 进程&lt;/h2>
&lt;!--
One of the main challenges is that Ingress-NGINX runs the web proxy server (NGINX) alongside the Ingress
controller (the component that has access to Kubernetes API that and that creates the `nginx.conf` file).
-->
&lt;p>主要挑战之一是 Ingress-NGINX 运行着 Web 代理服务器（NGINX），并与 Ingress 控制器一起运行
（后者是一个可以访问 Kubernetes API 并创建 &lt;code>nginx.conf&lt;/code> 的组件）。&lt;/p>
&lt;!--
So, NGINX does have the same access to the filesystem of the controller (and Kubernetes service account token, and other configurations from the container). While splitting those components is our end goal, the project needed a fast response; that lead us to the idea of using `chroot()`.
-->
&lt;p>因此，NGINX 对控制器的文件系统（和 Kubernetes 服务帐户令牌，以及容器中的其他配置）具有相同的访问权限。
虽然拆分这些组件是我们的最终目标，但该项目需要快速响应；这让我们想到了使用 &lt;code>chroot()&lt;/code>。&lt;/p>
&lt;!--
Let's take a look into what an Ingress-NGINX container looked like before this change:
-->
&lt;p>让我们看一下 Ingress-NGINX 容器在此更改之前的样子：&lt;/p>
&lt;p>&lt;img src="ingress-pre-chroot.png" alt="Ingress NGINX pre chroot">&lt;/p>
&lt;!--
As we can see, the same container (not the Pod, the container!) that provides HTTP Proxy is the one that watches Ingress objects and writes the Container Volume
-->
&lt;p>正如我们所见，用来提供 HTTP Proxy 的容器（不是 Pod，是容器！）也是是监视 Ingress
对象并将数据写入容器卷的容器。&lt;/p>
&lt;!--
Now, meet the new architecture:
-->
&lt;p>现在，见识一下新架构：&lt;/p>
&lt;p>&lt;img src="ingress-post-chroot.png" alt="Ingress NGINX post chroot">&lt;/p>
&lt;!--
What does all of this mean? A basic summary is: that we are isolating the NGINX service as a container inside the
controller container.
-->
&lt;p>这一切意味着什么？一个基本的总结是：我们将 NGINX 服务隔离为控制器容器内的容器。&lt;/p>
&lt;!--
While this is not strictly true, to understand what was done here, it's good to understand how
Linux containers (and underlying mechanisms such as kernel namespaces) work.
You can read about cgroups in the Kubernetes glossary: [`cgroup`](https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-cgroup) and learn more about cgroups interact with namespaces in the NGINX project article
[What Are Namespaces and cgroups, and How Do They Work?](https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work/).
(As you read that, bear in mind that Linux kernel namespaces are a different thing from
[Kubernetes namespaces](/docs/concepts/overview/working-with-objects/namespaces/)).
-->
&lt;p>虽然这并不完全正确，但要了解这里所做的事情，最好了解 Linux 容器（以及内核命名空间等底层机制）是如何工作的。
你可以在 Kubernetes 词汇表中阅读有关 cgroup 的信息：&lt;a href="https://kubernetes.io/zh-cn/docs/reference/glossary/?fundamental=true#term-cgroup">&lt;code>cgroup&lt;/code>&lt;/a>，
并在 NGINX 项目文章&lt;a href="https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work/">什么是命名空间和 cgroup，以及它们如何工作？&lt;/a>
中了解有关 cgroup 与命名空间交互的更多信息。（当你阅读时，请记住 Linux 内核命名空间与
&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/overview/working-with-objects/namespaces/">Kubernetes 命名空间&lt;/a>不同）。&lt;/p>
&lt;!--
## Skip the talk, what do I need to use this new approach?
-->
&lt;h2 id="跳过谈话-我需要什么才能使用这种新方法">跳过谈话，我需要什么才能使用这种新方法？&lt;/h2>
&lt;!--
While this increases the security, we made this feature an opt-in in this release so you can have
time to make the right adjustments in your environment(s). This new feature is only available from
release v1.2.0 of the Ingress-NGINX controller.
-->
&lt;p>虽然这增加了安全性，但我们在这个版本中把这个功能作为一个选项，这样你就可以有时间在你的环境中做出正确的调整。
此新功能仅在 Ingress-NGINX 控制器的 v1.2.0 版本中可用。&lt;/p>
&lt;!--
There are two required changes in your deployments to use this feature:
* Append the suffix "-chroot" to the container image name. For example: `gcr.io/k8s-staging-ingress-nginx/controller-chroot:v1.2.0`
* In your Pod template for the Ingress controller, find where you add the capability `NET_BIND_SERVICE` and add the capability `SYS_CHROOT`. After you edit the manifest, you'll see a snippet like:
-->
&lt;p>要使用这个功能，在你的部署中有两个必要的改变：&lt;/p>
&lt;ul>
&lt;li>将后缀 &amp;quot;-chroot&amp;quot; 添加到容器镜像名称中。例如：&lt;code>gcr.io/k8s-staging-ingress-nginx/controller-chroot:v1.2.0&lt;/code>&lt;/li>
&lt;li>在你的 Ingress 控制器的 Pod 模板中，找到添加 &lt;code>NET_BIND_SERVICE&lt;/code> 权能的位置并添加 &lt;code>SYS_CHROOT&lt;/code> 权能。
编辑清单后，你将看到如下代码段：&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">capabilities&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">drop&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- ALL&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">add&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- NET_BIND_SERVICE&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- SYS_CHROOT&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
If you deploy the controller using the official Helm chart then change the following setting in
`values.yaml`:
-->
&lt;p>如果你使用官方 Helm Chart 部署控制器，则在 &lt;code>values.yaml&lt;/code> 中更改以下设置：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">controller&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">chroot&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Ingress controllers are normally set up cluster-wide (the IngressClass API is cluster scoped). If you manage the
Ingress-NGINX controller but you're not the overall cluster operator, then check with your cluster admin about
whether you can use the `SYS_CHROOT` capability, **before** you enable it in your deployment.
-->
&lt;p>Ingress 控制器通常部署在集群作用域（IngressClass API 是集群作用域的）。
如果你管理 Ingress-NGINX 控制器但你不是整个集群的操作员，
请在部署中启用它&lt;strong>之前&lt;/strong>与集群管理员确认你是否可以使用 &lt;code>SYS_CHROOT&lt;/code> 功能。&lt;/p>
&lt;!--
## OK, but how does this increase the security of my Ingress controller?
Take the following configuration snippet and imagine, for some reason it was added to your `nginx.conf`:
-->
&lt;h2 id="好吧-但这如何能提高我的-ingress-控制器的安全性呢">好吧，但这如何能提高我的 Ingress 控制器的安全性呢？&lt;/h2>
&lt;p>以下面的配置片段为例，想象一下，由于某种原因，它被添加到你的 &lt;code>nginx.conf&lt;/code> 中：&lt;/p>
&lt;pre tabindex="0">&lt;code>location /randomthing/ {
alias /;
autoindex on;
}
&lt;/code>&lt;/pre>&lt;!--
If you deploy this configuration, someone can call `http://website.example/randomthing` and get some listing (and access) to the whole filesystem of the Ingress controller.
Now, can you spot the difference between chrooted and non chrooted Nginx on the listings below?
-->
&lt;p>如果你部署了这种配置，有人可以调用 &lt;code>http://website.example/randomthing&lt;/code> 并获取对 Ingress 控制器的整个文件系统的一些列表（和访问权限）。&lt;/p>
&lt;p>现在，你能在下面的列表中发现 chroot 处理过和未经 chroot 处理过的 Nginx 之间的区别吗？&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>不额外调用 &lt;code>chroot()&lt;/code>&lt;/th>
&lt;th>额外调用 &lt;code>chroot()&lt;/code>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>bin&lt;/code>&lt;/td>
&lt;td>&lt;code>bin&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>dev&lt;/code>&lt;/td>
&lt;td>&lt;code>dev&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>etc&lt;/code>&lt;/td>
&lt;td>&lt;code>etc&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>home&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>lib&lt;/code>&lt;/td>
&lt;td>&lt;code>lib&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>media&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>mnt&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>opt&lt;/code>&lt;/td>
&lt;td>&lt;code>opt&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>proc&lt;/code>&lt;/td>
&lt;td>&lt;code>proc&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>root&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>run&lt;/code>&lt;/td>
&lt;td>&lt;code>run&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>sbin&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>srv&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>sys&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>tmp&lt;/code>&lt;/td>
&lt;td>&lt;code>tmp&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>usr&lt;/code>&lt;/td>
&lt;td>&lt;code>usr&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>var&lt;/code>&lt;/td>
&lt;td>&lt;code>var&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>dbg&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>nginx-ingress-controller&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>wait-shutdown&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!--
The one in left side is not chrooted. So NGINX has full access to the filesystem. The one in right side is chrooted, so a new filesystem with only the required files to make NGINX work is created.
-->
&lt;p>左侧的那个没有 chroot 处理。所以 NGINX 可以完全访问文件系统。右侧的那个经过 chroot 处理，
因此创建了一个新文件系统，其中只有使 NGINX 工作所需的文件。&lt;/p>
&lt;!--
## What about other security improvements in this release?
-->
&lt;h2 id="此版本中的其他安全改进如何">此版本中的其他安全改进如何？&lt;/h2>
&lt;!--
We know that the new `chroot()` mechanism helps address some portion of the risk, but still, someone
can try to inject commands to read, for example, the `nginx.conf` file and extract sensitive information.
-->
&lt;p>我们知道新的 &lt;code>chroot()&lt;/code> 机制有助于解决部分风险，但仍然有人可以尝试注入命令来读取，例如 &lt;code>nginx.conf&lt;/code> 文件并提取敏感信息。&lt;/p>
&lt;!--
So, another change in this release (this is opt-out!) is the _deep inspector_.
We know that some directives or regular expressions may be dangerous to NGINX, so the deep inspector
checks all fields from an Ingress object (during its reconciliation, and also with a
[validating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook))
to verify if any fields contains these dangerous directives.
-->
&lt;p>所以，这个版本的另一个变化（可选择取消）是 &lt;strong>深度探测（Deep Inspector）&lt;/strong>。
我们知道某些指令或正则表达式可能对 NGINX 造成危险，因此深度探测器会检查 Ingress 对象中的所有字段
（在其协调期间，并且还使用&lt;a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook">验证准入 webhook&lt;/a>)
验证是否有任何字段包含这些危险指令。&lt;/p>
&lt;!--
The ingress controller already does this for annotations, and our goal is to move this existing validation to happen inside
deep inspection as part of a future release.
-->
&lt;p>Ingress 控制器已经通过注解做了这个工作，我们的目标是把现有的验证转移到深度探测中，作为未来版本的一部分。&lt;/p>
&lt;!--
You can take a look into the existing rules in [https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go](https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go).
-->
&lt;p>你可以在 &lt;a href="https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go">https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go&lt;/a> 中查看现有规则。&lt;/p>
&lt;!--
Due to the nature of inspecting and matching all strings within relevant Ingress objects, this new feature may consume a bit more CPU. You can disable it by running the ingress controller with the command line argument `--deep-inspect=false`.
-->
&lt;p>由于检查和匹配相关 Ingress 对象中的所有字符串的性质，此新功能可能会消耗更多 CPU。
你可以通过使用命令行参数 &lt;code>--deep-inspect=false&lt;/code> 运行 Ingress 控制器来禁用它。&lt;/p>
&lt;!--
## What's next?
This is not our final goal. Our final goal is to split the control plane and the data plane processes.
In fact, doing so will help us also achieve a [Gateway](https://gateway-api.sigs.k8s.io/) API implementation,
as we may have a different controller as soon as it "knows" what to provide to the data plane
(we need some help here!!)
-->
&lt;h2 id="下一步是什么">下一步是什么?&lt;/h2>
&lt;p>这不是我们的最终目标。我们的最终目标是拆分控制平面和数据平面进程。
事实上，这样做也将帮助我们实现 &lt;a href="https://gateway-api.sigs.k8s.io/">Gateway&lt;/a> API 实现，
因为一旦它“知道”要提供什么，我们可能会有不同的控制器 数据平面（我们需要一些帮助！！）&lt;/p>
&lt;!--
Some other projects in Kubernetes already take this approach
(like [KPNG](https://github.com/kubernetes-sigs/kpng), the proposed replacement for `kube-proxy`),
and we plan to align with them and get the same experience for Ingress-NGINX.
-->
&lt;p>Kubernetes 中的其他一些项目已经采用了这种方法（如 &lt;a href="https://github.com/kubernetes-sigs/kpng">KPNG&lt;/a>，
建议替换 &lt;code>kube-proxy&lt;/code>），我们计划与他们保持一致，并为 Ingress-NGINX 获得相同的体验。&lt;/p>
&lt;!--
## Further reading
If you want to take a look into how chrooting was done in Ingress NGINX, take a look
into [https://github.com/kubernetes/ingress-nginx/pull/8337](https://github.com/kubernetes/ingress-nginx/pull/8337)
The release v1.2.0 containing all the changes can be found at
[https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0](https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0)
-->
&lt;h2 id="延伸阅读">延伸阅读&lt;/h2>
&lt;p>如果你想了解如何在 Ingress NGINX 中完成 chrooting，请查看
&lt;a href="https://github.com/kubernetes/ingress-nginx/pull/8337">https://github.com/kubernetes/ingress-nginx/pull/8337&lt;/a>。
包含所有更改的版本 v1.2.0 可以在以下位置找到
&lt;a href="https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0">https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes 1.24 中的移除和弃用</title><link>https://kubernetes.io/zh-cn/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes Removals and Deprecations In 1.24"
date: 2022-04-07
slug: upcoming-changes-in-kubernetes-1-24
-->
&lt;!--
**Author**: Mickey Boxell (Oracle)
As Kubernetes evolves, features and APIs are regularly revisited and removed. New features may offer
an alternative or improved approach to solving existing problems, motivating the team to remove the
old approach.
-->
&lt;p>&lt;strong>作者&lt;/strong>：Mickey Boxell (Oracle)&lt;/p>
&lt;p>随着 Kubernetes 的发展，一些特性和 API 会被定期重检和移除。
新特性可能会提供替代或改进的方法，来解决现有的问题，从而激励团队移除旧的方法。&lt;/p>
&lt;!--
We want to make sure you are aware of the changes coming in the Kubernetes 1.24 release. The release will
**deprecate** several (beta) APIs in favor of stable versions of the same APIs. The major change coming
in the Kubernetes 1.24 release is the
[removal of Dockershim](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim).
This is discussed below and will be explored in more depth at release time. For an early look at the
changes coming in Kubernetes 1.24, take a look at the in-progress
[CHANGELOG](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md).
-->
&lt;p>我们希望确保你了解 Kubernetes 1.24 版本的变化。该版本将 &lt;strong>弃用&lt;/strong> 一些（测试版/beta）API，
转而支持相同 API 的稳定版本。Kubernetes 1.24
版本的主要变化是&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">移除 Dockershim&lt;/a>。
这将在下面讨论，并将在发布时更深入地探讨。
要提前了解 Kubernetes 1.24 中的更改，请查看正在更新中的
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">CHANGELOG&lt;/a>。&lt;/p>
&lt;!--
## A note about Dockershim
It's safe to say that the removal receiving the most attention with the release of Kubernetes 1.24
is Dockershim. Dockershim was deprecated in v1.20. As noted in the [Kubernetes 1.20 changelog](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation):
"Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet
uses a module called "dockershim" which implements CRI support for Docker and it has seen maintenance
issues in the Kubernetes community." With the upcoming release of Kubernetes 1.24, the Dockershim will
finally be removed.
-->
&lt;h2 id="a-note-about-dockershim">关于 Dockershim &lt;/h2>
&lt;p>可以肯定地说，随着 Kubernetes 1.24 的发布，最受关注的是移除 Dockershim。
Dockershim 在 1.20 版本中已被弃用。如
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">Kubernetes 1.20 变更日志&lt;/a>中所述：
&amp;quot;Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet
uses a module called &amp;quot;dockershim&amp;quot; which implements CRI support for Docker and it has seen maintenance
issues in the Kubernetes community.&amp;quot;
随着即将发布的 Kubernetes 1.24，Dockershim 将最终被移除。&lt;/p>
&lt;!--
In the article [Don't Panic: Kubernetes and Docker](/blog/2020/12/02/dont-panic-kubernetes-and-docker/),
the authors succinctly captured the change's impact and encouraged users to remain calm:
> Docker as an underlying runtime is being deprecated in favor of runtimes that use the
> Container Runtime Interface (CRI) created for Kubernetes. Docker-produced images
> will continue to work in your cluster with all runtimes, as they always have.
-->
&lt;p>在文章&lt;a href="https://kubernetes.io/zh-cn/blog/2020/12/02/dont-panic-kubernetes-and-docker/">别慌: Kubernetes 和 Docker&lt;/a> 中，
作者简洁地记述了变化的影响，并鼓励用户保持冷静：&lt;/p>
&lt;blockquote>
&lt;p>弃用 Docker 这个底层运行时，转而支持符合为 Kubernetes 创建的容器运行接口
Container Runtime Interface (CRI) 的运行时。
Docker 构建的镜像，将在你的集群的所有运行时中继续工作，一如既往。&lt;/p>
&lt;/blockquote>
&lt;!--
Several guides have been created with helpful information about migrating from dockershim
to container runtimes that are directly compatible with Kubernetes. You can find them on the
[Migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/)
page in the Kubernetes documentation.
-->
&lt;p>已经有一些文档指南，提供了关于从 dockershim 迁移到与 Kubernetes 直接兼容的容器运行时的有用信息。
你可以在 Kubernetes 文档中的&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/">从 dockershim 迁移&lt;/a>
页面上找到它们。&lt;/p>
&lt;!--
For more information about why Kubernetes is moving away from dockershim, check out the aptly
named: [Kubernetes is Moving on From Dockershim](/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/)
and the [updated dockershim removal FAQ](/blog/2022/02/17/dockershim-faq/).
Take a look at the [Is Your Cluster Ready for v1.24?](/blog/2022/03/31/ready-for-dockershim-removal/) post to learn about how to ensure your cluster continues to work after upgrading from v1.23 to v1.24.
-->
&lt;p>有关 Kubernetes 为何不再使用 dockershim 的更多信息，
请参见：&lt;a href="https://kubernetes.io/zh-cn/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">Kubernetes 即将移除 Dockershim&lt;/a>
和&lt;a href="https://kubernetes.io/zh-cn/blog/2022/02/17/dockershim-faq/">最新的弃用 Dockershim 的常见问题&lt;/a>。&lt;/p>
&lt;p>查看&lt;a href="https://kubernetes.io/zh-cn/blog/2022/03/31/ready-for-dockershim-removal/">你的集群准备好使用 v1.24 版本了吗？&lt;/a> 一文，
了解如何确保你的集群在从 1.23 版本升级到 1.24 版本后继续工作。&lt;/p>
&lt;!--
## The Kubernetes API removal and deprecation process
Kubernetes contains a large number of components that evolve over time. In some cases, this
evolution results in APIs, flags, or entire features, being removed. To prevent users from facing
breaking changes, Kubernetes contributors adopted a feature [deprecation policy](/docs/reference/using-api/deprecation-policy/).
This policy ensures that stable APIs may only be deprecated when a newer stable version of that
same API is available and that APIs have a minimum lifetime as indicated by the following stability levels:
* Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.
* Beta or pre-release API versions must be supported for 3 releases after deprecation.
* Alpha or experimental API versions may be removed in any release without prior deprecation notice.
-->
&lt;h2 id="the-Kubernetes-api-removal-and-deprecation-process">Kubernetes API 移除和弃用流程 &lt;/h2>
&lt;p>Kubernetes 包含大量随时间演变的组件。在某些情况下，这种演变会导致 API、标志或整个特性被移除。
为了防止用户面对重大变化，Kubernetes 贡献者采用了一项特性&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-policy/">弃用策略&lt;/a>。
此策略确保仅当同一 API 的较新稳定版本可用并且
API 具有以下稳定性级别所指示的最短生命周期时，才可能弃用稳定版本 API：&lt;/p>
&lt;ul>
&lt;li>正式发布 (GA) 或稳定的 API 版本可能被标记为已弃用，但不得在 Kubernetes 的主版本中移除。&lt;/li>
&lt;li>测试版（beta）或预发布 API 版本在弃用后必须支持 3 个版本。&lt;/li>
&lt;li>Alpha 或实验性 API 版本可能会在任何版本中被移除，恕不另行通知。&lt;/li>
&lt;/ul>
&lt;!--
Removals follow the same deprecation policy regardless of whether an API is removed due to a beta feature
graduating to stable or because that API was not proven to be successful. Kubernetes will continue to make
sure migration options are documented whenever APIs are removed.
-->
&lt;p>移除遵循相同的弃用政策，无论 API 是由于 测试版（beta）功能逐渐稳定还是因为该
API 未被证明是成功的而被移除。
Kubernetes 将继续确保在移除 API 时提供用来迁移的文档。&lt;/p>
&lt;!--
**Deprecated** APIs are those that have been marked for removal in a future Kubernetes release. **Removed**
APIs are those that are no longer available for use in current, supported Kubernetes versions after having
been deprecated. These removals have been superseded by newer, stable/generally available (GA) APIs.
-->
&lt;p>&lt;strong>弃用的&lt;/strong> API 是指那些已标记为在未来 Kubernetes 版本中移除的 API。
&lt;strong>移除的&lt;/strong> API 是指那些在被弃用后不再可用于当前受支持的 Kubernetes 版本的 API。
这些移除的 API 已被更新的、稳定的/普遍可用的 (GA) API 所取代。&lt;/p>
&lt;!--
## API removals, deprecations, and other changes for Kubernetes 1.24
* [Dynamic kubelet configuration](https://github.com/kubernetes/enhancements/issues/281): `DynamicKubeletConfig` is used to enable the dynamic configuration of the kubelet. The `DynamicKubeletConfig` flag was deprecated in Kubernetes 1.22. In v1.24, this feature gate will be removed from the kubelet. See [Reconfigure kubelet](/docs/tasks/administer-cluster/reconfigure-kubelet/). Refer to the ["Dynamic kubelet config is removed" KEP](https://github.com/kubernetes/enhancements/issues/281) for more information.
-->
&lt;h2 id="api-removals-deprecations-and-other-changes-for-kubernetes-1.24">Kubernetes 1.24 的 API 移除、弃用和其他更改 &lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/281">动态 kubelet 配置&lt;/a>: &lt;code>DynamicKubeletConfig&lt;/code>
用于启用 kubelet 的动态配置。Kubernetes 1.22 中弃用 &lt;code>DynamicKubeletConfig&lt;/code> 标志。
在 1.24 版本中，此特性门控将从 kubelet 中移除。请参阅&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/reconfigure-kubelet/">重新配置 kubelet&lt;/a>。
更多详细信息，请参阅&lt;a href="https://github.com/kubernetes/enhancements/issues/281">“移除动态 kubelet 配置” 的 KEP&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* [Dynamic log sanitization](https://github.com/kubernetes/kubernetes/pull/107207): The experimental dynamic log sanitization feature is deprecated and will be removed in v1.24. This feature introduced a logging filter that could be applied to all Kubernetes system components logs to prevent various types of sensitive information from leaking via logs. Refer to [KEP-1753: Kubernetes system components logs sanitization](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation) for more information and an [alternative approach](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#alternatives=).
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107207">动态日志清洗&lt;/a>：实验性的动态日志清洗功能已被弃用，
将在 1.24 版本中被移除。该功能引入了一个日志过滤器，可以应用于所有 Kubernetes 系统组件的日志，
以防止各种类型的敏感信息通过日志泄漏。有关更多信息和替代方法，请参阅
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation">KEP-1753: Kubernetes 系统组件日志清洗&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* [Removing Dockershim from kubelet](https://github.com/kubernetes/enhancements/issues/2221): the Container Runtime Interface (CRI) for Docker (i.e. Dockershim) is currently a built-in container runtime in the kubelet code base. It was deprecated in v1.20. As of v1.24, the kubelet will no longer have dockershim. Check out this blog on [what you need to do be ready for v1.24](/blog/2022/03/31/ready-for-dockershim-removal/).
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2221">从 kubelet 中移除 Dockershim&lt;/a>：Docker
的容器运行时接口(CRI)（即 Dockershim）目前是 kubelet 代码中内置的容器运行时。它在 1.20 版本中已被弃用。
从 1.24 版本开始，kubelet 已经移除 dockershim。查看这篇博客，
&lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">了解你需要为 1.24 版本做些什么&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* [Storage capacity tracking for pod scheduling](https://github.com/kubernetes/enhancements/issues/1472): The CSIStorageCapacity API supports exposing currently available storage capacity via CSIStorageCapacity objects and enhances scheduling of pods that use CSI volumes with late binding. In v1.24, the CSIStorageCapacity API will be stable. The API graduating to stable initates the deprecation of the v1beta1 CSIStorageCapacity API. Refer to the [Storage Capacity Constraints for Pod Scheduling KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking) for more information.
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1472">Pod 调度的存储容量追踪&lt;/a>：CSIStorageCapacity API
支持通过 CSIStorageCapacity 对象暴露当前可用的存储容量，并增强了使用带有延迟绑定的 CSI 卷的 Pod 的调度。
CSIStorageCapacity API 自 1.24 版本起提供稳定版本。升级到稳定版的 API 将弃用 v1beta1 CSIStorageCapacity API。
更多信息请参见 &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking">Pod 调度存储容量约束 KEP&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* [The `master` label is no longer present on kubeadm control plane nodes](https://github.com/kubernetes/kubernetes/pull/107533). For new clusters, the label 'node-role.kubernetes.io/master' will no longer be added to control plane nodes, only the label 'node-role.kubernetes.io/control-plane' will be added. For more information, refer to [KEP-2067: Rename the kubeadm "master" label and taint](https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint).
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107533">kubeadm 控制面节点上不再存在 &lt;code>master&lt;/code> 标签&lt;/a>。
对于新集群，控制平面节点将不再添加 'node-role.kubernetes.io/master' 标签，
只会添加 'node-role.kubernetes.io/control-plane' 标签。更多信息请参考
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint">KEP-2067：重命名 kubeadm “master” 标签和污点&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* [VolumeSnapshot v1beta1 CRD will be removed](https://github.com/kubernetes/enhancements/issues/177). Volume snapshot and restore functionality for Kubernetes and the [Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md) (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, entered beta in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.21 and is now unsupported. Refer to [KEP-177: CSI Snapshot](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot) and [kubernetes-csi/external-snapshotter](https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v4.1.0) for more information.
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/177">VolumeSnapshot v1beta1 CRD 在 1.24 版本中将被移除&lt;/a>。
Kubernetes 和 &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface&lt;/a> (CSI)
的卷快照和恢复功能，在 1.20 版本中进入测试版。该功能提供标准化 API 设计 (CRD ) 并为 CSI 卷驱动程序添加了 PV 快照/恢复支持，
VolumeSnapshot v1beta1 在 1.21 版本中已被弃用，现在不受支持。更多信息请参考
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot">KEP-177：CSI 快照&lt;/a>和
&lt;a href="https://github.com/kubernetes-csi/external-snapshotter/releases/tag/v4.1.0">kubernetes-csi/external-snapshotter&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
## What to do
### Dockershim removal
As stated earlier, there are several guides about
[Migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/).
You can start with [Finding what container runtime are on your nodes](/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/).
If your nodes are using dockershim, there are other possible Docker Engine dependencies such as
Pods or third-party tools executing Docker commands or private registries in the Docker configuration file. You can follow the
[Check whether Dockershim removal affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/) guide to review possible
Docker Engine dependencies. Before upgrading to v1.24, you decide to either remain using Docker Engine and
[Migrate Docker Engine nodes from dockershim to cri-dockerd](/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/) or migrate to a CRI-compatible runtime. Here's a guide to
[change the container runtime on a node from Docker Engine to containerd](/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/).
-->
&lt;h2 id="what-to-do">需要做什么 &lt;/h2>
&lt;h3 id="dockershim-removal">移除 Dockershim &lt;/h3>
&lt;p>如前所述，有一些关于从 &lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/">dockershim 迁移&lt;/a>的指南。
你可以&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">从查明节点上所使用的容器运行时&lt;/a>开始。
如果你的节点使用 dockershim，则还有其他可能的 Docker Engine 依赖项，
例如 Pod 或执行 Docker 命令的第三方工具或 Docker 配置文件中的私有镜像库。
你可以按照&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">检查移除 Dockershim 是否对你有影响&lt;/a>
的指南来查看可能的 Docker 引擎依赖项。在升级到 1.24 版本之前，你决定要么继续使用 Docker Engine 并
&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/">将 Docker Engine 节点从 dockershim 迁移到 cri-dockerd&lt;/a>，
要么迁移到与 CRI 兼容的运行时。这是&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">将节点上的容器运行时从 Docker Engine 更改为 containerd&lt;/a> 的指南。&lt;/p>
&lt;!--
### `kubectl convert`
The [`kubectl convert`](/docs/tasks/tools/included/kubectl-convert-overview/) plugin for `kubectl`
can be helpful to address migrating off deprecated APIs. The plugin facilitates the conversion of
manifests between different API versions, for example, from a deprecated to a non-deprecated API
version. More general information about the API migration process can be found in the [Deprecated API Migration Guide](/docs/reference/using-api/deprecation-guide/).
Follow the [install `kubectl convert` plugin](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin)
documentation to download and install the `kubectl-convert` binary.
-->
&lt;h3 id="kubectl-convert">&lt;code>kubectl convert&lt;/code> &lt;/h3>
&lt;p>kubectl 的 &lt;a href="https://kubernetes.io/zh-cn/docs/tasks/tools/included/kubectl-convert-overview/">&lt;code>kubectl convert&lt;/code>&lt;/a>
插件有助于解决弃用 API 的迁移问题。该插件方便了不同 API 版本之间清单的转换，
例如，从弃用的 API 版本到非弃用的 API 版本。
关于 API 迁移过程的更多信息可以在&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-guide/">已弃用 API 的迁移指南&lt;/a>中找到。
按照&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin">安装 &lt;code>kubectl convert&lt;/code> 插件&lt;/a>
文档下载并安装 &lt;code>kubectl-convert&lt;/code> 二进制文件。&lt;/p>
&lt;!--
### Looking ahead
The Kubernetes 1.25 and 1.26 releases planned for later this year will stop serving beta versions
of several currently stable Kubernetes APIs. The v1.25 release will also remove PodSecurityPolicy,
which was deprecated with Kubernetes 1.21 and will not graduate to stable. See [PodSecurityPolicy
Deprecation: Past, Present, and Future](/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/) for more information.
-->
&lt;h3 id="looking-ahead">展望未来 &lt;/h3>
&lt;p>计划在今年晚些时候发布的 Kubernetes 1.25 和 1.26 版本，将停止提供一些
Kubernetes API 的 Beta 版本，这些 API 当前为稳定版。1.25 版本还将移除 PodSecurityPolicy，
它已在 Kubernetes 1.21 版本中被弃用，并且不会升级到稳定版。有关详细信息，请参阅
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy 弃用：过去、现在和未来&lt;/a>。&lt;/p>
&lt;!--
The official [list of API removals planned for Kubernetes 1.25](/docs/reference/using-api/deprecation-guide/#v1-25) is:
-->
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-guide/#v1-25">Kubernetes 1.25 计划移除的 API 的官方列表&lt;/a>是：&lt;/p>
&lt;ul>
&lt;li>Beta CronJob API (batch/v1beta1)&lt;/li>
&lt;li>Beta EndpointSlice API (discovery.k8s.io/v1beta1)&lt;/li>
&lt;li>Beta Event API (events.k8s.io/v1beta1)&lt;/li>
&lt;li>Beta HorizontalPodAutoscaler API (autoscaling/v2beta1)&lt;/li>
&lt;li>Beta PodDisruptionBudget API (policy/v1beta1)&lt;/li>
&lt;li>Beta PodSecurityPolicy API (policy/v1beta1)&lt;/li>
&lt;li>Beta RuntimeClass API (node.k8s.io/v1beta1)&lt;/li>
&lt;/ul>
&lt;!--
The official [list of API removals planned for Kubernetes 1.26](/docs/reference/using-api/deprecation-guide/#v1-26) is:
* The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)
* The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)
-->
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-guide/#v1-26">Kubernetes 1.26 计划移除的 API 的官方列表&lt;/a>是：&lt;/p>
&lt;ul>
&lt;li>Beta FlowSchema 和 PriorityLevelConfiguration API (flowcontrol.apiserver.k8s.io/v1beta1)&lt;/li>
&lt;li>Beta HorizontalPodAutoscaler API (autoscaling/v2beta2)&lt;/li>
&lt;/ul>
&lt;!--
### Want to know more?
Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:
* [Kubernetes 1.21](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation)
* [Kubernetes 1.22](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation)
* [Kubernetes 1.23](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation)
* We will formally announce the deprecations that come with [Kubernetes 1.24](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation) as part of the CHANGELOG for that release.
For information on the process of deprecation and removal, check out the official Kubernetes [deprecation policy](/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api) document.
-->
&lt;h3 id="want-to-know-more">了解更多&lt;/h3>
&lt;p>Kubernetes 发行说明中宣告了弃用信息。你可以在以下版本的发行说明中看到待弃用的公告：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23&lt;/a>&lt;/li>
&lt;li>我们将正式宣布 &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24&lt;/a> 的弃用信息，
作为该版本 CHANGELOG 的一部分。&lt;/li>
&lt;/ul>
&lt;p>有关弃用和移除过程的信息，请查看 Kubernetes 官方&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">弃用策略&lt;/a>文档。&lt;/p></description></item><item><title>Blog: 你的集群准备好使用 v1.24 版本了吗？</title><link>https://kubernetes.io/zh-cn/blog/2022/03/31/ready-for-dockershim-removal/</link><pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/03/31/ready-for-dockershim-removal/</guid><description>
&lt;!--
layout: blog
title: "Is Your Cluster Ready for v1.24?"
date: 2022-03-31
slug: ready-for-dockershim-removal
-->
&lt;!--
**Author:** Kat Cosgrove
-->
&lt;p>&lt;strong>作者:&lt;/strong> Kat Cosgrove&lt;/p>
&lt;!--
Way back in December of 2020, Kubernetes announced the [deprecation of Dockershim](/blog/2020/12/02/dont-panic-kubernetes-and-docker/). In Kubernetes, dockershim is a software shim that allows you to use the entire Docker engine as your container runtime within Kubernetes. In the upcoming v1.24 release, we are removing Dockershim - the delay between deprecation and removal in line with the [project’s policy](https://kubernetes.io/docs/reference/using-api/deprecation-policy/) of supporting features for at least one year after deprecation. If you are a cluster operator, this guide includes the practical realities of what you need to know going into this release. Also, what do you need to do to ensure your cluster doesn’t fall over!
-->
&lt;p>早在 2020 年 12 月，Kubernetes 就宣布&lt;a href="https://kubernetes.io/zh-cn/blog/2020/12/02/dont-panic-kubernetes-and-docker/">弃用 Dockershim&lt;/a>。
在 Kubernetes 中，dockershim 是一个软件 shim，
它允许你将整个 Docker 引擎用作 Kubernetes 中的容器运行时。
在即将发布的 v1.24 版本中，我们将移除 Dockershim -
在宣布弃用之后到彻底移除这段时间内，我们至少预留了一年的时间继续支持此功能，
这符合相关的&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-policy/">项目策略&lt;/a>。
如果你是集群操作员，则该指南包含你在此版本中需要了解的实际情况。
另外还包括你需要做些什么来确保你的集群不会崩溃！&lt;/p>
&lt;!--
## First, does this even affect you?
-->
&lt;h2 id="首先-这对你有影响吗">首先，这对你有影响吗？&lt;/h2>
&lt;!--
If you are rolling your own cluster or are otherwise unsure whether or not this removal affects you, stay on the safe side and [check to see if you have any dependencies on Docker Engine](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/). Please note that using Docker Desktop to build your application containers is not a Docker dependency for your cluster. Container images created by Docker are compliant with the [Open Container Initiative (OCI)](https://opencontainers.org/), a Linux Foundation governance structure that defines industry standards around container formats and runtimes. They will work just fine on any container runtime supported by Kubernetes.
-->
&lt;p>如果你正在管理自己的集群或不确定此删除是否会影响到你，
请保持安全状态并&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">检查你对 Docker Engine 是否有依赖&lt;/a>。
请注意，使用 Docker Desktop 构建应用程序容器并不算是集群对 Docker 有依赖。
Docker 创建的容器镜像符合 &lt;a href="https://opencontainers.org/">Open Container Initiative (OCI)&lt;/a> 规范，
而 OCI 是 Linux 基金会的一种治理架构，负责围绕容器格式和运行时定义行业标准。
这些镜像可以在 Kubernetes 支持的任何容器运行时上正常工作。&lt;/p>
&lt;!--
If you are using a managed Kubernetes service from a cloud provider, and you haven’t explicitly changed the container runtime, there may be nothing else for you to do. Amazon EKS, Azure AKS, and Google GKE all default to containerd now, though you should make sure they do not need updating if you have any node customizations. To check the runtime of your nodes, follow [Find Out What Container Runtime is Used on a Node](/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/).
-->
&lt;p>如果你使用的是云服务提供商管理的 Kubernetes 服务，
并且你确定没有更改过容器运行时，那么你可能不需要做任何事情。
Amazon EKS、Azure AKS 和 Google GKE 现在都默认使用 containerd，
但如果你的集群中有任何自定义的节点，你要确保它们不需要被更新。
要检查节点的运行时，请参考&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">查明节点上所使用的容器运行时&lt;/a>。&lt;/p>
&lt;!--
Regardless of whether you are rolling your own cluster or using a managed Kubernetes service from a cloud provider, you may need to [migrate telemetry or security agents that rely on Docker Engine](/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/).
-->
&lt;p>无论你是在管理自己的集群还是使用云服务提供商管理的 Kubernetes 服务，
你可能都需要&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/">迁移依赖 Docker Engine 的遥测或安全代理&lt;/a>。&lt;/p>
&lt;!--
## I have a Docker dependency. What now?
-->
&lt;h2 id="我对-docker-有依赖-现在该怎么办">我对 Docker 有依赖。现在该怎么办？&lt;/h2>
&lt;!--
If your Kubernetes cluster depends on Docker Engine and you intend to upgrade to Kubernetes v1.24 (which you should eventually do for security and similar reasons), you will need to change your container runtime from Docker Engine to something else or use [cri-dockerd](https://github.com/Mirantis/cri-dockerd). Since [containerd](https://containerd.io/) is a graduated CNCF project and the runtime within Docker itself, it’s a safe bet as an alternative container runtime. Fortunately, the Kubernetes project has already documented the process of [changing a node’s container runtime](/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/), using containerd as an example. Instructions are similar for switching to one of the other supported runtimes.
-->
&lt;p>如果你的 Kubernetes 集群对 Docker Engine 有依赖，
并且你打算升级到 Kubernetes v1.24 版本（出于安全和类似原因，你最终应该这样做），
你需要将容器运行时从 Docker Engine 更改为其他方式或使用 &lt;a href="https://github.com/Mirantis/cri-dockerd">cri-dockerd&lt;/a>。
由于 &lt;a href="https://containerd.io/">containerd&lt;/a> 是一个已经毕业的 CNCF 项目，
并且是 Docker 本身的运行时，因此用它作为容器运行时的替代方式是一个安全的选择。
幸运的是，Kubernetes 项目已经以 containerd 为例，
提供了&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">更改节点容器运行时&lt;/a>的过程文档。
切换到其它支持的运行时的操作指令与此类似。&lt;/p>
&lt;!--
## I want to upgrade Kubernetes, and I need to maintain compatibility with Docker as a runtime. What are my options?
-->
&lt;h2 id="我想升级-kubernetes-并且我需要保持与-docker-作为运行时的兼容性-我有哪些选择">我想升级 Kubernetes，并且我需要保持与 Docker 作为运行时的兼容性。我有哪些选择？&lt;/h2>
&lt;!--
Fear not, you aren’t being left out in the cold and you don’t have to take the security risk of staying on an old version of Kubernetes. Mirantis and Docker have jointly released, and are maintaining, a replacement for dockershim. That replacement is called [cri-dockerd](https://github.com/Mirantis/cri-dockerd). If you do need to maintain compatibility with Docker as a runtime, install cri-dockerd following the instructions in the project’s documentation.
-->
&lt;p>别担心，你不会被冷落，也不必冒着安全风险继续使用旧版本的 Kubernetes。
Mirantis 和 Docker 已经联合发布并正在维护 dockershim 的替代品。
这种替代品称为 &lt;a href="https://github.com/Mirantis/cri-dockerd">cri-dockerd&lt;/a>。
如果你确实需要保持与 Docker 作为运行时的兼容性，请按照项目文档中的说明安装 cri-dockerd。&lt;/p>
&lt;!--
## Is that it?
-->
&lt;h2 id="这样就可以了吗">这样就可以了吗？&lt;/h2>
&lt;!--
Yes. As long as you go into this release aware of the changes being made and the details of your own clusters, and you make sure to communicate clearly with your development teams, it will be minimally dramatic. You may have some changes to make to your cluster, application code, or scripts, but all of these requirements are documented. Switching from using Docker Engine as your runtime to using [one of the other supported container runtimes](/docs/setup/production-environment/container-runtimes/) effectively means removing the middleman, since the purpose of dockershim is to access the container runtime used by Docker itself. From a practical perspective, this removal is better both for you and for Kubernetes maintainers in the long-run.
-->
&lt;p>是的。只要你深入了解此版本所做的变更和你自己集群的详细信息，
并确保与你的开发团队进行清晰的沟通，它的不确定性就会降到最低。
你可能需要对集群、应用程序代码或脚本进行一些更改，但所有这些要求都已经有说明指导。
从使用 Docker Engine 作为运行时，切换到使用&lt;a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/">其他任何一种支持的容器运行时&lt;/a>，
这意味着移除了中间层的组件，因为 dockershim 的作用是访问 Docker 本身使用的容器运行时。
从实际角度长远来看，这种移除对你和 Kubernetes 维护者都更有好处。&lt;/p>
&lt;!--
If you still have questions, please first check the [Dockershim Removal FAQ](/blog/2022/02/17/dockershim-faq/).
-->
&lt;p>如果你仍有疑问，请先查看&lt;a href="https://kubernetes.io/zh-cn/blog/2022/02/17/dockershim-faq/">弃用 Dockershim 的常见问题&lt;/a>。&lt;/p></description></item><item><title>Blog: 认识我们的贡献者 - 亚太地区（澳大利亚-新西兰地区）</title><link>https://kubernetes.io/zh-cn/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</link><pubDate>Wed, 16 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</guid><description>
&lt;!--
layout: blog
title: "Meet Our Contributors - APAC (Aus-NZ region)"
date: 2022-03-16
slug: meet-our-contributors-au-nz-ep-02
canonicalUrl: https://www.kubernetes.dev/blog/2022/03/14/meet-our-contributors-au-nz-ep-02/
-->
&lt;!--
**Authors &amp; Interviewers:** [Anubhav Vardhan](https://github.com/anubha-v-ardhan), [Atharva Shinde](https://github.com/Atharva-Shinde), [Avinesh Tripathi](https://github.com/AvineshTripathi), [Brad McCoy](https://github.com/bradmccoydev), [Debabrata Panigrahi](https://github.com/Debanitrkl), [Jayesh Srivastava](https://github.com/jayesh-srivastava), [Kunal Verma](https://github.com/verma-kunal), [Pranshu Srivastava](https://github.com/PranshuSrivastava), [Priyanka Saggu](github.com/Priyankasaggu11929/), [Purneswar Prasad](https://github.com/PurneswarPrasad), [Vedant Kakde](https://github.com/vedant-kakde)
-->
&lt;p>&lt;strong>作者和采访者：&lt;/strong>
&lt;a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan&lt;/a>、
&lt;a href="https://github.com/Atharva-Shinde">Atharva Shinde&lt;/a>、
&lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>、
&lt;a href="https://github.com/bradmccoydev">Brad McCoy&lt;/a>、
&lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>、
&lt;a href="https://github.com/jayesh-srivastava">Jayesh Srivastava&lt;/a>、
&lt;a href="https://github.com/verma-kunal">Kunal Verma&lt;/a>、
&lt;a href="https://github.com/PranshuSrivastava">Pranshu Srivastava&lt;/a>、
&lt;a href="github.com/Priyankasaggu11929/">Priyanka Saggu&lt;/a>、
&lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>、
&lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;hr>
&lt;!--
Good day, everyone 👋
-->
&lt;p>大家好👋&lt;/p>
&lt;!--
Welcome back to the second episode of the "Meet Our Contributors" blog post series for APAC.
-->
&lt;p>欢迎来到亚太地区的”认识我们的贡献者”博文系列第二期。&lt;/p>
&lt;!--
This post will feature four outstanding contributors from the Australia and New Zealand regions, who have played diverse leadership and community roles in the Upstream Kubernetes project.
-->
&lt;p>这篇文章将介绍来自澳大利亚和新西兰地区的四位杰出贡献者，
他们在上游 Kubernetes 项目中承担着不同子项目的领导者和社区贡献者的角色。&lt;/p>
&lt;!--
So, without further ado, let's get straight to the blog.
-->
&lt;p>闲话少说，让我们直接进入主题。&lt;/p>
&lt;h2 id="caleb-woodbine-https-github-com-bobymcbobs">&lt;a href="https://github.com/BobyMCbobs">Caleb Woodbine&lt;/a>&lt;/h2>
&lt;!--
Caleb Woodbine is currently a member of the ii.nz organisation.
-->
&lt;p>Caleb Woodbine 目前是 ii.nz 组织的成员。&lt;/p>
&lt;!--
He began contributing to the Kubernetes project in 2018 as a member of the Kubernetes Conformance working group. His experience was positive, and he benefited from early guidance from [Hippie Hacker](https://github.com/hh), a fellow contributor from New Zealand.
-->
&lt;p>他于 2018 年作为 Kubernetes Conformance 工作组的成员开始为 Kubernetes 项目做贡献。
他积极向上，他从一位来自新西兰的贡献者 &lt;a href="https://github.com/hh">Hippie Hacker&lt;/a> 的早期指导中受益匪浅。&lt;/p>
&lt;!--
He has made major contributions to Kubernetes project since then through `SIG k8s-infra` and `k8s-conformance` working group.
-->
&lt;p>他在 &lt;code>SIG k8s-infra&lt;/code> 和 &lt;code>k8s-conformance&lt;/code> 工作组为 Kubernetes 项目做出了重大贡献。&lt;/p>
&lt;!--
Caleb is also a co-organizer of the [CloudNative NZ](https://www.meetup.com/cloudnative-nz/) community events, which aim to expand the reach of Kubernetes project throughout New Zealand in order to encourage technical education and improved employment opportunities.
-->
&lt;p>Caleb 也是 &lt;a href="https://www.meetup.com/cloudnative-nz/">CloudNative NZ&lt;/a>
社区活动的联合组织者，该活动旨在扩大 Kubernetes 项目在整个新西兰的影响力，以鼓励科技教育和改善就业机会。&lt;/p>
&lt;!--
> _There need to be more outreach in APAC and the educators and universities must pick up Kubernetes, as they are very slow and about 8+ years out of date. NZ tends to rather pay overseas than educate locals on the latest cloud tech Locally._
-->
&lt;blockquote>
&lt;p>亚太地区需要更多的外联活动，教育工作者和大学必须学习 Kubernetes，因为他们非常缓慢，
而且已经落后了8年多。新西兰倾向于在海外付费，而不是教育当地人最新的云技术。&lt;/p>
&lt;/blockquote>
&lt;h2 id="dylan-graham-https-github-com-dylangraham">&lt;a href="https://github.com/DylanGraham">Dylan Graham&lt;/a>&lt;/h2>
&lt;!--
Dylan Graham is a cloud engineer from Adeliade, Australia. He has been contributing to the upstream Kubernetes project since 2018.
-->
&lt;p>Dylan Graham 是来自澳大利亚 Adeliade 的云计算工程师。自 2018 年以来，他一直在为上游 Kubernetes 项目做出贡献。&lt;/p>
&lt;!--
He stated that being a part of such a large-scale project was initially overwhelming, but that the community's friendliness and openness assisted him in getting through it.
-->
&lt;p>他表示，成为如此大项目的一份子，最初压力是比较大的，但社区的友好和开放帮助他度过了难关。&lt;/p>
&lt;!--
He began by contributing to the project documentation and is now mostly focused on the community support for the APAC region.
-->
&lt;p>开始在项目文档方面做贡献，现在主要致力于为亚太地区提供社区支持。&lt;/p>
&lt;!--
He believes that consistent attendance at community/project meetings, taking on project tasks, and seeking community guidance as needed can help new aspiring developers become effective contributors.
-->
&lt;p>他相信，持续参加社区/项目会议，承担项目任务，并在需要时寻求社区指导，可以帮助有抱负的新开发人员成为有效的贡献者。&lt;/p>
&lt;!--
> _The feeling of being a part of a large community is really special. I've met some amazing people, even some before the pandemic in real life :)_
-->
&lt;blockquote>
&lt;p>成为大社区的一份子感觉真的很特别。我遇到了一些了不起的人，甚至是在现实生活中疫情发生之前。&lt;/p>
&lt;/blockquote>
&lt;h2 id="hippie-hacker-https-github-com-hh">&lt;a href="https://github.com/hh">Hippie Hacker&lt;/a>&lt;/h2>
&lt;!--
Hippie has worked for the CNCF.io as a Strategic Initiatives contractor from New Zealand for almost 5+ years. He is an active contributor to k8s-infra, API conformance testing, Cloud provider conformance submissions, and apisnoop.cncf.io domains of the upstream Kubernetes &amp; CNCF projects.
-->
&lt;p>Hippie 来自新西兰，曾在 CNCF.io 作为战略计划承包商工作 5 年多。他是 k8s-infra、
API 一致性测试、云提供商一致性提交以及上游 Kubernetes 和 CNCF 项目 apisnoop.cncf.io 域的积极贡献者。&lt;/p>
&lt;!--
He recounts their early involvement with the Kubernetes project, which began roughly 5 years ago when their firm, ii.nz, demonstrated [network booting from a Raspberry Pi using PXE and running Gitlab in-cluster to install Kubernetes on servers](https://ii.nz/post/bringing-the-cloud-to-your-community/).
-->
&lt;p>他讲述了他们早期参与 Kubernetes 项目的情况，该项目始于大约 5 年前，当时他们的公司 ii.nz
演示了&lt;a href="https://ii.nz/post/bringing-the-cloud-to-your-community/">使用 PXE 从 Raspberry Pi 启动网络，并在集群中运行Gitlab，以便在服务器上安装 Kubernetes &lt;/a>&lt;/p>
&lt;!--
He describes their own contributing experience as someone who, at first, tried to do all of the hard lifting on their own, but eventually saw the benefit of group contributions which reduced burnout and task division which allowed folks to keep moving forward on their own momentum.
-->
&lt;p>他描述了自己的贡献经历：一开始，他试图独自完成所有艰巨的任务，但最终看到了团队协作贡献的好处，
分工合作减少了过度疲劳，这让人们能够凭借自己的动力继续前进。&lt;/p>
&lt;!--
He recommends that new contributors use pair programming.
-->
&lt;p>他建议新的贡献者结对编程。&lt;/p>
&lt;!--
> _The cross pollination of approaches and two pairs of eyes on the same work can often yield a much more amplified effect than a PR comment / approval alone can afford._
-->
&lt;blockquote>
&lt;p>针对一个项目，多人关注和交叉交流往往比单独的评审、批准 PR 能产生更大的效果。&lt;/p>
&lt;/blockquote>
&lt;h2 id="nick-young-https-github-com-youngnick">&lt;a href="https://github.com/youngnick">Nick Young&lt;/a>&lt;/h2>
&lt;!--
Nick Young works at VMware as a technical lead for Contour, a CNCF ingress controller. He was active with the upstream Kubernetes project from the beginning, and eventually became the chair of the LTS working group, where he advocated user concerns. He is currently the SIG Network Gateway API subproject's maintainer.
-->
&lt;p>Nick Young 在 VMware 工作，是 CNCF 入口控制器 Contour 的技术负责人。
他从一开始就积极参与上游 Kubernetes 项目，最终成为 LTS 工作组的主席，
他提倡关注用户。他目前是 SIG Network Gateway API 子项目的维护者。&lt;/p>
&lt;!--
His contribution path was notable in that he began working on major areas of the Kubernetes project early on, skewing his trajectory.
-->
&lt;p>他的贡献之路是引人注目的，因为他很早就在 Kubernetes 项目的主要领域工作，这改变了他的轨迹。&lt;/p>
&lt;!--
He asserts the best thing a new contributor can do is to "start contributing". Naturally, if it is relevant to their employment, that is excellent; however, investing non-work time in contributing can pay off in the long run in terms of work. He believes that new contributors, particularly those who are currently Kubernetes users, should be encouraged to participate in higher-level project discussions.
-->
&lt;p>他断言，一个新贡献者能做的最好的事情就是“开始贡献”。当然，如果与他的工作息息相关，那好极了;
然而，把非工作时间投入到贡献中去，从长远来看可以在工作上获得回报。
他认为，应该鼓励新的贡献者，特别是那些目前是 Kubernetes 用户的人，参与到更高层次的项目讨论中来。&lt;/p>
&lt;!--
> _Just being active and contributing will get you a long way. Once you've been active for a while, you'll find that you're able to answer questions, which will mean you're asked questions, and before you know it you are an expert._
-->
&lt;blockquote>
&lt;p>只要积极主动，做出贡献，你就可以走很远。一旦你活跃了一段时间，你会发现你能够解答别人的问题，
这意味着会有人请教你或和你讨论，在你意识到这一点之前，你就已经是专家了。&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;!--
If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.
-->
&lt;p>如果你对我们接下来应该采访的人有任何意见/建议，请在 #sig-contribex 中告知我们。
非常感谢你的建议。我们很高兴有更多的人帮助我们接触到社区中更优秀的人。&lt;/p>
&lt;!--
We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋
-->
&lt;p>我们下期再见。祝你有个愉快的贡献之旅!👋&lt;/p></description></item><item><title>Blog: 更新：移除 Dockershim 的常见问题</title><link>https://kubernetes.io/zh-cn/blog/2022/02/17/dockershim-faq/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/02/17/dockershim-faq/</guid><description>
&lt;!--
layout: blog
title: "Updated: Dockershim Removal FAQ"
linkTitle: "Dockershim Removal FAQ"
date: 2022-02-17
slug: dockershim-faq
aliases: [ '/dockershim' ]
-->
&lt;!--
**This supersedes the original
[Dockershim Deprecation FAQ](/blog/2020/12/02/dockershim-faq/) article,
published in late 2020. The article includes updates from the v1.24
release of Kubernetes.**
-->
&lt;p>&lt;strong>本文是针对 2020 年末发布的&lt;a href="https://kubernetes.io/zh-cn/blog/2020/12/02/dockershim-faq/">弃用 Dockershim 的常见问题&lt;/a>的博客更新。
本文包括 Kubernetes v1.24 版本的更新。&lt;/strong>&lt;/p>
&lt;hr>
&lt;!--
This document goes over some frequently asked questions regarding the
removal of _dockershim_ from Kubernetes. The removal was originally
[announced](/blog/2020/12/08/kubernetes-1-20-release-announcement/)
as a part of the Kubernetes v1.20 release. The Kubernetes
[v1.24 release](/releases/#release-v1-24) actually removed the dockershim
from Kubernetes.
-->
&lt;p>本文介绍了一些关于从 Kubernetes 中移除 &lt;em>dockershim&lt;/em> 的常见问题。
该移除最初是作为 Kubernetes v1.20
版本的一部分&lt;a href="https://kubernetes.io/zh-cn/blog/2020/12/08/kubernetes-1-20-release-announcement/">宣布&lt;/a>的。
Kubernetes 在 &lt;a href="https://kubernetes.io/releases/#release-v1-24">v1.24 版&lt;/a>移除了 dockershim。&lt;/p>
&lt;!--
For more on what that means, check out the blog post
[Don't Panic: Kubernetes and Docker](/blog/2020/12/02/dont-panic-kubernetes-and-docker/).
-->
&lt;p>关于细节请参考博文
&lt;a href="https://kubernetes.io/zh-cn/blog/2020/12/02/dont-panic-kubernetes-and-docker/">别慌: Kubernetes 和 Docker&lt;/a>。&lt;/p>
&lt;!--
To determine the impact that the removal of dockershim would have for you or your organization,
you can read [Check whether dockershim removal affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/).
-->
&lt;p>要确定移除 dockershim 是否会对你或你的组织的影响，可以查阅：
&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">检查弃用 Dockershim 对你的影响&lt;/a>
这篇文章。&lt;/p>
&lt;!--
In the months and days leading up to the Kubernetes 1.24 release, Kubernetes contributors worked hard to try to make this a smooth transition.
-->
&lt;p>在 Kubernetes 1.24 发布之前的几个月和几天里，Kubernetes
贡献者努力试图让这个过渡顺利进行。&lt;/p>
&lt;!--
- A blog post detailing our [commitment and next steps](/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/).
- Checking if there were major blockers to migration to [other container runtimes](/docs/setup/production-environment/container-runtimes/#container-runtimes).
- Adding a [migrating from dockershim](/docs/tasks/administer-cluster/migrating-from-dockershim/) guide.
- Creating a list of
[articles on dockershim removal and on using CRI-compatible runtimes](/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/).
That list includes some of the already mentioned docs, and also covers selected external sources
(including vendor guides).
-->
&lt;ul>
&lt;li>一篇详细说明&lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">承诺和后续操作&lt;/a>的博文。&lt;/li>
&lt;li>检查是否存在迁移到其他 &lt;a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/#container-runtimes">容器运行时&lt;/a> 的主要障碍。&lt;/li>
&lt;li>添加 &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">从 dockershim 迁移&lt;/a>的指南。&lt;/li>
&lt;li>创建了一个&lt;a href="https://kubernetes.io/zh-cn/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/">有关 dockershim 移除和使用 CRI 兼容运行时的列表&lt;/a>。
该列表包括一些已经提到的文档，还涵盖了选定的外部资源（包括供应商指南）。&lt;/li>
&lt;/ul>
&lt;!--
### Why was the dockershim removed from Kubernetes?
-->
&lt;h3 id="why-was-the-dockershim-removed-from-kubernetes">为什么会从 Kubernetes 中移除 dockershim ？&lt;/h3>
&lt;!--
Early versions of Kubernetes only worked with a specific container runtime:
Docker Engine. Later, Kubernetes added support for working with other container runtimes.
The CRI standard was [created](/blog/2016/12/container-runtime-interface-cri-in-kubernetes/) to
enable interoperability between orchestrators (like Kubernetes) and many different container
runtimes.
Docker Engine doesn't implement that interface (CRI), so the Kubernetes project created
special code to help with the transition, and made that _dockershim_ code part of Kubernetes
itself.
-->
&lt;p>Kubernetes 的早期版本仅适用于特定的容器运行时：Docker Engine。
后来，Kubernetes 增加了对使用其他容器运行时的支持。&lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">创建&lt;/a> CRI
标准是为了实现编排器（如 Kubernetes）和许多不同的容器运行时之间交互操作。
Docker Engine 没有实现（CRI）接口，因此 Kubernetes 项目创建了特殊代码来帮助过渡，
并使 dockershim 代码成为 Kubernetes 的一部分。&lt;/p>
&lt;!--
The dockershim code was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
[Dockershim Removal Kubernetes Enhancement Proposal][drkep].
In fact, maintaining dockershim had become a heavy burden on the Kubernetes maintainers.
-->
&lt;p>dockershim 代码一直是一个临时解决方案（因此得名：shim）。
你可以阅读 &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">Kubernetes 移除 Dockershim 增强方案&lt;/a>
以了解相关的社区讨论和计划。
事实上，维护 dockershim 已经成为 Kubernetes 维护者的沉重负担。&lt;/p>
&lt;!--
Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing the dockershim from Kubernetes allows further development in those areas.
-->
&lt;p>此外，在较新的 CRI 运行时中实现了与 dockershim 不兼容的功能，例如 cgroups v2 和用户命名空间。
从 Kubernetes 中移除 dockershim 允许在这些领域进行进一步的开发。&lt;/p>
&lt;!--
### Are Docker and containers the same thing?
-->
&lt;h3 id="are-docker-and-containers-the-same-thing">Docker 和容器一样吗？&lt;/h3>
&lt;!--
Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.
-->
&lt;p>Docker 普及了 Linux 容器模式，并在开发底层技术方面发挥了重要作用，但是 Linux
中的容器已经存在了很长时间，容器生态系统已经发展到比 Docker 广泛得多。
OCI 和 CRI 等标准帮助许多工具在我们的生态系统中发展壮大，其中一些替代了 Docker
的某些方面，而另一些则增强了现有功能。&lt;/p>
&lt;!--
### Will my existing container images still work?
-->
&lt;h3 id="will-my-existing-container-images-still-work">我现有的容器镜像是否仍然有效？&lt;/h3>
&lt;!--
Yes, the images produced from `docker build` will work with all CRI implementations.
All your existing images will still work exactly the same.
-->
&lt;p>是的，从 &lt;code>docker build&lt;/code> 生成的镜像将适用于所有 CRI 实现，
现有的所有镜像仍将完全相同。&lt;/p>
&lt;!--
#### What about private images?
-->
&lt;h4 id="what-about-private-images">私有镜像呢？&lt;/h4>
&lt;!--
Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.
-->
&lt;p>当然可以，所有 CRI 运行时都支持在 Kubernetes 中使用的相同的 pull secrets
配置，无论是通过 PodSpec 还是 ServiceAccount。&lt;/p>
&lt;!--
### Can I still use Docker Engine in Kubernetes 1.23?
-->
&lt;h3 id="can-i-still-use-docker-engine-in-kubernetes-1-23">在 Kubernetes 1.23 版本中还可以使用 Docker Engine 吗？&lt;/h3>
&lt;!--
Yes, the only thing changed in 1.20 is a single warning log printed at [kubelet]
startup if using Docker Engine as the runtime. You'll see this warning in all versions up to 1.23. The dockershim removal occurred
in Kubernetes 1.24.
-->
&lt;p>可以使用，在 1.20 版本中唯一的改动是，如果使用 Docker Engine，
在 &lt;a href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
启动时会打印一个警告日志。
你将在 1.23 版本及以前版本看到此警告，dockershim 已在 Kubernetes 1.24 版本中移除 。&lt;/p>
&lt;!--
If you're running Kubernetes v1.24 or later, see [Can I still use Docker Engine as my container runtime?](#can-i-still-use-docker-engine-as-my-container-runtime).
(Remember, you can switch away from the dockershim if you're using any supported Kubernetes release; from release v1.24, you
**must** switch as Kubernetes no longer includes the dockershim).
-->
&lt;p>如果你运行的是 Kubernetes v1.24 或更高版本，请参阅
&lt;a href="#can-i-still-use-docker-engine-as-my-container-runtime">我仍然可以使用 Docker Engine 作为我的容器运行时吗？&lt;/a>
（如果你使用任何支持 dockershim 的版本，可以随时切换离开；从版本 v1.24
开始，因为 Kubernetes 不再包含 dockershim，你&lt;strong>必须&lt;/strong>切换）。&lt;/p>
&lt;!--
### Which CRI implementation should I use?
-->
&lt;h3 id="which-cri-implementation-should-i-use">我应该用哪个 CRI 实现？&lt;/h3>
&lt;!--
That’s a complex question and it depends on a lot of factors. If Docker Engine is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the [CNCF landscape] in case another would be an
even better fit for your environment.
-->
&lt;p>这是一个复杂的问题，依赖于许多因素。
如果你正在使用 Docker Engine，迁移到 containerd
应该是一个相对容易地转换，并将获得更好的性能和更少的开销。
然而，我们鼓励你探索 &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF landscape&lt;/a> 提供的所有选项，做出更适合你的选择。&lt;/p>
&lt;!--
#### Can I still use Docker Engine as my container runtime?
-->
&lt;h4 id="can-i-still-use-docker-engine-as-my-container-runtime">我还可以使用 Docker Engine 作为我的容器运行时吗？&lt;/h4>
&lt;!--
First off, if you use Docker on your own PC to develop or test containers: nothing changes.
You can still use Docker locally no matter what container runtime(s) you use for your
Kubernetes clusters. Containers make this kind of interoperability possible.
-->
&lt;p>首先，如果你在自己的电脑上使用 Docker 用来做开发或测试容器：它将与之前没有任何变化。
无论你为 Kubernetes 集群使用什么容器运行时，你都可以在本地使用 Docker。容器使这种交互成为可能。&lt;/p>
&lt;!--
Mirantis and Docker have [committed][mirantis] to maintaining a replacement adapter for
Docker Engine, and to maintain that adapter even after the in-tree dockershim is removed
from Kubernetes. The replacement adapter is named [`cri-dockerd`](https://github.com/Mirantis/cri-dockerd).
-->
&lt;p>Mirantis 和 Docker 已&lt;a href="https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/">承诺&lt;/a>
为 Docker Engine 维护一个替代适配器，
并在 dockershim 从 Kubernetes 移除后维护该适配器。
替代适配器名为 &lt;a href="https://github.com/Mirantis/cri-dockerd">&lt;code>cri-dockerd&lt;/code>&lt;/a>。&lt;/p>
&lt;!--
You can install `cri-dockerd` and use it to connect the kubelet to Docker Engine. Read [Migrate Docker Engine nodes from dockershim to cri-dockerd](/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/) to learn more.
-->
&lt;p>你可以安装 &lt;code>cri-dockerd&lt;/code> 并使用它将 kubelet 连接到 Docker Engine。
阅读&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/">将 Docker Engine 节点从 dockershim 迁移到 cri-dockerd&lt;/a>
以了解更多信息。&lt;/p>
&lt;!--
### Are there examples of folks using other runtimes in production today?
-->
&lt;h3 id="are-there-examples-of-folks-using-other-runtimes-in-production-today">现在是否有在生产系统中使用其他运行时的例子？&lt;/h3>
&lt;!--
All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.
-->
&lt;p>Kubernetes 所有项目在所有版本中出产的工件（Kubernetes 二进制文件）都经过了验证。&lt;/p>
&lt;!--
Additionally, the [kind] project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the [CRI-O] runtime in production since June 2019.
-->
&lt;p>此外，&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> 项目使用 containerd 已经有一段时间了，并且提高了其用例的稳定性。
Kind 和 containerd 每天都会被多次使用来验证对 Kubernetes 代码库的任何更改。
其他相关项目也遵循同样的模式，从而展示了其他容器运行时的稳定性和可用性。
例如，OpenShift 4.x 从 2019 年 6 月以来，就一直在生产环境中使用 &lt;a href="https://cri-o.io/">CRI-O&lt;/a> 运行时。&lt;/p>
&lt;!--
For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation ([CNCF]).
-->
&lt;p>至于其他示例和参考资料，你可以查看 containerd 和 CRI-O 的使用者列表，
这两个容器运行时是云原生基金会（&lt;a href="https://cncf.io">CNCF&lt;/a>）下的项目。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### People keep referencing OCI, what is that?
-->
&lt;h3 id="people-keep-referencing-oci-what-is-that">人们总在谈论 OCI，它是什么？&lt;/h3>
&lt;!--
OCI stands for the [Open Container Initiative], which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of [runc], which is the underlying default runtime for both
[containerd] and [CRI-O]. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.
-->
&lt;p>OCI 是 &lt;a href="https://opencontainers.org/about/overview/">Open Container Initiative&lt;/a> 的缩写，
它标准化了容器工具和底层实现之间的大量接口。
它们维护了打包容器镜像（OCI image）和运行时（OCI runtime）的标准规范。
它们还以 &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> 的形式维护了一个 runtime-spec 的真实实现，
这也是 &lt;a href="https://containerd.io/">containerd&lt;/a> 和 &lt;a href="https://cri-o.io/">CRI-O&lt;/a> 依赖的默认运行时。
CRI 建立在这些底层规范之上，为管理容器提供端到端的标准。&lt;/p>
&lt;!--
### What should I look out for when changing CRI implementations?
-->
&lt;h3 id="what-should-i-look-out-for-when-changing-cri-implementations">当切换 CRI 实现时，应该注意什么？&lt;/h3>
&lt;!--
While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:
-->
&lt;p>虽然 Docker 和大多数 CRI（包括 containerd）之间的底层容器化代码是相同的，
但其周边部分却存在差异。迁移时要考虑如下常见事项：&lt;/p>
&lt;!--
- Logging configuration
- Runtime resource limitations
- Node provisioning scripts that call docker or use Docker Engine via its control socket
- Plugins for `kubectl` that require the `docker` CLI or the Docker Engine control socket
- Tools from the Kubernetes project that require direct access to Docker Engine
(for example: the deprecated `kube-imagepuller` tool)
- Configuration of functionality like `registry-mirrors` and insecure registries
- Other support scripts or daemons that expect Docker Engine to be available and are run
outside of Kubernetes (for example, monitoring or security agents)
- GPUs or special hardware and how they integrate with your runtime and Kubernetes
-->
&lt;ul>
&lt;li>日志配置&lt;/li>
&lt;li>运行时的资源限制&lt;/li>
&lt;li>调用 docker 或通过其控制套接字使用 Docker Engine 的节点配置脚本&lt;/li>
&lt;li>需要 &lt;code>docker&lt;/code> 命令或 Docker Engine 控制套接字的 &lt;code>kubectl&lt;/code> 插件&lt;/li>
&lt;li>需要直接访问 Docker Engine 的 Kubernetes 工具（例如：已弃用的 'kube-imagepuller' 工具）&lt;/li>
&lt;li>配置 &lt;code>registry-mirrors&lt;/code> 和不安全的镜像仓库等功能&lt;/li>
&lt;li>保障 Docker Engine 可用、且运行在 Kubernetes 之外的脚本或守护进程（例如：监视或安全代理）&lt;/li>
&lt;li>GPU 或特殊硬件，以及它们如何与你的运行时和 Kubernetes 集成&lt;/li>
&lt;/ul>
&lt;!--
If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you've customized
your `dockerd` configuration, you’ll need to adapt that for your new container
runtime where possible.
-->
&lt;p>如果你只是用了 Kubernetes 资源请求/限制或基于文件的日志收集 DaemonSet，它们将继续稳定工作，
但是如果你用了自定义了 dockerd 配置，则可能需要为新的容器运行时做一些适配工作。&lt;/p>
&lt;!--
Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the [`crictl`][cr] tool as a drop-in replacement (see
[mapping from docker cli to crictl](https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl))
and for the latter you can use newer container build options like [img], [buildah],
[kaniko], or [buildkit-cli-for-kubectl] that don’t require Docker.
-->
&lt;p>另外还有一个需要关注的点，那就是当创建镜像时，系统维护或嵌入容器方面的任务将无法工作。
对于前者，可以用 &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> 工具作为临时替代方案
(参阅&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl">从 docker cli 到 crictl 的映射&lt;/a>)。
对于后者，可以用新的容器创建选项，例如
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>、
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>、
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a> 或
&lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a>，
他们都不需要 Docker。&lt;/p>
&lt;!--
For containerd, you can start with their [documentation] to see what configuration
options are available as you migrate things over.
-->
&lt;p>对于 containerd，你可查阅有关它的&lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">文档&lt;/a>，
获取迁移时可用的配置选项。&lt;/p>
&lt;!--
For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on [Container Runtimes].
-->
&lt;p>有关如何在 Kubernetes 中使用 containerd 和 CRI-O 的说明，
请参阅 &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">Kubernetes 相关文档&lt;/a>。&lt;/p>
&lt;!--
### What if I have more questions?
-->
&lt;h3 id="what-if-i-have-more-questions">我还有其他问题怎么办？&lt;/h3>
&lt;!--
If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: https://discuss.kubernetes.io/.
-->
&lt;p>如果你使用了供应商支持的 Kubernetes 发行版，你可以咨询供应商他们产品的升级计划。
对于最终用户的问题，请把问题发到我们的最终用户社区的&lt;a href="https://discuss.kubernetes.io/">论坛&lt;/a>。&lt;/p>
&lt;!--
You can discuss the decision to remove dockershim via a dedicated
[GitHub issue](https://github.com/kubernetes/kubernetes/issues/106917).
-->
&lt;p>你可以通过专用 &lt;a href="https://github.com/kubernetes/kubernetes/issues/106917">GitHub 问题&lt;/a>
讨论删除 dockershim 的决定。&lt;/p>
&lt;!--
You can also check out the excellent blog post
[Wait, Docker is deprecated in Kubernetes now?][dep] a more in-depth technical
discussion of the changes.
-->
&lt;p>你也可以看看这篇优秀的博客文章：&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">等等，Docker 被 Kubernetes 弃用了?&lt;/a>
对这些变化进行更深入的技术讨论。&lt;/p>
&lt;!--
### Is there any tooling that can help me find dockershim in use?
-->
&lt;h3 id="is-there-any-tooling-that-can-help-me-find-dockershim-in-use">是否有任何工具可以帮助我找到正在使用的 dockershim？&lt;/h3>
&lt;!--
Yes! The [Detector for Docker Socket (DDS)][dds] is a kubectl plugin that you can
install and then use to check your cluster. DDS can detect if active Kubernetes workloads
are mounting the Docker Engine socket (`docker.sock`) as a volume.
Find more details and usage patterns in the DDS project's [README][dds].
-->
&lt;p>是的！ &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">Docker Socket 检测器 (DDS)&lt;/a> 是一个 kubectl 插件，
你可以安装它用于检查你的集群。 DDS 可以检测运行中的 Kubernetes
工作负载是否将 Docker Engine 套接字 (&lt;code>docker.sock&lt;/code>) 作为卷挂载。
在 DDS 项目的 &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">README&lt;/a> 中查找更多详细信息和使用方法。&lt;/p>
&lt;!--
### Can I have a hug?
-->
&lt;h3 id="can-i-have-a-hug">我可以加入吗？&lt;/h3>
&lt;!--
Yes, we're still giving hugs as requested. 🤗🤗🤗
-->
&lt;p>当然，只要你愿意，随时随地欢迎。🤗🤗🤗&lt;/p></description></item><item><title>Blog: SIG Node CI 子项目庆祝测试改进两周年</title><link>https://kubernetes.io/zh-cn/blog/2022/02/16/sig-node-ci-subproject-celebrates/</link><pubDate>Wed, 16 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/02/16/sig-node-ci-subproject-celebrates/</guid><description>
&lt;!--
---
layout: blog
title: 'SIG Node CI Subproject Celebrates Two Years of Test Improvements'
date: 2022-02-16
slug: sig-node-ci-subproject-celebrates
canonicalUrl: https://www.kubernetes.dev/blog/2022/02/16/sig-node-ci-subproject-celebrates-two-years-of-test-improvements/
url: /zh-cn/blog/2022/02/sig-node-ci-subproject-celebrates
---
-->
&lt;p>&lt;strong>作者：&lt;/strong> Sergey Kanzhelev (Google), Elana Hashman (Red Hat)&lt;/p>
&lt;!--**Authors:** Sergey Kanzhelev (Google), Elana Hashman (Red Hat)-->
&lt;!--Ensuring the reliability of SIG Node upstream code is a continuous effort
that takes a lot of behind-the-scenes effort from many contributors.
There are frequent releases of Kubernetes, base operating systems,
container runtimes, and test infrastructure that result in a complex matrix that
requires attention and steady investment to "keep the lights on."
In May 2020, the Kubernetes node special interest group ("SIG Node") organized a new
subproject for continuous integration (CI) for node-related code and tests. Since its
inauguration, the SIG Node CI subproject has run a weekly meeting, and even the full hour
is often not enough to complete triage of all bugs, test-related PRs and issues, and discuss all
related ongoing work within the subgroup.-->
&lt;p>保证 SIG 节点上游代码的可靠性是一项持续的工作，需要许多贡献者在幕后付出大量努力。
Kubernetes、基础操作系统、容器运行时和测试基础架构的频繁发布，导致了一个复杂的矩阵，
需要关注和稳定的投资来“保持灯火通明”。2020 年 5 月，Kubernetes Node 特殊兴趣小组
（“SIG Node”）为节点相关代码和测试组织了一个新的持续集成（CI）子项目。自成立以来，SIG Node CI
子项目每周举行一次会议，即使一整个小时通常也不足以完成对所有缺陷、测试相关的 PR 和问题的分类，
并讨论组内所有相关的正在进行的工作。&lt;/p>
&lt;!--Over the past two years, we've fixed merge-blocking and release-blocking tests, reducing time to merge Kubernetes contributors' pull requests thanks to reduced test flakes. When we started, Node test jobs only passed 42% of the time, and through our efforts, we now ensure a consistent >90% job pass rate. We've closed 144 test failure issues and merged 176 pull requests just in kubernetes/kubernetes. And we've helped subproject participants ascend the Kubernetes contributor ladder, with 3 new org members, 6 new reviewers, and 2 new approvers.-->
&lt;p>在过去两年中，我们修复了阻塞合并和阻塞发布的测试，由于减少了测试缺陷，缩短了合并 Kubernetes
贡献者的拉取请求的时间。通过我们的努力，任务通过率由开始时 42% 提高至稳定大于 90% 。我们已经解决了 144 个测试失败问题，
并在 kubernetes/kubernetes 中合并了 176 个拉取请求。
我们还帮助子项目参与者提升了 Kubernetes 贡献者的等级，新增了 3 名组织成员、6 名评审员和 2 名审批员。&lt;/p>
&lt;!--The Node CI subproject is an approachable first stop to help new contributors
get started with SIG Node. There is a low barrier to entry for new contributors
to address high-impact bugs and test fixes, although there is a long
road before contributors can climb the entire contributor ladder:
it took over a year to establish two new approvers for the group.
The complexity of all the different components that power Kubernetes nodes
and its test infrastructure requires a sustained investment over a long period
for developers to deeply understand the entire system,
both at high and low levels of detail.-->
&lt;p>Node CI 子项目是一个可入门的第一站，帮助新参与者开始使用 SIG Node。对于新贡献者来说，
解决影响较大的缺陷和测试修复的门槛很低，尽管贡献者要攀登整个贡献者阶梯还有很长的路要走：
为该团队培养了两个新的审批人花了一年多的时间。为 Kubernetes 节点及其测试基础设施提供动力的所有
不同组件的复杂性要求开发人员在很长一段时间内进行持续投资，
以深入了解整个系统，从宏观到微观。&lt;/p>
&lt;!--We have several regular contributors at our meetings, however; our reviewers
and approvers pool is still small. It is our goal to continue to grow
contributors to ensure a sustainable distribution of work
that does not just fall to a few key approvers.-->
&lt;p>虽然在我们的会议上有几个比较固定的贡献者；但是我们的评审员和审批员仍然很少。
我们的目标是继续增加贡献者，以确保工作的可持续分配，而不仅仅是少数关键批准者。&lt;/p>
&lt;!--It's not always obvious how subprojects within SIGs are formed, operate,
and work. Each is unique to its sponsoring SIG and tailored to the projects
that the group is intended to support. As a group that has welcomed many
first-time SIG Node contributors, we'd like to share some of the details and
accomplishments over the past two years,
helping to demystify our inner workings and celebrate the hard work
of all our dedicated contributors!-->
&lt;p>SIG 中的子项目如何形成、运行和工作并不总是显而易见的。每一个都是其背后的 SIG 所独有的，
并根据该小组打算支持的项目量身定制。作为一个欢迎了许多第一次 SIG Node 贡献者的团队，
我们想分享过去两年的一些细节和成就，帮助揭开我们内部工作的神秘面纱，并庆祝我们所有专注贡献者的辛勤工作！&lt;/p>
&lt;!--## Timeline-->
&lt;h2 id="时间线">时间线&lt;/h2>
&lt;!--***May 2020.*** SIG Node CI group was formed on May 11, 2020, with more than
[30 volunteers](https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#bookmark=id.vsb8pqnf4gib)
signed up, to improve SIG Node CI signal and overall observability.
Victor Pickard focused on getting
[testgrid jobs](https://testgrid.k8s.io/sig-node) passing
when Ning Liao suggested forming a group around this effort and came up with
the [original group charter document](https://docs.google.com/document/d/1yS-XoUl6GjZdjrwxInEZVHhxxLXlTIX2CeWOARmD8tY/edit#heading=h.te6sgum6s8uf).
The SIG Node chairs sponsored group creation with Victor as a subproject lead.
Sergey Kanzhelev joined Victor shortly after as a co-lead.-->
&lt;p>&lt;em>&lt;strong>2020 年 5 月&lt;/strong>&lt;/em> SIG Node CI 组于 2020 年 5 月 11 日成立，超过
&lt;a href="https://docs.google.com/document/d/1fb-ugvgdSVIkkuJ388_nhp2pBTy_4HEVg5848Xy7n5U/edit#bookmark=id.vsb8pqnf4gib">30 名志愿者&lt;/a>
注册，以改进 SIG Node CI 信号和整体可观测性。
Victor Pickard 专注于让 &lt;a href="https://testgrid.k8s.io/sig-node">testgrid 可以运行&lt;/a> ，
当时 Ning Liao 建议围绕这项工作组建一个小组，并提出
&lt;a href="https://docs.google.com/document/d/1yS-XoUl6GjZdjrwxInEZVHhxxLXlTIX2CeWOARmD8tY/edit#heading=h.te6sgum6s8uf">最初的小组章程文件&lt;/a> 。
SIG Node 赞助成立以 Victor 作为子项目负责人的小组。Sergey Kanzhelev 不久后就加入 Victor，担任联合领导人。&lt;/p>
&lt;!--At the kick-off meeting, we discussed which tests to concentrate on fixing first
and discussed merge-blocking and release-blocking tests, many of which were failing due
to infrastructure issues or buggy test code.-->
&lt;p>在启动会议上，我们讨论了应该首先集中精力修复哪些测试，并讨论了阻塞合并和阻塞发布的测试，
其中许多测试由于基础设施问题或错误的测试代码而失败。&lt;/p>
&lt;!--The subproject launched weekly hour-long meetings to discuss ongoing work
discussion and triage.-->
&lt;p>该子项目每周召开一小时的会议，讨论正在进行的工作会谈和分类。&lt;/p>
&lt;!--***June 2020.*** Morgan Bauer, Karan Goel, and Jorge Alarcon Ochoa were
recognized as reviewers for the SIG Node CI group for their contributions,
helping significantly with the early stages of the subproject.
David Porter and Roy Yang also joined the SIG test failures GitHub team.-->
&lt;p>&lt;em>&lt;strong>2020 年 6 月&lt;/strong>&lt;/em> Morgan Bauer 、 Karan Goel 和 Jorge Alarcon Ochoa
因其贡献而被公认为 SIG Node CI 小组的评审员，为该子项目的早期阶段提供了重要帮助。
David Porter 和 Roy Yang 也加入了 SIG 检测失败的 GitHub 测试团队。&lt;/p>
&lt;!--***August 2020.*** All merge-blocking and release-blocking tests were passing,
with some flakes. However, only 42% of all SIG Node test jobs were green, as there
were many flakes and failing tests.-->
&lt;p>&lt;em>&lt;strong>2020 年 8 月&lt;/strong>&lt;/em> 所有的阻塞合并和阻塞发布的测试都通过了，伴有一些逻辑问题。
然而，只有 42% 的 SIG Node 测试作业是绿色的，
因为有许多逻辑错误和失败的测试。&lt;/p>
&lt;!--***October 2020.*** Amim Knabben becomes a Kubernetes org member for his
contributions to the subproject.-->
&lt;p>&lt;em>&lt;strong>2020 年 10 月&lt;/strong>&lt;/em> Amim Knabben 因对子项目的贡献成为 Kubernetes 组织成员。&lt;/p>
&lt;!--***January 2021.*** With healthy presubmit and critical periodic jobs passing,
the subproject discussed its goal for cleaning up the rest of periodic tests
and ensuring they passed without flakes.-->
&lt;p>&lt;em>&lt;strong>2021 年 1 月&lt;/strong>&lt;/em> 随着健全的预提交和关键定期工作的通过，子项目讨论了清理其余定期测试并确保其顺利通过的目标。&lt;/p>
&lt;!--Elana Hashman joined the subproject, stepping up to help lead it after
Victor's departure.-->
&lt;p>Elana Hashman 加入了这个子项目，在 Victor 离开后帮助领导该项目。&lt;/p>
&lt;!--***February 2021.*** Artyom Lukianov becomes a Kubernetes org member for his
contributions to the subproject.-->
&lt;p>&lt;em>&lt;strong>2021 年 2 月&lt;/strong>&lt;/em> Artyom Lukianov 因其对子项目的贡献成为 Kubernetes 组织成员。&lt;/p>
&lt;!--***August 2021.*** After SIG Node successfully ran a [bug scrub](https://groups.google.com/g/kubernetes-dev/c/w2ghO4ihje0/m/VeEql1LJBAAJ)
to clean up its bug backlog, the scope of the meeting was extended to
include bug triage to increase overall reliability, anticipating issues
before they affect the CI signal.-->
&lt;p>&lt;em>&lt;strong>2021 年 8 月&lt;/strong>&lt;/em> 在 SIG Node 成功运行 &lt;a href="https://groups.google.com/g/kubernetes-dev/c/w2ghO4ihje0/m/VeEql1LJBAAJ">bug scrub&lt;/a>
以清理其累积的缺陷之后，会议的范围扩大到包括缺陷分类以提高整体可靠性，
在问题影响 CI 信号之前预测问题。&lt;/p>
&lt;!--Subproject leads Elana Hashman and Sergey Kanzhelev are both recognized as
approvers on all node test code, supported by SIG Node and SIG Testing.-->
&lt;p>子项目负责人 Elana Hashman 和 Sergey Kanzhelev 都被认为是所有节点测试代码的审批人，由 SIG node 和 SIG Testing 支持。&lt;/p>
&lt;!--***September 2021.*** After significant deflaking progress with serial tests in
the 1.22 release spearheaded by Francesco Romani, the subproject set a goal
for getting the serial job fully passing by the 1.23 release date.-->
&lt;p>&lt;em>&lt;strong>2021 年 9 月&lt;/strong>&lt;/em> 在 Francesco Romani 牵头的 1.22 版本系列测试取得重大进展后，
该子项目设定了一个目标，即在 1.23 发布日期之前让串行任务完全通过。&lt;/p>
&lt;!--Mike Miranda becomes a Kubernetes org member for his contributions
to the subproject.-->
&lt;p>Mike Miranda 因其对子项目的贡献成为 Kubernetes 组织成员。&lt;/p>
&lt;!--***November 2021.*** Throughout 2021, SIG Node had no merge or
release-blocking test failures. Many flaky tests from past releases are removed
from release-blocking dashboards as they had been fully cleaned up.-->
&lt;p>&lt;em>&lt;strong>2021 年 11 月&lt;/strong>&lt;/em> 在整个 2021 年， SIG Node 没有合并或发布的测试失败。
过去版本中的许多古怪测试都已从阻止发布的仪表板中删除，因为它们已被完全清理。&lt;/p>
&lt;!--Danielle Lancashire was recognized as a reviewer for SIG Node's subgroup, test code.-->
&lt;p>Danielle Lancashire 被公认为 SIG Node 子组测试代码的评审员。&lt;/p>
&lt;!--The final node serial tests were completely fixed. The serial tests consist of
many disruptive and slow tests which tend to be flakey and are hard
to troubleshoot. By the 1.23 release freeze, the last serial tests were
fixed and the job was passing without flakes.-->
&lt;p>最终节点系列测试已完全修复。系列测试由许多中断性和缓慢的测试组成，这些测试往往是碎片化的，很难排除故障。
到 1.23 版本冻结时，最后一次系列测试已修复，作业顺利通过。&lt;/p>
&lt;!--[![Slack announcement that Serial tests are green](serial-tests-green.png)](https://kubernetes.slack.com/archives/C0BP8PW9G/p1638211041322900)-->
&lt;p>&lt;a href="https://kubernetes.slack.com/archives/C0BP8PW9G/p1638211041322900">&lt;img src="serial-tests-green.png" alt="宣布系列测试为绿色">&lt;/a>&lt;/p>
&lt;!--The 1.23 release got a special shout out for the tests quality and CI signal.
The SIG Node CI subproject was proud to have helped contribute to such
a high-quality release, in part due to our efforts in identifying
and fixing flakes in Node and beyond.-->
&lt;p>1.23 版本在测试质量和 CI 信号方面得到了特别的关注。SIG Node CI 子项目很自豪能够为这样一个高质量的发布做出贡献，
部分原因是我们在识别和修复节点内外的碎片方面所做的努力。&lt;/p>
&lt;!--[![Slack shoutout that release was mostly green](release-mostly-green.png)](https://kubernetes.slack.com/archives/C92G08FGD/p1637175755023200)-->
&lt;p>&lt;a href="https://kubernetes.slack.com/archives/C92G08FGD/p1637175755023200">&lt;img src="release-mostly-green.png" alt="Slack 大声宣布发布的版本大多是绿色的">&lt;/a>&lt;/p>
&lt;!--***December 2021.*** An estimated 90% of test jobs were passing at the time of
the 1.23 release (up from 42% in August 2020).-->
&lt;p>&lt;em>&lt;strong>2021 年 12 月&lt;/strong>&lt;/em> 在 1.23 版本发布时，估计有 90% 的测试工作通过了测试（2020 年 8 月为 42%）。&lt;/p>
&lt;!--Dockershim code was removed from Kubernetes. This affected nearly half of SIG Node's
test jobs and the SIG Node CI subproject reacted quickly and retargeted all the
tests. SIG Node was the first SIG to complete test migrations off dockershim,
providing examples for other affected SIGs. The vast majority of new jobs passed
at the time of introduction without further fixes required. The [effort of
removing dockershim](https://k8s.io/dockershim)) from Kubernetes is ongoing.
There are still some wrinkles from the dockershim removal as we uncover more
dependencies on dockershim, but we plan to stabilize all test jobs
by the 1.24 release.-->
&lt;p>Dockershim 代码已从 Kubernetes 中删除。这影响了 SIG Node 近一半的测试作业，
SIG Node CI 子项目反应迅速，并重新确定了所有测试的目标。
SIG Node 是第一个完成 dockershim 外测试迁移的 SIG ，为其他受影响的 SIG 提供了示例。
绝大多数新工作在引入时都已通过，无需进一步修复。
从 Kubernetes 中&lt;a href="https://k8s.io/dockershim">将 dockershim 除名的工作&lt;/a> 正在进行中。
随着我们发现 dockershim 对 dockershim 的依赖性越来越大，dockershim 的删除仍然存在一些问题，
但我们计划在 1.24 版本之前确保所有测试任务稳定。&lt;/p>
&lt;!--## Statistics-->
&lt;h2 id="统计数据">统计数据&lt;/h2>
&lt;!--Our regular meeting attendees and subproject participants for the past few months:-->
&lt;p>我们过去几个月的定期会议与会者和子项目参与者：&lt;/p>
&lt;ul>
&lt;li>Aditi Sharma&lt;/li>
&lt;li>Artyom Lukianov&lt;/li>
&lt;li>Arnaud Meukam&lt;/li>
&lt;li>Danielle Lancashire&lt;/li>
&lt;li>David Porter&lt;/li>
&lt;li>Davanum Srinivas&lt;/li>
&lt;li>Elana Hashman&lt;/li>
&lt;li>Francesco Romani&lt;/li>
&lt;li>Matthias Bertschy&lt;/li>
&lt;li>Mike Miranda&lt;/li>
&lt;li>Paco Xu&lt;/li>
&lt;li>Peter Hunt&lt;/li>
&lt;li>Ruiwen Zhao&lt;/li>
&lt;li>Ryan Phillips&lt;/li>
&lt;li>Sergey Kanzhelev&lt;/li>
&lt;li>Skyler Clark&lt;/li>
&lt;li>Swati Sehgal&lt;/li>
&lt;li>Wenjun Wu&lt;/li>
&lt;/ul>
&lt;!--The [kubernetes/test-infra](https://github.com/kubernetes/test-infra/) source code repository contains test definitions. The number of
Node PRs just in that repository:
- 2020 PRs (since May): [183](https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2020-05-01..2020-12-31+-author%3Ak8s-infra-ci-robot+)
- 2021 PRs: [264](https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2021-01-01..2021-12-31+-author%3Ak8s-infra-ci-robot+)-->
&lt;p>&lt;a href="https://github.com/kubernetes/test-infra/">kubernetes/test-infra&lt;/a> 源代码存储库包含测试定义。该存储库中的节点 PR 数：&lt;/p>
&lt;ul>
&lt;li>2020 年 PR（自 5 月起）：&lt;a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2020-05-01..2020-12-31+-author%3Ak8s-infra-ci-robot+">183&lt;/a>&lt;/li>
&lt;li>2021 年 PR：&lt;a href="https://github.com/kubernetes/test-infra/pulls?q=is%3Apr+is%3Aclosed+label%3Asig%2Fnode+created%3A2021-01-01..2021-12-31+-author%3Ak8s-infra-ci-robot+">264&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--Triaged issues and PRs on CI board (including triaging away from the subgroup scope):
- 2020 (since May)：[132](https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2020-05-01..2020-12-31)
- 2021: [532](https：//github.com/issues?q=project%3Akubernetes%2F43+created%3A2021-01-01..2021-12-31+)-->
&lt;p>CI 委员会上的问题和 PRs 分类（包括子组范围之外的分类）：&lt;/p>
&lt;ul>
&lt;li>2020 年（自 5 月起）：&lt;a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2020-05-01..2020-12-31">132&lt;/a>&lt;/li>
&lt;li>2021 年：&lt;a href="https://github.com/issues?q=project%3Akubernetes%2F43+created%3A2021-01-01..2021-12-31+">532&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--## Future-->
&lt;h2 id="未来">未来&lt;/h2>
&lt;!--Just "keeping the lights on" is a bold task and we are committed to improving this experience.
We are working to simplify the triage and review processes for SIG Node.
Specifically, we are working on better test organization, naming,
and tracking:-->
&lt;p>只是“保持灯亮”是一项大胆的任务，我们致力于改善这种体验。
我们正在努力简化 SIG Node 的分类和审查流程。&lt;/p>
&lt;p>具体来说，我们正在致力于更好的测试组织、命名和跟踪：&lt;/p>
&lt;!-- - https://github.com/kubernetes/enhancements/pull/3042
- https://github.com/kubernetes/test-infra/issues/24641
- [Kubernetes SIG-Node CI Testgrid Tracker](https://docs.google.com/spreadsheets/d/1IwONkeXSc2SG_EQMYGRSkfiSWNk8yWLpVhPm-LOTbGM/edit#gid=0)-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/pull/3042">https://github.com/kubernetes/enhancements/pull/3042&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/issues/24641">https://github.com/kubernetes/test-infra/issues/24641&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.google.com/spreadsheets/d/1IwONkeXSc2SG_EQMYGRSkfiSWNk8yWLpVhPm-LOTbGM/edit#gid=0">Kubernetes SIG Node CI 测试网格跟踪器&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--We are also constantly making progress on improved tests debuggability and de-flaking.
If any of this interests you, we'd love for you to join us!
There's plenty to learn in debugging test failures, and it will help you gain
familiarity with the code that SIG Node maintains.-->
&lt;p>我们还在改进测试的可调试性和去剥落方面不断取得进展。&lt;/p>
&lt;p>如果你对此感兴趣，我们很乐意您能加入我们！
在调试测试失败中有很多东西需要学习，它将帮助你熟悉 SIG Node 维护的代码。&lt;/p>
&lt;!--You can always find information about the group on the
[SIG Node](https://github.com/kubernetes/community/tree/master/sig-node) page.
We give group updates at our maintainer track sessions, such as
[KubeCon + CloudNativeCon Europe 2021](https://kccnceu2021.sched.com/event/iE8E/kubernetes-sig-node-intro-and-deep-dive-elana-hashman-red-hat-sergey-kanzhelev-google) 和
[KubeCon + CloudNative North America 2021](https://kccncna2021.sched.com/event/lV9D/kubenetes-sig-node-intro-and-deep-dive-elana-hashman-derek-carr-red-hat-sergey-kanzhelev-dawn-chen-google?iframe=no&amp;w=100%&amp;sidebar=yes&amp;bg=no)。
Join us in our mission to keep the kubelet and other SIG Node components reliable and ensure smooth and uneventful releases!-->
&lt;p>你可以在 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node&lt;/a> 页面上找到有关该组的信息。
我们在我们的维护者轨道会议上提供组更新，例如：
&lt;a href="https://kccnceu2021.sched.com/event/iE8E/kubernetes-sig-node-intro-and-deep-dive-elana-hashman-red-hat-sergey-kanzhelev-google">KubeCon + CloudNativeCon Europe 2021&lt;/a> 和
&lt;a href="https://kccncna2021.sched.com/event/lV9D/kubenetes-sig-node-intro-and-deep-dive-elana-hashman-derek-carr-red-hat-sergey-kanzhelev-dawn-chen-google?iframe=no&amp;amp;w=100%25&amp;amp;sidebar=yes&amp;amp;bg=no">KubeCon + CloudNative North America 2021&lt;/a>。
加入我们的使命，保持 kubelet 和其他 SIG Node 组件的可靠性，确保顺顺利利发布！&lt;/p></description></item><item><title>Blog: 关注 SIG Multicluster</title><link>https://kubernetes.io/zh-cn/blog/2022/02/07/sig-multicluster-spotlight-2022/</link><pubDate>Mon, 07 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/02/07/sig-multicluster-spotlight-2022/</guid><description>
&lt;!--
layout: blog
title: "Spotlight on SIG Multicluster"
date: 2022-02-07
slug: sig-multicluster-spotlight-2022
canonicalUrl: https://www.kubernetes.dev/blog/2022/02/04/sig-multicluster-spotlight-2022/
-->
&lt;!--
**Authors:** Dewan Ahmed (Aiven) and Chris Short (AWS)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Dewan Ahmed (Aiven) 和 Chris Short (AWS)&lt;/p>
&lt;!--
## Introduction
[SIG Multicluster](https://github.com/kubernetes/community/tree/master/sig-multicluster) is the SIG focused on how Kubernetes concepts are expanded and used beyond the cluster boundary. Historically, Kubernetes resources only interacted within that boundary - KRU or Kubernetes Resource Universe (not an actual Kubernetes concept). Kubernetes clusters, even now, don't really know anything about themselves or, about other clusters. Absence of cluster identifiers is a case in point. With the growing adoption of multicloud and multicluster deployments, the work SIG Multicluster doing is gaining a lot of attention. In this blog, [Jeremy Olmsted-Thompson, Google](https://twitter.com/jeremyot) and [Chris Short, AWS](https://twitter.com/ChrisShort) discuss the interesting problems SIG Multicluster is solving and how you can get involved. Their initials **JOT** and **CS** will be used for brevity.
-->
&lt;h2 id="简介">简介&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-multicluster">SIG Multicluster&lt;/a>
是专注于如何拓展 Kubernetes 的概念并将其用于集群边界之外的 SIG。
以往 Kubernetes 资源仅在 Kubernetes Resource Universe (KRU) 这个边界内进行交互，其中 KRU 不是一个实际的 Kubernetes 概念。
即使是现在，Kubernetes 集群对自身或其他集群并不真正了解。集群标识符的缺失就是一个例子。
随着多云和多集群部署日益普及，SIG Multicluster 所做的工作越来越受到关注。
在这篇博客中，&lt;a href="https://twitter.com/jeremyot">来自 Google 的 Jeremy Olmsted-Thompson&lt;/a> 和
&lt;a href="https://twitter.com/ChrisShort">来自 AWS 的 Chris Short&lt;/a> 讨论了 SIG Multicluster
正在解决的一些有趣的问题和以及大家如何参与其中。
为简洁起见，下文将使用他们两位的首字母 &lt;strong>JOT&lt;/strong> 和 &lt;strong>CS&lt;/strong>。&lt;/p>
&lt;!--
## A summary of their conversation
**CS**: How long has the SIG Multicluster existed and how was the SIG in its infancy? How long have you been with this SIG?
**JOT**: I've been around for almost two years in the SIG Multicluster. All I know about the infancy years is from the lore but even in the early days, it was always about solving this same problem. Early efforts have been things like [KubeFed](https://github.com/kubernetes-sigs/kubefed). I think there are still folks using KubeFed but it's a smaller slice. Back then, I think people out there deploying large numbers of Kubernetes clusters were really not at a point where we had a ton of real concrete use cases. Projects like KubeFed and [Cluster Registry](https://github.com/kubernetes-retired/cluster-registry) were developed around that time and the need back then can be associated to these projects. The motivation for these projects were how do we solve the problems that we think people are **going to have**, when they start expanding to multiple clusters. Honestly, in some ways, it was trying to do too much at that time.
-->
&lt;h2 id="谈话总结">谈话总结&lt;/h2>
&lt;p>&lt;strong>CS&lt;/strong>：SIG Multicluster 存在多久了？SIG 在起步阶段情况如何？你参与这个 SIG 多长时间了？&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>：我在 SIG Multicluster 工作了将近两年。我所知道的关于初创时期的情况都来自传说，但即使在早期，也一直是为了解决相同的问题。
早期工作的例子之一是 &lt;a href="https://github.com/kubernetes-sigs/kubefed">KubeFed&lt;/a>。
我认为仍然有一些人在使用 KubeFed，但它只是一小部分。
那时，我认为人们在部署大量 Kubernetes 集群时，还没有达到我们拥有大量实际具体用例的地步。
像 KubeFed 和 &lt;a href="https://github.com/kubernetes-retired/cluster-registry">Cluster Registry&lt;/a>
这样的项目就是在那个时候开发的，当时的需求可以与这些项目相关联。
这些项目的动机是如何解决我们认为在开始扩展到多个集群时 &lt;strong>会遇到的问题&lt;/strong>。
老实说，在某些方面，当时它试图做得太多了。&lt;/p>
&lt;!--
**CS**: How does KubeFed differ from the current state of SIG Multicluster? How does the **lore** differ from the **now**?
**JOT**: Yeah, it was like trying to get ahead of potential problems instead of addressing specific problems. I think towards the end of 2019, there was a slow down in SIG multicluster work and we kind of picked it back up with one of the most active recent projects that is the [SIG Multicluster services (MCS)](https://github.com/kubernetes-sigs/mcs-api).
Now this is the shift to solving real specific problems. For example,
> I've got workloads that are spread across multiple clusters and I need them to talk to each other.
-->
&lt;p>&lt;strong>CS&lt;/strong>：KubeFed 与 SIG Multicluster 的现状有何不同？&lt;strong>初创期&lt;/strong> 与 &lt;strong>现在&lt;/strong> 有何不同？&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>：嗯嗯，这就像总是要预防潜在问题而不是解决眼前具体的问题。我认为在 2019 年底，
SIG Multicluster 工作有所放缓，我们通过最近最活跃的项目之一
&lt;a href="https://github.com/kubernetes-sigs/mcs-api">SIG Multicluster services (MCS)&lt;/a> 将其重新拾起。&lt;/p>
&lt;p>现在我们向解决实际的具体问题开始转变。比如说。&lt;/p>
&lt;blockquote>
&lt;p>我的工作负载分布在多个集群中，我需要它们相互通信。&lt;/p>
&lt;/blockquote>
&lt;!--
Okay, that's very straightforward and we know that we need to solve that. To get started, let's make sure that these projects can work together on a common API so you get the same kind of portability that you get with Kubernetes.
There's a few implementations of the MCS API out there and more are being developed. But, we didn't build an implementation because depending on how you're deploying things there could be hundreds of implementations. As long as you only need the basic Multicluster service functionality, it'll just work on whatever background you want, whether it's Submariner, GKE, or a service mesh.
-->
&lt;p>好吧，这是非常直接的，我们也知道需要解决这个问题。
首先，让我们确保这些项目可以在一个通用的 API 上协同工作，这样你就可以获得与 Kubernetes 相同的可移植性。&lt;/p>
&lt;p>目前有一些 MCS API 的实现，并且更多的实现正在开发中。但是，我们没有建立一个实现，
因为取决于你的部署方式不同，可能会有数百种实现。
只要你所需要的基本的多集群服务功能，它就可以在你想要的任何背景下工作，无论是 Submariner、GKE 还是服务网格。&lt;/p>
&lt;!--
My favorite example of "then vs. now" is cluster ID. A few years ago, there was an effort to define a cluster ID. A lot of really good thought went into this concept, for example, how do we make a cluster ID is unique across multiple clusters. How do we make this ID globally unique so it'll work in every contact? Let's say, there's an acquisition or merger of teams - does the cluster IDs still remain unique for those teams?
With Multicluster services, we found the need for an actual cluster ID, and it has a very specific need. To address this specific need, we're no longer considering every single Kubernetes cluster out there rather the ClusterSets - a grouping of clusters that work together in some kind of bounds. That's a much narrower scope than considering clusters everywhere in time and space. It also leaves flexibility for an implementer to define the boundary (a ClusterSet) beyond which this cluster ID will no longer be unique. -->
&lt;p>我最喜欢的“过去与现在“的例子是集群 ID。几年前曾经有过定义集群 ID 的尝试。
针对这个概念，有很多非常好的想法。例如，我们如何使集群 ID 在多个集群中是唯一的。
我们如何使这个 ID 全球范围内唯一，以便它在各个通讯中发挥作用？
假设有团队被收购或合并 - 集群 ID 对于这些团队仍然是唯一的吗？&lt;/p>
&lt;p>在 Multicluster 服务的相关工作中，我们发现需要一个实际的集群 ID，并且这一需求非常具体。
为了满足这一特定需求，我们不再考虑一个个 Kubernetes 集群，而是考虑 ClusterSets — 在某种范围内协同工作的集群分组。
与考虑所有时间点和所有空间位置上存在的集群相比，这一范畴要窄得多。
这一概念还让实现者具备了定义边界（ClusterSet）的灵活性，在该边界之外，该集群 ID 将不再是唯一的。&lt;/p>
&lt;!--
**CS**: How do you feel about the current state of SIG Multicluster versus where you're hoping to be in future?
**JOT**: There's a few projects that are kind of getting started, for example, Work API. In the future, I think that some common practices around how do we deploy things across clusters are going to develop.
> If I have clusters deployed in a bunch of different regions; what's the best way to actually do that?
-->
&lt;p>&lt;strong>CS&lt;/strong>：你对 SIG Multicluster 的现状有何看法，你希望未来达到什么样的目标？&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>：有一些项目正在起步，例如 Work API。 在未来，我认为围绕着如何跨集群部署应用的一些共同做法将会发展起来。&lt;/p>
&lt;blockquote>
&lt;p>如果我的集群部署在不同的地区，那么最好的方式是什么？&lt;/p>
&lt;/blockquote>
&lt;!--
The answer is, almost always, "it depends". Why are you doing this? Is it because there's some kind of compliance that makes you care about locality? Is it performance? Is it availability?
I think revisiting registry patterns will probably be a natural step after we have cluster IDs, that is, how do you actually associate these clusters together? Maybe you've got a distributed deployment that you run in your own data centers all over the world. I imagine that expanding the API in that space is going to be important as more multi cluster features develop. It really depends on what the community starts doing with these tools.
-->
&lt;p>答案几乎总是“视情况而定”。你为什么要这样做？是因为某种合规性使你关注位置吗？是性能问题吗？是可用性吗？&lt;/p>
&lt;p>我认为，在我们有了集群 ID 之后，重新审视注册表模式可能是很自然的一步，也就是说，
你如何将这些集群真正关联在一起？也许你有一个分布式部署，你在世界各地的数据中心运行。
我想随着多集群特性的进一步开发，扩展该领域的 API 将变得很重要。
这实际上取决于社区开始使用这些工具做什么。&lt;/p>
&lt;!--
**CS**: In the early days of Kubernetes, we used to have a few large Kubernetes clusters and now we're dealing with many small Kubernetes clusters - even multiple clusters for our own dev environments. How has this shift from a few large clusters to many small clusters affected the SIG? Has it accelerated the work or make it challenging in any way?
-->
&lt;p>&lt;strong>CS&lt;/strong>：在 Kubernetes 的早期，我们只有寥寥几个大型的 Kubernetes 集群，而现在我们面对的是大量的小型 Kubernetes 集群，就像我自己所在的开发环境中就使用了多个集群。
这种从几个大集群到许多小集群的转变对 SIG 有何影响？它是否加快了工作进度或在某种程度上使得问题变得更困难？&lt;/p>
&lt;!--
**JOT**: I think that it has created a lot of ambiguity that needs solving. Originally, you'd have a dev cluster, a staging cluster, and a prod cluster. When the multi region thing came in, we started needing dev/staging/prod clusters, per region. And then, sometimes clusters really need more isolation due to compliance or some regulations issues. Thus, we're ending up with a lot of clusters. I think figuring out the right balance on how many clusters should you actually have is important. The power of Kubernetes is being able to deploy a lot of things managed by a single control plane. So, it's not like every single workload that gets deployed should be in its own cluster. But I think it's pretty clear that we can't put every single workload in a single cluster.
-->
&lt;p>&lt;strong>JOT&lt;/strong>：我认为它带来了很多需要解决的歧义。最初，你可能拥有一个 dev 集群、一个 staging 集群和一个 prod 集群。
当引入了多区域的考量时，我们开始在每个区域部署 dev/staging/prod 集群。
再后来，有时由于合规性或某些法规问题，集群确实需要更多的隔离。
因此，我们最终会有很多集群。我认为在你究竟应该有多少个集群上找到平衡是很重要的。Kubernetes 的强大之处在于能够部署由单个控制平面管理的大量事物。
因此，并不是每个被部署的工作负载都应该在自己的集群中。
但是，我认为同样很明显的是，我们不能将所有工作负载都放在一个集群中。&lt;/p>
&lt;!--
**CS**: What are some of your favorite things about this SIG?
**JOT**: The complexity of the problems, the people and the newness of the space. We don't have right answers and we have to figure this out. At the beginning, we couldn't even think about multi clusters because there was no way to connect services across clusters. Now there is and we're starting to go tackle those problems, I think that this is a really fun place to be in because I expect that the SIG is going to get a lot busier the next couple of years. It's a very collaborative group and we definitely would like more people to come join us, get involved, raise their problems and bring their ideas.
-->
&lt;p>&lt;strong>CS&lt;/strong>：你最喜欢 SIG 的哪些方面？&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>：问题的复杂性、人的因素和领域的新颖性。我们还没有正确的答案，我们必须找到正确的答案。
一开始，我们甚至无法考虑多集群，因为无法跨集群连接服务。
现在我们开始着手解决这些问题，我认为这是一个非常有趣的地方，因为我预计 SIG 在未来几年会变得更加繁忙。
这是一个协作很密切的团体，我们绝对希望更多的人参与、加入我们，提出他们的问题和想法。&lt;/p>
&lt;!--
**CS**: What do you think keeps people in this group? How has the pandemic affected you?
-->
&lt;p>&lt;strong>CS&lt;/strong>：你认为是什么让人们留在这个群体中？疫情对你有何影响？&lt;/p>
&lt;!--
**JOT**: I think it definitely got a little bit quieter during the pandemic. But for the most part; it's a very distributed group so whether you're calling in to our weekly meetings from a conference room or from your home, it doesn't make that huge of a difference. During the pandemic, a lot of people had time to focus on what's next for their scale and growth. I think that's what keeps people in the group - we have real problems that need to be solved which are very new in this space. And it's fun :)
-->
&lt;p>&lt;strong>JOT&lt;/strong>：我认为在疫情期间这个群体肯定会变得安静一些。但在大多数情况下，这是一个非常分散的小组，
因此无论你在会议室或者在家中参加我们的每周会议，都不会产生太大的影响。在疫情期间，很多人有时间专注于他们接下来的规模和增长。
我认为这就是让人们留在团队中的原因 - 我们有真正的问题需要解决，这些问题在这个领域是非常新颖的、有趣的。&lt;/p>
&lt;!--
## Wrap up
**CS**: That's all we have for today. Thanks Jeremy for your time.
**JOT**: Thanks Chris. Everybody is welcome at our [bi-weekly meetings](https://github.com/kubernetes/community/tree/master/sig-multicluster#meetings). We love as many people to come as possible and welcome all questions and all ideas. It's a new space and it'd be great to grow the community.
-->
&lt;h2 id="结束语">结束语&lt;/h2>
&lt;p>&lt;strong>CS&lt;/strong>：这就是我们今天的全部内容，感谢 Jeremy 的时间。&lt;/p>
&lt;p>&lt;strong>JOT&lt;/strong>：谢谢 Chris。我们的&lt;a href="https://github.com/kubernetes/community/tree/master/sig-multicluster#meetings">双周会议&lt;/a>
欢迎所有人参加。我们希望尽可能多的人前来，并欢迎所有问题与想法。
这是一个新的领域，如果能让社区发展起来，那就太好了。&lt;/p></description></item><item><title>Blog: 确保准入控制器的安全</title><link>https://kubernetes.io/zh-cn/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</link><pubDate>Wed, 19 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/</guid><description>
&lt;!--
layout: blog
title: "Securing Admission Controllers"
date: 2022-01-19
slug: secure-your-admission-controllers-and-webhooks
-->
&lt;!--
**Author:** Rory McCune (Aqua Security)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Rory McCune (Aqua Security)&lt;/p>
&lt;!--
[Admission control](/docs/reference/access-authn-authz/admission-controllers/) is a key part of Kubernetes security, alongside authentication and authorization.
Webhook admission controllers are extensively used to help improve the security of Kubernetes clusters in a variety of ways including restricting the privileges of workloads and ensuring that images deployed to the cluster meet organization’s security requirements.
-->
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/admission-controllers/">准入控制&lt;/a>和认证、授权都是 Kubernetes 安全性的关键部分。
Webhook 准入控制器被广泛用于以多种方式帮助提高 Kubernetes 集群的安全性，
包括限制工作负载权限和确保部署到集群的镜像满足组织安全要求。&lt;/p>
&lt;!--
However, as with any additional component added to a cluster, security risks can present themselves.
A security risk example is if the deployment and management of the admission controller are not handled correctly. To help admission controller users and designers manage these risks appropriately,
the [security documentation](https://github.com/kubernetes/community/tree/master/sig-security#security-docs) subgroup of SIG Security has spent some time developing a [threat model for admission controllers](https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control).
This threat model looks at likely risks which may arise from the incorrect use of admission controllers, which could allow security policies to be bypassed, or even allow an attacker to get unauthorised access to the cluster.
-->
&lt;p>然而，与添加到集群中的任何其他组件一样，安全风险也会随之出现。
一个安全风险示例是没有正确处理准入控制器的部署和管理。
为了帮助准入控制器用户和设计人员适当地管理这些风险，
SIG Security 的&lt;a href="https://github.com/kubernetes/community/tree/master/sig-security#security-docs">安全文档&lt;/a>小组
花费了一些时间来开发一个&lt;a href="https://github.com/kubernetes/sig-security/tree/main/sig-security-docs/papers/admission-control">准入控制器威胁模型&lt;/a>。
这种威胁模型着眼于由于不正确使用准入控制器而产生的可能的风险，可能允许绕过安全策略，甚至允许攻击者未经授权访问集群。&lt;/p>
&lt;!--
From the threat model, we developed a set of security best practices that should be adopted to ensure that cluster operators can get the security benefits of admission controllers whilst avoiding any risks from using them.
-->
&lt;p>基于这个威胁模型，我们开发了一套安全最佳实践。
你应该采用这些实践来确保集群操作员可以获得准入控制器带来的安全优势，同时避免使用它们带来的任何风险。&lt;/p>
&lt;!--
## Admission controllers and good practices for security
-->
&lt;h2 id="准入控制器和安全的良好做法">准入控制器和安全的良好做法&lt;/h2>
&lt;!--
From the threat model, a couple of themes emerged around how to ensure the security of admission controllers.
-->
&lt;p>基于这个威胁模型，围绕着如何确保准入控制器的安全性出现了几个主题。&lt;/p>
&lt;!--
### Secure webhook configuration
-->
&lt;h3 id="安全的-webhook-配置">安全的 webhook 配置&lt;/h3>
&lt;!--
It’s important to ensure that any security component in a cluster is well configured and admission controllers are no different here. There are a couple of security best practices to consider when using admission controllers
-->
&lt;p>确保集群中的任何安全组件都配置良好是很重要的，在这里准入控制器也并不例外。
使用准入控制器时需要考虑几个安全最佳实践：&lt;/p>
&lt;!--
* **Correctly configured TLS for all webhook traffic**. Communications between the API server and the admission controller webhook should be authenticated and encrypted to ensure that attackers who may be in a network position to view or modify this traffic cannot do so. To achieve this access the API server and webhook must be using certificates from a trusted certificate authority so that they can validate their mutual identities
-->
&lt;ul>
&lt;li>&lt;strong>为所有 webhook 流量正确配置了 TLS&lt;/strong>。
API 服务器和准入控制器 webhook 之间的通信应该经过身份验证和加密，以确保处于网络中查看或修改此流量的攻击者无法查看或修改。
要实现此访问，API 服务器和 webhook 必须使用来自受信任的证书颁发机构的证书，以便它们可以验证相互的身份。&lt;/li>
&lt;/ul>
&lt;!--
* **Only authenticated access allowed**. If an attacker can send an admission controller large numbers of requests, they may be able to overwhelm the service causing it to fail. Ensuring all access requires strong authentication should mitigate that risk.
-->
&lt;ul>
&lt;li>&lt;strong>只允许经过身份验证的访问&lt;/strong>。
如果攻击者可以向准入控制器发送大量请求，他们可能会压垮服务导致其失败。
确保所有访问都需要强身份验证可以降低这种风险。&lt;/li>
&lt;/ul>
&lt;!--
* **Admission controller fails closed**. This is a security practice that has a tradeoff, so whether a cluster operator wants to configure it will depend on the cluster’s threat model. If an admission controller fails closed, when the API server can’t get a response from it, all deployments will fail. This stops attackers bypassing the admission controller by disabling it, but, can disrupt the cluster’s operation. As clusters can have multiple webhooks, one approach to hit a middle ground might be to have critical controls on a fail closed setups and less critical controls allowed to fail open.
-->
&lt;ul>
&lt;li>&lt;strong>准入控制器关闭失败&lt;/strong>。
这是一种需要权衡的安全实践，集群操作员是否要对其进行配置取决于集群的威胁模型。
如果一个准入控制器关闭失败，当 API 服务器无法从它得到响应时，所有的部署都会失败。
这可以阻止攻击者通过禁用准入控制器绕过准入控制器，但可能会破坏集群的运行。
由于集群可以有多个 webhook，因此一种折中的方法是对关键控制允许故障关闭，
并允许不太关键的控制进行故障打开。&lt;/li>
&lt;/ul>
&lt;!--
* **Regular reviews of webhook configuration**. Configuration mistakes can lead to security issues, so it’s important that the admission controller webhook configuration is checked to make sure the settings are correct. This kind of review could be done automatically by an Infrastructure As Code scanner or manually by an administrator.
-->
&lt;ul>
&lt;li>&lt;strong>定期审查 webhook 配置&lt;/strong>。
配置错误可能导致安全问题，因此检查准入控制器 webhook 配置以确保设置正确非常重要。
这种审查可以由基础设施即代码扫描程序自动完成，也可以由管理员手动完成。&lt;/li>
&lt;/ul>
&lt;!--
### Secure cluster configuration for admission control
-->
&lt;h3 id="为准入控制保护集群配置">为准入控制保护集群配置&lt;/h3>
&lt;!--
In most cases, the admission controller webhook used by a cluster will be installed as a workload in the cluster. As a result, it’s important to ensure that Kubernetes' security features that could impact its operation are well configured.
-->
&lt;p>在大多数情况下，集群使用的准入控制器 webhook 将作为工作负载安装在集群中。
因此，确保正确配置了可能影响其操作的 Kubernetes 安全特性非常重要。&lt;/p>
&lt;!--
* **Restrict [RBAC](/docs/reference/access-authn-authz/rbac/) rights**. Any user who has rights which would allow them to modify the configuration of the webhook objects or the workload that the admission controller uses could disrupt its operation. So it’s important to make sure that only cluster administrators have those rights.
-->
&lt;ul>
&lt;li>&lt;strong>限制 &lt;a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/">RBAC&lt;/a> 权限&lt;/strong>。
任何有权修改 webhook 对象的配置或准入控制器使用的工作负载的用户都可以破坏其运行。
因此，确保只有集群管理员拥有这些权限非常重要。&lt;/li>
&lt;/ul>
&lt;!--
* **Prevent privileged workloads**. One of the realities of container systems is that if a workload is given certain privileges, it will be possible to break out to the underlying cluster node and impact other containers on that node. Where admission controller services run in the cluster they’re protecting, it’s important to ensure that any requirement for privileged workloads is carefully reviewed and restricted as much as possible.
-->
&lt;ul>
&lt;li>&lt;strong>防止特权工作负载&lt;/strong>。
容器系统的一个现实是，如果工作负载被赋予某些特权，
则有可能逃逸到下层的集群节点并影响该节点上的其他容器。
如果准入控制器服务在它们所保护的集群上运行，
一定要确保对特权工作负载的所有请求都要经过仔细审查并尽可能地加以限制。&lt;/li>
&lt;/ul>
&lt;!--
* **Strictly control external system access**. As a security service in a cluster admission controller systems will have access to sensitive information like credentials. To reduce the risk of this information being sent outside the cluster, [network policies](/docs/concepts/services-networking/network-policies/) should be used to restrict the admission controller services access to external networks.
-->
&lt;ul>
&lt;li>&lt;strong>严格控制外部系统访问&lt;/strong>。
作为集群中的安全服务，准入控制器系统将有权访问敏感信息，如凭证。
为了降低此信息被发送到集群外的风险，
应使用&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/network-policies/">网络策略&lt;/a>
来限制准入控制器服务对外部网络的访问。&lt;/li>
&lt;/ul>
&lt;!--
* **Each cluster has a dedicated webhook**. Whilst it may be possible to have admission controller webhooks that serve multiple clusters, there is a risk when using that model that an attack on the webhook service would have a larger impact where it’s shared. Also where multiple clusters use an admission controller there will be increased complexity and access requirements, making it harder to secure.
-->
&lt;ul>
&lt;li>&lt;strong>每个集群都有一个专用的 webhook&lt;/strong>。
虽然可能让准入控制器 webhook 服务于多个集群的，
但在使用该模型时存在对 webhook 服务的攻击会对共享它的地方产生更大影响的风险。
此外，在多个集群使用准入控制器的情况下，复杂性和访问要求也会增加，从而更难保护其安全。&lt;/li>
&lt;/ul>
&lt;!--
### Admission controller rules
-->
&lt;h3 id="准入控制器规则">准入控制器规则&lt;/h3>
&lt;!--
A key element of any admission controller used for Kubernetes security is the rulebase it uses. The rules need to be able to accurately meet their goals avoiding false positive and false negative results.
-->
&lt;p>对于用于 Kubernetes 安全的所有准入控制器而言，一个关键元素是它使用的规则库。
规则需要能够准确地满足其目标，避免假阳性和假阴性结果。&lt;/p>
&lt;!--
* **Regularly test and review rules**. Admission controller rules need to be tested to ensure their accuracy. They also need to be regularly reviewed as the Kubernetes API will change with each new version, and rules need to be assessed with each Kubernetes release to understand any changes that may be required to keep them up to date.
-->
&lt;ul>
&lt;li>&lt;strong>定期测试和审查规则&lt;/strong>。
需要测试准入控制器规则以确保其准确性。
还需要定期审查，因为 Kubernetes API 会随着每个新版本而改变，
并且需要在每个 Kubernetes 版本中评估规则，以了解使他们保持最新版本所需要做的任何改变。&lt;/li>
&lt;/ul></description></item><item><title>Blog: 认识我们的贡献者 - 亚太地区（印度地区）</title><link>https://kubernetes.io/zh-cn/blog/2022/01/10/meet-our-contributors-india-ep-01/</link><pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/01/10/meet-our-contributors-india-ep-01/</guid><description>
&lt;!--
layout: blog
title: "Meet Our Contributors - APAC (India region)"
date: 2022-01-10
slug: meet-our-contributors-india-ep-01
canonicalUrl: https://www.kubernetes.dev/blog/2022/01/10/meet-our-contributors-india-ep-01/
-->
&lt;!--
**Authors &amp; Interviewers:** [Anubhav Vardhan](https://github.com/anubha-v-ardhan), [Atharva Shinde](https://github.com/Atharva-Shinde), [Avinesh Tripathi](https://github.com/AvineshTripathi), [Debabrata Panigrahi](https://github.com/Debanitrkl), [Kunal Verma](https://github.com/verma-kunal), [Pranshu Srivastava](https://github.com/PranshuSrivastava), [Pritish Samal](https://github.com/CIPHERTron), [Purneswar Prasad](https://github.com/PurneswarPrasad), [Vedant Kakde](https://github.com/vedant-kakde)
-->
&lt;p>&lt;strong>作者和采访者：&lt;/strong> &lt;a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan&lt;/a>、
&lt;a href="https://github.com/Atharva-Shinde">Atharva Shinde&lt;/a>、
&lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>、
&lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>、
&lt;a href="https://github.com/verma-kunal">Kunal Verma&lt;/a>、
&lt;a href="https://github.com/PranshuSrivastava">Pranshu Srivastava&lt;/a>、
&lt;a href="https://github.com/CIPHERTron">Pritish Samal&lt;/a>、
&lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>、
&lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;!--
**Editor:** [Priyanka Saggu](https://psaggu.com)
-->
&lt;p>&lt;strong>编辑：&lt;/strong> &lt;a href="https://psaggu.com">Priyanka Saggu&lt;/a>&lt;/p>
&lt;hr>
&lt;!--
Good day, everyone 👋
-->
&lt;p>大家好 👋&lt;/p>
&lt;!--
Welcome to the first episode of the APAC edition of the "Meet Our Contributors" blog post series.
-->
&lt;p>欢迎来到亚太地区的“认识我们的贡献者”博文系列第一期。&lt;/p>
&lt;!--
In this post, we'll introduce you to five amazing folks from the India region who have been actively contributing to the upstream Kubernetes projects in a variety of ways, as well as being the leaders or maintainers of numerous community initiatives.
-->
&lt;p>在这篇文章中，我们将向你介绍来自印度地区的五位优秀贡献者，他们一直在以各种方式积极地为上游 Kubernetes 项目做贡献，同时也是众多社区倡议的领导者和维护者。&lt;/p>
&lt;!--
💫 *Let's get started, so without further ado…*
-->
&lt;p>💫 &lt;em>闲话少说，我们开始吧。&lt;/em>&lt;/p>
&lt;h2 id="arsh-sharma-https-github-com-rinkiyakedad">&lt;a href="https://github.com/RinkiyaKeDad">Arsh Sharma&lt;/a>&lt;/h2>
&lt;!--
Arsh is currently employed with Okteto as a Developer Experience engineer. As a new contributor, he realised that 1:1 mentorship opportunities were quite beneficial in getting him started with the upstream project.
-->
&lt;p>Arsh 目前在 Okteto 公司中担任开发者体验工程师职务。作为一名新的贡献者，他意识到一对一的指导机会让他在开始上游项目中受益匪浅。&lt;/p>
&lt;!--
He is presently a CI Signal shadow on the Kubernetes 1.23 release team. He is also contributing to the SIG Testing and SIG Docs projects, as well as to the [cert-manager](https://github.com/cert-manager/infrastructure) tools development work that is being done under the aegis of SIG Architecture.
-->
&lt;p>他目前是 Kubernetes 1.23 版本团队的 CI Signal 经理。他还致力于为 SIG Testing 和 SIG Docs 项目提供贡献，并且在 SIG Architecture 项目中负责 &lt;a href="https://github.com/cert-manager/infrastructure">证书管理器&lt;/a> 工具的开发工作。&lt;/p>
&lt;!--
To the newcomers, Arsh helps plan their early contributions sustainably.
-->
&lt;p>对于新人来说，Arsh 帮助他们可持续地计划早期贡献。&lt;/p>
&lt;!--
> _I would encourage folks to contribute in a way that's sustainable. What I mean by that
> is that it's easy to be very enthusiastic early on and take up more stuff than one can
> actually handle. This can often lead to burnout in later stages. It's much more sustainable
> to work on things iteratively._
-->
&lt;blockquote>
&lt;p>我鼓励大家以可持续的方式为社区做贡献。
我的意思是，一个人很容易在早期的时候非常有热情，并且承担了很多超出个人实际能力的事情。
这通常会导致后期的倦怠。迭代地处理事情会让大家对社区的贡献变得可持续。&lt;/p>
&lt;/blockquote>
&lt;h2 id="kunal-kushwaha-https-github-com-kunal-kushwaha">&lt;a href="https://github.com/kunal-kushwaha">Kunal Kushwaha&lt;/a>&lt;/h2>
&lt;!--
Kunal Kushwaha is a core member of the Kubernetes marketing council. He is also a CNCF ambassador and one of the founders of the [CNCF Students Program](https://community.cncf.io/cloud-native-students/).. He also served as a Communications role shadow during the 1.22 release cycle.
-->
&lt;p>Kunal Kushwaha 是 Kubernetes 营销委员会的核心成员。他同时也是 &lt;a href="https://community.cncf.io/cloud-native-students/">CNCF 学生计划&lt;/a> 的创始人之一。他还在 1.22 版本周期中担任通信经理一职。&lt;/p>
&lt;!--
At the end of his first year, Kunal began contributing to the [fabric8io kubernetes-client](https://github.com/fabric8io/kubernetes-client) project. He was then selected to work on the same project as part of Google Summer of Code. Kunal mentored people on the same project, first through Google Summer of Code then through Google Code-in.
-->
&lt;p>在他的第一年结束时，Kunal 开始为 &lt;a href="https://github.com/fabric8io/kubernetes-client">fabric8io kubernetes-client&lt;/a> 项目做贡献。然后，他被推选从事同一项目，此项目是 Google Summer of Code 的一部分。Kunal 在 Google Summer of Code、Google Code-in 等项目中指导过很多人。&lt;/p>
&lt;!--
As an open-source enthusiast, he believes that diverse participation in the community is beneficial since it introduces new perspectives and opinions and respect for one's peers. He has worked on various open-source projects, and his participation in communities has considerably assisted his development as a developer.
-->
&lt;p>作为一名开源爱好者，他坚信，社区的多元化参与是非常有益的，因为他引入了新的观念和观点，并尊重自己的伙伴。它曾参与过各种开源项目，他在这些社区中的参与对他作为开发者的发展有很大帮助。&lt;/p>
&lt;!--
> _I believe if you find yourself in a place where you do not know much about the
> project, that's a good thing because now you can learn while contributing and the
> community is there to help you. It has helped me a lot in gaining skills, meeting
> people from around the world and also helping them. You can learn on the go,
> you don't have to be an expert. Make sure to also check out no code contributions
> because being a beginner is a skill and you can bring new perspectives to the
> organisation._
-->
&lt;blockquote>
&lt;p>我相信，如果你发现自己在一个了解不多的项目当中，那是件好事，
因为现在你可以一边贡献一边学习，社区也会帮助你。
它帮助我获得了很多技能，认识了来自世界各地的人，也帮助了他们。
你可以在这个过程中学习，自己不一定必须是专家。
请重视非代码贡献，因为作为初学者这是一项技能，你可以为组织带来新的视角。&lt;/p>
&lt;/blockquote>
&lt;h2 id="madhav-jivarajani-https-github-com-madhavjivrajani">&lt;a href="https://github.com/MadhavJivrajani">Madhav Jivarajani&lt;/a>&lt;/h2>
&lt;!--
Madhav Jivarajani works on the VMware Upstream Kubernetes stability team. He began contributing to the Kubernetes project in January 2021 and has since made significant contributions to several areas of work under SIG Architecture, SIG API Machinery, and SIG ContribEx (contributor experience).
-->
&lt;p>Madhav Jivarajani 在 VMware 上游 Kubernetes 稳定性团队工作。他于 2021 年 1 月开始为 Kubernetes 项目做贡献，此后在 SIG Architecture、SIG API Machinery 和 SIG ContribEx（贡献者经验）等项目的几个工作领域做出了重大贡献。&lt;/p>
&lt;!--
Among several significant contributions are his recent efforts toward the Archival of [design proposals](https://github.com/kubernetes/community/issues/6055), refactoring the ["groups" codebase](https://github.com/kubernetes/k8s.io/pull/2713) under k8s-infra repository to make it mockable and testable, and improving the functionality of the [GitHub k8s bot](https://github.com/kubernetes/test-infra/issues/23129).
-->
&lt;p>在这几个重要项目中，他最近致力于&lt;a href="https://github.com/kubernetes/community/issues/6055">设计方案&lt;/a>的存档工作，
重构 k8s-infra 存储库下的 &lt;a href="https://github.com/kubernetes/k8s.io/pull/2713">&amp;quot;组&amp;quot;代码库&lt;/a>，
使其具有可模拟性和可测试性，以及改进 &lt;a href="https://github.com/kubernetes/test-infra/issues/23129">GitHub k8s 机器人&lt;/a>的功能。&lt;/p>
&lt;!--
In addition to his technical efforts, Madhav oversees many projects aimed at assisting new contributors. He organises bi-weekly "KEP reading club" sessions to help newcomers understand the process of adding new features, deprecating old ones, and making other key changes to the upstream project. He has also worked on developing [Katacoda scenarios](https://github.com/kubernetes-sigs/contributor-katacoda) to assist new contributors to become acquainted with the process of contributing to k/k. In addition to his current efforts to meet with community members every week, he has organised several [new contributors workshops (NCW)](https://www.youtube.com/watch?v=FgsXbHBRYIc).
-->
&lt;p>除了在技术方面的贡献，Madhav 还监督许多旨在帮助新贡献者的项目。
他每两周组织一次的 “KEP 阅读俱乐部” 会议，帮助新人了解添加新功能、
摒弃旧功能以及对上游项目进行其他关键更改的过程。他还致力于开发
&lt;a href="https://github.com/kubernetes-sigs/contributor-katacoda">Katacoda 场景&lt;/a>，
以帮助新的贡献者在为 k/k 做贡献的过程更加熟练。目前除了每周与社区成员会面外，
他还组织了几个&lt;a href="https://www.youtube.com/watch?v=FgsXbHBRYIc">新贡献者讲习班（NCW）&lt;/a>。&lt;/p>
&lt;!--
> _I initially did not know much about Kubernetes. I joined because the community was
> super friendly. But what made me stay was not just the people, but the project itself.
> My solution to not feeling overwhelmed in the community was to gain as much context
> and knowledge into the topics that I was interested in and were being discussed. And
> as a result I continued to dig deeper into Kubernetes and the design of it.
> I am a systems nut &amp; thus Kubernetes was an absolute goldmine for me._
-->
&lt;blockquote>
&lt;p>一开始我对 Kubernetes 了解并不多。我加入社区是因为社区超级友好。
但让我留下来的不仅仅是人，还有项目本身。我在社区中不会感到不知所措，
这是因为我能够在感兴趣的和正在讨论的主题中获得尽可能多的背景和知识。
因此，我将继续深入探讨 Kubernetes 及其设计。
我是一个系统迷，kubernetes 对我来说绝对是一个金矿。&lt;/p>
&lt;/blockquote>
&lt;h2 id="rajas-kakodkar-https-github-com-rajaskakodkar">&lt;a href="https://github.com/rajaskakodkar">Rajas Kakodkar&lt;/a>&lt;/h2>
&lt;!--
Rajas Kakodkar currently works at VMware as a Member of Technical Staff. He has been engaged in many aspects of the upstream Kubernetes project since 2019.
-->
&lt;p>Rajas Kakodkar 目前在 VMware 担任技术人员。自 2019 年以来，他一直多方面地从事上游 kubernetes 项目。&lt;/p>
&lt;!--
He is now a key contributor to the Testing special interest group. He is also active in the SIG Network community. Lately, Rajas has contributed significantly to the [NetworkPolicy++](https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/) and [`kpng`](https://github.com/kubernetes-sigs/kpng) sub-projects.
-->
&lt;p>他现在是 Testing 特别兴趣小组的关键贡献者。他还活跃在 SIG Network 社区。最近，Rajas 为 &lt;a href="https://docs.google.com/document/d/1AtWQy2fNa4qXRag9cCp5_HsefD7bxKe3ea2RPn8jnSs/">NetworkPolicy++&lt;/a> 和 &lt;a href="https://github.com/kubernetes-sigs/kpng">&lt;code>kpng&lt;/code>&lt;/a> 子项目做出了重大贡献。&lt;/p>
&lt;!--
One of the first challenges he ran across was that he was in a different time zone than the upstream project's regular meeting hours. However, async interactions on community forums progressively corrected that problem.
-->
&lt;p>他遇到的第一个挑战是，他所处的时区与上游项目的日常会议时间不同。不过，社区论坛上的异步交互逐渐解决了这个问题。&lt;/p>
&lt;!--
> _I enjoy contributing to Kubernetes not just because I get to work on
> cutting edge tech but more importantly because I get to work with
> awesome people and help in solving real world problems._
-->
&lt;blockquote>
&lt;p>我喜欢为 kubernetes 做出贡献，不仅因为我可以从事尖端技术工作，
更重要的是，我可以和优秀的人一起工作，并帮助解决现实问题。&lt;/p>
&lt;/blockquote>
&lt;h2 id="rajula-vineet-reddy-https-github-com-rajula96reddy">&lt;a href="https://github.com/rajula96reddy">Rajula Vineet Reddy&lt;/a>&lt;/h2>
&lt;!--
Rajula Vineet Reddy, a Junior Engineer at CERN, is a member of the Marketing Council team under SIG ContribEx . He also served as a release shadow for SIG Release during the 1.22 and 1.23 Kubernetes release cycles.
-->
&lt;p>Rajula Vineet Reddy，CERN 的初级工程师，是 SIG ContribEx 项目下营销委员会的成员。在 Kubernetes 1.22 和 1.23 版本周期中，他还担任 SIG Release 的版本经理。&lt;/p>
&lt;!--
He started looking at the Kubernetes project as part of a university project with the help of one of his professors. Over time, he spent a significant amount of time reading the project's documentation, Slack discussions, GitHub issues, and blogs, which helped him better grasp the Kubernetes project and piqued his interest in contributing upstream. One of his key contributions was his assistance with automation in the SIG ContribEx Upstream Marketing subproject.
-->
&lt;p>在他的一位教授的帮助下，他开始将 kubernetes 项目作为大学项目的一部分。慢慢地，他花费了大量的时间阅读项目的文档、Slack 讨论、GitHub issues 和博客，这有助于他更好地掌握 kubernetes 项目，并激发了他对上游项目做贡献的兴趣。他的主要贡献之一是他在SIG ContribEx上游营销子项目中协助实现了自动化。&lt;/p>
&lt;!--
According to Rajula, attending project meetings and shadowing various project roles are vital for learning about the community.
-->
&lt;p>Rajas 说，参与项目会议和跟踪各种项目角色对于了解社区至关重要。&lt;/p>
&lt;!--
> _I find the community very helpful and it's always_
> “you get back as much as you contribute”.
> _The more involved you are, the more you will understand, get to learn and
> contribute new things._
>
> _The first step to_ “come forward and start” _is hard. But it's all gonna be
> smooth after that. Just take that jump._
-->
&lt;blockquote>
&lt;p>我发现社区非常有帮助，而且总是“你得到的回报和你贡献的一样多”。
你参与得越多，你就越会了解、学习和贡献新东西。&lt;/p>
&lt;p>“挺身而出”的第一步是艰难的。但在那之后一切都会顺利的。勇敢地参与进来吧。&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;!--
If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. We're thrilled to have other folks assisting us in reaching out to even more wonderful individuals of the community. Your suggestions would be much appreciated.
-->
&lt;p>如果你对我们下一步应该采访谁有任何意见/建议，请在 #sig-contribex 中告知我们。我们很高兴有其他人帮助我们接触社区中更优秀的人。我们将不胜感激。&lt;/p>
&lt;!--
We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋
-->
&lt;p>我们下期见。最后，祝大家都能快乐地为社区做贡献！👋&lt;/p></description></item><item><title>Blog: Kubernetes 即将移除 Dockershim：承诺和下一步</title><link>https://kubernetes.io/zh-cn/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</link><pubDate>Fri, 07 Jan 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes is Moving on From Dockershim: Commitments and Next Steps"
date: 2022-01-07
slug: kubernetes-is-moving-on-from-dockershim
-->
&lt;!--
**Authors:** Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Sergey Kanzhelev (Google), Jim Angel (Google), Davanum Srinivas (VMware), Shannon Kularathna (Google), Chris Short (AWS), Dawn Chen (Google)&lt;/p>
&lt;!--
Kubernetes is removing dockershim in the upcoming v1.24 release. We're excited
to reaffirm our community values by supporting open source container runtimes,
enabling a smaller kubelet, and increasing engineering velocity for teams using
Kubernetes. If you [use Docker Engine as a container runtime](/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/)
for your Kubernetes cluster, get ready to migrate in 1.24! To check if you're
affected, refer to [Check whether dockershim removal affects you](/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/).
-->
&lt;p>Kubernetes 将在即将发布的 1.24 版本中移除 dockershim。我们很高兴能够通过支持开源容器运行时、支持更小的
kubelet 以及为使用 Kubernetes 的团队提高工程速度来重申我们的社区价值。
如果你&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">使用 Docker Engine 作为 Kubernetes 集群的容器运行时&lt;/a>，
请准备好在 1.24 中迁移！要检查你是否受到影响，
请参考&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">检查移除 Dockershim 对你的影响&lt;/a>。&lt;/p>
&lt;!--
## Why we’re moving away from dockershim
Docker was the first container runtime used by Kubernetes. This is one of the
reasons why Docker is so familiar to many Kubernetes users and enthusiasts.
Docker support was hardcoded into Kubernetes – a component the project refers to
as dockershim.
-->
&lt;h2 id="why-we-re-moving-away-from-dockershim">为什么我们要离开 dockershim &lt;/h2>
&lt;p>Docker 是 Kubernetes 使用的第一个容器运行时。
这也是许多 Kubernetes 用户和爱好者如此熟悉 Docker 的原因之一。
对 Docker 的支持被硬编码到 Kubernetes 中——一个被项目称为 dockershim 的组件。&lt;/p>
&lt;!--
As containerization became an industry standard, the Kubernetes project added support
for additional runtimes. This culminated in the implementation of the
container runtime interface (CRI), letting system components (like the kubelet)
talk to container runtimes in a standardized way. As a result, dockershim became
an anomaly in the Kubernetes project.
-->
&lt;p>随着容器化成为行业标准，Kubernetes 项目增加了对其他运行时的支持。
最终实现了容器运行时接口（CRI），让系统组件（如 kubelet）以标准化的方式与容器运行时通信。
因此，dockershim 成为了 Kubernetes 项目中的一个异常现象。&lt;/p>
&lt;!--
Dependencies on Docker and dockershim have crept into various tools
and projects in the CNCF ecosystem ecosystem, resulting in fragile code.
By removing the
dockershim CRI, we're embracing the first value of CNCF: "[Fast is better than
slow](https://github.com/cncf/foundation/blob/master/charter.md#3-values)".
Stay tuned for future communications on the topic!
-->
&lt;p>对 Docker 和 dockershim 的依赖已经渗透到 CNCF 生态系统中的各种工具和项目中，这导致了代码脆弱。&lt;/p>
&lt;p>通过删除 dockershim CRI，我们拥抱了 CNCF 的第一个价值：
“&lt;a href="https://github.com/cncf/foundation/blob/master/charter.md#3-values">快比慢好&lt;/a>”。
请继续关注未来关于这个话题的交流!&lt;/p>
&lt;!--
## Deprecation timeline
We [formally announced](/blog/2020/12/08/kubernetes-1-20-release-announcement/) the dockershim deprecation in December 2020. Full removal is targeted
in Kubernetes 1.24, in April 2022. This timeline
aligns with our [deprecation policy](/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior),
which states that deprecated behaviors must function for at least 1 year
after their announced deprecation.
-->
&lt;h2 id="deprecation-timeline">弃用时间线 &lt;/h2>
&lt;p>我们&lt;a href="https://kubernetes.io/zh-cn/blog/2020/12/08/kubernetes-1-20-release-announcement/">正式宣布&lt;/a>于
2020 年 12 月弃用 dockershim。目标是在 2022 年 4 月，
Kubernetes 1.24 中完全移除 dockershim。
此时间线与我们的&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior">弃用策略&lt;/a>一致，
即规定已弃用的行为必须在其宣布弃用后至少运行 1 年。&lt;/p>
&lt;!--
We'll support Kubernetes version 1.23, which includes
dockershim, for another year in the Kubernetes project. For managed
Kubernetes providers, vendor support is likely to last even longer, but this is
dependent on the companies themselves. Regardless, we're confident all cluster operations will have
time to migrate. If you have more questions about the dockershim removal, refer
to the [Dockershim Deprecation FAQ](/dockershim).
-->
&lt;p>包括 dockershim 的 Kubernetes 1.23 版本，在 Kubernetes 项目中将再支持一年。
对于托管 Kubernetes 的供应商，供应商支持可能会持续更长时间，但这取决于公司本身。
无论如何，我们相信所有集群操作都有时间进行迁移。如果你有更多关于 dockershim 移除的问题，
请参考&lt;a href="https://kubernetes.io/zh-cn/blog/2020/12/02/dockershim-faq/">弃用 Dockershim 的常见问题&lt;/a>。&lt;/p>
&lt;!--
We asked you whether you feel prepared for the migration from dockershim in this
survey: [Are you ready for Dockershim removal](/blog/2021/11/12/are-you-ready-for-dockershim-removal/).
We had over 600 responses. To everybody who took time filling out the survey,
thank you.
-->
&lt;p>在这个&lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">你是否为 dockershim 的删除做好了准备&lt;/a>的调查中，
我们询问你是否为 dockershim 的迁移做好了准备。我们收到了 600 多个回复。
感谢所有花时间填写调查问卷的人。&lt;/p>
&lt;!--
The results show that we still have a lot of ground to cover to help you to
migrate smoothly. Other container runtimes exist, and have been promoted
extensively. However, many users told us they still rely on dockershim,
and sometimes have dependencies that need to be re-worked. Some of these
dependencies are outside of your control. Based on your feedback, here are some
of the steps we are taking to help.
-->
&lt;p>结果表明，在帮助你顺利迁移方面，我们还有很多工作要做。
存在其他容器运行时，并且已被广泛推广。但是，许多用户告诉我们他们仍然依赖 dockershim，
并且有时需要重新处理依赖项。其中一些依赖项超出控制范围。
根据收集到的反馈，我们采取了一些措施提供帮助。&lt;/p>
&lt;!--
## Our next steps
Based on the feedback you provided:
- CNCF and the 1.24 release team are committed to delivering documentation in
time for the 1.24 release. This includes more informative blog posts like this
one, updating existing code samples, tutorials, and tasks, and producing a
migration guide for cluster operators.
- We are reaching out to the rest of the CNCF community to help prepare them for
this change.
-->
&lt;h2 id="our-next-steps">我们的下一个步骤&lt;/h2>
&lt;p>根据提供的反馈：&lt;/p>
&lt;ul>
&lt;li>CNCF 和 1.24 版本团队致力于及时交付 1.24 版本的文档。这包括像本文这样的包含更多信息的博客文章，
更新现有的代码示例、教程和任务，并为集群操作人员生成迁移指南。&lt;/li>
&lt;li>我们正在联系 CNCF 社区的其他成员，帮助他们为这一变化做好准备。&lt;/li>
&lt;/ul>
&lt;!--
If you're part of a project with dependencies on dockershim, or if you're
interested in helping with the migration effort, please join us! There's always
room for more contributors, whether to our transition tools or to our
documentation. To get started, say hello in the
[#sig-node](https://kubernetes.slack.com/archives/C0BP8PW9G)
channel on [Kubernetes Slack](https://slack.kubernetes.io/)!
-->
&lt;p>如果你是依赖 dockershim 的项目的一部分，或者如果你有兴趣帮助参与迁移工作，请加入我们！
无论是我们的迁移工具还是我们的文档，总是有更多贡献者的空间。
作为起步，请在 &lt;a href="https://slack.kubernetes.io/">Kubernetes Slack&lt;/a> 上的
&lt;a href="https://kubernetes.slack.com/archives/C0BP8PW9G">#sig-node&lt;/a> 频道打个招呼！&lt;/p>
&lt;!--
## Final thoughts
As a project, we've already seen cluster operators increasingly adopt other
container runtimes through 2021.
We believe there are no major blockers to migration. The steps we're taking to
improve the migration experience will light the path more clearly for you.
-->
&lt;h2 id="final-thoughts">最终想法 &lt;/h2>
&lt;p>作为一个项目，我们已经看到集群运营商在 2021 年之前越来越多地采用其他容器运行时。
我们相信迁移没有主要障碍。我们为改善迁移体验而采取的步骤将为你指明更清晰的道路。&lt;/p>
&lt;!--
We understand that migration from dockershim is yet another action you may need to
do to keep your Kubernetes infrastructure up to date. For most of you, this step
will be straightforward and transparent. In some cases, you will encounter
hiccups or issues. The community has discussed at length whether postponing the
dockershim removal would be helpful. For example, we recently talked about it in
the [SIG Node discussion on November 11th](https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid)
and in the [Kubernetes Steering committee meeting held on December 6th](https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx).
We already [postponed](https://github.com/kubernetes/enhancements/pull/2481/) it
once in 2021 because the adoption rate of other
runtimes was lower than we wanted, which also gave us more time to identify
potential blocking issues.
-->
&lt;p>我们知道，从 dockershim 迁移是你可能需要执行的另一项操作，以保证你的 Kubernetes 基础架构保持最新。
对于你们中的大多数人来说，这一步将是简单明了的。在某些情况下，你会遇到问题。
社区已经详细讨论了推迟 dockershim 删除是否会有所帮助。
例如，我们最近在 &lt;a href="https://docs.google.com/document/d/1Ne57gvidMEWXR70OxxnRkYquAoMpt56o75oZtg-OeBg/edit#bookmark=id.r77y11bgzid">11 月 11 日的 SIG Node 讨论&lt;/a>和
&lt;a href="https://docs.google.com/document/d/1qazwMIHGeF3iUh5xMJIJ6PDr-S3bNkT8tNLRkSiOkOU/edit#bookmark=id.m0ir406av7jx">12 月 6 日 Kubernetes Steering 举行的委员会会议&lt;/a>谈到了它。
我们已经在 2021 年&lt;a href="https://github.com/kubernetes/enhancements/pull/2481/">推迟&lt;/a>它一次，
因为其他运行时的采用率低于我们的预期，这也给了我们更多的时间来识别潜在的阻塞问题。&lt;/p>
&lt;!--
At this point, we believe that the value that you (and Kubernetes) gain from
dockershim removal makes up for the migration effort you'll have. Start planning
now to avoid surprises. We'll have more updates and guides before Kubernetes
1.24 is released.
-->
&lt;p>在这一点上，我们相信你（和 Kubernetes）从移除 dockershim 中获得的价值可以弥补你将要进行的迁移工作。
现在就开始计划以避免出现意外。在 Kubernetes 1.24 发布之前，我们将提供更多更新信息和指南。&lt;/p></description></item><item><title>Blog: Security Profiles Operator v0.4.0 中的新功能</title><link>https://kubernetes.io/zh-cn/blog/2021/12/17/security-profiles-operator/</link><pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/12/17/security-profiles-operator/</guid><description>
&lt;!--
layout: blog
title: "What's new in Security Profiles Operator v0.4.0"
date: 2021-12-17
slug: security-profiles-operator
-->
&lt;!--
**Authors:** Jakub Hrozek, Juan Antonio Osorio, Paulo Gomes, Sascha Grunert
-->
&lt;p>&lt;strong>作者:&lt;/strong> Jakub Hrozek, Juan Antonio Osorio, Paulo Gomes, Sascha Grunert&lt;/p>
&lt;hr>
&lt;!--
The [Security Profiles Operator (SPO)](https://sigs.k8s.io/security-profiles-operator)
is an out-of-tree Kubernetes enhancement to make the management of
[seccomp](https://en.wikipedia.org/wiki/Seccomp),
[SELinux](https://en.wikipedia.org/wiki/Security-Enhanced_Linux) and
[AppArmor](https://en.wikipedia.org/wiki/AppArmor) profiles easier and more
convenient. We're happy to announce that we recently [released
v0.4.0](https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.4.0)
of the operator, which contains a ton of new features, fixes and usability
improvements.
-->
&lt;p>&lt;a href="https://sigs.k8s.io/security-profiles-operator">Security Profiles Operator (SPO)&lt;/a>
是一种树外 Kubernetes 增强功能，用于更方便、更便捷地管理 &lt;a href="https://en.wikipedia.org/wiki/Seccomp">seccomp&lt;/a>、
&lt;a href="https://zh.wikipedia.org/wiki/%E5%AE%89%E5%85%A8%E5%A2%9E%E5%BC%BA%E5%BC%8FLinux">SELinux&lt;/a> 和
&lt;a href="https://zh.wikipedia.org/wiki/AppArmor">AppArmor&lt;/a> 配置文件。
我们很高兴地宣布，我们最近&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.4.0">发布了 v0.4.0&lt;/a>
的 Operator，其中包含了大量的新功能、缺陷修复和可用性改进。&lt;/p>
&lt;!--
## What's new
It has been a while since the last
[v0.3.0](https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.3.0)
release of the operator. We added new features, fine-tuned existing ones and
reworked our documentation in 290 commits over the past half year.
-->
&lt;h2 id="有哪些新特性">有哪些新特性&lt;/h2>
&lt;p>距离上次的 &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/releases/tag/v0.3.0">v0.3.0&lt;/a>
的发布已经有一段时间了。在过去的半年里，我们增加了新的功能，对现有的功能进行了微调，
并且在过去的半年里，我们通过 290 个提交重新编写了文档。&lt;/p>
&lt;!--
One of the highlights is that we're now able to record seccomp and SELinux
profiles using the operators [log enricher](https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#log-enricher-based-recording).
This allows us to reduce the dependencies required for profile recording to have
[auditd](https://linux.die.net/man/8/auditd) or
[syslog](https://en.wikipedia.org/wiki/Syslog) (as fallback) running on the
nodes. All profile recordings in the operator work in the same way by using the
`ProfileRecording` CRD as well as their corresponding [label
selectors](/docs/concepts/overview/working-with-objects/labels). The log
enricher itself can be also used to gather meaningful insights about seccomp and
SELinux messages of a node. Checkout the [official
documentation](https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-the-log-enricher)
to learn more about it.
-->
&lt;p>亮点之一是我们现在能够使用 Operator 的&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#log-enricher-based-recording">日志增强组件&lt;/a>
记录 seccomp 和 SELinux 的配置文件。
这使我们能够减少配置文件记录所需的依赖事项，使得仅剩的依赖变为在节点上运行
&lt;a href="https://linux.die.net/man/8/auditd">auditd&lt;/a> 或 &lt;a href="https://en.wikipedia.org/wiki/Syslog">syslog&lt;/a>（作为一种回退机制）。
通过使用 &lt;code>ProfileRecording&lt;/code> CRD 及其对应的&lt;a href="https://kubernetes.io/zh-cn/concepts/overview/working-with-objects/labels">标签选择算符&lt;/a>，
Operator 中的所有配置文件记录都以相同的方式工作。
日志增强组件本身也可用于获得有关节点上的 seccomp 和 SELinux 消息的有意义的洞察。
查看&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-the-log-enricher">官方文档&lt;/a>
了解更多信息。&lt;/p>
&lt;!--
### seccomp related improvements
Beside the log enricher based recording we now offer an alternative to record
seccomp profiles by utilizing [ebpf](https://ebpf.io). This optional feature can
be enabled by setting `enableBpfRecorder` to `true`. This results in running a
dedicated container, which ships a custom bpf module on every node to collect
the syscalls for containers. It even supports older Kernel versions which do not
expose the [BPF Type Format (BTF)](https://www.kernel.org/doc/html/latest/bpf/btf.html) per
default as well as the `amd64` and `arm64` architectures. Checkout
[our documentation](https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#ebpf-based-recording)
to see it in action. By the way, we now add the seccomp profile architecture of
the recorder host to the recorded profile as well.
-->
&lt;h3 id="与-seccomp-有关的改进">与 seccomp 有关的改进&lt;/h3>
&lt;p>除了基于日志丰富器的记录之外，我们现在还使用 &lt;a href="https://ebpf.io">ebpf&lt;/a>
作为记录 seccomp 配置文件的一种替代方法。可以通过将 &lt;code>enableBpfRecorder&lt;/code> 设置为 &lt;code>true&lt;/code> 来启用此可选功能。
启用之后会导致一个专用的容器被启动运行；该容器在每个节点上提供一个自定义 bpf 模块以收集容器的系统调用。
它甚至支持默认不公开 &lt;a href="https://www.kernel.org/doc/html/latest/bpf/btf.html">BPF 类型格式 (BTF)&lt;/a>
的旧内核版本以及 &lt;code>amd64 &lt;/code> 和 &lt;code>arm64&lt;/code> 架构。查看 &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#ebpf-based-recording">我们的文档&lt;/a>
以查看它的实际效果。顺便说一句，我们现在也将记录器主机的 seccomp 配置文件体系结构添加到记录的配置文件中。&lt;/p>
&lt;!--
We also graduated the seccomp profile API from `v1alpha1` to `v1beta1`. This
aligns with our overall goal to stabilize the CRD APIs over time. The only thing
which has changed is that the seccomp profile type `Architectures` now points to
`[]Arch` instead of `[]*Arch`.
-->
&lt;p>我们还将 seccomp 配置文件 API 从 &lt;code>v1alpha1&lt;/code> 升级到 &lt;code>v1beta1&lt;/code>。
这符合我们随着时间的推移稳定 CRD API 的总体目标。
唯一改变的是 seccomp 配置文件类型 &lt;code>Architectures&lt;/code> 现在指向 &lt;code>[]Arch&lt;/code> 而不是 &lt;code>[]*Arch&lt;/code>。&lt;/p>
&lt;!--
### SELinux enhancements
Managing SELinux policies (an equivalent to using `semodule` that
you would normally call on a single server) is not done by SPO
itself, but by another container called selinuxd to provide better
isolation. This release switched to using selinuxd containers from
a personal repository to images located under [our team's quay.io
repository](https://quay.io/organization/security-profiles-operator).
The selinuxd repository has moved as well to [the containers GitHub
organization](https://github.com/containers/selinuxd).
-->
&lt;h3 id="selinux-增强功能">SELinux 增强功能&lt;/h3>
&lt;p>管理 SELinux 策略（相当于使用通常在单个服务器上调用的 &lt;code>semodule&lt;/code> ）不是由 SPO 本身完成的，
而是由另一个名为 selinuxd 的容器完成，以提供更好的隔离。此版本将所使用的 selinuxd
容器镜像从个人仓库迁移到位于&lt;a href="https://quay.io/organization/security-profiles-operator">我们团队的 quay.io 仓库&lt;/a>下的镜像。
selinuxd 仓库也已移至&lt;a href="https://github.com/containers/selinuxd">GitHub 组织 containers&lt;/a>。&lt;/p>
&lt;!--
Please note that selinuxd links dynamically to `libsemanage` and mounts the
SELinux directories from the nodes, which means that the selinuxd container
must be running the same distribution as the cluster nodes. SPO defaults
to using CentOS-8 based containers, but we also build Fedora based ones.
If you are using another distribution and would like us to add support for
it, please file [an issue against selinuxd](https://github.com/containers/selinuxd/issues).
-->
&lt;p>请注意，selinuxd 动态链接到 libsemanage 并挂载节点上的 SELinux 目录，
这意味着 selinuxd 容器必须与集群节点运行相同的发行版。SPO 默认使用基于 CentOS-8 的容器，
但我们也构建基于 Fedora 的容器。如果你使用其他发行版并希望我们添加对它的支持，
请&lt;a href="https://github.com/containers/selinuxd/issues">针对 selinuxd 提交 issue&lt;/a>。&lt;/p>
&lt;!--
#### Profile Recording
This release adds support for recording of SELinux profiles.
The recording itself is managed via an instance of a `ProfileRecording` Custom
Resource as seen in an
[example](https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/profilerecording-selinux-logs.yaml)
in our repository. From the user's point of view it works pretty much the same
as recording of seccomp profiles.
-->
&lt;h4 id="配置文件记录">配置文件记录&lt;/h4>
&lt;p>此版本（0.4.0）增加了记录 SELinux 配置文件的支持。记录本身是通过 &lt;code>ProfileRecording&lt;/code> 自定义资源的实例管理的，
如我们仓库中的&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/profilerecording-selinux-logs.yaml">示例&lt;/a>
所示。从用户的角度来看，它的工作原理与记录 seccomp 配置文件几乎相同。&lt;/p>
&lt;!--
Under the hood, to know what the workload is doing SPO installs a special
permissive policy called [selinuxrecording](https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/profiles/selinuxrecording.cil)
on startup which allows everything and logs all AVCs to `audit.log`.
These AVC messages are scraped by the log enricher component and when
the recorded workload exits, the policy is created.
-->
&lt;p>在后台，为了知道工作负载在做什么，SPO 安装了一个名为 &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/profiles/selinuxrecording.cil">selinuxrecording&lt;/a>
的、限制宽松的策略，允许执行所有操作并将所有 AVC 记录到 &lt;code>audit.log&lt;/code> 中。
这些 AVC 消息由日志增强组件抓取，当所记录的工作负载退出时，该策略被创建。&lt;/p>
&lt;!--
#### `SELinuxProfile` CRD graduation
An `v1alpha2` version of the `SelinuxProfile` object has been introduced. This
removes the raw Common Intermediate Language (CIL) from the object itself and
instead adds a simple policy language to ease the writing and parsing
experience.
-->
&lt;h4 id="selinuxprofile-crd-毕业">&lt;code>SELinuxProfile&lt;/code> CRD 毕业&lt;/h4>
&lt;p>我们引入了 &lt;code>SelinuxProfile&lt;/code> 对象的 &lt;code>v1alpha2&lt;/code> 版本。
这个版本从对象中删除了原始的通用中间语言 (CIL)，并添加了一种简单的策略语言来简化编写和解析体验。&lt;/p>
&lt;!--
Alongside, a `RawSelinuxProfile` object was also introduced. This contains a
wrapped and raw representation of the policy. This was intended for folks to be
able to take their existing policies into use as soon as possible. However, on
validations are done here.
-->
&lt;p>此外，我们还引入了 &lt;code>RawSelinuxProfile&lt;/code> 对象。该对象包含策略的包装和原始表示。
引入此对象是为了让人们能够尽快将他们现有的策略付诸实现。但是，策略的验证是在这里完成的。&lt;/p>
&lt;!--
### AppArmor support
This version introduces the initial support for AppArmor, allowing users to load and
unload AppArmor profiles into cluster nodes by using the new [AppArmorProfile](https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/crds/apparmorprofile.yaml) CRD.
-->
&lt;h3 id="apparmor-支持">AppArmor 支持&lt;/h3>
&lt;p>0.4.0 版本引入了对 AppArmor 的初始支持，允许用户通过使用新的
&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/deploy/base/crds/apparmorprofile.yaml">AppArmorProfile&lt;/a>
在集群节点中 CRD 加载或卸载 AppArmor 配置文件。&lt;/p>
&lt;!--
To enable AppArmor support use the [enableAppArmor feature gate](https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/config.yaml#L10) switch of your SPO configuration.
Then use our [apparmor example](https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/apparmorprofile.yaml) to deploy your first profile across your cluster.
-->
&lt;p>要启用 AppArmor 支持，请使用 SPO 配置的 &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/config.yaml#L10">enableAppArmor 特性门控&lt;/a>开关。
然后使用我们的 &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/examples/apparmorprofile.yaml">apparmor 示例&lt;/a> 在集群中部署你第一个配置文件。&lt;/p>
&lt;!--
### Metrics
The operator now exposes metrics, which are described in detail in
our new [metrics documentation](https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-metrics).
We decided to secure the metrics retrieval process by using
[kube-rbac-proxy](https://github.com/brancz/kube-rbac-proxy), while we ship an
additional `spo-metrics-client` cluster role (and binding) to retrieve the
metrics from within the cluster. If you're using
[OpenShift](https://www.redhat.com/en/technologies/cloud-computing/openshift),
then we provide an out of the box working
[`ServiceMonitor`](https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#automatic-servicemonitor-deployment)
to access the metrics.
-->
&lt;h3 id="指标">指标&lt;/h3>
&lt;p>Operator 现在能够公开在我们的新&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#using-metrics">指标文档&lt;/a>中详细描述的指标。
我们决定使用 &lt;a href="https://github.com/brancz/kube-rbac-proxy">kube-rbac-proxy&lt;/a> 来保护指标检索过程，
同时我们提供了一个额外的 &lt;code>spo-metrics-client&lt;/code> 集群角色（和绑定）以从集群内检索指标。
如果你使用 &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift">OpenShift&lt;/a>，
那么我们提供了一个开箱即用的 &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#automatic-servicemonitor-deployment">&lt;code>ServiceMonitor&lt;/code>&lt;/a>
来访问指标。&lt;/p>
&lt;!--
#### Debuggability and robustness
Beside all those new features, we decided to restructure parts of the Security
Profiles Operator internally to make it better to debug and more robust. For
example, we now maintain an internal [gRPC](https://grpc.io) API to communicate
within the operator across different features. We also improved the performance
of the log enricher, which now caches results for faster retrieval of the log
data. The operator can be put into a more [verbose log mode](https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#set-logging-verbosity)
by setting `verbosity` from `0` to `1`.
-->
&lt;h4 id="可调试性和稳健性">可调试性和稳健性&lt;/h4>
&lt;p>除了所有这些新功能外，我们还决定在内部重组安全配置文件操作程序的部分内容，使其更易于调试和更稳健。
例如，我们现在维护了一个内部 &lt;a href="https://grpc.io">gRPC&lt;/a> API，以便在 Operator 内部跨不同功能组件进行通信。
我们还提高了日志增强组件的性能，现在它可以缓存结果，以便更快地检索日志数据。
Operator 可以通过将 &lt;code>verbosity&lt;/code> 设置从 &lt;code>0&lt;/code> 改为 &lt;code>1&lt;/code>，启用更详细的日志模式(&lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#set-logging-verbosity">https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#set-logging-verbosity&lt;/a>)。&lt;/p>
&lt;!--
We also print the used `libseccomp` and `libbpf` versions on startup, as well as
expose CPU and memory profiling endpoints for each container via the
[`enableProfiling` option](https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#enable-cpu-and-memory-profiling).
Dedicated liveness and startup probes inside of the operator daemon will now
additionally improve the life cycle of the operator.
-->
&lt;p>我们还在启动时打印所使用的 &lt;code>libseccomp&lt;/code> 和 &lt;code>libbpf&lt;/code> 版本，
并通过 &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator/blob/71b3915/installation-usage.md#enable-cpu-and-memory-profiling">&lt;code>enableProfiling&lt;/code> 选项&lt;/a>
公开每个容器的 CPU 和内存性能分析端点。
Operator 守护程序内部的专用的存活态探测和启动探测现在能进一步改善 Operator 的生命周期管理。&lt;/p>
&lt;!--
## Conclusion
Thank you for reading this update. We're looking forward to future enhancements
of the operator and would love to get your feedback about the latest release.
Feel free to reach out to us via the Kubernetes slack
[#security-profiles-operator](https://kubernetes.slack.com/messages/security-profiles-operator)
for any feedback or question.
-->
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>感谢你阅读这次更新。我们期待着 Operater 的未来改进，并希望得到你对最新版本的反馈。
欢迎通过 Kubernetes slack &lt;a href="https://kubernetes.slack.com/messages/security-profiles-operator">#security-profiles-operator&lt;/a>
与我们联系，提出任何反馈或问题。&lt;/p></description></item><item><title>Blog: Kubernetes 1.23: StatefulSet PVC 自动删除 (alpha)</title><link>https://kubernetes.io/zh-cn/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/</link><pubDate>Thu, 16 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/12/16/kubernetes-1-23-statefulset-pvc-auto-deletion/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.23: StatefulSet PVC Auto-Deletion (alpha)'
date: 2021-12-16
slug: kubernetes-1-23-statefulset-pvc-auto-deletion
-->
&lt;!--
**Author:** Matthew Cary (Google)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Matthew Cary (谷歌)&lt;/p>
&lt;!--
Kubernetes v1.23 introduced a new, alpha-level policy for
[StatefulSets](/docs/concepts/workloads/controllers/statefulset/) that controls the lifetime of
[PersistentVolumeClaims](/docs/concepts/storage/persistent-volumes/) (PVCs) generated from the
StatefulSet spec template for cases when they should be deleted automatically when the StatefulSet
is deleted or pods in the StatefulSet are scaled down.
-->
&lt;p>Kubernetes v1.23 为 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/">StatefulSets&lt;/a>
引入了一个新的 alpha 级策略，用来控制由 StatefulSet 规约模板生成的
&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/storage/persistent-volumes/">PersistentVolumeClaims&lt;/a> (PVCs) 的生命周期，
用于当删除 StatefulSet 或减少 StatefulSet 中的 Pods 数量时 PVCs 应该被自动删除的场景。&lt;/p>
&lt;!--
## What problem does this solve?
-->
&lt;h2 id="它解决了什么问题">它解决了什么问题？&lt;/h2>
&lt;!--
A StatefulSet spec can include Pod and PVC templates. When a replica is first created, the
Kubernetes control plane creates a PVC for that replica if one does not already exist. The behavior
before Kubernetes v1.23 was that the control plane never cleaned up the PVCs created for
StatefulSets - this was left up to the cluster administrator, or to some add-on automation that
you’d have to find, check suitability, and deploy. The common pattern for managing PVCs, either
manually or through tools such as Helm, is that the PVCs are tracked by the tool that manages them,
with explicit lifecycle. Workflows that use StatefulSets must determine on their own what PVCs are
created by a StatefulSet and what their lifecycle should be.
-->
&lt;p>StatefulSet 规约中可以包含 Pod 和 PVC 的模板。当副本先被创建时，如果 PVC 还不存在，
Kubernetes 控制面会为该副本自动创建一个 PVC。在 Kubernetes 1.23 版本之前，
控制面不会删除 StatefulSet 创建的 PVCs——这依赖集群管理员或你需要部署一些额外的适用的自动化工具来处理。
管理 PVC 的常见模式是通过手动或使用 Helm 等工具，PVC 的具体生命周期由管理它的工具跟踪。
使用 StatefulSet 时必须自行确定 StatefulSet 创建哪些 PVC，以及它们的生命周期应该是什么。&lt;/p>
&lt;!--
Before this new feature, when a StatefulSet-managed replica disappears, either because the
StatefulSet is reducing its replica count, or because its StatefulSet is deleted, the PVC and its
backing volume remains and must be manually deleted. While this behavior is appropriate when the
data is critical, in many cases the persistent data in these PVCs is either temporary, or can be
reconstructed from another source. In those cases, PVCs and their backing volumes remaining after
their StatefulSet or replicas have been deleted are not necessary, incur cost, and require manual
cleanup.
-->
&lt;p>在这个新特性之前，当一个 StatefulSet 管理的副本消失时，无论是因为 StatefulSet 减少了它的副本数，
还是因为它的 StatefulSet 被删除了，PVC 及其下层的卷仍然存在，需要手动删除。
当存储数据比较重要时，这样做是合理的，但在许多情况下，这些 PVC 中的持久化数据要么是临时的，
要么可以从另一个源端重建。在这些情况下，删除 StatefulSet 或减少副本后留下的 PVC 及其下层的卷是不必要的，
还会产生成本，需要手动清理。&lt;/p>
&lt;!--
## The new StatefulSet PVC retention policy
-->
&lt;h2 id="新的-statefulset-pvc-保留策略">新的 StatefulSet PVC 保留策略&lt;/h2>
&lt;!--
If you enable the alpha feature, a StatefulSet spec includes a PersistentVolumeClaim retention
policy. This is used to control if and when PVCs created from a StatefulSet’s `volumeClaimTemplate`
are deleted. This first iteration of the retention policy contains two situations where PVCs may be
deleted.
-->
&lt;p>如果你启用这个新 alpha 特性，StatefulSet 规约中就可以包含 PersistentVolumeClaim 的保留策略。
该策略用于控制是否以及何时删除基于 StatefulSet 的 &lt;code>volumeClaimTemplate&lt;/code> 属性所创建的 PVCs。
保留策略的首次迭代包含两种可能删除 PVC 的情况。&lt;/p>
&lt;!--
The first situation is when the StatefulSet resource is deleted (which implies that all replicas are
also deleted). This is controlled by the `whenDeleted` policy. The second situation, controlled by
`whenScaled` is when the StatefulSet is scaled down, which removes some but not all of the replicas
in a StatefulSet. In both cases the policy can either be `Retain`, where the corresponding PVCs are
not touched, or `Delete`, which means that PVCs are deleted. The deletion is done with a normal
[object deletion](/docs/concepts/architecture/garbage-collection/), so that, for example, all
retention policies for the underlying PV are respected.
-->
&lt;p>第一种情况是 StatefulSet 资源被删除时（这意味着所有副本也被删除），这由 &lt;code>whenDeleted&lt;/code> 策略控制的。
第二种情况是 StatefulSet 缩小时，即删除 StatefulSet 部分副本，这由 &lt;code>whenScaled&lt;/code> 策略控制。
在这两种情况下，策略即可以是 &lt;code>Retain&lt;/code> 不涉及相应 PVCs 的改变，也可以是 &lt;code>Delete&lt;/code> 即删除对应的 PVCs。
删除是通过普通的&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/architecture/garbage-collection/">对象删除&lt;/a>完成的，
因此，的所有保留策略都会被遵照执行。&lt;/p>
&lt;!--
This policy forms a matrix with four cases. I’ll walk through and give an example for each one.
-->
&lt;p>该策略形成包含四种情况的矩阵。我将逐一介绍，并为每一种情况给出一个例子。&lt;/p>
&lt;!--
* **`whenDeleted` and `whenScaled` are both `Retain`.** This matches the existing behavior for
StatefulSets, where no PVCs are deleted. This is also the default retention policy. It’s
appropriate to use when data on StatefulSet volumes may be irreplaceable and should only be
deleted manually.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> 和 &lt;code>whenScaled&lt;/code> 都是 &lt;code>Retain&lt;/code>。&lt;/strong> 这与 StatefulSets 的现有行为一致，
即不删除 PVCs。 这也是默认的保留策略。它适用于 StatefulSet
卷上的数据是不可替代的且只能手动删除的情况。&lt;/li>
&lt;/ul>
&lt;!--
* **`whenDeleted` is `Delete` and `whenScaled` is `Retain`.** In this case, PVCs are deleted only when
the entire StatefulSet is deleted. If the StatefulSet is scaled down, PVCs are not touched,
meaning they are available to be reattached if a scale-up occurs with any data from the previous
replica. This might be used for a temporary StatefulSet, such as in a CI instance or ETL
pipeline, where the data on the StatefulSet is needed only during the lifetime of the
StatefulSet lifetime, but while the task is running the data is not easily reconstructible. Any
retained state is needed for any replicas that scale down and then up.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> 是 &lt;code>Delete&lt;/code> 但 &lt;code>whenScaled&lt;/code> 是 &lt;code>Retain&lt;/code>。&lt;/strong> 在这种情况下，
只有当整个 StatefulSet 被删除时，PVCs 才会被删除。
如果减少 StatefulSet 副本，PVCs 不会删除，这意味着如果增加副本时，可以从前一个副本重新连接所有数据。
这可能用于临时的 StatefulSet，例如在 CI 实例或 ETL 管道中，
StatefulSet 上的数据仅在 StatefulSet 生命周期内才需要，但在任务运行时数据不易重构。
任何保留状态对于所有先缩小后扩大的副本都是必需的。&lt;/li>
&lt;/ul>
&lt;!--
* **`whenDeleted` and `whenScaled` are both `Delete`.** PVCs are deleted immediately when their
replica is no longer needed. Note this does not include when a Pod is deleted and a new version
rescheduled, for example when a node is drained and Pods need to migrate elsewhere. The PVC is
deleted only when the replica is no longer needed as signified by a scale-down or StatefulSet
deletion. This use case is for when data does not need to live beyond the life of its
replica. Perhaps the data is easily reconstructable and the cost savings of deleting unused PVCs
is more important than quick scale-up, or perhaps that when a new replica is created, any data
from a previous replica is not usable and must be reconstructed anyway.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> 和 &lt;code>whenScaled&lt;/code> 都是 &lt;code>Delete&lt;/code>。&lt;/strong> 当其副本不再被需要时，PVCs 会立即被删除。
注意，这并不包括 Pod 被删除且有新版本被调度的情况，例如当节点被腾空而 Pod 需要迁移到别处时。
只有当副本不再被需要时，如按比例缩小或删除 StatefulSet 时，才会删除 PVC。
此策略适用于数据生命周期短于副本生命周期的情况。即数据很容易重构，
且删除未使用的 PVC 所节省的成本比快速增加副本更重要，或者当创建一个新的副本时，
来自以前副本的任何数据都不可用，必须重新构建。&lt;/li>
&lt;/ul>
&lt;!--
* **`whenDeleted` is `Retain` and `whenScaled` is `Delete`.** This is similar to the previous case,
when there is little benefit to keeping PVCs for fast reuse during scale-up. An example of a
situation where you might use this is an Elasticsearch cluster. Typically you would scale that
workload up and down to match demand, whilst ensuring a minimum number of replicas (for example:
3). When scaling down, data is migrated away from removed replicas and there is no benefit to
retaining those PVCs. However, it can be useful to bring the entire Elasticsearch cluster down
temporarily for maintenance. If you need to take the Elasticsearch system offline, you can do
this by temporarily deleting the StatefulSet, and then bringing the Elasticsearch cluster back
by recreating the StatefulSet. The PVCs holding the Elasticsearch data will still exist and the
new replicas will automatically use them.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;code>whenDeleted&lt;/code> 是 &lt;code>Retain&lt;/code> 但 &lt;code>whenScaled&lt;/code> 是 &lt;code>Delete&lt;/code>。&lt;/strong> 这与前一种情况类似，
在增加副本时用保留的 PVCs 快速重构几乎没有什么益处。例如 Elasticsearch 集群就是使用的这种方式。
通常，你需要增大或缩小工作负载来满足业务诉求，同时确保最小数量的副本（例如：3）。
当减少副本时，数据将从已删除的副本迁移出去，保留这些 PVCs 没有任何用处。
但是，这对临时关闭整个 Elasticsearch 集群进行维护时是很有用的。
如果需要使 Elasticsearch 系统脱机，可以通过临时删除 StatefulSet 来实现，
然后通过重新创建 StatefulSet 来恢复 Elasticsearch 集群。
保存 Elasticsearch 数据的 PVCs 不会被删除，新的副本将自动使用它们。&lt;/li>
&lt;/ul>
&lt;!--
Visit the
[documentation](/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies) to
see all the details.
-->
&lt;p>查阅&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-policies">文档&lt;/a>
获取更多详细信息。&lt;/p>
&lt;!--
## What’s next?
-->
&lt;h2 id="下一步是什么">下一步是什么？&lt;/h2>
&lt;!--
Enable the feature and try it out! Enable the `StatefulSetAutoDeletePVC` feature gate on a cluster,
then create a StatefulSet using the new policy. Test it out and tell us what you think!
-->
&lt;p>启用该功能并尝试一下！在集群上启用 &lt;code>StatefulSetAutoDeletePVC&lt;/code> 功能，然后使用新策略创建 StatefulSet。
测试一下，告诉我们你的体验！&lt;/p>
&lt;!--
I'm very curious to see if this owner reference mechanism works well in practice. For example, we
realized there is no mechanism in Kubernetes for knowing who set a reference, so it’s possible that
the StatefulSet controller may fight with custom controllers that set their own
references. Fortunately, maintaining the existing retention behavior does not involve any new owner
references, so default behavior will be compatible.
-->
&lt;p>我很好奇这个属主引用机制在实践中是否有效。例如，我们意识到 Kubernetes 中没有可以知道谁设置了引用的机制，
因此 StatefulSet 控制器可能会与设置自己的引用的自定义控制器发生冲突。
幸运的是，维护现有的保留行为不涉及任何新属主引用，因此默认行为是兼容的。&lt;/p>
&lt;!--
Please tag any issues you report with the label `sig/apps` and assign them to Matthew Cary
([@mattcary](https://github.com/mattcary) at GitHub).
-->
&lt;p>请用标签 &lt;code>sig/apps&lt;/code> 标记你报告的任何问题，并将它们分配给 Matthew Cary
(在 GitHub上 &lt;a href="https://github.com/mattcary">@mattcary&lt;/a>)。&lt;/p>
&lt;!--
Enjoy!
-->
&lt;p>尽情体验吧！&lt;/p></description></item><item><title>Blog: Kubernetes 1.23：树内存储向 CSI 卷迁移工作的进展更新</title><link>https://kubernetes.io/zh-cn/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/</guid><description>
&lt;!---
layout: blog
title: "Kubernetes 1.23: Kubernetes In-Tree to CSI Volume Migration Status Update"
date: 2021-12-10
slug: storage-in-tree-to-csi-migration-status-update
-->
&lt;!---
**Author:** Jiawei Wang (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Jiawei Wang（谷歌）&lt;/p>
&lt;!---
The Kubernetes in-tree storage plugin to [Container Storage Interface (CSI)](/blog/2019/01/15/container-storage-interface-ga/) migration infrastructure has already been [beta](/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/) since v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
-->
&lt;p>自 Kubernetes v1.14 引入容器存储接口（&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface, CSI&lt;/a>）的工作达到 alpha 阶段后，自 v1.17 起，Kubernetes 树内存储插件（in-tree storage plugin）向 CSI 的迁移基础设施已步入 &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">beta 阶段&lt;/a>。&lt;/p>
&lt;!---
Since then, SIG Storage and other Kubernetes special interest groups are working to ensure feature stability and compatibility in preparation for GA.
This article is intended to give a status update to the feature as well as changes between Kubernetes 1.17 and 1.23. In addition, I will also cover the future roadmap for the CSI migration feature GA for each storage plugin.
-->
&lt;p>自那时起，Kubernetes 存储特别兴趣组（special interest groups, SIG）及其他 Kubernetes 特别兴趣组就在努力确保这一功能的稳定性和兼容性，为正式发布做准备。
本文旨在介绍该功能的最新开发进展，以及 Kubernetes v1.17 到 v1.23 之间的变化。此外，我还将介绍每个存储插件的 CSI 迁移功能达到正式发布阶段的未来路线图。&lt;/p>
&lt;!---
## Quick recap: What is CSI Migration, and why migrate?
The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms - especially vendor specific plugins.
Kubernetes support for the [Container Storage Interface](https://github.com/container-storage-interface/spec/blob/master/spec.md#README) has been
[generally available](/blog/2019/01/15/container-storage-interface-ga/) since Kubernetes v1.13.
Support for using CSI drivers was introduced to make it easier to add and maintain new integrations between Kubernetes and storage backend technologies. Using CSI drivers allows for for better maintainability (driver authors can define their own release cycle and support lifecycle) and reduce the opportunity for vulnerabilities (with less in-tree code, the risks of a mistake are reduced, and cluster operators can select only the storage drivers that their cluster requires).
-->
&lt;h2 id="quick-recap-what-is-csi-migration-and-why-migrate">快速回顾：CSI 迁移功能是什么？为什么要迁移？ &lt;/h2>
&lt;p>容器存储接口旨在帮助 Kubernetes 取代其现有的树内存储驱动机制──特别是供应商的特定插件。自 v1.13 起，Kubernetes 对&lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#README">容器存储接口&lt;/a>的支持工作已达到&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">正式发布阶段&lt;/a>。引入对 CSI 驱动的支持，将使得 Kubernetes 和存储后端技术之间的集成工作更易建立和维护。使用 CSI 驱动可以实现更好的可维护性（驱动作者可以决定自己的发布周期和支持生命周期）、减少出现漏洞的机会（得益于更少的树内代码，出现错误的风险会降低。另外，集群操作员可以只选择集群需要的存储驱动）。&lt;/p>
&lt;!---
As more CSI Drivers were created and became production ready, SIG Storage group wanted all Kubernetes users to benefit from the CSI model. However, we cannot break API compatibility with the existing storage API types. The solution we came up with was CSI migration: a feature that translates in-tree APIs to equivalent CSI APIs and delegates operations to a replacement CSI driver.
-->
&lt;p>随着更多的 CSI 驱动诞生并进入生产就绪阶段，Kubernetes 存储特别兴趣组希望所有 Kubernetes 用户都能从 CSI 模型中受益──然而，我们不应破坏与现有存储 API 类型的 API 兼容性。对此，我们给出的解决方案是 CSI 迁移：该功能实现将树内存储 API 翻译成等效的 CSI API，并把操作委托给一个替换的 CSI 驱动来完成。&lt;/p>
&lt;!---
The CSI migration effort enables the replacement of existing in-tree storage plugins such as `kubernetes.io/gce-pd` or `kubernetes.io/aws-ebs` with a corresponding [CSI driver](https://kubernetes-csi.github.io/docs/introduction.html) from the storage backend.
If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. Existing `StorageClass`, `PersistentVolume` and `PersistentVolumeClaim` objects should continue to work.
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing workloads that utilize PVCs which are backed by in-tree storage plugins will continue to function as they always have.
However, behind the scenes, Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.
-->
&lt;p>CSI 迁移工作使存储后端现有的树内存储插件（如 &lt;code>kubernetes.io/gce-pd&lt;/code> 或 &lt;code>kubernetes.io/aws-ebs&lt;/code>）能够被相应的 &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html">CSI 驱动&lt;/a> 所取代。如果 CSI 迁移功能正确发挥作用，Kubernetes 终端用户应该不会注意到有什么变化。现有的 &lt;code>StorageClass&lt;/code>、&lt;code>PersistentVolume&lt;/code> 和 &lt;code>PersistentVolumeClaim&lt;/code> 对象应继续工作。当 Kubernetes 集群管理员更新集群以启用 CSI 迁移功能时，利用到 PVCs&lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>（由树内存储插件支持）的现有工作负载将继续像以前一样运作──不过在幕后，Kubernetes 将所有存储管理操作（以前面向树内存储驱动的）交给 CSI 驱动控制。&lt;/p>
&lt;!---
For example, suppose you are a `kubernetes.io/gce-pd` user, after CSI migration, you can still use `kubernetes.io/gce-pd` to provision new volumes, mount existing GCE-PD volumes or delete existing volumes. All existing API/Interface will still function correctly. However, the underlying function calls are all going through the [GCE PD CSI driver](https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver) instead of the in-tree Kubernetes function.
-->
&lt;p>举个例子。假设你是 &lt;code>kubernetes.io/gce-pd&lt;/code> 用户，在启用 CSI 迁移功能后，你仍然可以使用 &lt;code>kubernetes.io/gce-pd&lt;/code> 来配置新卷、挂载现有的 GCE-PD 卷或删除现有卷。所有现有的 API/接口 仍将正常工作──只不过，底层功能调用都将通向 &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE PD CSI 驱动&lt;/a>，而不是 Kubernetes 的树内存储功能。&lt;/p>
&lt;!---
This enables a smooth transition for end users. Additionally as storage plugin developers, we can reduce the burden of maintaining the in-tree storage plugins and eventually remove them from the core Kubernetes binary.
-->
&lt;p>这使得 Kubernetes 终端用户可以顺利过渡。另外，对于存储插件的开发者，我们可以减少他们维护树内存储插件的负担，并最终将这些插件从 Kubernetes 核心的二进制中移除。&lt;/p>
&lt;!---
## What has been changed, and what's new?
Building on the work done in Kubernetes v1.17 and earlier, the releases since then have
made a series of changes:
-->
&lt;h2 id="what-has-been-changed-and-what-s-new">改进与更新 &lt;/h2>
&lt;p>在 Kubernetes v1.17 及更早的工作基础上，此后的发布有了以下一系列改变：&lt;/p>
&lt;!---
### New feature gates
The Kubernetes v1.21 release deprecated the `CSIMigration{provider}Complete` feature flags, and stopped honoring them. In their place came new feature flags named `InTreePlugin{vendor}Unregister`, that replace the old feature flag and retain all the functionality that `CSIMigration{provider}Complete` provided.
-->
&lt;h3 id="new-feature-gates">新的特性门控（feature gate） &lt;/h3>
&lt;p>Kubernetes v1.21 弃用了 &lt;code>CSIMigration{provider}Complete&lt;/code> 特性参数（feature flag），它们不再生效。取而代之的是名为 &lt;code>InTreePlugin{vendor}Unregister&lt;/code> 的新特性参数，它们保留了 &lt;code>CSIMigration{provider}Complete&lt;/code> 提供的所有功能。&lt;/p>
&lt;!---
`CSIMigration{provider}Complete` was introduced before as a supplementary feature gate once CSI migration is enabled on all of the nodes. This flag unregisters the in-tree storage plugin you specify with the `{provider}` part of the flag name.
-->
&lt;p>&lt;code>CSIMigration{provider}Complete&lt;/code> 是作为 CSI 迁移功能在所有节点上启用后的补充特性门控于之前引入的。这个参数可注销参数名称中 &lt;code>{provider}&lt;/code> 部分所指定的树内存储插件。&lt;/p>
&lt;!---
When you enable that feature gate, then instead of using the in-tree driver code, your cluster directly selects and uses the relevant CSI driver. This happens without any check for whether CSI migration is enabled on the node, or whether you have in fact deployed that CSI driver.
-->
&lt;p>当你启用该特性门控时，你的集群不再使用树内驱动代码，而是直接选择并使用相应的 CSI 驱动。同时，集群并不检查节点上 CSI 迁移功能是否启用，以及 CSI 驱动是否实际部署。&lt;/p>
&lt;!---
While this feature gate is a great helper, SIG Storage (and, I'm sure, lots of cluster operators) also wanted a feature gate that lets you disable an in-tree storage plugin, even without also enabling CSI migration. For example, you might want to disable the EBS storage plugin on a GCE cluster, because EBS volumes are specific to a different vendor's cloud (AWS).
-->
&lt;p>虽然这一特性门控是一个很好的帮手，但 Kubernetes 存储特别兴趣组（以及，我相信还有很多集群操作员）同样希望有一个特性门控可以让你即使在不启用 CSI 迁移功能时，也能禁用树内存储插件。例如，你可能希望在一个 GCE 集群上禁用 EBS 存储插件，因为 EBS 卷是其他供应商的云（AWS）所专有的。&lt;/p>
&lt;!---
To make this possible, Kubernetes v1.21 introduced a new feature flag set: `InTreePlugin{vendor}Unregister`.
`InTreePlugin{vendor}Unregister` is a standalone feature gate that can be enabled and disabled independently from CSI Migration. When enabled, the component will not register the specific in-tree storage plugin to the supported list. If the cluster operator only enables this flag, end users will get an error from PVC saying it cannot find the plugin when the plugin is used. The cluster operator may want to enable this regardless of CSI Migration if they do not want to support the legacy in-tree APIs and only support CSI moving forward.
-->
&lt;p>为了使这成为可能，Kubernetes v1.21 引入了一个新的特性参数集合：&lt;code>InTreePlugin{vendor}Unregister&lt;/code>。&lt;/p>
&lt;p>&lt;code>InTreePlugin{vendor}Unregister&lt;/code> 是一种特性门控，可以独立于 CSI 迁移功能来启用或禁用。当启用此种特性门控时，组件将不会把相应的树内存储插件注册到支持的列表中。如果集群操作员只启用了这种参数，终端用户将在使用该插件的 PVC&lt;sup id="fnref1:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup> 处遇到错误，提示其找不到插件。如果集群操作员不想支持过时的树内存储 API，只支持 CSI，那么他们可能希望启用这种特性门控而不考虑 CSI 迁移功能。&lt;/p>
&lt;!---
### Observability
Kubernetes v1.21 introduced [metrics](https://github.com/kubernetes/kubernetes/issues/98279) for tracking CSI migration.
You can use these metrics to observe how your cluster is using storage services and whether access to that storage is using the legacy in-tree driver or its CSI-based replacement.
-->
&lt;h3 id="observability">可观察性 &lt;/h3>
&lt;p>Kubernetes v1.21 引入了跟踪 CSI 迁移功能的&lt;a href="https://github.com/kubernetes/kubernetes/issues/98279">指标&lt;/a>。你可以使用这些指标来观察你的集群是如何使用存储服务的，以及对该存储的访问使用的是传统的树内驱动还是基于 CSI 的替代。&lt;/p>
&lt;!---
| Components | Metrics | Notes |
| -------------------------------------------- | ---------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Kube-Controller-Manager | storage_operation_duration_seconds | A new label `migrated` is added to the metric to indicate whether this storage operation is a CSI migration operation(string value `true` for enabled and `false` for not enabled). |
| Kubelet | csi_operations_seconds | The new metric exposes labels including `driver_name`, `method_name`, `grpc_status_code` and `migrated`. The meaning of these labels is identical to `csi_sidecar_operations_seconds`. |
| CSI Sidecars(provisioner, attacher, resizer) | csi_sidecar_operations_seconds | A new label `migrated` is added to the metric to indicate whether this storage operation is a CSI migration operation(string value `true` for enabled and `false` for not enabled). |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>组件&lt;/th>
&lt;th>指标&lt;/th>
&lt;th>注释&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kube-Controller-Manager&lt;/td>
&lt;td>storage_operation_duration_seconds&lt;/td>
&lt;td>一个新的标签 &lt;code>migrated&lt;/code> 被添加到指标中，以表明该存储操作是否由 CSI 迁移功能操作（字符串值为 &lt;code>true&lt;/code> 表示启用，&lt;code>false&lt;/code> 表示未启用）。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Kubelet&lt;/td>
&lt;td>csi_operations_seconds&lt;/td>
&lt;td>新的指标提供的标签包括 &lt;code>driver_name&lt;/code>、&lt;code>method_name&lt;/code>、&lt;code>grpc_status_code&lt;/code> 和 &lt;code>migrated&lt;/code>。这些标签的含义与 &lt;code>csi_sidecar_operations_seconds&lt;/code> 相同。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>CSI Sidecars(provisioner, attacher, resizer)&lt;/td>
&lt;td>csi_sidecar_operations_seconds&lt;/td>
&lt;td>一个新的标签 &lt;code>migrated&lt;/code> 被添加到指标中，以表明该存储操作是否由 CSI 迁移功能操作（字符串值为 &lt;code>true&lt;/code> 表示启用，&lt;code>false&lt;/code> 表示未启用）。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!---
### Bug fixes and feature improvement
We have fixed numerous bugs like dangling attachment, garbage collection, incorrect topology label through the help of our beta testers.
-->
&lt;h3 id="bug-fixes-and-feature-improvement">错误修复和功能改进 &lt;/h3>
&lt;p>籍由 beta 测试人员的帮助，我们修复了许多错误──如悬空附件、垃圾收集、拓扑标签错误等。&lt;/p>
&lt;!---
### Cloud Provider &amp;&amp; Cluster Lifecycle Collaboration
SIG Storage has been working closely with SIG Cloud Provider and SIG Cluster Lifecycle on the rollout of CSI migration.
If you are a user of a managed Kubernetes service, check with your provider if anything needs to be done. In many cases, the provider will manage the migration and no additional work is required.
-->
&lt;h3 id="cloud-provider-cluster-lifecycle-collaboration">与 Kubernetes 云提供商特别兴趣组、集群生命周期特别兴趣组的合作 &lt;/h3>
&lt;p>Kubernetes 存储特别兴趣组与云提供商特别兴趣组和集群生命周期特别兴趣组，正为了 CSI 迁移功能上线而密切合作。&lt;/p>
&lt;p>如果你采用托管 Kubernetes 服务，请询问你的供应商是否有什么工作需要完成。在许多情况下，供应商将管理迁移，你不需要做额外的工作。&lt;/p>
&lt;!---
If you use a distribution of Kubernetes, check its official documentation for information about support for this feature. For the CSI Migration feature graduation to GA, SIG Storage and SIG Cluster Lifecycle are collaborating towards making the migration mechanisms available in tooling (such as kubeadm) as soon as they're available in Kubernetes itself.
-->
&lt;p>如果你使用的是 Kubernetes 的发行版，请查看其官方文档，了解对该功能的支持情况。对于已进入正式发布阶段的 CSI 迁移功能，Kubernetes 存储特别兴趣组正与Kubernetes 集群生命周期特别兴趣组合作，以便在这些功能于 Kubernetes 中可用时，使迁移机制也进入到周边工具（如 kubeadm）中。&lt;/p>
&lt;!---
## What is the timeline / status? {#timeline-and-status}
The current and targeted releases for each individual driver is shown in the table below:
-->
&lt;h2 id="timeline-and-status">时间计划及当前状态 &lt;/h2>
&lt;p>各驱动的当前发布及目标发布如下表所示：&lt;/p>
&lt;!---
| Driver | Alpha | Beta (in-tree deprecated) | Beta (on-by-default) | GA | Target "in-tree plugin" removal |
| ---------------- | ----- | ------------------------- | -------------------- | ------------- | ------------------------------- |
| AWS EBS | 1.14 | 1.17 | 1.23 | 1.24 (Target) | 1.26 (Target) |
| GCE PD | 1.14 | 1.17 | 1.23 | 1.24 (Target) | 1.26 (Target) |
| OpenStack Cinder | 1.14 | 1.18 | 1.21 | 1.24 (Target) | 1.26 (Target) |
| Azure Disk | 1.15 | 1.19 | 1.23 | 1.24 (Target) | 1.26 (Target) |
| Azure File | 1.15 | 1.21 | 1.24 (Target) | 1.25 (Target) | 1.27 (Target) |
| vSphere | 1.18 | 1.19 | 1.24 (Target) | 1.25 (Target) | 1.27 (Target) |
| Ceph RBD | 1.23 |
| Portworx | 1.23 |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>驱动&lt;/th>
&lt;th>Alpha&lt;/th>
&lt;th>Beta（启用树内插件）&lt;/th>
&lt;th>Beta（默认启用）&lt;/th>
&lt;th>正式发布&lt;/th>
&lt;th>目标：移除“树内存储插件”&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AWS EBS&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GCE PD&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenStack Cinder&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure Disk&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure File&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vSphere&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.24 (Target)&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ceph RBD&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Portworx&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!---
The following storage drivers will not have CSI migration support. The ScaleIO driver was already removed; the others are deprecated and will be removed from core Kubernetes.
-->
&lt;p>以下存储驱动将不会支持 CSI 迁移功能。其中 ScaleIO 驱动已经被移除；其他驱动都被弃用，并将从 Kubernetes 核心中删除。&lt;/p>
&lt;!---
| Driver | Deprecated | Code Removal |
| --------- | ---------- | ------------- |
| ScaleIO | 1.16 | 1.22 |
| Flocker | 1.22 | 1.25 (Target) |
| Quobyte | 1.22 | 1.25 (Target) |
| StorageOS | 1.22 | 1.25 (Target) |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>驱动&lt;/th>
&lt;th>被弃用&lt;/th>
&lt;th>代码移除&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ScaleIO&lt;/td>
&lt;td>1.16&lt;/td>
&lt;td>1.22&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Flocker&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Quobyte&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>StorageOS&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25 (Target)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!---
## What's next?
With more CSI drivers graduating to GA, we hope to soon mark the overall CSI Migration feature as GA. We are expecting cloud provider in-tree storage plugins code removal to happen by Kubernetes v1.26 and v1.27.
-->
&lt;h2 id="what-s-next">下一步的计划 &lt;/h2>
&lt;p>随着更多的 CSI 驱动进入正式发布阶段，我们希望尽快将整个 CSI 迁移功能标记为正式发布状态。我们计划在 Kubernetes v1.26 和 v1.27 之前移除云提供商树内存储插件的代码。&lt;/p>
&lt;!---
## What should I do as a user?
Note that all new features for the Kubernetes storage system (such as volume snapshotting) will only be added to the CSI interface. Therefore, if you are starting up a new cluster, creating stateful applications for the first time, or require these new features we recommend using CSI drivers natively (instead of the in-tree volume plugin API). Follow the [updated user guides for CSI drivers](https://kubernetes-csi.github.io/docs/drivers.html) and use the new CSI APIs.
-->
&lt;h2 id="what-should-i-do-as-a-user">作为用户，我应该做什么？ &lt;/h2>
&lt;p>请注意，Kubernetes 存储系统的所有新功能（如卷快照）将只被添加到 CSI 接口。因此，如果你正在启动一个新的集群、首次创建有状态的应用程序，或者需要这些新功能，我们建议你在本地使用 CSI 驱动（而不是树内卷插件 API）。遵循&lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">最新的 CSI 驱动用户指南&lt;/a>并使用新的 CSI API。&lt;/p>
&lt;!---
However, if you choose to roll a cluster forward or continue using specifications with the legacy volume APIs, CSI Migration will ensure we continue to support those deployments with the new CSI drivers. However, if you want to leverage new features like snapshot, it will require a manual migration to re-import an existing intree PV as a CSI PV.
-->
&lt;p>然而，如果您选择沿用现有集群或继续使用传统卷 API 的规约，CSI 迁移功能将确保我们通过新 CSI 驱动继续支持这些部署。但是，如果您想利用快照等新功能，则需要进行手动迁移，将现有的树内持久卷重新导入为 CSI 持久卷。&lt;/p>
&lt;!---
## How do I get involved?
The Kubernetes Slack channel [#csi-migration](https://kubernetes.slack.com/messages/csi-migration) along with any of the standard [SIG Storage communication channels](https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact) are great mediums to reach out to the SIG Storage and migration working group teams.
-->
&lt;h2 id="how-do-i-get-involved">我如何参与其中？ &lt;/h2>
&lt;p>Kubernetes Slack 频道 &lt;a href="https://kubernetes.slack.com/messages/csi-migration">#csi-migration&lt;/a> 以及任何一个标准的 &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">SIG Storage 通信频道&lt;/a>都是与 Kubernetes 存储特别兴趣组和迁移工作组团队联系的绝佳媒介。&lt;/p>
&lt;!---
This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help move the project forward:
* Michelle Au (msau42)
* Jan Šafránek (jsafrane)
* Hemant Kumar (gnufied)
-->
&lt;p>该项目，和其他所有 Kubernetes 项目一样，是许多来自不同背景的贡献者共同努力的结果。我们非常感谢在过去几个季度里挺身而出帮助推动项目发展的贡献者们：&lt;/p>
&lt;ul>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;li>Jan Šafránek (jsafrane)&lt;/li>
&lt;li>Hemant Kumar (gnufied)&lt;/li>
&lt;/ul>
&lt;!---
Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution to the CSI migration feature:
* Andy Zhang (andyzhangz)
* Divyen Patel (divyenpatel)
* Deep Debroy (ddebroy)
* Humble Devassy Chirammal (humblec)
* Jing Xu (jingxu97)
* Jordan Liggitt (liggitt)
* Matthew Cary (mattcary)
* Matthew Wong (wongma7)
* Neha Arora (nearora-msft)
* Oksana Naumov (trierra)
* Saad Ali (saad-ali)
* Tim Bannister (sftim)
* Xing Yang (xing-yang)
-->
&lt;p>特别感谢以下人士对 CSI 迁移功能的精辟评论、全面考虑和宝贵贡献：&lt;/p>
&lt;ul>
&lt;li>Andy Zhang (andyzhangz)&lt;/li>
&lt;li>Divyen Patel (divyenpatel)&lt;/li>
&lt;li>Deep Debroy (ddebroy)&lt;/li>
&lt;li>Humble Devassy Chirammal (humblec)&lt;/li>
&lt;li>Jing Xu (jingxu97)&lt;/li>
&lt;li>Jordan Liggitt (liggitt)&lt;/li>
&lt;li>Matthew Cary (mattcary)&lt;/li>
&lt;li>Matthew Wong (wongma7)&lt;/li>
&lt;li>Neha Arora (nearora-msft)&lt;/li>
&lt;li>Oksana Naumov (trierra)&lt;/li>
&lt;li>Saad Ali (saad-ali)&lt;/li>
&lt;li>Tim Bannister (sftim)&lt;/li>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;/ul>
&lt;!---
Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the [Kubernetes Storage Special Interest Group (SIG)](https://github.com/kubernetes/community/tree/master/sig-storage). We’re rapidly growing and always welcome new contributors.
-->
&lt;p>有兴趣参与 CSI 或 Kubernetes 存储系统任何部分的设计和开发的人，请加入 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes 存储特别兴趣组&lt;/a>。我们正在迅速成长，并一直欢迎新的贡献者。&lt;/p>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>持久卷申领（PersistentVolumeClaim，PVC）&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>Blog: Kubernetes 1.23：IPv4/IPv6 双协议栈网络达到 GA</title><link>https://kubernetes.io/zh-cn/blog/2021/12/08/dual-stack-networking-ga/</link><pubDate>Wed, 08 Dec 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/12/08/dual-stack-networking-ga/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.23: Dual-stack IPv4/IPv6 Networking Reaches GA'
date: 2021-12-08
slug: dual-stack-networking-ga
-->
&lt;!--
**Author:** Bridget Kromhout (Microsoft)
-->
&lt;p>&lt;strong>作者:&lt;/strong> Bridget Kromhout (微软)&lt;/p>
&lt;!--
"When will Kubernetes have IPv6?" This question has been asked with increasing frequency ever since alpha support for IPv6 was first added in k8s v1.9. While Kubernetes has supported IPv6-only clusters since v1.18, migration from IPv4 to IPv6 was not yet possible at that point. At long last, [dual-stack IPv4/IPv6 networking](https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack/) has reached general availability (GA) in Kubernetes v1.23.
What does dual-stack networking mean for you? Let’s take a look…
-->
&lt;p>“Kubernetes 何时支持 IPv6？” 自从 k8s v1.9 版本中首次添加对 IPv6 的 alpha 支持以来，这个问题的讨论越来越频繁。
虽然 Kubernetes 从 v1.18 版本开始就支持纯 IPv6 集群，但当时还无法支持 IPv4 迁移到 IPv6。
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/563-dual-stack/">IPv4/IPv6 双协议栈网络&lt;/a>
在 Kubernetes v1.23 版本中进入正式发布（GA）阶段。&lt;/p>
&lt;p>让我们来看看双协议栈网络对你来说意味着什么？&lt;/p>
&lt;!--
## Service API updates
-->
&lt;h2 id="更新-service-api">更新 Service API&lt;/h2>
&lt;!--
[Services](/docs/concepts/services-networking/service/) were single-stack before 1.20, so using both IP families meant creating one Service per IP family. The user experience was simplified in 1.20, when Services were re-implemented to allow both IP families, meaning a single Service can handle both IPv4 and IPv6 workloads. Dual-stack load balancing is possible between services running any combination of IPv4 and IPv6.
-->
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/">Services&lt;/a> 在 1.20 版本之前是单协议栈的，
因此，使用两个 IP 协议族意味着需为每个 IP 协议族创建一个 Service。在 1.20 版本中对用户体验进行简化，
重新实现了 Service 以支持两个 IP 协议族，这意味着一个 Service 就可以处理 IPv4 和 IPv6 协议。
对于 Service 而言，任意的 IPv4 和 IPv6 协议组合都可以实现负载均衡。&lt;/p>
&lt;!--
The Service API now has new fields to support dual-stack, replacing the single ipFamily field.
* You can select your choice of IP family by setting `ipFamilyPolicy` to one of three options: SingleStack, PreferDualStack, or RequireDualStack. A service can be changed between single-stack and dual-stack (within some limits).
* Setting `ipFamilies` to a list of families assigned allows you to set the order of families used.
* `clusterIPs` is inclusive of the previous `clusterIP` but allows for multiple entries, so it’s no longer necessary to run duplicate services, one in each of the two IP families. Instead, you can assign cluster IP addresses in both IP families.
-->
&lt;p>Service API 现在有了支持双协议栈的新字段，取代了单一的 ipFamily 字段。&lt;/p>
&lt;ul>
&lt;li>你可以通过将 &lt;code>ipFamilyPolicy&lt;/code> 字段设置为 &lt;code>SingleStack&lt;/code>、&lt;code>PreferDualStack&lt;/code> 或
&lt;code>RequireDualStack&lt;/code> 来设置 IP 协议族。Service 可以在单协议栈和双协议栈之间进行转换(在某些限制内)。&lt;/li>
&lt;li>设置 &lt;code>ipFamilies&lt;/code> 为指定的协议族列表，可用来设置使用协议族的顺序。&lt;/li>
&lt;li>'clusterIPs' 的能力在涵盖了之前的 'clusterIP'的情况下，还允许设置多个 IP 地址。
所以不再需要运行重复的 Service，在两个 IP 协议族中各运行一个。你可以在两个 IP 协议族中分配集群 IP 地址。&lt;/li>
&lt;/ul>
&lt;!--
Note that Pods are also dual-stack. For a given pod, there is no possibility of setting multiple IP addresses in the same family.
-->
&lt;p>请注意，Pods 也是双协议栈的。对于一个给定的 Pod，不可能在同一协议族中设置多个 IP 地址。&lt;/p>
&lt;!--
## Default behavior remains single-stack
-->
&lt;h2 id="默认行为仍然是单协议栈">默认行为仍然是单协议栈&lt;/h2>
&lt;!--
Starting in 1.20 with the re-implementation of dual-stack services as alpha, the underlying networking for Kubernetes has included dual-stack whether or not a cluster was configured with the feature flag to enable dual-stack.
-->
&lt;p>从 1.20 版本开始，重新实现的双协议栈服务处于 Alpha 阶段，无论集群是否配置了启用双协议栈的特性标志，
Kubernetes 的底层网络都已经包括了双协议栈。&lt;/p>
&lt;!--
Kubernetes 1.23 removed that feature flag as part of graduating the feature to stable. Dual-stack networking is always available if you want to configure it. You can set your cluster network to operate as single-stack IPv4, as single-stack IPv6, or as dual-stack IPv4/IPv6.
-->
&lt;p>Kubernetes 1.23 删除了这个特性标志，说明该特性已经稳定。
如果你想要配置双协议栈网络，这一能力总是存在的。
你可以将集群网络设置为 IPv4 单协议栈 、IPv6 单协议栈或 IPV4/IPV6 双协议栈 。&lt;/p>
&lt;!--
While Services are set according to what you configure, Pods default to whatever the CNI plugin sets. If your CNI plugin assigns single-stack IPs, you will have single-stack unless `ipFamilyPolicy` specifies PreferDualStack or RequireDualStack. If your CNI plugin assigns dual-stack IPs, `pod.status.PodIPs` defaults to dual-stack.
-->
&lt;p>虽然 Service 是根据你的配置设置的，但 Pod 默认是由 CNI 插件设置的。
如果你的 CNI 插件分配单协议栈 IP，那么就是单协议栈，除非 &lt;code>ipFamilyPolicy&lt;/code> 设置为 &lt;code>PreferDualStack&lt;/code> 或 &lt;code>RequireDualStack&lt;/code>。
如果你的 CNI 插件分配双协议栈 IP，则 &lt;code>pod.status.PodIPs&lt;/code> 默认为双协议栈。&lt;/p>
&lt;!--
Even though dual-stack is possible, it is not mandatory to use it. Examples in the documentation show the variety possible in [dual-stack service configurations](/docs/concepts/services-networking/dual-stack/#dual-stack-service-configuration-scenarios).
-->
&lt;p>尽管双协议栈是可用的，但并不强制你使用它。
在&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/dual-stack/#dual-stack-service-configuration-scenarios">双协议栈服务配置&lt;/a>
文档中的示例列出了可能出现的各种场景.&lt;/p>
&lt;!--
## Try dual-stack right now
-->
&lt;h2 id="现在尝试双协议栈">现在尝试双协议栈&lt;/h2>
&lt;!--
While upstream Kubernetes now supports [dual-stack networking](/docs/concepts/services-networking/dual-stack/) as a GA or stable feature, each provider’s support of dual-stack Kubernetes may vary. Nodes need to be provisioned with routable IPv4/IPv6 network interfaces. Pods need to be dual-stack. The [network plugin](/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/) is what assigns the IP addresses to the Pods, so it's the network plugin being used for the cluster that needs to support dual-stack. Some Container Network Interface (CNI) plugins support dual-stack, as does kubenet.
-->
&lt;p>虽然现在上游 Kubernetes 支持&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/dual-stack/">双协议栈网络&lt;/a>
作为 GA 或稳定特性，但每个提供商对双协议栈 Kubernetes 的支持可能会有所不同。节点需要提供可路由的 IPv4/IPv6 网络接口。
Pod 需要是双协议栈的。&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">网络插件&lt;/a>
是用来为 Pod 分配 IP 地址的，所以集群需要支持双协议栈的网络插件。一些容器网络接口（CNI）插件支持双协议栈，例如 kubenet。&lt;/p>
&lt;!--
Ecosystem support of dual-stack is increasing; you can create [dual-stack clusters with kubeadm](/docs/setup/production-environment/tools/kubeadm/dual-stack-support/), try a [dual-stack cluster locally with KIND](https://kind.sigs.k8s.io/docs/user/configuration/#ip-family), and deploy dual-stack clusters in cloud providers (after checking docs for CNI or kubenet availability).
-->
&lt;p>支持双协议栈的生态系统在不断壮大；你可以使用
&lt;a href="https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/dual-stack-support/">kubeadm 创建双协议栈集群&lt;/a>,
在本地尝试用 &lt;a href="https://kind.sigs.k8s.io/docs/user/configuration/#ip-family">KIND 创建双协议栈集群&lt;/a>，
还可以将双协议栈集群部署到云上（在查阅 CNI 或 kubenet 可用性的文档之后）&lt;/p>
&lt;!--
## Get involved with SIG Network
-->
&lt;h2 id="加入-network-sig">加入 Network SIG&lt;/h2>
&lt;!--
SIG-Network wants to learn from community experiences with dual-stack networking to find out more about evolving needs and your use cases. The [SIG-network update video from KubeCon NA 2021](https://www.youtube.com/watch?v=uZ0WLxpmBbY&amp;list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP&amp;index=4) summarizes the SIG’s recent updates, including dual-stack going to stable in 1.23.
-->
&lt;p>SIG-Network 希望从双协议栈网络的社区体验中学习，以了解更多不断变化的需求和你的用例信息。
&lt;a href="https://www.youtube.com/watch?v=uZ0WLxpmBbY&amp;amp;list=PLj6h78yzYM2Nd1U4RMhv7v88fdiFqeYAP&amp;amp;index=4">SIG-network 更新了来自 KubeCon 2021 北美大会的视频&lt;/a>
总结了 SIG 最近的更新，包括双协议栈将在 1.23 版本中稳定。&lt;/p>
&lt;!--
The current SIG-Network [KEPs](https://github.com/orgs/kubernetes/projects/10) and [issues](https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork) on GitHub illustrate the SIG’s areas of emphasis. The [dual-stack API server](https://github.com/kubernetes/enhancements/issues/2438) is one place to consider contributing.
-->
&lt;p>当前 SIG-Network 在 GitHub 上的 &lt;a href="https://github.com/orgs/kubernetes/projects/10">KEPs&lt;/a> 和
&lt;a href="https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork">issues&lt;/a>
说明了该 SIG 的重点领域。&lt;a href="https://github.com/kubernetes/enhancements/issues/2438">双协议栈 API 服务器&lt;/a>
是一个考虑贡献的方向。&lt;/p>
&lt;!--
[SIG-Network meetings](https://github.com/kubernetes/community/tree/master/sig-network#meetings) are a friendly, welcoming venue for you to connect with the community and share your ideas. Looking forward to hearing from you!
-->
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-network#meetings">SIG-Network 会议&lt;/a>
是一个友好、热情的场所，你可以与社区联系并分享你的想法。期待你的加入！&lt;/p>
&lt;!--
## Acknowledgments
-->
&lt;h2 id="致谢">致谢&lt;/h2>
&lt;!--
The dual-stack networking feature represents the work of many Kubernetes contributors. Thanks to all who contributed code, experience reports, documentation, code reviews, and everything in between. Bridget Kromhout details this community effort in [Dual-Stack Networking in Kubernetes](https://containerjournal.com/features/dual-stack-networking-in-kubernetes/). KubeCon keynotes by Tim Hockin &amp; Khaled (Kal) Henidak in 2019 ([The Long Road to IPv4/IPv6 Dual-stack Kubernetes](https://www.youtube.com/watch?v=o-oMegdZcg4)) and by Lachlan Evenson in 2021 ([And Here We Go: Dual-stack Networking in Kubernetes](https://www.youtube.com/watch?v=lVrt8F2B9CM)) talk about the dual-stack journey, spanning five years and a great many lines of code.
-->
&lt;p>许多 Kubernetes 贡献者为双协议栈网络做出了贡献。感谢所有贡献了代码、经验报告、文档、代码审查以及其他工作的人。
Bridget Kromhout 在 &lt;a href="https://containerjournal.com/features/dual-stack-networking-in-kubernetes/">Kubernetes的双协议栈网络&lt;/a>
中详细介绍了这项社区工作。Tim Hockin 和 Khaled (Kal) Henidak 在 2019 年的 KubeCon 大会演讲
（&lt;a href="https://www.youtube.com/watch?v=o-oMegdZcg4">Kubernetes 通往 IPv4/IPv6 双协议栈的漫漫长路&lt;/a>）
和 Lachlan Evenson 在 2021 年演讲（&lt;a href="https://www.youtube.com/watch?v=o-oMegdZcg4">我们来啦，Kubernetes 双协议栈网络&lt;/a>）
中讨论了双协议栈的发展旅程，耗时 5 年和海量代码。&lt;/p></description></item><item><title>Blog: 公布 2021 年指导委员会选举结果</title><link>https://kubernetes.io/zh-cn/blog/2021/11/08/steering-committee-results-2021/</link><pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/11/08/steering-committee-results-2021/</guid><description>
&lt;!--
layout: blog
title: "Announcing the 2021 Steering Committee Election Results"
date: 2021-11-08
slug: steering-committee-results-2021
-->
&lt;!--
**Author**: Kaslin Fields
-->
&lt;p>&lt;strong>作者&lt;/strong>：Kaslin Fields&lt;/p>
&lt;!--
The [2021 Steering Committee Election](https://github.com/kubernetes/community/tree/master/events/elections/2021) is now complete. The Kubernetes Steering Committee consists of 7 seats, 4 of which were up for election in 2021. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.
-->
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/events/elections/2021">2021 年指导委员会选举&lt;/a>现已完成。
Kubernetes 指导委员会由 7 个席位组成，其中 4 个席位将在 2021 年进行选举。
新任委员会成员任期 2 年，所有成员均由 Kubernetes 社区选举产生。&lt;/p>
&lt;!--
This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their [charter](https://github.com/kubernetes/steering/blob/master/charter.md).
-->
&lt;p>这个社区机构非常重要，因为它监督整个 Kubernetes 项目的治理。
你可以在其&lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">章程&lt;/a>中了解更多关于指导委员会的角色。&lt;/p>
&lt;!--
## Results
-->
&lt;h2 id="选举结果">选举结果&lt;/h2>
&lt;!--
Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):
-->
&lt;p>祝贺当选的委员会成员，他们的两年任期即刻生效（按 GitHub handle 字母排序）:&lt;/p>
&lt;!--
* **Christoph Blecker ([@cblecker](https://github.com/cblecker)), Red Hat**
* **Stephen Augustus ([@justaugustus](https://github.com/justaugustus)), Cisco**
* **Paris Pittman ([@parispittman](https://github.com/parispittman)), Apple**
* **Tim Pepper ([@tpepper](https://github.com/tpepper)), VMware**
-->
&lt;ul>
&lt;li>&lt;strong>Christoph Blecker（&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>）， 红帽&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Stephen Augustus（&lt;a href="https://github.com/justaugustus">@justaugustus&lt;/a>）， 思科&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittman（&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>)， 苹果&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Tim Pepper（&lt;a href="https://github.com/tpepper">@tpepper&lt;/a>）， VMware&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
They join continuing members:
-->
&lt;p>他们加入永久成员：&lt;/p>
&lt;!--
* **Davanum Srinivas ([@dims](https://github.com/dims)), VMware**
* **Jordan Liggitt ([@liggitt](https://github.com/liggitt)), Google**
* **Bob Killen ([@mrbobbytables](https://github.com/mrbobbytables)), Google**
-->
&lt;ul>
&lt;li>&lt;strong>Davanum Srinivas（&lt;a href="https://github.com/dims">@dims&lt;/a>）， VMware&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Jordan Liggitt （&lt;a href="https://github.com/liggitt">@liggitt&lt;/a>）， 谷歌&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Bob Killen （&lt;a href="https://github.com/mrbobbytables">@mrbobbytables&lt;/a>）， 谷歌&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
Paris Pittman and Christoph Blecker are returning Steering Committee Members.
-->
&lt;p>Paris Pittman 和 Christoph Blecker 将回到指导委员会。&lt;/p>
&lt;!--
## Big Thanks
-->
&lt;h2 id="非常感谢">非常感谢&lt;/h2>
&lt;!--
Thank you and congratulations on a successful election to this round’s election officers:
-->
&lt;p>感谢并祝贺完成本轮成功选举的选举官们:&lt;/p>
&lt;ul>
&lt;li>Alison Dowdney, (&lt;a href="https://github.com/alisondy">@alisondy&lt;/a>)&lt;/li>
&lt;li>Noah Kantrowitz (&lt;a href="https://github.com/coderanger">@coderanger&lt;/a>)&lt;/li>
&lt;li>Josh Berkus (&lt;a href="https://github.com/jberkus">@jberkus&lt;/a>)&lt;/li>
&lt;/ul>
&lt;!--
Special thanks to Arnaud Meukam ([@ameukam](https://github.com/ameukam)), k8s-infra liaison, who enabled our voting software on community-owned infrastructure.
-->
&lt;p>特别感谢 k8s-infra 联络员 Arnaud Meukam（&lt;a href="https://github.com/ameukam">@ameukam&lt;/a>），
他在社区的基础设施上启动了我们的投票软件。&lt;/p>
&lt;!--
Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:
-->
&lt;p>感谢荣誉退休的指导委员会成员。对你们之前对社区的贡献表示感谢:&lt;/p>
&lt;ul>
&lt;li>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>)&lt;/li>
&lt;li>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>)&lt;/li>
&lt;/ul>
&lt;!--
And thank you to all the candidates who came forward to run for election.
-->
&lt;p>感谢所有前来参加竞选的候选人。&lt;/p>
&lt;!--
## Get Involved with the Steering Committee
-->
&lt;h2 id="参与指导委员会">参与指导委员会&lt;/h2>
&lt;!--
This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee [backlog items](https://github.com/kubernetes/steering/projects/1) and weigh in by filing an issue or creating a PR against their [repo](https://github.com/kubernetes/steering). They have an open meeting on [the first Monday at 9:30am PT of every month](https://github.com/kubernetes/steering) and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list steering@kubernetes.io.
-->
&lt;p>与所有 Kubernetes 一样，这个管理机构对所有人开放。
你可以查看指导委员会的&lt;a href="https://github.com/kubernetes/steering/projects/1">待办事项&lt;/a>，
通过在他们的 &lt;a href="https://github.com/kubernetes/steering">repo&lt;/a>
中提交一个 issue 或创建一个 PR 来参与讨论。
他们在&lt;a href="https://github.com/kubernetes/steering">每月的第一个星期一上午 9:30&lt;/a> 举行公开会议，
并定期参加会见我们的贡献者活动。也可以通过他们的公共邮件列表 &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a> 联系他们。&lt;/p>
&lt;!--
You can see what the Steering Committee meetings are all about by watching past meetings on the [YouTube Playlist](https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM).
-->
&lt;p>你可以在 &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube 播放列表&lt;/a>
上观看之前的会议视频，了解指导委员会的会议讨论内容。&lt;/p>
&lt;hr>
&lt;!--
_This post was written by the [Upstream Marketing Working Group](https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing). If you want to write stories about the Kubernetes community, learn more about us._
-->
&lt;p>&lt;em>本文是由&lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing">上游营销工作组&lt;/a>撰写的。
如果你想撰写有关 Kubernetes 社区的故事，请了解更多关于我们的信息。&lt;/em>&lt;/p></description></item><item><title>Blog: 关注 SIG Node</title><link>https://kubernetes.io/zh-cn/blog/2021/09/27/sig-node-spotlight-2021/</link><pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/09/27/sig-node-spotlight-2021/</guid><description>
&lt;!--
---
layout: blog
title: "Spotlight on SIG Node"
date: 2021-09-27
slug: sig-node-spotlight-2021
---
-->
&lt;p>&lt;strong>Author:&lt;/strong> Dewan Ahmed, Red Hat&lt;/p>
&lt;!--
**Author:** Dewan Ahmed, Red Hat
-->
&lt;!--
## Introduction
In Kubernetes, a _Node_ is a representation of a single machine in your cluster. [SIG Node](https://github.com/kubernetes/community/tree/master/sig-node) owns that very important Node component and supports various subprojects such as Kubelet, Container Runtime Interface (CRI) and more to support how the pods and host resources interact. In this blog, we have summarized our conversation with [Elana Hashman (EH)](https://twitter.com/ehashdn) &amp; [Sergey Kanzhelev (SK)](https://twitter.com/SergeyKanzhelev), who walk us through the various aspects of being a part of the SIG and share some insights about how others can get involved.
-->
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;p>在 Kubernetes 中，一个 &lt;em>Node&lt;/em> 是你集群中的某台机器。
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node&lt;/a> 负责这一非常重要的 Node 组件并支持各种子项目，
如 Kubelet, Container Runtime Interface (CRI) 以及其他支持 Pod 和主机资源间交互的子项目。
在这篇文章中，我们总结了和 &lt;a href="https://twitter.com/ehashdn">Elana Hashman (EH)&lt;/a> &amp;amp; &lt;a href="https://twitter.com/SergeyKanzhelev">Sergey Kanzhelev (SK)&lt;/a> 的对话，是他们带领我们了解作为此 SIG 一份子的各个方面，并分享一些关于其他人如何参与的见解。&lt;/p>
&lt;!--
## A summary of our conversation
### Could you tell us a little about what SIG Node does?
SK: SIG Node is a vertical SIG responsible for the components that support the controlled interactions between the pods and host resources. We manage the lifecycle of pods that are scheduled to a node. This SIG's focus is to enable a broad set of workload types, including workloads with hardware specific or performance sensitive requirements. All while maintaining isolation boundaries between pods on a node, as well as the pod and the host. This SIG maintains quite a few components and has many external dependencies (like container runtimes or operating system features), which makes the complexity we deal with huge. We tame the complexity and aim to continuously improve node reliability.
-->
&lt;h2 id="我们的对话总结">我们的对话总结&lt;/h2>
&lt;h3 id="你能告诉我们一些关于-sig-node-的工作吗">你能告诉我们一些关于 SIG Node 的工作吗？&lt;/h3>
&lt;p>SK：SIG Node 是一个垂直 SIG，负责支持 Pod 和主机资源之间受控互动的组件。我们管理被调度到节点上的 Pod 的生命周期。
这个 SIG 的重点是支持广泛的工作负载类型，包括具有硬件特性或性能敏感要求的工作负载。同时保持节点上 Pod 之间的隔离边界，以及 Pod 和主机的隔离边界。
这个 SIG 维护了相当多的组件，并有许多外部依赖（如容器运行时间或操作系统功能），这使得我们处理起来十分复杂。但我们战胜了这种复杂度，旨在不断提高节点的可靠性。&lt;/p>
&lt;!--
### "SIG Node is a vertical SIG" could you explain a bit more?
EH: There are two kinds of SIGs: horizontal and vertical. Horizontal SIGs are concerned with a particular function of every component in Kubernetes: for example, SIG Security considers security aspects of every component in Kubernetes, or SIG Instrumentation looks at the logs, metrics, traces and events of every component in Kubernetes. Such SIGs don't tend to own a lot of code.
Vertical SIGs, on the other hand, own a single component, and are responsible for approving and merging patches to that code base. SIG Node owns the "Node" vertical, pertaining to the kubelet and its lifecycle. This includes the code for the kubelet itself, as well as the node controller, the container runtime interface, and related subprojects like the node problem detector.
-->
&lt;h3 id="你能再解释一下-sig-node-是一种垂直-sig-的含义吗">你能再解释一下 “SIG Node 是一种垂直 SIG” 的含义吗？&lt;/h3>
&lt;p>EH：有两种 SIG：横向和垂直。横向 SIG 关注 Kubernetes 中每个组件的特定功能：例如，SIG Security 考虑 Kubernetes 中每个组件的安全方面，或者 SIG Instrumentation 关注 Kubernetes 中每个组件的日志、度量、跟踪和事件。
这样的 SIG 并不太会拥有大量的代码。&lt;/p>
&lt;p>相反，垂直 SIG 拥有一个单一的组件，并负责批准和合并该代码库的补丁。
SIG Node 拥有 &amp;quot;Node&amp;quot; 的垂直性，与 kubelet 和它的生命周期有关。这包括 kubelet 本身的代码，以及节点控制器、容器运行时接口和相关的子项目，比如节点问题检测器。&lt;/p>
&lt;!--
### How did the CI subproject start? Is this specific to SIG Node and how does it help the SIG?
SK: The subproject started as a follow up after one of the releases was blocked by numerous test failures of critical tests. These tests haven’t started falling all at once, rather continuous lack of attention led to slow degradation of tests quality. SIG Node was always prioritizing quality and reliability, and forming of the subproject was a way to highlight this priority.
-->
&lt;h3 id="ci-子项目是如何开始的-这是专门针对-sig-node-的吗-它对-sig-有什么帮助">CI 子项目是如何开始的？这是专门针对 SIG Node 的吗？它对 SIG 有什么帮助？&lt;/h3>
&lt;p>SK：该子项目是在其中一个版本因关键测试的大量测试失败而受阻后开始跟进的。
这些测试并不是一下子就开始下降的，而是持续的缺乏关注导致了测试质量的缓慢下降。
SIG Node 一直将质量和可靠性放在首位，组建这个子项目是强调这一优先事项的一种方式。&lt;/p>
&lt;!--
### As the 3rd largest SIG in terms of number of issues and PRs, how does your SIG juggle so much work?
EH: It helps to be organized. When I increased my contributions to the SIG in January of 2021, I found myself overwhelmed by the volume of pull requests and issues and wasn't sure where to start. We were already tracking test-related issues and pull requests on the CI subproject board, but that was missing a lot of our bugfixes and feature work. So I began putting together a triage board for the rest of our pull requests, which allowed me to sort each one by status and what actions to take, and documented its use for other contributors. We closed or merged over 500 issues and pull requests tracked by our two boards in each of the past two releases. The Kubernetes devstats showed that we have significantly increased our velocity as a result.
In June, we ran our first bug scrub event to work through the backlog of issues filed against SIG Node, ensuring they were properly categorized. We closed over 130 issues over the course of this 48 hour global event, but as of writing we still have 333 open issues.
-->
&lt;h3 id="作为-issue-和-pr-数量第三大的-sig-你们-sig-是如何兼顾这么多工作的">作为 issue 和 PR 数量第三大的 SIG，你们 SIG 是如何兼顾这么多工作的？&lt;/h3>
&lt;p>EH：这归功于有组织性。当我在 2021 年 1 月增加对 SIG 的贡献时，我发现自己被大量的 PR 和 issue 淹没了，不知道该从哪里开始。
我们已经在 CI 子项目板上跟踪与测试有关的 issue 和 PR 请求，但这缺少了很多 bug 修复和功能工作。
因此，我开始为我们剩余的 PR 建立一个分流板，这使我能够根据状态和采取的行动对其进行分类，并为其他贡献者记录它的用途。
在过去的两个版本中，我们关闭或合并了超过 500 个 issue 和 PR。Kubernetes devstats 显示，我们的速度因此而大大提升。&lt;/p>
&lt;p>6月，我们进行了第一次 bug 清除活动，以解决针对 SIG Node 的积压问题，确保它们被正确归类。
在这次 48 小时的全球活动中，我们关闭了 130 多个问题，但截至发稿时，我们仍有 333 个问题没有解决。&lt;/p>
&lt;!--
### Why should new and existing contributors consider joining SIG Node?
SK: Being a SIG Node contributor gives you skills and recognition that are rewarding and useful. Understanding under the hood of a kubelet helps architecting better apps, tune and optimize those apps, and gives leg up in issues troubleshooting. If you are a new contributor, SIG Node gives you the foundational knowledge that is key to understanding why other Kubernetes components are designed the way they are. Existing contributors may benefit as many features will require SIG Node changes one way or another. So being a SIG Node contributor helps building features in other SIGs faster.
SIG Node maintains numerous components, many of which have dependency on external projects or OS features. This makes the onboarding process quite lengthy and demanding. But if you are up for a challenge, there is always a place for you, and a group of people to support.
-->
&lt;h3 id="为什么新的和现有的贡献者应该考虑加入-node-兴趣小组呢">为什么新的和现有的贡献者应该考虑加入 Node 兴趣小组呢？&lt;/h3>
&lt;p>SK：作为 SIG Node 的贡献者会带给你有意义且有用的技能和认可度。
了解 Kubelet 的内部结构有助于构建更好的应用程序，调整和优化这些应用程序，并在 issue 排查上获得优势。
如果你是一个新手贡献者，SIG Node 为你提供了基础知识，这是理解其他 Kubernetes 组件的设计方式的关键。
现在的贡献者可能会受益于许多功能都需要 SIG Node 的这种或那种变化。所以成为 SIG Node 的贡献者有助于更快地建立其他 SIG 的功能。&lt;/p>
&lt;p>SIG Node 维护着许多组件，其中许多组件都依赖于外部项目或操作系统功能。这使得入职过程相当冗长和苛刻。
但如果你愿意接受挑战，总有一个地方适合你，也有一群人支持你。&lt;/p>
&lt;!--
### What do you do to help new contributors get started?
EH: Getting started in SIG Node can be intimidating, since there is so much work to be done, our SIG meetings are very large, and it can be hard to find a place to start.
I always encourage new contributors to work on things that they have some investment in already. In SIG Node, that might mean volunteering to help fix a bug that you have personally been affected by, or helping to triage bugs you care about by priority.
To come up to speed on any open source code base, there are two strategies you can take: start by exploring a particular issue deeply, and follow that to expand the edges of your knowledge as needed, or briefly review as many issues and change requests as you possibly can to get a higher level picture of how the component works. Ultimately, you will need to do both if you want to become a Node reviewer or approver.
[Davanum Srinivas](https://twitter.com/dims) and I each ran a cohort of group mentoring to help teach new contributors the skills to become Node reviewers, and if there's interest we can work to find a mentor to run another session. I also encourage new contributors to attend our Node CI Subproject meeting: it's a smaller audience and we don't record the triage sessions, so it can be a less intimidating way to get started with the SIG.
-->
&lt;h3 id="你是如何帮助新手贡献者开始工作的">你是如何帮助新手贡献者开始工作的？&lt;/h3>
&lt;p>EH：在 SIG Node 的起步工作可能是令人生畏的，因为有太多的工作要做，我们的 SIG 会议非常大，而且很难找到一个开始的地方。&lt;/p>
&lt;p>我总是鼓励新手贡献者在他们已经有一些投入的方向上更进一步。
在 SIG Node 中，这可能意味着自愿帮助修复一个只影响到你个人的 bug，或者按优先级去分流你关心的 bug。&lt;/p>
&lt;p>为了尽快了解任何开源代码库，你可以采取两种策略：从深入探索一个特定的问题开始，然后根据需要扩展你的知识边缘，或者单纯地尽可能多的审查 issues 和变更请求，以了解更高层次的组件工作方式。
最终，如果你想成为一名 Node reviewer 或 approver，两件事是不可避免的。&lt;/p>
&lt;p>&lt;a href="https://twitter.com/dims">Davanum Srinivas&lt;/a> 和我各自举办了一次小组辅导，以帮助教导新手贡献者成为 Node reviewer 的技能，如果有兴趣，我们可以努力寻找一个导师来举办另一次会议。
我也鼓励新手贡献者参加我们的 Node CI 子项目会议：它的听众较少，而且我们不记录分流会议，所以它可以是一个比较温和的方式来开始 SIG 之旅。&lt;/p>
&lt;!--
### Are there any particular skills you’d like to recruit for? What skills are contributors to SIG Usability likely to learn?
SK: SIG Node works on many workstreams in very different areas. All of these areas are on system level. For the typical code contributions you need to have a passion for building and utilizing low level APIs and writing performant and reliable components. Being a contributor you will learn how to debug and troubleshoot, profile, and monitor these components, as well as user workload that is run by these components. Often, with the limited to no access to Nodes, as they are running production workloads.
The other way of contribution is to help document SIG node features. This type of contribution requires a deep understanding of features, and ability to explain them in simple terms.
Finally, we are always looking for feedback on how best to run your workload. Come and explain specifics of it, and what features in SIG Node components may help to run it better.
-->
&lt;h3 id="有什么特别的技能者是你想招募的吗-对-sig-可用性的贡献者可能会学到什么技能">有什么特别的技能者是你想招募的吗？对 SIG 可用性的贡献者可能会学到什么技能？&lt;/h3>
&lt;p>SK：SIG Node 在大相径庭的领域从事许多工作流。所有这些领域都是系统级的。
对于典型的代码贡献，你需要对建立和善用低级别的 API 以及编写高性能和可靠的组件有热情。
作为一个贡献者，你将学习如何调试和排除故障，剖析和监控这些组件，以及由这些组件运行的用户工作负载。
通常情况下，由于节点正在运行生产工作负载，所以对节点的访问是有限的，甚至是没有的。&lt;/p>
&lt;p>另一种贡献方式是帮助记录 SIG Node 的功能。这种类型的贡献需要对功能有深刻的理解，并有能力用简单的术语解释它们。&lt;/p>
&lt;p>最后，我们一直在寻找关于如何最好地运行你的工作负载的反馈。来解释一下它的具体情况，以及 SIG Node 组件中的哪些功能可能有助于更好地运行它。&lt;/p>
&lt;!--
### What are you getting positive feedback on, and what’s coming up next for SIG Node?
EH: Over the past year SIG Node has adopted some new processes to help manage our feature development and Kubernetes enhancement proposals, and other SIGs have looked to us for inspiration in managing large workloads. I hope that this is an area we can continue to provide leadership in and further iterate on.
We have a great balance of new features and deprecations in flight right now. Deprecations of unused or difficult to maintain features help us keep technical debt and maintenance load under control, and examples include the dockershim and DynamicKubeletConfiguration deprecations. New features will unlock additional functionality in end users' clusters, and include exciting features like support for cgroups v2, swap memory, graceful node shutdowns, and device management policies.
-->
&lt;h3 id="你在哪些方面得到了积极的反馈-以及-sig-node-的下一步计划是什么">你在哪些方面得到了积极的反馈，以及 SIG Node 的下一步计划是什么？&lt;/h3>
&lt;p>EH：在过去的一年里，SIG Node 采用了一些新的流程来帮助管理我们的功能开发和 Kubernetes 增强提议，其他 SIG 也向我们寻求在管理大型工作负载方面的灵感。
我希望这是一个我们可以继续领导并进一步迭代的领域。&lt;/p>
&lt;p>现在，我们在新功能和废弃功能之间保持了很好的平衡。
废弃未使用或难以维护的功能有助于我们控制技术债务和维护负荷，例子包括 dockershim 和 DynamicKubeletConfiguration 的废弃。
新功能将在终端用户的集群中释放更多的功能，包括令人兴奋的功能，如支持 cgroups v2、交换内存、优雅的节点关闭和设备管理策略。&lt;/p>
&lt;!--
### Any closing thoughts/resources you’d like to share?
SK/EH: It takes time and effort to get to any open source community. SIG Node may overwhelm you at first with the number of participants, volume of work, and project scope. But it is totally worth it. Join our welcoming community! [SIG Node GitHub Repo](https://github.com/kubernetes/community/tree/master/sig-node) contains many useful resources including Slack, mailing list and other contact info.
-->
&lt;h3 id="最后你有什么想法-资源要分享吗">最后你有什么想法/资源要分享吗？&lt;/h3>
&lt;p>SK/EH：进入任何开源社区都需要时间和努力。一开始 SIG Node 可能会因为参与者的数量、工作量和项目范围而让你不知所措。但这是完全值得的。
请加入我们这个热情的社区! &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">SIG Node GitHub Repo&lt;/a>
包含许多有用的资源，包括 Slack、邮件列表和其他联系信息。&lt;/p>
&lt;!--
## Wrap Up
SIG Node hosted a [KubeCon + CloudNativeCon Europe 2021 talk](https://www.youtube.com/watch?v=z5aY4e2RENA) with an intro and deep dive to their awesome SIG. Join the SIG's meetings to find out about the most recent research results, what the plans are for the forthcoming year, and how to get involved in the upstream Node team as a contributor!
-->
&lt;h2 id="总结">总结&lt;/h2>
&lt;p>SIG Node 举办了一场 &lt;a href="https://www.youtube.com/watch?v=z5aY4e2RENA">KubeCon + CloudNativeCon Europe 2021 talk&lt;/a>，对他们强大的 SIG 进行了介绍和深入探讨。
加入 SIG 的会议，了解最新的研究成果，未来一年的计划是什么，以及如何作为贡献者参与到上游的 Node 团队中!&lt;/p></description></item><item><title>Blog: 更新 NGINX-Ingress 以使用稳定的 Ingress API</title><link>https://kubernetes.io/zh-cn/blog/2021/07/26/update-with-ingress-nginx/</link><pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/07/26/update-with-ingress-nginx/</guid><description>
&lt;!--
layout: blog
title: 'Updating NGINX-Ingress to use the stable Ingress API'
date: 2021-07-26
slug: update-with-ingress-nginx
-->
&lt;!--
**Authors:** James Strong, Ricardo Katz
-->
&lt;p>&lt;strong>作者：&lt;/strong> James Strong, Ricardo Katz&lt;/p>
&lt;!--
With all Kubernetes APIs, there is a process to creating, maintaining, and
ultimately deprecating them once they become GA. The networking.k8s.io API group is no
different. The upcoming Kubernetes 1.22 release will remove several deprecated APIs
that are relevant to networking:
-->
&lt;p>对于所有 Kubernetes API，一旦它们被正式发布（GA），就有一个创建、维护和最终弃用它们的过程。
networking.k8s.io API 组也不例外。
即将发布的 Kubernetes 1.22 版本将移除几个与网络相关的已弃用 API：&lt;/p>
&lt;!--
- the `networking.k8s.io/v1beta1` API version of [IngressClass](/docs/concepts/services-networking/ingress/#ingress-class)
- all beta versions of [Ingress](/docs/concepts/services-networking/ingress/): `extensions/v1beta1` and `networking.k8s.io/v1beta1`
-->
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/#ingress-class">IngressClass&lt;/a> 的 &lt;code>networking.k8s.io/v1beta1&lt;/code> API 版本&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/">Ingress&lt;/a> 的所有 Beta 版本: &lt;code>extensions/v1beta1&lt;/code> 和 &lt;code>networking.k8s.io/v1beta1&lt;/code>&lt;/li>
&lt;/ul>
&lt;!--
On a v1.22 Kubernetes cluster, you'll be able to access Ingress and IngressClass
objects through the stable (v1) APIs, but access via their beta APIs won't be possible.
-->
&lt;p>在 v1.22 Kubernetes 集群上，你能够通过稳定版本（v1）的 API 访问 Ingress 和 IngressClass 对象，
但无法通过其 Beta API 访问。&lt;/p>
&lt;!--
This change has been in
in discussion since
[2017](https://github.com/kubernetes/kubernetes/issues/43214),
[2019](https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/) with
1.16 Kubernetes API deprecations, and most recently in
KEP-1453:
[Graduate Ingress API to GA](https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/1453-ingress-api#122).
-->
&lt;p>自 &lt;a href="https://github.com/kubernetes/kubernetes/issues/43214">2017&lt;/a>、
&lt;a href="https://kubernetes.io/blog/2019/07/18/api-deprecations-in-1-16/">2019&lt;/a>
以来一直讨论关于 Kubernetes 1.16 弃用 API 的更改，
最近的讨论是在 KEP-1453：&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-network/1453-ingress-api#122">Ingress API 毕业到 GA&lt;/a>。&lt;/p>
&lt;!--
During community meetings, the networking Special Interest Group has decided to continue
supporting Kubernetes versions older than 1.22 with Ingress-NGINX version 0.47.0.
Support for Ingress-NGINX will continue for six months after Kubernetes 1.22
is released. Any additional bug fixes and CVEs for Ingress-NGINX will be
addressed on a need-by-need basis.
-->
&lt;p>在社区会议中，网络特别兴趣小组决定继续支持带有 0.47.0 版本 Ingress-NGINX 的早于 1.22 版本的 Kubernetes。
在 Kubernetes 1.22 发布后，对 Ingress-NGINX 的支持将持续六个月。
团队会根据需要解决 Ingress-NGINX 的额外错误修复和 CVE 问题。&lt;/p>
&lt;!--
Ingress-NGINX will have separate branches and releases of Ingress-NGINX to
support this model, mirroring the Kubernetes project process. Future
releases of the Ingress-NGINX project will track and support the latest
versions of Kubernetes.
-->
&lt;p>Ingress-NGINX 将拥有独立的分支和发布版本来支持这个模型，与 Kubernetes 项目流程相一致。
Ingress-NGINX 项目的未来版本将跟踪和支持最新版本的 Kubernetes。&lt;/p>
&lt;!--
&lt;table>&lt;caption style="display: none;">Ingress NGINX supported version with Kubernetes Versions&lt;/caption>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Kubernetes 版本&lt;/th>
&lt;th style="text-align:left">Ingress-NGINX version&lt;/th>
&lt;th style="text-align:left">Notes&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">v1.22&lt;/td>
&lt;td style="text-align:left">v1.0.0-alpha.2&lt;/td>
&lt;td style="text-align:left">New features, plus bug fixes.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.21&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. No end-of-support date announced.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.20&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. No end-of-support date announced.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.19&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">Bugfixes only, and just for security issues or crashes. Fixes only provided until 6 months after Kubernetes v1.22.0 is released.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
-->
&lt;table>&lt;caption style="display: none;">Kubernetes 各版本支持的 Ingress NGINX 版本&lt;/caption>
&lt;thead>
&lt;tr>
&lt;th style="text-align:left">Kubernetes 版本&lt;/th>
&lt;th style="text-align:left">Ingress-NGINX 版本&lt;/th>
&lt;th style="text-align:left">公告&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:left">v1.22&lt;/td>
&lt;td style="text-align:left">v1.0.0-alpha.2&lt;/td>
&lt;td style="text-align:left">新特性，以及错误修复。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.21&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">仅修复安全问题或系统崩溃的错误。没有宣布终止支持日期。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.20&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">仅修复安全问题或系统崩溃的错误。没有宣布终止支持日期。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="text-align:left">v1.19&lt;/td>
&lt;td style="text-align:left">v0.47.x&lt;/td>
&lt;td style="text-align:left">仅修复安全问题或系统崩溃的错误。仅在 Kubernetes v1.22.0 发布后的 6 个月内提供修复支持。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!--
Because of the updates in Kubernetes 1.22, **v0.47.0** will not work with
Kubernetes 1.22.
-->
&lt;p>由于 Kubernetes 1.22 中的更新，&lt;strong>v0.47.0&lt;/strong> 将无法与 Kubernetes 1.22 一起使用。&lt;/p>
&lt;!--
# What you need to do
-->
&lt;h2 id="你需要做什么">你需要做什么&lt;/h2>
&lt;!--
The team is currently in the process of upgrading ingress-nginx to support
the v1 migration, you can track the progress
[here](https://github.com/kubernetes/ingress-nginx/pull/7156).
We're not making feature improvements to `ingress-nginx` until after the support for
Ingress v1 is complete.
-->
&lt;p>团队目前正在升级 Ingress-NGINX 以支持向 v1 的迁移，
你可以在&lt;a href="https://github.com/kubernetes/ingress-nginx/pull/7156">此处&lt;/a>跟踪进度。
在对 Ingress v1 的支持完成之前，
我们不会对功能进行改进。&lt;/p>
&lt;!--
In the meantime to ensure no compatibility issues:
-->
&lt;p>同时，团队会确保没有兼容性问题：&lt;/p>
&lt;!--
* Update to the latest version of Ingress-NGINX; currently
[v0.47.0](https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v0.47.0)
-->
&lt;ul>
&lt;li>更新到最新的 Ingress-NGINX 版本，
目前是 &lt;a href="https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v0.47.0">v0.47.0&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
* After Kubernetes 1.22 is released, ensure you are using the latest version of
Ingress-NGINX that supports the stable APIs for Ingress and IngressClass.
-->
&lt;ul>
&lt;li>Kubernetes 1.22 发布后，请确保使用的是支持 Ingress 和 IngressClass 稳定 API 的最新版本的 Ingress-NGINX。&lt;/li>
&lt;/ul>
&lt;!--
* Test Ingress-NGINX version v1.0.0-alpha.2 with Cluster versions >= 1.19
and report any issues to the projects Github page.
-->
&lt;ul>
&lt;li>使用集群版本 &amp;gt;= 1.19 测试 Ingress-NGINX 版本 v1.0.0-alpha.2，并将任何问题报告给项目 GitHub 页面。&lt;/li>
&lt;/ul>
&lt;!--
The community’s feedback and support in this effort is welcome. The
Ingress-NGINX Sub-project regularly holds community meetings where we discuss
this and other issues facing the project. For more information on the sub-project,
please see [SIG Network](https://github.com/kubernetes/community/tree/master/sig-network).
-->
&lt;p>欢迎社区对此工作的反馈和支持。
Ingress-NGINX 子项目定期举行社区会议，
我们会讨论这个问题以及项目面临的其他问题。
有关子项目的更多信息，请参阅 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network&lt;/a>。&lt;/p></description></item><item><title>Blog: 聚焦 SIG Usability</title><link>https://kubernetes.io/zh-cn/blog/2021/07/15/sig-usability-spotlight-2021/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/07/15/sig-usability-spotlight-2021/</guid><description>
&lt;!--
layout: blog
title: "Spotlight on SIG Usability"
date: 2021-07-15
slug: sig-usability-spotlight-2021
-->
&lt;!--
**Author:** Kunal Kushwaha, Civo
-->
&lt;p>&lt;strong>作者：&lt;/strong> Kunal Kushwaha、Civo&lt;/p>
&lt;!--
## Introduction
-->
&lt;h2 id="介绍">介绍&lt;/h2>
&lt;!--
Are you interested in learning about what [SIG Usability](https://github.com/kubernetes/community/tree/master/sig-usability) does
and how you can get involved? Well, you're at the right place.
SIG Usability is all about making Kubernetes more accessible to new folks, and its main activity is conducting user research for the community.
In this blog, we have summarized our conversation with [Gaby Moreno](https://twitter.com/morengab),
who walks us through the various aspects of being a part of the SIG and shares some insights about how others can get involved.
-->
&lt;p>你是否有兴趣了解 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-usability">SIG Usability&lt;/a> 做什么？
你是否想知道如何参与？那你来对地方了。
SIG Usability 旨在让 Kubernetes 更易于触达新的伙伴，其主要活动是针对社区实施用户调研。
在本博客中，我们总结了与 Gaby Moreno 的对话，
他向我们介绍了成为 SIG 成员的各个方面，并分享了一些关于其他人如何参与的见解。&lt;/p>
&lt;!--
Gaby is a co-lead for SIG Usability.
She works as a Product Designer at IBM and enjoys working on the user experience of open,
hybrid cloud technologies like Kubernetes, OpenShift, Terraform, and Cloud Foundry.
-->
&lt;p>Gaby 是 SIG Usability 的联合负责人。
她在 IBM 担任产品设计师，
喜欢研究 Kubernetes、OpenShift、Terraform 和 Cloud Foundry 等开放式混合云技术的用户体验。&lt;/p>
&lt;!--
## A summary of our conversation
-->
&lt;h2 id="我们谈话的摘要">我们谈话的摘要&lt;/h2>
&lt;!--
### Q. Could you tell us a little about what SIG Usability does?
-->
&lt;h3 id="问-你能告诉我们一些关于-sig-usability-的事情吗">问：你能告诉我们一些关于 SIG Usability 的事情吗？&lt;/h3>
&lt;!--
A. SIG Usability at a high level started because there was no dedicated user experience team for Kubernetes.
The extent of SIG Usability is focussed on the end-client ease of use of the Kubernetes project.
The main activity is user research for the community, which includes speaking to Kubernetes users.
-->
&lt;p>答：简单而言，启动 SIG Usability 的原因是那时 Kubernetes 没有专门的用户体验团队。
SIG Usability 的关注领域集中在为 Kubernetes 项目最终客户提供的易用性上。
主要活动是社区的用户调研，包括对 Kubernetes 用户宣讲。&lt;/p>
&lt;!--
This covers points like user experience and accessibility.
The objectives of the SIG are to guarantee that the Kubernetes project is maximally usable by people of a wide range of foundations and capacities,
such as incorporating internationalization and ensuring the openness of documentation.
-->
&lt;p>所涉及的包括用户体验和可访问性等方面。
SIG 的目标是确保 Kubernetes 项目能够最大限度地被具有各类不同基础和能力的人使用，
例如引入文档的国际化并确保其开放性。&lt;/p>
&lt;!--
### Q. Why should new and existing contributors consider joining SIG Usability?
-->
&lt;h3 id="问-为什么新的和现有的贡献者应该考虑加入-sig-usability">问：为什么新的和现有的贡献者应该考虑加入 SIG Usability？&lt;/h3>
&lt;!--
A. There are plenty of territories where new contributors can begin. For example:
-->
&lt;p>答：新的贡献者可以在很多领域着手。例如：&lt;/p>
&lt;!--
- User research projects, where people can help understand the usability of the end-user experiences, including error messages, end-to-end tasks, etc.
-->
&lt;ul>
&lt;li>用户研究项目可以让人们帮助了解最终用户体验的可用性，包括错误消息、端到端任务等。&lt;/li>
&lt;/ul>
&lt;!--
- Accessibility guidelines for Kubernetes community artifacts, examples include:
internationalization of documentation, color choices for people with color blindness, ensuring compatibility with screen reader technology,
user interface design for core components with user interfaces, and more.
-->
&lt;ul>
&lt;li>Kubernetes 社区组件的可访问性指南，包括：文档的国际化、色盲人群的颜色选择、
确保与屏幕阅读器技术的兼容性、核心 UI 组件的用户界面设计等等。&lt;/li>
&lt;/ul>
&lt;!--
### Q. What do you do to help new contributors get started?
-->
&lt;h3 id="问-如何帮助新的贡献者入门">问：如何帮助新的贡献者入门？&lt;/h3>
&lt;!--
A. New contributors can get started by shadowing one of the user interviews, going through user interview transcripts, analyzing them, and designing surveys.
-->
&lt;p>答：新的贡献者们刚开始可以旁观参与其中一个用户访谈，浏览用户访谈记录，分析这些记录并设计调查过程。&lt;/p>
&lt;!--
SIG Usability is also open to new project ideas.
If you have an idea, we’ll do what we can to support it.
There are regular SIG Meetings where people can ask their questions live.
These meetings are also recorded for those who may not be able to attend.
As always, you can reach out to us on Slack as well.
-->
&lt;p>SIG Usability 也对新的项目想法持开放态度。
如果你有想法，我们将尽我们所能支持它。
我们有定期的 SIG 会议，人们可以现场提问。
这些会议也会录制会议视频，方便那些可能无法参会的人。
与往常一样，你也可以在 Slack 上与我们联系。&lt;/p>
&lt;!--
### Q. What does the survey include?
-->
&lt;h3 id="问-调查包括什么">问：调查包括什么？&lt;/h3>
&lt;!--
A. In simple terms, the survey gathers information about how people use Kubernetes,
such as trends in learning to deploy a new system, error messages they receive, and workflows.
-->
&lt;p>答：简单来说，调查会收集人们如何使用 Kubernetes 的信息，
例如学习部署新系统的趋势、他们收到的错误消息和工作流程。&lt;/p>
&lt;!--
One of our goals is to standardize the responses accordingly.
The ultimate goal is to analyze survey responses for important user stories whose needs aren't being met.
-->
&lt;p>我们的目标之一是根据需要对反馈进行标准化。
最终目标是分析那些需求没有得到满足的重要用户故事的调查反馈。&lt;/p>
&lt;!--
### Q. Are there any particular skills you’d like to recruit for? What skills are contributors to SIG Usability likely to learn?
-->
&lt;h3 id="问-招募贡献者时你希望他们具备什么特别的技能吗-sig-usability-的贡献者可能要学习哪些技能">问：招募贡献者时你希望他们具备什么特别的技能吗？SIG Usability 的贡献者可能要学习哪些技能？&lt;/h3>
&lt;!--
A. Although contributing to SIG Usability does not have any pre-requisites as such,
experience with user research, qualitative research, or prior experience with how to conduct an interview would be great plus points.
Quantitative research, like survey design and screening, is also helpful and something that we expect contributors to learn.
-->
&lt;p>答：虽然为 SIG Usability 做贡献没有任何先决条件，
但用户研究、定性研究的经验或之前如何进行访谈的经验将是很好的加分项。
定量研究，如调查设计和筛选，也很有帮助，也是我们希望贡献者学习的东西。&lt;/p>
&lt;!--
### Q. What are you getting positive feedback on, and what’s coming up next for SIG Usability?
-->
&lt;h3 id="问-您在哪些方面获得了积极的反馈-以及-sig-usability-接下来会发生什么">问：您在哪些方面获得了积极的反馈，以及 SIG Usability 接下来会发生什么？&lt;/h3>
&lt;!--
A. We have had new members joining and coming to monthly meetings regularly and showing interests in becoming a contributor and helping the community.
We have also had a lot of people reach out to us via Slack showcasing their interest in the SIG.
-->
&lt;p>答：我们一直有新成员加入并经常参加月度会议，并表现出对成为贡献者和帮助社区的兴趣。
我们也有很多人通过 Slack 与我们联系，表达他们对 SIG 的兴趣。&lt;/p>
&lt;!--
Currently, we are focused on finishing the study mentioned in our [talk](https://www.youtube.com/watch?v=Byn0N_ZstE0),
also our project for this year. We are always happy to have new contributors join us.
-->
&lt;p>目前，我们正专注于完成我们&lt;a href="https://www.youtube.com/watch?v=Byn0N_ZstE0">演讲&lt;/a>中提到的调研，
也是我们今年的项目。我们总是很高兴有新的贡献者加入我们。&lt;/p>
&lt;!--
### Q: Any closing thoughts/resources you’d like to share?
-->
&lt;h3 id="问-在结束之前-你还有什么想法-资源要分享吗">问：在结束之前，你还有什么想法/资源要分享吗？&lt;/h3>
&lt;!--
A. We love meeting new contributors and assisting them in investigating different Kubernetes project spaces.
We will work with and team up with other SIGs to facilitate engaging with end-users, running studies,
and help them integrate accessible design practices into their development practices.
-->
&lt;p>答：我们喜欢结识新的贡献者并帮助他们研究不同的 Kubernetes 项目领域。
我们将与其他 SIG 合作，以促进与最终用户的互动，开展调研，并帮助他们将可访问的设计实践整合到他们的开发实践中。&lt;/p>
&lt;!--
Here are some resources for you to get started:
- [GitHub](https://github.com/kubernetes/community/tree/master/sig-usability)
- [Mailing list](https://groups.google.com/g/kubernetes-sig-usability)
- [Open Community Issues/PRs](https://github.com/kubernetes/community/labels/sig%2Fusability)
- [Slack](https://slack.k8s.io/)
- [Slack channel #sig-usability](https://kubernetes.slack.com/archives/CLC5EF63T)
-->
&lt;p>这里有一些资源供你入门：&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-usability">GitHub&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/g/kubernetes-sig-usability">邮件列表&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fusability">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.slack.com/archives/CLC5EF63T">Slack 频道 #sig-usability&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Wrap Up
-->
&lt;h2 id="总结">总结&lt;/h2>
&lt;!--
SIG Usability hosted a [KubeCon talk](https://www.youtube.com/watch?v=Byn0N_ZstE0) about studying Kubernetes users' experiences.
The talk focuses on updates to the user study projects, understanding who is using Kubernetes,
what they are trying to achieve, how the project is addressing their needs, and where we need to improve the project and the client experience.
Join the SIG's update to find out about the most recent research results,
what the plans are for the forthcoming year, and how to get involved in the upstream usability team as a contributor!
-->
&lt;p>SIG Usability 举办了一个关于调研 Kubernetes 用户体验的 &lt;a href="https://www.youtube.com/watch?v=Byn0N_ZstE0">KubeCon 演讲&lt;/a>。
演讲的重点是用户调研项目的更新，了解谁在使用 Kubernetes、他们试图实现什么、项目如何满足他们的需求、以及我们需要改进项目和客户体验的地方。
欢迎加入 SIG 的更新，了解最新的调研成果、来年的计划以及如何作为贡献者参与上游可用性团队！&lt;/p></description></item><item><title>Blog: 卷健康监控的 Alpha 更新</title><link>https://kubernetes.io/zh-cn/blog/2021/04/16/volume-health-monitoring-alpha-update/</link><pubDate>Fri, 16 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/04/16/volume-health-monitoring-alpha-update/</guid><description>
&lt;!--
layout: blog
title: "Volume Health Monitoring Alpha Update"
date: 2021-04-16
slug: volume-health-monitoring-alpha-update
-->
&lt;!--
**Author:** Xing Yang (VMware)
-->
&lt;p>&lt;strong>作者：&lt;/strong> Xing Yang (VMware)&lt;/p>
&lt;!--
The CSI Volume Health Monitoring feature, originally introduced in 1.19 has undergone a large update for the 1.21 release.
-->
&lt;p>最初在 1.19 中引入的 CSI 卷健康监控功能在 1.21 版本中进行了大规模更新。&lt;/p>
&lt;!--
## Why add Volume Health Monitoring to Kubernetes?
-->
&lt;h2 id="为什么要向-kubernetes-添加卷健康监控">为什么要向 Kubernetes 添加卷健康监控？&lt;/h2>
&lt;!--
Without Volume Health Monitoring, Kubernetes has no knowledge of the state of the underlying volumes of a storage system after a PVC is provisioned and used by a Pod. Many things could happen to the underlying storage system after a volume is provisioned in Kubernetes. For example, the volume could be deleted by accident outside of Kubernetes, the disk that the volume resides on could fail, it could be out of capacity, the disk may be degraded which affects its performance, and so on. Even when the volume is mounted on a pod and used by an application, there could be problems later on such as read/write I/O errors, file system corruption, accidental unmounting of the volume outside of Kubernetes, etc. It is very hard to debug and detect root causes when something happened like this.
-->
&lt;p>如果没有卷健康监控，在 PVC 被 Pod 配置和使用后，Kubernetes 将不知道存储系统的底层卷的状态。
在 Kubernetes 中配置卷后，底层存储系统可能会发生很多事情。
例如，卷可能在 Kubernetes 之外被意外删除、卷所在的磁盘可能发生故障、容量不足、磁盘可能被降级而影响其性能等等。
即使卷被挂载到 Pod 上并被应用程序使用，以后也可能会出现诸如读/写 I/O 错误、文件系统损坏、在 Kubernetes 之外被意外卸载卷等问题。
当发生这样的事情时，调试和检测根本原因是非常困难的。&lt;/p>
&lt;!--
Volume health monitoring can be very beneficial to Kubernetes users. It can communicate with the CSI driver to retrieve errors detected by the underlying storage system. PVC events can be reported up to the user to take action. For example, if the volume is out of capacity, they could request a volume expansion to get more space.
-->
&lt;p>卷健康监控对 Kubernetes 用户非常有益。
它可以与 CSI 驱动程序通信以检索到底层存储系统检测到的错误。
用户可以收到报告上来的 PVC 事件继而采取行动。
例如，如果卷容量不足，他们可以请求卷扩展以获得更多空间。&lt;/p>
&lt;!--
## What is Volume Health Monitoring?
-->
&lt;h2 id="什么是卷健康监控">什么是卷健康监控？&lt;/h2>
&lt;!--
CSI Volume Health Monitoring allows CSI Drivers to detect abnormal volume conditions from the underlying storage systems and report them as events on PVCs or Pods.
-->
&lt;p>CSI 卷健康监控允许 CSI 驱动程序检测来自底层存储系统的异常卷状况，并将其作为 PVC 或 Pod 上的事件报送。&lt;/p>
&lt;!--
The Kubernetes components that monitor the volumes and report events with volume health information include the following:
-->
&lt;p>监控卷和使用卷健康信息报送事件的 Kubernetes 组件包括：&lt;/p>
&lt;!--
* Kubelet, in addition to gathering the existing volume stats will watch the volume health of the PVCs on that node. If a PVC has an abnormal health condition, an event will be reported on the pod object using the PVC. If multiple pods are using the same PVC, events will be reported on all pods using that PVC.
-->
&lt;ul>
&lt;li>Kubelet 除了收集现有的卷统计信息外，还将观察该节点上 PVC 的卷健康状况。
如果 PVC 的健康状况异常，则会在使用 PVC 的 Pod 对象上报送事件。
如果多个 Pod 使用相同的 PVC，则将在使用该 PVC 的所有 Pod 上报送事件。&lt;/li>
&lt;/ul>
&lt;!--
* An [External Volume Health Monitor Controller](https://github.com/kubernetes-csi/external-health-monitor) watches volume health of the PVCs and reports events on the PVCs.
-->
&lt;ul>
&lt;li>一个&lt;a href="https://github.com/kubernetes-csi/external-health-monitor">外部卷健康监视控制器&lt;/a>监视 PVC 的卷健康并报告 PVC 上的事件。&lt;/li>
&lt;/ul>
&lt;!--
Note that the node side volume health monitoring logic was an external agent when this feature was first introduced in the Kubernetes 1.19 release. In Kubernetes 1.21, the node side volume health monitoring logic was moved from the external agent into the Kubelet, to avoid making duplicate CSI function calls. With this change in 1.21, a new alpha [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) `CSIVolumeHealth` was introduced for the volume health monitoring logic in Kubelet.
-->
&lt;p>请注意，在 Kubernetes 1.19 版本中首次引入此功能时，节点侧卷健康监控逻辑是一个外部代理。
在 Kubernetes 1.21 中，节点侧卷健康监控逻辑从外部代理移至 Kubelet，以避免 CSI 函数重复调用。
随着 1.21 中的这一变化，为 Kubelet 中的卷健康监控逻辑引入了一个新的 alpha &lt;a href="https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/feature-gates/">特性门&lt;/a> &lt;code>CSIVolumeHealth&lt;/code>。&lt;/p>
&lt;!--
Currently the Volume Health Monitoring feature is informational only as it only reports abnormal volume health events on PVCs or Pods. Users will need to check these events and manually fix the problems. This feature serves as a stepping stone towards programmatic detection and resolution of volume health issues by Kubernetes in the future.
-->
&lt;p>目前，卷健康监控功能仅供参考，因为它只报送 PVC 或 Pod 上的异常卷健康事件。
用户将需要检查这些事件并手动修复问题。
此功能可作为 Kubernetes 未来以编程方式检测和解决卷健康问题的基石。&lt;/p>
&lt;!--
## How do I use Volume Health on my Kubernetes Cluster?
-->
&lt;h2 id="如何在-kubernetes-集群上使用卷健康">如何在 Kubernetes 集群上使用卷健康？&lt;/h2>
&lt;!--
To use the Volume Health feature, first make sure the CSI driver you are using supports this feature. Refer to this [CSI drivers doc](https://kubernetes-csi.github.io/docs/drivers.html) to find out which CSI drivers support this feature.
-->
&lt;p>要使用卷健康功能，首先确保你使用的 CSI 驱动程序支持此功能。
请参阅此 &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">CSI 驱动程序文档&lt;/a>以了解哪些 CSI 驱动程序支持此功能。&lt;/p>
&lt;!--
To enable Volume Health Monitoring from the node side, the alpha feature gate `CSIVolumeHealth` needs to be enabled.
-->
&lt;p>要从节点侧启用卷健康监控，需要启用 alpha 特性门 &lt;code>CSIVolumeHealth&lt;/code>。&lt;/p>
&lt;!--
If a CSI driver supports the Volume Health Monitoring feature from the controller side, events regarding abnormal volume conditions will be recorded on PVCs.
-->
&lt;p>如果 CSI 驱动程序支持控制器端的卷健康监控功能，则有关异常卷条件的事件将记录在 PVC 上。&lt;/p>
&lt;!--
If a CSI driver supports the Volume Health Monitoring feature from the controller side, user can also get events regarding node failures if the `enable-node-watcher` flag is set to true when deploying the External Health Monitor Controller. When a node failure event is detected, an event will be reported on the PVC to indicate that pods using this PVC are on a failed node.
-->
&lt;p>如果 CSI 驱动程序支持控制器端的卷健康监控功能，
当部署外部健康监控控制器时 &lt;code>enable-node-watcher&lt;/code> 标志设置为 true，用户还可以获得有关节点故障的事件。
当检测到节点故障事件时，会在 PVC 上报送一个事件，指示使用该 PVC 的 Pod 在故障节点上。&lt;/p>
&lt;!--
If a CSI driver supports the Volume Health Monitoring feature from the node side, events regarding abnormal volume conditions will be recorded on pods using the PVCs.
-->
&lt;p>如果 CSI 驱动程序支持节点端的卷健康监控功能，则有关异常卷条件的事件将使用 PVC 记录在 Pod 上。&lt;/p>
&lt;!--
## As a storage vendor, how do I add support for volume health to my CSI driver?
-->
&lt;h2 id="作为存储供应商-如何向-csi-驱动程序添加对卷健康的支持">作为存储供应商，如何向 CSI 驱动程序添加对卷健康的支持？&lt;/h2>
&lt;!--
Volume Health Monitoring includes two parts:
* An External Volume Health Monitoring Controller monitors volume health from the controller side.
* Kubelet monitors volume health from the node side.
-->
&lt;p>卷健康监控包括两个部分：&lt;/p>
&lt;ul>
&lt;li>外部卷健康监控控制器从控制器端监控卷健康。&lt;/li>
&lt;li>Kubelet 从节点端监控卷的健康状况。&lt;/li>
&lt;/ul>
&lt;!--
For details, see the [CSI spec](https://github.com/container-storage-interface/spec/blob/master/spec.md) and the [Kubernetes-CSI Driver Developer Guide](https://kubernetes-csi.github.io/docs/volume-health-monitor.html).
-->
&lt;p>有关详细信息，请参阅 &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI 规约&lt;/a>
和 &lt;a href="https://kubernetes-csi.github.io/docs/volume-health-monitor.html">Kubernetes-CSI 驱动开发者指南&lt;/a>。&lt;/p>
&lt;!--
There is a sample implementation for volume health in [CSI host path driver](https://github.com/kubernetes-csi/csi-driver-host-path).
-->
&lt;p>&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path">CSI 主机路径驱动程序&lt;/a>中有一个卷健康的示例实现。&lt;/p>
&lt;!--
### Controller Side Volume Health Monitoring
-->
&lt;h3 id="控制器端卷健康监控">控制器端卷健康监控&lt;/h3>
&lt;!--
To learn how to deploy the External Volume Health Monitoring controller, see [CSI external-health-monitor-controller](https://kubernetes-csi.github.io/docs/external-health-monitor-controller.html) in the CSI documentation.
-->
&lt;p>要了解如何部署外部卷健康监控控制器，
请参阅 CSI 文档中的 &lt;a href="https://kubernetes-csi.github.io/docs/external-health-monitor-controller.html">CSI external-health-monitor-controller&lt;/a>。&lt;/p>
&lt;!--
The External Health Monitor Controller calls either `ListVolumes` or `ControllerGetVolume` CSI RPC and reports VolumeConditionAbnormal events with messages on PVCs if abnormal volume conditions are detected. Only CSI drivers with `LIST_VOLUMES` and `VOLUME_CONDITION` controller capability or `GET_VOLUME` and `VOLUME_CONDITION` controller capability support Volume Health Monitoring in the external controller.
-->
&lt;p>如果检测到异常卷条件，
外部健康监视器控制器调用 &lt;code>ListVolumes&lt;/code> 或者 &lt;code>ControllerGetVolume&lt;/code> CSI RPC 并报送 VolumeConditionAbnormal 事件以及 PVC 上的消息。
只有具有 &lt;code>LIST_VOLUMES&lt;/code> 和 &lt;code>VOLUME_CONDITION&lt;/code> 控制器能力、
或者具有 &lt;code>GET_VOLUME&lt;/code> 和 &lt;code>VOLUME_CONDITION&lt;/code> 能力的 CSI 驱动程序才支持外部控制器中的卷健康监控。&lt;/p>
&lt;!--
To implement the volume health feature from the controller side, a CSI driver **must** add support for the new controller capabilities.
-->
&lt;p>要从控制器端实现卷健康功能，CSI 驱动程序&lt;strong>必须&lt;/strong>添加对新控制器功能的支持。&lt;/p>
&lt;!--
If a CSI driver supports `LIST_VOLUMES` and `VOLUME_CONDITION` controller capabilities, it **must** implement controller RPC `ListVolumes` and report the volume condition in the response.
-->
&lt;p>如果 CSI 驱动程序支持 &lt;code>LIST_VOLUMES&lt;/code> 和 &lt;code>VOLUME_CONDITION&lt;/code> 控制器功能，它&lt;strong>必须&lt;/strong>实现控制器 RPC &lt;code>ListVolumes&lt;/code> 并在响应中报送卷状况。&lt;/p>
&lt;!--
If a CSI driver supports `GET_VOLUME` and `VOLUME_CONDITION` controller capability, it **must** implement controller PRC `ControllerGetVolume` and report the volume condition in the response.
-->
&lt;p>如果 CSI 驱动程序支持 &lt;code>GET_VOLUME&lt;/code> 和 &lt;code>VOLUME_CONDITION&lt;/code> 控制器功能，它&lt;strong>必须&lt;/strong>实现控制器 PRC &lt;code>ControllerGetVolume&lt;/code> 并在响应中报送卷状况。&lt;/p>
&lt;!--
If a CSI driver supports `LIST_VOLUMES`, `GET_VOLUME`, and `VOLUME_CONDITION` controller capabilities, only `ListVolumes` CSI RPC will be invoked by the External Health Monitor Controller.
-->
&lt;p>如果 CSI 驱动程序支持 &lt;code>LIST_VOLUMES&lt;/code>、&lt;code>GET_VOLUME&lt;/code> 和 &lt;code>VOLUME_CONDITION&lt;/code> 控制器功能，则外部健康监视控制器将仅调用 &lt;code>ListVolumes&lt;/code> CSI RPC。&lt;/p>
&lt;!--
### Node Side Volume Health Monitoring
-->
&lt;h3 id="节点侧卷健康监控">节点侧卷健康监控&lt;/h3>
&lt;!--
Kubelet calls `NodeGetVolumeStats` CSI RPC and reports VolumeConditionAbnormal events with messages on Pods if abnormal volume conditions are detected. Only CSI drivers with `VOLUME_CONDITION` node capability support Volume Health Monitoring in Kubelet.
-->
&lt;p>如果检测到异常的卷条件，
Kubelet 会调用 &lt;code>NodeGetVolumeStats&lt;/code> CSI RPC 并报送 VolumeConditionAbnormal 事件以及 Pod 上的信息。
只有具有 &lt;code>VOLUME_CONDITION&lt;/code> 节点功能的 CSI 驱动程序才支持 Kubelet 中的卷健康监控。&lt;/p>
&lt;!--
To implement the volume health feature from the node side, a CSI driver **must** add support for the new node capabilities.
-->
&lt;p>要从节点端实现卷健康功能，CSI 驱动程序&lt;strong>必须&lt;/strong>添加对新节点功能的支持。&lt;/p>
&lt;!--
If a CSI driver supports `VOLUME_CONDITION` node capability, it **must** report the volume condition in node RPC `NodeGetVoumeStats`.
-->
&lt;p>如果 CSI 驱动程序支持 &lt;code>VOLUME_CONDITION&lt;/code> 节点能力，它&lt;strong>必须&lt;/strong>在节点 RPC &lt;code>NodeGetVoumeStats&lt;/code> 中报送卷状况。&lt;/p>
&lt;!--
## What’s next?
-->
&lt;h2 id="下一步是什么">下一步是什么？&lt;/h2>
&lt;!--
Depending on feedback and adoption, the Kubernetes team plans to push the CSI volume health implementation to beta in either 1.22 or 1.23.
-->
&lt;p>根据反馈和采纳情况，Kubernetes 团队计划在 1.22 或 1.23 中将 CSI 卷健康实施推向 beta。&lt;/p>
&lt;!--
We are also exploring how to use volume health information for programmatic detection and automatic reconcile in Kubernetes.
-->
&lt;p>我们还在探索如何在 Kubernetes 中使用卷健康信息进行编程检测和自动协调。&lt;/p>
&lt;!--
## How can I learn more?
-->
&lt;h2 id="如何了解更多">如何了解更多？&lt;/h2>
&lt;!--
To learn the design details for Volume Health Monitoring, read the [Volume Health Monitor](https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor) enhancement proposal.
-->
&lt;p>要了解卷健康监控的设计细节，请阅读&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor">卷健康监控&lt;/a>增强提案。&lt;/p>
&lt;!--
The Volume Health Monitor controller source code is at [https://github.com/kubernetes-csi/external-health-monitor](https://github.com/kubernetes-csi/external-health-monitor).
-->
&lt;p>卷健康检测控制器源代码位于：
&lt;a href="https://github.com/kubernetes-csi/external-health-monitor">https://github.com/kubernetes-csi/external-health-monitor&lt;/a>。&lt;/p>
&lt;!--
There are also more details about volume health checks in the [Container Storage Interface Documentation](https://kubernetes-csi.github.io/docs/).
-->
&lt;p>&lt;a href="https://kubernetes-csi.github.io/docs/">容器存储接口文档&lt;/a>中还有关于卷健康检查的更多详细信息。&lt;/p>
&lt;!--
## How do I get involved?
-->
&lt;h2 id="如何参与">如何参与？&lt;/h2>
&lt;!--
The [Kubernetes Slack channel #csi](https://kubernetes.slack.com/messages/csi) and any of the [standard SIG Storage communication channels](https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact) are great mediums to reach out to the SIG Storage and the CSI team.
-->
&lt;p>&lt;a href="https://kubernetes.slack.com/messages/csi">Kubernetes Slack 频道 #csi&lt;/a>
和任何&lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">标准 SIG Storage 通信频道&lt;/a>都是联系 SIG Storage 和 CSI 团队的绝佳媒介。&lt;/p>
&lt;!--
We offer a huge thank you to the contributors who helped release this feature in 1.21. We want to thank Yuquan Ren ([NickrenREN](https://github.com/nickrenren)) who implemented the initial volume health monitor controller and agent in the external health monitor repo, thank Ran Xu ([fengzixu](https://github.com/fengzixu)) who moved the volume health monitoring logic from the external agent to Kubelet in 1.21, and we offer special thanks to the following people for their insightful reviews: David Ashpole ([dashpole](https://github.com/dashpole)), Michelle Au ([msau42](https://github.com/msau42)), David Eads ([deads2k](https://github.com/deads2k)), Elana Hashman ([ehashman](https://github.com/ehashman)), Seth Jennings ([sjenning](https://github.com/sjenning)), and Jiawei Wang ([Jiawei0227](https://github.com/Jiawei0227)).
-->
&lt;p>我们非常感谢在 1.21 中帮助发布此功能的贡献者。
我们要感谢 Yuquan Ren (&lt;a href="https://github.com/nickrenren">NickrenREN&lt;/a>) 在外部健康监控仓库中实现了初始卷健康监控控制器和代理，
感谢 Ran Xu (&lt;a href="https://github.com/fengzixu">fengzixu&lt;/a>) 在 1.21 中将卷健康监控逻辑从外部代理转移到 Kubelet，
我们特别感谢以下人员的深刻评论：
David Ashpole (&lt;a href="https://github.com/dashpole">dashpole&lt;/a>)、
Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>)、
David Eads (&lt;a href="https://github.com/deads2k">deads2k&lt;/a>)、
Elana Hashman (&lt;a href="https://github.com/ehashman">ehashman&lt;/a>)、
Seth Jennings (&lt;a href="https://github.com/sjenning">sjenning&lt;/a>) 和 Jiawei Wang (&lt;a href="https://github.com/Jiawei0227">Jiawei0227&lt;/a>)&lt;/p>
&lt;!--
Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the [Kubernetes Storage Special Interest Group](https://github.com/kubernetes/community/tree/master/sig-storage) (SIG). We’re rapidly growing and always welcome new contributors.
-->
&lt;p>那些有兴趣参与 CSI 或 Kubernetes 存储系统任何部分的设计和开发的人，
请加入 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage 特殊兴趣小组&lt;/a>（SIG）。
我们正在迅速发展，并且欢迎新的贡献者。&lt;/p></description></item><item><title>Blog: 弃用 PodSecurityPolicy：过去、现在、未来</title><link>https://kubernetes.io/zh-cn/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</link><pubDate>Tue, 06 Apr 2021 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/</guid><description>
&lt;!--
title: "PodSecurityPolicy Deprecation: Past, Present, and Future"
-->
&lt;!--
**Author:** Tabitha Sable (Kubernetes SIG Security)
-->
&lt;p>作者：Tabitha Sable（Kubernetes SIG Security）&lt;/p>
&lt;!--
PodSecurityPolicy (PSP) is being deprecated in Kubernetes 1.21, to be released later this week.
This starts the countdown to its removal, but doesn’t change anything else.
PodSecurityPolicy will continue to be fully functional for several more releases before being removed completely.
In the meantime, we are developing a replacement for PSP that covers key use cases more easily and sustainably.
-->
&lt;p>PodSecurityPolicy (PSP) 在 Kubernetes 1.21 中被弃用。&lt;!--to be released later this week这句感觉没必要翻译，非漏译-->
PSP 日后会被移除，但目前不会改变任何其他内容。在移除之前，PSP 将继续在后续多个版本中完全正常运行。
与此同时，我们正在开发 PSP 的替代品，希望可以更轻松、更可持续地覆盖关键用例。&lt;/p>
&lt;!--
What are Pod Security Policies? Why did we need them? Why are they going away, and what’s next?
How does this affect you? These key questions come to mind as we prepare to say goodbye to PSP,
so let’s walk through them together. We’ll start with an overview of how features get removed from Kubernetes.
-->
&lt;p>什么是 PSP？为什么需要 PSP？为什么要弃用，未来又将如何发展？
这对你有什么影响？当我们准备告别 PSP，这些关键问题浮现在脑海中，
所以让我们一起来讨论吧。本文首先概述 Kubernetes 如何移除一些特性。&lt;/p>
&lt;!--
## What does deprecation mean in Kubernetes?
-->
&lt;h2 id="kubernetes-中的弃用是什么意思">Kubernetes 中的弃用是什么意思？&lt;/h2>
&lt;!--
Whenever a Kubernetes feature is set to go away, our [deprecation policy](/docs/reference/using-api/deprecation-policy/)
is our guide. First the feature is marked as deprecated, then after enough time has passed, it can finally be removed.
-->
&lt;p>每当 Kubernetes 决定弃用某项特性时，我们会遵循&lt;a href="https://kubernetes.io/zh-cn/docs/reference/using-api/deprecation-policy/">弃用策略&lt;/a>。
首先将该特性标记为已弃用，然后经过足够长的时间后，最终将其移除。&lt;/p>
&lt;!--
Kubernetes 1.21 starts the deprecation process for PodSecurityPolicy. As with all feature deprecations,
PodSecurityPolicy will continue to be fully functional for several more releases.
The current plan is to remove PSP from Kubernetes in the 1.25 release.
-->
&lt;p>Kubernetes 1.21 启动了 PodSecurityPolicy 的弃用流程。与弃用任何其他功能一样，
PodSecurityPolicy 将继续在后续几个版本中完全正常运行。目前的计划是在 1.25 版本中将其移除。&lt;/p>
&lt;!--
Until then, PSP is still PSP. There will be at least a year during which the newest Kubernetes releases will
still support PSP, and nearly two years until PSP will pass fully out of all supported Kubernetes versions.
-->
&lt;p>在彻底移除之前，PSP 仍然是 PSP。至少在未来一年时间内，最新的 Kubernetes
版本仍将继续支持 PSP。大约两年之后，PSP 才会在所有受支持的 Kubernetes 版本中彻底消失。&lt;/p>
&lt;!--
## What is PodSecurityPolicy?
-->
&lt;h2 id="什么是-podsecuritypolicy">什么是 PodSecurityPolicy？&lt;/h2>
&lt;!--
[PodSecurityPolicy](/docs/concepts/security/pod-security-policy/) is
a built-in [admission controller](/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/)
that allows a cluster administrator to control security-sensitive aspects of the Pod specification.
-->
&lt;p>&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-policy/">PodSecurityPolicy&lt;/a>
是一个内置的&lt;a href="https://kubernetes.io/blog/2019/03/21/a-guide-to-kubernetes-admission-controllers/">准入控制器&lt;/a>，
允许集群管理员控制 Pod 规约中涉及安全的敏感内容。&lt;/p>
&lt;!--
First, one or more PodSecurityPolicy resources are created in a cluster to define the requirements Pods must meet.
Then, RBAC rules are created to control which PodSecurityPolicy applies to a given pod.
If a pod meets the requirements of its PSP, it will be admitted to the cluster as usual.
In some cases, PSP can also modify Pod fields, effectively creating new defaults for those fields.
If a Pod does not meet the PSP requirements, it is rejected, and cannot run.
-->
&lt;p>首先，在集群中创建一个或多个 PodSecurityPolicy 资源来定义 Pod 必须满足的要求。
然后，创建 RBAC 规则来决定为特定的 Pod 应用哪个 PodSecurityPolicy。
如果 Pod 满足其 PSP 的要求，则照常被允许进入集群。
在某些情况下，PSP 还可以修改 Pod 字段，有效地为这些字段创建新的默认值。
如果 Pod 不符合 PSP 要求，则被拒绝进入集群，并且无法运行。&lt;/p>
&lt;!--
One more important thing to know about PodSecurityPolicy: it’s not the same as
[PodSecurityContext](/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context).
-->
&lt;p>关于 PodSecurityPolicy，还需要了解：它与
&lt;a href="https://kubernetes.io/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context">PodSecurityContext&lt;/a> 不同。&lt;/p>
&lt;!--
A part of the Pod specification, PodSecurityContext (and its per-container counterpart `SecurityContext`)
is the collection of fields that specify many of the security-relevant settings for a Pod.
The security context dictates to the kubelet and container runtime how the Pod should actually be run.
In contrast, the PodSecurityPolicy only constrains (or defaults) the values that may be set on the security context.
-->
&lt;p>作为 Pod 规约的一部分，PodSecurityContext（及其每个容器对应的 &lt;code>SecurityContext&lt;/code>）
是一组字段的集合，这些字段为 Pod 指定了与安全相关的许多设置。
安全上下文指示 kubelet 和容器运行时究竟应该如何运行 Pod。
相反，PodSecurityPolicy 仅约束可能在安全上下文中设置的值（或设置默认值）。&lt;/p>
&lt;!--
The deprecation of PSP does not affect PodSecurityContext in any way.
-->
&lt;p>弃用 PSP 不会以任何方式影响 PodSecurityContext。&lt;/p>
&lt;!--
## Why did we need PodSecurityPolicy?
-->
&lt;h2 id="以前为什么需要-podsecuritypolicy">以前为什么需要 PodSecurityPolicy？&lt;/h2>
&lt;!--
In Kubernetes, we define resources such as Deployments, StatefulSets, and Services that
represent the building blocks of software applications. The various controllers inside
a Kubernetes cluster react to these resources, creating further Kubernetes resources or
configuring some software or hardware to accomplish our goals.
-->
&lt;p>在 Kubernetes 中，我们定义了 Deployment、StatefulSet 和 Service 等资源。
这些资源代表软件应用程序的构建块。Kubernetes 集群中的各种控制器根据这些资源做出反应，
创建更多的 Kubernetes 资源或配置一些软件或硬件来实现我们的目标。&lt;/p>
&lt;!--
In most Kubernetes clusters,
RBAC (Role-Based Access Control) [rules](/docs/reference/access-authn-authz/rbac/#role-and-clusterrole)
control access to these resources. `list`, `get`, `create`, `edit`, and `delete` are
the sorts of API operations that RBAC cares about,
but _RBAC does not consider what settings are being put into the resources it controls_.
For example, a Pod can be almost anything from a simple webserver to
a privileged command prompt offering full access to the underlying server node and all the data.
It’s all the same to RBAC: a Pod is a Pod is a Pod.
-->
&lt;p>在大多数 Kubernetes 集群中，由 RBAC（基于角色的访问控制）&lt;a href="https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/#role-and-clusterrole">规则&lt;/a>
控制对这些资源的访问。 &lt;code>list&lt;/code>、&lt;code>get&lt;/code>、&lt;code>create&lt;/code>、&lt;code>edit&lt;/code> 和 &lt;code>delete&lt;/code> 是 RBAC 关心的 API 操作类型，
但 &lt;strong>RBAC 不考虑其所控制的资源中加入了哪些设置&lt;/strong>。例如，
Pod 几乎可以是任何东西，例如简单的网络服务器，或者是特权命令提示（提供对底层服务器节点和所有数据的完全访问权限）。
这对 RBAC 来说都是一样的：Pod 就是 Pod 而已。&lt;/p>
&lt;!--
To control what sorts of settings are allowed in the resources defined in your cluster,
you need Admission Control in addition to RBAC. Since Kubernetes 1.3,
PodSecurityPolicy has been the built-in way to do that for security-related Pod fields.
Using PodSecurityPolicy, you can prevent “create Pod” from automatically meaning “root on every cluster node,”
without needing to deploy additional external admission controllers.
-->
&lt;p>要控制集群中定义的资源允许哪些类型的设置，除了 RBAC 之外，还需要准入控制。
从 Kubernetes 1.3 开始，内置 PodSecurityPolicy 一直被作为 Pod 安全相关字段的准入控制机制。
使用 PodSecurityPolicy，可以防止“创建 Pod”这个能力自动变成“每个集群节点上的 root 用户”，
并且无需部署额外的外部准入控制器。&lt;/p>
&lt;!--
## Why is PodSecurityPolicy going away?
-->
&lt;h2 id="现在为什么-podsecuritypolicy-要消失">现在为什么 PodSecurityPolicy 要消失？&lt;/h2>
&lt;!--
In the years since PodSecurityPolicy was first introduced, we have realized that
PSP has some serious usability problems that can’t be addressed without making breaking changes.
-->
&lt;p>自从首次引入 PodSecurityPolicy 以来，我们已经意识到 PSP 存在一些严重的可用性问题，
只有做出断裂式的改变才能解决。&lt;/p>
&lt;!--
The way PSPs are applied to Pods has proven confusing to nearly everyone that has attempted to use them.
It is easy to accidentally grant broader permissions than intended,
and difficult to inspect which PSP(s) apply in a given situation. The “changing Pod defaults” feature can be handy,
but is only supported for certain Pod settings and it’s not obvious when they will or will not apply to your Pod.
Without a “dry run” or audit mode, it’s impractical to retrofit PSP to existing clusters safely,
and it’s impossible for PSP to ever be enabled by default.
-->
&lt;p>实践证明，PSP 应用于 Pod 的方式让几乎所有尝试使用它们的人都感到困惑。
很容易意外授予比预期更广泛的权限，并且难以查看某种特定情况下应用了哪些 PSP。
“更改 Pod 默认值”功能很方便，但仅支持某些 Pod 设置，而且无法明确知道它们何时会或不会应用于的 Pod。
如果没有“试运行”或审计模式，将 PSP 安全地改造并应用到现有集群是不切实际的，并且永远都不可能默认启用 PSP。&lt;/p>
&lt;!--
For more information about these and other PSP difficulties, check out
SIG Auth’s KubeCon NA 2019 Maintainer Track session video:
&lt;div class="youtube-quote-sm">
&lt;iframe src="https://www.youtube.com/embed/SFtHRmPuhEw?start=953" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
-->
&lt;p>有关这些问题和其他 PSP 困难的更多信息，请查看
KubeCon NA 2019 的 SIG Auth 维护者频道会议记录：
&lt;div class="youtube-quote-sm">
&lt;iframe src="https://www.youtube.com/embed/SFtHRmPuhEw?start=953" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;!--
Today, you’re not limited only to deploying PSP or writing your own custom admission controller.
Several external admission controllers are available that incorporate lessons learned from PSP to
provide a better user experience. [K-Rail](https://github.com/cruise-automation/k-rail),
[Kyverno](https://github.com/kyverno/kyverno/), and
[OPA/Gatekeeper](https://github.com/open-policy-agent/gatekeeper/) are all well-known, and each has its fans.
-->
&lt;p>如今，你不再局限于部署 PSP 或编写自己的自定义准入控制器。
有几个外部准入控制器可用，它们结合了从 PSP 中吸取的经验教训以提供更好的用户体验。
&lt;a href="https://github.com/cruise-automation/k-rail">K-Rail&lt;/a>、
&lt;a href="https://github.com/kyverno/kyverno/">Kyverno&lt;/a>、
&lt;a href="https://github.com/open-policy-agent/gatekeeper/">OPA/Gatekeeper&lt;/a> 都家喻户晓，各有粉丝。&lt;/p>
&lt;!--
Although there are other good options available now, we believe there is still value in
having a built-in admission controller available as a choice for users. With this in mind,
we turn toward building what’s next, inspired by the lessons learned from PSP.
-->
&lt;p>尽管现在还有其他不错的选择，但我们认为，提供一个内置的准入控制器供用户选择，仍然是有价值的事情。
考虑到这一点，以及受 PSP 经验的启发，我们转向下一步。&lt;/p>
&lt;!--
## What’s next?
-->
&lt;h2 id="下一步是什么">下一步是什么？&lt;/h2>
&lt;!--
Kubernetes SIG Security, SIG Auth, and a diverse collection of other community members
have been working together for months to ensure that what’s coming next is going to be awesome.
We have developed a Kubernetes Enhancement Proposal ([KEP 2579](https://github.com/kubernetes/enhancements/issues/2579))
and a prototype for a new feature, currently being called by the temporary name "PSP Replacement Policy."
We are targeting an Alpha release in Kubernetes 1.22.
-->
&lt;p>Kubernetes SIG Security、SIG Auth 和其他社区成员几个月来一直在倾力合作，确保接下来的方案能令人惊叹。
我们拟定了 Kubernetes 增强提案（&lt;a href="https://github.com/kubernetes/enhancements/issues/2579">KEP 2579&lt;/a>）
以及一个新功能的原型，目前称之为“PSP 替代策略”。
我们的目标是在 Kubernetes 1.22 中发布 Alpha 版本。&lt;/p>
&lt;!--
PSP Replacement Policy starts with the realization that
since there is a robust ecosystem of external admission controllers already available,
PSP’s replacement doesn’t need to be all things to all people.
Simplicity of deployment and adoption is the key advantage a built-in admission controller has
compared to an external webhook, so we have focused on how to best utilize that advantage.
-->
&lt;p>PSP 替代策略始于，我们认识到已经有一个强大的外部准入控制器生态系统可用，
所以，PSP 的替代品不需要满足所有人的所有需求。与外部 Webhook 相比，
部署和采用的简单性是内置准入控制器的关键优势。我们专注于如何最好地利用这一优势。&lt;/p>
&lt;!--
PSP Replacement Policy is designed to be as simple as practically possible
while providing enough flexibility to really be useful in production at scale.
It has soft rollout features to enable retrofitting it to existing clusters,
and is configurable enough that it can eventually be active by default.
It can be deactivated partially or entirely, to coexist with external admission controllers for advanced use cases.
-->
&lt;p>PSP 替代策略旨在尽可能简单，同时提供足够的灵活性以支撑大规模生产场景。
它具有柔性上线能力，以便于将其改装到现有集群，并且新的策略是可配置的，可以设置为默认启用。
PSP 替代策略可以被部分或全部禁用，以便在高级使用场景中与外部准入控制器共存。&lt;/p>
&lt;!--
## What does this mean for you?
-->
&lt;h2 id="这对你意味着什么">这对你意味着什么？&lt;/h2>
&lt;!--
What this all means for you depends on your current PSP situation.
If you’re already using PSP, there’s plenty of time to plan your next move.
Please review the PSP Replacement Policy KEP and think about how well it will suit your use case.
-->
&lt;p>这一切对你意味着什么取决于你当前的 PSP 情况。如果已经在使用 PSP，那么你有足够的时间来计划下一步行动。
请查看 PSP 替代策略 KEP 并考虑它是否适合你的使用场景。&lt;/p>
&lt;!--
If you’re making extensive use of the flexibility of PSP with numerous PSPs and complex binding rules,
you will likely find the simplicity of PSP Replacement Policy too limiting.
Use the next year to evaluate the other admission controller choices in the ecosystem.
There are resources available to ease this transition,
such as the [Gatekeeper Policy Library](https://github.com/open-policy-agent/gatekeeper-library).
-->
&lt;p>如果你已经在通过众多 PSP 和复杂的绑定规则深度利用 PSP 的灵活性，你可能会发现 PSP 替代策略的简单性有太多限制。
此时，建议你在接下来的一年中评估一下生态系统中的其他准入控制器选择。有些资源可以让这种过渡更容易，
比如 &lt;a href="https://github.com/open-policy-agent/gatekeeper-library">Gatekeeper Policy Library&lt;/a>。&lt;/p>
&lt;!--
If your use of PSP is relatively simple, with a few policies and straightforward binding to
service accounts in each namespace, you will likely find PSP Replacement Policy to be a good match for your needs.
Evaluate your PSPs compared to the Kubernetes [Pod Security Standards](/docs/concepts/security/pod-security-standards/)
to get a feel for where you’ll be able to use the Restricted, Baseline, and Privileged policies.
Please follow along with or contribute to the KEP and subsequent development,
and try out the Alpha release of PSP Replacement Policy when it becomes available.
-->
&lt;p>如果只是使用 PSP 的基础功能，只用几个策略并直接绑定到每个命名空间中的服务帐户，
你可能会发现 PSP 替代策略非常适合你的需求。
对比 Kubernetes &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-standards/">Pod 安全标准&lt;/a> 评估你的 PSP，
了解可以在哪些情形下使用限制策略、基线策略和特权策略。
欢迎关注或为 KEP 和后续发展做出贡献，并在可用时试用 PSP 替代策略的 Alpha 版本。&lt;/p>
&lt;!--
If you’re just beginning your PSP journey, you will save time and effort by keeping it simple.
You can approximate the functionality of PSP Replacement Policy today by using the Pod Security Standards’ PSPs.
If you set the cluster default by binding a Baseline or Restricted policy to the `system:serviceaccounts` group,
and then make a more-permissive policy available as needed in certain
Namespaces [using ServiceAccount bindings](/docs/concepts/policy/pod-security-policy/#run-another-pod),
you will avoid many of the PSP pitfalls and have an easy migration to PSP Replacement Policy.
If your needs are much more complex than this, your effort is probably better spent adopting
one of the more fully-featured external admission controllers mentioned above.
-->
&lt;p>如果刚刚开始 PSP 之旅，你可以通过保持简单来节省时间和精力。
你可以使用 Pod 安全标准的 PSP 来获得和目前 PSP 替代策略相似的功能。
如果你将基线策略或限制策略绑定到 &lt;code>system:serviceaccounts&lt;/code> 组来设置集群默认值，
然后&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/policy/pod-security-policy/#run-another-pod">使用 ServiceAccount 绑定&lt;/a>
在某些命名空间下根据需要制定更宽松的策略，就可以避免许多 PSP 陷阱并轻松迁移到 PSP 替代策略。
如果你的需求比这复杂得多，那么建议将精力花在采用比上面提到的功能更全的某个外部准入控制器。&lt;/p>
&lt;!--
We’re dedicated to making Kubernetes the best container orchestration tool we can,
and sometimes that means we need to remove longstanding features to make space for better things to come.
When that happens, the Kubernetes deprecation policy ensures you have plenty of time to plan your next move.
In the case of PodSecurityPolicy, several options are available to suit a range of needs and use cases.
Start planning ahead now for PSP’s eventual removal, and please consider contributing to its replacement! Happy securing!
-->
&lt;p>我们致力于使 Kubernetes 成为我们可以做到的最好的容器编排工具，
有时这意味着我们需要删除长期存在的功能，以便为更好的特性腾出空间。
发生这种情况时，Kubernetes 弃用策略可确保你有足够的时间来计划下一步行动。
对于 PodSecurityPolicy，有几个选项可以满足一系列需求和用例。
现在就开始为 PSP 的最终移除提前制定计划，请考虑为它的替换做出贡献！&lt;!--omitted "Happy securing!" as suggested-->&lt;/p>
&lt;!--
**Acknowledgment:** It takes a wonderful group to make wonderful software.
Thanks are due to everyone who has contributed to the PSP replacement effort,
especially (in alphabetical order) Tim Allclair, Ian Coldwater, and Jordan Liggitt.
It’s been a joy to work with y’all on this.
-->
&lt;p>&lt;strong>致谢：&lt;/strong> 研发优秀的软件需要优秀的团队。感谢为 PSP 替代工作做出贡献的所有人，
尤其是（按字母顺序）Tim Allclair、Ian Coldwater 和 Jordan Liggitt。
和你们共事非常愉快。&lt;/p></description></item><item><title>Blog: 一个编排高可用应用的 Kubernetes 自定义调度器</title><link>https://kubernetes.io/zh-cn/blog/2020/12/21/writing-crl-scheduler/</link><pubDate>Mon, 21 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2020/12/21/writing-crl-scheduler/</guid><description>
&lt;!--
---
layout: blog
title: "A Custom Kubernetes Scheduler to Orchestrate Highly Available Applications"
date: 2020-12-21
slug: writing-crl-scheduler
---
-->
&lt;p>&lt;strong>作者&lt;/strong>: Chris Seto (Cockroach Labs)&lt;/p>
&lt;!--
**Author**: Chris Seto (Cockroach Labs)
-->
&lt;!--
As long as you're willing to follow the rules, deploying on Kubernetes and air travel can be quite pleasant. More often than not, things will "just work". However, if one is interested in travelling with an alligator that must remain alive or scaling a database that must remain available, the situation is likely to become a bit more complicated. It may even be easier to build one's own plane or database for that matter. Travelling with reptiles aside, scaling a highly available stateful system is no trivial task.
-->
&lt;p>只要你愿意遵守规则，那么在 Kubernetes 上的部署和探索可以是相当愉快的。更多时候，事情会 &amp;quot;顺利进行&amp;quot;。
然而，如果一个人对与必须保持存活的鳄鱼一起旅行或者是对必须保持可用的数据库进行扩展有兴趣，
情况可能会变得更复杂一点。
相较于这个问题，建立自己的飞机或数据库甚至还可能更容易一些。撇开与鳄鱼的旅行不谈，扩展一个高可用的有状态系统也不是一件小事。&lt;/p>
&lt;!--
Scaling any system has two main components:
1. Adding or removing infrastructure that the system will run on, and
2. Ensuring that the system knows how to handle additional instances of itself being added and removed.
-->
&lt;p>任何系统的扩展都有两个主要组成部分。&lt;/p>
&lt;ol>
&lt;li>增加或删除系统将运行的基础架构，以及&lt;/li>
&lt;li>确保系统知道如何处理自身额外实例的添加和删除。&lt;/li>
&lt;/ol>
&lt;!--
Most stateless systems, web servers for example, are created without the need to be aware of peers. Stateful systems, which includes databases like CockroachDB, have to coordinate with their peer instances and shuffle around data. As luck would have it, CockroachDB handles data redistribution and replication. The tricky part is being able to tolerate failures during these operations by ensuring that data and instances are distributed across many failure domains (availability zones).
-->
&lt;p>大多数无状态系统，例如网络服务器，在创建时不需要意识到对等实例。而有状态的系统，包括像 CockroachDB 这样的数据库，
必须与它们的对等实例协调，并对数据进行 shuffle。运气好的话，CockroachDB 可以处理数据的再分布和复制。
棘手的部分是在确保数据和实例分布在许多故障域（可用性区域）的操作过程中能够容忍故障的发生。&lt;/p>
&lt;!--
One of Kubernetes' responsibilities is to place "resources" (e.g, a disk or container) into the cluster and satisfy the constraints they request. For example: "I must be in availability zone _A_" (see [Running in multiple zones](/docs/setup/best-practices/multiple-zones/#nodes-are-labeled)), or "I can't be placed onto the same node as this other Pod" (see [Affinity and anti-affinity](/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)).
-->
&lt;p>Kubernetes 的职责之一是将 &amp;quot;资源&amp;quot;（如磁盘或容器）放入集群中，并满足其请求的约束。
例如。&amp;quot;我必须在可用性区域 &lt;em>A&lt;/em>&amp;quot;（见&lt;a href="https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#nodes-are-labeled">在多个区域运行&lt;/a>），
或者 &amp;quot;我不能被放置到与某个 Pod 相同的节点上&amp;quot;
（见&lt;a href="https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#nodes-are-labeled">亲和与反亲和&lt;/a>）。&lt;/p>
&lt;!--
As an addition to those constraints, Kubernetes offers [Statefulsets](/docs/concepts/workloads/controllers/statefulset/) that provide identity to Pods as well as persistent storage that "follows" these identified pods. Identity in a StatefulSet is handled by an increasing integer at the end of a pod's name. It's important to note that this integer must always be contiguous: in a StatefulSet, if pods 1 and 3 exist then pod 2 must also exist.
-->
&lt;p>作为对这些约束的补充，Kubernetes 提供了 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/">StatefulSets&lt;/a>，
为 Pod 提供身份，以及 &amp;quot;跟随&amp;quot; 这些指定 Pod 的持久化存储。
在 StatefulSet 中，身份是由 Pod 名称末尾一个呈增序的整数处理的。
值得注意的是，这个整数必须始终是连续的：在一个 StatefulSet 中，
如果 Pod 1 和 3 存在，那么 Pod 2 也必须存在。&lt;/p>
&lt;!--
Under the hood, CockroachCloud deploys each region of CockroachDB as a StatefulSet in its own Kubernetes cluster - see [Orchestrate CockroachDB in a Single Kubernetes Cluster](https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes.html).
In this article, I'll be looking at an individual region, one StatefulSet and one Kubernetes cluster which is distributed across at least three availability zones.
-->
&lt;p>在架构上，CockroachCloud 将 CockroachDB 的每个区域作为 StatefulSet 部署在自己的 Kubernetes 集群中 --
参见 &lt;a href="https://www.cockroachlabs.com/docs/stable/orchestrate-cockroachdb-with-kubernetes.html">Orchestrate CockroachDB in a Single Kubernetes Cluster&lt;/a>。
在这篇文章中，我将着眼于一个单独的区域，一个 StatefulSet 和一个至少分布有三个可用区的 Kubernetes 集群。&lt;/p>
&lt;!--
A three-node CockroachCloud cluster would look something like this:
-->
&lt;p>一个三节点的 CockroachCloud 集群如下所示：&lt;/p>
&lt;!--
![3-node, multi-zone cockroachdb cluster](image01.png)
-->
&lt;p>&lt;img src="image01.png" alt="3-node, multi-zone cockroachdb cluster">&lt;/p>
&lt;!--
When adding additional resources to the cluster we also distribute them across zones. For the speediest user experience, we add all Kubernetes nodes at the same time and then scale up the StatefulSet.
-->
&lt;p>在向集群增加额外的资源时，我们也会将它们分布在各个区域。
为了获得最快的用户体验，我们同时添加所有 Kubernetes 节点，然后扩大 StatefulSet 的规模。&lt;/p>
&lt;!--
![illustration of phases: adding Kubernetes nodes to the multi-zone cockroachdb cluster](image02.png)
-->
&lt;p>&lt;img src="image02.png" alt="illustration of phases: adding Kubernetes nodes to the multi-zone cockroachdb cluster">&lt;/p>
&lt;!--
Note that anti-affinities are satisfied no matter the order in which pods are assigned to Kubernetes nodes. In the example, pods 0, 1 and 2 were assigned to zones A, B, and C respectively, but pods 3 and 4 were assigned in a different order, to zones B and A respectively. The anti-affinity is still satisfied because the pods are still placed in different zones.
-->
&lt;p>请注意，无论 Pod 被分配到 Kubernetes 节点的顺序如何，都会满足反亲和性。
在这个例子中，Pod 0、1、2 分别被分配到 A、B、C 区，但 Pod 3 和 4 以不同的顺序被分配到 B 和 A 区。
反亲和性仍然得到满足，因为 Pod 仍然被放置在不同的区域。&lt;/p>
&lt;!--
To remove resources from a cluster, we perform these operations in reverse order.
-->
&lt;p>要从集群中移除资源，我们以相反的顺序执行这些操作。&lt;/p>
&lt;!--
We first scale down the StatefulSet and then remove from the cluster any nodes lacking a CockroachDB pod.
-->
&lt;p>我们首先缩小 StatefulSet 的规模，然后从集群中移除任何缺少 CockroachDB Pod 的节点。&lt;/p>
&lt;!--
![illustration of phases: scaling down pods in a multi-zone cockroachdb cluster in Kubernetes](image03.png)
-->
&lt;p>&lt;img src="image03.png" alt="illustration of phases: scaling down pods in a multi-zone cockroachdb cluster in Kubernetes">&lt;/p>
&lt;!--
Now, remember that pods in a StatefulSet of size _n_ must have ids in the range `[0,n)`. When scaling down a StatefulSet by _m_, Kubernetes removes _m_ pods, starting from the highest ordinals and moving towards the lowest, [the reverse in which they were added](/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees).
Consider the cluster topology below:
-->
&lt;p>现在，请记住，规模为 &lt;em>n&lt;/em> 的 StatefulSet 中的 Pods 一定具有 &lt;code>[0,n)&lt;/code> 范围内的 id。
当把一个 StatefulSet 规模缩减了 &lt;em>m&lt;/em> 时，Kubernetes 会移除 &lt;em>m&lt;/em> 个 Pod，从最高的序号开始，向最低的序号移动，
&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">与它们被添加的顺序相反&lt;/a>。
考虑一下下面的集群拓扑结构。&lt;/p>
&lt;!--
![illustration: cockroachdb cluster: 6 nodes distributed across 3 availability zones](image04.png)
-->
&lt;p>&lt;img src="image04.png" alt="illustration: cockroachdb cluster: 6 nodes distributed across 3 availability zones">&lt;/p>
&lt;!--
As ordinals 5 through 3 are removed from this cluster, the statefulset continues to have a presence across all 3 availability zones.
-->
&lt;p>当从这个集群中移除 5 号到 3 号 Pod 时，这个 StatefulSet 仍然横跨三个可用区。&lt;/p>
&lt;!--
![illustration: removing 3 nodes from a 6-node, 3-zone cockroachdb cluster](image05.png)
-->
&lt;p>&lt;img src="image05.png" alt="illustration: removing 3 nodes from a 6-node, 3-zone cockroachdb cluster">&lt;/p>
&lt;!--
However, Kubernetes' scheduler doesn't _guarantee_ the placement above as we expected at first.
-->
&lt;p>然而，Kubernetes 的调度器并不像我们一开始预期的那样 &lt;em>保证&lt;/em> 上面的分布。&lt;/p>
&lt;!--
Our combined knowledge of the following is what lead to this misconception.
* Kubernetes' ability to [automatically spread Pods across zone](/docs/setup/best-practices/multiple-zones/#pods-are-spread-across-zones)
* The behavior that a StatefulSet with _n_ replicas, when Pods are being deployed, they are created sequentially, in order from `{0..n-1}`. See [StatefulSet](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees) for more details.
-->
&lt;p>我们对以下内容的综合认识是导致这种误解的原因。&lt;/p>
&lt;ul>
&lt;li>Kubernetes &lt;a href="https://kubernetes.io/zh-cn/docs/setup/best-practices/multiple-zones/#pods-are-spread-across-zones">自动跨区分配 Pod&lt;/a> 的能力&lt;/li>
&lt;li>一个有 &lt;em>n&lt;/em> 个副本的 StatefulSet，当 Pod 被部署时，它们会按照 &lt;code>{0...n-1}&lt;/code> 的顺序依次创建。
更多细节见 &lt;a href="https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees">StatefulSet&lt;/a>。&lt;/li>
&lt;/ul>
&lt;!--
Consider the following topology:
-->
&lt;p>考虑以下拓扑结构：&lt;/p>
&lt;!--
![illustration: 6-node cockroachdb cluster distributed across 3 availability zones](image06.png)
-->
&lt;p>&lt;img src="image06.png" alt="illustration: 6-node cockroachdb cluster distributed across 3 availability zones">&lt;/p>
&lt;!--
These pods were created in order and they are spread across all availability zones in the cluster. When ordinals 5 through 3 are terminated, this cluster will lose its presence in zone C!
-->
&lt;p>这些 Pod 是按顺序创建的，它们分布在集群里所有可用区。当序号 5 到 3 的 Pod 被终止时，
这个集群将从 C 区消失!&lt;/p>
&lt;!--
![illustration: terminating 3 nodes in 6-node cluster spread across 3 availability zones, where 2/2 nodes in the same availability zone are terminated, knocking out that AZ](image07.png)
-->
&lt;p>&lt;img src="image07.png" alt="illustration: terminating 3 nodes in 6-node cluster spread across 3 availability zones, where 2/2 nodes in the same availability zone are terminated, knocking out that AZ">&lt;/p>
&lt;!--
Worse yet, our automation, at the time, would remove Nodes A-2, B-2, and C-2. Leaving CRDB-1 in an unscheduled state as persistent volumes are only available in the zone they are initially created in.
-->
&lt;p>更糟糕的是，在这个时候，我们的自动化机制将删除节点 A-2，B-2，和 C-2。
并让 CRDB-1 处于未调度状态，因为持久性卷只在其创建时所处的区域内可用。&lt;/p>
&lt;!--
To correct the latter issue, we now employ a "hunt and peck" approach to removing machines from a cluster. Rather than blindly removing Kubernetes nodes from the cluster, only nodes without a CockroachDB pod would be removed. The much more daunting task was to wrangle the Kubernetes scheduler.
-->
&lt;p>为了纠正后一个问题，我们现在采用了一种“狩猎和啄食”的方法来从集群中移除机器。
与其盲目地从集群中移除 Kubernetes 节点，不如只移除没有 CockroachDB Pod 的节点。
更为艰巨的任务是管理 Kubernetes 的调度器。&lt;/p>
&lt;!--
## A session of brainstorming left us with 3 options:
### 1. Upgrade to kubernetes 1.18 and make use of Pod Topology Spread Constraints
While this seems like it could have been the perfect solution, at the time of writing Kubernetes 1.18 was unavailable on the two most common managed Kubernetes services in public cloud, EKS and GKE.
Furthermore, [pod topology spread constraints](/docs/concepts/scheduling-eviction/topology-spread-constraints/) were still a beta feature in 1.18 which meant that it [wasn't guaranteed to be available in managed clusters](https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#kubernetes_feature_choices) even when v1.18 became available.
The entire endeavour was concerningly reminiscent of checking [caniuse.com](https://caniuse.com/) when Internet Explorer 8 was still around.
-->
&lt;h2 id="一场头脑风暴后我们有了-3-个选择">一场头脑风暴后我们有了 3 个选择。&lt;/h2>
&lt;h3 id="1-升级到-kubernetes-1-18-并利用-pod-拓扑分布约束">1. 升级到 kubernetes 1.18 并利用 Pod 拓扑分布约束&lt;/h3>
&lt;p>虽然这似乎是一个完美的解决方案，但在写这篇文章的时候，Kubernetes 1.18 在公有云中两个最常见的
托管 Kubernetes 服务（ EKS 和 GKE ）上是不可用的。
此外，&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/">Pod 拓扑分布约束&lt;/a>在 1.18 中仍是测试版功能，
这意味着即使在 v1.18 可用时，它&lt;a href="https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters#kubernetes_feature_choices">也不能保证在托管集群中可用&lt;/a>。
整个努力让人联想到在 Internet Explorer 8 还存在的时候访问 &lt;a href="https://caniuse.com/">caniuse.com&lt;/a>。&lt;/p>
&lt;!--
### 2. Deploy a statefulset _per zone_.
Rather than having one StatefulSet distributed across all availability zones, a single StatefulSet with node affinities per zone would allow manual control over our zonal topology.
Our team had considered this as an option in the past which made it particularly appealing.
Ultimately, we decided to forego this option as it would have required a massive overhaul to our codebase and performing the migration on existing customer clusters would have been an equally large undertaking.
-->
&lt;h3 id="2-在每个区部署一个-statefulset">2. 在每个区部署一个 StatefulSet。&lt;/h3>
&lt;p>与跨所有可用区部署一个 StatefulSet 相比，在每个区部署一个带有节点亲和性的 StatefulSet 可以实现手动控制分区拓扑结构。
我们的团队过去曾考虑过这个选项，我们也倾向此选项。
但最终，我们决定放弃这个方案，因为这需要对我们的代码库进行大规模的修改，而且在现有的客户集群上进行迁移也是一个同样大的工程。&lt;/p>
&lt;!--
### 3. Write a custom Kubernetes scheduler.
Thanks to an example from [Kelsey Hightower](https://github.com/kelseyhightower/scheduler) and a blog post from [Banzai Cloud](https://banzaicloud.com/blog/k8s-custom-scheduler/), we decided to dive in head first and write our own [custom Kubernetes scheduler](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/).
Once our proof-of-concept was deployed and running, we quickly discovered that the Kubernetes' scheduler is also responsible for mapping persistent volumes to the Pods that it schedules.
The output of [`kubectl get events`](/docs/tasks/extend-kubernetes/configure-multiple-schedulers/#verifying-that-the-pods-were-scheduled-using-the-desired-schedulers) had led us to believe there was another system at play.
In our journey to find the component responsible for storage claim mapping, we discovered the [kube-scheduler plugin system](/docs/concepts/scheduling-eviction/scheduling-framework/). Our next POC was a `Filter` plugin that determined the appropriate availability zone by pod ordinal, and it worked flawlessly!
Our [custom scheduler plugin](https://github.com/cockroachlabs/crl-scheduler) is open source and runs in all of our CockroachCloud clusters.
Having control over how our StatefulSet pods are being scheduled has let us scale out with confidence.
We may look into retiring our plugin once pod topology spread constraints are available in GKE and EKS, but the maintenance overhead has been surprisingly low.
Better still: the plugin's implementation is orthogonal to our business logic. Deploying it, or retiring it for that matter, is as simple as changing the `schedulerName` field in our StatefulSet definitions.
---
_[Chris Seto](https://twitter.com/_ostriches) is a software engineer at Cockroach Labs and works on their Kubernetes automation for [CockroachCloud](https://cockroachlabs.cloud), CockroachDB._
-->
&lt;h3 id="3-编写一个自定义的-kubernetes-调度器">3. 编写一个自定义的 Kubernetes 调度器&lt;/h3>
&lt;p>感谢 &lt;a href="https://github.com/kelseyhightower/scheduler">Kelsey Hightower&lt;/a> 的例子和
&lt;a href="https://banzaicloud.com/blog/k8s-custom-scheduler/">Banzai Cloud&lt;/a> 的博文，我们决定投入进去，编写自己的&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/configure-multiple-schedulers/">自定义 Kubernetes 调度器&lt;/a>。
一旦我们的概念验证被部署和运行，我们很快就发现，Kubernetes 的调度器也负责将持久化卷映射到它所调度的 Pod 上。
&lt;a href="https://kubernetes.io/zh-cn/docs/tasks/extend-kubernetes/configure-multiple-schedulers/#verifying-that-the-pods-wer-scheduled-using-the-desired-schedulers">&lt;code>kubectl get events&lt;/code>&lt;/a>
的输出让我们相信有另一个系统在发挥作用。
在我们寻找负责存储声明映射的组件的过程中，我们发现了
&lt;a href="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/scheduling-framework/">kube-scheduler 插件系统&lt;/a>。
我们的下一个 POC 是一个&amp;quot;过滤器&amp;quot;插件，它通过 Pod 的序号来确定适当的可用区域，并且工作得非常完美。&lt;/p>
&lt;p>我们的&lt;a href="https://github.com/cockroachlabs/crl-scheduler">自定义调度器插件&lt;/a>是开源的，并在我们所有的 CockroachCloud 集群中运行。
对 StatefulSet Pod 的调度方式有掌控力，让我们有信心扩大规模。
一旦 GKE 和 EKS 中的 Pod 拓扑分布约束可用，我们可能会考虑让我们的插件退役，但其维护的开销出乎意料地低。
更好的是：该插件的实现与我们的业务逻辑是横向的。部署它，或取消它，就像改变 StatefulSet 定义中的 &amp;quot;schedulerName&amp;quot; 字段一样简单。&lt;/p>
&lt;hr>
&lt;p>&lt;a href="https://twitter.com/_ostriches">Chris Seto&lt;/a> 是 Cockroach 实验室的一名软件工程师，负责
&lt;a href="https://cockroachlabs.cloud">CockroachCloud&lt;/a> CockroachDB 的 Kubernetes 自动化。&lt;/p></description></item><item><title>Blog: Kubernetes 1.20：CSI 驱动程序中的 Pod 身份假扮和短时卷</title><link>https://kubernetes.io/zh-cn/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</link><pubDate>Fri, 18 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh-cn/blog/2020/12/18/kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi/</guid><description>
&lt;!--
layout: blog
title: 'Kubernetes 1.20: Pod Impersonation and Short-lived Volumes in CSI Drivers'
date: 2020-12-18
slug: kubernetes-1.20-pod-impersonation-short-lived-volumes-in-csi
-->
&lt;!--
**Author**: Shihang Zhang (Google)
-->
&lt;p>&lt;strong>作者&lt;/strong>: Shihang Zhang（谷歌）&lt;/p>
&lt;!--
Typically when a [CSI](https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md) driver mounts credentials such as secrets and certificates, it has to authenticate against storage providers to access the credentials. However, the access to those credentials are controlled on the basis of the pods' identities rather than the CSI driver's identity. CSI drivers, therefore, need some way to retrieve pod's service account token.
-->
&lt;p>通常，当 &lt;a href="https://github.com/container-storage-interface/spec/blob/baa71a34651e5ee6cb983b39c03097d7aa384278/spec.md">CSI&lt;/a> 驱动程序挂载
诸如 Secret 和证书之类的凭据时，它必须通过存储提供者的身份认证才能访问这些凭据。
然而，对这些凭据的访问是根据 Pod 的身份而不是 CSI 驱动程序的身份来控制的。
因此，CSI 驱动程序需要某种方法来取得 Pod 的服务帐户令牌。&lt;/p>
&lt;!--
Currently there are two suboptimal approaches to achieve this, either by granting CSI drivers the permission to use TokenRequest API or by reading tokens directly from the host filesystem.
-->
&lt;p>当前，有两种不是那么理想的方法来实现这一目的，要么通过授予 CSI 驱动程序使用 TokenRequest API 的权限，要么直接从主机文件系统中读取令牌。&lt;/p>
&lt;!--
Both of them exhibit the following drawbacks:
-->
&lt;p>两者都存在以下缺点：&lt;/p>
&lt;!--
- Violating the principle of least privilege
- Every CSI driver needs to re-implement the logic of getting the pod’s service account token
-->
&lt;ul>
&lt;li>违反最少特权原则&lt;/li>
&lt;li>每个 CSI 驱动程序都需要重新实现获取 Pod 的服务帐户令牌的逻辑&lt;/li>
&lt;/ul>
&lt;!--
The second approach is more problematic due to:
-->
&lt;p>第二种方式问题更多，因为：&lt;/p>
&lt;!--
- The audience of the token defaults to the kube-apiserver
- The token is not guaranteed to be available (e.g. `AutomountServiceAccountToken=false`)
- The approach does not work for CSI drivers that run as a different (non-root) user from the pods. See [file permission section for service account token](https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission)
- The token might be legacy Kubernetes service account token which doesn’t expire if `BoundServiceAccountTokenVolume=false`
-->
&lt;ul>
&lt;li>令牌的受众默认为 kube-apiserver&lt;/li>
&lt;li>该令牌不能保证可用（例如，&lt;code>AutomountServiceAccountToken=false&lt;/code>）&lt;/li>
&lt;li>该方法不适用于以与 Pod 不同的（非 root 用户）用户身份运行的 CSI 驱动程序。请参见
&lt;a href="https://github.com/kubernetes/enhancements/blob/f40c24a5da09390bd521be535b38a4dbab09380c/keps/sig-storage/20180515-svcacct-token-volumes.md#file-permission">服务帐户令牌的文件许可权部分&lt;/a>&lt;/li>
&lt;li>该令牌可能是旧的 Kubernetes 服务帐户令牌，如果 &lt;code>BoundServiceAccountTokenVolume=false&lt;/code>，该令牌不会过期。&lt;/li>
&lt;/ul>
&lt;!--
Kubernetes 1.20 introduces an alpha feature, `CSIServiceAccountToken`, to improve the security posture. The new feature allows CSI drivers to receive pods' [bound service account tokens](https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md).
-->
&lt;p>Kubernetes 1.20 引入了一个内测功能 &lt;code>CSIServiceAccountToken&lt;/code> 以改善安全状况。这项新功能允许 CSI 驱动程序接收 Pod 的&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/1205-bound-service-account-tokens/README.md">绑定服务帐户令牌&lt;/a>。&lt;/p>
&lt;!--
This feature also provides a knob to re-publish volumes so that short-lived volumes can be refreshed.
-->
&lt;p>此功能还提供了一个重新发布卷的能力，以便可以刷新短时卷。&lt;/p>
&lt;!--
## Pod Impersonation
### Using GCP APIs
-->
&lt;h2 id="pod-身份假扮">Pod 身份假扮&lt;/h2>
&lt;h3 id="使用-gcp-apis">使用 GCP APIs&lt;/h3>
&lt;!--
Using [Workload Identity](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity), a Kubernetes service account can authenticate as a Google service account when accessing Google Cloud APIs. If a CSI driver needs to access GCP APIs on behalf of the pods that it is mounting volumes for, it can use the pod's service account token to [exchange for GCP tokens](https://cloud.google.com/iam/docs/reference/sts/rest). The pod's service account token is plumbed through the volume context in `NodePublishVolume` RPC calls when the feature `CSIServiceAccountToken` is enabled. For example: accessing [Google Secret Manager](https://cloud.google.com/secret-manager/) via a [secret store CSI driver](https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp).
-->
&lt;p>使用 &lt;a href="https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity">Workload Identity&lt;/a>，Kubernetes 服务帐户可以在访问 Google Cloud API 时验证为 Google 服务帐户。
如果 CSI 驱动程序要代表其为挂载卷的 Pod 访问 GCP API，则可以使用 Pod 的服务帐户令牌来
&lt;a href="https://cloud.google.com/iam/docs/reference/sts/rest">交换 GCP 令牌&lt;/a>。启用功能 &lt;code>CSIServiceAccountToken&lt;/code> 后，
可通过 &lt;code>NodePublishVolume&lt;/code> RPC 调用中的卷上下文来访问 Pod 的服务帐户令牌。例如：通过 &lt;a href="https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp">Secret 存储 CSI 驱动&lt;/a>
访问 &lt;a href="https://cloud.google.com/secret-manager/">Google Secret Manager&lt;/a>。&lt;/p>
&lt;!--
### Using Vault
If users configure [Kubernetes as an auth method](https://www.vaultproject.io/docs/auth/kubernetes), Vault uses the `TokenReview` API to validate the Kubernetes service account token. For CSI drivers using Vault as resources provider, they need to present the pod's service account to Vault. For example, [secrets store CSI driver](https://github.com/hashicorp/secrets-store-csi-driver-provider-vault) and [cert manager CSI driver](https://github.com/jetstack/cert-manager-csi).
-->
&lt;h3 id="使用vault">使用Vault&lt;/h3>
&lt;p>如果用户将 &lt;a href="https://www.vaultproject.io/docs/auth/kubernetes">Kubernetes 作为身份验证方法&lt;/a>配置，
则 Vault 使用 &lt;code>TokenReview&lt;/code> API 来验证 Kubernetes 服务帐户令牌。
对于使用 Vault 作为资源提供者的 CSI 驱动程序，它们需要将 Pod 的服务帐户提供给 Vault。
例如，&lt;a href="https://github.com/hashicorp/secrets-store-csi-driver-provider-vault">Secret 存储 CSI 驱动&lt;/a>和
&lt;a href="https://github.com/jetstack/cert-manager-csi">证书管理器 CSI 驱动&lt;/a>。&lt;/p>
&lt;!--
## Short-lived Volumes
To keep short-lived volumes such as certificates effective, CSI drivers can specify `RequiresRepublish=true` in their`CSIDriver` object to have the kubelet periodically call `NodePublishVolume` on mounted volumes. These republishes allow CSI drivers to ensure that the volume content is up-to-date.
-->
&lt;h2 id="短时卷">短时卷&lt;/h2>
&lt;p>为了使诸如证书之类的短时卷保持有效，CSI 驱动程序可以在其 &lt;code>CSIDriver&lt;/code> 对象中指定 &lt;code>RequiresRepublish=true&lt;/code>，
以使 kubelet 定期针对已挂载的卷调用 &lt;code>NodePublishVolume&lt;/code>。
这些重新发布操作使 CSI 驱动程序可以确保卷内容是最新的。&lt;/p>
&lt;!--
## Next steps
This feature is alpha and projected to move to beta in 1.21. See more in the following KEP and CSI documentation:
-->
&lt;h2 id="下一步">下一步&lt;/h2>
&lt;p>此功能是 Alpha 版，预计将在 1.21 版中移至 Beta 版。 请参阅以下 KEP 和 CSI 文档中的更多内容：&lt;/p>
&lt;!--
- [KEP-1855: Service Account Token for CSI Driver](https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md)
- [Token Requests](https://kubernetes-csi.github.io/docs/token-requests.html)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1855-csi-driver-service-account-token/README.md">KEP-1855: CSI 驱动程序的服务帐户令牌&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/token-requests.html">令牌请求&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Your feedback is always welcome!
- SIG-Auth [meets regularly](https://github.com/kubernetes/community/tree/master/sig-auth#meetings) and can be reached via [Slack and the mailing list](https://github.com/kubernetes/community/tree/master/sig-auth#contact)
- SIG-Storage [meets regularly](https://github.com/kubernetes/community/tree/master/sig-storage#meetings) and can be reached via [Slack and the mailing list](https://github.com/kubernetes/community/tree/master/sig-storage#contact).
-->
&lt;p>随时欢迎您提供反馈!&lt;/p>
&lt;ul>
&lt;li>SIG-Auth &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#meetings">定期开会&lt;/a>，可以通过 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-auth#contact">Slack 和邮件列表&lt;/a>加入&lt;/li>
&lt;li>SIG-Storage &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">定期开会&lt;/a>，可以通过 &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack 和邮件列表&lt;/a>加入&lt;/li>
&lt;/ul></description></item></channel></rss>