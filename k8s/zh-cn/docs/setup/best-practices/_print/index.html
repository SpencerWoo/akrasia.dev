<!doctype html><html lang=zh-cn class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/setup/best-practices/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/best-practices/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/best-practices/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/best-practices/><link rel=alternate hreflang=hi href=https://kubernetes.io/hi/docs/setup/best-practices/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/best-practices/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/zh-cn/docs/setup/best-practices/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>最佳实践 | Kubernetes</title><meta property="og:title" content="最佳实践"><meta property="og:description" content="生产级别的容器编排系统"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/zh-cn/docs/setup/best-practices/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="最佳实践"><meta itemprop=description content="生产级别的容器编排系统"><meta name=twitter:card content="summary"><meta name=twitter:title content="最佳实践"><meta name=twitter:description content="生产级别的容器编排系统"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/zh-cn/docs/setup/best-practices/"><meta property="og:title" content="最佳实践"><meta name=twitter:title content="最佳实践"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/zh-cn/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/zh-cn/docs/>文档</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/blog/>Kubernetes 博客</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/training/>培训</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/partners/>合作伙伴</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/community/>社区</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/case-studies/>案例分析</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>版本列表</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/zh-cn/docs/setup/best-practices/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/zh-cn/docs/setup/best-practices/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/zh-cn/docs/setup/best-practices/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/zh-cn/docs/setup/best-practices/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/zh-cn/docs/setup/best-practices/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>中文 (Chinese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/setup/best-practices/>English</a>
<a class=dropdown-item href=/ko/docs/setup/best-practices/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/setup/best-practices/>日本語 (Japanese)</a>
<a class=dropdown-item href=/id/docs/setup/best-practices/>Bahasa Indonesia</a>
<a class=dropdown-item href=/hi/docs/setup/best-practices/>हिन्दी (Hindi)</a>
<a class=dropdown-item href=/uk/docs/setup/best-practices/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.</p><p><a href=/zh-cn/docs/setup/best-practices/>返回本页常规视图</a>.</p></div><h1 class=title>最佳实践</h1><ul><li>1: <a href=#pg-c797ee17120176c685455db89ae091a9>大规模集群的注意事项</a></li><li>2: <a href=#pg-970615c97499e3651fd3a98e0387cefc>运行于多可用区环境</a></li><li>3: <a href=#pg-f89867de1d34943f1524f67a241f5cc9>校验节点设置</a></li><li>4: <a href=#pg-92a61cf5b0575aa3500f7665b68127d1>强制实施 Pod 安全性标准</a></li><li>5: <a href=#pg-0394f813094b7a35058dffe5b8bacd20>PKI 证书和要求</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-c797ee17120176c685455db89ae091a9>1 - 大规模集群的注意事项</h1><p>集群是运行 Kubernetes 代理的、
由<a class=glossary-tooltip title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label=控制平面>控制平面</a>管理的一组
<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>（物理机或虚拟机）。
Kubernetes v1.25 单个集群支持的最大节点数为 5000。
更具体地说，Kubernetes 旨在适应满足以下<strong>所有</strong>标准的配置：</p><ul><li>每个节点的 Pod 数量不超过 110</li><li>节点数不超过 5000</li><li>Pod 总数不超过 150000</li><li>容器总数不超过 300000</li></ul><p>你可以通过添加或删除节点来扩展集群。集群扩缩的方式取决于集群的部署方式。</p><h2 id=quota-issues>云供应商资源配额</h2><p>为避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑以下事项：</p><ul><li>请求增加云资源的配额，例如：<ul><li>计算实例</li><li>CPUs</li><li>存储卷</li><li>使用中的 IP 地址</li><li>数据包过滤规则集</li><li>负载均衡数量</li><li>网络子网</li><li>日志流</li></ul></li><li>由于某些云供应商限制了创建新实例的速度，因此通过分批启动新节点来控制集群扩展操作，并在各批之间有一个暂停。</li></ul><h2 id=control-plane-components>控制面组件</h2><p>对于大型集群，你需要一个具有足够计算能力和其他资源的控制平面。</p><p>通常，你将在每个故障区域运行一个或两个控制平面实例，
先垂直缩放这些实例，然后在到达下降点（垂直）后再水平缩放。</p><p>你应该在每个故障区域至少应运行一个实例，以提供容错能力。
Kubernetes 节点不会自动将流量引向相同故障区域中的控制平面端点。
但是，你的云供应商可能有自己的机制来执行此操作。</p><p>例如，使用托管的负载均衡器时，你可以配置负载均衡器发送源自故障区域 <strong>A</strong> 中的 kubelet 和 Pod 的流量，
并将该流量仅定向到也位于区域 <strong>A</strong> 中的控制平面主机。
如果单个控制平面主机或端点故障区域 <strong>A</strong> 脱机，则意味着区域 <strong>A</strong> 中的节点的所有控制平面流量现在都在区域之间发送。
在每个区域中运行多个控制平面主机能降低出现这种结果的可能性。</p><h3 id=etcd-storage>etcd 存储</h3><p>为了提高大规模集群的性能，你可以将事件对象存储在单独的专用 etcd 实例中。</p><p>在创建集群时，你可以（使用自定义工具）：</p><ul><li>启动并配置额外的 etcd 实例</li><li>配置 <a class=glossary-tooltip title='提供 Kubernetes API 服务的控制面组件。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API 服务器'>API 服务器</a>，将它用于存储事件</li></ul><p>有关为大型集群配置和管理 etcd 的详细信息，
请参阅<a href=/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/>为 Kubernetes 运行 etcd 集群</a>
和使用 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>kubeadm 创建一个高可用 etcd 集群</a>。</p><h3 id=addon-resources>插件资源</h3><p>Kubernetes <a href=/zh-cn/docs/concepts/configuration/manage-resources-containers/>资源限制</a>
有助于最大程度地减少内存泄漏的影响以及 Pod 和容器可能对其他组件的其他方式的影响。
这些资源限制适用于<a class=glossary-tooltip title='扩展 Kubernetes 功能的资源。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/cluster-administration/addons/ target=_blank aria-label=插件>插件</a>资源，
就像它们适用于应用程序工作负载一样。</p><p>例如，你可以对日志组件设置 CPU 和内存限制</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-cloud-logging<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>fluent/fluentd-kubernetes-daemonset:v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span></code></pre></div><p>插件的默认限制通常基于从中小规模 Kubernetes 集群上运行每个插件的经验收集的数据。
插件在大规模集群上运行时，某些资源消耗常常比其默认限制更多。
如果在不调整这些值的情况下部署了大规模集群，则插件可能会不断被杀死，因为它们不断达到内存限制。
或者，插件可能会运行，但由于 CPU 时间片的限制而导致性能不佳。</p><p>为避免遇到集群插件资源问题，在创建大规模集群时，请考虑以下事项：</p><ul><li>部分垂直扩展插件 —— 总有一个插件副本服务于整个集群或服务于整个故障区域。
对于这些附加组件，请在扩大集群时加大资源请求和资源限制。</li><li>许多水平扩展插件 —— 你可以通过运行更多的 Pod 来增加容量——但是在大规模集群下，
可能还需要稍微提高 CPU 或内存限制。
VerticalPodAutoscaler 可以在 <strong>recommender</strong> 模式下运行，
以提供有关请求和限制的建议数字。</li><li>一些插件在每个节点上运行一个副本，并由 DaemonSet 控制：
例如，节点级日志聚合器。与水平扩展插件的情况类似，
你可能还需要稍微提高 CPU 或内存限制。</li></ul><h2 id=接下来>接下来</h2><p><code>VerticalPodAutoscaler</code> 是一种自定义资源，你可以将其部署到集群中，帮助你管理资源请求和 Pod 的限制。
访问 <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme>Vertical Pod Autoscaler</a>
以了解有关 <code>VerticalPodAutoscaler</code> 的更多信息，
以及如何使用它来扩展集群组件（包括对集群至关重要的插件）的信息。</p><p><a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme>集群自动扩缩器</a>
与许多云供应商集成在一起，帮助你在你的集群中，按照资源需求级别运行正确数量的节点。</p><p><a href=https://github.com/kubernetes/autoscaler/tree/master/addon-resizer#readme>addon resizer</a>
可帮助你在集群规模变化时自动调整插件的大小。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-970615c97499e3651fd3a98e0387cefc>2 - 运行于多可用区环境</h1><p>本页描述如何跨多个区（Zone）中运行集群。</p><h2 id=背景>背景</h2><p>Kubernetes 从设计上允许同一个 Kubernetes 集群跨多个失效区来运行，
通常这些区位于某个称作 <em>区域（region）</em> 逻辑分组中。
主要的云提供商都将区域定义为一组失效区的集合（也称作 <em>可用区（Availability Zones）</em>），
能够提供一组一致的功能特性：每个区域内，各个可用区提供相同的 API 和服务。</p><p>典型的云体系结构都会尝试降低某个区中的失效影响到其他区中服务的概率。</p><h2 id=control-plane-behavior>控制面行为</h2><p>所有的<a href=/zh-cn/docs/concepts/overview/components/#control-plane-components>控制面组件</a>
都支持以一组可相互替换的资源池的形式来运行，每个组件都有多个副本。</p><p>当你部署集群控制面时，应将控制面组件的副本跨多个失效区来部署。
如果可用性是一个很重要的指标，应该选择至少三个失效区，并将每个
控制面组件（API 服务器、调度器、etcd、控制器管理器）复制多个副本，
跨至少三个失效区来部署。如果你在运行云控制器管理器，则也应该将
该组件跨所选的三个失效区来部署。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>Kubernetes 并不会为 API 服务器端点提供跨失效区的弹性。
你可以为集群 API 服务器使用多种技术来提升其可用性，包括使用
DNS 轮转、SRV 记录或者带健康检查的第三方负载均衡解决方案等等。</div><h2 id=node-behavior>节点行为</h2><p>Kubernetes 自动为负载资源（如<a class=glossary-tooltip title=管理集群上的多副本应用。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>
或 <a class=glossary-tooltip title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>)）
跨集群中不同节点来部署其 Pods。
这种分布逻辑有助于降低失效带来的影响。</p><p>节点启动时，每个节点上的 kubelet 会向 Kubernetes API 中代表该 kubelet 的 Node 对象
添加 <a class=glossary-tooltip title=用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=标签>标签</a>。
这些标签可能包含<a href=/zh-cn/docs/reference/labels-annotations-taints/#topologykubernetesiozone>区信息</a>。</p><p>如果你的集群跨了多个可用区或者地理区域，你可以使用节点标签，结合
<a href=/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod 拓扑分布约束</a>
来控制如何在你的集群中多个失效域之间分布 Pods。这里的失效域可以是
地理区域、可用区甚至是特定节点。
这些提示信息使得<a class=glossary-tooltip title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=调度器>调度器</a>
能够更好地分布 Pods，以实现更好的可用性，降低因为某种失效给整个工作负载
带来的风险。</p><p>例如，你可以设置一种约束，确保某个 StatefulSet 中的三个副本都运行在
不同的可用区中，只要其他条件允许。你可以通过声明的方式来定义这种约束，
而不需要显式指定每个工作负载使用哪些可用区。</p><h3 id=distributing-nodes-across-zones>跨多个区分布节点</h3><p>Kubernetes 的核心逻辑并不会帮你创建节点，你需要自行完成此操作，或者使用
类似 <a href=https://cluster-api.sigs.k8s.io/>Cluster API</a> 这类工具来替你管理节点。</p><p>使用类似 Cluster API 这类工具，你可以跨多个失效域来定义一组用做你的集群
工作节点的机器，以及当整个区的服务出现中断时如何自动治愈集群的策略。</p><h2 id=为-pods-手动指定区>为 Pods 手动指定区</h2><p>你可以应用<a href=/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector>节点选择算符约束</a>
到你所创建的 Pods 上，或者为 Deployment、StatefulSet 或 Job 这类工作负载资源
中的 Pod 模板设置此类约束。</p><h2 id=跨区的存储访问>跨区的存储访问</h2><p>当创建持久卷时，<code>PersistentVolumeLabel</code>
<a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>
会自动向那些链接到特定区的 PersistentVolume 添加区标签。
<a class=glossary-tooltip title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=调度器>调度器</a>通过其
<code>NoVolumeZoneConflict</code> 断言确保申领给定 PersistentVolume 的 Pods 只会
被调度到该卷所在的可用区。</p><p>你可以为 PersistentVolumeClaim 指定<a class=glossary-tooltip title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/storage/storage-classes/ target=_blank aria-label=StorageClass>StorageClass</a>
以设置该类中的存储可以使用的失效域（区）。
要了解如何配置能够感知失效域或区的 StorageClass，请参阅
<a href=/zh-cn/docs/concepts/storage/storage-classes/#allowed-topologies>可用的拓扑逻辑</a>。</p><h2 id=networking>网络</h2><p>Kubernetes 自身不提供与可用区相关的联网配置。
你可以使用<a href=/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>网络插件</a>
来配置集群的联网，该网络解决方案可能拥有一些与可用区相关的元素。
例如，如果你的云提供商支持 <code>type=LoadBalancer</code> 的 Service，则负载均衡器
可能仅会将请求流量发送到运行在负责处理给定连接的负载均衡器组件所在的区。
请查阅云提供商的文档了解详细信息。</p><p>对于自定义的或本地集群部署，也可以考虑这些因素
<a class=glossary-tooltip title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>
<a class=glossary-tooltip title='Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/services-networking/ingress/ target=_blank aria-label=Ingress>Ingress</a> 的行为，
包括处理不同失效区的方法，在很大程度上取决于你的集群是如何搭建的。</p><h2 id=fault-recovery>失效恢复</h2><p>在搭建集群时，你可能需要考虑当某区域中的所有失效区都同时掉线时，是否以及如何
恢复服务。例如，你是否要求在某个区中至少有一个节点能够运行 Pod？
请确保任何对集群很关键的修复工作都不要指望集群中至少有一个健康节点。
例如：当所有节点都不健康时，你可能需要运行某个修复性的 Job，
该 Job 要设置特定的<a class=glossary-tooltip title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=容忍度>容忍度</a>
以便修复操作能够至少将一个节点恢复为可用状态。</p><p>Kubernetes 对这类问题没有现成的解决方案；不过这也是要考虑的因素之一。</p><h2 id=接下来>接下来</h2><p>要了解调度器如何在集群中放置 Pods 并遵从所配置的约束，可参阅
<a href=/zh-cn/docs/concepts/scheduling-eviction/>调度与驱逐</a>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f89867de1d34943f1524f67a241f5cc9>3 - 校验节点设置</h1><nav id=TableOfContents><ul><li><a href=#node-conformance-test>节点一致性测试</a></li><li><a href=#node-prerequisite>节点的前提条件</a></li><li><a href=#running-node-conformance-test>运行节点一致性测试</a></li><li><a href=#running-node-conformance-test-for-other-architectures>针对其他硬件体系结构运行节点一致性测试</a></li><li><a href=#running-selected-test>运行特定的测试</a></li><li><a href=#caveats>注意事项</a></li></ul></nav><h2 id=node-conformance-test>节点一致性测试</h2><p><strong>节点一致性测试</strong> 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。
测试验证节点是否满足 Kubernetes 的最低要求；通过测试的节点有资格加入 Kubernetes 集群。</p><p>该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。</p><h2 id=node-prerequisite>节点的前提条件</h2><p>要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：</p><ul><li>容器运行时 (Docker)</li><li>Kubelet</li></ul><h2 id=running-node-conformance-test>运行节点一致性测试</h2><p>要运行节点一致性测试，请执行以下步骤：</p><ol><li>得出 kubelet 的 <code>--kubeconfig</code> 的值；例如：<code>--kubeconfig=/var/lib/kubelet/config.yaml</code>。
由于测试框架启动了本地控制平面来测试 kubelet，因此使用 <code>http://localhost:8080</code>
作为API 服务器的 URL。
一些其他的 kubelet 命令行参数可能会被用到：<ul><li><code>--cloud-provider</code>：如果使用 <code>--cloud-provider=gce</code>，需要移除这个参数来运行测试。</li></ul></li></ol><ol start=2><li><p>使用以下命令运行节点一致性测试：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># $CONFIG_DIR 是你 Kubelet 的 pod manifest 路径。</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># $LOG_DIR 是测试的输出路径。</span>
</span></span><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  registry.k8s.io/node-test:0.2
</span></span></code></pre></div></li></ol><h2 id=running-node-conformance-test-for-other-architectures>针对其他硬件体系结构运行节点一致性测试</h2><p>Kubernetes 也为其他硬件体系结构的系统提供了节点一致性测试的 Docker 镜像：</p><table><thead><tr><th>架构</th><th style=text-align:center>镜像</th></tr></thead><tbody><tr><td>amd64</td><td style=text-align:center>node-test-amd64</td></tr><tr><td>arm</td><td style=text-align:center>node-test-arm</td></tr><tr><td>arm64</td><td style=text-align:center>node-test-arm64</td></tr></tbody></table><h2 id=running-selected-test>运行特定的测试</h2><p>要运行特定测试，请使用你希望运行的测试的特定表达式覆盖环境变量 <code>FOCUS</code>。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>FOCUS</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># Only run MirrorPod test</span>
</span></span><span style=display:flex><span>  registry.k8s.io/node-test:0.2
</span></span></code></pre></div><p>要跳过特定的测试，请使用你希望跳过的测试的常规表达式覆盖环境变量 <code>SKIP</code>。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>SKIP</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># 运行除 MirrorPod 测试外的所有一致性测试内容</span>
</span></span><span style=display:flex><span>  registry.k8s.io/node-test:0.2
</span></span></code></pre></div><p>节点一致性测试是<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md>节点端到端测试</a>的容器化版本。</p><p>默认情况下，它会运行所有一致性测试。</p><p>理论上，只要合理地配置容器和挂载所需的卷，就可以运行任何的节点端到端测试用例。但是这里<strong>强烈建议只运行一致性测试</strong>，因为运行非一致性测试需要很多复杂的配置。</p><h2 id=caveats>注意事项</h2><ul><li>测试会在节点上遗留一些 Docker 镜像，包括节点一致性测试本身的镜像和功能测试相关的镜像。</li><li>测试会在节点上遗留一些死的容器。这些容器是在功能测试的过程中创建的。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-92a61cf5b0575aa3500f7665b68127d1>4 - 强制实施 Pod 安全性标准</h1><p>本页提供实施 <a href=/zh-cn/docs/concepts/security/pod-security-standards>Pod 安全标准（Pod Security Standards）</a>
时的一些最佳实践。</p><h2 id=使用内置的-pod-安全性准入控制器>使用内置的 Pod 安全性准入控制器</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.25 [stable]</code></div><p><a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/#podsecurity>Pod 安全性准入控制器</a>
尝试替换已被废弃的 PodSecurityPolicies。</p><h3 id=configure-all-cluster-namespaces>配置所有集群名字空间</h3><p>完全未经配置的名字空间应该被视为集群安全模型中的重大缺陷。
我们建议花一些时间来分析在每个名字空间中执行的负载的类型，
并通过引用 Pod 安全性标准来确定每个负载的合适级别。
未设置标签的名字空间应该视为尚未被评估。</p><p>针对所有名字空间中的所有负载都具有相同的安全性需求的场景，
我们提供了一个<a href=/zh-cn/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/#applying-to-all-namespaces>示例</a>
用来展示如何批量应用 Pod 安全性标签。</p><h3 id=拥抱最小特权原则>拥抱最小特权原则</h3><p>在一个理想环境中，每个名字空间中的每个 Pod 都会满足 <code>restricted</code> 策略的需求。
不过，这既不可能也不现实，某些负载会因为合理的原因而需要特权上的提升。</p><ul><li>允许 <code>privileged</code> 负载的名字空间需要建立并实施适当的访问控制机制。</li><li>对于运行在特权宽松的名字空间中的负载，需要维护其独特安全性需求的文档。
如果可能的话，要考虑如何进一步约束这些需求。</li></ul><h3 id=采用多种模式的策略>采用多种模式的策略</h3><p>Pod 安全性标准准入控制器的 <code>audit</code> 和 <code>warn</code> 模式（mode）
能够在不影响现有负载的前提下，让该控制器更方便地收集关于 Pod 的重要的安全信息。</p><p>针对所有名字空间启用这些模式是一种好的实践，将它们设置为你最终打算 <code>enforce</code> 的
<em>期望的</em> 级别和版本。这一阶段中所生成的警告和审计注解信息可以帮助你到达这一状态。
如果你期望负载的作者能够作出变更以便适应期望的级别，可以启用 <code>warn</code> 模式。
如果你希望使用审计日志了监控和驱动变更，以便负载能够适应期望的级别，可以启用 <code>audit</code> 模式。</p><p>当你将 <code>enforce</code> 模式设置为期望的取值时，这些模式在不同的场合下仍然是有用的：</p><ul><li>通过将 <code>warn</code> 设置为 <code>enforce</code> 相同的级别，客户可以在尝试创建无法通过合法检查的 Pod
（或者包含 Pod 模板的资源）时收到警告信息。这些信息会帮助于更新资源使其合规。</li><li>在将 <code>enforce</code> 锁定到特定的非最新版本的名字空间中，将 <code>audit</code> 和 <code>warn</code>
模式设置为 <code>enforce</code> 一样的级别而非 <code>latest</code> 版本，
这样可以方便看到之前版本所允许但当前最佳实践中被禁止的设置。</li></ul><h2 id=third-party-alternatives>第三方替代方案</h2><div class="alert alert-secondary callout third-party-content" role=alert><strong>说明：</strong>
本部分链接到提供 Kubernetes 所需功能的第三方项目。Kubernetes 项目作者不负责这些项目。此页面遵循<a href=https://github.com/cncf/foundation/blob/master/website-guidelines.md target=_blank>CNCF 网站指南</a>，按字母顺序列出项目。要将项目添加到此列表中，请在提交更改之前阅读<a href=/docs/contribute/style/content-guide/#third-party-content>内容指南</a>。</div><p>Kubernetes 生态系统中也有一些其他强制实施安全设置的替代方案处于开发状态中：</p><ul><li><a href=https://github.com/kubewarden>Kubewarden</a>.</li><li><a href=https://kyverno.io/policies/>Kyverno</a>.</li><li><a href=https://github.com/open-policy-agent/gatekeeper>OPA Gatekeeper</a>.</li></ul><p>采用 <em>内置的</em> 方案（例如 PodSecurity 准入控制器）还是第三方工具，
这一决策完全取决于你自己的情况。在评估任何解决方案时，对供应链的信任都是至关重要的。
最终，使用前述方案中的 <em>任何</em> 一种都好过放任自流。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0394f813094b7a35058dffe5b8bacd20>5 - PKI 证书和要求</h1><p>Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果你是使用
<a href=/zh-cn/docs/reference/setup-tools/kubeadm/>kubeadm</a> 安装的 Kubernetes，
则会自动生成集群所需的证书。你还可以生成自己的证书。
例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。</p><h2 id=how-certificates-are-used-by-your-cluster>集群是如何使用证书的</h2><p>Kubernetes 需要 PKI 才能执行以下操作：</p><ul><li>Kubelet 的客户端证书，用于 API 服务器身份验证</li><li>Kubelet <a href=/zh-cn/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/#client-and-serving-certificates>服务端证书</a>，
用于 API 服务器与 Kubelet 的会话</li><li>API 服务器端点的证书</li><li>集群管理员的客户端证书，用于 API 服务器身份认证</li><li>API 服务器的客户端证书，用于和 Kubelet 的会话</li><li>API 服务器的客户端证书，用于和 etcd 的会话</li><li>控制器管理器的客户端证书或 kubeconfig，用于和 API 服务器的会话</li><li>调度器的客户端证书或 kubeconfig，用于和 API 服务器的会话</li><li><a href=/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/>前端代理</a>的客户端及服务端证书</li></ul><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>只有当你运行 kube-proxy
并要支持<a href=/zh-cn/docs/tasks/extend-kubernetes/setup-extension-api-server/>扩展 API 服务器</a>时，
才需要 <code>front-proxy</code> 证书。</div><p>etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。</p><h2 id=where-certificates-are-stored>证书存放的位置</h2><p>假如通过 kubeadm 安装 Kubernetes，大多数证书都存储在 <code>/etc/kubernetes/pki</code>。
本文档中的所有路径都是相对于该目录的，但用户账户证书除外，kubeadm 将其放在 <code>/etc/kubernetes</code> 中。</p><h2 id=configure-certificates-manually>手动配置证书</h2><p>如果你不想通过 kubeadm 生成这些必需的证书，你可以使用一个单一的根 CA
来创建这些证书或者直接提供所有证书。
参见<a href=/zh-cn/docs/tasks/administer-cluster/certificates/>证书</a>以进一步了解创建自己的证书机构。
关于管理证书的更多信息，请参见<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/>使用 kubeadm 进行证书管理</a>。</p><h3 id=single-root-ca>单根 CA</h3><p>你可以创建由管理员控制的单根 CA。该根 CA 可以创建多个中间 CA，并将所有进一步的创建委托给 Kubernetes。</p><p>需要这些 CA：</p><table><thead><tr><th>路径</th><th>默认 CN</th><th>描述</th></tr></thead><tbody><tr><td>ca.crt,key</td><td>kubernetes-ca</td><td>Kubernetes 通用 CA</td></tr><tr><td>etcd/ca.crt,key</td><td>etcd-ca</td><td>与 etcd 相关的所有功能</td></tr><tr><td>front-proxy-ca.crt,key</td><td>kubernetes-front-proxy-ca</td><td>用于<a href=/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/>前端代理</a></td></tr></tbody></table><p>上面的 CA 之外，还需要获取用于服务账户管理的密钥对，也就是 <code>sa.key</code> 和 <code>sa.pub</code>。</p><p>下面的例子说明了上表中所示的 CA 密钥和证书文件。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-ca.key
</span></span></span></code></pre></div><h3 id=all-certificates>所有的证书</h3><p>如果你不想将 CA 的私钥拷贝至你的集群中，你也可以自己生成全部的证书。</p><p>需要这些证书：</p><table><thead><tr><th>默认 CN</th><th>父级 CA</th><th>O (位于 Subject 中)</th><th>类型</th><th>主机 (SAN)</th></tr></thead><tbody><tr><td>kube-etcd</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-peer</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd-ca</td><td></td><td>client</td><td></td></tr><tr><td>kube-apiserver-etcd-client</td><td>etcd-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>kube-apiserver</td><td>kubernetes-ca</td><td></td><td>server</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>&lt;advertise_IP></code>, <code>[1]</code></td></tr><tr><td>kube-apiserver-kubelet-client</td><td>kubernetes-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>front-proxy-client</td><td>kubernetes-front-proxy-ca</td><td></td><td>client</td><td></td></tr></tbody></table><p>[1]: 用来连接到集群的不同 IP 或 DNS 名
（就像 <a href=/zh-cn/docs/reference/setup-tools/kubeadm/>kubeadm</a> 为负载均衡所使用的固定
IP 或 DNS 名：<code>kubernetes</code>、<code>kubernetes.default</code>、<code>kubernetes.default.svc</code>、
<code>kubernetes.default.svc.cluster</code>、<code>kubernetes.default.svc.cluster.local</code>）。</p><p>其中，<code>kind</code> 对应一种或多种类型的 <a href=https://pkg.go.dev/k8s.io/api/certificates/v1beta1#KeyUsage>x509 密钥用途</a>：</p><table><thead><tr><th>kind</th><th>密钥用途</th></tr></thead><tbody><tr><td>server</td><td>数字签名、密钥加密、服务端认证</td></tr><tr><td>client</td><td>数字签名、密钥加密、客户端认证</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>上面列出的 Hosts/SAN 是推荐的配置方式；如果需要特殊安装，则可以在所有服务器证书上添加其他 SAN。</div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>对于 kubeadm 用户：</p><ul><li>不使用私钥，将证书复制到集群 CA 的方案，在 kubeadm 文档中将这种方案称为外部 CA。</li><li>如果将以上列表与 kubeadm 生成的 PKI 进行比较，你会注意到，如果使用外部 etcd，则不会生成
<code>kube-etcd</code>、<code>kube-etcd-peer</code> 和 <code>kube-etcd-healthcheck-client</code> 证书。</li></ul></div><h3 id=certificate-paths>证书路径</h3><p>证书应放置在建议的路径中（以便 <a href=/zh-cn/docs/reference/setup-tools/kubeadm/>kubeadm</a>
使用）。无论使用什么位置，都应使用给定的参数指定路径。</p><table><thead><tr><th>默认 CN</th><th>建议的密钥路径</th><th>建议的证书路径</th><th>命令</th><th>密钥参数</th><th>证书参数</th></tr></thead><tbody><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>kube-apiserver</td><td></td><td>--etcd-cafile</td></tr><tr><td>kube-apiserver-etcd-client</td><td>apiserver-etcd-client.key</td><td>apiserver-etcd-client.crt</td><td>kube-apiserver</td><td>--etcd-keyfile</td><td>--etcd-certfile</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-apiserver</td><td></td><td>--client-ca-file</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-controller-manager</td><td>--cluster-signing-key-file</td><td>--client-ca-file, --root-ca-file, --cluster-signing-cert-file</td></tr><tr><td>kube-apiserver</td><td>apiserver.key</td><td>apiserver.crt</td><td>kube-apiserver</td><td>--tls-private-key-file</td><td>--tls-cert-file</td></tr><tr><td>kube-apiserver-kubelet-client</td><td>apiserver-kubelet-client.key</td><td>apiserver-kubelet-client.crt</td><td>kube-apiserver</td><td>--kubelet-client-key</td><td>--kubelet-client-certificate</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-apiserver</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-controller-manager</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-client</td><td>front-proxy-client.key</td><td>front-proxy-client.crt</td><td>kube-apiserver</td><td>--proxy-client-key-file</td><td>--proxy-client-cert-file</td></tr><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>etcd</td><td></td><td>--trusted-ca-file, --peer-trusted-ca-file</td></tr><tr><td>kube-etcd</td><td>etcd/server.key</td><td>etcd/server.crt</td><td>etcd</td><td>--key-file</td><td>--cert-file</td></tr><tr><td>kube-etcd-peer</td><td>etcd/peer.key</td><td>etcd/peer.crt</td><td>etcd</td><td>--peer-key-file</td><td>--peer-cert-file</td></tr><tr><td>etcd-ca</td><td></td><td>etcd/ca.crt</td><td>etcdctl</td><td></td><td>--cacert</td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd/healthcheck-client.key</td><td>etcd/healthcheck-client.crt</td><td>etcdctl</td><td>--key</td><td>--cert</td></tr></tbody></table><p>注意事项同样适用于服务帐户密钥对：</p><table><thead><tr><th>私钥路径</th><th>公钥路径</th><th>命令</th><th>参数</th></tr></thead><tbody><tr><td>sa.key</td><td></td><td>kube-controller-manager</td><td>--service-account-private-key-file</td></tr><tr><td></td><td>sa.pub</td><td>kube-apiserver</td><td>--service-account-key-file</td></tr></tbody></table><p>下面的例子展示了自行生成所有密钥和证书时所需要提供的文件路径。
这些路径基于<a href=/zh-cn/docs/setup/best-practices/certificates/#certificate-paths>前面的表格</a>。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver-etcd-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver-etcd-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver-kubelet-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver-kubelet-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/server.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/server.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/peer.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/peer.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/healthcheck-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/healthcheck-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/sa.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/sa.pub
</span></span></span></code></pre></div><h2 id=configure-certificates-for-user-accounts>为用户帐户配置证书</h2><p>你必须手动配置以下管理员帐户和服务帐户：</p><table><thead><tr><th>文件名</th><th>凭据名称</th><th>默认 CN</th><th>O (位于 Subject 中)</th></tr></thead><tbody><tr><td>admin.conf</td><td>default-admin</td><td>kubernetes-admin</td><td>system:masters</td></tr><tr><td>kubelet.conf</td><td>default-auth</td><td>system:node:<code>&lt;nodeName></code> （参阅注释）</td><td>system:nodes</td></tr><tr><td>controller-manager.conf</td><td>default-controller-manager</td><td>system:kube-controller-manager</td><td></td></tr><tr><td>scheduler.conf</td><td>default-scheduler</td><td>system:kube-scheduler</td><td></td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>kubelet.conf</code> 中 <code>&lt;nodeName></code> 的值 <strong>必须</strong> 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。
有关更多详细信息，请阅读<a href=/zh-cn/docs/reference/access-authn-authz/node/>节点授权</a>。</div><ol><li><p>对于每个配置，请都使用给定的 CN 和 O 生成 x509 证书/密钥偶对。</p></li><li><p>为每个配置运行下面的 <code>kubectl</code> 命令：</p></li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-cluster default-cluster --server<span style=color:#666>=</span>https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-credentials &lt;credential-name&gt; --client-key &lt;path-to-key&gt;.pem --client-certificate &lt;path-to-cert&gt;.pem --embed-certs
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-context default-system --cluster default-cluster --user &lt;credential-name&gt;
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config use-context default-system
</span></span></code></pre></div><p>这些文件用途如下：</p><table><thead><tr><th>文件名</th><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>admin.conf</td><td>kubectl</td><td>配置集群的管理员</td></tr><tr><td>kubelet.conf</td><td>kubelet</td><td>集群中的每个节点都需要一份</td></tr><tr><td>controller-manager.conf</td><td>kube-controller-manager</td><td>必需添加到 <code>manifests/kube-controller-manager.yaml</code> 清单中</td></tr><tr><td>scheduler.conf</td><td>kube-scheduler</td><td>必需添加到 <code>manifests/kube-scheduler.yaml</code> 清单中</td></tr></tbody></table><p>下面是前表中所列文件的完整路径。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>/etc/kubernetes/admin.conf
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/kubelet.conf
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/controller-manager.conf
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/scheduler.conf
</span></span></span></code></pre></div></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/zh-cn/docs/home/>主页</a>
<a class=text-white href=/zh-cn/blog/>博客</a>
<a class=text-white href=/zh-cn/training/>培训</a>
<a class=text-white href=/zh-cn/partners/>合作伙伴</a>
<a class=text-white href=/zh-cn/community/>社区</a>
<a class=text-white href=/zh-cn/case-studies/>案例分析</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes 作者 | 文档发布基于 <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a> 授权许可</small><br><small class=text-white>Copyright &copy; 2023 Linux 基金会&reg;。保留所有权利。Linux 基金会已注册并使用商标。如需了解 Linux 基金会的商标列表，请访问<a href=https://www.linuxfoundation.org/trademark-usage class=light-text>商标使用页面</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>