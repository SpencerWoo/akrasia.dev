<!doctype html><html lang=zh-cn class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/setup/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/setup/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/setup/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/setup/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/setup/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/><link rel=alternate hreflang=hi href=https://kubernetes.io/hi/docs/setup/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/setup/><link rel=alternate hreflang=pl href=https://kubernetes.io/pl/docs/setup/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/zh-cn/docs/setup/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>入门 | Kubernetes</title><meta property="og:title" content="入门"><meta property="og:description" content="生产级别的容器编排系统"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/zh-cn/docs/setup/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="入门"><meta itemprop=description content="生产级别的容器编排系统"><meta name=twitter:card content="summary"><meta name=twitter:title content="入门"><meta name=twitter:description content="生产级别的容器编排系统"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content=" 本节列出了设置和运行 Kubernetes 的不同方法。
安装 Kubernetes 时，请根据以下条件选择安装类型：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。
你可以下载 Kubernetes，在本地机器、云或你自己的数据中心上部署 Kubernetes 集群。
某些 Kubernetes 组件， 比如 kube-apiserver 或 kube-proxy 等， 可以在集群中以容器镜像部署。
建议尽可能将 Kubernetes 组件作为容器镜像运行，并且让 Kubernetes 管理这些组件。 但是运行容器的相关组件 —— 尤其是 kubelet，不在此列。
如果你不想自己管理 Kubernetes 集群，则可以选择托管服务，包括经过认证的平台。 在各种云和裸机环境中，还有其他标准化和定制的解决方案。
学习环境 如果正打算学习 Kubernetes，请使用 Kubernetes 社区支持 或生态系统中的工具在本地计算机上设置 Kubernetes 集群。 请参阅安装工具。
生产环境 在评估生产环境的解决方案时， 请考虑要自己管理 Kubernetes 集群（或相关抽象）的哪些方面，将哪些托付给提供商。
对于你自己管理的集群，官方支持的用于部署 Kubernetes 的工具是 kubeadm。
接下来 下载 Kubernetes 下载并安装工具，包括 kubectl 在内 为新集群选择容器运行时 了解集群设置的最佳实践 Kubernetes 的设计是让其控制平面在 Linux 上运行的。 在集群中，你可以在 Linux 或其他操作系统（包括 Windows）上运行应用程序。
学习配置包含 Windows 节点的集群 "><meta property="og:description" content=" 本节列出了设置和运行 Kubernetes 的不同方法。
安装 Kubernetes 时，请根据以下条件选择安装类型：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。
你可以下载 Kubernetes，在本地机器、云或你自己的数据中心上部署 Kubernetes 集群。
某些 Kubernetes 组件， 比如 kube-apiserver 或 kube-proxy 等， 可以在集群中以容器镜像部署。
建议尽可能将 Kubernetes 组件作为容器镜像运行，并且让 Kubernetes 管理这些组件。 但是运行容器的相关组件 —— 尤其是 kubelet，不在此列。
如果你不想自己管理 Kubernetes 集群，则可以选择托管服务，包括经过认证的平台。 在各种云和裸机环境中，还有其他标准化和定制的解决方案。
学习环境 如果正打算学习 Kubernetes，请使用 Kubernetes 社区支持 或生态系统中的工具在本地计算机上设置 Kubernetes 集群。 请参阅安装工具。
生产环境 在评估生产环境的解决方案时， 请考虑要自己管理 Kubernetes 集群（或相关抽象）的哪些方面，将哪些托付给提供商。
对于你自己管理的集群，官方支持的用于部署 Kubernetes 的工具是 kubeadm。
接下来 下载 Kubernetes 下载并安装工具，包括 kubectl 在内 为新集群选择容器运行时 了解集群设置的最佳实践 Kubernetes 的设计是让其控制平面在 Linux 上运行的。 在集群中，你可以在 Linux 或其他操作系统（包括 Windows）上运行应用程序。
学习配置包含 Windows 节点的集群 "><meta name=twitter:description content=" 本节列出了设置和运行 Kubernetes 的不同方法。
安装 Kubernetes 时，请根据以下条件选择安装类型：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。
你可以下载 Kubernetes，在本地机器、云或你自己的数据中心上部署 Kubernetes 集群。
某些 Kubernetes 组件， 比如 kube-apiserver 或 kube-proxy 等， 可以在集群中以容器镜像部署。
建议尽可能将 Kubernetes 组件作为容器镜像运行，并且让 Kubernetes 管理这些组件。 但是运行容器的相关组件 —— 尤其是 kubelet，不在此列。
如果你不想自己管理 Kubernetes 集群，则可以选择托管服务，包括经过认证的平台。 在各种云和裸机环境中，还有其他标准化和定制的解决方案。
学习环境 如果正打算学习 Kubernetes，请使用 Kubernetes 社区支持 或生态系统中的工具在本地计算机上设置 Kubernetes 集群。 请参阅安装工具。
生产环境 在评估生产环境的解决方案时， 请考虑要自己管理 Kubernetes 集群（或相关抽象）的哪些方面，将哪些托付给提供商。
对于你自己管理的集群，官方支持的用于部署 Kubernetes 的工具是 kubeadm。
接下来 下载 Kubernetes 下载并安装工具，包括 kubectl 在内 为新集群选择容器运行时 了解集群设置的最佳实践 Kubernetes 的设计是让其控制平面在 Linux 上运行的。 在集群中，你可以在 Linux 或其他操作系统（包括 Windows）上运行应用程序。
学习配置包含 Windows 节点的集群 "><meta property="og:url" content="https://kubernetes.io/zh-cn/docs/setup/"><meta property="og:title" content="入门"><meta name=twitter:title content="入门"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/zh-cn/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/zh-cn/docs/>文档</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/blog/>Kubernetes 博客</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/training/>培训</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/partners/>合作伙伴</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/community/>社区</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/case-studies/>案例分析</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>版本列表</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/zh-cn/docs/setup/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/zh-cn/docs/setup/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/zh-cn/docs/setup/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/zh-cn/docs/setup/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/zh-cn/docs/setup/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>中文 (Chinese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/setup/>English</a>
<a class=dropdown-item href=/ko/docs/setup/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/setup/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/setup/>Français (French)</a>
<a class=dropdown-item href=/de/docs/setup/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/setup/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/setup/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/setup/>Bahasa Indonesia</a>
<a class=dropdown-item href=/hi/docs/setup/>हिन्दी (Hindi)</a>
<a class=dropdown-item href=/ru/docs/setup/>Русский (Russian)</a>
<a class=dropdown-item href=/pl/docs/setup/>Polski (Polish)</a>
<a class=dropdown-item href=/uk/docs/setup/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.</p><p><a href=/zh-cn/docs/setup/>返回本页常规视图</a>.</p></div><h1 class=title>入门</h1><ul><li>1: <a href=#pg-0b597086a9d1382f86abadcfeab657d6>学习环境</a></li><ul></ul><li>2: <a href=#pg-4e14853fdaa3bd273f31a60112b9b5ac>生产环境</a></li><ul><li>2.1: <a href=#pg-a77d3feb6e6d9978f32fa14622642e9a>容器运行时</a></li><li>2.2: <a href=#pg-00e1646f68aeb89f9722cf6f6cfcad94>使用部署工具安装 Kubernetes</a></li><ul><li>2.2.1: <a href=#pg-a16f59f325a17cdeed324d5c889f7f73>使用 kubeadm 引导集群</a></li><ul><li>2.2.1.1: <a href=#pg-29e59491dd6118b23072dfe9ebb93323>安装 kubeadm</a></li><li>2.2.1.2: <a href=#pg-c3689df4b0c61a998e79d91a865aa244>对 kubeadm 进行故障排查</a></li><li>2.2.1.3: <a href=#pg-134ed1f6142a98e6ac681a1ba4920e53>使用 kubeadm 创建集群</a></li><li>2.2.1.4: <a href=#pg-4c656c5eda3e1c06ad1aedebdc04a211>使用 kubeadm API 定制组件</a></li><li>2.2.1.5: <a href=#pg-015edbc7cc688d31b1d1edce7c186135>高可用拓扑选项</a></li><li>2.2.1.6: <a href=#pg-3941d5c3409342219bf7e03128b8ecb6>利用 kubeadm 创建高可用集群</a></li><li>2.2.1.7: <a href=#pg-8160424c22d24f7d2d63c521e107dbf8>使用 kubeadm 创建一个高可用 etcd 集群</a></li><li>2.2.1.8: <a href=#pg-07709e71de6b4ac2573041c31213dbeb>使用 kubeadm 配置集群中的每个 kubelet</a></li><li>2.2.1.9: <a href=#pg-df2f3f20d404ebe2b03fcda1fcee50e7>使用 kubeadm 支持双协议栈</a></li></ul><li>2.2.2: <a href=#pg-478acca1934b6d89a0bc00fb25bfe5b6>使用 kOps 安装 Kubernetes</a></li><li>2.2.3: <a href=#pg-f8b4964187fe973644e06ee629eff1de>使用 Kubespray 安装 Kubernetes</a></li></ul><li>2.3: <a href=#pg-d2f55eefe7222b7c637875af9c3ec199>Turnkey 云解决方案</a></li></ul><li>3: <a href=#pg-84b6491601d6a2b3da4cd5a105c866ba>最佳实践</a></li><ul><li>3.1: <a href=#pg-c797ee17120176c685455db89ae091a9>大规模集群的注意事项</a></li><li>3.2: <a href=#pg-970615c97499e3651fd3a98e0387cefc>运行于多可用区环境</a></li><li>3.3: <a href=#pg-f89867de1d34943f1524f67a241f5cc9>校验节点设置</a></li><li>3.4: <a href=#pg-92a61cf5b0575aa3500f7665b68127d1>强制实施 Pod 安全性标准</a></li><li>3.5: <a href=#pg-0394f813094b7a35058dffe5b8bacd20>PKI 证书和要求</a></li></ul></ul><div class=content><p>本节列出了设置和运行 Kubernetes 的不同方法。</p><p>安装 Kubernetes 时，请根据以下条件选择安装类型：易于维护、安全性、可控制性、可用资源以及操作和管理 Kubernetes 集群所需的专业知识。</p><p>你可以<a href=/zh-cn/releases/download/>下载 Kubernetes</a>，在本地机器、云或你自己的数据中心上部署 Kubernetes 集群。</p><p>某些 <a href=/zh-cn/docs/concepts/overview/components/>Kubernetes 组件</a>，
比如 <code>kube-apiserver</code> 或 <code>kube-proxy</code> 等，
可以在集群中以<a href=/zh-cn/releases/download/#container-images>容器镜像</a>部署。</p><p><strong>建议</strong>尽可能将 Kubernetes 组件作为容器镜像运行，并且让 Kubernetes 管理这些组件。
但是运行容器的相关组件 —— 尤其是 kubelet，不在此列。</p><p>如果你不想自己管理 Kubernetes 集群，则可以选择托管服务，包括<a href=/zh-cn/docs/setup/production-environment/turnkey-solutions/>经过认证的平台</a>。
在各种云和裸机环境中，还有其他标准化和定制的解决方案。</p><h2 id=学习环境>学习环境</h2><p>如果正打算学习 Kubernetes，请使用 Kubernetes 社区支持
或生态系统中的工具在本地计算机上设置 Kubernetes 集群。
请参阅<a href=/zh-cn/docs/tasks/tools/>安装工具</a>。</p><h2 id=生产环境>生产环境</h2><p>在评估<a href=/zh-cn/docs/setup/production-environment/>生产环境</a>的解决方案时，
请考虑要自己管理 Kubernetes 集群（或相关抽象）的哪些方面，将哪些托付给提供商。</p><p>对于你自己管理的集群，官方支持的用于部署 Kubernetes 的工具是
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/>kubeadm</a>。</p><h2 id=接下来>接下来</h2><ul><li><a href=/zh-cn/releases/download/>下载 Kubernetes</a></li><li>下载并<a href=/zh-cn/docs/tasks/tools/>安装工具</a>，包括 kubectl 在内</li><li>为新集群选择<a href=/zh-cn/docs/setup/production-environment/container-runtimes/>容器运行时</a></li><li>了解集群设置的<a href=/zh-cn/docs/setup/best-practices/>最佳实践</a></li></ul><p>Kubernetes 的设计是让其<a class=glossary-tooltip title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label=控制平面>控制平面</a>在 Linux 上运行的。
在集群中，你可以在 Linux 或其他操作系统（包括 Windows）上运行应用程序。</p><ul><li>学习<a href=/zh-cn/docs/concepts/windows/>配置包含 Windows 节点的集群</a></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-0b597086a9d1382f86abadcfeab657d6>1 - 学习环境</h1></div><div class=td-content style=page-break-before:always><h1 id=pg-4e14853fdaa3bd273f31a60112b9b5ac>2 - 生产环境</h1><p>生产质量的 Kubernetes 集群需要规划和准备。
如果你的 Kubernetes 集群是用来运行关键负载的，该集群必须被配置为弹性的（Resilient）。
本页面阐述你在安装生产就绪的集群或将现有集群升级为生产用途时可以遵循的步骤。
如果你已经熟悉生产环境安装，因此只关注一些链接，则可以跳到<a href=#what-s-next>接下来</a>节。</p><h2 id=production-considerations>生产环境考量</h2><p>通常，一个生产用 Kubernetes 集群环境与个人学习、开发或测试环境所使用的 Kubernetes 相比有更多的需求。
生产环境可能需要被很多用户安全地访问，需要提供一致的可用性，以及能够与需求变化相适配的资源。</p><p>在你决定在何处运行你的生产用 Kubernetes 环境（在本地或者在云端），
以及你希望承担或交由他人承担的管理工作量时，
需要考察以下因素如何影响你对 Kubernetes 集群的需求：</p><ul><li><strong>可用性</strong>：一个单机的 Kubernetes <a href=/zh-cn/docs/setup/#%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83>学习环境</a>
具有单点失效特点。创建高可用的集群则意味着需要考虑：<ul><li>将控制面与工作节点分开</li><li>在多个节点上提供控制面组件的副本</li><li>为针对集群的 <a class=glossary-tooltip title='提供 Kubernetes API 服务的控制面组件。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API 服务器'>API 服务器</a>
的流量提供负载均衡</li><li>随着负载的合理需要，提供足够的可用的（或者能够迅速变为可用的）工作节点</li></ul></li></ul><ul><li><strong>规模</strong>：如果你预期你的生产用 Kubernetes 环境要承受固定量的请求，
你可能可以针对所需要的容量来一次性完成安装。
不过，如果你预期服务请求会随着时间增长，或者因为类似季节或者特殊事件的原因而发生剧烈变化，
你就需要规划如何处理请求上升时对控制面和工作节点的压力，或者如何缩减集群规模以减少未使用资源的消耗。</li></ul><ul><li><strong>安全性与访问管理</strong>：在你自己的学习环境 Kubernetes 集群上，你拥有完全的管理员特权。
但是针对运行着重要工作负载的共享集群，用户账户不止一两个时，
就需要更细粒度的方案来确定谁或者哪些主体可以访问集群资源。
你可以使用基于角色的访问控制（<a href=/zh-cn/docs/reference/access-authn-authz/rbac/>RBAC</a>）
和其他安全机制来确保用户和负载能够访问到所需要的资源，
同时确保工作负载及集群自身仍然是安全的。
你可以通过管理<a href=/zh-cn/docs/concepts/policy/>策略</a>和
<a href=/zh-cn/docs/concepts/configuration/manage-resources-containers>容器资源</a>
来针对用户和工作负载所可访问的资源设置约束。</li></ul><p>在自行构建 Kubernetes 生产环境之前，
请考虑将这一任务的部分或者全部交给<a href=/zh-cn/docs/setup/production-environment/turnkey-solutions>云方案承包服务</a>提供商或者其他
<a href=/zh-cn/partners/>Kubernetes 合作伙伴</a>。选项有：</p><ul><li><strong>无服务</strong>：仅是在第三方设备上运行负载，完全不必管理集群本身。
你需要为 CPU 用量、内存和磁盘请求等付费。</li><li><strong>托管控制面</strong>：让供应商决定集群控制面的规模和可用性，并负责打补丁和升级等操作。</li><li><strong>托管工作节点</strong>：配置一个节点池来满足你的需要，由供应商来确保节点始终可用，并在需要的时候完成升级。</li><li><strong>集成</strong>：有一些供应商能够将 Kubernetes 与一些你可能需要的其他服务集成，
这类服务包括存储、容器镜像仓库、身份认证方法以及开发工具等。</li></ul><p>无论你是自行构造一个生产用 Kubernetes 集群还是与合作伙伴一起协作，
请审阅下面章节以评估你的需求，因为这关系到你的集群的<strong>控制面</strong>、<strong>工作节点</strong>、<strong>用户访问</strong>以及<strong>负载资源</strong>。</p><h2 id=production-cluster-setup>生产用集群安装</h2><p>在生产质量的 Kubernetes 集群中，控制面用不同的方式来管理集群和可以分布到多个计算机上的服务。
每个工作节点则代表的是一个可配置来运行 Kubernetes Pod 的实体。</p><h3 id=production-control-plane>生产用控制面</h3><p>最简单的 Kubernetes 集群中，整个控制面和工作节点服务都运行在同一台机器上。
你可以通过添加工作节点来提升环境运算能力，正如
<a href=/zh-cn/docs/concepts/overview/components/>Kubernetes 组件</a>示意图所示。
如果只需要集群在很短的一段时间内可用，或者可以在某些事物出现严重问题时直接丢弃，
这种配置可能符合你的需要。</p><p>如果你需要一个更为持久的、高可用的集群，那么就需要考虑扩展控制面的方式。
根据设计，运行在一台机器上的单机控制面服务不是高可用的。
如果你认为保持集群的正常运行并需要确保它在出错时可以被修复是很重要的，
可以考虑以下步骤：</p><ul><li><strong>选择部署工具</strong>：你可以使用类似 kubeadm、kops 和 kubespray 这类工具来部署控制面。
参阅<a href=/zh-cn/docs/setup/production-environment/tools/>使用部署工具安装 Kubernetes</a>
以了解使用这类部署方法来完成生产就绪部署的技巧。
存在不同的<a href=/zh-cn/docs/setup/production-environment/container-runtimes/>容器运行时</a>
可供你的部署采用。</li></ul><ul><li><strong>管理证书</strong>：控制面服务之间的安全通信是通过证书来完成的。
证书是在部署期间自动生成的，或者你也可以使用自己的证书机构来生成它们。
参阅 <a href=/zh-cn/docs/setup/best-practices/certificates/>PKI 证书和需求</a>了解细节。</li></ul><ul><li><strong>为 API 服务器配置负载均衡</strong>：配置负载均衡器来将外部的 API 请求散布给运行在不同节点上的 API 服务实例。
参阅<a href=/zh-cn/docs/tasks/access-application-cluster/create-external-load-balancer/>创建外部负载均衡器</a>了解细节。</li></ul><ul><li><strong>分离并备份 etcd 服务</strong>：etcd 服务可以运行于其他控制面服务所在的机器上，
也可以运行在不同的机器上以获得更好的安全性和可用性。
因为 etcd 存储着集群的配置数据，应该经常性地对 etcd 数据库进行备份，
以确保在需要的时候你可以修复该数据库。与配置和使用 etcd 相关的细节可参阅
<a href=/https://etcd.io/docs/v3.5/faq/>etcd FAQ</a>。
更多的细节可参阅<a href=/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/>为 Kubernetes 运维 etcd 集群</a>
和<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>使用 kubeadm 配置高可用的 etcd 集群</a>。</li></ul><ul><li><strong>创建多控制面系统</strong>：为了实现高可用性，控制面不应被限制在一台机器上。
如果控制面服务是使用某 init 服务（例如 systemd）来运行的，每个服务应该至少运行在三台机器上。
不过，将控制面作为服务运行在 Kubernetes Pod 中可以确保你所请求的个数的服务始终保持可用。
调度器应该是可容错的，但不是高可用的。
某些部署工具会安装 <a href=https://raft.github.io/>Raft</a> 票选算法来对 Kubernetes 服务执行领导者选举。
如果主节点消失，另一个服务会被选中并接手相应服务。</li></ul><ul><li><strong>跨多个可用区</strong>：如果保持你的集群一直可用这点非常重要，可以考虑创建一个跨多个数据中心的集群；
在云环境中，这些数据中心被视为可用区。若干个可用区在一起可构成地理区域。
通过将集群分散到同一区域中的多个可用区内，即使某个可用区不可用，整个集群能够继续工作的机会也大大增加。
更多的细节可参阅<a href=/zh-cn/docs/setup/best-practices/multiple-zones/>跨多个可用区运行</a>。</li></ul><ul><li><strong>管理演进中的特性</strong>：如果你计划长时间保留你的集群，就需要执行一些维护其健康和安全的任务。
例如，如果你采用 kubeadm 安装的集群，
则有一些可以帮助你完成
<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/>证书管理</a>
和<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade>升级 kubeadm 集群</a>
的指令。
参见<a href=/zh-cn/docs/tasks/administer-cluster>管理集群</a>了解一个 Kubernetes
管理任务的较长列表。</li></ul><p>如要了解运行控制面服务时可使用的选项，可参阅
<a href=/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver</a>、
<a href=/zh-cn/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-manager</a> 和
<a href=/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a>
组件参考页面。
如要了解高可用控制面的例子，可参阅<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/>高可用拓扑结构选项</a>、
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/>使用 kubeadm 创建高可用集群</a>
以及<a href=/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/>为 Kubernetes 运维 etcd 集群</a>。
关于制定 etcd 备份计划，可参阅<a href=/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster>对 etcd 集群执行备份</a>。</p><h3 id=production-worker-nodes>生产用工作节点</h3><p>生产质量的工作负载需要是弹性的；它们所依赖的其他组件（例如 CoreDNS）也需要是弹性的。
无论你是自行管理控制面还是让云供应商来管理，你都需要考虑如何管理工作节点
（有时也简称为<strong>节点</strong>）。</p><ul><li><strong>配置节点</strong>：节点可以是物理机或者虚拟机。如果你希望自行创建和管理节点，
你可以安装一个受支持的操作系统，之后添加并运行合适的<a href=/zh-cn/docs/concepts/overview/components/#node-components>节点服务</a>。考虑：<ul><li>在安装节点时要通过配置适当的内存、CPU 和磁盘读写速率、存储容量来满足你的负载的需求。</li><li>是否通用的计算机系统即足够，还是你有负载需要使用 GPU 处理器、Windows 节点或者 VM 隔离。</li></ul></li></ul><ul><li><strong>验证节点</strong>：参阅<a href=/zh-cn/docs/setup/best-practices/node-conformance/>验证节点配置</a>以了解如何确保节点满足加入到 Kubernetes 集群的需求。</li></ul><ul><li><strong>添加节点到集群中</strong>：如果你自行管理你的集群，你可以通过安装配置你的机器，
之后或者手动加入集群，或者让它们自动注册到集群的 API 服务器。
参阅<a href=/zh-cn/docs/concepts/architecture/nodes/>节点</a>节，了解如何配置 Kubernetes 以便以这些方式来添加节点。</li></ul><ul><li><strong>扩缩节点</strong>：制定一个扩充集群容量的规划，你的集群最终会需要这一能力。
参阅<a href=/zh-cn/docs/setup/best-practices/cluster-large/>大规模集群考察事项</a>
以确定你所需要的节点数；
这一规模是基于你要运行的 Pod 和容器个数来确定的。
如果你自行管理集群节点，这可能意味着要购买和安装你自己的物理设备。</li></ul><ul><li><strong>节点自动扩缩容</strong>：大多数云供应商支持
<a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme>集群自动扩缩器（Cluster Autoscaler）</a>
以便替换不健康的节点、根据需求来增加或缩减节点个数。
参阅<a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md>常见问题</a>
了解自动扩缩器的工作方式，并参阅
<a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment>Deployment</a>
了解不同云供应商是如何实现集群自动扩缩器的。
对于本地集群，有一些虚拟化平台可以通过脚本来控制按需启动新节点。</li></ul><ul><li><strong>安装节点健康检查</strong>：对于重要的工作负载，你会希望确保节点以及在节点上运行的 Pod 处于健康状态。
通过使用 <a href=/zh-cn/docs/tasks/debug/debug-cluster/monitor-node-health/>Node Problem Detector</a>，
你可以确保你的节点是健康的。</li></ul><h3 id=production-user-management>生产级用户环境</h3><p>在生产环境中，情况可能不再是你或者一小组人在访问集群，而是几十上百人需要访问集群。
在学习环境或者平台原型环境中，你可能具有一个可以执行任何操作的管理账号。
在生产环境中，你会需要对不同名字空间具有不同访问权限级别的很多账号。</p><p>建立一个生产级别的集群意味着你需要决定如何有选择地允许其他用户访问集群。
具体而言，你需要选择验证尝试访问集群的人的身份标识（身份认证），
并确定他们是否被许可执行他们所请求的操作（鉴权）：</p><ul><li><strong>认证（Authentication）</strong>：API 服务器可以使用客户端证书、持有者令牌、
身份认证代理或者 HTTP 基本认证机制来完成身份认证操作。
你可以选择你要使用的认证方法。通过使用插件，
API 服务器可以充分利用你所在组织的现有身份认证方法，
例如 LDAP 或者 Kerberos。
关于认证 Kubernetes 用户身份的不同方法的描述，
可参阅<a href=/zh-cn/docs/reference/access-authn-authz/authentication/>身份认证</a>。</li></ul><ul><li><strong>鉴权（Authorization）</strong>：当你准备为一般用户执行权限判定时，
你可能会需要在 RBAC 和 ABAC 鉴权机制之间做出选择。
参阅<a href=/zh-cn/docs/reference/access-authn-authz/authorization/>鉴权概述</a>，
了解对用户账户（以及访问你的集群的服务账户）执行鉴权的不同模式。<ul><li><strong>基于角色的访问控制</strong>（<a href=/zh-cn/docs/reference/access-authn-authz/rbac/>RBAC</a>）：
让你通过为通过身份认证的用户授权特定的许可集合来控制集群访问。
访问许可可以针对某特定名字空间（Role）或者针对整个集群（ClusterRole）。
通过使用 RoleBinding 和 ClusterRoleBinding 对象，这些访问许可可以被关联到特定的用户身上。</li></ul><ul><li><strong>基于属性的访问控制</strong>（<a href=/zh-cn/docs/reference/access-authn-authz/abac/>ABAC</a>）：
让你能够基于集群中资源的属性来创建访问控制策略，基于对应的属性来决定允许还是拒绝访问。
策略文件的每一行都给出版本属性（apiVersion 和 kind）以及一个规约属性的映射，
用来匹配主体（用户或组）、资源属性、非资源属性（/version 或 /apis）和只读属性。
参阅<a href=/zh-cn/docs/reference/access-authn-authz/abac/#examples>示例</a>以了解细节。</li></ul></li></ul><p>作为在你的生产用 Kubernetes 集群中安装身份认证和鉴权机制的负责人，要考虑的事情如下：</p><ul><li><strong>设置鉴权模式</strong>：当 Kubernetes API 服务器（<a href=/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver</a>）启动时，
所支持的鉴权模式必须使用 <code>--authorization-mode</code> 标志配置。
例如，<code>kube-apiserver.yaml</code>（位于 <code>/etc/kubernetes/manifests</code> 下）中对应的标志可以设置为 <code>Node,RBAC</code>。
这样就会针对已完成身份认证的请求执行 Node 和 RBAC 鉴权。</li></ul><ul><li><strong>创建用户证书和角色绑定（RBAC）</strong>：如果你在使用 RBAC 鉴权，用户可以创建由集群 CA 签名的
CertificateSigningRequest（CSR）。接下来你就可以将 Role 和 ClusterRole 绑定到每个用户身上。
参阅<a href=/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/>证书签名请求</a>了解细节。</li></ul><ul><li><strong>创建组合属性的策略（ABAC）</strong>：如果你在使用 ABAC 鉴权，
你可以设置属性组合以构造策略对所选用户或用户组执行鉴权，
判定他们是否可访问特定的资源（例如 Pod）、名字空间或者 apiGroup。
进一步的详细信息可参阅<a href=/zh-cn/docs/reference/access-authn-authz/abac/#examples>示例</a>。</li></ul><ul><li><strong>考虑准入控制器</strong>：针对指向 API 服务器的请求的其他鉴权形式还包括
<a href=/zh-cn/docs/reference/access-authn-authz/authentication/#webhook-token-authentication>Webhook 令牌认证</a>。
Webhook 和其他特殊的鉴权类型需要通过向 API
服务器添加<a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>来启用。</li></ul><h2 id=set-limits-on-workload-resources>为负载资源设置约束</h2><p>生产环境负载的需求可能对 Kubernetes 的控制面内外造成压力。
在针对你的集群的负载执行配置时，要考虑以下条目：</p><ul><li><strong>设置名字空间限制</strong>：为每个名字空间的内存和 CPU 设置配额。
参阅<a href=/zh-cn/docs/tasks/administer-cluster/manage-resources/>管理内存、CPU 和 API 资源</a>以了解细节。
你也可以设置<a href=/blog/2020/08/14/introducing-hierarchical-namespaces/>层次化名字空间</a>来继承这类约束。</li></ul><ul><li><strong>为 DNS 请求做准备</strong>：如果你希望工作负载能够完成大规模扩展，你的 DNS 服务也必须能够扩大规模。
参阅<a href=/zh-cn/docs/tasks/administer-cluster/dns-horizontal-autoscaling/>自动扩缩集群中 DNS 服务</a>。</li></ul><ul><li><strong>创建额外的服务账户</strong>：用户账户决定用户可以在集群上执行的操作，服务账号则定义的是在特定名字空间中
Pod 的访问权限。默认情况下，Pod 使用所在名字空间中的 default 服务账号。
参阅<a href=/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/>管理服务账号</a>以了解如何创建新的服务账号。
例如，你可能需要：<ul><li>为 Pod 添加 Secret，以便 Pod 能够从某特定的容器镜像仓库拉取镜像。
参阅<a href=/zh-cn/docs/tasks/configure-pod-container/configure-service-account/>为 Pod 配置服务账号</a>以获得示例。</li><li>为服务账号设置 RBAC 访问许可。参阅<a href=/zh-cn/docs/reference/access-authn-authz/rbac/#service-account-permissions>服务账号访问许可</a>了解细节。</li></ul></li></ul><h2 id=接下来>接下来</h2><ul><li>决定你是想自行构造自己的生产用 Kubernetes，
还是从某可用的<a href=/zh-cn/docs/setup/production-environment/turnkey-solutions/>云服务外包厂商</a>或
<a href=/zh-cn/partners/>Kubernetes 合作伙伴</a>获得集群。</li><li>如果你决定自行构造集群，则需要规划如何处理<a href=/zh-cn/docs/setup/best-practices/certificates/>证书</a>并为类似
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>etcd</a> 和
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/>API 服务器</a>这些功能组件配置高可用能力。</li></ul><ul><li>选择使用 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/>kubeadm</a>、
<a href=/zh-cn/docs/setup/production-environment/tools/kops/>kops</a> 或
<a href=/zh-cn/docs/setup/production-environment/tools/kubespray/>Kubespray</a> 作为部署方法。</li></ul><ul><li>通过决定<a href=/zh-cn/docs/reference/access-authn-authz/authentication/>身份认证</a>和<a href=/zh-cn/docs/reference/access-authn-authz/authorization/>鉴权</a>方法来配置用户管理。</li></ul><ul><li>通过配置<a href=/zh-cn/docs/tasks/administer-cluster/manage-resources/>资源限制</a>、
<a href=/zh-cn/docs/tasks/administer-cluster/dns-horizontal-autoscaling/>DNS 自动扩缩</a>和<a href=/zh-cn/docs/reference/access-authn-authz/service-accounts-admin/>服务账号</a>来为应用负载作准备。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a77d3feb6e6d9978f32fa14622642e9a>2.1 - 容器运行时</h1><div class="alert alert-secondary callout note" role=alert><strong>说明：</strong> 自 1.24 版起，Dockershim 已从 Kubernetes 项目中移除。阅读 <a href=/zh-cn/dockershim>Dockershim 移除的常见问题</a>了解更多详情。</div><p>你需要在集群内每个节点上安装一个
<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>
以使 Pod 可以运行在上面。本文概述了所涉及的内容并描述了与节点设置相关的任务。</p><p>Kubernetes 1.25
要求你使用符合<a class=glossary-tooltip title='一组与 kubelet 集成的容器运行时 API' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/components/#container-runtime target=_blank aria-label=容器运行时接口>容器运行时接口</a>（CRI）的运行时。</p><p>有关详细信息，请参阅 <a href=#cri-versions>CRI 版本支持</a>。
本页简要介绍在 Kubernetes 中几个常见的容器运行时的用法。</p><ul><li><a href=#containerd>containerd</a></li><li><a href=#cri-o>CRI-O</a></li><li><a href=#docker>Docker Engine</a></li><li><a href=#mcr>Mirantis Container Runtime</a></li></ul><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>v1.24 之前的 Kubernetes 版本直接集成了 Docker Engine 的一个组件，名为 <strong>dockershim</strong>。
这种特殊的直接整合不再是 Kubernetes 的一部分
（这次删除被作为 v1.20 发行版本的一部分<a href=/zh-cn/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation>宣布</a>）。</p><p>你可以阅读<a href=/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/>检查 Dockershim 移除是否会影响你</a>以了解此删除可能会如何影响你。
要了解如何使用 dockershim 进行迁移，
请参阅<a href=/zh-cn/docs/tasks/administer-cluster/migrating-from-dockershim/>从 dockershim 迁移</a>。</p><p>如果你正在运行 v1.25 以外的 Kubernetes 版本，查看对应版本的文档。</p></div><h2 id=install-and-configure-prerequisites>安装和配置先决条件</h2><p>以下步骤将通用设置应用于 Linux 上的 Kubernetes 节点。</p><p>如果你确定不需要某个特定设置，则可以跳过它。</p><p>有关更多信息，请参阅<a href=/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>网络插件要求</a>或特定容器运行时的文档。</p><h3 id=转发-ipv4-并让-iptables-看到桥接流量>转发 IPv4 并让 iptables 看到桥接流量</h3><p>通过运行 <code>lsmod | grep br_netfilter</code> 来验证 <code>br_netfilter</code> 模块是否已加载。</p><p>若要显式加载此模块，请运行 <code>sudo modprobe br_netfilter</code>。</p><p>为了让 Linux 节点的 iptables 能够正确查看桥接流量，请确认 <code>sysctl</code> 配置中的
<code>net.bridge.bridge-nf-call-iptables</code> 设置为 1。例如：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>overlay
</span></span></span><span style=display:flex><span><span style=color:#b44>br_netfilter
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo modprobe overlay
</span></span><span style=display:flex><span>sudo modprobe br_netfilter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 设置所需的 sysctl 参数，参数在重新启动后保持不变</span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-iptables  = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.ipv4.ip_forward                 = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 应用 sysctl 参数而不重新启动</span>
</span></span><span style=display:flex><span>sudo sysctl --system
</span></span></code></pre></div><h2 id=cgroup-drivers>cgroup 驱动</h2><p>在 Linux 上，<a class=glossary-tooltip title='一组具有可选资源隔离、审计和限制的 Linux 进程。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label=控制组（CGroup）>控制组（CGroup）</a>用于限制分配给进程的资源。</p><p><a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> 和底层容器运行时都需要对接控制组来强制执行
<a href=/zh-cn/docs/concepts/configuration/manage-resources-containers/>为 Pod 和容器管理资源</a>
并为诸如 CPU、内存这类资源设置请求和限制。若要对接控制组，kubelet 和容器运行时需要使用一个 <strong>cgroup 驱动</strong>。
关键的一点是 kubelet 和容器运行时需使用相同的 cgroup 驱动并且采用相同的配置。</p><p>可用的 cgroup 驱动有两个：</p><ul><li><a href=#cgroupfs-cgroup-driver><code>cgroupfs</code></a></li><li><a href=#systemd-cgroup-driver><code>systemd</code></a></li></ul><h3 id=cgroupfs-cgroup-driver>cgroupfs 驱动</h3><p><code>cgroupfs</code> 驱动是 kubelet 中默认的 cgroup 驱动。当使用 <code>cgroupfs</code> 驱动时，
kubelet 和容器运行时将直接对接 cgroup 文件系统来配置 cgroup。</p><p>当 <a href=https://www.freedesktop.org/wiki/Software/systemd/>systemd</a> 是初始化系统时，
<strong>不</strong> 推荐使用 <code>cgroupfs</code> 驱动，因为 systemd 期望系统上只有一个 cgroup 管理器。
此外，如果你使用 <a href=/zh-cn/docs/concepts/architecture/cgroups>cgroup v2</a>，
则应用 <code>systemd</code> cgroup 驱动取代 <code>cgroupfs</code>。</p><h3 id=systemd-cgroup-driver>systemd cgroup 驱动</h3><p>当某个 Linux 系统发行版使用 <a href=https://www.freedesktop.org/wiki/Software/systemd/>systemd</a>
作为其初始化系统时，初始化进程会生成并使用一个 root 控制组（<code>cgroup</code>），并充当 cgroup 管理器。</p><p>systemd 与 cgroup 集成紧密，并将为每个 systemd 单元分配一个 cgroup。
因此，如果你 <code>systemd</code> 用作初始化系统，同时使用 <code>cgroupfs</code> 驱动，则系统中会存在两个不同的 cgroup 管理器。</p><p>同时存在两个 cgroup 管理器将造成系统中针对可用的资源和使用中的资源出现两个视图。某些情况下，
将 kubelet 和容器运行时配置为使用 <code>cgroupfs</code>、但为剩余的进程使用 <code>systemd</code>
的那些节点将在资源压力增大时变得不稳定。</p><p>当 systemd 是选定的初始化系统时，缓解这个不稳定问题的方法是针对 kubelet 和容器运行时将
<code>systemd</code> 用作 cgroup 驱动。</p><p>要将 <code>systemd</code> 设置为 cgroup 驱动，需编辑 <a href=/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/><code>KubeletConfiguration</code></a>
的 <code>cgroupDriver</code> 选项，并将其设置为 <code>systemd</code>。例如：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>cgroupDriver</span>:<span style=color:#bbb> </span>systemd<span style=color:#bbb>
</span></span></span></code></pre></div><p>如果你将 <code>systemd</code> 配置为 kubelet 的 cgroup 驱动，你也必须将 <code>systemd</code> 配置为容器运行时的 cgroup 驱动。
参阅容器运行时文档，了解指示说明。例如：</p><ul><li><a href=#containerd-systemd>containerd</a></li><li><a href=#cri-o>CRI-O</a></li></ul><div class="alert alert-warning caution callout" role=alert><strong>注意：</strong><p>注意：更改已加入集群的节点的 cgroup 驱动是一项敏感的操作。
如果 kubelet 已经使用某 cgroup 驱动的语义创建了 Pod，更改运行时以使用别的
cgroup 驱动，当为现有 Pod 重新创建 PodSandbox 时会产生错误。
重启 kubelet 也可能无法解决此类问题。</p><p>如果你有切实可行的自动化方案，使用其他已更新配置的节点来替换该节点，
或者使用自动化方案来重新安装。</p></div><h3 id=将-kubeadm-管理的集群迁移到-systemd-驱动>将 kubeadm 管理的集群迁移到 <code>systemd</code> 驱动</h3><p>如果你希望将现有的由 kubeadm 管理的集群迁移到 <code>systemd</code> cgroup 驱动，
请按照<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/>配置 cgroup 驱动</a>操作。</p><h2 id=cri-versions>CRI 版本支持</h2><p>你的容器运行时必须至少支持 v1alpha2 版本的容器运行时接口。</p><p>Kubernetes 1.25 默认使用 v1 版本的 CRI API。如果容器运行时不支持 v1 版本的 API，
则 kubelet 会回退到使用（已弃用的）v1alpha2 版本的 API。</p><h2 id=容器运行时>容器运行时</h2><div class="alert alert-secondary callout third-party-content" role=alert><strong>说明：</strong>
本部分链接到提供 Kubernetes 所需功能的第三方项目。Kubernetes 项目作者不负责这些项目。此页面遵循<a href=https://github.com/cncf/foundation/blob/master/website-guidelines.md target=_blank>CNCF 网站指南</a>，按字母顺序列出项目。要将项目添加到此列表中，请在提交更改之前阅读<a href=/docs/contribute/style/content-guide/#third-party-content>内容指南</a>。</div><h3 id=containerd>containerd</h3><p>本节概述了使用 containerd 作为 CRI 运行时的必要步骤。</p><p>使用以下命令在系统上安装 Containerd：</p><p>按照<a href=https://github.com/containerd/containerd/blob/main/docs/getting-started.md>开始使用 containerd</a> 的说明进行操作。
创建有效的配置文件 <code>config.toml</code> 后返回此步骤。</p><ul class="nav nav-tabs" id=找到-config-toml-文件 role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#%e6%89%be%e5%88%b0-config-toml-%e6%96%87%e4%bb%b6-0 role=tab aria-controls=找到-config-toml-文件-0 aria-selected=true>Linux</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#%e6%89%be%e5%88%b0-config-toml-%e6%96%87%e4%bb%b6-1 role=tab aria-controls=找到-config-toml-文件-1>Windows</a></li></ul><div class=tab-content id=找到-config-toml-文件><div id=找到-config-toml-文件-0 class="tab-pane show active" role=tabpanel aria-labelledby=找到-config-toml-文件-0><p><p>你可以在路径 <code>/etc/containerd/config.toml</code> 下找到此文件。</p></div><div id=找到-config-toml-文件-1 class=tab-pane role=tabpanel aria-labelledby=找到-config-toml-文件-1><p><p>你可以在路径 <code>C:\Program Files\containerd\config.toml</code> 下找到此文件。</p></div></div><p>在 Linux 上，containerd 的默认 CRI 套接字是 <code>/run/containerd/containerd.sock</code>。
在 Windows 上，默认 CRI 端点是 <code>npipe://./pipe/containerd-containerd</code>。</p><h4 id=containerd-systemd>配置 <code>systemd</code> cgroup 驱动</h4><p>结合 <code>runc</code> 使用 <code>systemd</code> cgroup 驱动，在 <code>/etc/containerd/config.toml</code> 中设置：</p><pre tabindex=0><code>[plugins.&#34;io.containerd.grpc.v1.cri&#34;.containerd.runtimes.runc]
  ...
  [plugins.&#34;io.containerd.grpc.v1.cri&#34;.containerd.runtimes.runc.options]
    SystemdCgroup = true
</code></pre><p>如果你使用 <a href=/zh-cn/docs/concepts/architecture/cgroups>cgroup v2</a>，则推荐 <code>systemd</code> cgroup 驱动。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>如果你从软件包（例如，RPM 或者 <code>.deb</code>）中安装 containerd，你可能会发现其中默认禁止了
CRI 集成插件。</p><p>你需要启用 CRI 支持才能在 Kubernetes 集群中使用 containerd。
要确保 <code>cri</code> 没有出现在 <code>/etc/containerd/config.toml</code> 文件中 <code>disabled_plugins</code>
列表内。如果你更改了这个文件，也请记得要重启 <code>containerd</code>。</p></div><p>如果你应用此更改，请确保重新启动 containerd：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo systemctl restart containerd
</span></span></code></pre></div><p>当使用 kubeadm 时，请手动配置
<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/#configuring-the-kubelet-cgroup-driver>kubelet 的 cgroup 驱动</a>。</p><h4 id=override-pause-image-containerd>重载沙箱（pause）镜像</h4><p>在你的 <a href=https://github.com/containerd/containerd/blob/main/docs/cri/config.md>containerd 配置</a>中，
你可以通过设置以下选项重载沙箱镜像：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[plugins.<span style=color:#b44>&#34;io.containerd.grpc.v1.cri&#34;</span>]
</span></span><span style=display:flex><span>  sandbox_image = <span style=color:#b44>&#34;registry.k8s.io/pause:3.2&#34;</span>
</span></span></code></pre></div><p>一旦你更新了这个配置文件，可能就同样需要重启 <code>containerd</code>：<code>systemctl restart containerd</code>。</p><h3 id=cri-o>CRI-O</h3><p>本节包含安装 CRI-O 作为容器运行时的必要步骤。</p><p>要安装 CRI-O，请按照 <a href=https://github.com/cri-o/cri-o/blob/main/install.md#readme>CRI-O 安装说明</a>执行操作。</p><h4 id=cgroup-driver>cgroup 驱动</h4><p>CRI-O 默认使用 systemd cgroup 驱动，这对你来说可能工作得很好。
要切换到 <code>cgroupfs</code> cgroup 驱动，请编辑 <code>/etc/crio/crio.conf</code> 或在
<code>/etc/crio/crio.conf.d/02-cgroup-manager.conf</code> 中放置一个插入式配置，例如：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[crio.runtime]
</span></span><span style=display:flex><span>conmon_cgroup = <span style=color:#b44>&#34;pod&#34;</span>
</span></span><span style=display:flex><span>cgroup_manager = <span style=color:#b44>&#34;cgroupfs&#34;</span>
</span></span></code></pre></div><p>你还应该注意当使用 CRI-O 时，并且 CRI-O 的 cgroup 设置为 <code>cgroupfs</code> 时，必须将 <code>conmon_cgroup</code> 设置为值 <code>pod</code>。
通常需要保持 kubelet 的 cgroup 驱动配置（通常通过 kubeadm 完成）和 CRI-O 同步。</p><p>对于 CRI-O，CRI 套接字默认为 <code>/var/run/crio/crio.sock</code>。</p><h4 id=override-pause-image-cri-o>重载沙箱（pause）镜像</h4><p>在你的 <a href=https://github.com/cri-o/cri-o/blob/main/docs/crio.conf.5.md>CRI-O 配置</a>中，
你可以设置以下配置值：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[crio.image]
</span></span><span style=display:flex><span>pause_image=<span style=color:#b44>&#34;registry.k8s.io/pause:3.6&#34;</span>
</span></span></code></pre></div><p>这一设置选项支持动态配置重加载来应用所做变更：<code>systemctl reload crio</code>。
也可以通过向 <code>crio</code> 进程发送 <code>SIGHUP</code> 信号来实现。</p><h3 id=docker>Docker Engine</h3><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>以下操作假设你使用 <a href=https://github.com/Mirantis/cri-dockerd><code>cri-dockerd</code></a> 适配器来将
Docker Engine 与 Kubernetes 集成。</div><ol><li>在你的每个节点上，遵循<a href=https://docs.docker.com/engine/install/#server>安装 Docker Engine</a>
指南为你的 Linux 发行版安装 Docker。</li></ol><ol start=2><li>按照源代码仓库中的说明安装 <a href=https://github.com/Mirantis/cri-dockerd><code>cri-dockerd</code></a>。</li></ol><p>对于 <code>cri-dockerd</code>，默认情况下，CRI 套接字是 <code>/run/cri-dockerd.sock</code>。</p><h3 id=mcr>Mirantis 容器运行时</h3><p><a href=https://docs.mirantis.com/mcr/20.10/overview.html>Mirantis Container Runtime</a> (MCR)
是一种商用容器运行时，以前称为 Docker 企业版。
你可以使用 MCR 中包含的开源 <a href=https://github.com/Mirantis/cri-dockerd><code>cri-dockerd</code></a>
组件将 Mirantis Container Runtime 与 Kubernetes 一起使用。</p><p>要了解有关如何安装 Mirantis Container Runtime 的更多信息，
请访问 <a href=https://docs.mirantis.com/mcr/20.10/install.html>MCR 部署指南</a>。</p><p>检查名为 <code>cri-docker.socket</code> 的 systemd 单元以找出 CRI 套接字的路径。</p><h4 id=override-pause-image-cri-dockerd-mcr>重载沙箱（pause）镜像</h4><p><code>cri-dockerd</code> 适配器能够接受指定用作 Pod 的基础容器的容器镜像（“pause 镜像”）作为命令行参数。
要使用的命令行参数是 <code>--pod-infra-container-image</code>。</p><h2 id=接下来>接下来</h2><p>除了容器运行时，你的集群还需要有效的<a href=/zh-cn/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>网络插件</a>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-00e1646f68aeb89f9722cf6f6cfcad94>2.2 - 使用部署工具安装 Kubernetes</h1></div><div class=td-content><h1 id=pg-a16f59f325a17cdeed324d5c889f7f73>2.2.1 - 使用 kubeadm 引导集群</h1></div><div class=td-content><h1 id=pg-29e59491dd6118b23072dfe9ebb93323>2.2.1.1 - 安装 kubeadm</h1><p><img src=/images/kubeadm-stacked-color.png align=right width=150px></img>
本页面显示如何安装 <code>kubeadm</code> 工具箱。
有关在执行此安装过程后如何使用 kubeadm 创建集群的信息，
请参见<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>使用 kubeadm 创建集群</a>。</p><h2 id=准备开始>准备开始</h2><ul><li>一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux
发行版以及一些不提供包管理器的发行版提供通用的指令。</li><li>每台机器 2 GB 或更多的 RAM（如果少于这个数字将会影响你应用的运行内存）。</li><li>CPU 2 核心及以上。</li><li>集群中的所有机器的网络彼此均能相互连接（公网和内网都可以）。</li><li>节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见<a href=#verify-mac-address>这里</a>了解更多详细信息。</li><li>开启机器上的某些端口。请参见<a href=#check-required-ports>这里</a>了解更多详细信息。</li><li>禁用交换分区。为了保证 kubelet 正常工作，你<strong>必须</strong>禁用交换分区。</li></ul><h2 id=verify-mac-address>确保每个节点上 MAC 地址和 product_uuid 的唯一性</h2><ul><li>你可以使用命令 <code>ip link</code> 或 <code>ifconfig -a</code> 来获取网络接口的 MAC 地址</li><li>可以使用 <code>sudo cat /sys/class/dmi/id/product_uuid</code> 命令对 product_uuid 校验</li></ul><p>一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。
Kubernetes 使用这些值来唯一确定集群中的节点。
如果这些值在每个节点上不唯一，可能会导致安装<a href=https://github.com/kubernetes/kubeadm/issues/31>失败</a>。</p><h2 id=check-network-adapters>检查网络适配器</h2><p>如果你有一个以上的网络适配器，同时你的 Kubernetes 组件通过默认路由不可达，我们建议你预先添加 IP 路由规则，
这样 Kubernetes 集群就可以通过对应的适配器完成连接。</p><h2 id=check-required-ports>检查所需端口</h2><p>启用这些<a href=/zh-cn/docs/reference/ports-and-protocols/>必要的端口</a>后才能使 Kubernetes 的各组件相互通信。
可以使用 netcat 之类的工具来检查端口是否启用，例如：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>nc 127.0.0.1 <span style=color:#666>6443</span>
</span></span></code></pre></div><p>你使用的 Pod 网络插件 (详见后续章节) 也可能需要开启某些特定端口。
由于各个 Pod 网络插件的功能都有所不同，请参阅他们各自文档中对端口的要求。</p><h2 id=installing-runtime>安装容器运行时</h2><p>为了在 Pod 中运行容器，Kubernetes 使用
<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label='容器运行时（Container Runtime）'>容器运行时（Container Runtime）</a>。</p><p>默认情况下，Kubernetes 使用
<a class=glossary-tooltip title='一组与 kubelet 集成的容器运行时 API' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/components/#container-runtime target=_blank aria-label='容器运行时接口（Container Runtime Interface，CRI）'>容器运行时接口（Container Runtime Interface，CRI）</a>
来与你所选择的容器运行时交互。</p><p>如果你不指定运行时，kubeadm 会自动尝试通过扫描已知的端点列表来检测已安装的容器运行时。</p><p>如果检测到有多个或者没有容器运行时，kubeadm 将抛出一个错误并要求你指定一个想要使用的运行时。</p><p>参阅<a href=/zh-cn/docs/setup/production-environment/container-runtimes/>容器运行时</a>
以了解更多信息。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>Docker Engine 没有实现 <a href=/zh-cn/docs/concepts/architecture/cri/>CRI</a>，
而这是容器运行时在 Kubernetes 中工作所需要的。
为此，必须安装一个额外的服务 <a href=https://github.com/Mirantis/cri-dockerd>cri-dockerd</a>。
cri-dockerd 是一个基于传统的内置 Docker 引擎支持的项目，
它在 1.24 版本从 kubelet 中<a href=/zh-cn/dockershim>移除</a>。</div><p>下面的表格包括被支持的操作系统的已知端点。</p><ul class="nav nav-tabs" id=container-runtime role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#container-runtime-0 role=tab aria-controls=container-runtime-0 aria-selected=true>Linux</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#container-runtime-1 role=tab aria-controls=container-runtime-1>Windows</a></li></ul><div class=tab-content id=container-runtime><div id=container-runtime-0 class="tab-pane show active" role=tabpanel aria-labelledby=container-runtime-0><p><table><caption style=display:none>Linux 容器运行时</caption><thead><tr><th>运行时</th><th>Unix 域套接字</th></tr></thead><tbody><tr><td>containerd</td><td><code>unix:///var/run/containerd/containerd.sock</code></td></tr><tr><td>CRI-O</td><td><code>unix:///var/run/crio/crio.sock</code></td></tr><tr><td>Docker Engine（使用 cri-dockerd）</td><td><code>unix:///var/run/cri-dockerd.sock</code></td></tr></tbody></table></div><div id=container-runtime-1 class=tab-pane role=tabpanel aria-labelledby=container-runtime-1><p><table><caption style=display:none>Windows 容器运行时</caption><thead><tr><th>运行时</th><th>Windows 命名管道路径</th></tr></thead><tbody><tr><td>containerd</td><td><code>npipe:////./pipe/containerd-containerd</code></td></tr><tr><td>Docker Engine（使用 cri-dockerd）</td><td><code>npipe:////./pipe/cri-dockerd</code></td></tr></tbody></table></div></div><h2 id=installing-kubeadm-kubelet-and-kubectl>安装 kubeadm、kubelet 和 kubectl</h2><p>你需要在每台机器上安装以下的软件包：</p><ul><li><p><code>kubeadm</code>：用来初始化集群的指令。</p></li><li><p><code>kubelet</code>：在集群中的每个节点上用来启动 Pod 和容器等。</p></li><li><p><code>kubectl</code>：用来与集群通信的命令行工具。</p></li></ul><p>kubeadm <strong>不能</strong>帮你安装或者管理 <code>kubelet</code> 或 <code>kubectl</code>，
所以你需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配。
如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。
然而，控制平面与 kubelet 之间可以存在<strong>一个</strong>次要版本的偏差，但 kubelet
的版本不可以超过 API 服务器的版本。
例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。</p><p>有关安装 <code>kubectl</code> 的信息，请参阅<a href=/zh-cn/docs/tasks/tools/>安装和设置 kubectl</a> 文档。</p><div class="alert alert-danger warning callout" role=alert><strong>警告：</strong><p>这些指南不包括系统升级时使用的所有 Kubernetes 程序包。这是因为 kubeadm 和 Kubernetes
有<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>特殊的升级注意事项</a>。</div><p>关于版本偏差的更多信息，请参阅以下文档：</p><ul><li>Kubernetes <a href=/zh-cn/releases/version-skew-policy/>版本与版本间的偏差策略</a></li><li>kubeadm 特定的<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy>版本偏差策略</a></li></ul><ul class="nav nav-tabs" id=k8s-install role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-0 role=tab aria-controls=k8s-install-0 aria-selected=true>基于 Debian 的发行版</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-1 role=tab aria-controls=k8s-install-1>基于 Red Hat 的发行版</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-2 role=tab aria-controls=k8s-install-2>无包管理器的情况</a></li></ul><div class=tab-content id=k8s-install><div id=k8s-install-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-0><p><ol><li><p>更新 <code>apt</code> 包索引并安装使用 Kubernetes <code>apt</code> 仓库所需要的包：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install -y apt-transport-https ca-certificates curl
</span></span></code></pre></div></li></ol><ol start=2><li><p>下载 Google Cloud 公开签名秘钥：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</span></span></code></pre></div></li></ol><ol start=3><li><p>添加 Kubernetes <code>apt</code> 仓库：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span></code></pre></div></li></ol><ol start=4><li><p>更新 <code>apt</code> 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install -y kubelet kubeadm kubectl
</span></span><span style=display:flex><span>sudo apt-mark hold kubelet kubeadm kubectl
</span></span></code></pre></div></li></ol><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>在低于 Debian 12 和 Ubuntu 22.04 的发行版本中，<code>/etc/apt/keyrings</code> 默认不存在。
如有需要，你可以创建此目录，并将其设置为对所有人可读，但仅对管理员可写。</div></div><div id=k8s-install-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span></span></span><span style=display:flex><span><span style=color:#b44>[kubernetes]
</span></span></span><span style=display:flex><span><span style=color:#b44>name=Kubernetes
</span></span></span><span style=display:flex><span><span style=color:#b44>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
</span></span></span><span style=display:flex><span><span style=color:#b44>enabled=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#b44>exclude=kubelet kubeadm kubectl
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 将 SELinux 设置为 permissive 模式（相当于将其禁用）</span>
</span></span><span style=display:flex><span>sudo setenforce <span style=color:#666>0</span>
</span></span><span style=display:flex><span>sudo sed -i <span style=color:#b44>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo yum install -y kubelet kubeadm kubectl --disableexcludes<span style=color:#666>=</span>kubernetes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div><p><strong>请注意：</strong></p><ul><li><p>通过运行命令 <code>setenforce 0</code> 和 <code>sed ...</code> 将 SELinux 设置为 permissive 模式可以有效地将其禁用。
这是允许容器访问主机文件系统所必需的，而这些操作是为了例如 Pod 网络工作正常。</p><p>你必须这么做，直到 kubelet 做出对 SELinux 的支持进行升级为止。</p></li><li><p>如果你知道如何配置 SELinux 则可以将其保持启用状态，但可能需要设定 kubeadm 不支持的部分配置</p></li><li><p>如果由于该 Red Hat 的发行版无法解析 <code>basearch</code> 导致获取 <code>baseurl</code> 失败，请将 <code>\$basearch</code> 替换为你计算机的架构。
输入 <code>uname -m</code> 以查看该值。
例如，<code>x86_64</code> 的 <code>baseurl</code> URL 可以是：<code>https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</code>。</p></li></ul></div><div id=k8s-install-2 class=tab-pane role=tabpanel aria-labelledby=k8s-install-2><p><p>安装 CNI 插件（大多数 Pod 网络都需要）：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CNI_PLUGINS_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.1.1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>DEST</span><span style=color:#666>=</span><span style=color:#b44>&#34;/opt/cni/bin&#34;</span>
</span></span><span style=display:flex><span>sudo mkdir -p <span style=color:#b44>&#34;</span><span style=color:#b8860b>$DEST</span><span style=color:#b44>&#34;</span>
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_PLUGINS_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cni-plugins-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_PLUGINS_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tgz&#34;</span> | sudo tar -C <span style=color:#b44>&#34;</span><span style=color:#b8860b>$DEST</span><span style=color:#b44>&#34;</span> -xz
</span></span></code></pre></div><p>定义要下载命令文件的目录。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>DOWNLOAD_DIR</code> 变量必须被设置为一个可写入的目录。
如果你在运行 Flatcar Container Linux，可设置 <code>DOWNLOAD_DIR="/opt/bin"</code>。</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#666>=</span><span style=color:#b44>&#34;/usr/local/bin&#34;</span>
</span></span><span style=display:flex><span>sudo mkdir -p <span style=color:#b44>&#34;</span><span style=color:#b8860b>$DOWNLOAD_DIR</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div><p>安装 crictl（kubeadm/kubelet 容器运行时接口（CRI）所需）</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.25.0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/crictl-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tar.gz&#34;</span> | sudo tar -C <span style=color:#b8860b>$DOWNLOAD_DIR</span> -xz
</span></span></code></pre></div><p>安装 <code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code> 并添加 <code>kubelet</code> 系统服务：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>RELEASE</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>curl -sSL https://dl.k8s.io/release/stable.txt<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f>cd</span> <span style=color:#b8860b>$DOWNLOAD_DIR</span>
</span></span><span style=display:flex><span>sudo curl -L --remote-name-all https://dl.k8s.io/release/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span>/bin/linux/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span>/<span style=color:#666>{</span>kubeadm,kubelet<span style=color:#666>}</span>
</span></span><span style=display:flex><span>sudo chmod +x <span style=color:#666>{</span>kubeadm,kubelet<span style=color:#666>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.4.0&#34;</span>
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service
</span></span><span style=display:flex><span>sudo mkdir -p /etc/systemd/system/kubelet.service.d
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</span></span></code></pre></div><p>请参照<a href=/zh-cn/docs/tasks/tools/#kubectl>安装工具页面</a>的说明安装 <code>kubelet</code>。
激活并启动 <code>kubelet</code>：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>Flatcar Container Linux 发行版会将 <code>/usr/</code> 目录挂载为一个只读文件系统。
在启动引导你的集群之前，你需要执行一些额外的操作来配置一个可写入的目录。
参见 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#usr-mounted-read-only/>kubeadm 故障排查指南</a>
以了解如何配置一个可写入的目录。</div></div></div><p>kubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。</p><h2 id=configuring-a-cgroup-driver>配置 cgroup 驱动程序</h2><p>容器运行时和 kubelet 都具有名字为
<a href=/zh-cn/docs/setup/production-environment/container-runtimes/>"cgroup driver"</a>
的属性，该属性对于在 Linux 机器上管理 CGroups 而言非常重要。</p><div class="alert alert-danger warning callout" role=alert><strong>警告：</strong><p>你需要确保容器运行时和 kubelet 所使用的是相同的 cgroup 驱动，否则 kubelet
进程会失败。</p><p>相关细节可参见<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/>配置 cgroup 驱动</a>。</p></div><h2 id=troubleshooting>故障排查</h2><p>如果你在使用 kubeadm 时遇到困难，请参阅我们的
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>故障排查文档</a>。</p><h2 id=接下来>接下来</h2><ul><li><a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>使用 kubeadm 创建集群</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c3689df4b0c61a998e79d91a865aa244>2.2.1.2 - 对 kubeadm 进行故障排查</h1><p>与任何程序一样，你可能会在安装或者运行 kubeadm 时遇到错误。
本文列举了一些常见的故障场景，并提供可帮助你理解和解决这些问题的步骤。</p><p>如果你的问题未在下面列出，请执行以下步骤：</p><ul><li><p>如果你认为问题是 kubeadm 的错误：</p><ul><li>转到 <a href=https://github.com/kubernetes/kubeadm/issues>github.com/kubernetes/kubeadm</a> 并搜索存在的问题。</li><li>如果没有问题，请 <a href=https://github.com/kubernetes/kubeadm/issues/new>打开</a> 并遵循问题模板。</li></ul></li><li><p>如果你对 kubeadm 的工作方式有疑问，可以在 <a href=https://slack.k8s.io/>Slack</a> 上的 <code>#kubeadm</code> 频道提问，
或者在 <a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a> 上提问。
请加入相关标签，例如 <code>#kubernetes</code> 和 <code>#kubeadm</code>，这样其他人可以帮助你。</p></li></ul><h2 id=由于缺少-rbac-无法将-v1-18-node-加入-v1-17-集群>由于缺少 RBAC，无法将 v1.18 Node 加入 v1.17 集群</h2><p>自从 v1.18 后，如果集群中已存在同名 Node，kubeadm 将禁止 Node 加入集群。
这需要为 bootstrap-token 用户添加 RBAC 才能 GET Node 对象。</p><p>但这会导致一个问题，v1.18 的 <code>kubeadm join</code> 无法加入由 kubeadm v1.17 创建的集群。</p><p>要解决此问题，你有两种选择：</p><p>使用 kubeadm v1.18 在控制平面节点上执行 <code>kubeadm init phase bootstrap-token</code>。
请注意，这也会启用 bootstrap-token 的其余权限。</p><p>或者，也可以使用 <code>kubectl apply -f ...</code> 手动应用以下 RBAC：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRoleBinding<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>roleRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Group<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:bootstrappers:kubeadm:default-node-token<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=在安装过程中没有找到-ebtables-或者其他类似的可执行文件>在安装过程中没有找到 <code>ebtables</code> 或者其他类似的可执行文件</h2><p>如果在运行 <code>kubeadm init</code> 命令时，遇到以下的警告</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ebtables not found in system path
</span></span><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ethtool not found in system path
</span></span></code></pre></div><p>那么或许在你的节点上缺失 <code>ebtables</code>、<code>ethtool</code> 或者类似的可执行文件。
你可以使用以下命令安装它们：</p><ul><li>对于 Ubuntu/Debian 用户，运行 <code>apt install ebtables ethtool</code> 命令。</li><li>对于 CentOS/Fedora 用户，运行 <code>yum install ebtables ethtool</code> 命令。</li></ul><h2 id=在安装过程中-kubeadm-一直等待控制平面就绪>在安装过程中，kubeadm 一直等待控制平面就绪</h2><p>如果你注意到 <code>kubeadm init</code> 在打印以下行后挂起：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#666>[</span>apiclient<span style=color:#666>]</span> Created API client, waiting <span style=color:#a2f;font-weight:700>for</span> the control plane to become ready
</span></span></code></pre></div><p>这可能是由许多问题引起的。最常见的是：</p><ul><li>网络连接问题。在继续之前，请检查你的计算机是否具有全部联通的网络连接。</li><li>容器运行时的 cgroup 驱动不同于 kubelet 使用的 cgroup 驱动。要了解如何正确配置 cgroup 驱动，
请参阅<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/>配置 cgroup 驱动</a>。</li><li>控制平面上的 Docker 容器持续进入崩溃状态或（因其他原因）挂起。你可以运行 <code>docker ps</code> 命令来检查以及 <code>docker logs</code> 命令来检视每个容器的运行日志。
对于其他容器运行时，请参阅<a href=/zh-cn/docs/tasks/debug/debug-cluster/crictl/>使用 crictl 对 Kubernetes 节点进行调试</a>。</li></ul><h2 id=当删除托管容器时-kubeadm-阻塞>当删除托管容器时 kubeadm 阻塞</h2><p>如果容器运行时停止并且未删除 Kubernetes 所管理的容器，可能发生以下情况：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo kubeadm reset
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>[preflight] Running pre-flight checks
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in &#34;/var/lib/kubelet&#34;
[reset] Removing kubernetes-managed containers
(block)
</code></pre><p>一个可行的解决方案是重新启动 Docker 服务，然后重新运行 <code>kubeadm reset</code>：
你也可以使用 <code>crictl</code> 来调试容器运行时的状态。
参见<a href=/zh-cn/docs/tasks/debug/debug-cluster/crictl/>使用 CRICTL 调试 Kubernetes 节点</a>。</p><h2 id=pod-处于-runcontainererror-crashloopbackoff-或者-error-状态>Pod 处于 <code>RunContainerError</code>、<code>CrashLoopBackOff</code> 或者 <code>Error</code> 状态</h2><p>在 <code>kubeadm init</code> 命令运行后，系统中不应该有 Pod 处于这类状态。</p><ul><li><p>在 <code>kubeadm init</code> 命令执行完后，如果有 Pod 处于这些状态之一，请在 kubeadm
仓库提起一个 issue。<code>coredns</code> (或者 <code>kube-dns</code>) 应该处于 <code>Pending</code> 状态，
直到你部署了网络插件为止。</p></li><li><p>如果在部署完网络插件之后，有 Pod 处于 <code>RunContainerError</code>、<code>CrashLoopBackOff</code>
或 <code>Error</code> 状态之一，并且 <code>coredns</code> （或者 <code>kube-dns</code>）仍处于 <code>Pending</code> 状态，
那很可能是你安装的网络插件由于某种原因无法工作。你或许需要授予它更多的
RBAC 特权或使用较新的版本。请在 Pod Network 提供商的问题跟踪器中提交问题，
然后在此处分类问题。</p></li></ul><h2 id=coredns-停滞在-pending-状态><code>coredns</code> 停滞在 <code>Pending</code> 状态</h2><p>这一行为是 <strong>预期之中</strong> 的，因为系统就是这么设计的。
kubeadm 的网络供应商是中立的，因此管理员应该选择
<a href=/zh-cn/docs/concepts/cluster-administration/addons/>安装 Pod 的网络插件</a>。
你必须完成 Pod 的网络配置，然后才能完全部署 CoreDNS。
在网络被配置好之前，DNS 组件会一直处于 <code>Pending</code> 状态。</p><h2 id=hostport-服务无法工作><code>HostPort</code> 服务无法工作</h2><p>此 <code>HostPort</code> 和 <code>HostIP</code> 功能是否可用取决于你的 Pod 网络配置。请联系 Pod 网络插件的作者，
以确认 <code>HostPort</code> 和 <code>HostIP</code> 功能是否可用。</p><p>已验证 Calico、Canal 和 Flannel CNI 驱动程序支持 HostPort。</p><p>有关更多信息，请参考 <a href=https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md>CNI portmap 文档</a>.</p><p>如果你的网络提供商不支持 portmap CNI 插件，你或许需要使用
<a href=/zh-cn/docs/concepts/services-networking/service/#type-nodeport>NodePort 服务的功能</a>
或者使用 <code>HostNetwork=true</code>。</p><h2 id=无法通过其服务-ip-访问-pod>无法通过其服务 IP 访问 Pod</h2><ul><li><p>许多网络附加组件尚未启用 <a href=/zh-cn/docs/tasks/debug/debug-application/debug-service/#a-pod-fails-to-reach-itself-via-the-service-ip>hairpin 模式</a>
该模式允许 Pod 通过其服务 IP 进行访问。这是与 <a href=https://github.com/containernetworking/cni/issues/476>CNI</a> 有关的问题。
请与网络附加组件提供商联系，以获取他们所提供的 hairpin 模式的最新状态。</p></li><li><p>如果你正在使用 VirtualBox (直接使用或者通过 Vagrant 使用)，你需要
确保 <code>hostname -i</code> 返回一个可路由的 IP 地址。默认情况下，第一个接口连接不能路由的仅主机网络。
解决方法是修改 <code>/etc/hosts</code>，请参考示例 <a href=https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11>Vagrantfile</a>。</p></li></ul><h2 id=tls-证书错误>TLS 证书错误</h2><p>以下错误说明证书可能不匹配。</p><pre tabindex=0><code class=language-none data-lang=none># kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &#34;crypto/rsa: verification error&#34; while trying to verify candidate authority certificate &#34;kubernetes&#34;)
</code></pre><ul><li><p>验证 <code>$HOME/.kube/config</code> 文件是否包含有效证书，
并在必要时重新生成证书。在 kubeconfig 文件中的证书是 base64 编码的。
该 <code>base64 --decode</code> 命令可以用来解码证书，<code>openssl x509 -text -noout</code>
命令可以用于查看证书信息。</p></li><li><p>使用如下方法取消设置 <code>KUBECONFIG</code> 环境变量的值：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>unset</span> KUBECONFIG
</span></span></code></pre></div><p>或者将其设置为默认的 <code>KUBECONFIG</code> 位置：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div></li><li><p>另一个方法是覆盖 <code>kubeconfig</code> 的现有用户 "管理员"：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>mv  <span style=color:#b8860b>$HOME</span>/.kube <span style=color:#b8860b>$HOME</span>/.kube.bak
</span></span><span style=display:flex><span>mkdir <span style=color:#b8860b>$HOME</span>/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div></li></ul><h2 id=kubelet-client-cert>Kubelet 客户端证书轮换失败</h2><p>默认情况下，kubeadm 使用 <code>/etc/kubernetes/kubelet.conf</code> 中指定的 <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> 符号链接
来配置 kubelet 自动轮换客户端证书。如果此轮换过程失败，你可能会在 kube-apiserver 日志中看到诸如
<code>x509: certificate has expired or is not yet valid</code> 之类的错误。要解决此问题，你必须执行以下步骤：</p><ol><li>从故障节点备份和删除 <code>/etc/kubernetes/kubelet.conf</code> 和 <code>/var/lib/kubelet/pki/kubelet-client*</code>。</li><li>在集群中具有 <code>/etc/kubernetes/pki/ca.key</code> 的、正常工作的控制平面节点上
执行 <code>kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf</code>。
<code>$NODE</code> 必须设置为集群中现有故障节点的名称。
手动修改生成的 <code>kubelet.conf</code> 以调整集群名称和服务器端点，
或传递 <code>kubeconfig user --config</code>（此命令接受 <code>InitConfiguration</code>）。
如果你的集群没有 <code>ca.key</code>，你必须在外部对 <code>kubelet.conf</code> 中的嵌入式证书进行签名。</li></ol><ol start=3><li>将得到的 <code>kubelet.conf</code> 文件复制到故障节点上，作为 <code>/etc/kubernetes/kubelet.conf</code>。</li><li>在故障节点上重启 kubelet（<code>systemctl restart kubelet</code>），等待 <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> 重新创建。</li></ol><ol start=5><li><p>手动编辑 <code>kubelet.conf</code> 指向轮换的 kubelet 客户端证书，方法是将 <code>client-certificate-data</code> 和 <code>client-key-data</code> 替换为：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>client-certificate</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>client-key</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style=color:#bbb>
</span></span></span></code></pre></div></li></ol><ol start=6><li>重新启动 kubelet。</li><li>确保节点状况变为 <code>Ready</code>。</li></ol><h2 id=在-vagrant-中使用-flannel-作为-pod-网络时的默认-nic>在 Vagrant 中使用 flannel 作为 Pod 网络时的默认 NIC</h2><p>以下错误可能表明 Pod 网络中出现问题：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Error from server <span style=color:#666>(</span>NotFound<span style=color:#666>)</span>: the server could not find the requested resource
</span></span></code></pre></div><ul><li><p>如果你正在 Vagrant 中使用 flannel 作为 Pod 网络，则必须指定 flannel 的默认接口名称。</p><p>Vagrant 通常为所有 VM 分配两个接口。第一个为所有主机分配了 IP 地址 <code>10.0.2.15</code>，用于获得 NATed 的外部流量。</p><p>这可能会导致 flannel 出现问题，它默认为主机上的第一个接口。这导致所有主机认为它们具有
相同的公共 IP 地址。为防止这种情况，传递 <code>--iface eth1</code> 标志给 flannel 以便选择第二个接口。</p></li></ul><h2 id=容器使用的非公共-ip>容器使用的非公共 IP</h2><p>在某些情况下 <code>kubectl logs</code> 和 <code>kubectl run</code> 命令或许会返回以下错误，即便除此之外集群一切功能正常：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
</span></span></code></pre></div><ul><li><p>这或许是由于 Kubernetes 使用的 IP 无法与看似相同的子网上的其他 IP 进行通信的缘故，
可能是由机器提供商的政策所导致的。</p></li><li><p>DigitalOcean 既分配一个共有 IP 给 <code>eth0</code>，也分配一个私有 IP 在内部用作其浮动 IP 功能的锚点，
然而 <code>kubelet</code> 将选择后者作为节点的 <code>InternalIP</code> 而不是公共 IP。</p><p>使用 <code>ip addr show</code> 命令代替 <code>ifconfig</code> 命令去检查这种情况，因为 <code>ifconfig</code> 命令
不会显示有问题的别名 IP 地址。或者指定的 DigitalOcean 的 API 端口允许从 droplet 中
查询 anchor IP：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
</span></span></code></pre></div><p>解决方法是通知 <code>kubelet</code> 使用哪个 <code>--node-ip</code>。当使用 DigitalOcean 时，可以是公网IP（分配给 <code>eth0</code> 的），
或者是私网IP（分配给 <code>eth1</code> 的）。私网 IP 是可选的。
<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/#kubeadm-k8s-io-v1beta3-NodeRegistrationOptions>kubadm <code>NodeRegistrationOptions</code> 结构</a>
的 <code>KubeletExtraArgs</code> 部分被用来处理这种情况。</p><p>然后重启 <code>kubelet</code>：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li></ul><h2 id=coredns-pod-有-crashloopbackoff-或者-error-状态><code>coredns</code> Pod 有 <code>CrashLoopBackOff</code> 或者 <code>Error</code> 状态</h2><p>如果有些节点运行的是旧版本的 Docker，同时启用了 SELinux，你或许会遇到 <code>coredns</code> Pod 无法启动的情况。
要解决此问题，你可以尝试以下选项之一：</p><ul><li><p>升级到 <a href=/zh-cn/docs/setup/production-environment/container-runtimes/#docker>Docker 的较新版本</a>。</p></li><li><p><a href=https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux>禁用 SELinux</a>.</p></li><li><p>修改 <code>coredns</code> 部署以设置 <code>allowPrivilegeEscalation</code> 为 <code>true</code>：</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl -n kube-system get deployment coredns -o yaml | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  sed <span style=color:#b44>&#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g&#39;</span> | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  kubectl apply -f -
</span></span></code></pre></div><p>CoreDNS 处于 <code>CrashLoopBackOff</code> 时的另一个原因是当 Kubernetes 中部署的 CoreDNS Pod 检测
到环路时。<a href=https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters>有许多解决方法</a>
可以避免在每次 CoreDNS 监测到循环并退出时，Kubernetes 尝试重启 CoreDNS Pod 的情况。</p><div class="alert alert-danger warning callout" role=alert><strong>警告：</strong> 禁用 SELinux 或设置 <code>allowPrivilegeEscalation</code> 为 <code>true</code> 可能会损害集群的安全性。</div><h2 id=etcd-pod-持续重启>etcd Pod 持续重启</h2><p>如果你遇到以下错误：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &#34;process_linux.go:110: decoding init error from pipe caused \&#34;read parent: connection reset by peer\&#34;&#34;
</span></span></span></code></pre></div><p>如果你使用 Docker 1.13.1.84 运行 CentOS 7 就会出现这种问题。
此版本的 Docker 会阻止 kubelet 在 etcd 容器中执行。</p><p>为解决此问题，请选择以下选项之一：</p><ul><li><p>回滚到早期版本的 Docker，例如 1.13.1-75</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
</span></span></code></pre></div></li><li><p>安装较新的推荐版本之一，例如 18.06:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span>yum install docker-ce-18.06.1.ce-3.el7.x86_64
</span></span></code></pre></div></li></ul><h2 id=无法将以逗号分隔的值列表传递给-component-extra-args-标志内的参数>无法将以逗号分隔的值列表传递给 <code>--component-extra-args</code> 标志内的参数</h2><p><code>kubeadm init</code> 标志例如 <code>--component-extra-args</code> 允许你将自定义参数传递给像
kube-apiserver 这样的控制平面组件。然而，由于解析 (<code>mapStringString</code>) 的基础类型值，此机制将受到限制。</p><p>如果你决定传递一个支持多个逗号分隔值（例如
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists"</code>）参数，
将出现 <code>flag: malformed pair, expect string=string</code> 错误。
发生这种问题是因为参数列表 <code>--apiserver-extra-args</code> 预期的是 <code>key=value</code> 形式，
而这里的 <code>NamespacesExists</code> 被误认为是缺少取值的键名。</p><p>一种解决方法是尝试分离 <code>key=value</code> 对，像这样：
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"</code>
但这将导致键 <code>enable-admission-plugins</code> 仅有值 <code>NamespaceExists</code>。</p><p>已知的解决方法是使用 kubeadm
<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>。</p><h2 id=在节点被云控制管理器初始化之前-kube-proxy-就被调度了>在节点被云控制管理器初始化之前，kube-proxy 就被调度了</h2><p>在云环境场景中，可能出现在云控制管理器完成节点地址初始化之前，kube-proxy 就被调度到新节点了。
这会导致 kube-proxy 无法正确获取节点的 IP 地址，并对管理负载平衡器的代理功能产生连锁反应。</p><p>在 kube-proxy Pod 中可以看到以下错误：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
</span></span></span><span style=display:flex><span><span style=color:#888>proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
</span></span></span></code></pre></div><p>一种已知的解决方案是修补 kube-proxy DaemonSet，以允许在控制平面节点上调度它，
而不管它们的条件如何，将其与其他节点保持隔离，直到它们的初始保护条件消除：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl -n kube-system patch ds kube-proxy -p<span style=color:#666>=</span><span style=color:#b44>&#39;{ &#34;spec&#34;: { &#34;template&#34;: { &#34;spec&#34;: { &#34;tolerations&#34;: [ { &#34;key&#34;: &#34;CriticalAddonsOnly&#34;, &#34;operator&#34;: &#34;Exists&#34; }, { &#34;effect&#34;: &#34;NoSchedule&#34;, &#34;key&#34;: &#34;node-role.kubernetes.io/control-plane&#34; } ] } } } }&#39;</span>
</span></span></code></pre></div><p>此问题的跟踪<a href=https://github.com/kubernetes/kubeadm/issues/1027>在这里</a>。</p><h2 id=usr-mounted-read-only>节点上的 <code>/usr</code> 被以只读方式挂载</h2><p>在类似 Fedora CoreOS 或者 Flatcar Container Linux 这类 Linux 发行版本中，
目录 <code>/usr</code> 是以只读文件系统的形式挂载的。
在支持 <a href=https://github.com/kubernetes/community/blob/ab55d85/contributors/devel/sig-storage/flexvolume.md>FlexVolume</a>时，
类似 kubelet 和 kube-controller-manager 这类 Kubernetes 组件使用默认路径
<code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code>，
而 FlexVolume 的目录 <strong>必须是可写入的</strong>，该功能特性才能正常工作。
（<strong>注意</strong>：FlexVolume 在 Kubernetes v1.23 版本中已被弃用）</p><p>为了解决这个问题，你可以使用 kubeadm 的<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a> 来配置 FlexVolume 的目录。</p><p>在（使用 <code>kubeadm init</code> 创建的）主控制节点上，使用 <code>--config</code>
参数传入如下文件：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>flex-volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>在加入到集群中的节点上，使用下面的文件：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>或者，你要可以更改 <code>/etc/fstab</code> 使得 <code>/usr</code> 目录能够以可写入的方式挂载，
不过请注意这样做本质上是在更改 Linux 发行版的某种设计原则。</p><h2 id=kubeadm-upgrade-plan-输出错误信息-context-deadline-exceeded><code>kubeadm upgrade plan</code> 输出错误信息 <code>context deadline exceeded</code></h2><p>在使用 <code>kubeadm</code> 来升级某运行外部 etcd 的 Kubernetes 集群时可能显示这一错误信息。
这并不是一个非常严重的一个缺陷，之所以出现此错误信息，原因是老的 kubeadm
版本会对外部 etcd 集群执行版本检查。你可以继续执行 <code>kubeadm upgrade apply ...</code>。</p><p>这一问题已经在 1.19 版本中得到修复。</p><h2 id=kubeadm-reset-会卸载-var-lib-kubelet><code>kubeadm reset</code> 会卸载 <code>/var/lib/kubelet</code></h2><p>如果已经挂载了 <code>/var/lib/kubelet</code> 目录，执行 <code>kubeadm reset</code>
操作的时候会将其卸载。</p><p>要解决这一问题，可以在执行了 <code>kubeadm reset</code> 操作之后重新挂载
<code>/var/lib/kubelet</code> 目录。</p><p>这是一个在 1.15 中引入的故障，已经在 1.20 版本中修复。</p><h2 id=无法在-kubeadm-集群中安全地使用-metrics-server>无法在 kubeadm 集群中安全地使用 metrics-server</h2><p>在 kubeadm 集群中可以通过为 <a href=https://github.com/kubernetes-sigs/metrics-server>metrics-server</a>
设置 <code>--kubelet-insecure-tls</code> 来以不安全的形式使用该服务。
建议不要在生产环境集群中这样使用。</p><p>如果你需要在 metrics-server 和 kubelet 之间使用 TLS，会有一个问题，
kubeadm 为 kubelet 部署的是自签名的服务证书。这可能会导致 metrics-server
端报告下面的错误信息：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>x509: certificate signed by unknown authority
</span></span></span><span style=display:flex><span><span style=color:#888>x509: certificate is valid for IP-foo not IP-bar
</span></span></span></code></pre></div><p>参见<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs>为 kubelet 启用签名的服务证书</a>
以进一步了解如何在 kubeadm 集群中配置 kubelet 使用正确签名了的服务证书。</p><p>另请参阅 <a href=https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-to-run-metrics-server-securely>How to run the metrics-server securely</a>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-134ed1f6142a98e6ac681a1ba4920e53>2.2.1.3 - 使用 kubeadm 创建集群</h1><p><img src=/images/kubeadm-stacked-color.png align=right width=150px></img>
使用 <code>kubeadm</code>，你能创建一个符合最佳实践的最小化 Kubernetes 集群。
事实上，你可以使用 <code>kubeadm</code> 配置一个通过
<a href=/blog/2017/10/software-conformance-certification/>Kubernetes 一致性测试</a>的集群。
<code>kubeadm</code> 还支持其他集群生命周期功能，
例如<a href=/zh-cn/docs/reference/access-authn-authz/bootstrap-tokens/>启动引导令牌</a>和集群升级。</p><p>kubeadm 工具很棒，如果你需要：</p><ul><li>一个尝试 Kubernetes 的简单方法。</li><li>一个现有用户可以自动设置集群并测试其应用程序的途径。</li><li>其他具有更大范围的生态系统和/或安装工具中的构建模块。</li></ul><p>你可以在各种机器上安装和使用 <code>kubeadm</code>：笔记本电脑，
一组云服务器，Raspberry Pi 等。无论是部署到云还是本地，
你都可以将 <code>kubeadm</code> 集成到预配置系统中，例如 Ansible 或 Terraform。</p><h2 id=准备开始>准备开始</h2><p>要遵循本指南，你需要：</p><ul><li>一台或多台运行兼容 deb/rpm 的 Linux 操作系统的计算机；例如：Ubuntu 或 CentOS。</li><li>每台机器 2 GB 以上的内存，内存不足时应用会受限制。</li><li>用作控制平面节点的计算机上至少有 2 个 CPU。</li><li>集群中所有计算机之间具有完全的网络连接。你可以使用公共网络或专用网络。</li></ul><p>你还需要使用可以在新集群中部署特定 Kubernetes 版本对应的 <code>kubeadm</code>。</p><p><a href=/zh-cn/releases/version-skew-policy/#supported-versions>Kubernetes 版本及版本偏差策略</a>适用于 <code>kubeadm</code> 以及整个 Kubernetes。
查阅该策略以了解支持哪些版本的 Kubernetes 和 <code>kubeadm</code>。
该页面是为 Kubernetes v1.25 编写的。</p><p><code>kubeadm</code> 工具的整体功能状态为一般可用性（GA）。一些子功能仍在积极开发中。
随着工具的发展，创建集群的实现可能会略有变化，但总体实现应相当稳定。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong> 根据定义，在 <code>kubeadm alpha</code> 下的所有命令均在 alpha 级别上受支持。</div><h2 id=objectives>目标</h2><ul><li>安装单个控制平面的 Kubernetes 集群</li><li>在集群上安装 Pod 网络，以便你的 Pod 可以相互连通</li></ul><h2 id=instructions>操作指南</h2><h3 id=preparing-the-hosts>主机准备</h3><p>在所有主机上安装 <a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a> 和 kubeadm。
详细说明和其他前提条件，请参见<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>安装 kubeadm</a>。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>如果你已经安装了kubeadm，执行 <code>apt-get update && apt-get upgrade</code> 或 <code>yum update</code>
以获取 kubeadm 的最新版本。</p><p>升级时，kubelet 每隔几秒钟重新启动一次，
在 crashloop 状态中等待 kubeadm 发布指令。crashloop 状态是正常现象。
初始化控制平面后，kubelet 将正常运行。</p></div><h3 id=preparing-the-required-container-images>准备所需的容器镜像</h3><p>这个步骤是可选的，只适用于你希望 <code>kubeadm init</code> 和 <code>kubeadm join</code> 不去下载存放在 <code>registry.k8s.io</code> 上的默认的容器镜像的情况。</p><p>当你在离线的节点上创建一个集群的时候，Kubeadm 有一些命令可以帮助你预拉取所需的镜像。
阅读<a href=/zh-cn/docs/reference/setup-tools/kubeadm/kubeadm-init#without-internet-connection>离线运行 kubeadm</a>
获取更多的详情。</p><p>Kubeadm 允许你给所需要的镜像指定一个自定义的镜像仓库。
阅读<a href=/zh-cn/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images>使用自定义镜像</a>
获取更多的详情。</p><h3 id=initializing-your-control-plane-node>初始化控制平面节点</h3><p>控制平面节点是运行控制平面组件的机器，
包括 <a class=glossary-tooltip title='一致且高度可用的键值存储，用作 Kubernetes 的所有集群数据的后台数据库。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a> （集群数据库）
和 <a class=glossary-tooltip title='提供 Kubernetes API 服务的控制面组件。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API Server'>API Server</a>
（命令行工具 <a class=glossary-tooltip title='kubectl 是用来和 Kubernetes 集群进行通信的命令行工具。' data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a> 与之通信）。</p><ol><li>（推荐）如果计划将单个控制平面 kubeadm 集群升级成高可用，
你应该指定 <code>--control-plane-endpoint</code> 为所有控制平面节点设置共享端点。
端点可以是负载均衡器的 DNS 名称或 IP 地址。</li><li>选择一个 Pod 网络插件，并验证是否需要为 <code>kubeadm init</code> 传递参数。
根据你选择的第三方网络插件，你可能需要设置 <code>--pod-network-cidr</code> 的值。
请参阅<a href=#pod-network>安装 Pod 网络附加组件</a>。</li></ol><ol><li>（可选）<code>kubeadm</code> 试图通过使用已知的端点列表来检测容器运行时。
使用不同的容器运行时或在预配置的节点上安装了多个容器运行时，请为 <code>kubeadm init</code> 指定 <code>--cri-socket</code> 参数。
请参阅<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime>安装运行时</a>。</li><li>（可选）除非另有说明，否则 <code>kubeadm</code> 使用与默认网关关联的网络接口来设置此控制平面节点 API server 的广播地址。
要使用其他网络接口，请为 <code>kubeadm init</code> 设置 <code>--apiserver-advertise-address=&lt;ip-address></code> 参数。
要部署使用 IPv6 地址的 Kubernetes 集群，
必须指定一个 IPv6 地址，例如 <code>--apiserver-advertise-address=2001:db8::101</code></li></ol><p>要初始化控制平面节点，请运行：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init &lt;args&gt;
</span></span></code></pre></div><h3 id=considerations-about-apiserver-advertise-address-and-controlplaneendpoint>关于 apiserver-advertise-address 和 ControlPlaneEndpoint 的注意事项</h3><p><code>--apiserver-advertise-address</code> 可用于为控制平面节点的 API server 设置广播地址，
<code>--control-plane-endpoint</code> 可用于为所有控制平面节点设置共享端点。</p><p><code>--control-plane-endpoint</code> 允许 IP 地址和可以映射到 IP 地址的 DNS 名称。
请与你的网络管理员联系，以评估有关此类映射的可能解决方案。</p><p>这是一个示例映射：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>192.168.0.102 cluster-endpoint
</span></span></span></code></pre></div><p>其中 <code>192.168.0.102</code> 是此节点的 IP 地址，<code>cluster-endpoint</code> 是映射到该 IP 的自定义 DNS 名称。
这将允许你将 <code>--control-plane-endpoint=cluster-endpoint</code> 传递给 <code>kubeadm init</code>，并将相同的 DNS 名称传递给 <code>kubeadm join</code>。
稍后你可以修改 <code>cluster-endpoint</code> 以指向高可用性方案中的负载均衡器的地址。</p><p>kubeadm 不支持将没有 <code>--control-plane-endpoint</code> 参数的单个控制平面集群转换为高可用性集群。</p><h3 id=more-information>更多信息</h3><p>有关 <code>kubeadm init</code> 参数的更多信息，请参见 <a href=/zh-cn/docs/reference/setup-tools/kubeadm/>kubeadm 参考指南</a>。</p><p>要使用配置文件配置 <code>kubeadm init</code> 命令，
请参见<a href=/zh-cn/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>带配置文件使用 kubeadm init</a>。</p><p>要自定义控制平面组件，包括可选的对控制平面组件和 etcd 服务器的活动探针提供 IPv6 支持，
请参阅<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/control-plane-flags/>自定义参数</a>。</p><p>要重新配置一个已经创建的集群，请参见
<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure>重新配置一个 kubeadm 集群</a>。</p><p>要再次运行 <code>kubeadm init</code>，你必须首先<a href=#tear-down>卸载集群</a>。</p><p>如果将具有不同架构的节点加入集群，
请确保已部署的 DaemonSet 对这种体系结构具有容器镜像支持。</p><p><code>kubeadm init</code> 首先运行一系列预检查以确保机器
准备运行 Kubernetes。这些预检查会显示警告并在错误时退出。然后 <code>kubeadm init</code>
下载并安装集群控制平面组件。这可能会需要几分钟。
完成之后你应该看到：</p><pre tabindex=0><code class=language-none data-lang=none>Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><p>要使非 root 用户可以运行 kubectl，请运行以下命令，
它们也是 <code>kubeadm init</code> 输出的一部分：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p <span style=color:#b8860b>$HOME</span>/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div><p>或者，如果你是 <code>root</code> 用户，则可以运行：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div><div class="alert alert-danger warning callout" role=alert><strong>警告：</strong><p>kubeadm 对 <code>admin.conf</code> 中的证书进行签名时，将其配置为
<code>Subject: O = system:masters, CN = kubernetes-admin</code>。
<code>system:masters</code> 是一个例外的、超级用户组，可以绕过鉴权层（例如 RBAC）。
不要将 <code>admin.conf</code> 文件与任何人共享，应该使用 <code>kubeadm kubeconfig user</code>
命令为其他用户生成 kubeconfig 文件，完成对他们的定制授权。
更多细节请参见<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-certs#kubeconfig-additional-users>为其他用户生成 kubeconfig 文件</a>。</div><p>记录 <code>kubeadm init</code> 输出的 <code>kubeadm join</code> 命令。
你需要此命令<a href=#join-nodes>将节点加入集群</a>。</p><p>令牌用于控制平面节点和加入节点之间的相互身份验证。
这里包含的令牌是密钥。确保它的安全，
因为拥有此令牌的任何人都可以将经过身份验证的节点添加到你的集群中。
可以使用 <code>kubeadm token</code> 命令列出，创建和删除这些令牌。
请参阅 <a href=/zh-cn/docs/reference/setup-tools/kubeadm/kubeadm-token/>kubeadm 参考指南</a>。</p><h3 id=pod-network>安装 Pod 网络附加组件</h3><div class="alert alert-warning caution callout" role=alert><strong>注意：</strong><p>本节包含有关网络设置和部署顺序的重要信息。
在继续之前，请仔细阅读所有建议。</p><p><strong>你必须部署一个基于 Pod 网络插件的
<a class=glossary-tooltip title='容器网络接口 (Container network interface；CNI) 插件是遵循 appc/CNI 协议的一类网络插件。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label=容器网络接口>容器网络接口</a>
(CNI)，以便你的 Pod 可以相互通信。
在安装网络之前，集群 DNS (CoreDNS) 将不会启动。</strong></p><ul><li>注意你的 Pod 网络不得与任何主机网络重叠：
如果有重叠，你很可能会遇到问题。
（如果你发现网络插件的首选 Pod 网络与某些主机网络之间存在冲突，
则应考虑使用一个合适的 CIDR 块来代替，
然后在执行 <code>kubeadm init</code> 时使用 <code>--pod-network-cidr</code> 参数并在你的网络插件的 YAML 中替换它）。</li></ul><ul><li>默认情况下，<code>kubeadm</code> 将集群设置为使用和强制使用 <a href=/zh-cn/docs/reference/access-authn-authz/rbac/>RBAC</a>（基于角色的访问控制）。
确保你的 Pod 网络插件支持 RBAC，以及用于部署它的 manifests 也是如此。</li></ul><ul><li>如果要为集群使用 IPv6（双协议栈或仅单协议栈 IPv6 网络），
请确保你的 Pod 网络插件支持 IPv6。
IPv6 支持已在 CNI <a href=https://github.com/containernetworking/cni/releases/tag/v0.6.0>v0.6.0</a> 版本中添加。</li></ul></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong> kubeadm 应该是与 CNI 无关的，对 CNI 驱动进行验证目前不在我们的端到端测试范畴之内。
如果你发现与 CNI 插件相关的问题，应在其各自的问题跟踪器中记录而不是在 kubeadm
或 kubernetes 问题跟踪器中记录。</div><p>一些外部项目为 Kubernetes 提供使用 CNI 的 Pod 网络，其中一些还支持<a href=/zh-cn/docs/concepts/services-networking/network-policies/>网络策略</a>。</p><p>请参阅实现 <a href=/zh-cn/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>Kubernetes 网络模型</a>的附加组件列表。</p><p>你可以使用以下命令在控制平面节点或具有 kubeconfig 凭据的节点上安装 Pod 网络附加组件：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f &lt;add-on.yaml&gt;
</span></span></code></pre></div><p>每个集群只能安装一个 Pod 网络。</p><p>安装 Pod 网络后，你可以通过在 <code>kubectl get pods --all-namespaces</code> 输出中检查
CoreDNS Pod 是否 <code>Running</code> 来确认其是否正常运行。
一旦 CoreDNS Pod 启用并运行，你就可以继续加入节点。</p><p>如果你的网络无法正常工作或 CoreDNS 不在“运行中”状态，请查看 <code>kubeadm</code> 的
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>故障排除指南</a>。</p><h3 id=managed-node-labels>托管节点标签</h3><p>默认情况下，kubeadm 启用 <a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction</a>
准入控制器来限制 kubelets 在节点注册时可以应用哪些标签。准入控制器文档描述 kubelet <code>--node-labels</code> 选项允许使用哪些标签。
其中 <code>node-role.kubernetes.io/control-plane</code> 标签就是这样一个受限制的标签，
kubeadm 在节点创建后使用特权客户端手动应用此标签。
你可以使用一个有特权的 kubeconfig，比如由 kubeadm 管理的 <code>/etc/kubernetes/admin.conf</code>，
通过执行 <code>kubectl label</code> 来手动完成操作。</p><h3 id=control-plane-node-isolation>控制平面节点隔离</h3><p>默认情况下，出于安全原因，你的集群不会在控制平面节点上调度 Pod。
如果你希望能够在控制平面节点上调度 Pod，例如单机 Kubernetes 集群，请运行:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl taint nodes --all node-role.kubernetes.io/control-plane- node-role.kubernetes.io/master-
</span></span></code></pre></div><p>输出看起来像：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>node &#34;test-01&#34; untainted
</span></span></span></code></pre></div><p>这将从任何拥有 <code>node-role.kubernetes.io/control-plane:NoSchedule</code>
污点的节点（包括控制平面节点）上移除该污点。
这意味着调度程序将能够在任何地方调度 Pod。</p><h3 id=join-nodes>加入节点</h3><p>节点是你的工作负载（容器和 Pod 等）运行的地方。要将新节点添加到集群，请对每台计算机执行以下操作：</p><ul><li><p>SSH 到机器</p></li><li><p>成为 root （例如 <code>sudo su -</code>）</p></li><li><p>必要时<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime>安装一个运行时</a></p></li><li><p>运行 <code>kubeadm init</code> 输出的命令，例如：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</span></span></code></pre></div></li></ul><p>如果没有令牌，可以通过在控制平面节点上运行以下命令来获取令牌：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token list
</span></span></code></pre></div><p>输出类似于以下内容：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
</span></span></span><span style=display:flex><span><span style=color:#888>8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                   signing          token generated by     bootstrappers:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                    &#39;kubeadm init&#39;.        kubeadm:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                                           default-node-token
</span></span></span></code></pre></div><p>默认情况下，令牌会在 24 小时后过期。如果要在当前令牌过期后将节点加入集群，
则可以通过在控制平面节点上运行以下命令来创建新令牌：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token create
</span></span></code></pre></div><p>输出类似于以下内容：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>5didvk.d09sbcov8ph2amjw
</span></span></span></code></pre></div><p>如果你没有 <code>--discovery-token-ca-cert-hash</code> 的值，则可以通过在控制平面节点上执行以下命令链来获取它：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>   openssl dgst -sha256 -hex | sed <span style=color:#b44>&#39;s/^.* //&#39;</span>
</span></span></code></pre></div><p>输出类似于以下内容：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong> 要为 <code>&lt;control-plane-host>:&lt;control-plane-port></code> 指定 IPv6 元组，必须将 IPv6 地址括在方括号中，例如：<code>[2001:db8::101]:2073</code></div><p>输出应类似于：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>[preflight] Running pre-flight checks
</span></span></span><span style=display:flex><span><span style=color:#888></span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#888>... (log output of join workflow) ...
</span></span></span><span style=display:flex><span><span style=color:#888></span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#888>Node join complete:
</span></span></span><span style=display:flex><span><span style=color:#888>* Certificate signing request sent to control-plane and response
</span></span></span><span style=display:flex><span><span style=color:#888>  received.
</span></span></span><span style=display:flex><span><span style=color:#888>* Kubelet informed of new secure connection details.
</span></span></span><span style=display:flex><span><span style=color:#888></span><span>
</span></span></span><span style=display:flex><span><span></span><span style=color:#888>Run &#39;kubectl get nodes&#39; on control-plane to see this machine join.
</span></span></span></code></pre></div><p>几秒钟后，当你在控制平面节点上执行 <code>kubectl get nodes</code>，你会注意到该节点出现在输出中。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong> 由于集群节点通常是按顺序初始化的，CoreDNS Pod 很可能都运行在第一个控制面节点上。
为了提供更高的可用性，请在加入至少一个新节点后
使用 <code>kubectl -n kube-system rollout restart deployment coredns</code> 命令，重新平衡这些 CoreDNS Pod。</div><h3 id=optional-controlling-your-cluster-from-machines-other-than-the-control-plane-node>（可选）从控制平面节点以外的计算机控制集群</h3><p>为了使 kubectl 在其他计算机（例如笔记本电脑）上与你的集群通信，
你需要将管理员 kubeconfig 文件从控制平面节点复制到工作站，如下所示：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf get nodes
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>上面的示例假定为 root 用户启用了 SSH 访问。如果不是这种情况，
你可以使用 <code>scp</code> 将 <code>admin.conf</code> 文件复制给其他允许访问的用户。</p><p>admin.conf 文件为用户提供了对集群的超级用户特权。
该文件应谨慎使用。对于普通用户，建议生成一个你为其授予特权的唯一证书。
你可以使用 <code>kubeadm alpha kubeconfig user --client-name &lt;CN></code> 命令执行此操作。
该命令会将 KubeConfig 文件打印到 STDOUT，你应该将其保存到文件并分发给用户。
之后，使用 <code>kubectl create (cluster)rolebinding</code> 授予特权。</p></div><h3 id=optional-proxying-api-server-to-localhost>（可选）将 API 服务器代理到本地主机</h3><p>如果要从集群外部连接到 API 服务器，则可以使用 <code>kubectl proxy</code>：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf proxy
</span></span></code></pre></div><p>你现在可以在本地访问 API 服务器 <code>http://localhost:8001/api/v1</code>。</p><h2 id=tear-down>清理</h2><p>如果你在集群中使用了一次性服务器进行测试，则可以关闭这些服务器，而无需进一步清理。你可以使用 <code>kubectl config delete-cluster</code> 删除对集群的本地引用。</p><p>但是，如果要更干净地取消配置集群，
则应首先<a href=/docs/reference/generated/kubectl/kubectl-commands#drain>清空节点</a>并确保该节点为空，
然后取消配置该节点。</p><h3 id=remove-the-node>删除节点</h3><p>使用适当的凭证与控制平面节点通信，运行：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain &lt;node name&gt; --delete-emptydir-data --force --ignore-daemonsets
</span></span></code></pre></div><p>在删除节点之前，请重置 <code>kubeadm</code> 安装的状态：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm reset
</span></span></code></pre></div><p>重置过程不会重置或清除 iptables 规则或 IPVS 表。如果你希望重置 iptables，则必须手动进行：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>iptables -F <span style=color:#666>&amp;&amp;</span> iptables -t nat -F <span style=color:#666>&amp;&amp;</span> iptables -t mangle -F <span style=color:#666>&amp;&amp;</span> iptables -X
</span></span></code></pre></div><p>如果要重置 IPVS 表，则必须运行以下命令：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -C
</span></span></code></pre></div><p>现在删除节点：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete node &lt;node name&gt;
</span></span></code></pre></div><p>如果你想重新开始，只需运行 <code>kubeadm init</code> 或 <code>kubeadm join</code> 并加上适当的参数。</p><h3 id=clean-up-the-control-plane>清理控制平面</h3><p>你可以在控制平面主机上使用 <code>kubeadm reset</code> 来触发尽力而为的清理。</p><p>有关此子命令及其选项的更多信息，请参见 <a href=/zh-cn/docs/reference/setup-tools/kubeadm/kubeadm-reset/><code>kubeadm reset</code></a> 参考文档。</p><h2 id=whats-next>下一步</h2><ul><li>使用 <a href=https://github.com/heptio/sonobuoy>Sonobuoy</a> 验证集群是否正常运行。</li><li><a id=lifecycle>有关使用 kubeadm 升级集群的详细信息，请参阅<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>升级 kubeadm 集群</a>。</li><li>在 <a href=/zh-cn/docs/reference/setup-tools/kubeadm/>kubeadm 参考文档</a>中了解有关 <code>kubeadm</code> 进阶用法的信息。</li><li>了解有关 Kubernetes <a href=/zh-cn/docs/concepts/>概念</a>和 <a href=/zh-cn/docs/reference/kubectl/><code>kubectl</code></a>的更多信息。</li><li>有关 Pod 网络附加组件的更多列表，请参见<a href=/zh-cn/docs/concepts/cluster-administration/networking/>集群网络</a>页面。</li><li><a id=other-addons>请参阅<a href=/zh-cn/docs/concepts/cluster-administration/addons/>附加组件列表</a>以探索其他附加组件，
包括用于 Kubernetes 集群的日志记录、监视、网络策略、可视化和控制的工具。</li><li>配置集群如何处理集群事件的日志以及
在 Pod 中运行的应用程序。
有关所涉及内容的概述，请参见<a href=/zh-cn/docs/concepts/cluster-administration/logging/>日志架构</a>。</li></ul><h3 id=feedback>反馈</h3><ul><li>有关漏洞，访问 <a href=https://github.com/kubernetes/kubeadm/issues>kubeadm GitHub issue tracker</a></li><li>有关支持，访问
<a href=https://kubernetes.slack.com/messages/kubeadm/>#kubeadm</a> Slack 频道</li><li>General SIG 集群生命周期开发 Slack 频道:
<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle/>#sig-cluster-lifecycle</a></li><li>SIG 集群生命周期 <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme>SIG information</a></li><li>SIG 集群生命周期邮件列表:
<a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle>kubernetes-sig-cluster-lifecycle</a></li></ul><h2 id=version-skew-policy>版本偏差策略</h2><p>虽然 kubeadm 允许所管理的组件有一定程度的版本偏差，
但是建议你将 kubeadm 的版本与控制平面组件、kube-proxy 和 kubelet 的版本相匹配。</p><h3 id=kubeadm-s-skew-against-the-kubernetes-version>kubeadm 中的 Kubernetes 版本偏差</h3><p>kubeadm 可以与 Kubernetes 组件一起使用，这些组件的版本与 kubeadm 相同，或者比它大一个版本。
Kubernetes 版本可以通过使用 <code>--kubeadm init</code> 的 <code>--kubernetes-version</code> 标志或使用 <code>--config</code> 时的
<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/><code>ClusterConfiguration.kubernetesVersion</code></a>
字段指定给 kubeadm。
这个选项将控制 kube-apiserver、kube-controller-manager、kube-scheduler 和 kube-proxy 的版本。</p><p>例子：</p><ul><li>kubeadm 的版本为 1.25。</li><li><code>kubernetesVersion</code> 必须为 1.25 或者 1.24。</li></ul><h3 id=kubeadm-s-skew-against-the-kubelet>kubeadm 中 kubelet 的版本偏差</h3><p>与 Kubernetes 版本类似，kubeadm 可以使用与 kubeadm 相同版本的 kubelet，
或者比 kubeadm 老一个版本的 kubelet。</p><p>例子：</p><ul><li>kubeadm 的版本为 1.25。</li><li>主机上的 kubelet 必须为 1.25 或者 1.24。</li></ul><h3 id=kubeadm-s-skew-against-kubeadm>kubeadm 支持的 kubeadm 的版本偏差</h3><p>kubeadm 命令在现有节点或由 kubeadm 管理的整个集群上的操作有一定限制。</p><p>如果新的节点加入到集群中，用于 <code>kubeadm join</code> 的 kubeadm 二进制文件必须与用 <code>kubeadm init</code>
创建集群或用 <code>kubeadm upgrade</code> 升级同一节点时所用的 kubeadm 版本一致。
类似的规则适用于除了 <code>kubeadm upgrade</code> 以外的其他 kubeadm 命令。</p><p><code>kubeadm join</code> 的例子：</p><ul><li>使用 <code>kubeadm init</code> 创建集群时使用版本为 1.25 的 kubeadm。</li><li>添加节点所用的 kubeadm 可执行文件为版本 。</li></ul><p>对于正在升级的节点，所使用的的 kubeadm 必须与管理该节点的 kubeadm 具有相同的
MINOR 版本或比后者新一个 MINOR 版本。</p><p><code>kubeadm upgrade</code> 的例子:</p><ul><li>用于创建或升级节点的 kubeadm 版本为 1.24。</li><li>用于升级节点的 kubeadm 版本必须为 1.24 或 1.25。</li></ul><p>要了解更多关于不同 Kubernetes 组件之间的版本偏差，请参见
<a href=/zh-cn/releases/version-skew-policy/>版本偏差策略</a>。</p><h2 id=limitations>局限性</h2><h3 id=resilience>集群弹性</h3><p>此处创建的集群具有单个控制平面节点，运行单个 etcd 数据库。
这意味着如果控制平面节点发生故障，你的集群可能会丢失数据并且可能需要从头开始重新创建。</p><p>解决方法：</p><ul><li>定期<a href=https://coreos.com/etcd/docs/latest/admin_guide.html>备份 etcd</a>。
kubeadm 配置的 etcd 数据目录位于控制平面节点上的 <code>/var/lib/etcd</code> 中。</li></ul><ul><li>使用多个控制平面节点。你可以阅读
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/>可选的高可用性拓扑</a>选择集群拓扑提供的
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/>高可用性</a>。</li></ul><h3 id=multi-platform>平台兼容性</h3><p>kubeadm deb/rpm 软件包和二进制文件是为 amd64、arm (32-bit)、arm64、ppc64le 和 s390x
构建的遵循<a href=https://git.k8s.io/design-proposals-archive/multi-platform.md>多平台提案</a>。</p><p>从 v1.12 开始还支持用于控制平面和附加组件的多平台容器镜像。</p><p>只有一些网络提供商为所有平台提供解决方案。
请查阅上方的网络提供商清单或每个提供商的文档以确定提供商是否支持你选择的平台。</p><h2 id=troubleshooting>故障排除</h2><p>如果你在使用 kubeadm 时遇到困难，
请查阅我们的<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>故障排除文档</a>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c656c5eda3e1c06ad1aedebdc04a211>2.2.1.4 - 使用 kubeadm API 定制组件</h1><p>本页面介绍了如何自定义 kubeadm 部署的组件。
你可以使用 <code>ClusterConfiguration</code> 结构中定义的参数，或者在每个节点上应用补丁来定制控制平面组件。
你可以使用 <code>KubeletConfiguration</code> 和 <code>KubeProxyConfiguration</code> 结构分别定制 kubelet 和 kube-proxy 组件。</p><p>所有这些选项都可以通过 kubeadm 配置 API 实现。
有关配置中的每个字段的详细信息，你可以导航到我们的
<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/>API 参考页面</a> 。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>kubeadm 目前不支持对 CoreDNS 部署进行定制。
你必须手动更新 <code>kube-system/coredns</code> <a class=glossary-tooltip title='ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/tasks/configure-pod-container/configure-pod-configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a>
并在更新后重新创建 CoreDNS <a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>。
或者，你可以跳过默认的 CoreDNS 部署并部署你自己的 CoreDNS 变种。
有关更多详细信息，请参阅<a href=/zh-cn/docs/reference/setup-tools/kubeadm/kubeadm-init/#init-phases>在 kubeadm 中使用 init phase</a>.</div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>要重新配置已创建的集群，请参阅<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure>重新配置 kubeadm 集群</a>。</div><h2 id=customizing-the-control-plane-with-flags-in-clusterconfiguration>使用 <code>ClusterConfiguration</code> 中的标志自定义控制平面</h2><p>kubeadm <code>ClusterConfiguration</code> 对象为用户提供了一种方法，
用以覆盖传递给控制平面组件（如 APIServer、ControllerManager、Scheduler 和 Etcd）的默认参数。
各组件配置使用如下字段定义：</p><ul><li><code>apiServer</code></li><li><code>controllerManager</code></li><li><code>scheduler</code></li><li><code>etcd</code></li></ul><p>这些结构包含一个通用的 <code>extraArgs</code> 字段，该字段由 <code>key: value</code> 组成。
要覆盖控制平面组件的参数：</p><ol><li>将适当的字段 <code>extraArgs</code> 添加到配置中。</li><li>向字段 <code>extraArgs</code> 添加要覆盖的参数值。</li><li>用 <code>--config &lt;YOUR CONFIG YAML></code> 运行 <code>kubeadm init</code>。</li></ol><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>你可以通过运行 <code>kubeadm config print init-defaults</code> 并将输出保存到你所选的文件中，
以默认值形式生成 <code>ClusterConfiguration</code> 对象。</div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>ClusterConfiguration</code> 对象目前在 kubeadm 集群中是全局的。
这意味着你添加的任何标志都将应用于同一组件在不同节点上的所有实例。
要在不同节点上为每个组件应用单独的配置，你可以使用<a href=#patches>补丁</a>。</div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>当前不支持重复的参数（keys）或多次传递相同的参数 <code>--foo</code>。
要解决此问题，你必须使用<a href=#patches>补丁</a>。</div><h3 id=apiserver-flags>APIServer 参数</h3><p>有关详细信息，请参阅 <a href=/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver 参考文档</a>。</p><p>使用示例：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>anonymous-auth</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;false&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>enable-admission-plugins</span>:<span style=color:#bbb> </span>AlwaysPullImages,DefaultStorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>audit-log-path</span>:<span style=color:#bbb> </span>/home/johndoe/audit.log<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=controllermanager-flags>ControllerManager 参数</h3><p>有关详细信息，请参阅 <a href=/zh-cn/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-manager 参考文档</a>。</p><p>使用示例：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-key-file</span>:<span style=color:#bbb> </span>/home/johndoe/keys/ca.key<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deployment-controller-sync-period</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;50&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=scheduler-flags>Scheduler 参数</h2><p>有关详细信息，请参阅 <a href=/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler 参考文档</a>。</p><p>使用示例：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduler</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>config</span>:<span style=color:#bbb> </span>/etc/kubernetes/scheduler-config.yaml<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraVolumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>schedulerconfig<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb> </span>/home/johndoe/schedconfig.yaml<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/kubernetes/scheduler-config.yaml<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;File&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=etcd-flags>Etcd 参数</h3><p>有关详细信息，请参阅 <a href=https://etcd.io/docs/>etcd 服务文档</a>.</p><p>使用示例：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>etcd</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>local</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>election-timeout</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=patches>使用补丁定制</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.22 [beta]</code></div><p>Kubeadm 允许将包含补丁文件的目录传递给各个节点上的 <code>InitConfiguration</code> 和 <code>JoinConfiguration</code>。
这些补丁可被用作组件配置写入磁盘之前的最后一个自定义步骤。</p><p>可以使用 <code>--config &lt;你的 YAML 格式控制文件></code> 将配置文件传递给 <code>kubeadm init</code>：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>patches</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>directory</span>:<span style=color:#bbb> </span>/home/user/somedir<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>对于 <code>kubeadm init</code>，你可以传递一个包含 <code>ClusterConfiguration</code> 和 <code>InitConfiguration</code> 的文件，以 <code>---</code> 分隔。</div><p>你可以使用 <code>--config &lt;你的 YAML 格式配置文件></code> 将配置文件传递给 <code>kubeadm join</code>：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>patches</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>directory</span>:<span style=color:#bbb> </span>/home/user/somedir<span style=color:#bbb>
</span></span></span></code></pre></div><p>补丁目录必须包含名为 <code>target[suffix][+patchtype].extension</code> 的文件。
例如，<code>kube-apiserver0+merge.yaml</code> 或只是 <code>etcd.json</code>。</p><ul><li><code>target</code> 可以是 <code>kube-apiserver</code>、<code>kube-controller-manager</code>、<code>kube-scheduler</code>、<code>etcd</code> 和 <code>kubeletconfiguration</code> 之一。</li><li><code>patchtype</code> 可以是 <code>strategy</code>、<code>merge</code> 或 <code>json</code> 之一，并且这些必须匹配
<a href=/zh-cn/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch>kubectl 支持</a> 的补丁格式。
默认补丁类型是 <code>strategic</code> 的。</li><li><code>extension</code> 必须是 <code>json</code> 或 <code>yaml</code>。</li><li><code>suffix</code> 是一个可选字符串，可用于确定首先按字母数字应用哪些补丁。</li></ul><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>如果你使用 <code>kubeadm upgrade</code> 升级 kubeadm 节点，你必须再次提供相同的补丁，以便在升级后保留自定义配置。
为此，你可以使用 <code>--patches</code> 参数，该参数必须指向同一目录。 <code>kubeadm upgrade</code> 目前不支持用于相同目的的 API 结构配置。</div><h2 id=kubelet>自定义 kubelet</h2><p>要自定义 kubelet，你可以在同一配置文件中的 <code>ClusterConfiguration</code> 或 <code>InitConfiguration</code>
之外添加一个 <a href=/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>，用 <code>---</code> 分隔。
然后可以将此文件传递给 <code>kubeadm init</code>，kubeadm 会将相同的
<code>KubeletConfiguration</code> 配置应用于集群中的所有节点。</p><p>要在基础 <code>KubeletConfiguration</code> 上应用特定节点的配置，你可以使用
<a href=#patches><code>kubeletconfiguration</code> 补丁定制</a>。</p><p>或者你可以使用 <code>kubelet</code> 参数进行覆盖，方法是将它们传递到 <code>InitConfiguration</code> 和 <code>JoinConfiguration</code>
支持的 <code>nodeRegistration.kubeletExtraArgs</code> 字段中。一些 kubelet 参数已被弃用，
因此在使用这些参数之前，请在 <a href=/zh-cn/docs/reference/command-line-tools-reference/kubelet>kubelet 参考文档</a> 中检查它们的状态。</p><p>更多详情，请参阅<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/kubelet-integration>使用 kubeadm 配置集群中的每个 kubelet</a></p><h2 id=customizing-kube-proxy>自定义 kube-proxy</h2><p>要自定义 kube-proxy，你可以在 <code>ClusterConfiguration</code> 或 <code>InitConfiguration</code>
之外添加一个由 <code>---</code> 分隔的 <code>KubeProxyConfiguration</code>， 传递给 <code>kubeadm init</code>。</p><p>可以导航到 <a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/>API 参考页面</a>查看更多详情，</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>kubeadm 将 kube-proxy 部署为 <a class=glossary-tooltip title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/controllers/daemonset/ target=_blank aria-label=DaemonSet>DaemonSet</a>，
这意味着 <code>KubeProxyConfiguration</code> 将应用于集群中的所有 kube-proxy 实例。</div></div><div class=td-content style=page-break-before:always><h1 id=pg-015edbc7cc688d31b1d1edce7c186135>2.2.1.5 - 高可用拓扑选项</h1><p>本页面介绍了配置高可用（HA）Kubernetes 集群拓扑的两个选项。</p><p>你可以设置 HA 集群：</p><ul><li>使用堆叠（stacked）控制平面节点，其中 etcd 节点与控制平面节点共存</li><li>使用外部 etcd 节点，其中 etcd 在与控制平面不同的节点上运行</li></ul><p>在设置 HA 集群之前，你应该仔细考虑每种拓扑的优缺点。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>kubeadm 静态引导 etcd 集群。
阅读 etcd <a href=https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static>集群指南</a>以获得更多详细信息。</div><h2 id=stacked-etcd-topology>堆叠（Stacked）etcd 拓扑</h2><p>堆叠（Stacked）HA 集群是一种这样的<a href=https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91>拓扑</a>，
其中 etcd 分布式数据存储集群堆叠在 kubeadm 管理的控制平面节点上，作为控制平面的一个组件运行。</p><p>每个控制平面节点运行 <code>kube-apiserver</code>、<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。
<code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。</p><p>每个控制平面节点创建一个本地 etcd 成员（member），这个 etcd 成员只与该节点的 <code>kube-apiserver</code> 通信。
这同样适用于本地 <code>kube-controller-manager</code> 和 <code>kube-scheduler</code> 实例。</p><p>这种拓扑将控制平面和 etcd 成员耦合在同一节点上。相对使用外部 etcd 集群，
设置起来更简单，而且更易于副本管理。</p><p>然而，堆叠集群存在耦合失败的风险。如果一个节点发生故障，则 etcd 成员和控制平面实例都将丢失，
并且冗余会受到影响。你可以通过添加更多控制平面节点来降低此风险。</p><p>因此，你应该为 HA 集群运行至少三个堆叠的控制平面节点。</p><p>这是 kubeadm 中的默认拓扑。当使用 <code>kubeadm init</code> 和 <code>kubeadm join --control-plane</code> 时，
在控制平面节点上会自动创建本地 etcd 成员。</p><p><img src=/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg alt="堆叠的 etcd 拓扑"></p><h2 id=external-etcd-topology>外部 etcd 拓扑</h2><p>具有外部 etcd 的 HA 集群是一种这样的<a href=https://zh.wikipedia.org/wiki/%E7%BD%91%E7%BB%9C%E6%8B%93%E6%89%91>拓扑</a>，
其中 etcd 分布式数据存储集群在独立于控制平面节点的其他节点上运行。</p><p>就像堆叠的 etcd 拓扑一样，外部 etcd 拓扑中的每个控制平面节点都会运行
<code>kube-apiserver</code>、<code>kube-scheduler</code> 和 <code>kube-controller-manager</code> 实例。
同样，<code>kube-apiserver</code> 使用负载均衡器暴露给工作节点。但是 etcd 成员在不同的主机上运行，
每个 etcd 主机与每个控制平面节点的 <code>kube-apiserver</code> 通信。</p><p>这种拓扑结构解耦了控制平面和 etcd 成员。因此它提供了一种 HA 设置，
其中失去控制平面实例或者 etcd 成员的影响较小，并且不会像堆叠的 HA 拓扑那样影响集群冗余。</p><p>但此拓扑需要两倍于堆叠 HA 拓扑的主机数量。
具有此拓扑的 HA 集群至少需要三个用于控制平面节点的主机和三个用于 etcd 节点的主机。</p><p><img src=/images/kubeadm/kubeadm-ha-topology-external-etcd.svg alt="外部 etcd 拓扑"></p><h2 id=接下来>接下来</h2><ul><li><a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/>使用 kubeadm 设置高可用集群</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3941d5c3409342219bf7e03128b8ecb6>2.2.1.6 - 利用 kubeadm 创建高可用集群</h1><p>本文讲述了使用 kubeadm 设置一个高可用的 Kubernetes 集群的两种不同方式：</p><ul><li>使用具有堆叠的控制平面节点。这种方法所需基础设施较少。etcd 成员和控制平面节点位于同一位置。</li><li>使用外部 etcd 集群。这种方法所需基础设施较多。控制平面的节点和 etcd 成员是分开的。</li></ul><p>在下一步之前，你应该仔细考虑哪种方法更好地满足你的应用程序和环境的需求。
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/>高可用拓扑选项</a> 讲述了每种方法的优缺点。</p><p>如果你在安装 HA 集群时遇到问题，请在 kubeadm <a href=https://github.com/kubernetes/kubeadm/issues/new>问题跟踪</a>里向我们提供反馈。</p><p>你也可以阅读<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>升级文档</a>。</p><div class="alert alert-warning caution callout" role=alert><strong>注意：</strong><p>这篇文档没有讲述在云提供商上运行集群的问题。在云环境中，
此处记录的方法不适用于类型为 LoadBalancer 的服务对象，也不适用于具有动态 PersistentVolume 的对象。</div><h2 id=准备开始>准备开始</h2><p>根据集群控制平面所选择的拓扑结构不同，准备工作也有所差异：</p><ul class="nav nav-tabs" id=prerequisite-tabs role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#prerequisite-tabs-0 role=tab aria-controls=prerequisite-tabs-0 aria-selected=true>堆叠（Stacked） etcd 拓扑</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#prerequisite-tabs-1 role=tab aria-controls=prerequisite-tabs-1>外部 etcd 拓扑</a></li></ul><div class=tab-content id=prerequisite-tabs><div id=prerequisite-tabs-0 class="tab-pane show active" role=tabpanel aria-labelledby=prerequisite-tabs-0><p><p>需要准备：</p><ul><li>配置满足 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B>kubeadm 的最低要求</a>
的三台机器作为控制面节点。控制平面节点为奇数有利于机器故障或者分区故障时重新选举。<ul><li>机器已经安装好<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>，并正常运行</li></ul></li><li>配置满足 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B>kubeadm 的最低要求</a>
的三台机器作为工作节点<ul><li>机器已经安装好<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>，并正常运行</li></ul></li><li>在集群中，确保所有计算机之间存在全网络连接（公网或私网）</li><li>在所有机器上具有 sudo 权限<ul><li>可以使用其他工具；本教程以 <code>sudo</code> 举例</li></ul></li><li>从某台设备通过 SSH 访问系统中所有节点的能力</li><li>所有机器上已经安装 <code>kubeadm</code> 和 <code>kubelet</code></li></ul><p><strong>拓扑详情请参考<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology>堆叠（Stacked）etcd 拓扑</a>。</strong></p></div><div id=prerequisite-tabs-1 class=tab-pane role=tabpanel aria-labelledby=prerequisite-tabs-1><p><p>需要准备：</p><ul><li>配置满足 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B>kubeadm 的最低要求</a>
的三台机器作为控制面节点。控制平面节点为奇数有利于机器故障或者分区故障时重新选举。<ul><li>机器已经安装好<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>，并正常运行</li></ul></li><li>配置满足 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%87%86%E5%A4%87%E5%BC%80%E5%A7%8B>kubeadm 的最低要求</a>
的三台机器作为工作节点<ul><li>机器已经安装好<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>，并正常运行</li></ul></li><li>在集群中，确保所有计算机之间存在全网络连接（公网或私网）</li><li>在所有机器上具有 sudo 权限<ul><li>可以使用其他工具；本教程以 <code>sudo</code> 举例</li></ul></li><li>从某台设备通过 SSH 访问系统中所有节点的能力</li><li>所有机器上已经安装 <code>kubeadm</code> 和 <code>kubelet</code></li></ul><p>还需要准备：</p><ul><li>给 etcd 集群使用的另外至少三台机器。为了分布式一致性算法达到更好的投票效果，集群必须由奇数个节点组成。<ul><li>机器上已经安装 <code>kubeadm</code> 和 <code>kubelet</code>。</li><li>机器上同样需要安装好容器运行时，并能正常运行。</li></ul></li></ul><p><strong>拓扑详情请参考<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology/#external-etcd-topology>外部 etcd 拓扑</a>。</strong></p></div></div><h3 id=容器镜像>容器镜像</h3><p>每台主机需要能够从 Kubernetes 容器镜像仓库（<code>registry.k8s.io</code>）读取和拉取镜像。
想要在无法拉取 Kubernetes 仓库镜像的机器上部署高可用集群也是可行的。通过其他的手段保证主机上已经有对应的容器镜像即可。</p><h3 id=kubectl>命令行</h3><p>一旦集群创建成功，需要在 PC 上<a href=/zh-cn/docs/tasks/tools/#kubectl>安装 kubectl</a> 用于管理 Kubernetes。
为了方便故障排查，也可以在每个控制平面节点上安装 <code>kubectl</code>。</p><h2 id=这两种方法的第一步>这两种方法的第一步</h2><h3 id=为-kube-apiserver-创建负载均衡器>为 kube-apiserver 创建负载均衡器</h3><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>使用负载均衡器需要许多配置。你的集群搭建可能需要不同的配置。下面的例子只是其中的一方面配置。</div><ol><li><p>创建一个名为 kube-apiserver 的负载均衡器解析 DNS。</p><ul><li><p>在云环境中，应该将控制平面节点放置在 TCP 转发负载平衡后面。
该负载均衡器将流量分配给目标列表中所有运行状况良好的控制平面节点。
API 服务器的健康检查是在 kube-apiserver 的监听端口（默认值 <code>:6443</code>）
上进行的一个 TCP 检查。</p></li><li><p>不建议在云环境中直接使用 IP 地址。</p></li><li><p>负载均衡器必须能够在 API 服务器端口上与所有控制平面节点通信。
它还必须允许其监听端口的入站流量。</p></li><li><p>确保负载均衡器的地址始终匹配 kubeadm 的 <code>ControlPlaneEndpoint</code> 地址。</p></li><li><p>阅读<a href=https://git.k8s.io/kubeadm/docs/ha-considerations.md#options-for-software-load-balancing>软件负载平衡选项指南</a>
以获取更多详细信息。</p></li></ul></li></ol><ol start=2><li><p>添加第一个控制平面节点到负载均衡器并测试连接：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>nc -v LOAD_BALANCER_IP PORT
</span></span></code></pre></div><p>由于 API 服务器尚未运行，预期会出现一个连接拒绝错误。
然而超时意味着负载均衡器不能和控制平面节点通信。
如果发生超时，请重新配置负载均衡器与控制平面节点进行通信。</p></li><li><p>将其余控制平面节点添加到负载均衡器目标组。</p></li></ol><h2 id=使用堆控制平面和-etcd-节点>使用堆控制平面和 etcd 节点</h2><h3 id=控制平面节点的第一步>控制平面节点的第一步</h3><ol><li><p>初始化控制平面：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo kubeadm init --control-plane-endpoint <span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span> --upload-certs
</span></span></code></pre></div><ul><li>你可以使用 <code>--kubernetes-version</code> 标志来设置要使用的 Kubernetes 版本。
建议将 kubeadm、kebelet、kubectl 和 Kubernetes 的版本匹配。</li><li>这个 <code>--control-plane-endpoint</code> 标志应该被设置成负载均衡器的地址或 DNS 和端口。</li><li>这个 <code>--upload-certs</code> 标志用来将在所有控制平面实例之间的共享证书上传到集群。
如果正好相反，你更喜欢手动地通过控制平面节点或者使用自动化工具复制证书，
请删除此标志并参考如下部分<a href=#manual-certs>证书分配手册</a>。</li></ul><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>标志 <code>kubeadm init</code>、<code>--config</code> 和 <code>--certificate-key</code> 不能混合使用，
因此如果你要使用
<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/>kubeadm 配置</a>，你必须在相应的配置结构
（位于 <code>InitConfiguration</code> 和 <code>JoinConfiguration: controlPlane</code>）添加 <code>certificateKey</code> 字段。</div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>一些 CNI 网络插件需要更多配置，例如指定 Pod IP CIDR，而其他插件则不需要。参考
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>CNI 网络文档</a>。
通过传递 <code>--pod-network-cidr</code> 标志添加 Pod CIDR，或者你可以使用 kubeadm
配置文件，在 <code>ClusterConfiguration</code> 的 <code>networking</code> 对象下设置 <code>podSubnet</code> 字段。</div><ul><li>输出类似于：</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>...
</span></span><span style=display:flex><span>You can now join any number of control-plane node by running the following <span style=color:#a2f>command</span> on each as a root:
</span></span><span style=display:flex><span>kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
</span></span><span style=display:flex><span>As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style=display:flex><span>  kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><ul><li><p>将此输出复制到文本文件。 稍后你将需要它来将控制平面节点和工作节点加入集群。</p></li><li><p>当使用 <code>--upload-certs</code> 调用 <code>kubeadm init</code> 时，主控制平面的证书被加密并上传到 <code>kubeadm-certs</code> Secret 中。</p></li><li><p>要重新上传证书并生成新的解密密钥，请在已加入集群节点的控制平面上使用以下命令：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo kubeadm init phase upload-certs --upload-certs
</span></span></code></pre></div></li></ul><ul><li>你还可以在 <code>init</code> 期间指定自定义的 <code>--certificate-key</code>，以后可以由 <code>join</code> 使用。
要生成这样的密钥，可以使用以下命令：</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubeadm certs certificate-key
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>kubeadm-certs</code> Secret 和解密密钥会在两个小时后失效。</div><div class="alert alert-warning caution callout" role=alert><strong>注意：</strong><p>正如命令输出中所述，证书密钥可访问集群敏感数据。请妥善保管！</div></li></ol><ol start=2><li><p>应用你所选择的 CNI 插件：
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>请遵循以下指示</a>
安装 CNI 驱动。如果适用，请确保配置与 kubeadm 配置文件中指定的 Pod
CIDR 相对应。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>在进行下一步之前，必须选择并部署合适的网络插件。
否则集群不会正常运行。</div></li></ol><ol start=3><li><p>输入以下内容，并查看控制平面组件的 Pod 启动：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pod -n kube-system -w
</span></span></code></pre></div></li></ol><h3 id=其余控制平面节点的步骤>其余控制平面节点的步骤</h3><p>对于每个其他控制平面节点，你应该：</p><ol><li><p>执行先前由第一个节点上的 <code>kubeadm init</code> 输出提供给你的 join 命令。
它看起来应该像这样：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span></code></pre></div><ul><li>这个 <code>--control-plane</code> 标志通知 <code>kubeadm join</code> 创建一个新的控制平面。</li><li><code>--certificate-key ...</code> 将导致从集群中的 <code>kubeadm-certs</code> Secret
下载控制平面证书并使用给定的密钥进行解密。</li></ul></li></ol><h2 id=外部-etcd-节点>外部 etcd 节点</h2><p>使用外部 etcd 节点设置集群类似于用于堆叠 etcd 的过程，
不同之处在于你应该首先设置 etcd，并在 kubeadm 配置文件中传递 etcd 信息。</p><h3 id=设置-ectd-集群>设置 ectd 集群</h3><ol><li><p>按照<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>这里</a>的指示去设置。</p></li><li><p>根据<a href=#manual-certs>这里</a> 的描述配置 SSH。</p></li><li><p>将以下文件从集群中的任何 etcd 节点复制到第一个控制平面节点：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#666>=</span><span style=color:#b44>&#34;ubuntu@10.0.0.7&#34;</span>
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span></code></pre></div><ul><li>用第一台控制平面节点的 <code>user@host</code> 替换 <code>CONTROL_PLANE</code> 的值。</li></ul></li></ol><h3 id=设置第一个控制平面节点>设置第一个控制平面节点</h3><ol><li><p>用以下内容创建一个名为 <code>kubeadm-config.yaml</code> 的文件：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>stable<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controlPlaneEndpoint</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># change this (see below)</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>etcd</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>external</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>endpoints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- https://ETCD_0_IP:2379<span style=color:#bbb> </span><span style=color:#080;font-style:italic># 适当地更改 ETCD_0_IP</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- https://ETCD_1_IP:2379<span style=color:#bbb> </span><span style=color:#080;font-style:italic># 适当地更改 ETCD_1_IP</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- https://ETCD_2_IP:2379<span style=color:#bbb> </span><span style=color:#080;font-style:italic># 适当地更改 ETCD_2_IP</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caFile</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/etcd/ca.crt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>certFile</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/apiserver-etcd-client.crt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>keyFile</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/apiserver-etcd-client.key<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>这里的堆叠（stacked）etcd 和外部 etcd 之前的区别在于设置外部 etcd
需要一个 <code>etcd</code> 的 <code>external</code> 对象下带有 etcd 端点的配置文件。
如果是内部 etcd，是自动管理的。</div><ul><li><p>在你的集群中，将配置模板中的以下变量替换为适当值：</p></li><li><p><code>LOAD_BALANCER_DNS</code></p></li><li><p><code>LOAD_BALANCER_PORT</code></p></li><li><p><code>ETCD_0_IP</code></p></li><li><p><code>ETCD_1_IP</code></p></li><li><p><code>ETCD_2_IP</code></p></li></ul></li></ol><p>以下的步骤与设置内置 etcd 的集群是相似的：</p><ol><li><p>在节点上运行 <code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code> 命令。</p></li><li><p>记下输出的 join 命令，这些命令将在以后使用。</p></li><li><p>应用你选择的 CNI 插件。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>在进行下一步之前，必须选择并部署合适的网络插件。
否则集群不会正常运行。</div></li></ol><h3 id=其他控制平面节点的步骤>其他控制平面节点的步骤</h3><p>步骤与设置内置 etcd 相同：</p><ul><li>确保第一个控制平面节点已完全初始化。</li><li>使用保存到文本文件的 join 命令将每个控制平面节点连接在一起。
建议一次加入一个控制平面节点。</li><li>不要忘记默认情况下，<code>--certificate-key</code> 中的解密秘钥会在两个小时后过期。</li></ul><h2 id=列举控制平面之后的常见任务>列举控制平面之后的常见任务</h2><h3 id=安装工作节点>安装工作节点</h3><p>你可以使用之前存储的 <code>kubeadm init</code> 命令的输出将工作节点加入集群中：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><h2 id=manual-certs>手动证书分发</h2><p>如果你选择不将 <code>kubeadm init</code> 与 <code>--upload-certs</code> 命令一起使用，
则意味着你将必须手动将证书从主控制平面节点复制到将要加入的控制平面节点上。</p><p>有许多方法可以实现这种操作。下面的例子使用了 <code>ssh</code> 和 <code>scp</code>：</p><p>如果要在单独的一台计算机控制所有节点，则需要 SSH。</p><ol><li><p>在你的主设备上启用 ssh-agent，要求该设备能访问系统中的所有其他节点：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>eval</span> <span style=color:#a2f;font-weight:700>$(</span>ssh-agent<span style=color:#a2f;font-weight:700>)</span>
</span></span></code></pre></div></li></ol><ol start=2><li><p>将 SSH 身份添加到会话中：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ssh-add ~/.ssh/path_to_private_key
</span></span></code></pre></div></li></ol><ol start=3><li><p>检查节点间的 SSH 以确保连接是正常运行的</p><ul><li>SSH 到任何节点时，请确保添加 <code>-A</code> 标志。
此标志允许你通过 SSH 登录到节点后从该节点上访问你自己 PC 上的 SSH 代理。
如果你不完全信任该节点上的用户会话安全，可以考虑使用其他替代方法。</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ssh -A 10.0.0.7
</span></span></code></pre></div><ul><li>当在任何节点上使用 sudo 时，请确保保持环境变量设置，以便 SSH
转发能够正常工作：</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo -E -s
</span></span></code></pre></div></li></ol><ol start=4><li><p>在所有节点上配置 SSH 之后，你应该在运行过 <code>kubeadm init</code> 命令的第一个控制平面节点上运行以下脚本。
该脚本会将证书从第一个控制平面节点复制到另一个控制平面节点：</p><p>在以下示例中，用其他控制平面节点的 IP 地址替换 <code>CONTROL_PLANE_IPS</code>。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 可定制</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#666>=</span><span style=color:#b44>&#34;10.0.0.7 10.0.0.8&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> host in <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#b68;font-weight:700>}</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.pub <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.crt
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic># 如果你正使用外部 etcd，忽略下一行</span>
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.key
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div><div class="alert alert-warning caution callout" role=alert><strong>注意：</strong><p>只需要复制上面列表中的证书。kubeadm 将负责生成其余证书以及加入控制平面实例所需的 SAN。
如果你错误地复制了所有证书，由于缺少所需的 SAN，创建其他节点可能会失败。</div></li></ol><ol start=5><li><p>然后，在每个即将加入集群的控制平面节点上，你必须先运行以下脚本，然后再运行 <code>kubeadm join</code>。
该脚本会将先前复制的证书从主目录移动到 <code>/etc/kubernetes/pki</code>：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 可定制</span>
</span></span><span style=display:flex><span>mkdir -p /etc/kubernetes/pki/etcd
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.pub /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 如果你正使用外部 etcd，忽略下一行</span>
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</span></span></code></pre></div></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-8160424c22d24f7d2d63c521e107dbf8>2.2.1.7 - 使用 kubeadm 创建一个高可用 etcd 集群</h1><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>在本指南中，使用 kubeadm 作为外部 etcd 节点管理工具，请注意 kubeadm 不计划支持此类节点的证书更换或升级。
对于长期规划是使用 <a href=https://github.com/kubernetes-sigs/etcdadm>etcdadm</a> 增强工具来管理这些方面。</div><p>默认情况下，kubeadm 在每个控制平面节点上运行一个本地 etcd 实例。也可以使用外部的 etcd 集群，并在不同的主机上提供 etcd 实例。
这两种方法的区别在 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/ha-topology>高可用拓扑的选项</a> 页面中阐述。</p><p>这个任务将指导你创建一个由三个成员组成的高可用外部 etcd 集群，该集群在创建过程中可被 kubeadm 使用。</p><h2 id=准备开始>准备开始</h2><ul><li>三个可以通过 2379 和 2380 端口相互通信的主机。本文档使用这些作为默认端口。不过，它们可以通过 kubeadm 的配置文件进行自定义。</li></ul><ul><li>每个主机必须安装 systemd 和 bash 兼容的 shell。</li><li>每台主机必须<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>安装有容器运行时、kubelet 和 kubeadm</a>。</li></ul><ul><li>每个主机都应该能够访问 Kubernetes 容器镜像仓库 (registry.k8s.io)，
或者使用 <code>kubeadm config images list/pull</code> 列出/拉取所需的 etcd 镜像。
本指南将把 etcd 实例设置为由 kubelet 管理的<a href=/zh-cn/docs/tasks/configure-pod-container/static-pod/>静态 Pod</a>。</li></ul><ul><li>一些可以用来在主机间复制文件的基础设施。例如 <code>ssh</code> 和 <code>scp</code> 就可以满足需求。</li></ul><h2 id=建立集群>建立集群</h2><p>一般来说，是在一个节点上生成所有证书并且只分发这些<em>必要</em>的文件到其它节点上。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>kubeadm 包含生成下述证书所需的所有必要的密码学工具；在这个例子中，不需要其他加密工具。</div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>下面的例子使用 IPv4 地址，但是你也可以使用 IPv6 地址配置 kubeadm、kubelet 和 etcd。一些 Kubernetes 选项支持双协议栈，但是 etcd 不支持。
关于 Kubernetes 双协议栈支持的更多细节，请参见 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/dual-stack-support/>kubeadm 的双栈支持</a>。</div><ol><li><p>将 kubelet 配置为 etcd 的服务管理器。</p><p><div class="alert alert-info note callout" role=alert><strong>说明：</strong> 你必须在要运行 etcd 的所有主机上执行此操作。</div>由于 etcd 是首先创建的，因此你必须通过创建具有更高优先级的新文件来覆盖
kubeadm 提供的 kubelet 单元文件。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>[Service]
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=
</span></span></span><span style=display:flex><span><span style=color:#b44># 将下面的 &#34;systemd&#34; 替换为你的容器运行时所使用的 cgroup 驱动。
</span></span></span><span style=display:flex><span><span style=color:#b44># kubelet 的默认值为 &#34;cgroupfs&#34;。
</span></span></span><span style=display:flex><span><span style=color:#b44># 如果需要的话，将 &#34;--container-runtime-endpoint &#34; 的值替换为一个不同的容器运行时。
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
</span></span></span><span style=display:flex><span><span style=color:#b44>Restart=always
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div><p>检查 kubelet 的状态以确保其处于运行状态：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>systemctl status kubelet
</span></span></code></pre></div></li></ol><ol start=2><li><p>为 kubeadm 创建配置文件。</p><p>使用以下脚本为每个将要运行 etcd 成员的主机生成一个 kubeadm 配置文件。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#080;font-style:italic># 使用你的主机 IP 替换 HOST0、HOST1 和 HOST2 的 IP 地址</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST0</span><span style=color:#666>=</span>10.0.0.6
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST1</span><span style=color:#666>=</span>10.0.0.7
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST2</span><span style=color:#666>=</span>10.0.0.8
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 使用你的主机名更新 NAME0、NAME1 和 NAME2</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>NAME0</span><span style=color:#666>=</span><span style=color:#b44>&#34;infra0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>NAME1</span><span style=color:#666>=</span><span style=color:#b44>&#34;infra1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>NAME2</span><span style=color:#666>=</span><span style=color:#b44>&#34;infra2&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 创建临时目录来存储将被分发到其它主机上的文件</span>
</span></span><span style=display:flex><span>mkdir -p /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#b8860b>HOSTS</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAMES</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAME0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAME1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAME2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span>!HOSTS[@]<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOSTS</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAME</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAMES</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>---
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta3&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: InitConfiguration
</span></span></span><span style=display:flex><span><span style=color:#b44>nodeRegistration:
</span></span></span><span style=display:flex><span><span style=color:#b44>    name: ${NAME}
</span></span></span><span style=display:flex><span><span style=color:#b44>localAPIEndpoint:
</span></span></span><span style=display:flex><span><span style=color:#b44>    advertiseAddress: ${HOST}
</span></span></span><span style=display:flex><span><span style=color:#b44>---
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta3&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: ClusterConfiguration
</span></span></span><span style=display:flex><span><span style=color:#b44>etcd:
</span></span></span><span style=display:flex><span><span style=color:#b44>    local:
</span></span></span><span style=display:flex><span><span style=color:#b44>        serverCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        peerCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        extraArgs:
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster-state: new
</span></span></span><span style=display:flex><span><span style=color:#b44>            name: ${NAME}
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            advertise-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-advertise-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div></li></ol><ol start=3><li><p>生成证书颁发机构</p><p>如果你已经拥有 CA，那么唯一的操作是复制 CA 的 <code>crt</code> 和 <code>key</code> 文件到
<code>etc/kubernetes/pki/etcd/ca.crt</code> 和 <code>/etc/kubernetes/pki/etcd/ca.key</code>。
复制完这些文件后继续下一步，“为每个成员创建证书”。</p><p>如果你还没有 CA，则在 <code>$HOST0</code>（你为 kubeadm 生成配置文件的位置）上运行此命令。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubeadm init phase certs etcd-ca
</span></span></code></pre></div><p>这一操作创建如下两个文件：</p><ul><li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li><li><code>/etc/kubernetes/pki/etcd/ca.key</code></li></ul></li></ol><ol start=4><li><p>为每个成员创建证书</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 清理不可重复使用的证书</span>
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 不需要移动 certs 因为它们是给 HOST0 使用的</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 清理不应从此主机复制的证书</span>
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span></code></pre></div></li></ol><ol start=5><li><p>复制证书和 kubeadm 配置</p><p>证书已生成，现在必须将它们移动到对应的主机。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>scp -r /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>/* <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>:
</span></span><span style=display:flex><span>ssh <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>USER@HOST $ sudo -Es
</span></span><span style=display:flex><span>root@HOST $ chown -R root:root pki
</span></span><span style=display:flex><span>root@HOST $ mv pki /etc/kubernetes/
</span></span></code></pre></div></li></ol><ol start=6><li><p>确保已经所有预期的文件都存在</p><p><code>$HOST0</code> 所需文件的完整列表如下：</p><pre tabindex=0><code class=language-none data-lang=none>/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>在 <code>$HOST1</code> 上：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:navy;font-weight:700>$</span>HOME
</span></span><span style=display:flex><span><span style=color:#888>└── kubeadmcfg.yaml
</span></span></span><span style=display:flex><span><span style=color:#888>---
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki
</span></span></span><span style=display:flex><span><span style=color:#888>├── apiserver-etcd-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>├── apiserver-etcd-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>└── etcd
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── healthcheck-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── healthcheck-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── peer.crt
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── peer.key
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── server.crt
</span></span></span><span style=display:flex><span><span style=color:#888>    └── server.key
</span></span></span></code></pre></div><p>在 <code>$HOST2</code> 上：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:navy;font-weight:700>$</span>HOME
</span></span><span style=display:flex><span><span style=color:#888>└── kubeadmcfg.yaml
</span></span></span><span style=display:flex><span><span style=color:#888>---
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki
</span></span></span><span style=display:flex><span><span style=color:#888>├── apiserver-etcd-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>├── apiserver-etcd-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>└── etcd
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── healthcheck-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── healthcheck-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── peer.crt
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── peer.key
</span></span></span><span style=display:flex><span><span style=color:#888>    ├── server.crt
</span></span></span><span style=display:flex><span><span style=color:#888>    └── server.key
</span></span></span></code></pre></div></li></ol><ol start=7><li><p>创建静态 Pod 清单</p><p>既然证书和配置已经就绪，是时候去创建清单了。
在每台主机上运行 <code>kubeadm</code> 命令来生成 etcd 使用的静态清单。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span> root@HOST0 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span> root@HOST1 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span> root@HOST2 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span></code></pre></div></li></ol><ol start=8><li><p>可选：检查集群运行状况</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker run --rm -it <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--net host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-v /etc/kubernetes:/etc/kubernetes registry.k8s.io/etcd:<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCD_TAG</span><span style=color:#b68;font-weight:700>}</span> etcdctl <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--cert /etc/kubernetes/pki/etcd/peer.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--key /etc/kubernetes/pki/etcd/peer.key <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--cacert /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--endpoints https://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>:2379 endpoint health --cluster
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST0 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 16.283339ms
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST1 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 19.44402ms
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST2 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 35.926451ms
</span></span></code></pre></div><ul><li>将 <code>${ETCD_TAG}</code> 设置为你的 etcd 镜像的版本标签，例如 <code>3.4.3-0</code>。
要查看 kubeadm 使用的 etcd 镜像和标签，请执行
<code>kubeadm config images list --kubernetes-version ${K8S_VERSION}</code>，
例如，其中的 <code>${K8S_VERSION}</code> 可以是 <code>v1.17.0</code>。</li><li>将 <code>${HOST0}</code> 设置为要测试的主机的 IP 地址。</li></ul></li></ol><h2 id=接下来>接下来</h2><p>一旦拥有了一个正常工作的 3 成员的 etcd 集群，你就可以基于
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/high-availability/>使用 kubeadm 外部 etcd 的方法</a>，
继续部署一个高可用的控制平面。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-07709e71de6b4ac2573041c31213dbeb>2.2.1.8 - 使用 kubeadm 配置集群中的每个 kubelet</h1><div class="alert alert-secondary callout note" role=alert><strong>说明：</strong> 自 1.24 版起，Dockershim 已从 Kubernetes 项目中移除。阅读 <a href=/zh-cn/dockershim>Dockershim 移除的常见问题</a>了解更多详情。</div><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.11 [stable]</code></div><p>kubeadm CLI 工具的生命周期与 <a href=/zh-cn/docs/reference/command-line-tools-reference/kubelet>kubelet</a>
解耦；kubelet 是一个守护程序，在 Kubernetes 集群中的每个节点上运行。
当 Kubernetes 初始化或升级时，kubeadm CLI 工具由用户执行，而 kubelet 始终在后台运行。</p><p>由于kubelet是守护程序，因此需要通过某种初始化系统或服务管理器进行维护。
当使用 DEB 或 RPM 安装 kubelet 时，配置系统去管理 kubelet。
你可以改用其他服务管理器，但需要手动地配置。</p><p>集群中涉及的所有 kubelet 的一些配置细节都必须相同，
而其他配置方面则需要基于每个 kubelet 进行设置，以适应给定机器的不同特性（例如操作系统、存储和网络）。
你可以手动地管理 kubelet 的配置，但是 kubeadm 现在提供一种 <code>KubeletConfiguration</code> API
类型用于<a href=#configure-kubelets-using-kubeadm>集中管理 kubelet 的配置</a>。</p><h2 id=kubelet-configuration-patterns>Kubelet 配置模式</h2><p>以下各节讲述了通过使用 kubeadm 简化 kubelet 配置模式，而不是在每个节点上手动地管理 kubelet 配置。</p><h3 id=propagating-cluster-level-configuration-to-each-kubelet>将集群级配置传播到每个 kubelet 中</h3><p>你可以通过 <code>kubeadm init</code> 和 <code>kubeadm join</code> 命令为 kubelet 提供默认值。
有趣的示例包括使用其他容器运行时或通过服务器设置不同的默认子网。</p><p>如果你想使用子网 <code>10.96.0.0/12</code> 作为服务的默认网段，你可以给 kubeadm 传递 <code>--service-cidr</code> 参数：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init --service-cidr 10.96.0.0/12
</span></span></code></pre></div><p>现在，可以从该子网分配服务的虚拟 IP。
你还需要通过 kubelet 使用 <code>--cluster-dns</code> 标志设置 DNS 地址。
在集群中的每个管理器和节点上的 kubelet 的设置需要相同。
kubelet 提供了一个版本化的结构化 API 对象，该对象可以配置 kubelet
中的大多数参数，并将此配置推送到集群中正在运行的每个 kubelet 上。
此对象被称为 <a href=/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>。
<code>KubeletConfiguration</code> 允许用户指定标志，例如用骆峰值代表集群的 DNS IP 地址，如下所示：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusterDNS</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:#666>10.96.0.10</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>有关 <code>KubeletConfiguration</code> 的更多详细信息，请参阅<a href=#configure-kubelets-using-kubeadm>本节</a>。</p><h3 id=providing-instance-specific-configuration-details>提供特定于某实例的配置细节</h3><p>由于硬件、操作系统、网络或者其他主机特定参数的差异。某些主机需要特定的 kubelet 配置。
以下列表提供了一些示例。</p><ul><li><p>由 kubelet 配置标志 <code>--resolv-conf</code> 指定的 DNS 解析文件的路径在操作系统之间可能有所不同，
它取决于你是否使用 <code>systemd-resolved</code>。
如果此路径错误，则在其 kubelet 配置错误的节点上 DNS 解析也将失败。</p></li><li><p>除非你使用云驱动，否则默认情况下 Node API 对象的 <code>.metadata.name</code> 会被设置为计算机的主机名。
如果你需要指定一个与机器的主机名不同的节点名称，你可以使用 <code>--hostname-override</code> 标志覆盖默认值。</p></li><li><p>当前，kubelet 无法自动检测容器运行时使用的 cgroup 驱动程序，
但是值 <code>--cgroup-driver</code> 必须与容器运行时使用的 cgroup 驱动程序匹配，以确保 kubelet 的健康运行状况。</p></li><li><p>要指定容器运行时，你必须用 <code>--container-runtime-endpoint=&lt;path></code> 标志来指定端点。</p></li></ul><p>应用此类特定于实例的配置的推荐方法是使用
<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/control-plane-flags#patches><code>KubeletConfiguration</code> 补丁</a>。</p><h2 id=configure-kubelets-using-kubeadm>使用 kubeadm 配置 kubelet</h2><p>如果自定义的 <a href=/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a> API 对象使用像 <code>kubeadm ... --config some-config-file.yaml</code> 这样的配置文件进行传递，则可以配置 kubeadm 启动的 kubelet。</p><p>通过调用 <code>kubeadm config print init-defaults --component-configs KubeletConfiguration</code>，
你可以看到此结构中的所有默认值。</p><p>也可以在基础 <code>KubeletConfiguration</code> 上应用实例特定的补丁。
阅读<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/control-plane-flags#customizing-the-kubelet>自定义 kubelet</a>
来获取有关各个字段的更多信息。</p><h3 id=workflow-when-using-kubeadm-init>使用 <code>kubeadm init</code> 时的工作流程</h3><p>当调用 <code>kubeadm init</code> 时，kubelet 的配置会被写入磁盘 <code>/var/lib/kubelet/config.yaml</code>，
并上传到集群 <code>kube-system</code> 命名空间的 <code>kubelet-config</code> ConfigMap。
kubelet 配置信息也被写入 <code>/etc/kubernetes/kubelet.conf</code>，其中包含集群内所有 kubelet 的基线配置。
此配置文件指向允许 kubelet 与 API 服务器通信的客户端证书。
这解决了<a href=#propagating-cluster-level-configuration-to-each-kubelet>将集群级配置传播到每个 kubelet</a> 的需求。</p><p>针对<a href=#providing-instance-specific-configuration-details>为特定实例提供配置细节</a>的第二种模式，
kubeadm 的解决方法是将环境文件写入 <code>/var/lib/kubelet/kubeadm-flags.env</code>，其中包含了一个标志列表，
当 kubelet 启动时，该标志列表会传递给 kubelet 标志在文件中的显示方式如下：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>KUBELET_KUBEADM_ARGS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--flag1=value1 --flag2=value2 ...&#34;</span>
</span></span></code></pre></div><p>除了启动 kubelet 时所使用的标志外，该文件还包含动态参数，例如 cgroup 驱动程序以及是否使用其他容器运行时套接字（<code>--cri-socket</code>）。</p><p>将这两个文件编组到磁盘后，如果使用 systemd，则 kubeadm 尝试运行以下两个命令：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>如果重新加载和重新启动成功，则正常的 <code>kubeadm init</code> 工作流程将继续。</p><h3 id=workflow-when-using-kubeadm-join>使用 <code>kubeadm join</code> 时的工作流程</h3><p>当运行 <code>kubeadm join</code> 时，kubeadm 使用 Bootstrap Token 证书执行 TLS 引导，该引导会获取一份证书，
该证书需要下载 <code>kubelet-config</code> ConfigMap 并把它写入 <code>/var/lib/kubelet/config.yaml</code> 中。
动态环境文件的生成方式恰好与 <code>kubeadm init</code> 完全相同。</p><p>接下来，<code>kubeadm</code> 运行以下两个命令将新配置加载到 kubelet 中：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>在 kubelet 加载新配置后，kubeadm 将写入 <code>/etc/kubernetes/bootstrap-kubelet.conf</code> KubeConfig 文件中，
该文件包含 CA 证书和引导程序令牌。
kubelet 使用这些证书执行 TLS 引导程序并获取唯一的凭据，该凭据被存储在 <code>/etc/kubernetes/kubelet.conf</code> 中。</p><p>当 <code>/etc/kubernetes/kubelet.conf</code> 文件被写入后，kubelet 就完成了 TLS 引导过程。
Kubeadm 在完成 TLS 引导过程后将删除 <code>/etc/kubernetes/bootstrap-kubelet.conf</code> 文件。</p><h2 id=the-kubelet-drop-in-file-for-systemd>kubelet 的 systemd drop-in 文件</h2><p><code>kubeadm</code> 中附带了有关系统如何运行 kubelet 的 systemd 配置文件。
请注意 kubeadm CLI 命令不会修改此文件。</p><p>通过 <code>kubeadm</code> <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf>DEB 包</a>
或者 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf>RPM 包</a>
安装的配置文件被写入 <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> 并由 systemd 使用。
它对原来的 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service>RPM 版本 <code>kubelet.service</code></a>
或者 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service>DEB 版本 <code>kubelet.service</code></a>
作了增强：</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>下面的内容只是一个例子。如果你不想使用包管理器，
请遵循<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#k8s-install-2>没有包管理器</a>)
章节的指南。</div><pre tabindex=0><code class=language-none data-lang=none>[Service]
Environment=&#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&#34;
Environment=&#34;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&#34;
# 这是 &#34;kubeadm init&#34; 和 &#34;kubeadm join&#34; 运行时生成的文件，
# 动态地填充 KUBELET_KUBEADM_ARGS 变量
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# 这是一个文件，用户在不得已下可以将其用作替代 kubelet args。
# 用户最好使用 .NodeRegistration.KubeletExtraArgs 对象在配置文件中替代。
# KUBELET_EXTRA_ARGS 应该从此文件中获取。
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
</code></pre><p>此文件指定由 kubeadm 为 kubelet 管理的所有文件的默认位置。</p><ul><li>用于 TLS 引导程序的 KubeConfig 文件为 <code>/etc/kubernetes/bootstrap-kubelet.conf</code>，
但仅当 <code>/etc/kubernetes/kubelet.conf</code> 不存在时才能使用。</li><li>具有唯一 kubelet 标识的 KubeConfig 文件为 <code>/etc/kubernetes/kubelet.conf</code>。</li><li>包含 kubelet 的组件配置的文件为 <code>/var/lib/kubelet/config.yaml</code>。</li><li>包含的动态环境的文件 <code>KUBELET_KUBEADM_ARGS</code> 是来源于 <code>/var/lib/kubelet/kubeadm-flags.env</code>。</li><li>包含用户指定标志替代的文件 <code>KUBELET_EXTRA_ARGS</code> 是来源于
<code>/etc/default/kubelet</code>（对于 DEB），或者 <code>/etc/sysconfig/kubelet</code>（对于 RPM）。
<code>KUBELET_EXTRA_ARGS</code> 在标志链中排在最后，并且在设置冲突时具有最高优先级。</li></ul><h2 id=kubernetes-binaries-and-package-contents>Kubernetes 可执行文件和软件包内容</h2><p>Kubernetes 版本对应的 DEB 和 RPM 软件包是：</p><table><thead><tr><th>软件包名称</th><th>描述</th></tr></thead><tbody><tr><td><code>kubeadm</code></td><td>给 kubelet 安装 <code>/usr/bin/kubeadm</code> CLI 工具和 <a href=#the-kubelet-drop-in-file-for-systemd>kubelet 的 systemd drop-in 文件</a>。</td></tr><tr><td><code>kubelet</code></td><td>安装 <code>/usr/bin/kubelet</code> 可执行文件。</td></tr><tr><td><code>kubectl</code></td><td>安装 <code>/usr/bin/kubectl</code> 可执行文件。</td></tr><tr><td><code>cri-tools</code></td><td>从 <a href=https://github.com/kubernetes-sigs/cri-tools>cri-tools git 仓库</a>中安装 <code>/usr/bin/crictl</code> 可执行文件。</td></tr><tr><td><code>kubernetes-cni</code></td><td>从 <a href=https://github.com/containernetworking/plugins>plugins git 仓库</a>中安装 <code>/opt/cni/bin</code> 可执行文件。</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-df2f3f20d404ebe2b03fcda1fcee50e7>2.2.1.9 - 使用 kubeadm 支持双协议栈</h1><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.23 [stable]</code></div><p>你的集群包含<a href=/zh-cn/docs/concepts/services-networking/dual-stack/>双协议栈</a>组网支持，
这意味着集群网络允许你在两种地址族间任选其一。在集群中，控制面可以为同一个
<a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> 或者 <a class=glossary-tooltip title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>
同时赋予 IPv4 和 IPv6 地址。</p><h2 id=准备开始>准备开始</h2><p>你需要已经遵从<a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>安装 kubeadm</a>
中所给的步骤安装了 <a class=glossary-tooltip title='用来快速安装 Kubernetes 并搭建安全稳定的集群的工具。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/tools/kubeadm/ target=_blank aria-label=kubeadm>kubeadm</a> 工具。</p><p>针对你要作为<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>使用的每台服务器，
确保其允许 IPv6 转发。在 Linux 节点上，你可以通过以 root 用户在每台服务器上运行
<code>sysctl -w net.ipv6.conf.all.forwarding=1</code> 来完成设置。</p><p>你需要一个可以使用的 IPv4 和 IPv6 地址范围。集群操作人员通常为 IPv4 使用
私有地址范围。对于 IPv6，集群操作人员通常会基于分配给该操作人员的地址范围，
从 <code>2000::/3</code> 中选择一个全局的单播地址块。你不需要将集群的 IP 地址范围路由
到公众互联网。</p><p>所分配的 IP 地址数量应该与你计划运行的 Pod 和 Service 的数量相适应。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>如果你在使用 <code>kubeadm upgrade</code> 命令升级现有的集群，<code>kubeadm</code> 不允许更改 Pod
的 IP 地址范围（“集群 CIDR”），也不允许更改集群的服务地址范围（“Service CIDR”）。</div><h3 id=create-a-dual-stack-cluster>创建双协议栈集群</h3><p>要使用 <code>kubeadm init</code> 创建一个双协议栈集群，你可以传递与下面的例子类似的命令行参数：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># 这里的地址范围仅作示例使用</span>
</span></span><span style=display:flex><span>kubeadm init --pod-network-cidr<span style=color:#666>=</span>10.244.0.0/16,2001:db8:42:0::/56 --service-cidr<span style=color:#666>=</span>10.96.0.0/16,2001:db8:42:1::/112
</span></span></code></pre></div><p>为了更便于理解，参看下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>，
该文件用于双协议栈控制面的主控制节点。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>networking</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.244.0.0</span>/16,2001:db8:42:0::/56<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.96.0.0</span>/16,2001:db8:42:1::/112<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>localAPIEndpoint</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>advertiseAddress</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10.100.0.1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bindPort</span>:<span style=color:#bbb> </span><span style=color:#666>6443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node-ip</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.2</span>,fd00:1:2:3::2<span style=color:#bbb>
</span></span></span></code></pre></div><p>InitConfiguration 中的 <code>advertiseAddress</code> 给出 API 服务器将公告自身要监听的
IP 地址。<code>advertiseAddress</code> 的取值与 <code>kubeadm init</code> 的标志
<code>--apiserver-advertise-address</code> 的取值相同。</p><p>运行 kubeadm 来实例化双协议栈控制面节点：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubeadm init --config<span style=color:#666>=</span>kubeadm-config.yaml
</span></span></code></pre></div><p>kube-controller-manager 标志 <code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code>
是使用默认值来设置的。参见<a href=/zh-cn/docs/concepts/services-networking/dual-stack#configure-ipv4-ipv6-dual-stack>配置 IPv4/IPv6 双协议栈</a>。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>标志 <code>--apiserver-advertise-address</code> 不支持双协议栈。</div><h3 id=join-a-node-to-dual-stack-cluster>向双协议栈集群添加节点</h3><p>在添加节点之前，请确保该节点具有 IPv6 可路由的网络接口并且启用了 IPv6 转发。</p><p>下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>
示例用于向集群中添加工作节点。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>discovery</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bootstrapToken</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiServerEndpoint</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.1</span>:<span style=color:#666>6443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>token</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;clvldh.vjjwg16ucnhp94qr&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caCertHashes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># 请更改上面的认证信息，使之与你的集群中实际使用的令牌和 CA 证书匹配</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node-ip</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.3</span>,fd00:1:2:3::3<span style=color:#bbb>
</span></span></span></code></pre></div><p>下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>
示例用于向集群中添加另一个控制面节点。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controlPlane</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>localAPIEndpoint</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>advertiseAddress</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10.100.0.2&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bindPort</span>:<span style=color:#bbb> </span><span style=color:#666>6443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>discovery</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bootstrapToken</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiServerEndpoint</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.1</span>:<span style=color:#666>6443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>token</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;clvldh.vjjwg16ucnhp94qr&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caCertHashes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># 请更改上面的认证信息，使之与你的集群中实际使用的令牌和 CA 证书匹配</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node-ip</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.4</span>,fd00:1:2:3::4<span style=color:#bbb>
</span></span></span></code></pre></div><p>JoinConfiguration.controlPlane 中的 <code>advertiseAddress</code> 设定 API 服务器将公告自身要监听的
IP 地址。<code>advertiseAddress</code> 的取值与 <code>kubeadm join</code> 的标志
<code>--apiserver-advertise-address</code> 的取值相同。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubeadm join --config<span style=color:#666>=</span>kubeadm-config.yaml
</span></span></code></pre></div><h3 id=create-a-single-stack-cluster>创建单协议栈集群</h3><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>双协议栈支持并不意味着你需要使用双协议栈来寻址。
你可以部署一个启用了双协议栈联网特性的单协议栈集群。</div><p>为了更便于理解，参看下面的名为 <code>kubeadm-config.yaml</code> 的 kubeadm
<a href=/zh-cn/docs/reference/config-api/kubeadm-config.v1beta3/>配置文件</a>示例，
该文件用于单协议栈控制面节点。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>networking</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.244.0.0</span>/16<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.96.0.0</span>/16<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=接下来>接下来</h2><ul><li><a href=/zh-cn/docs/tasks/network/validate-dual-stack>验证 IPv4/IPv6 双协议栈</a>联网</li><li>阅读<a href=/zh-cn/docs/concepts/services-networking/dual-stack/>双协议栈</a>集群网络</li><li>进一步了解 kubeadm <a href=/docs/reference/config-api/kubeadm-config.v1beta3/>配置格式</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-478acca1934b6d89a0bc00fb25bfe5b6>2.2.2 - 使用 kOps 安装 Kubernetes</h1><p>本篇快速入门介绍了如何在 AWS 上轻松安装 Kubernetes 集群。
本篇使用了一个名为 <a href=https://github.com/kubernetes/kops><code>kOps</code></a> 的工具。</p><p><code>kOps</code> 是一个自动化的制备系统：</p><ul><li>全自动安装流程</li><li>使用 DNS 识别集群</li><li>自我修复：一切都在自动扩缩组中运行</li><li>支持多种操作系统（Amazon Linux、Debian、Flatcar、RHEL、Rocky 和 Ubuntu），
参考 <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/images.md>images.md</a>。</li><li>支持高可用，参考 <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/high_availability.md>high_availability.md</a>。</li><li>可以直接提供或者生成 terraform 清单，参考 <a href=https://github.com/kubernetes/kops/blob/master/docs/terraform.md>terraform.md</a>。</li></ul><h2 id=准备开始>准备开始</h2><ul><li>你必须安装 <a href=/zh-cn/docs/tasks/tools/>kubectl</a>。</li><li>你必须安装<a href=https://github.com/kubernetes/kops#installing>安装</a> <code>kops</code>
到 64 位的（AMD64 和 Intel 64）设备架构上。</li><li>你必须拥有一个 <a href=https://docs.aws.amazon.com/zh_cn/polly/latest/dg/setting-up.html>AWS 账户</a>，
生成 <a href=https://docs.aws.amazon.com/zh_cn/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>IAM 秘钥</a>
并<a href=https://docs.aws.amazon.com/zh_cn/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration>配置</a>
该秘钥。IAM 用户需要<a href=https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user>足够的权限许可</a>。</li></ul><h2 id=creating-a-cluster>创建集群</h2><h3 id=1-5-安装-kops>(1/5) 安装 kops</h3><h4 id=安装>安装</h4><p>从<a href=https://github.com/kubernetes/kops/releases>下载页面</a>下载 kops
（从源代码构建也很方便）：</p><ul class="nav nav-tabs" id=kops-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#kops-installation-0 role=tab aria-controls=kops-installation-0 aria-selected=true>macOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#kops-installation-1 role=tab aria-controls=kops-installation-1>Linux</a></li></ul><div class=tab-content id=kops-installation><div id=kops-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=kops-installation-0><p><p>使用下面的命令下载最新发布版本：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-darwin-amd64
</span></span></code></pre></div><p>要下载特定版本，使用特定的 kops 版本替换下面命令中的部分：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</span></span></code></pre></div><p>例如，要下载 kops v1.20.0，输入：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-darwin-amd64
</span></span></code></pre></div><p>令 kops 二进制文件可执行：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>chmod +x kops-darwin-amd64
</span></span></code></pre></div><p>将 kops 二进制文件移到你的 PATH 下：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo mv kops-darwin-amd64 /usr/local/bin/kops
</span></span></code></pre></div><p>你也可以使用 <a href=https://brew.sh/>Homebrew</a> 安装 kops：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</span></span></code></pre></div></div><div id=kops-installation-1 class=tab-pane role=tabpanel aria-labelledby=kops-installation-1><p><p>使用命令下载最新发布版本：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-linux-amd64
</span></span></code></pre></div><p>要下载 kops 的特定版本，用特定的 kops 版本替换下面命令中的部分：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</span></span></code></pre></div><p>例如，要下载 kops v1.20 版本，输入：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64
</span></span></code></pre></div><p>令 kops 二进制文件可执行：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>chmod +x kops-linux-amd64
</span></span></code></pre></div><p>将 kops 二进制文件移到 PATH 下：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo mv kops-linux-amd64 /usr/local/bin/kops
</span></span></code></pre></div><p>你也可以使用 <a href=https://docs.brew.sh/Homebrew-on-Linux>Homebrew</a> 来安装 kops。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</span></span></code></pre></div></div></div><h3 id=2-5-为你的集群创建一个-route53-域名>(2/5) 为你的集群创建一个 route53 域名</h3><p>kops 在集群内部和外部都使用 DNS 进行发现操作，这样你可以从客户端访问
kubernetes API 服务器。</p><p>kops 对集群名称有明显的要求：它应该是有效的 DNS 名称。这样一来，你就不会再使集群混乱，
可以与同事明确共享集群，并且无需依赖记住 IP 地址即可访问集群。</p><p>你可以，或许应该使用子域名来划分集群。作为示例，我们将使用域名 <code>useast1.dev.example.com</code>。
这样，API 服务器端点域名将为 <code>api.useast1.dev.example.com</code>。</p><p>Route53 托管区域可以服务子域名。你的托管区域可能是 <code>useast1.dev.example.com</code>，还有 <code>dev.example.com</code> 甚至 <code>example.com</code>。
kops 可以与以上任何一种配合使用，因此通常你出于组织原因选择不同的托管区域。
例如，允许你在 <code>dev.example.com</code> 下创建记录，但不能在 <code>example.com</code> 下创建记录。</p><p>假设你使用 <code>dev.example.com</code> 作为托管区域。你可以使用
<a href=https://docs.aws.amazon.com/zh_cn/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html>正常流程</a>
或者使用诸如 <code>aws route53 create-hosted-zone --name dev.example.com --caller-reference 1</code>
之类的命令来创建该托管区域。</p><p>然后，你必须在父域名中设置你的 DNS 记录，以便该域名中的记录可以被解析。
在这里，你将在 <code>example.com</code> 中为 <code>dev</code> 创建 DNS 记录。
如果它是根域名，则可以在域名注册机构配置 DNS 记录。
例如，你需要在购买 <code>example.com</code> 的地方配置 <code>example.com</code>。</p><p>检查你的 route53 域已经被正确设置（这是导致问题的最常见原因！）。
如果你安装了 dig 工具，则可以通过运行以下步骤再次检查集群是否配置正确：</p><p><code>dig NS dev.example.com</code></p><p>你应该看到 Route53 分配了你的托管区域的 4 条 DNS 记录。</p><h3 id=3-5-创建一个-s3-存储桶来存储集群状态>(3/5) 创建一个 S3 存储桶来存储集群状态</h3><p>kops 使你即使在安装后也可以管理集群。为此，它必须跟踪已创建的集群及其配置、所使用的密钥等。
此信息存储在 S3 存储桶中。S3 权限用于控制对存储桶的访问。</p><p>多个集群可以使用同一 S3 存储桶，并且你可以在管理同一集群的同事之间共享一个
S3 存储桶 - 这比传递 kubecfg 文件容易得多。
但是有权访问 S3 存储桶的任何人都将拥有对所有集群的管理访问权限，
因此你不想在运营团队之外共享它。</p><p>因此，通常每个运维团队都有一个 S3 存储桶（而且名称通常对应于上面托管区域的名称！）</p><p>在我们的示例中，我们选择 <code>dev.example.com</code> 作为托管区域，因此我们选择
<code>clusters.dev.example.com</code> 作为 S3 存储桶名称。</p><ul><li>导出 <code>AWS_PROFILE</code> 文件（如果你需要选择一个配置文件用来使 AWS CLI 正常工作）</li><li>使用 <code>aws s3 mb s3://clusters.dev.example.com</code> 创建 S3 存储桶</li><li>你可以进行 <code>export KOPS_STATE_STORE=s3://clusters.dev.example.com</code> 操作，
然后 kops 将默认使用此位置。
我们建议将其放入你的 bash profile 文件或类似文件中。</li></ul><h3 id=4-5-建立你的集群配置>(4/5) 建立你的集群配置</h3><p>运行 <code>kops create cluster</code> 以创建你的集群配置：</p><p><code>kops create cluster --zones=us-east-1c useast1.dev.example.com</code></p><p>kops 将为你的集群创建配置。请注意，它<strong>仅</strong>创建配置，实际上并没有创建云资源。
你将在下一步中使用 <code>kops update cluster</code> 进行创建。
这使你有机会查看配置或进行更改。</p><p>它打印出可用于进一步探索的命令：</p><ul><li>使用以下命令列出集群：<code>kops get cluster</code></li><li>使用以下命令编辑该集群：<code>kops edit cluster useast1.dev.example.com</code></li><li>使用以下命令编辑你的节点实例组：<code>kops edit ig --name = useast1.dev.example.com nodes</code></li><li>使用以下命令编辑你的主实例组：<code>kops edit ig --name = useast1.dev.example.com master-us-east-1c</code></li></ul><p>如果这是你第一次使用 kops，请花几分钟尝试一下！实例组是一组实例，将被注册为 Kubernetes 节点。
在 AWS 上，这是通过 auto-scaling-groups 实现的。你可以有多个实例组。
例如，你可能想要混合了 Spot 实例和按需实例的节点，或者混合了 GPU 实例和非 GPU 实例的节点。</p><h3 id=5-5-在-aws-中创建集群>(5/5) 在 AWS 中创建集群</h3><p>运行 <code>kops update cluster</code> 以在 AWS 中创建集群：</p><p><code>kops update cluster useast1.dev.example.com --yes</code></p><p>这需要几秒钟的时间才能运行，但实际上集群可能需要几分钟才能准备就绪。
每当更改集群配置时，都会使用 <code>kops update cluster</code> 工具。
它将在集群中应用你对配置进行的更改，根据需要重新配置 AWS 或者 Kubernetes。</p><p>例如，在你运行 <code>kops edit ig nodes</code> 之后，然后运行 <code>kops update cluster --yes</code>
应用你的配置，有时你还必须运行 <code>kops rolling-update cluster</code> 立即回滚更新配置。</p><p>如果没有 <code>--yes</code> 参数，<code>kops update cluster</code> 操作将向你显示其操作的预览效果。这对于生产集群很方便！</p><h3 id=探索其他附加组件>探索其他附加组件</h3><p>请参阅<a href=/zh-cn/docs/concepts/cluster-administration/addons/>附加组件列表</a>探索其他附加组件，
包括用于 Kubernetes 集群的日志记录、监视、网络策略、可视化和控制的工具。</p><h2 id=cleanup>清理</h2><ul><li>删除集群：<code>kops delete cluster useast1.dev.example.com --yes</code></li></ul><h2 id=接下来>接下来</h2><ul><li>了解有关 Kubernetes 的<a href=/zh-cn/docs/concepts/>概念</a>和
<a href=/zh-cn/docs/reference/kubectl/><code>kubectl</code></a> 的更多信息。</li><li>参阅 <code>kOps</code> <a href=https://kops.sigs.k8s.io/>进阶用法</a> 获取教程、最佳实践和进阶配置选项。</li><li>通过 Slack：<a href=https://github.com/kubernetes/kops#other-ways-to-communicate-with-the-contributors>社区讨论</a>
参与 <code>kOps</code> 社区讨论。</li><li>通过解决或提出一个 <a href=https://github.com/kubernetes/kops/issues>GitHub Issue</a> 来为 <code>kOps</code> 做贡献。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f8b4964187fe973644e06ee629eff1de>2.2.3 - 使用 Kubespray 安装 Kubernetes</h1><p>此快速入门有助于使用 <a href=https://github.com/kubernetes-sigs/kubespray>Kubespray</a>
安装在 GCE、Azure、OpenStack、AWS、vSphere、Equinix Metal（曾用名 Packet）、Oracle Cloud
Infrastructure（实验性）或 Baremetal 上托管的 Kubernetes 集群。</p><p>Kubespray 是由若干 <a href=https://docs.ansible.com/>Ansible</a> Playbook、
<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md#inventory>清单（inventory）</a>、
制备工具和通用 OS/Kubernetes 集群配置管理任务的领域知识组成的。</p><p>Kubespray 提供：</p><ul><li>高可用性集群</li><li>可组合属性（例如可选择网络插件）</li><li>支持大多数流行的 Linux 发行版<ul><li>Flatcar Container Linux</li><li>Debian Bullseye、Buster、Jessie、Stretch</li><li>Ubuntu 16.04、18.04、20.04、22.04</li><li>CentOS/RHEL 7、8、9</li><li>Fedora 35、36</li><li>Fedora CoreOS</li><li>openSUSE Leap 15.x/Tumbleweed</li><li>Oracle Linux 7、8、9</li><li>Alma Linux 8、9</li><li>Rocky Linux 8、9</li><li>Kylin Linux Advanced Server V10</li><li>Amazon Linux 2</li></ul></li><li>持续集成测试</li></ul><p>要选择最适合你的用例的工具，请阅读
<a href=/zh-cn/docs/reference/setup-tools/kubeadm/>kubeadm</a> 和
<a href=/zh-cn/docs/setup/production-environment/tools/kops/>kops</a>
之间的<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md>这份比较</a>。</p><h2 id=creating-a-cluster>创建集群</h2><h3 id=1-5-满足下层设施要求>（1/5）满足下层设施要求</h3><p>按以下<a href=https://github.com/kubernetes-sigs/kubespray#requirements>要求</a>来配置服务器：</p><ul><li><strong>Kubernetes</strong> 的最低版本要求为 V1.22</li><li><strong>在将运行 Ansible 命令的计算机上安装 Ansible v2.11（或更高版本）、Jinja 2.11（或更高版本）和 python-netaddr</strong></li><li>目标服务器必须<strong>能够访问 Internet</strong> 才能拉取 Docker 镜像。否则，
需要其他配置（<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md>请参见离线环境</a>）</li><li>目标服务器配置为允许 <strong>IPv4 转发</strong></li><li>如果针对 Pod 和 Service 使用 IPv6，则目标服务器配置为允许 <strong>IPv6 转发</strong></li><li><strong>防火墙不是由 kubespray 管理的</strong>。你需要根据需求设置适当的规则策略。为了避免部署过程中出现问题，可以禁用防火墙。</li><li>如果从非 root 用户帐户运行 kubespray，则应在目标服务器中配置正确的特权升级方法并指定
<code>ansible_become</code> 标志或命令参数 <code>--become</code> 或 <code>-b</code></li></ul><p>Kubespray 提供以下实用程序来帮助你设置环境：</p><ul><li>为以下云驱动提供的 <a href=https://www.terraform.io/>Terraform</a> 脚本：<ul><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws>AWS</a></li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/openstack>OpenStack</a></li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/metal>Equinix Metal</a></li></ul></li></ul><h3 id=2-5-编写清单文件>（2/5）编写清单文件</h3><p>设置服务器后，请创建一个
<a href=https://docs.ansible.com/ansible/latest/network/getting_started/first_inventory.html>Ansible 的清单文件</a>。
你可以手动执行此操作，也可以通过动态清单脚本执行此操作。有关更多信息，请参阅
“<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#building-your-own-inventory>建立你自己的清单</a>”。</p><h3 id=3-5-规划集群部署>（3/5）规划集群部署</h3><p>Kubespray 能够自定义部署的许多方面：</p><ul><li>选择部署模式： kubeadm 或非 kubeadm</li><li>CNI（网络）插件</li><li>DNS 配置</li><li>控制平面的选择：本机/可执行文件或容器化</li><li>组件版本</li><li>Calico 路由反射器</li><li>组件运行时选项<ul><li><a class=glossary-tooltip title='Docker 是一种可以提供操作系统级别虚拟化（也称作容器）的软件技术。' data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a></li><li><a class=glossary-tooltip title=强调简单性、健壮性和可移植性的一种容器运行时 data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></li><li><a class=glossary-tooltip title='专用于 Kubernetes 的轻量级容器运行时软件' data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></li></ul></li><li>证书生成方式</li></ul><p>可以修改<a href=https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html>变量文件</a>以进行
Kubespray 定制。
如果你刚刚开始使用 Kubespray，请考虑使用 Kubespray 默认设置来部署你的集群并探索 Kubernetes。</p><h3 id=4-5-部署集群>（4/5）部署集群</h3><p>接下来，部署你的集群：</p><p>使用 <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#starting-custom-deployment>ansible-playbook</a>
进行集群部署。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ansible-playbook -i your/inventory/inventory.ini cluster.yml -b -v <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --private-key<span style=color:#666>=</span>~/.ssh/private_key
</span></span></code></pre></div><p>大型部署（超过 100 个节点）
可能需要<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md>特定的调整</a>，
以获得最佳效果。</p><h3 id=5-5-验证部署>（5/5）验证部署</h3><p>Kubespray 提供了一种使用
<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md>Netchecker</a>
验证 Pod 间连接和 DNS 解析的方法。
Netchecker 确保 netchecker-agents Pod 可以解析 DNS 请求，
并在默认命名空间内对每个请求执行 ping 操作。
这些 Pod 模仿其他工作负载类似的行为，并用作集群运行状况指示器。</p><h2 id=cluster-operations>集群操作</h2><p>Kubespray 提供了其他 Playbook 来管理集群： <strong>scale</strong> 和 <strong>upgrade</strong>。</p><h3 id=scale-your-cluster>扩展集群</h3><p>你可以通过运行 scale playbook 向集群中添加工作节点。有关更多信息，
请参见 “<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#adding-nodes>添加节点</a>”。
你可以通过运行 remove-node playbook 来从集群中删除工作节点。有关更多信息，
请参见 “<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#remove-nodes>删除节点</a>”。</p><h3 id=upgrade-your-cluster>升级集群</h3><p>你可以通过运行 upgrade-cluster Playbook 来升级集群。有关更多信息，请参见
“<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md>升级</a>”。</p><h2 id=cleanup>清理</h2><p>你可以通过 <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml>reset playbook</a>
重置节点并清除所有与 Kubespray 一起安装的组件。</p><div class="alert alert-warning caution callout" role=alert><strong>注意：</strong><p>运行 reset playbook 时，请确保不要意外地将生产集群作为目标！</div><h2 id=feedback>反馈</h2><ul><li>Slack 频道：<a href=https://kubernetes.slack.com/messages/kubespray/>#kubespray</a>
（你可以在<a href=https://slack.k8s.io/>此处</a>获得邀请）。</li><li><a href=https://github.com/kubernetes-sigs/kubespray/issues>GitHub 问题</a>。</li></ul><h2 id=接下来>接下来</h2><ul><li>查看有关 Kubespray 的
<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/roadmap.md>路线图</a>的计划工作。</li><li>查阅有关 <a href=https://github.com/kubernetes-sigs/kubespray>Kubespray</a> 的更多信息。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d2f55eefe7222b7c637875af9c3ec199>2.3 - Turnkey 云解决方案</h1><p>本页列示 Kubernetes 认证解决方案供应商。
在每一个供应商分页，你可以学习如何安装和设置生产就绪的集群。</p><script>function updateLandscapeSource(e,t){console.log({button:e,shouldUpdateFragment:t});try{if(t)window.location.hash="#"+e.id;else{var n=document.querySelectorAll("#landscape");let t=e.dataset.landscapeTypes,s="https://landscape.cncf.io/card-mode?category="+encodeURIComponent(t)+"&grouping=category&embed=yes";n[0].src=s}}catch(e){console.log({message:"error handling Landscape switch",error:e})}}document.addEventListener("DOMContentLoaded",function(){let t=()=>{if(window.location.hash){let e=document.querySelectorAll(".landscape-trigger"+window.location.hash);e.length==1&&(landscapeSource=e[0],console.log("Updating Landscape source based on fragment:",window.location.hash.substring(1)),updateLandscapeSource(landscapeSource,!1))}};var e,n=document.querySelectorAll(".landscape-trigger");if(n.forEach(e=>{e.onclick=function(){updateLandscapeSource(e,!0)}}),e=document.querySelectorAll(".landscape-trigger.landscape-default"),e.length==1){let t=e[0];updateLandscapeSource(t,!1)}window.addEventListener("hashchange",t,!1),t()})</script><div id=frameHolder><iframe frameborder=0 id=landscape scrolling=no src="https://landscape.cncf.io/card-mode?category=certified-kubernetes-hosted&grouping=category&embed=yes" style=width:1px;min-width:100%></iframe>
<script src=https://landscape.cncf.io/iframeResizer.js></script></div></div><div class=td-content style=page-break-before:always><h1 id=pg-84b6491601d6a2b3da4cd5a105c866ba>3 - 最佳实践</h1></div><div class=td-content><h1 id=pg-c797ee17120176c685455db89ae091a9>3.1 - 大规模集群的注意事项</h1><p>集群是运行 Kubernetes 代理的、
由<a class=glossary-tooltip title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label=控制平面>控制平面</a>管理的一组
<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>（物理机或虚拟机）。
Kubernetes v1.25 单个集群支持的最大节点数为 5000。
更具体地说，Kubernetes 旨在适应满足以下<strong>所有</strong>标准的配置：</p><ul><li>每个节点的 Pod 数量不超过 110</li><li>节点数不超过 5000</li><li>Pod 总数不超过 150000</li><li>容器总数不超过 300000</li></ul><p>你可以通过添加或删除节点来扩展集群。集群扩缩的方式取决于集群的部署方式。</p><h2 id=quota-issues>云供应商资源配额</h2><p>为避免遇到云供应商配额问题，在创建具有大规模节点的集群时，请考虑以下事项：</p><ul><li>请求增加云资源的配额，例如：<ul><li>计算实例</li><li>CPUs</li><li>存储卷</li><li>使用中的 IP 地址</li><li>数据包过滤规则集</li><li>负载均衡数量</li><li>网络子网</li><li>日志流</li></ul></li><li>由于某些云供应商限制了创建新实例的速度，因此通过分批启动新节点来控制集群扩展操作，并在各批之间有一个暂停。</li></ul><h2 id=control-plane-components>控制面组件</h2><p>对于大型集群，你需要一个具有足够计算能力和其他资源的控制平面。</p><p>通常，你将在每个故障区域运行一个或两个控制平面实例，
先垂直缩放这些实例，然后在到达下降点（垂直）后再水平缩放。</p><p>你应该在每个故障区域至少应运行一个实例，以提供容错能力。
Kubernetes 节点不会自动将流量引向相同故障区域中的控制平面端点。
但是，你的云供应商可能有自己的机制来执行此操作。</p><p>例如，使用托管的负载均衡器时，你可以配置负载均衡器发送源自故障区域 <strong>A</strong> 中的 kubelet 和 Pod 的流量，
并将该流量仅定向到也位于区域 <strong>A</strong> 中的控制平面主机。
如果单个控制平面主机或端点故障区域 <strong>A</strong> 脱机，则意味着区域 <strong>A</strong> 中的节点的所有控制平面流量现在都在区域之间发送。
在每个区域中运行多个控制平面主机能降低出现这种结果的可能性。</p><h3 id=etcd-storage>etcd 存储</h3><p>为了提高大规模集群的性能，你可以将事件对象存储在单独的专用 etcd 实例中。</p><p>在创建集群时，你可以（使用自定义工具）：</p><ul><li>启动并配置额外的 etcd 实例</li><li>配置 <a class=glossary-tooltip title='提供 Kubernetes API 服务的控制面组件。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API 服务器'>API 服务器</a>，将它用于存储事件</li></ul><p>有关为大型集群配置和管理 etcd 的详细信息，
请参阅<a href=/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/>为 Kubernetes 运行 etcd 集群</a>
和使用 <a href=/zh-cn/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>kubeadm 创建一个高可用 etcd 集群</a>。</p><h3 id=addon-resources>插件资源</h3><p>Kubernetes <a href=/zh-cn/docs/concepts/configuration/manage-resources-containers/>资源限制</a>
有助于最大程度地减少内存泄漏的影响以及 Pod 和容器可能对其他组件的其他方式的影响。
这些资源限制适用于<a class=glossary-tooltip title='扩展 Kubernetes 功能的资源。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/cluster-administration/addons/ target=_blank aria-label=插件>插件</a>资源，
就像它们适用于应用程序工作负载一样。</p><p>例如，你可以对日志组件设置 CPU 和内存限制</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-cloud-logging<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>fluent/fluentd-kubernetes-daemonset:v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span></code></pre></div><p>插件的默认限制通常基于从中小规模 Kubernetes 集群上运行每个插件的经验收集的数据。
插件在大规模集群上运行时，某些资源消耗常常比其默认限制更多。
如果在不调整这些值的情况下部署了大规模集群，则插件可能会不断被杀死，因为它们不断达到内存限制。
或者，插件可能会运行，但由于 CPU 时间片的限制而导致性能不佳。</p><p>为避免遇到集群插件资源问题，在创建大规模集群时，请考虑以下事项：</p><ul><li>部分垂直扩展插件 —— 总有一个插件副本服务于整个集群或服务于整个故障区域。
对于这些附加组件，请在扩大集群时加大资源请求和资源限制。</li><li>许多水平扩展插件 —— 你可以通过运行更多的 Pod 来增加容量——但是在大规模集群下，
可能还需要稍微提高 CPU 或内存限制。
VerticalPodAutoscaler 可以在 <strong>recommender</strong> 模式下运行，
以提供有关请求和限制的建议数字。</li><li>一些插件在每个节点上运行一个副本，并由 DaemonSet 控制：
例如，节点级日志聚合器。与水平扩展插件的情况类似，
你可能还需要稍微提高 CPU 或内存限制。</li></ul><h2 id=接下来>接下来</h2><p><code>VerticalPodAutoscaler</code> 是一种自定义资源，你可以将其部署到集群中，帮助你管理资源请求和 Pod 的限制。
访问 <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme>Vertical Pod Autoscaler</a>
以了解有关 <code>VerticalPodAutoscaler</code> 的更多信息，
以及如何使用它来扩展集群组件（包括对集群至关重要的插件）的信息。</p><p><a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme>集群自动扩缩器</a>
与许多云供应商集成在一起，帮助你在你的集群中，按照资源需求级别运行正确数量的节点。</p><p><a href=https://github.com/kubernetes/autoscaler/tree/master/addon-resizer#readme>addon resizer</a>
可帮助你在集群规模变化时自动调整插件的大小。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-970615c97499e3651fd3a98e0387cefc>3.2 - 运行于多可用区环境</h1><p>本页描述如何跨多个区（Zone）中运行集群。</p><h2 id=背景>背景</h2><p>Kubernetes 从设计上允许同一个 Kubernetes 集群跨多个失效区来运行，
通常这些区位于某个称作 <em>区域（region）</em> 逻辑分组中。
主要的云提供商都将区域定义为一组失效区的集合（也称作 <em>可用区（Availability Zones）</em>），
能够提供一组一致的功能特性：每个区域内，各个可用区提供相同的 API 和服务。</p><p>典型的云体系结构都会尝试降低某个区中的失效影响到其他区中服务的概率。</p><h2 id=control-plane-behavior>控制面行为</h2><p>所有的<a href=/zh-cn/docs/concepts/overview/components/#control-plane-components>控制面组件</a>
都支持以一组可相互替换的资源池的形式来运行，每个组件都有多个副本。</p><p>当你部署集群控制面时，应将控制面组件的副本跨多个失效区来部署。
如果可用性是一个很重要的指标，应该选择至少三个失效区，并将每个
控制面组件（API 服务器、调度器、etcd、控制器管理器）复制多个副本，
跨至少三个失效区来部署。如果你在运行云控制器管理器，则也应该将
该组件跨所选的三个失效区来部署。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>Kubernetes 并不会为 API 服务器端点提供跨失效区的弹性。
你可以为集群 API 服务器使用多种技术来提升其可用性，包括使用
DNS 轮转、SRV 记录或者带健康检查的第三方负载均衡解决方案等等。</div><h2 id=node-behavior>节点行为</h2><p>Kubernetes 自动为负载资源（如<a class=glossary-tooltip title=管理集群上的多副本应用。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>
或 <a class=glossary-tooltip title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>)）
跨集群中不同节点来部署其 Pods。
这种分布逻辑有助于降低失效带来的影响。</p><p>节点启动时，每个节点上的 kubelet 会向 Kubernetes API 中代表该 kubelet 的 Node 对象
添加 <a class=glossary-tooltip title=用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=标签>标签</a>。
这些标签可能包含<a href=/zh-cn/docs/reference/labels-annotations-taints/#topologykubernetesiozone>区信息</a>。</p><p>如果你的集群跨了多个可用区或者地理区域，你可以使用节点标签，结合
<a href=/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod 拓扑分布约束</a>
来控制如何在你的集群中多个失效域之间分布 Pods。这里的失效域可以是
地理区域、可用区甚至是特定节点。
这些提示信息使得<a class=glossary-tooltip title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=调度器>调度器</a>
能够更好地分布 Pods，以实现更好的可用性，降低因为某种失效给整个工作负载
带来的风险。</p><p>例如，你可以设置一种约束，确保某个 StatefulSet 中的三个副本都运行在
不同的可用区中，只要其他条件允许。你可以通过声明的方式来定义这种约束，
而不需要显式指定每个工作负载使用哪些可用区。</p><h3 id=distributing-nodes-across-zones>跨多个区分布节点</h3><p>Kubernetes 的核心逻辑并不会帮你创建节点，你需要自行完成此操作，或者使用
类似 <a href=https://cluster-api.sigs.k8s.io/>Cluster API</a> 这类工具来替你管理节点。</p><p>使用类似 Cluster API 这类工具，你可以跨多个失效域来定义一组用做你的集群
工作节点的机器，以及当整个区的服务出现中断时如何自动治愈集群的策略。</p><h2 id=为-pods-手动指定区>为 Pods 手动指定区</h2><p>你可以应用<a href=/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector>节点选择算符约束</a>
到你所创建的 Pods 上，或者为 Deployment、StatefulSet 或 Job 这类工作负载资源
中的 Pod 模板设置此类约束。</p><h2 id=跨区的存储访问>跨区的存储访问</h2><p>当创建持久卷时，<code>PersistentVolumeLabel</code>
<a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>
会自动向那些链接到特定区的 PersistentVolume 添加区标签。
<a class=glossary-tooltip title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=调度器>调度器</a>通过其
<code>NoVolumeZoneConflict</code> 断言确保申领给定 PersistentVolume 的 Pods 只会
被调度到该卷所在的可用区。</p><p>你可以为 PersistentVolumeClaim 指定<a class=glossary-tooltip title='StorageClass 是管理员用来描述可用的不同存储类型的一种方法。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/storage/storage-classes/ target=_blank aria-label=StorageClass>StorageClass</a>
以设置该类中的存储可以使用的失效域（区）。
要了解如何配置能够感知失效域或区的 StorageClass，请参阅
<a href=/zh-cn/docs/concepts/storage/storage-classes/#allowed-topologies>可用的拓扑逻辑</a>。</p><h2 id=networking>网络</h2><p>Kubernetes 自身不提供与可用区相关的联网配置。
你可以使用<a href=/zh-cn/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>网络插件</a>
来配置集群的联网，该网络解决方案可能拥有一些与可用区相关的元素。
例如，如果你的云提供商支持 <code>type=LoadBalancer</code> 的 Service，则负载均衡器
可能仅会将请求流量发送到运行在负责处理给定连接的负载均衡器组件所在的区。
请查阅云提供商的文档了解详细信息。</p><p>对于自定义的或本地集群部署，也可以考虑这些因素
<a class=glossary-tooltip title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>
<a class=glossary-tooltip title='Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/services-networking/ingress/ target=_blank aria-label=Ingress>Ingress</a> 的行为，
包括处理不同失效区的方法，在很大程度上取决于你的集群是如何搭建的。</p><h2 id=fault-recovery>失效恢复</h2><p>在搭建集群时，你可能需要考虑当某区域中的所有失效区都同时掉线时，是否以及如何
恢复服务。例如，你是否要求在某个区中至少有一个节点能够运行 Pod？
请确保任何对集群很关键的修复工作都不要指望集群中至少有一个健康节点。
例如：当所有节点都不健康时，你可能需要运行某个修复性的 Job，
该 Job 要设置特定的<a class=glossary-tooltip title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=容忍度>容忍度</a>
以便修复操作能够至少将一个节点恢复为可用状态。</p><p>Kubernetes 对这类问题没有现成的解决方案；不过这也是要考虑的因素之一。</p><h2 id=接下来>接下来</h2><p>要了解调度器如何在集群中放置 Pods 并遵从所配置的约束，可参阅
<a href=/zh-cn/docs/concepts/scheduling-eviction/>调度与驱逐</a>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f89867de1d34943f1524f67a241f5cc9>3.3 - 校验节点设置</h1><nav id=TableOfContents><ul><li><a href=#node-conformance-test>节点一致性测试</a></li><li><a href=#node-prerequisite>节点的前提条件</a></li><li><a href=#running-node-conformance-test>运行节点一致性测试</a></li><li><a href=#running-node-conformance-test-for-other-architectures>针对其他硬件体系结构运行节点一致性测试</a></li><li><a href=#running-selected-test>运行特定的测试</a></li><li><a href=#caveats>注意事项</a></li></ul></nav><h2 id=node-conformance-test>节点一致性测试</h2><p><strong>节点一致性测试</strong> 是一个容器化的测试框架，提供了针对节点的系统验证和功能测试。
测试验证节点是否满足 Kubernetes 的最低要求；通过测试的节点有资格加入 Kubernetes 集群。</p><p>该测试主要检测节点是否满足 Kubernetes 的最低要求，通过检测的节点有资格加入 Kubernetes 集群。</p><h2 id=node-prerequisite>节点的前提条件</h2><p>要运行节点一致性测试，节点必须满足与标准 Kubernetes 节点相同的前提条件。节点至少应安装以下守护程序：</p><ul><li>容器运行时 (Docker)</li><li>Kubelet</li></ul><h2 id=running-node-conformance-test>运行节点一致性测试</h2><p>要运行节点一致性测试，请执行以下步骤：</p><ol><li>得出 kubelet 的 <code>--kubeconfig</code> 的值；例如：<code>--kubeconfig=/var/lib/kubelet/config.yaml</code>。
由于测试框架启动了本地控制平面来测试 kubelet，因此使用 <code>http://localhost:8080</code>
作为API 服务器的 URL。
一些其他的 kubelet 命令行参数可能会被用到：<ul><li><code>--cloud-provider</code>：如果使用 <code>--cloud-provider=gce</code>，需要移除这个参数来运行测试。</li></ul></li></ol><ol start=2><li><p>使用以下命令运行节点一致性测试：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># $CONFIG_DIR 是你 Kubelet 的 pod manifest 路径。</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># $LOG_DIR 是测试的输出路径。</span>
</span></span><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  registry.k8s.io/node-test:0.2
</span></span></code></pre></div></li></ol><h2 id=running-node-conformance-test-for-other-architectures>针对其他硬件体系结构运行节点一致性测试</h2><p>Kubernetes 也为其他硬件体系结构的系统提供了节点一致性测试的 Docker 镜像：</p><table><thead><tr><th>架构</th><th style=text-align:center>镜像</th></tr></thead><tbody><tr><td>amd64</td><td style=text-align:center>node-test-amd64</td></tr><tr><td>arm</td><td style=text-align:center>node-test-arm</td></tr><tr><td>arm64</td><td style=text-align:center>node-test-arm64</td></tr></tbody></table><h2 id=running-selected-test>运行特定的测试</h2><p>要运行特定测试，请使用你希望运行的测试的特定表达式覆盖环境变量 <code>FOCUS</code>。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>FOCUS</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># Only run MirrorPod test</span>
</span></span><span style=display:flex><span>  registry.k8s.io/node-test:0.2
</span></span></code></pre></div><p>要跳过特定的测试，请使用你希望跳过的测试的常规表达式覆盖环境变量 <code>SKIP</code>。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>SKIP</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># 运行除 MirrorPod 测试外的所有一致性测试内容</span>
</span></span><span style=display:flex><span>  registry.k8s.io/node-test:0.2
</span></span></code></pre></div><p>节点一致性测试是<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md>节点端到端测试</a>的容器化版本。</p><p>默认情况下，它会运行所有一致性测试。</p><p>理论上，只要合理地配置容器和挂载所需的卷，就可以运行任何的节点端到端测试用例。但是这里<strong>强烈建议只运行一致性测试</strong>，因为运行非一致性测试需要很多复杂的配置。</p><h2 id=caveats>注意事项</h2><ul><li>测试会在节点上遗留一些 Docker 镜像，包括节点一致性测试本身的镜像和功能测试相关的镜像。</li><li>测试会在节点上遗留一些死的容器。这些容器是在功能测试的过程中创建的。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-92a61cf5b0575aa3500f7665b68127d1>3.4 - 强制实施 Pod 安全性标准</h1><p>本页提供实施 <a href=/zh-cn/docs/concepts/security/pod-security-standards>Pod 安全标准（Pod Security Standards）</a>
时的一些最佳实践。</p><h2 id=使用内置的-pod-安全性准入控制器>使用内置的 Pod 安全性准入控制器</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.25 [stable]</code></div><p><a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/#podsecurity>Pod 安全性准入控制器</a>
尝试替换已被废弃的 PodSecurityPolicies。</p><h3 id=configure-all-cluster-namespaces>配置所有集群名字空间</h3><p>完全未经配置的名字空间应该被视为集群安全模型中的重大缺陷。
我们建议花一些时间来分析在每个名字空间中执行的负载的类型，
并通过引用 Pod 安全性标准来确定每个负载的合适级别。
未设置标签的名字空间应该视为尚未被评估。</p><p>针对所有名字空间中的所有负载都具有相同的安全性需求的场景，
我们提供了一个<a href=/zh-cn/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/#applying-to-all-namespaces>示例</a>
用来展示如何批量应用 Pod 安全性标签。</p><h3 id=拥抱最小特权原则>拥抱最小特权原则</h3><p>在一个理想环境中，每个名字空间中的每个 Pod 都会满足 <code>restricted</code> 策略的需求。
不过，这既不可能也不现实，某些负载会因为合理的原因而需要特权上的提升。</p><ul><li>允许 <code>privileged</code> 负载的名字空间需要建立并实施适当的访问控制机制。</li><li>对于运行在特权宽松的名字空间中的负载，需要维护其独特安全性需求的文档。
如果可能的话，要考虑如何进一步约束这些需求。</li></ul><h3 id=采用多种模式的策略>采用多种模式的策略</h3><p>Pod 安全性标准准入控制器的 <code>audit</code> 和 <code>warn</code> 模式（mode）
能够在不影响现有负载的前提下，让该控制器更方便地收集关于 Pod 的重要的安全信息。</p><p>针对所有名字空间启用这些模式是一种好的实践，将它们设置为你最终打算 <code>enforce</code> 的
<em>期望的</em> 级别和版本。这一阶段中所生成的警告和审计注解信息可以帮助你到达这一状态。
如果你期望负载的作者能够作出变更以便适应期望的级别，可以启用 <code>warn</code> 模式。
如果你希望使用审计日志了监控和驱动变更，以便负载能够适应期望的级别，可以启用 <code>audit</code> 模式。</p><p>当你将 <code>enforce</code> 模式设置为期望的取值时，这些模式在不同的场合下仍然是有用的：</p><ul><li>通过将 <code>warn</code> 设置为 <code>enforce</code> 相同的级别，客户可以在尝试创建无法通过合法检查的 Pod
（或者包含 Pod 模板的资源）时收到警告信息。这些信息会帮助于更新资源使其合规。</li><li>在将 <code>enforce</code> 锁定到特定的非最新版本的名字空间中，将 <code>audit</code> 和 <code>warn</code>
模式设置为 <code>enforce</code> 一样的级别而非 <code>latest</code> 版本，
这样可以方便看到之前版本所允许但当前最佳实践中被禁止的设置。</li></ul><h2 id=third-party-alternatives>第三方替代方案</h2><div class="alert alert-secondary callout third-party-content" role=alert><strong>说明：</strong>
本部分链接到提供 Kubernetes 所需功能的第三方项目。Kubernetes 项目作者不负责这些项目。此页面遵循<a href=https://github.com/cncf/foundation/blob/master/website-guidelines.md target=_blank>CNCF 网站指南</a>，按字母顺序列出项目。要将项目添加到此列表中，请在提交更改之前阅读<a href=/docs/contribute/style/content-guide/#third-party-content>内容指南</a>。</div><p>Kubernetes 生态系统中也有一些其他强制实施安全设置的替代方案处于开发状态中：</p><ul><li><a href=https://github.com/kubewarden>Kubewarden</a>.</li><li><a href=https://kyverno.io/policies/>Kyverno</a>.</li><li><a href=https://github.com/open-policy-agent/gatekeeper>OPA Gatekeeper</a>.</li></ul><p>采用 <em>内置的</em> 方案（例如 PodSecurity 准入控制器）还是第三方工具，
这一决策完全取决于你自己的情况。在评估任何解决方案时，对供应链的信任都是至关重要的。
最终，使用前述方案中的 <em>任何</em> 一种都好过放任自流。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0394f813094b7a35058dffe5b8bacd20>3.5 - PKI 证书和要求</h1><p>Kubernetes 需要 PKI 证书才能进行基于 TLS 的身份验证。如果你是使用
<a href=/zh-cn/docs/reference/setup-tools/kubeadm/>kubeadm</a> 安装的 Kubernetes，
则会自动生成集群所需的证书。你还可以生成自己的证书。
例如，不将私钥存储在 API 服务器上，可以让私钥更加安全。此页面说明了集群必需的证书。</p><h2 id=how-certificates-are-used-by-your-cluster>集群是如何使用证书的</h2><p>Kubernetes 需要 PKI 才能执行以下操作：</p><ul><li>Kubelet 的客户端证书，用于 API 服务器身份验证</li><li>Kubelet <a href=/zh-cn/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/#client-and-serving-certificates>服务端证书</a>，
用于 API 服务器与 Kubelet 的会话</li><li>API 服务器端点的证书</li><li>集群管理员的客户端证书，用于 API 服务器身份认证</li><li>API 服务器的客户端证书，用于和 Kubelet 的会话</li><li>API 服务器的客户端证书，用于和 etcd 的会话</li><li>控制器管理器的客户端证书或 kubeconfig，用于和 API 服务器的会话</li><li>调度器的客户端证书或 kubeconfig，用于和 API 服务器的会话</li><li><a href=/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/>前端代理</a>的客户端及服务端证书</li></ul><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>只有当你运行 kube-proxy
并要支持<a href=/zh-cn/docs/tasks/extend-kubernetes/setup-extension-api-server/>扩展 API 服务器</a>时，
才需要 <code>front-proxy</code> 证书。</div><p>etcd 还实现了双向 TLS 来对客户端和对其他对等节点进行身份验证。</p><h2 id=where-certificates-are-stored>证书存放的位置</h2><p>假如通过 kubeadm 安装 Kubernetes，大多数证书都存储在 <code>/etc/kubernetes/pki</code>。
本文档中的所有路径都是相对于该目录的，但用户账户证书除外，kubeadm 将其放在 <code>/etc/kubernetes</code> 中。</p><h2 id=configure-certificates-manually>手动配置证书</h2><p>如果你不想通过 kubeadm 生成这些必需的证书，你可以使用一个单一的根 CA
来创建这些证书或者直接提供所有证书。
参见<a href=/zh-cn/docs/tasks/administer-cluster/certificates/>证书</a>以进一步了解创建自己的证书机构。
关于管理证书的更多信息，请参见<a href=/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/>使用 kubeadm 进行证书管理</a>。</p><h3 id=single-root-ca>单根 CA</h3><p>你可以创建由管理员控制的单根 CA。该根 CA 可以创建多个中间 CA，并将所有进一步的创建委托给 Kubernetes。</p><p>需要这些 CA：</p><table><thead><tr><th>路径</th><th>默认 CN</th><th>描述</th></tr></thead><tbody><tr><td>ca.crt,key</td><td>kubernetes-ca</td><td>Kubernetes 通用 CA</td></tr><tr><td>etcd/ca.crt,key</td><td>etcd-ca</td><td>与 etcd 相关的所有功能</td></tr><tr><td>front-proxy-ca.crt,key</td><td>kubernetes-front-proxy-ca</td><td>用于<a href=/zh-cn/docs/tasks/extend-kubernetes/configure-aggregation-layer/>前端代理</a></td></tr></tbody></table><p>上面的 CA 之外，还需要获取用于服务账户管理的密钥对，也就是 <code>sa.key</code> 和 <code>sa.pub</code>。</p><p>下面的例子说明了上表中所示的 CA 密钥和证书文件。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-ca.key
</span></span></span></code></pre></div><h3 id=all-certificates>所有的证书</h3><p>如果你不想将 CA 的私钥拷贝至你的集群中，你也可以自己生成全部的证书。</p><p>需要这些证书：</p><table><thead><tr><th>默认 CN</th><th>父级 CA</th><th>O (位于 Subject 中)</th><th>类型</th><th>主机 (SAN)</th></tr></thead><tbody><tr><td>kube-etcd</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-peer</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd-ca</td><td></td><td>client</td><td></td></tr><tr><td>kube-apiserver-etcd-client</td><td>etcd-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>kube-apiserver</td><td>kubernetes-ca</td><td></td><td>server</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>&lt;advertise_IP></code>, <code>[1]</code></td></tr><tr><td>kube-apiserver-kubelet-client</td><td>kubernetes-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>front-proxy-client</td><td>kubernetes-front-proxy-ca</td><td></td><td>client</td><td></td></tr></tbody></table><p>[1]: 用来连接到集群的不同 IP 或 DNS 名
（就像 <a href=/zh-cn/docs/reference/setup-tools/kubeadm/>kubeadm</a> 为负载均衡所使用的固定
IP 或 DNS 名：<code>kubernetes</code>、<code>kubernetes.default</code>、<code>kubernetes.default.svc</code>、
<code>kubernetes.default.svc.cluster</code>、<code>kubernetes.default.svc.cluster.local</code>）。</p><p>其中，<code>kind</code> 对应一种或多种类型的 <a href=https://pkg.go.dev/k8s.io/api/certificates/v1beta1#KeyUsage>x509 密钥用途</a>：</p><table><thead><tr><th>kind</th><th>密钥用途</th></tr></thead><tbody><tr><td>server</td><td>数字签名、密钥加密、服务端认证</td></tr><tr><td>client</td><td>数字签名、密钥加密、客户端认证</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>上面列出的 Hosts/SAN 是推荐的配置方式；如果需要特殊安装，则可以在所有服务器证书上添加其他 SAN。</div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>对于 kubeadm 用户：</p><ul><li>不使用私钥，将证书复制到集群 CA 的方案，在 kubeadm 文档中将这种方案称为外部 CA。</li><li>如果将以上列表与 kubeadm 生成的 PKI 进行比较，你会注意到，如果使用外部 etcd，则不会生成
<code>kube-etcd</code>、<code>kube-etcd-peer</code> 和 <code>kube-etcd-healthcheck-client</code> 证书。</li></ul></div><h3 id=certificate-paths>证书路径</h3><p>证书应放置在建议的路径中（以便 <a href=/zh-cn/docs/reference/setup-tools/kubeadm/>kubeadm</a>
使用）。无论使用什么位置，都应使用给定的参数指定路径。</p><table><thead><tr><th>默认 CN</th><th>建议的密钥路径</th><th>建议的证书路径</th><th>命令</th><th>密钥参数</th><th>证书参数</th></tr></thead><tbody><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>kube-apiserver</td><td></td><td>--etcd-cafile</td></tr><tr><td>kube-apiserver-etcd-client</td><td>apiserver-etcd-client.key</td><td>apiserver-etcd-client.crt</td><td>kube-apiserver</td><td>--etcd-keyfile</td><td>--etcd-certfile</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-apiserver</td><td></td><td>--client-ca-file</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-controller-manager</td><td>--cluster-signing-key-file</td><td>--client-ca-file, --root-ca-file, --cluster-signing-cert-file</td></tr><tr><td>kube-apiserver</td><td>apiserver.key</td><td>apiserver.crt</td><td>kube-apiserver</td><td>--tls-private-key-file</td><td>--tls-cert-file</td></tr><tr><td>kube-apiserver-kubelet-client</td><td>apiserver-kubelet-client.key</td><td>apiserver-kubelet-client.crt</td><td>kube-apiserver</td><td>--kubelet-client-key</td><td>--kubelet-client-certificate</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-apiserver</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-controller-manager</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-client</td><td>front-proxy-client.key</td><td>front-proxy-client.crt</td><td>kube-apiserver</td><td>--proxy-client-key-file</td><td>--proxy-client-cert-file</td></tr><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>etcd</td><td></td><td>--trusted-ca-file, --peer-trusted-ca-file</td></tr><tr><td>kube-etcd</td><td>etcd/server.key</td><td>etcd/server.crt</td><td>etcd</td><td>--key-file</td><td>--cert-file</td></tr><tr><td>kube-etcd-peer</td><td>etcd/peer.key</td><td>etcd/peer.crt</td><td>etcd</td><td>--peer-key-file</td><td>--peer-cert-file</td></tr><tr><td>etcd-ca</td><td></td><td>etcd/ca.crt</td><td>etcdctl</td><td></td><td>--cacert</td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd/healthcheck-client.key</td><td>etcd/healthcheck-client.crt</td><td>etcdctl</td><td>--key</td><td>--cert</td></tr></tbody></table><p>注意事项同样适用于服务帐户密钥对：</p><table><thead><tr><th>私钥路径</th><th>公钥路径</th><th>命令</th><th>参数</th></tr></thead><tbody><tr><td>sa.key</td><td></td><td>kube-controller-manager</td><td>--service-account-private-key-file</td></tr><tr><td></td><td>sa.pub</td><td>kube-apiserver</td><td>--service-account-key-file</td></tr></tbody></table><p>下面的例子展示了自行生成所有密钥和证书时所需要提供的文件路径。
这些路径基于<a href=/zh-cn/docs/setup/best-practices/certificates/#certificate-paths>前面的表格</a>。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver-etcd-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver-etcd-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver-kubelet-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/apiserver-kubelet-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-ca.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-ca.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/front-proxy-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/server.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/server.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/peer.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/peer.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/healthcheck-client.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/etcd/healthcheck-client.crt
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/sa.key
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/pki/sa.pub
</span></span></span></code></pre></div><h2 id=configure-certificates-for-user-accounts>为用户帐户配置证书</h2><p>你必须手动配置以下管理员帐户和服务帐户：</p><table><thead><tr><th>文件名</th><th>凭据名称</th><th>默认 CN</th><th>O (位于 Subject 中)</th></tr></thead><tbody><tr><td>admin.conf</td><td>default-admin</td><td>kubernetes-admin</td><td>system:masters</td></tr><tr><td>kubelet.conf</td><td>default-auth</td><td>system:node:<code>&lt;nodeName></code> （参阅注释）</td><td>system:nodes</td></tr><tr><td>controller-manager.conf</td><td>default-controller-manager</td><td>system:kube-controller-manager</td><td></td></tr><tr><td>scheduler.conf</td><td>default-scheduler</td><td>system:kube-scheduler</td><td></td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>kubelet.conf</code> 中 <code>&lt;nodeName></code> 的值 <strong>必须</strong> 与 kubelet 向 apiserver 注册时提供的节点名称的值完全匹配。
有关更多详细信息，请阅读<a href=/zh-cn/docs/reference/access-authn-authz/node/>节点授权</a>。</div><ol><li><p>对于每个配置，请都使用给定的 CN 和 O 生成 x509 证书/密钥偶对。</p></li><li><p>为每个配置运行下面的 <code>kubectl</code> 命令：</p></li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-cluster default-cluster --server<span style=color:#666>=</span>https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-credentials &lt;credential-name&gt; --client-key &lt;path-to-key&gt;.pem --client-certificate &lt;path-to-cert&gt;.pem --embed-certs
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-context default-system --cluster default-cluster --user &lt;credential-name&gt;
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config use-context default-system
</span></span></code></pre></div><p>这些文件用途如下：</p><table><thead><tr><th>文件名</th><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>admin.conf</td><td>kubectl</td><td>配置集群的管理员</td></tr><tr><td>kubelet.conf</td><td>kubelet</td><td>集群中的每个节点都需要一份</td></tr><tr><td>controller-manager.conf</td><td>kube-controller-manager</td><td>必需添加到 <code>manifests/kube-controller-manager.yaml</code> 清单中</td></tr><tr><td>scheduler.conf</td><td>kube-scheduler</td><td>必需添加到 <code>manifests/kube-scheduler.yaml</code> 清单中</td></tr></tbody></table><p>下面是前表中所列文件的完整路径。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>/etc/kubernetes/admin.conf
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/kubelet.conf
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/controller-manager.conf
</span></span></span><span style=display:flex><span><span style=color:#888>/etc/kubernetes/scheduler.conf
</span></span></span></code></pre></div></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/zh-cn/docs/home/>主页</a>
<a class=text-white href=/zh-cn/blog/>博客</a>
<a class=text-white href=/zh-cn/training/>培训</a>
<a class=text-white href=/zh-cn/partners/>合作伙伴</a>
<a class=text-white href=/zh-cn/community/>社区</a>
<a class=text-white href=/zh-cn/case-studies/>案例分析</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes 作者 | 文档发布基于 <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a> 授权许可</small><br><small class=text-white>Copyright &copy; 2023 Linux 基金会&reg;。保留所有权利。Linux 基金会已注册并使用商标。如需了解 Linux 基金会的商标列表，请访问<a href=https://www.linuxfoundation.org/trademark-usage class=light-text>商标使用页面</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>