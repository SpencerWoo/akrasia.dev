<!doctype html><html lang=zh-cn class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/scheduling-eviction/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>调度、抢占和驱逐 | Kubernetes</title><meta property="og:title" content="调度、抢占和驱逐"><meta property="og:description" content="在 Kubernetes 中，调度 (scheduling) 指的是确保 Pod 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pod 以便高优先级的 Pod 可以调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程。
"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="调度、抢占和驱逐"><meta itemprop=description content="在 Kubernetes 中，调度 (scheduling) 指的是确保 Pod 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pod 以便高优先级的 Pod 可以调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程。
"><meta name=twitter:card content="summary"><meta name=twitter:title content="调度、抢占和驱逐"><meta name=twitter:description content="在 Kubernetes 中，调度 (scheduling) 指的是确保 Pod 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pod 以便高优先级的 Pod 可以调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程。
"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="在 Kubernetes 中，调度 (scheduling) 指的是确保 Pod 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pod 以便高优先级的 Pod 可以调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程。
"><meta property="og:description" content="在 Kubernetes 中，调度 (scheduling) 指的是确保 Pod 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pod 以便高优先级的 Pod 可以调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程。
"><meta name=twitter:description content="在 Kubernetes 中，调度 (scheduling) 指的是确保 Pod 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pod 以便高优先级的 Pod 可以调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程。
"><meta property="og:url" content="https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/"><meta property="og:title" content="调度、抢占和驱逐"><meta name=twitter:title content="调度、抢占和驱逐"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/zh-cn/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/zh-cn/docs/>文档</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/blog/>Kubernetes 博客</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/training/>培训</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/partners/>合作伙伴</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/community/>社区</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/case-studies/>案例分析</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>版本列表</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>中文 (Chinese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/scheduling-eviction/>English</a>
<a class=dropdown-item href=/ko/docs/concepts/scheduling-eviction/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/scheduling-eviction/>日本語 (Japanese)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/scheduling-eviction/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/scheduling-eviction/>Bahasa Indonesia</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.</p><p><a href=/zh-cn/docs/concepts/scheduling-eviction/>返回本页常规视图</a>.</p></div><h1 class=title>调度、抢占和驱逐</h1><div class=lead>在 Kubernetes 中，调度 (scheduling) 指的是确保 Pod 匹配到合适的节点， 以便 kubelet 能够运行它们。抢占 (Preemption) 指的是终止低优先级的 Pod 以便高优先级的 Pod 可以调度运行的过程。驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程。</div><ul><li>1: <a href=#pg-598f36d691ab197f9d995784574b0a12>Kubernetes 调度器</a></li><li>2: <a href=#pg-21169f516071aea5d16734a4c27789a5>将 Pod 指派给节点</a></li><li>3: <a href=#pg-da22fe2278df236f71efbe672f392677>Pod 开销</a></li><li>4: <a href=#pg-6b8c85a6a88f4a81e6b79e197c293c31>Pod 拓扑分布约束</a></li><li>5: <a href=#pg-ede4960b56a3529ee0bfe7c8fe2d09a5>污点和容忍度</a></li><li>6: <a href=#pg-602208c95fe7b1f1170310ce993f5814>调度框架</a></li><li>7: <a href=#pg-d9574a30fcbc631b0d2a57850e161e89>调度器性能调优</a></li><li>8: <a href=#pg-961126cd43559012893979e568396a49>扩展资源的资源装箱</a></li><li>9: <a href=#pg-60e5a2861609e0848d58ce8bf99c4a31>Pod 优先级和抢占</a></li><li>10: <a href=#pg-78e0431b4b7516092662a7c289cbb304>节点压力驱逐</a></li><li>11: <a href=#pg-b87723bf81b079042860f0ebd37b0a64>API 发起的驱逐</a></li></ul><div class=content><p>在 Kubernetes 中，调度 (scheduling) 指的是确保 <a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>
匹配到合适的<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>，
以便 <a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> 能够运行它们。
抢占 (Preemption) 指的是终止低<a class=glossary-tooltip title='Pod 优先级表示一个 Pod 相对于其他 Pod 的重要性。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority target=_blank aria-label=优先级>优先级</a>的 Pod
以便高优先级的 Pod 可以调度运行的过程。
驱逐 (Eviction) 是在资源匮乏的节点上，主动让一个或多个 Pod 失效的过程。</p><h2 id=调度>调度</h2><ul><li><a href=/zh-cn/docs/concepts/scheduling-eviction/kube-scheduler/>Kubernetes 调度器</a></li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/>将 Pod 指派到节点</a></li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/pod-overhead/>Pod 开销</a></li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod 拓扑分布约束</a></li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/>污点和容忍度</a></li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/scheduling-framework>调度框架</a></li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/scheduler-perf-tuning/>调度器性能调试</a></li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/resource-bin-packing/>扩展资源的资源装箱</a></li></ul><h2 id=pod-干扰>Pod 干扰</h2><p><a href=/zh-cn/docs/concepts/workloads/pods/disruptions/>Pod 干扰</a> 是指节点上的
Pod 被自愿或非自愿终止的过程。</p><p>自愿干扰是由应用程序所有者或集群管理员有意启动的。非自愿干扰是无意的，
可能由不可避免的问题触发，如节点耗尽资源或意外删除。</p><ul><li><a href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级和抢占</a></li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/>节点压力驱逐</a></li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/api-eviction/>API 发起的驱逐</a></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-598f36d691ab197f9d995784574b0a12>1 - Kubernetes 调度器</h1><p>在 Kubernetes 中，<strong>调度</strong> 是指将 <a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>
放置到合适的<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>上，以便对应节点上的
<a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a> 能够运行这些 Pod。</p><h2 id=scheduling>调度概览</h2><p>调度器通过 Kubernetes 的监测（Watch）机制来发现集群中新创建且尚未被调度到节点上的 Pod。
调度器会将所发现的每一个未调度的 Pod 调度到一个合适的节点上来运行。
调度器会依据下文的调度原则来做出调度选择。</p><p>如果你想要理解 Pod 为什么会被调度到特定的节点上，
或者你想要尝试实现一个自定义的调度器，这篇文章将帮助你了解调度。</p><h2 id=kube-scheduler>kube-scheduler</h2><p><a href=/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a>
是 Kubernetes 集群的默认调度器，并且是集群
<a class=glossary-tooltip title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label=控制面>控制面</a> 的一部分。
如果你真得希望或者有这方面的需求，kube-scheduler
在设计上允许你自己编写一个调度组件并替换原有的 kube-scheduler。</p><p>对每一个新创建的 Pod 或者是未被调度的 Pod，kube-scheduler 会选择一个最优的节点去运行这个 Pod。
然而，Pod 内的每一个容器对资源都有不同的需求，
而且 Pod 本身也有不同的需求。因此，Pod 在被调度到节点上之前，
根据这些特定的调度需求，需要对集群中的节点进行一次过滤。</p><p>在一个集群中，满足一个 Pod 调度请求的所有节点称之为 <strong>可调度节点</strong>。
如果没有任何一个节点能满足 Pod 的资源请求，
那么这个 Pod 将一直停留在未调度状态直到调度器能够找到合适的 Node。</p><p>调度器先在集群中找到一个 Pod 的所有可调度节点，然后根据一系列函数对这些可调度节点打分，
选出其中得分最高的节点来运行 Pod。之后，调度器将这个调度决定通知给
kube-apiserver，这个过程叫做 <strong>绑定</strong>。</p><p>在做调度决定时需要考虑的因素包括：单独和整体的资源请求、硬件/软件/策略限制、
亲和以及反亲和要求、数据局部性、负载间的干扰等等。</p><h2 id=kube-scheduler-implementation>kube-scheduler 调度流程</h2><p>kube-scheduler 给一个 Pod 做调度选择时包含两个步骤：</p><ol><li>过滤</li><li>打分</li></ol><p>过滤阶段会将所有满足 Pod 调度需求的节点选出来。
例如，PodFitsResources 过滤函数会检查候选节点的可用资源能否满足 Pod 的资源请求。
在过滤之后，得出一个节点列表，里面包含了所有可调度节点；通常情况下，
这个节点列表包含不止一个节点。如果这个列表是空的，代表这个 Pod 不可调度。</p><p>在打分阶段，调度器会为 Pod 从所有可调度节点中选取一个最合适的节点。
根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。</p><p>最后，kube-scheduler 会将 Pod 调度到得分最高的节点上。
如果存在多个得分最高的节点，kube-scheduler 会从中随机选取一个。</p><p>支持以下两种方式配置调度器的过滤和打分行为：</p><ol><li><a href=/zh-cn/docs/reference/scheduling/policies>调度策略</a>
允许你配置过滤所用的 <strong>断言（Predicates）</strong> 和打分所用的 <strong>优先级（Priorities）</strong>。</li><li><a href=/zh-cn/docs/reference/scheduling/config/#profiles>调度配置</a> 允许你配置实现不同调度阶段的插件，
包括：<code>QueueSort</code>、<code>Filter</code>、<code>Score</code>、<code>Bind</code>、<code>Reserve</code>、<code>Permit</code> 等等。
你也可以配置 kube-scheduler 运行不同的配置文件。</li></ol><h2 id=接下来>接下来</h2><ul><li>阅读关于<a href=/zh-cn/docs/concepts/scheduling-eviction/scheduler-perf-tuning/>调度器性能调优</a></li><li>阅读关于 <a href=/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod 拓扑分布约束</a></li><li>阅读关于 kube-scheduler 的<a href=/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/>参考文档</a></li><li>阅读 <a href=/zh-cn/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler 配置参考 (v1beta3)</a></li><li>了解关于<a href=/zh-cn/docs/tasks/extend-kubernetes/configure-multiple-schedulers/>配置多个调度器</a> 的方式</li><li>了解关于<a href=/zh-cn/docs/tasks/administer-cluster/topology-manager/>拓扑结构管理策略</a></li><li>了解关于 <a href=/zh-cn/docs/concepts/scheduling-eviction/pod-overhead/>Pod 开销</a></li><li>了解关于如何在以下情形使用卷来调度 Pod：<ul><li><a href=/zh-cn/docs/concepts/storage/storage-classes/#volume-binding-mode>卷拓扑支持</a></li><li><a href=/zh-cn/docs/concepts/storage/storage-capacity/>存储容量跟踪</a></li><li><a href=/zh-cn/docs/concepts/storage/storage-limits/>特定于节点的卷数限制</a></li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-21169f516071aea5d16734a4c27789a5>2 - 将 Pod 指派给节点</h1><p>你可以约束一个 <a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>
以便 <strong>限制</strong> 其只能在特定的<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>上运行，
或优先在特定的节点上运行。
有几种方法可以实现这点，推荐的方法都是用
<a href=/zh-cn/docs/concepts/overview/working-with-objects/labels/>标签选择算符</a>来进行选择。
通常这样的约束不是必须的，因为调度器将自动进行合理的放置（比如，将 Pod 分散到节点上，
而不是将 Pod 放置在可用资源不足的节点上等等）。但在某些情况下，你可能需要进一步控制
Pod 被部署到哪个节点。例如，确保 Pod 最终落在连接了 SSD 的机器上，
或者将来自两个不同的服务且有大量通信的 Pods 被放置在同一个可用区。</p><p>你可以使用下列方法中的任何一种来选择 Kubernetes 对特定 Pod 的调度：</p><ul><li>与<a href=#built-in-node-labels>节点标签</a>匹配的 <a href=#nodeSelector>nodeSelector</a></li><li><a href=#affinity-and-anti-affinity>亲和性与反亲和性</a></li><li><a href=#nodename>nodeName</a> 字段</li><li><a href=#pod-topology-spread-constraints>Pod 拓扑分布约束</a></li></ul><h2 id=built-in-node-labels>节点标签</h2><p>与很多其他 Kubernetes 对象类似，节点也有<a href=/zh-cn/docs/concepts/overview/working-with-objects/labels/>标签</a>。
你可以<a href=/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node>手动地添加标签</a>。
Kubernetes 也会为集群中所有节点添加一些标准的标签。
参见<a href=/zh-cn/docs/reference/labels-annotations-taints/>常用的标签、注解和污点</a>以了解常见的节点标签。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>这些标签的取值是取决于云提供商的，并且是无法在可靠性上给出承诺的。
例如，<code>kubernetes.io/hostname</code> 的取值在某些环境中可能与节点名称相同，
而在其他环境中会取不同的值。</div><h2 id=node-isolation-restriction>节点隔离/限制</h2><p>通过为节点添加标签，你可以准备让 Pod 调度到特定节点或节点组上。
你可以使用这个功能来确保特定的 Pod 只能运行在具有一定隔离性，安全性或监管属性的节点上。</p><p>如果使用标签来实现节点隔离，建议选择节点上的
<a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
无法修改的标签键。
这可以防止受感染的节点在自身上设置这些标签，进而影响调度器将工作负载调度到受感染的节点。</p><p><a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction><code>NodeRestriction</code> 准入插件</a>防止
kubelet 使用 <code>node-restriction.kubernetes.io/</code> 前缀设置或修改标签。</p><p>要使用该标签前缀进行节点隔离：</p><ol><li>确保你在使用<a href=/zh-cn/docs/reference/access-authn-authz/node/>节点鉴权</a>机制并且已经启用了
<a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction 准入插件</a>。</li><li>将带有 <code>node-restriction.kubernetes.io/</code> 前缀的标签添加到 Node 对象，
然后在<a href=#nodeSelector>节点选择器</a>中使用这些标签。
例如，<code>example.com.node-restriction.kubernetes.io/fips=true</code> 或
<code>example.com.node-restriction.kubernetes.io/pci-dss=true</code>。</li></ol><h2 id=nodeselector>nodeSelector</h2><p><code>nodeSelector</code> 是节点选择约束的最简单推荐形式。你可以将 <code>nodeSelector</code> 字段添加到
Pod 的规约中设置你希望目标节点所具有的<a href=#built-in-node-labels>节点标签</a>。
Kubernetes 只会将 Pod 调度到拥有你所指定的每个标签的节点上。</p><p>进一步的信息可参见<a href=/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes>将 Pod 指派给节点</a>。</p><h2 id=affinity-and-anti-affinity>亲和性与反亲和性</h2><p><code>nodeSelector</code> 提供了一种最简单的方法来将 Pod 约束到具有特定标签的节点上。
亲和性和反亲和性扩展了你可以定义的约束类型。使用亲和性与反亲和性的一些好处有：</p><ul><li>亲和性、反亲和性语言的表达能力更强。<code>nodeSelector</code> 只能选择拥有所有指定标签的节点。
亲和性、反亲和性为你提供对选择逻辑的更强控制能力。</li><li>你可以标明某规则是“软需求”或者“偏好”，这样调度器在无法找到匹配节点时仍然调度该 Pod。</li><li>你可以使用节点上（或其他拓扑域中）运行的其他 Pod 的标签来实施调度约束，
而不是只能使用节点本身的标签。这个能力让你能够定义规则允许哪些 Pod 可以被放置在一起。</li></ul><p>亲和性功能由两种类型的亲和性组成：</p><ul><li><strong>节点亲和性</strong>功能类似于 <code>nodeSelector</code> 字段，但它的表达能力更强，并且允许你指定软规则。</li><li>Pod 间亲和性/反亲和性允许你根据其他 Pod 的标签来约束 Pod。</li></ul><h3 id=node-affinity>节点亲和性</h3><p>节点亲和性概念上类似于 <code>nodeSelector</code>，
它使你可以根据节点上的标签来约束 Pod 可以调度到哪些节点上。
节点亲和性有两种：</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code>：
调度器只有在规则被满足的时候才能执行调度。此功能类似于 <code>nodeSelector</code>，
但其语法表达能力更强。</li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code>：
调度器会尝试寻找满足对应规则的节点。如果找不到匹配的节点，调度器仍然会调度该 Pod。</li></ul><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>在上述类型中，<code>IgnoredDuringExecution</code> 意味着如果节点标签在 Kubernetes
调度 Pod 后发生了变更，Pod 仍将继续运行。</div><p>你可以使用 Pod 规约中的 <code>.spec.affinity.nodeAffinity</code> 字段来设置节点亲和性。
例如，考虑下面的 Pod 规约：</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/pod-with-node-affinity.yaml download=pods/pod-with-node-affinity.yaml><code>pods/pod-with-node-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-node-affinity-yaml")' title="Copy pods/pod-with-node-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-node-affinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- antarctica-east1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- antarctica-west1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>another-node-label-key<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- another-node-label-value<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0</span></span></code></pre></div></div></div><p>在这一示例中，所应用的规则如下：</p><ul><li>节点<strong>必须</strong>包含一个键名为 <code>topology.kubernetes.io/zone</code> 的标签，
并且该标签的取值<strong>必须</strong>为 <code>antarctica-east1</code> 或 <code>antarctica-west1</code>。</li><li>节点<strong>最好</strong>具有一个键名为 <code>another-node-label-key</code> 且取值为
<code>another-node-label-value</code> 的标签。</li></ul><p>你可以使用 <code>operator</code> 字段来为 Kubernetes 设置在解释规则时要使用的逻辑操作符。
你可以使用 <code>In</code>、<code>NotIn</code>、<code>Exists</code>、<code>DoesNotExist</code>、<code>Gt</code> 和 <code>Lt</code> 之一作为操作符。</p><p><code>NotIn</code> 和 <code>DoesNotExist</code> 可用来实现节点反亲和性行为。
你也可以使用<a href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/>节点污点</a>
将 Pod 从特定节点上驱逐。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>如果你同时指定了 <code>nodeSelector</code> 和 <code>nodeAffinity</code>，<strong>两者</strong> 必须都要满足，
才能将 Pod 调度到候选节点上。</p><p>如果你指定了多个与 <code>nodeAffinity</code> 类型关联的 <code>nodeSelectorTerms</code>，
只要其中一个 <code>nodeSelectorTerms</code> 满足的话，Pod 就可以被调度到节点上。</p><p>如果你指定了多个与同一 <code>nodeSelectorTerms</code> 关联的 <code>matchExpressions</code>，
则只有当所有 <code>matchExpressions</code> 都满足时 Pod 才可以被调度到节点上。</p></div><p>参阅<a href=/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/>使用节点亲和性来为 Pod 指派节点</a>，
以了解进一步的信息。</p><h4 id=node-affinity-weight>节点亲和性权重</h4><p>你可以为 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 亲和性类型的每个实例设置
<code>weight</code> 字段，其取值范围是 1 到 100。
当调度器找到能够满足 Pod 的其他调度请求的节点时，调度器会遍历节点满足的所有的偏好性规则，
并将对应表达式的 <code>weight</code> 值加和。</p><p>最终的加和值会添加到该节点的其他优先级函数的评分之上。
在调度器为 Pod 作出调度决定时，总分最高的节点的优先级也最高。</p><p>例如，考虑下面的 Pod 规约：</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/pod-with-affinity-anti-affinity.yaml download=pods/pod-with-affinity-anti-affinity.yaml><code>pods/pod-with-affinity-anti-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-affinity-anti-affinity-yaml")' title="Copy pods/pod-with-affinity-anti-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-affinity-anti-affinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-affinity-anti-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>kubernetes.io/os<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- linux<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>label-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- key-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>label-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- key-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0</span></span></code></pre></div></div></div><p>如果存在两个候选节点，都满足 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 规则，
其中一个节点具有标签 <code>label-1:key-1</code>，另一个节点具有标签 <code>label-2:key-2</code>，
调度器会考察各个节点的 <code>weight</code> 取值，并将该权重值添加到节点的其他得分值之上，</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>如果你希望 Kubernetes 能够成功地调度此例中的 Pod，你必须拥有打了
<code>kubernetes.io/os=linux</code> 标签的节点。</div><h4 id=node-affinity-per-scheduling-profile>逐个调度方案中设置节点亲和性</h4><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.20 [beta]</code></div><p>在配置多个<a href=/zh-cn/docs/reference/scheduling/config/#multiple-profiles>调度方案</a>时，
你可以将某个方案与节点亲和性关联起来，如果某个调度方案仅适用于某组特殊的节点时，
这样做是很有用的。
要实现这点，可以在<a href=/zh-cn/docs/reference/scheduling/config/>调度器配置</a>中为
<a href=/zh-cn/docs/reference/scheduling/config/#scheduling-plugins><code>NodeAffinity</code> 插件</a>的
<code>args</code> 字段添加 <code>addedAffinity</code>。例如：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>foo-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeAffinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>addedAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>scheduler-profile<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span>- foo<span style=color:#bbb>
</span></span></span></code></pre></div><p>这里的 <code>addedAffinity</code> 除遵从 Pod 规约中设置的节点亲和性之外，
还适用于将 <code>.spec.schedulerName</code> 设置为 <code>foo-scheduler</code>。
换言之，为了匹配 Pod，节点需要满足 <code>addedAffinity</code> 和 Pod 的 <code>.spec.NodeAffinity</code>。</p><p>由于 <code>addedAffinity</code> 对最终用户不可见，其行为可能对用户而言是出乎意料的。
应该使用与调度方案名称有明确关联的节点标签。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>DaemonSet 控制器<a href=/zh-cn/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler>为 DaemonSet 创建 Pods</a>，
但该控制器不理会调度方案。
DaemonSet 控制器创建 Pod 时，默认的 Kubernetes 调度器负责放置 Pod，
并遵从 DaemonSet 控制器中奢侈的 <code>nodeAffinity</code> 规则。</div><h3 id=inter-pod-affinity-and-anti-affinity>Pod 间亲和性与反亲和性</h3><p>Pod 间亲和性与反亲和性使你可以基于已经在节点上运行的 <strong>Pod</strong> 的标签来约束
Pod 可以调度到的节点，而不是基于节点上的标签。</p><p>Pod 间亲和性与反亲和性的规则格式为“如果 X 上已经运行了一个或多个满足规则 Y 的 Pod，
则这个 Pod 应该（或者在反亲和性的情况下不应该）运行在 X 上”。
这里的 X 可以是节点、机架、云提供商可用区或地理区域或类似的拓扑域，
Y 则是 Kubernetes 尝试满足的规则。</p><p>你通过<a href=/zh-cn/docs/concepts/overview/working-with-objects/labels/#label-selectors>标签选择算符</a>
的形式来表达规则（Y），并可根据需要指定选关联的名字空间列表。
Pod 在 Kubernetes 中是名字空间作用域的对象，因此 Pod 的标签也隐式地具有名字空间属性。
针对 Pod 标签的所有标签选择算符都要指定名字空间，Kubernetes
会在指定的名字空间内寻找标签。</p><p>你会通过 <code>topologyKey</code> 来表达拓扑域（X）的概念，其取值是系统用来标示域的节点标签键。
相关示例可参见<a href=/zh-cn/docs/reference/labels-annotations-taints/>常用标签、注解和污点</a>。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>Pod 间亲和性和反亲和性都需要相当的计算量，因此会在大规模集群中显著降低调度速度。
我们不建议在包含数百个节点的集群中使用这类设置。</div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>Pod 反亲和性需要节点上存在一致性的标签。换言之，
集群中每个节点都必须拥有与 <code>topologyKey</code> 匹配的标签。
如果某些或者所有节点上不存在所指定的 <code>topologyKey</code> 标签，调度行为可能与预期的不同。</div><h4 id=pod-间亲和性与反亲和性的类型>Pod 间亲和性与反亲和性的类型</h4><p>与<a href=#node-affinity>节点亲和性</a>类似，Pod 的亲和性与反亲和性也有两种类型：</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code></li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code></li></ul><p>例如，你可以使用 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 亲和性来告诉调度器，
将两个服务的 Pod 放到同一个云提供商可用区内，因为它们彼此之间通信非常频繁。
类似地，你可以使用 <code>preferredDuringSchedulingIgnoredDuringExecution</code>
反亲和性来将同一服务的多个 Pod 分布到多个云提供商可用区中。</p><p>要使用 Pod 间亲和性，可以使用 Pod 规约中的 <code>.affinity.podAffinity</code> 字段。
对于 Pod 间反亲和性，可以使用 Pod 规约中的 <code>.affinity.podAntiAffinity</code> 字段。</p><h4 id=an-example-of-a-pod-that-uses-pod-affinity>Pod 亲和性示例</h4><p>考虑下面的 Pod 规约：</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/pod-with-pod-affinity.yaml download=pods/pod-with-pod-affinity.yaml><code>pods/pod-with-pod-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-pod-affinity-yaml")' title="Copy pods/pod-with-pod-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-pod-affinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- S1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinityTerm</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- S2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>本示例定义了一条 Pod 亲和性规则和一条 Pod 反亲和性规则。Pod 亲和性规则配置为
<code>requiredDuringSchedulingIgnoredDuringExecution</code>，而 Pod 反亲和性配置为
<code>preferredDuringSchedulingIgnoredDuringExecution</code>。</p><p>亲和性规则表示，仅当节点和至少一个已运行且有 <code>security=S1</code> 的标签的
Pod 处于同一区域时，才可以将该 Pod 调度到节点上。
更确切的说，调度器必须将 Pod 调度到具有 <code>topology.kubernetes.io/zone=V</code>
标签的节点上，并且集群中至少有一个位于该可用区的节点上运行着带有
<code>security=S1</code> 标签的 Pod。</p><p>反亲和性规则表示，如果节点处于 Pod 所在的同一可用区且至少一个 Pod 具有
<code>security=S2</code> 标签，则该 Pod 不应被调度到该节点上。
更确切地说， 如果同一可用区中存在其他运行着带有 <code>security=S2</code> 标签的 Pod 节点，
并且节点具有标签 <code>topology.kubernetes.io/zone=R</code>，Pod 不能被调度到该节点上。</p><p>查阅<a href=https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md>设计文档</a>
以进一步熟悉 Pod 亲和性与反亲和性的示例。</p><p>你可以针对 Pod 间亲和性与反亲和性为其 <code>operator</code> 字段使用 <code>In</code>、<code>NotIn</code>、<code>Exists</code>、
<code>DoesNotExist</code> 等值。</p><p>原则上，<code>topologyKey</code> 可以是任何合法的标签键。出于性能和安全原因，<code>topologyKey</code>
有一些限制：</p><ul><li>对于 Pod 亲和性而言，在 <code>requiredDuringSchedulingIgnoredDuringExecution</code>
和 <code>preferredDuringSchedulingIgnoredDuringExecution</code> 中，<code>topologyKey</code>
不允许为空。</li><li>对于 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 要求的 Pod 反亲和性，
准入控制器 <code>LimitPodHardAntiAffinityTopology</code> 要求 <code>topologyKey</code> 只能是
<code>kubernetes.io/hostname</code>。如果你希望使用其他定制拓扑逻辑，
你可以更改准入控制器或者禁用之。</li></ul><p>除了 <code>labelSelector</code> 和 <code>topologyKey</code>，你也可以指定 <code>labelSelector</code>
要匹配的命名空间列表，方法是在 <code>labelSelector</code> 和 <code>topologyKey</code>
所在层同一层次上设置 <code>namespaces</code>。
如果 <code>namespaces</code> 被忽略或者为空，则默认为 Pod 亲和性/反亲和性的定义所在的命名空间。</p><h4 id=namespace-selector>名字空间选择算符</h4><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.24 [stable]</code></div><p>用户也可以使用 <code>namespaceSelector</code> 选择匹配的名字空间，<code>namespaceSelector</code>
是对名字空间集合进行标签查询的机制。
亲和性条件会应用到 <code>namespaceSelector</code> 所选择的名字空间和 <code>namespaces</code> 字段中所列举的名字空间之上。
注意，空的 <code>namespaceSelector</code>（<code>{}</code>）会匹配所有名字空间，而 null 或者空的
<code>namespaces</code> 列表以及 null 值 <code>namespaceSelector</code> 意味着“当前 Pod 的名字空间”。</p><h4 id=更实际的用例>更实际的用例</h4><p>Pod 间亲和性与反亲和性在与更高级别的集合（例如 ReplicaSet、StatefulSet、
Deployment 等）一起使用时，它们可能更加有用。
这些规则使得你可以配置一组工作负载，使其位于所定义的同一拓扑中；
例如优先将两个相关的 Pod 置于相同的节点上。</p><p>以一个三节点的集群为例。你使用该集群运行一个带有内存缓存（例如 Redis）的 Web 应用程序。
在此例中，还假设 Web 应用程序和内存缓存之间的延迟应尽可能低。
你可以使用 Pod 间的亲和性和反亲和性来尽可能地将该 Web 服务器与缓存并置。</p><p>在下面的 Redis 缓存 Deployment 示例中，副本上设置了标签 <code>app=store</code>。
<code>podAntiAffinity</code> 规则告诉调度器避免将多个带有 <code>app=store</code> 标签的副本部署到同一节点上。
因此，每个独立节点上会创建一个缓存实例。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-cache<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis:3.2-alpine<span style=color:#bbb>
</span></span></span></code></pre></div><p>下例的 Deployment 为 Web 服务器创建带有标签 <code>app=web-store</code> 的副本。
Pod 亲和性规则告诉调度器将每个副本放到存在标签为 <code>app=store</code> 的 Pod 的节点上。
Pod 反亲和性规则告诉调度器决不要在单个节点上放置多个 <code>app=web-store</code> 服务器。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- web-store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.16-alpine<span style=color:#bbb>
</span></span></span></code></pre></div><p>创建前面两个 Deployment 会产生如下的集群布局，每个 Web 服务器与一个缓存实例并置，
并分别运行在三个独立的节点上。</p><table><thead><tr><th style=text-align:center>node-1</th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th></tr></thead><tbody><tr><td style=text-align:center><em>webserver-1</em></td><td style=text-align:center><em>webserver-2</em></td><td style=text-align:center><em>webserver-3</em></td></tr><tr><td style=text-align:center><em>cache-1</em></td><td style=text-align:center><em>cache-2</em></td><td style=text-align:center><em>cache-3</em></td></tr></tbody></table><p>总体效果是每个缓存实例都非常可能被在同一个节点上运行的某个客户端访问。
这种方法旨在最大限度地减少偏差（负载不平衡）和延迟。</p><p>你可能还有使用 Pod 反亲和性的一些其他原因。
参阅 <a href=/zh-cn/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure>ZooKeeper 教程</a>
了解一个 StatefulSet 的示例，该 StatefulSet 配置了反亲和性以实现高可用，
所使用的是与此例相同的技术。</p><h2 id=nodename>nodeName</h2><p><code>nodeName</code> 是比亲和性或者 <code>nodeSelector</code> 更为直接的形式。<code>nodeName</code> 是 Pod
规约中的一个字段。如果 <code>nodeName</code> 字段不为空，调度器会忽略该 Pod，
而指定节点上的 kubelet 会尝试将 Pod 放到该节点上。
使用 <code>nodeName</code> 规则的优先级会高于使用 <code>nodeSelector</code> 或亲和性与非亲和性的规则。</p><p>使用 <code>nodeName</code> 来选择节点的方式有一些局限性：</p><ul><li>如果所指代的节点不存在，则 Pod 无法运行，而且在某些情况下可能会被自动删除。</li><li>如果所指代的节点无法提供用来运行 Pod 所需的资源，Pod 会失败，
而其失败原因中会给出是否因为内存或 CPU 不足而造成无法运行。</li><li>在云环境中的节点名称并不总是可预测的，也不总是稳定的。</li></ul><p>下面是一个使用 <code>nodeName</code> 字段的 Pod 规约示例：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeName</span>:<span style=color:#bbb> </span>kube-01<span style=color:#bbb>
</span></span></span></code></pre></div><p>上面的 Pod 只能运行在节点 <code>kube-01</code> 之上。</p><h2 id=pod-topology-spread-constraints>Pod 拓扑分布约束</h2><p>你可以使用 <strong>拓扑分布约束（Topology Spread Constraints）</strong> 来控制
<a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> 在集群内故障域之间的分布，
故障域的示例有区域（Region）、可用区（Zone）、节点和其他用户自定义的拓扑域。
这样做有助于提升性能、实现高可用或提升资源利用率。</p><p>阅读 <a href=/zh-cn/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod 拓扑分布约束</a>
以进一步了解这些约束的工作方式。</p><h2 id=接下来>接下来</h2><ul><li>进一步阅读<a href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/>污点与容忍度</a>文档。</li><li>阅读<a href=https://git.k8s.io/design-proposals-archive/scheduling/nodeaffinity.md>节点亲和性</a>
和<a href=https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md>Pod 间亲和性与反亲和性</a>
的设计文档。</li><li>了解<a href=/zh-cn/docs/tasks/administer-cluster/topology-manager/>拓扑管理器</a>如何参与节点层面资源分配决定。</li><li>了解如何使用 <a href=/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/>nodeSelector</a>。</li><li>了解如何使用<a href=/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/>亲和性和反亲和性</a>。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-da22fe2278df236f71efbe672f392677>3 - Pod 开销</h1><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.24 [stable]</code></div><p>在节点上运行 Pod 时，Pod 本身占用大量系统资源。这些是运行 Pod 内容器所需资源之外的资源。
在 Kubernetes 中，<em>POD 开销</em> 是一种方法，用于计算 Pod 基础设施在容器请求和限制之上消耗的资源。</p><p>在 Kubernetes 中，Pod 的开销是根据与 Pod 的 <a href=/zh-cn/docs/concepts/containers/runtime-class/>RuntimeClass</a>
相关联的开销在<a href=/zh-cn/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks>准入</a>时设置的。</p><p>如果启用了 Pod Overhead，在调度 Pod 时，除了考虑容器资源请求的总和外，还要考虑 Pod 开销。
类似地，kubelet 将在确定 Pod cgroups 的大小和执行 Pod 驱逐排序时也会考虑 Pod 开销。</p><h2 id=set-up>配置 Pod 开销</h2><p>你需要确保使用一个定义了 <code>overhead</code> 字段的 <code>RuntimeClass</code>。</p><h2 id=使用示例>使用示例</h2><p>要使用 Pod 开销，你需要一个定义了 <code>overhead</code> 字段的 RuntimeClass。
作为例子，下面的 RuntimeClass 定义中包含一个虚拟化所用的容器运行时，
RuntimeClass 如下，其中每个 Pod 大约使用 120MiB 用来运行虚拟机和寄宿操作系统：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>overhead</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podFixed</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;120Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>通过指定 <code>kata-fc</code> RuntimeClass 处理程序创建的工作负载会将内存和 CPU
开销计入资源配额计算、节点调度以及 Pod cgroup 尺寸确定。</p><p>假设我们运行下面给出的工作负载示例 test-pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>stdin</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tty</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>1500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></span></span></code></pre></div><p>在准入阶段 RuntimeClass <a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>
更新工作负载的 PodSpec 以包含
RuntimeClass 中定义的 <code>overhead</code>。如果 PodSpec 中已定义该字段，该 Pod 将会被拒绝。
在这个例子中，由于只指定了 RuntimeClass 名称，所以准入控制器更新了 Pod，使之包含 <code>overhead</code>。</p><p>在 RuntimeClass 准入控制器进行修改后，你可以查看更新后的 PodSpec：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.overhead}&#39;</span>
</span></span></code></pre></div><p>输出：</p><pre tabindex=0><code>map[cpu:250m memory:120Mi]
</code></pre><p>如果定义了 <a href=/zh-cn/docs/concepts/policy/resource-quotas/>ResourceQuota</a>,
则容器请求的总量以及 <code>overhead</code> 字段都将计算在内。</p><p>当 kube-scheduler 决定在哪一个节点调度运行新的 Pod 时，调度器会兼顾该 Pod 的
<code>overhead</code> 以及该 Pod 的容器请求总量。在这个示例中，调度器将资源请求和开销相加，
然后寻找具备 2.25 CPU 和 320 MiB 内存可用的节点。</p><p>一旦 Pod 被调度到了某个节点， 该节点上的 kubelet 将为该 Pod 新建一个
<a class=glossary-tooltip title='一组具有可选资源隔离、审计和限制的 Linux 进程。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label=cgroup>cgroup</a>。 底层容器运行时将在这个
Pod 中创建容器。</p><p>如果该资源对每一个容器都定义了一个限制（定义了限制值的 Guaranteed QoS 或者
Burstable QoS），kubelet 会为与该资源（CPU 的 <code>cpu.cfs_quota_us</code> 以及内存的
<code>memory.limit_in_bytes</code>）
相关的 Pod cgroup 设定一个上限。该上限基于 PodSpec 中定义的容器限制总量与 <code>overhead</code> 之和。</p><p>对于 CPU，如果 Pod 的 QoS 是 Guaranteed 或者 Burstable，kubelet 会基于容器请求总量与
PodSpec 中定义的 <code>overhead</code> 之和设置 <code>cpu.shares</code>。</p><p>请看这个例子，验证工作负载的容器请求：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.containers[*].resources.limits}&#39;</span>
</span></span></code></pre></div><p>容器请求总计 2000m CPU 和 200MiB 内存：</p><pre tabindex=0><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre><p>对照从节点观察到的情况来检查一下：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe node | grep test-pod -B2
</span></span></code></pre></div><p>该输出显示请求了 2250m CPU 以及 320MiB 内存。请求包含了 Pod 开销在内：</p><pre tabindex=0><code>  Namespace    Name       CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------    ----       ------------  ----------   ---------------  -------------  ---
  default      test-pod   2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre><h2 id=验证-pod-cgroup-限制>验证 Pod cgroup 限制</h2><p>在工作负载所运行的节点上检查 Pod 的内存 cgroups。在接下来的例子中，
将在该节点上使用具备 CRI 兼容的容器运行时命令行工具
<a href=https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md><code>crictl</code></a>。
这是一个显示 Pod 开销行为的高级示例， 预计用户不需要直接在节点上检查 cgroups。
首先在特定的节点上确定该 Pod 的标识符：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># 在该 Pod 被调度到的节点上执行如下命令：</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>POD_ID</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>sudo crictl pods --name test-pod -q<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div><p>可以依此判断该 Pod 的 cgroup 路径：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># 在该 Pod 被调度到的节点上执行如下命令：</span>
</span></span><span style=display:flex><span>sudo crictl inspectp -o<span style=color:#666>=</span>json <span style=color:#b8860b>$POD_ID</span> | grep cgroupsPath
</span></span></code></pre></div><p>执行结果的 cgroup 路径中包含了该 Pod 的 <code>pause</code> 容器。Pod 级别的 cgroup 在即上一层目录。</p><pre tabindex=0><code>  &#34;cgroupsPath&#34;: &#34;/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a&#34;
</code></pre><p>在这个例子中，该 Pod 的 cgroup 路径是 <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>。
验证内存的 Pod 级别 cgroup 设置：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># 在该 Pod 被调度到的节点上执行这个命令。</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 另外，修改 cgroup 的名称以匹配为该 Pod 分配的 cgroup。</span>
</span></span><span style=display:flex><span> cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</span></span></code></pre></div><p>和预期的一样，这一数值为 320 MiB。</p><pre tabindex=0><code>335544320
</code></pre><h3 id=可观察性>可观察性</h3><p>在 <a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a> 中可以通过
<code>kube_pod_overhead_*</code> 指标来协助确定何时使用 Pod 开销，
以及协助观察以一个既定开销运行的工作负载的稳定性。
该特性在 kube-state-metrics 的 1.9 发行版本中不可用，不过预计将在后续版本中发布。
在此之前，用户需要从源代码构建 kube-state-metrics。</p><h2 id=接下来>接下来</h2><ul><li>学习更多关于 <a href=/zh-cn/docs/concepts/containers/runtime-class/>RuntimeClass</a> 的信息</li><li>阅读 <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>PodOverhead 设计</a>增强建议以获取更多上下文</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6b8c85a6a88f4a81e6b79e197c293c31>4 - Pod 拓扑分布约束</h1><p>你可以使用 <strong>拓扑分布约束（Topology Spread Constraints）</strong> 来控制
<a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> 在集群内故障域之间的分布，
例如区域（Region）、可用区（Zone）、节点和其他用户自定义拓扑域。
这样做有助于实现高可用并提升资源利用率。</p><p>你可以将<a href=#cluster-level-default-constraints>集群级约束</a>设为默认值，或为个别工作负载配置拓扑分布约束。</p><h2 id=motivation>动机</h2><p>假设你有一个最多包含二十个节点的集群，你想要运行一个自动扩缩的
<a class=glossary-tooltip title='工作负载是在 Kubernetes 上运行的应用程序。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/ target=_blank aria-label=工作负载>工作负载</a>，请问要使用多少个副本？
答案可能是最少 2 个 Pod，最多 15 个 Pod。
当只有 2 个 Pod 时，你倾向于这 2 个 Pod 不要同时在同一个节点上运行：
你所遭遇的风险是如果放在同一个节点上且单节点出现故障，可能会让你的工作负载下线。</p><p>除了这个基本的用法之外，还有一些高级的使用案例，能够让你的工作负载受益于高可用性并提高集群利用率。</p><p>随着你的工作负载扩容，运行的 Pod 变多，将需要考虑另一个重要问题。
假设你有 3 个节点，每个节点运行 5 个 Pod。这些节点有足够的容量能够运行许多副本；
但与这个工作负载互动的客户端分散在三个不同的数据中心（或基础设施可用区）。
现在你可能不太关注单节点故障问题，但你会注意到延迟高于自己的预期，
在不同的可用区之间发送网络流量会产生一些网络成本。</p><p>你决定在正常运营时倾向于将类似数量的副本<a href=/zh-cn/docs/concepts/scheduling-eviction/>调度</a>
到每个基础设施可用区，且你想要该集群在遇到问题时能够自愈。</p><p>Pod 拓扑分布约束使你能够以声明的方式进行配置。</p><h2 id=topologyspreadconstraints-field><code>topologySpreadConstraints</code> 字段</h2><p>Pod API 包括一个 <code>spec.topologySpreadConstraints</code> 字段。这个字段的用法如下所示：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># 配置一个拓扑分布约束</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span>&lt;integer&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>minDomains</span>:<span style=color:#bbb> </span>&lt;integer&gt;<span style=color:#bbb> </span><span style=color:#080;font-style:italic># 可选；自从 v1.25 开始成为 Beta</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>&lt;string&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>&lt;string&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb> </span>&lt;object&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabelKeys</span>:<span style=color:#bbb> </span>&lt;list&gt;<span style=color:#bbb> </span><span style=color:#080;font-style:italic># 可选；自从 v1.25 开始成为 Alpha</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeAffinityPolicy</span>:<span style=color:#bbb> </span>[Honor|Ignore]<span style=color:#bbb> </span><span style=color:#080;font-style:italic># 可选；自从 v1.25 开始成为 Alpha</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeTaintsPolicy</span>:<span style=color:#bbb> </span>[Honor|Ignore]<span style=color:#bbb> </span><span style=color:#080;font-style:italic># 可选；自从 v1.25 开始成为 Alpha</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic>### 其他 Pod 字段置于此处</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>你可以运行 <code>kubectl explain Pod.spec.topologySpreadConstraints</code> 或参阅 Pod API
参考的<a href=/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling>调度</a>一节，
了解有关此字段的更多信息。</p><h3 id=spread-constraint-definition>分布约束定义</h3><p>你可以定义一个或多个 <code>topologySpreadConstraints</code> 条目以指导 kube-scheduler
如何将每个新来的 Pod 与跨集群的现有 Pod 相关联。这些字段包括：</p><ul><li><p><strong>maxSkew</strong> 描述这些 Pod 可能被均匀分布的程度。你必须指定此字段且该数值必须大于零。
其语义将随着 <code>whenUnsatisfiable</code> 的值发生变化：</p><ul><li>如果你选择 <code>whenUnsatisfiable: DoNotSchedule</code>，则 <code>maxSkew</code> 定义目标拓扑中匹配 Pod 的数量与
<strong>全局最小值</strong>（符合条件的域中匹配的最小 Pod 数量，如果符合条件的域数量小于 MinDomains 则为零）
之间的最大允许差值。例如，如果你有 3 个可用区，分别有 2、2 和 1 个匹配的 Pod，则 <code>MaxSkew</code> 设为 1，
且全局最小值为 1。</li><li>如果你选择 <code>whenUnsatisfiable: ScheduleAnyway</code>，则该调度器会更为偏向能够降低偏差值的拓扑域。</li></ul></li></ul><ul><li><p><strong>minDomains</strong> 表示符合条件的域的最小数量。此字段是可选的。域是拓扑的一个特定实例。
符合条件的域是其节点与节点选择器匹配的域。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>minDomains</code> 字段是一个 Alpha 字段，在 1.25 中默认被启用。
你可以通过禁用 <code>MinDomainsInPodToplogySpread</code>
<a href=/zh-cn/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>来禁用该字段。</div><ul><li>指定的 <code>minDomains</code> 值必须大于 0。你可以结合 <code>whenUnsatisfiable: DoNotSchedule</code> 仅指定 <code>minDomains</code>。</li><li>当符合条件的、拓扑键匹配的域的数量小于 <code>minDomains</code> 时，拓扑分布将“全局最小值”（global minimum）设为 0，
然后进行 <code>skew</code> 计算。“全局最小值” 是一个符合条件的域中匹配 Pod 的最小数量，
如果符合条件的域的数量小于 <code>minDomains</code>，则全局最小值为零。</li><li>当符合条件的拓扑键匹配域的个数等于或大于 <code>minDomains</code> 时，该值对调度没有影响。</li><li>如果你未指定 <code>minDomains</code>，则约束行为类似于 <code>minDomains</code> 等于 1。</li></ul></li></ul><ul><li><p><strong>topologyKey</strong> 是<a href=#node-labels>节点标签</a>的键。如果节点使用此键标记并且具有相同的标签值，
则将这些节点视为处于同一拓扑域中。我们将拓扑域中（即键值对）的每个实例称为一个域。
调度器将尝试在每个拓扑域中放置数量均衡的 Pod。
另外，我们将符合条件的域定义为其节点满足 nodeAffinityPolicy 和 nodeTaintsPolicy 要求的域。</p></li><li><p><strong>whenUnsatisfiable</strong> 指示如果 Pod 不满足分布约束时如何处理：</p><ul><li><code>DoNotSchedule</code>（默认）告诉调度器不要调度。</li><li><code>ScheduleAnyway</code> 告诉调度器仍然继续调度，只是根据如何能将偏差最小化来对节点进行排序。</li></ul></li><li><p><strong>labelSelector</strong> 用于查找匹配的 Pod。匹配此标签的 Pod 将被统计，以确定相应拓扑域中 Pod 的数量。
有关详细信息，请参考<a href=/zh-cn/docs/concepts/overview/working-with-objects/labels/#label-selectors>标签选择算符</a>。</p></li></ul><ul><li><p><strong>matchLabelKeys</strong> 是一个 Pod 标签键的列表，用于选择需要计算分布方式的 Pod 集合。
这些键用于从 Pod 标签中查找值，这些键值标签与 <code>labelSelector</code> 进行逻辑与运算，以选择一组已有的 Pod，
通过这些 Pod 计算新来 Pod 的分布方式。Pod 标签中不存在的键将被忽略。
null 或空列表意味着仅与 <code>labelSelector</code> 匹配。</p><p>借助 <code>matchLabelKeys</code>，用户无需在变更 Pod 修订版本时更新 <code>pod.spec</code>。
控制器或 Operator 只需要将不同修订版的 <code>label</code> 键设为不同的值。
调度器将根据 <code>matchLabelKeys</code> 自动确定取值。例如，如果用户使用 Deployment，
则他们可以使用由 Deployment 控制器自动添加的、以 <code>pod-template-hash</code> 为键的标签来区分单个
Deployment 的不同修订版。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>kubernetes.io/hostname<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchLabelKeys</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- pod-template-hash<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>matchLabelKeys</code> 字段是 1.25 中新增的一个 Alpha 字段。
你必须启用 <code>MatchLabelKeysInPodTopologySpread</code>
<a href=/zh-cn/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>才能使用此字段。</div></li></ul><ul><li><p><strong>nodeAffinityPolicy</strong> 表示我们在计算 Pod 拓扑分布偏差时将如何处理 Pod 的 nodeAffinity/nodeSelector。
选项为：</p><ul><li>Honor：只有与 nodeAffinity/nodeSelector 匹配的节点才会包括到计算中。</li><li>Ignore：nodeAffinity/nodeSelector 被忽略。所有节点均包括到计算中。</li></ul><p>如果此值为 nil，此行为等同于 Honor 策略。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>nodeAffinityPolicy</code> 是 1.25 中新增的一个 Alpha 级别字段。
你必须启用 <code>NodeInclusionPolicyInPodTopologySpread</code>
<a href=/zh-cn/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>才能使用此字段。</div></li></ul><ul><li><p><strong>nodeTaintsPolicy</strong> 表示我们在计算 Pod 拓扑分布偏差时将如何处理节点污点。选项为：</p><ul><li>Honor：包括不带污点的节点以及污点被新 Pod 所容忍的节点。</li><li>Ignore：节点污点被忽略。包括所有节点。</li></ul><p>如果此值为 null，此行为等同于 Ignore 策略。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>nodeTaintsPolicy</code> 是 1.25 中新增的一个 Alpha 级别字段。
你必须启用 <code>NodeInclusionPolicyInPodTopologySpread</code>
<a href=/zh-cn/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>才能使用此字段。</div></li></ul><p>当 Pod 定义了不止一个 <code>topologySpreadConstraint</code>，这些约束之间是逻辑与的关系。
kube-scheduler 会为新的 Pod 寻找一个能够满足所有约束的节点。</p><h3 id=node-labels>节点标签</h3><p>拓扑分布约束依赖于节点标签来标识每个<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>所在的拓扑域。
例如，某节点可能具有标签：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>region</span>:<span style=color:#bbb> </span>us-east-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>zone</span>:<span style=color:#bbb> </span>us-east-1a<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>为了简便，此示例未使用<a href=/zh-cn/docs/reference/labels-annotations-taints/>众所周知</a>的标签键
<code>topology.kubernetes.io/zone</code> 和 <code>topology.kubernetes.io/region</code>。
但是，建议使用那些已注册的标签键，而不是此处使用的私有（不合格）标签键 <code>region</code> 和 <code>zone</code>。</p><p>你无法对不同上下文之间的私有标签键的含义做出可靠的假设。</p></div><p>假设你有一个 4 节点的集群且带有以下标签：</p><pre tabindex=0><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre><p>那么，从逻辑上看集群如下：</p><figure><div class=mermaid>graph TB
subgraph "zoneB"
n3(Node3)
n4(Node4)
end
subgraph "zoneA"
n1(Node1)
n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>必须<a href=https://www.enable-javascript.com/>启用</a> JavaScript 才能查看此页内容</em></div></noscript><h2 id=Consistency>一致性</h2><p>你应该为一个组中的所有 Pod 设置相同的 Pod 拓扑分布约束。</p><p>通常，如果你正使用一个工作负载控制器，例如 Deployment，则 Pod 模板会帮你解决这个问题。
如果你混合不同的分布约束，则 Kubernetes 会遵循该字段的 API 定义；
但是，该行为可能更令人困惑，并且故障排除也没那么简单。</p><p>你需要一种机制来确保拓扑域（例如云提供商区域）中的所有节点具有一致的标签。
为了避免你需要手动为节点打标签，大多数集群会自动填充知名的标签，
例如 <code>topology.kubernetes.io/hostname</code>。检查你的集群是否支持此功能。</p><h2 id=topology-spread-constraint-examples>拓扑分布约束示例</h2><h3 id=example-one-topologyspreadconstraint>示例：一个拓扑分布约束</h3><p>假设你拥有一个 4 节点集群，其中标记为 <code>foo: bar</code> 的 3 个 Pod 分别位于 node1、node2 和 node3 中：</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>必须<a href=https://www.enable-javascript.com/>启用</a> JavaScript 才能查看此页内容</em></div></noscript><p>如果你希望新来的 Pod 均匀分布在现有的可用区域，则可以按如下设置其清单：</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/topology-spread-constraints/one-constraint.yaml download=pods/topology-spread-constraints/one-constraint.yaml><code>pods/topology-spread-constraints/one-constraint.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-one-constraint-yaml")' title="Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-one-constraint-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>从此清单看，<code>topologyKey: zone</code> 意味着均匀分布将只应用于存在标签键值对为 <code>zone: &lt;any value></code> 的节点
（没有 <code>zone</code> 标签的节点将被跳过）。如果调度器找不到一种方式来满足此约束，
则 <code>whenUnsatisfiable: DoNotSchedule</code> 字段告诉该调度器将新来的 Pod 保持在 pending 状态。</p><p>如果该调度器将这个新来的 Pod 放到可用区 <code>A</code>，则 Pod 的分布将成为 <code>[3, 1]</code>。
这意味着实际偏差是 2（计算公式为 <code>3 - 1</code>），这违反了 <code>maxSkew: 1</code> 的约定。
为了满足这个示例的约束和上下文，新来的 Pod 只能放到可用区 <code>B</code> 中的一个节点上：</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
p4(mypod) --> n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>必须<a href=https://www.enable-javascript.com/>启用</a> JavaScript 才能查看此页内容</em></div></noscript><p>或者</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
p4(mypod) --> n3
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>必须<a href=https://www.enable-javascript.com/>启用</a> JavaScript 才能查看此页内容</em></div></noscript><p>你可以调整 Pod 规约以满足各种要求：</p><ul><li>将 <code>maxSkew</code> 更改为更大的值，例如 <code>2</code>，这样新来的 Pod 也可以放在可用区 <code>A</code> 中。</li><li>将 <code>topologyKey</code> 更改为 <code>node</code>，以便将 Pod 均匀分布在节点上而不是可用区中。
在上面的例子中，如果 <code>maxSkew</code> 保持为 <code>1</code>，则新来的 Pod 只能放到 <code>node4</code> 节点上。</li><li>将 <code>whenUnsatisfiable: DoNotSchedule</code> 更改为 <code>whenUnsatisfiable: ScheduleAnyway</code>，
以确保新来的 Pod 始终可以被调度（假设满足其他的调度 API）。但是，最好将其放置在匹配 Pod 数量较少的拓扑域中。
请注意，这一优先判定会与其他内部调度优先级（如资源使用率等）排序准则一起进行标准化。</li></ul><h3 id=example-multiple-topologyspreadconstraints>示例：多个拓扑分布约束</h3><p>下面的例子建立在前面例子的基础上。假设你拥有一个 4 节点集群，
其中 3 个标记为 <code>foo: bar</code> 的 Pod 分别位于 node1、node2 和 node3 上：</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>必须<a href=https://www.enable-javascript.com/>启用</a> JavaScript 才能查看此页内容</em></div></noscript><p>可以组合使用 2 个拓扑分布约束来控制 Pod 在节点和可用区两个维度上的分布：</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/topology-spread-constraints/two-constraints.yaml download=pods/topology-spread-constraints/two-constraints.yaml><code>pods/topology-spread-constraints/two-constraints.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-two-constraints-yaml")' title="Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-two-constraints-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>node<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>在这种情况下，为了匹配第一个约束，新的 Pod 只能放置在可用区 <code>B</code> 中；
而在第二个约束中，新来的 Pod 只能调度到节点 <code>node4</code> 上。
该调度器仅考虑满足所有已定义约束的选项，因此唯一可行的选择是放置在节点 <code>node4</code> 上。</p><h3 id=example-conflicting-topologyspreadconstraints>示例：有冲突的拓扑分布约束</h3><p>多个约束可能导致冲突。假设有一个跨 2 个可用区的 3 节点集群：</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p4(Pod) --> n3(Node3)
p5(Pod) --> n3
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n1
p3(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>必须<a href=https://www.enable-javascript.com/>启用</a> JavaScript 才能查看此页内容</em></div></noscript><p>如果你将 <a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml><code>two-constraints.yaml</code></a>
（来自上一个示例的清单）应用到<strong>这个</strong>集群，你将看到 Pod <code>mypod</code> 保持在 <code>Pending</code> 状态。
出现这种情况的原因为：为了满足第一个约束，Pod <code>mypod</code> 只能放置在可用区 <code>B</code> 中；
而在第二个约束中，Pod <code>mypod</code> 只能调度到节点 <code>node2</code> 上。
两个约束的交集将返回一个空集，且调度器无法放置该 Pod。</p><p>为了应对这种情形，你可以提高 <code>maxSkew</code> 的值或修改其中一个约束才能使用 <code>whenUnsatisfiable: ScheduleAnyway</code>。
根据实际情形，例如若你在故障排查时发现某个漏洞修复工作毫无进展，你还可能决定手动删除一个现有的 Pod。</p><h4 id=interaction-with-node-affinity-and-node-selectors>与节点亲和性和节点选择算符的相互作用</h4><p>如果 Pod 定义了 <code>spec.nodeSelector</code> 或 <code>spec.affinity.nodeAffinity</code>，
调度器将在偏差计算中跳过不匹配的节点。</p><h3 id=example-topologyspreadconstraints-with-nodeaffinity>示例：带节点亲和性的拓扑分布约束</h3><p>假设你有一个跨可用区 A 到 C 的 5 节点集群：</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>必须<a href=https://www.enable-javascript.com/>启用</a> JavaScript 才能查看此页内容</em></div></noscript><figure><div class=mermaid>graph BT
subgraph "zoneC"
n5(Node5)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n5 k8s;
class zoneC cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>必须<a href=https://www.enable-javascript.com/>启用</a> JavaScript 才能查看此页内容</em></div></noscript><p>而且你知道可用区 <code>C</code> 必须被排除在外。在这种情况下，可以按如下方式编写清单，
以便将 Pod <code>mypod</code> 放置在可用区 <code>B</code> 上，而不是可用区 <code>C</code> 上。
同样，Kubernetes 也会一样处理 <code>spec.nodeSelector</code>。</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml download=pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml")' title="Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>NotIn<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- zoneC<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><h2 id=implicit-conventions>隐式约定</h2><p>这里有一些值得注意的隐式约定：</p><ul><li><p>只有与新来的 Pod 具有相同命名空间的 Pod 才能作为匹配候选者。</p></li><li><p>调度器会忽略没有任何 <code>topologySpreadConstraints[*].topologyKey</code> 的节点。这意味着：</p><ol><li>位于这些节点上的 Pod 不影响 <code>maxSkew</code> 计算，在上面的例子中，假设节点 <code>node1</code> 没有标签 "zone"，
则 2 个 Pod 将被忽略，因此新来的 Pod 将被调度到可用区 <code>A</code> 中。</li><li>新的 Pod 没有机会被调度到这类节点上。在上面的例子中，
假设节点 <code>node5</code> 带有 <strong>拼写错误的</strong> 标签 <code>zone-typo: zoneC</code>（且没有设置 <code>zone</code> 标签）。
节点 <code>node5</code> 接入集群之后，该节点将被忽略且针对该工作负载的 Pod 不会被调度到那里。</li></ol></li></ul><ul><li>注意，如果新 Pod 的 <code>topologySpreadConstraints[*].labelSelector</code> 与自身的标签不匹配，将会发生什么。
在上面的例子中，如果移除新 Pod 的标签，则 Pod 仍然可以放置到可用区 <code>B</code> 中的节点上，因为这些约束仍然满足。
然而，在放置之后，集群的不平衡程度保持不变。可用区 <code>A</code> 仍然有 2 个 Pod 带有标签 <code>foo: bar</code>，
而可用区 <code>B</code> 有 1 个 Pod 带有标签 <code>foo: bar</code>。如果这不是你所期望的，
更新工作负载的 <code>topologySpreadConstraints[*].labelSelector</code> 以匹配 Pod 模板中的标签。</li></ul><h2 id=cluster-level-default-constraints>集群级别的默认约束</h2><p>为集群设置默认的拓扑分布约束也是可能的。默认拓扑分布约束在且仅在以下条件满足时才会被应用到 Pod 上：</p><ul><li>Pod 没有在其 <code>.spec.topologySpreadConstraints</code> 中定义任何约束。</li><li>Pod 隶属于某个 Service、ReplicaSet、StatefulSet 或 ReplicationController。</li></ul><p>默认约束可以设置为<a href=/zh-cn/docs/reference/scheduling/config/#profiles>调度方案</a>中
<code>PodTopologySpread</code> 插件参数的一部分。约束的设置采用<a href=#topologyspreadconstraints-field>如前所述的 API</a>，
只是 <code>labelSelector</code> 必须为空。
选择算符是根据 Pod 所属的 Service、ReplicaSet、StatefulSet 或 ReplicationController 来设置的。</p><p>配置的示例可能看起来像下面这个样子：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PodTopologySpread<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultingType</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>默认配置下，<a href=/zh-cn/docs/reference/scheduling/config/#scheduling-plugins><code>SelectorSpread</code> 插件</a>是被禁用的。
Kubernetes 项目建议使用 <code>PodTopologySpread</code> 以执行类似行为。</div><h3 id=internal-default-constraints>内置默认约束</h3><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.24 [stable]</code></div><p>如果你没有为 Pod 拓扑分布配置任何集群级别的默认约束，
kube-scheduler 的行为就像你指定了以下默认拓扑约束一样：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;topology.kubernetes.io/zone&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span></code></pre></div><p>此外，原来用于提供等同行为的 <code>SelectorSpread</code> 插件默认被禁用。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>对于分布约束中所指定的拓扑键而言，<code>PodTopologySpread</code> 插件不会为不包含这些拓扑键的节点评分。
这可能导致在使用默认拓扑约束时，其行为与原来的 <code>SelectorSpread</code> 插件的默认行为不同。</p><p>如果你的节点不会 <strong>同时</strong> 设置 <code>kubernetes.io/hostname</code> 和 <code>topology.kubernetes.io/zone</code> 标签，
你应该定义自己的约束而不是使用 Kubernetes 的默认约束。</p></div><p>如果你不想为集群使用默认的 Pod 分布约束，你可以通过设置 <code>defaultingType</code> 参数为 <code>List</code>，
并将 <code>PodTopologySpread</code> 插件配置中的 <code>defaultConstraints</code> 参数置空来禁用默认 Pod 分布约束：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PodTopologySpread<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb> </span>[]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultingType</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=comparison-with-podaffinity-podantiaffinity>比较 podAffinity 和 podAntiAffinity</h2><p>在 Kubernetes 中，
<a href=/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>Pod 间亲和性和反亲和性</a>控制
Pod 彼此的调度方式（更密集或更分散）。</p><dl><dt><code>podAffinity</code></dt><dd>吸引 Pod；你可以尝试将任意数量的 Pod 集中到符合条件的拓扑域中。</dd><dt><code>podAntiAffinity</code></dt><dd>驱逐 Pod。如果将此设为 <code>requiredDuringSchedulingIgnoredDuringExecution</code> 模式，
则只有单个 Pod 可以调度到单个拓扑域；如果你选择 <code>preferredDuringSchedulingIgnoredDuringExecution</code>，
则你将丢失强制执行此约束的能力。</dd></dl><p>要实现更细粒度的控制，你可以设置拓扑分布约束来将 Pod 分布到不同的拓扑域下，从而实现高可用性或节省成本。
这也有助于工作负载的滚动更新和平稳地扩展副本规模。</p><p>有关详细信息，请参阅有关 Pod 拓扑分布约束的增强倡议的
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation>动机</a>一节。</p><h2 id=known-limitations>已知局限性</h2><ul><li><p>当 Pod 被移除时，无法保证约束仍被满足。例如，缩减某 Deployment 的规模时，Pod 的分布可能不再均衡。</p><p>你可以使用 <a href=https://github.com/kubernetes-sigs/descheduler>Descheduler</a> 来重新实现 Pod 分布的均衡。</p></li><li><p>具有污点的节点上匹配的 Pod 也会被统计。
参考 <a href=https://github.com/kubernetes/kubernetes/issues/80921>Issue 80921</a>。</p></li></ul><ul><li><p>该调度器不会预先知道集群拥有的所有可用区和其他拓扑域。
拓扑域由集群中存在的节点确定。在自动扩缩的集群中，如果一个节点池（或节点组）的节点数量缩减为零，
而用户正期望其扩容时，可能会导致调度出现问题。
因为在这种情况下，调度器不会考虑这些拓扑域，因为其中至少有一个节点。</p><p>你可以通过使用感知 Pod 拓扑分布约束并感知整个拓扑域集的集群自动扩缩工具来解决此问题。</p></li></ul><h2 id=接下来>接下来</h2><ul><li>博客：<a href=/blog/2020/05/introducing-podtopologyspread/>PodTopologySpread 介绍</a>详细解释了 <code>maxSkew</code>，
并给出了一些进阶的使用示例。</li><li>阅读针对 Pod 的 API
参考的<a href=/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling>调度</a>一节。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ede4960b56a3529ee0bfe7c8fe2d09a5>5 - 污点和容忍度</h1><p><a href=/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>节点亲和性</a>
是 <a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> 的一种属性，它使 Pod
被吸引到一类特定的<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>
（这可能出于一种偏好，也可能是硬性要求）。
<strong>污点（Taint）</strong> 则相反——它使节点能够排斥一类特定的 Pod。</p><p><strong>容忍度（Toleration）</strong> 是应用于 Pod 上的。容忍度允许调度器调度带有对应污点的 Pod。
容忍度允许调度但并不保证调度：作为其功能的一部分，
调度器也会<a href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/>评估其他参数</a>。</p><p>污点和容忍度（Toleration）相互配合，可以用来避免 Pod 被分配到不合适的节点上。
每个节点上都可以应用一个或多个污点，这表示对于那些不能容忍这些污点的 Pod，
是不会被该节点接受的。</p><h2 id=concepts>概念</h2><p>你可以使用命令 <a href=/docs/reference/generated/kubectl/kubectl-commands#taint>kubectl taint</a> 给节点增加一个污点。比如，</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</span></span></code></pre></div><p>给节点 <code>node1</code> 增加一个污点，它的键名是 <code>key1</code>，键值是 <code>value1</code>，效果是 <code>NoSchedule</code>。
这表示只有拥有和这个污点相匹配的容忍度的 Pod 才能够被分配到 <code>node1</code> 这个节点。</p><p>若要移除上述命令所添加的污点，你可以执行：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule-
</span></span></code></pre></div><p>你可以在 Pod 规约中为 Pod 设置容忍度。
下面两个容忍度均与上面例子中使用 <code>kubectl taint</code> 命令创建的污点相匹配，
因此如果一个 Pod 拥有其中的任何一个容忍度，都能够被调度到 <code>node1</code> ：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>这里是一个使用了容忍度的 Pod：</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/zh-cn/examples/pods/pod-with-toleration.yaml download=pods/pod-with-toleration.yaml><code>pods/pod-with-toleration.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-toleration-yaml")' title="Copy pods/pod-with-toleration.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-toleration-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example-key&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p><code>operator</code> 的默认值是 <code>Equal</code>。</p><p>一个容忍度和一个污点相“匹配”是指它们有一样的键名和效果，并且：</p><ul><li>如果 <code>operator</code> 是 <code>Exists</code> （此时容忍度不能指定 <code>value</code>），或者</li><li>如果 <code>operator</code> 是 <code>Equal</code> ，则它们的 <code>value</code> 应该相等</li></ul><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>存在两种特殊情况：</p><p>如果一个容忍度的 <code>key</code> 为空且 <code>operator</code> 为 <code>Exists</code>，
表示这个容忍度与任意的 key、value 和 effect 都匹配，即这个容忍度能容忍任何污点。</p><p>如果 <code>effect</code> 为空，则可以与所有键名 <code>key1</code> 的效果相匹配。</p></div><p>上述例子中 <code>effect</code> 使用的值为 <code>NoSchedule</code>，你也可以使用另外一个值 <code>PreferNoSchedule</code>。
这是“优化”或“软”版本的 <code>NoSchedule</code> —— 系统会 <strong>尽量</strong> 避免将 Pod 调度到存在其不能容忍污点的节点上，
但这不是强制的。<code>effect</code> 的值还可以设置为 <code>NoExecute</code>，下文会详细描述这个值。</p><p>你可以给一个节点添加多个污点，也可以给一个 Pod 添加多个容忍度设置。
Kubernetes 处理多个污点和容忍度的过程就像一个过滤器：从一个节点的所有污点开始遍历，
过滤掉那些 Pod 中存在与之相匹配的容忍度的污点。余下未被过滤的污点的 effect 值决定了
Pod 是否会被分配到该节点。需要注意以下情况：</p><ul><li>如果未被忽略的污点中存在至少一个 effect 值为 <code>NoSchedule</code> 的污点，
则 Kubernetes 不会将 Pod 调度到该节点。</li><li>如果未被忽略的污点中不存在 effect 值为 <code>NoSchedule</code> 的污点，
但是存在至少一个 effect 值为 <code>PreferNoSchedule</code> 的污点，
则 Kubernetes 会 <strong>尝试</strong> 不将 Pod 调度到该节点。</li><li>如果未被忽略的污点中存在至少一个 effect 值为 <code>NoExecute</code> 的污点，
则 Kubernetes 不会将 Pod 调度到该节点（如果 Pod 还未在节点上运行），
或者将 Pod 从该节点驱逐（如果 Pod 已经在节点上运行）。</li></ul><p>例如，假设你给一个节点添加了如下污点</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</span></span><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoExecute
</span></span><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key2</span><span style=color:#666>=</span>value2:NoSchedule
</span></span></code></pre></div><p>假定某个 Pod 有两个容忍度：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>在这种情况下，上述 Pod 不会被调度到上述节点，因为其没有容忍度和第三个污点相匹配。
但是如果在给节点添加上述污点之前，该 Pod 已经在上述节点运行，
那么它还可以继续运行在该节点上，因为第三个污点是三个污点中唯一不能被这个 Pod 容忍的。</p><p>通常情况下，如果给一个节点添加了一个 effect 值为 <code>NoExecute</code> 的污点，
则任何不能忍受这个污点的 Pod 都会马上被驱逐，任何可以忍受这个污点的 Pod 都不会被驱逐。
但是，如果 Pod 存在一个 effect 值为 <code>NoExecute</code> 的容忍度指定了可选属性
<code>tolerationSeconds</code> 的值，则表示在给节点添加了上述污点之后，
Pod 还能继续在节点上运行的时间。例如，</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>3600</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>这表示如果这个 Pod 正在运行，同时一个匹配的污点被添加到其所在的节点，
那么 Pod 还将继续在节点上运行 3600 秒，然后被驱逐。
如果在此之前上述污点被删除了，则 Pod 不会被驱逐。</p><h2 id=example-use-cases>使用例子</h2><p>通过污点和容忍度，可以灵活地让 Pod <strong>避开</strong>某些节点或者将 Pod 从某些节点驱逐。
下面是几个使用例子：</p><ul><li><strong>专用节点</strong>：如果想将某些节点专门分配给特定的一组用户使用，你可以给这些节点添加一个污点（即，
<code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>），
然后给这组用户的 Pod 添加一个相对应的容忍度
（通过编写一个自定义的<a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>，
很容易就能做到）。
拥有上述容忍度的 Pod 就能够被调度到上述专用节点，同时也能够被调度到集群中的其它节点。
如果你希望这些 Pod 只能被调度到上述专用节点，
那么你还需要给这些专用节点另外添加一个和上述污点类似的 label （例如：<code>dedicated=groupName</code>），
同时还要在上述准入控制器中给 Pod 增加节点亲和性要求，要求上述 Pod 只能被调度到添加了
<code>dedicated=groupName</code> 标签的节点上。</li></ul><ul><li><strong>配备了特殊硬件的节点</strong>：在部分节点配备了特殊硬件（比如 GPU）的集群中，
我们希望不需要这类硬件的 Pod 不要被调度到这些特殊节点，以便为后继需要这类硬件的 Pod 保留资源。
要达到这个目的，可以先给配备了特殊硬件的节点添加污点
（例如 <code>kubectl taint nodes nodename special=true:NoSchedule</code> 或
<code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>)，
然后给使用了这类特殊硬件的 Pod 添加一个相匹配的容忍度。
和专用节点的例子类似，添加这个容忍度的最简单的方法是使用自定义
<a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/>准入控制器</a>。
比如，我们推荐使用<a href=/zh-cn/docs/concepts/configuration/manage-resources-containers/#extended-resources>扩展资源</a>
来表示特殊硬件，给配置了特殊硬件的节点添加污点时包含扩展资源名称，
然后运行一个 <a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration>ExtendedResourceToleration</a>
准入控制器。此时，因为节点已经被设置污点了，没有对应容忍度的 Pod 不会被调度到这些节点。
但当你创建一个使用了扩展资源的 Pod 时，<code>ExtendedResourceToleration</code> 准入控制器会自动给
Pod 加上正确的容忍度，这样 Pod 就会被自动调度到这些配置了特殊硬件的节点上。
这种方式能够确保配置了特殊硬件的节点专门用于运行需要这些硬件的 Pod，
并且你无需手动给这些 Pod 添加容忍度。</li></ul><ul><li><strong>基于污点的驱逐</strong>: 这是在每个 Pod 中配置的在节点出现问题时的驱逐行为，
接下来的章节会描述这个特性。</li></ul><h2 id=taint-based-evictions>基于污点的驱逐</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.18 [stable]</code></div><p>前文提到过污点的效果值 <code>NoExecute</code> 会影响已经在节点上运行的 Pod，如下</p><ul><li>如果 Pod 不能忍受这类污点，Pod 会马上被驱逐。</li><li>如果 Pod 能够忍受这类污点，但是在容忍度定义中没有指定 <code>tolerationSeconds</code>，
则 Pod 还会一直在这个节点上运行。</li><li>如果 Pod 能够忍受这类污点，而且指定了 <code>tolerationSeconds</code>，
则 Pod 还能在这个节点上继续运行这个指定的时间长度。</li></ul><p>当某种条件为真时，节点控制器会自动给节点添加一个污点。当前内置的污点包括：</p><ul><li><code>node.kubernetes.io/not-ready</code>：节点未准备好。这相当于节点状况 <code>Ready</code> 的值为 "<code>False</code>"。</li><li><code>node.kubernetes.io/unreachable</code>：节点控制器访问不到节点. 这相当于节点状况 <code>Ready</code>
的值为 "<code>Unknown</code>"。</li><li><code>node.kubernetes.io/memory-pressure</code>：节点存在内存压力。</li><li><code>node.kubernetes.io/disk-pressure</code>：节点存在磁盘压力。</li><li><code>node.kubernetes.io/pid-pressure</code>: 节点的 PID 压力。</li><li><code>node.kubernetes.io/network-unavailable</code>：节点网络不可用。</li><li><code>node.kubernetes.io/unschedulable</code>: 节点不可调度。</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>：如果 kubelet 启动时指定了一个“外部”云平台驱动，
它将给当前节点添加一个污点将其标志为不可用。在 cloud-controller-manager
的一个控制器初始化这个节点后，kubelet 将删除这个污点。</li></ul><p>在节点被驱逐时，节点控制器或者 kubelet 会添加带有 <code>NoExecute</code> 效果的相关污点。
如果异常状态恢复正常，kubelet 或节点控制器能够移除相关的污点。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>控制面会限制向节点添加新污点的速率。这一速率限制可以管理多个节点同时不可达时
（例如出现网络中断的情况），可能触发的驱逐的数量。</div><p>你可以为 Pod 设置 <code>tolerationSeconds</code>，以指定当节点失效或者不响应时，
Pod 维系与该节点间绑定关系的时长。</p><p>比如，你可能希望在出现网络分裂事件时，对于一个与节点本地状态有着深度绑定的应用而言，
仍然停留在当前节点上运行一段较长的时间，以等待网络恢复以避免被驱逐。
你为这种 Pod 所设置的容忍度看起来可能是这样：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;node.kubernetes.io/unreachable&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>6000</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>Kubernetes 会自动给 Pod 添加针对 <code>node.kubernetes.io/not-ready</code> 和
<code>node.kubernetes.io/unreachable</code> 的容忍度，且配置 <code>tolerationSeconds=300</code>，
除非用户自身或者某控制器显式设置此容忍度。</p><p>这些自动添加的容忍度意味着 Pod 可以在检测到对应的问题之一时，在 5
分钟内保持绑定在该节点上。</p></div><p><a href=/zh-cn/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> 中的 Pod 被创建时，
针对以下污点自动添加的 <code>NoExecute</code> 的容忍度将不会指定 <code>tolerationSeconds</code>：</p><ul><li><code>node.kubernetes.io/unreachable</code></li><li><code>node.kubernetes.io/not-ready</code></li></ul><p>这保证了出现上述问题时 DaemonSet 中的 Pod 永远不会被驱逐。</p><h2 id=taint-nodes-by-condition>基于节点状态添加污点</h2><p>控制平面使用节点<a class=glossary-tooltip title='控制器通过 API 服务器监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/controller/ target=_blank aria-label=控制器>控制器</a>自动创建
与<a href=/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions>节点状况</a>
对应的、效果为 <code>NoSchedule</code> 的污点。</p><p>调度器在进行调度时检查污点，而不是检查节点状况。这确保节点状况不会直接影响调度。
例如，如果 <code>DiskPressure</code> 节点状况处于活跃状态，则控制平面添加
<code>node.kubernetes.io/disk-pressure</code> 污点并且不会调度新的 Pod 到受影响的节点。
如果 <code>MemoryPressure</code> 节点状况处于活跃状态，则控制平面添加
<code>node.kubernetes.io/memory-pressure</code> 污点。</p><p>对于新创建的 Pod，可以通过添加相应的 Pod 容忍度来忽略节点状况。
控制平面还在具有除 <code>BestEffort</code> 之外的
<a class=glossary-tooltip title='QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-qos-class' target=_blank aria-label='QoS 类'>QoS 类</a>的 Pod 上添加
<code>node.kubernetes.io/memory-pressure</code> 容忍度。
这是因为 Kubernetes 将 <code>Guaranteed</code> 或 <code>Burstable</code> QoS 类中的 Pod（甚至没有设置内存请求的 Pod）
视为能够应对内存压力，而新创建的 <code>BestEffort</code> Pod 不会被调度到受影响的节点上。</p><p>DaemonSet 控制器自动为所有守护进程添加如下 <code>NoSchedule</code> 容忍度，以防 DaemonSet 崩溃：</p><ul><li><code>node.kubernetes.io/memory-pressure</code></li><li><code>node.kubernetes.io/disk-pressure</code></li><li><code>node.kubernetes.io/pid-pressure</code> (1.14 或更高版本)</li><li><code>node.kubernetes.io/unschedulable</code> (1.10 或更高版本)</li><li><code>node.kubernetes.io/network-unavailable</code> (<strong>只适合主机网络配置</strong>)</li></ul><p>添加上述容忍度确保了向后兼容，你也可以选择自由向 DaemonSet 添加容忍度。</p><h2 id=接下来>接下来</h2><ul><li>阅读<a href=/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/>节点压力驱逐</a>，
以及如何配置其行为</li><li>阅读 <a href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-602208c95fe7b1f1170310ce993f5814>6 - 调度框架</h1><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes 1.19 [stable]</code></div><p>调度框架是面向 Kubernetes 调度器的一种插件架构，
它为现有的调度器添加了一组新的“插件” API。插件会被编译到调度器之中。
这些 API 允许大多数调度功能以插件的形式实现，同时使调度“核心”保持简单且可维护。
请参考<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md>调度框架的设计提案</a>
获取框架设计的更多技术信息。</p><h1 id=框架工作流程>框架工作流程</h1><p>调度框架定义了一些扩展点。调度器插件注册后在一个或多个扩展点处被调用。
这些插件中的一些可以改变调度决策，而另一些仅用于提供信息。</p><p>每次调度一个 Pod 的尝试都分为两个阶段，即 <strong>调度周期</strong> 和 <strong>绑定周期</strong>。</p><h2 id=调度周期和绑定周期>调度周期和绑定周期</h2><p>调度周期为 Pod 选择一个节点，绑定周期将该决策应用于集群。
调度周期和绑定周期一起被称为“调度上下文”。</p><p>调度周期是串行运行的，而绑定周期可能是同时运行的。</p><p>如果确定 Pod 不可调度或者存在内部错误，则可以终止调度周期或绑定周期。
Pod 将返回队列并重试。</p><h2 id=扩展点>扩展点</h2><p>下图显示了一个 Pod 的调度上下文以及调度框架公开的扩展点。
在此图片中，“过滤器”等同于“断言”，“评分”相当于“优先级函数”。</p><p>一个插件可以在多个扩展点处注册，以执行更复杂或有状态的任务。</p><figure class=diagram-large><img src=/images/docs/scheduling-framework-extensions.png><figcaption><h4>调度框架扩展点</h4></figcaption></figure><h3 id=queue-sort>队列排序</h3><p>这些插件用于对调度队列中的 Pod 进行排序。
队列排序插件本质上提供 <code>less(Pod1, Pod2)</code> 函数。
一次只能启动一个队列插件。</p><h3 id=pre-filter>PreFilter</h3><p>这些插件用于预处理 Pod 的相关信息，或者检查集群或 Pod 必须满足的某些条件。
如果 PreFilter 插件返回错误，则调度周期将终止。</p><h3 id=filter>Filter</h3><p>这些插件用于过滤出不能运行该 Pod 的节点。对于每个节点，
调度器将按照其配置顺序调用这些过滤插件。如果任何过滤插件将节点标记为不可行，
则不会为该节点调用剩下的过滤插件。节点可以被同时进行评估。</p><h3 id=post-filter>PostFilter</h3><p>这些插件在 Filter 阶段后调用，但仅在该 Pod 没有可行的节点时调用。
插件按其配置的顺序调用。如果任何 PostFilter 插件标记节点为“Schedulable”，
则其余的插件不会调用。典型的 PostFilter 实现是抢占，试图通过抢占其他 Pod
的资源使该 Pod 可以调度。</p><h3 id=pre-score>PreScore</h3><p>这些插件用于执行 “前置评分（pre-scoring）” 工作，即生成一个可共享状态供 Score 插件使用。
如果 PreScore 插件返回错误，则调度周期将终止。</p><h3 id=scoring>Score</h3><p>这些插件用于对通过过滤阶段的节点进行排序。调度器将为每个节点调用每个评分插件。
将有一个定义明确的整数范围，代表最小和最大分数。
在<a href=#normalize-scoring>标准化评分</a>阶段之后，调度器将根据配置的插件权重
合并所有插件的节点分数。</p><h3 id=normalize-scoring>NormalizeScore</h3><p>这些插件用于在调度器计算 Node 排名之前修改分数。
在此扩展点注册的插件被调用时会使用同一插件的 <a href=#scoring>Score</a> 结果。
每个插件在每个调度周期调用一次。</p><p>例如，假设一个 <code>BlinkingLightScorer</code> 插件基于具有的闪烁指示灯数量来对节点进行排名。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>ScoreNode</span>(_ <span style=color:#666>*</span>v1.pod, n <span style=color:#666>*</span>v1.Node) (<span style=color:#0b0;font-weight:700>int</span>, <span style=color:#0b0;font-weight:700>error</span>) {
</span></span><span style=display:flex><span>   <span style=color:#a2f;font-weight:700>return</span> <span style=color:#00a000>getBlinkingLightCount</span>(n)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>然而，最大的闪烁灯个数值可能比 <code>NodeScoreMax</code> 小。要解决这个问题，
<code>BlinkingLightScorer</code> 插件还应该注册该扩展点。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>NormalizeScores</span>(scores <span style=color:#a2f;font-weight:700>map</span>[<span style=color:#0b0;font-weight:700>string</span>]<span style=color:#0b0;font-weight:700>int</span>) {
</span></span><span style=display:flex><span>   highest <span style=color:#666>:=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>   <span style=color:#a2f;font-weight:700>for</span> _, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
</span></span><span style=display:flex><span>      highest = <span style=color:#00a000>max</span>(highest, score)
</span></span><span style=display:flex><span>   }
</span></span><span style=display:flex><span>   <span style=color:#a2f;font-weight:700>for</span> node, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
</span></span><span style=display:flex><span>      scores[node] = score<span style=color:#666>*</span>NodeScoreMax<span style=color:#666>/</span>highest
</span></span><span style=display:flex><span>   }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>如果任何 NormalizeScore 插件返回错误，则调度阶段将终止。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong> 希望执行“预保留”工作的插件应该使用 NormalizeScore 扩展点。</div><h3 id=reserve>Reserve</h3><p>Reserve 是一个信息性的扩展点。
管理运行时状态的插件（也成为“有状态插件”）应该使用此扩展点，以便
调度器在节点给指定 Pod 预留了资源时能够通知该插件。
这是在调度器真正将 Pod 绑定到节点之前发生的，并且它存在是为了防止
在调度器等待绑定成功时发生竞争情况。</p><p>这个是调度周期的最后一步。
一旦 Pod 处于保留状态，它将在绑定周期结束时触发 <a href=#unreserve>Unreserve</a> 插件
（失败时）或 <a href=#post-bind>PostBind</a> 插件（成功时）。</p><h3 id=permit>Permit</h3><p><em>Permit</em> 插件在每个 Pod 调度周期的最后调用，用于防止或延迟 Pod 的绑定。
一个允许插件可以做以下三件事之一：</p><ol><li><strong>批准</strong><br>一旦所有 Permit 插件批准 Pod 后，该 Pod 将被发送以进行绑定。</li></ol><ol><li><strong>拒绝</strong><br>如果任何 Permit 插件拒绝 Pod，则该 Pod 将被返回到调度队列。
这将触发<a href=#unreserve>Unreserve</a> 插件。</li></ol><ol><li><strong>等待</strong>（带有超时）<br>如果一个 Permit 插件返回 “等待” 结果，则 Pod 将保持在一个内部的 “等待中”
的 Pod 列表，同时该 Pod 的绑定周期启动时即直接阻塞直到得到
<a href=#frameworkhandle>批准</a>。如果超时发生，<strong>等待</strong> 变成 <strong>拒绝</strong>，并且 Pod
将返回调度队列，从而触发 <a href=#unreserve>Unreserve</a> 插件。</li></ol><div class="alert alert-info note callout" role=alert><strong>说明：</strong> 尽管任何插件可以访问 “等待中” 状态的 Pod 列表并批准它们
(查看 <a href=https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle><code>FrameworkHandle</code></a>)。
我们期望只有允许插件可以批准处于 “等待中” 状态的预留 Pod 的绑定。
一旦 Pod 被批准了，它将发送到 <a href=#pre-bind>PreBind</a> 阶段。</div><h3 id=pre-bind>PreBind</h3><p>这些插件用于执行 Pod 绑定前所需的所有工作。
例如，一个 PreBind 插件可能需要制备网络卷并且在允许 Pod 运行在该节点之前
将其挂载到目标节点上。</p><p>如果任何 PreBind 插件返回错误，则 Pod 将被 <a href=#unreserve>拒绝</a> 并且
退回到调度队列中。</p><h3 id=bind>Bind</h3><p>Bind 插件用于将 Pod 绑定到节点上。直到所有的 PreBind 插件都完成，Bind 插件才会被调用。
各 Bind 插件按照配置顺序被调用。Bind 插件可以选择是否处理指定的 Pod。
如果某 Bind 插件选择处理某 Pod，<strong>剩余的 Bind 插件将被跳过</strong>。</p><h3 id=post-bind>PostBind</h3><p>这是个信息性的扩展点。
PostBind 插件在 Pod 成功绑定后被调用。这是绑定周期的结尾，可用于清理相关的资源。</p><h3 id=unreserve>Unreserve</h3><p>这是个信息性的扩展点。
如果 Pod 被保留，然后在后面的阶段中被拒绝，则 Unreserve 插件将被通知。
Unreserve 插件应该清楚保留 Pod 的相关状态。</p><p>使用此扩展点的插件通常也使用 <a href=#reserve>Reserve</a>。</p><h2 id=插件-api>插件 API</h2><p>插件 API 分为两个步骤。首先，插件必须完成注册并配置，然后才能使用扩展点接口。
扩展点接口具有以下形式。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>type</span> Plugin <span style=color:#a2f;font-weight:700>interface</span> {
</span></span><span style=display:flex><span>   <span style=color:#00a000>Name</span>() <span style=color:#0b0;font-weight:700>string</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>type</span> QueueSortPlugin <span style=color:#a2f;font-weight:700>interface</span> {
</span></span><span style=display:flex><span>   Plugin
</span></span><span style=display:flex><span>   <span style=color:#00a000>Less</span>(<span style=color:#666>*</span>v1.pod, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>bool</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>type</span> PreFilterPlugin <span style=color:#a2f;font-weight:700>interface</span> {
</span></span><span style=display:flex><span>   Plugin
</span></span><span style=display:flex><span>   <span style=color:#00a000>PreFilter</span>(context.Context, <span style=color:#666>*</span>framework.CycleState, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>error</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>// ...
</span></span></span></code></pre></div><h1 id=插件配置>插件配置</h1><p>你可以在调度器配置中启用或禁用插件。
如果你在使用 Kubernetes v1.18 或更高版本，大部分调度
<a href=/zh-cn/docs/reference/scheduling/config/#scheduling-plugins>插件</a>
都在使用中且默认启用。</p><p>除了默认的插件，你还可以实现自己的调度插件并且将它们与默认插件一起配置。
你可以访问 <a href=https://github.com/kubernetes-sigs/scheduler-plugins>scheduler-plugins</a>
了解更多信息。</p><p>如果你正在使用 Kubernetes v1.18 或更高版本，你可以将一组插件设置为
一个调度器配置文件，然后定义不同的配置文件来满足各类工作负载。
了解更多关于<a href=/zh-cn/docs/reference/scheduling/config/#multiple-profiles>多配置文件</a>。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d9574a30fcbc631b0d2a57850e161e89>7 - 调度器性能调优</h1><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes 1.14 [beta]</code></div><p>作为 kubernetes 集群的默认调度器，
<a href=/zh-cn/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler>kube-scheduler</a>
主要负责将 Pod 调度到集群的 Node 上。</p><p>在一个集群中，满足一个 Pod 调度请求的所有 Node 称之为 <em>可调度</em> Node。
调度器先在集群中找到一个 Pod 的可调度 Node，然后根据一系列函数对这些可调度 Node 打分，
之后选出其中得分最高的 Node 来运行 Pod。
最后，调度器将这个调度决定告知 kube-apiserver，这个过程叫做 <em>绑定（Binding）</em>。</p><p>这篇文章将会介绍一些在大规模 Kubernetes 集群下调度器性能优化的方式。</p><p>在大规模集群中，你可以调节调度器的表现来平衡调度的延迟（新 Pod 快速就位）
和精度（调度器很少做出糟糕的放置决策）。</p><p>你可以通过设置 kube-scheduler 的 <code>percentageOfNodesToScore</code> 来配置这个调优设置。
这个 KubeSchedulerConfiguration 设置决定了调度集群中节点的阈值。</p><h3 id=设置阈值>设置阈值</h3><p><code>percentageOfNodesToScore</code> 选项接受从 0 到 100 之间的整数值。
0 值比较特殊，表示 kube-scheduler 应该使用其编译后的默认值。
如果你设置 <code>percentageOfNodesToScore</code> 的值超过了 100，
kube-scheduler 的表现等价于设置值为 100。</p><p>要修改这个值，先编辑 <a href=/zh-cn/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler 的配置文件</a>
然后重启调度器。
大多数情况下，这个配置文件是 <code>/etc/kubernetes/config/kube-scheduler.yaml</code>。</p><p>修改完成后，你可以执行</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -n kube-system | grep kube-scheduler
</span></span></code></pre></div><p>来检查该 kube-scheduler 组件是否健康。</p><h2 id=percentage-of-nodes-to-score>节点打分阈值</h2><p>要提升调度性能，kube-scheduler 可以在找到足够的可调度节点之后停止查找。
在大规模集群中，比起考虑每个节点的简单方法相比可以节省时间。</p><p>你可以使用整个集群节点总数的百分比作为阈值来指定需要多少节点就足够。
kube-scheduler 会将它转换为节点数的整数值。在调度期间，如果
kube-scheduler 已确认的可调度节点数足以超过了配置的百分比数量，
kube-scheduler 将停止继续查找可调度节点并继续进行
<a href=/zh-cn/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation>打分阶段</a>。</p><p><a href=#how-the-scheduler-iterates-over-nodes>调度器如何遍历节点</a> 详细介绍了这个过程。</p><h3 id=默认阈值>默认阈值</h3><p>如果你不指定阈值，Kubernetes 使用线性公式计算出一个比例，在 100-节点集群
下取 50%，在 5000-节点的集群下取 10%。这个自动设置的参数的最低值是 5%。</p><p>这意味着，调度器至少会对集群中 5% 的节点进行打分，除非用户将该参数设置的低于 5。</p><p>如果你想让调度器对集群内所有节点进行打分，则将 <code>percentageOfNodesToScore</code> 设置为 100。</p><h2 id=示例>示例</h2><p>下面就是一个将 <code>percentageOfNodesToScore</code> 参数设置为 50% 的例子。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1alpha1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>algorithmSource</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>provider</span>:<span style=color:#bbb> </span>DefaultProvider<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>percentageOfNodesToScore</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=调节-percentageofnodestoscore-参数>调节 percentageOfNodesToScore 参数</h3><p><code>percentageOfNodesToScore</code> 的值必须在 1 到 100 之间，而且其默认值是通过集群的规模计算得来的。
另外，还有一个 50 个 Node 的最小值是硬编码在程序中。</p><p>值得注意的是，该参数设置后可能会导致只有集群中少数节点被选为可调度节点，
很多节点都没有进入到打分阶段。这样就会造成一种后果，
一个本来可以在打分阶段得分很高的节点甚至都不能进入打分阶段。</p><p>由于这个原因，这个参数不应该被设置成一个很低的值。
通常的做法是不会将这个参数的值设置的低于 10。
很低的参数值一般在调度器的吞吐量很高且对节点的打分不重要的情况下才使用。
换句话说，只有当你更倾向于在可调度节点中任意选择一个节点来运行这个 Pod 时，
才使用很低的参数设置。</p><h3 id=how-the-scheduler-iterates-over-nodes>调度器做调度选择的时候如何覆盖所有的 Node</h3><p>如果你想要理解这一个特性的内部细节，那么请仔细阅读这一章节。</p><p>在将 Pod 调度到节点上时，为了让集群中所有节点都有公平的机会去运行这些 Pod，
调度器将会以轮询的方式覆盖全部的 Node。
你可以将 Node 列表想象成一个数组。调度器从数组的头部开始筛选可调度节点，
依次向后直到可调度节点的数量达到 <code>percentageOfNodesToScore</code> 参数的要求。
在对下一个 Pod 进行调度的时候，前一个 Pod 调度筛选停止的 Node 列表的位置，
将会来作为这次调度筛选 Node 开始的位置。</p><p>如果集群中的 Node 在多个区域，那么调度器将从不同的区域中轮询 Node，
来确保不同区域的 Node 接受可调度性检查。如下例，考虑两个区域中的六个节点：</p><pre tabindex=0><code>Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
</code></pre><p>调度器将会按照如下的顺序去评估 Node 的可调度性：</p><pre tabindex=0><code>Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
</code></pre><p>在评估完所有 Node 后，将会返回到 Node 1，从头开始。</p><h2 id=接下来>接下来</h2><ul><li>参见 <a href=/zh-cn/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler 配置参考 (v1beta3)</a></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-961126cd43559012893979e568396a49>8 - 扩展资源的资源装箱</h1><p>在 kube-scheduler 的<a href=/zh-cn/docs/reference/scheduling/config/#scheduling-plugins>调度插件</a>
<code>NodeResourcesFit</code> 中存在两种支持资源装箱（bin packing）的策略：<code>MostAllocated</code> 和
<code>RequestedToCapacityRatio</code>。</p><h2 id=enabling-bin-packing-using-mostallocated-strategy>使用 MostAllocated 策略启用资源装箱</h2><p><code>MostAllocated</code> 策略基于资源的利用率来为节点计分，优选分配比率较高的节点。
针对每种资源类型，你可以设置一个权重值以改变其对节点得分的影响。</p><p>要为插件 <code>NodeResourcesFit</code> 设置 <code>MostAllocated</code> 策略，
可以使用一个类似于下面这样的<a href=/zh-cn/docs/reference/scheduling/config/>调度器配置</a>：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>scoringStrategy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>MostAllocated<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeResourcesFit<span style=color:#bbb>
</span></span></span></code></pre></div><p>要进一步了解其它参数及其默认配置，请参阅
<a href=/zh-cn/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-NodeResourcesFitArgs><code>NodeResourcesFitArgs</code></a>
的 API 文档。</p><h2 id=enabling-bin-packing-using-requestedtocapacityratio>使用 RequestedToCapacityRatio 策略来启用资源装箱</h2><p><code>RequestedToCapacityRatio</code> 策略允许用户基于请求值与容量的比率，针对参与节点计分的每类资源设置权重。
这一策略使得用户可以使用合适的参数来对扩展资源执行装箱操作，进而提升大规模集群中稀有资源的利用率。
此策略根据所分配资源的一个配置函数来评价节点。
<code>NodeResourcesFit</code> 计分函数中的 <code>RequestedToCapacityRatio</code> 可以通过字段
<a href=/zh-cn/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-ScoringStrategy>scoringStrategy</a>
来控制。
在 <code>scoringStrategy</code> 字段中，你可以配置两个参数：<code>requestedToCapacityRatio</code>
和 <code>resources</code>。<code>requestedToCapacityRatio</code> 参数中的 <code>shape</code>
设置使得用户能够调整函数的算法，基于 <code>utilization</code> 和 <code>score</code> 值计算最少请求或最多请求。
<code>resources</code> 参数中包含计分过程中需要考虑的资源的 <code>name</code>，以及用来设置每种资源权重的 <code>weight</code>。</p><p>下面是一个配置示例，使用 <code>requestedToCapacityRatio</code> 字段为扩展资源 <code>intel.com/foo</code>
和 <code>intel.com/bar</code> 设置装箱行为：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>scoringStrategy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>requestedToCapacityRatio</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>RequestedToCapacityRatio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeResourcesFit<span style=color:#bbb>
</span></span></span></code></pre></div><p>使用 kube-scheduler 标志 <code>--config=/path/to/config/file</code>
引用 <code>KubeSchedulerConfiguration</code> 文件，可以将配置传递给调度器。</p><p>要进一步了解其它参数及其默认配置，可以参阅
<a href=/zh-cn/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-NodeResourcesFitArgs><code>NodeResourcesFitArgs</code></a>
的 API 文档。</p><h3 id=tuning-the-score-function>调整计分函数</h3><p><code>shape</code> 用于指定 <code>RequestedToCapacityRatio</code> 函数的行为。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>上面的参数在 <code>utilization</code> 为 0% 时给节点评分为 0，在 <code>utilization</code> 为
100% 时给节点评分为 10，因此启用了装箱行为。
要启用最少请求（least requested）模式，必须按如下方式反转得分值。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb> </span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span></code></pre></div><p><code>resources</code> 是一个可选参数，默认情况下设置为：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>它可以像下面这样用来添加扩展资源：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span></code></pre></div><p><code>weight</code> 参数是可选的，如果未指定，则设置为 1。
同时，<code>weight</code> 不能设置为负值。</p><h3 id=node-scoring-for-capacity-allocation>节点容量分配的评分</h3><p>本节适用于希望了解此功能的内部细节的人员。
以下是如何针对给定的一组值来计算节点得分的示例。</p><p>请求的资源：</p><pre tabindex=0><code>intel.com/foo : 2
memory: 256MB
cpu: 2
</code></pre><p>资源权重：</p><pre tabindex=0><code>intel.com/foo : 5
memory: 1
cpu: 3
</code></pre><pre tabindex=0><code>FunctionShapePoint {{0, 0}, {100, 10}}
</code></pre><p>节点 1 配置：</p><pre tabindex=0><code>可用：
  intel.com/foo : 4
  memory : 1 GB
  cpu: 8

已用：
  intel.com/foo: 1
  memory: 256MB
  cpu: 1
</code></pre><p>节点得分：</p><pre tabindex=0><code>intel.com/foo  = resourceScoringFunction((2+1),4)
               = (100 - ((4-3)*100/4)
               = (100 - 25)
               = 75                       # requested + used = 75% * available
               = rawScoringFunction(75) 
               = 7                        # floor(75/10) 

memory         = resourceScoringFunction((256+256),1024)
               = (100 -((1024-512)*100/1024))
               = 50                       # requested + used = 50% * available
               = rawScoringFunction(50)
               = 5                        # floor(50/10)

cpu            = resourceScoringFunction((2+1),8)
               = (100 -((8-3)*100/8))
               = 37.5                     # requested + used = 37.5% * available
               = rawScoringFunction(37.5)
               = 3                        # floor(37.5/10)

NodeScore   =  (7 * 5) + (5 * 1) + (3 * 3) / (5 + 1 + 3)
            =  5
</code></pre><p>节点 2 配置：</p><pre tabindex=0><code>可用：
  intel.com/foo: 8
  memory: 1GB
  cpu: 8

已用：
  intel.com/foo: 2
  memory: 512MB
  cpu: 6
</code></pre><p>节点得分：</p><pre tabindex=0><code>intel.com/foo  = resourceScoringFunction((2+2),8)
               = (100 - ((8-4)*100/8)
               = (100 - 50)
               = 50
               = rawScoringFunction(50)
               = 5

memory         = resourceScoringFunction((256+512),1024)
               = (100 -((1024-768)*100/1024))
               = 75
               = rawScoringFunction(75)
               = 7

cpu            = resourceScoringFunction((2+6),8)
               = (100 -((8-8)*100/8))
               = 100
               = rawScoringFunction(100)
               = 10

NodeScore   =  (5 * 5) + (7 * 1) + (10 * 3) / (5 + 1 + 3)
            =  7
</code></pre><h2 id=接下来>接下来</h2><ul><li>继续阅读<a href=/zh-cn/docs/concepts/scheduling-eviction/scheduling-framework/>调度器框架</a></li><li>继续阅读<a href=/zh-cn/docs/reference/scheduling/config/>调度器配置</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-60e5a2861609e0848d58ce8bf99c4a31>9 - Pod 优先级和抢占</h1><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.14 [stable]</code></div><p><a href=/zh-cn/docs/concepts/workloads/pods/>Pod</a> 可以有<strong>优先级</strong>。
优先级表示一个 Pod 相对于其他 Pod 的重要性。
如果一个 Pod 无法被调度，调度程序会尝试抢占（驱逐）较低优先级的 Pod，
以使悬决 Pod 可以被调度。</p><div class="alert alert-danger warning callout" role=alert><strong>警告：</strong><p>在一个并非所有用户都是可信的集群中，恶意用户可能以最高优先级创建 Pod，
导致其他 Pod 被驱逐或者无法被调度。
管理员可以使用 ResourceQuota 来阻止用户创建高优先级的 Pod。
参见<a href=/zh-cn/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default>默认限制优先级消费</a>。</div><h2 id=how-to-use-priority-and-preemption>如何使用优先级和抢占</h2><p>要使用优先级和抢占：</p><ol><li><p>新增一个或多个 <a href=#priorityclass>PriorityClass</a>。</p></li><li><p>创建 Pod，并将其 <a href=#pod-priority><code>priorityClassName</code></a> 设置为新增的 PriorityClass。
当然你不需要直接创建 Pod；通常，你将会添加 <code>priorityClassName</code> 到集合对象（如 Deployment）
的 Pod 模板中。</p></li></ol><p>继续阅读以获取有关这些步骤的更多信息。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>Kubernetes 已经提供了 2 个 PriorityClass：
<code>system-cluster-critical</code> 和 <code>system-node-critical</code>。
这些是常见的类，用于<a href=/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/>确保始终优先调度关键组件</a>。</div><h2 id=priorityclass>PriorityClass</h2><p>PriorityClass 是一个无命名空间对象，它定义了从优先级类名称到优先级整数值的映射。
名称在 PriorityClass 对象元数据的 <code>name</code> 字段中指定。
值在必填的 <code>value</code> 字段中指定。值越大，优先级越高。
PriorityClass 对象的名称必须是有效的
<a href=/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS 子域名</a>，
并且它不能以 <code>system-</code> 为前缀。</p><p>PriorityClass 对象可以设置任何小于或等于 10 亿的 32 位整数值。
较大的数字是为通常不应被抢占或驱逐的关键的系统 Pod 所保留的。
集群管理员应该为这类映射分别创建独立的 PriorityClass 对象。</p><p>PriorityClass 还有两个可选字段：<code>globalDefault</code> 和 <code>description</code>。
<code>globalDefault</code> 字段表示这个 PriorityClass 的值应该用于没有 <code>priorityClassName</code> 的 Pod。
系统中只能存在一个 <code>globalDefault</code> 设置为 true 的 PriorityClass。
如果不存在设置了 <code>globalDefault</code> 的 PriorityClass，
则没有 <code>priorityClassName</code> 的 Pod 的优先级为零。</p><p><code>description</code> 字段是一个任意字符串。
它用来告诉集群用户何时应该使用此 PriorityClass。</p><h3 id=notes-about-podpriority-and-existing-clusters>关于 PodPriority 和现有集群的注意事项</h3><ul><li><p>如果你升级一个已经存在的但尚未使用此特性的集群，该集群中已经存在的 Pod 的优先级等效于零。</p></li><li><p>添加一个将 <code>globalDefault</code> 设置为 <code>true</code> 的 PriorityClass 不会改变现有 Pod 的优先级。
此类 PriorityClass 的值仅用于添加 PriorityClass 后创建的 Pod。</p></li><li><p>如果你删除了某个 PriorityClass 对象，则使用被删除的 PriorityClass 名称的现有 Pod 保持不变，
但是你不能再创建使用已删除的 PriorityClass 名称的 Pod。</p></li></ul><h3 id=example-priorityclass>PriorityClass 示例</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>scheduling.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#666>1000000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>globalDefault</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>description</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;此优先级类应仅用于 XYZ 服务 Pod。&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=non-preempting-priority-class>非抢占式 PriorityClass</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.24 [stable]</code></div><p>配置了 <code>preemptionPolicy: Never</code> 的 Pod 将被放置在调度队列中较低优先级 Pod 之前，
但它们不能抢占其他 Pod。等待调度的非抢占式 Pod 将留在调度队列中，直到有足够的可用资源，
它才可以被调度。非抢占式 Pod，像其他 Pod 一样，受调度程序回退的影响。
这意味着如果调度程序尝试这些 Pod 并且无法调度它们，它们将以更低的频率被重试，
从而允许其他优先级较低的 Pod 排在它们之前。</p><p>非抢占式 Pod 仍可能被其他高优先级 Pod 抢占。</p><p><code>preemptionPolicy</code> 默认为 <code>PreemptLowerPriority</code>，
这将允许该 PriorityClass 的 Pod 抢占较低优先级的 Pod（现有默认行为也是如此）。
如果 <code>preemptionPolicy</code> 设置为 <code>Never</code>，则该 PriorityClass 中的 Pod 将是非抢占式的。</p><p>数据科学工作负载是一个示例用例。用户可以提交他们希望优先于其他工作负载的作业，
但不希望因为抢占运行中的 Pod 而导致现有工作被丢弃。
设置为 <code>preemptionPolicy: Never</code> 的高优先级作业将在其他排队的 Pod 之前被调度，
只要足够的集群资源“自然地”变得可用。</p><h3 id=example-non-preempting-priorityclass>非抢占式 PriorityClass 示例</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>scheduling.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority-nonpreempting<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#666>1000000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>preemptionPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>globalDefault</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>description</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;This priority class will not cause other pods to be preempted.&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=pod-priority>Pod 优先级</h2><p>在你拥有一个或多个 PriorityClass 对象之后，
你可以创建在其规约中指定这些 PriorityClass 名称之一的 Pod。
优先级准入控制器使用 <code>priorityClassName</code> 字段并填充优先级的整数值。
如果未找到所指定的优先级类，则拒绝 Pod。</p><p>以下 YAML 是 Pod 配置的示例，它使用在前面的示例中创建的 PriorityClass。
优先级准入控制器检查 Pod 规约并将其优先级解析为 1000000。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityClassName</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=effect-of-pod-priority-on-scheduling-order>Pod 优先级对调度顺序的影响</h3><p>当启用 Pod 优先级时，调度程序会按优先级对悬决 Pod 进行排序，
并且每个悬决的 Pod 会被放置在调度队列中其他优先级较低的悬决 Pod 之前。
因此，如果满足调度要求，较高优先级的 Pod 可能会比具有较低优先级的 Pod 更早调度。
如果无法调度此类 Pod，调度程序将继续并尝试调度其他较低优先级的 Pod。</p><h2 id=preemption>抢占</h2><p>Pod 被创建后会进入队列等待调度。
调度器从队列中挑选一个 Pod 并尝试将它调度到某个节点上。
如果没有找到满足 Pod 的所指定的所有要求的节点，则触发对悬决 Pod 的抢占逻辑。
让我们将悬决 Pod 称为 P。抢占逻辑试图找到一个节点，
在该节点中删除一个或多个优先级低于 P 的 Pod，则可以将 P 调度到该节点上。
如果找到这样的节点，一个或多个优先级较低的 Pod 会被从节点中驱逐。
被驱逐的 Pod 消失后，P 可以被调度到该节点上。</p><h3 id=user-exposed-information>用户暴露的信息</h3><p>当 Pod P 抢占节点 N 上的一个或多个 Pod 时，
Pod P 状态的 <code>nominatedNodeName</code> 字段被设置为节点 N 的名称。
该字段帮助调度程序跟踪为 Pod P 保留的资源，并为用户提供有关其集群中抢占的信息。</p><p>请注意，Pod P 不一定会调度到“被提名的节点（Nominated Node）”。
调度程序总是在迭代任何其他节点之前尝试“指定节点”。
在 Pod 因抢占而牺牲时，它们将获得体面终止期。
如果调度程序正在等待牺牲者 Pod 终止时另一个节点变得可用，
则调度程序可以使用另一个节点来调度 Pod P。
因此，Pod 规约中的 <code>nominatedNodeName</code> 和 <code>nodeName</code> 并不总是相同。
此外，如果调度程序抢占节点 N 上的 Pod，但随后比 Pod P 更高优先级的 Pod 到达，
则调度程序可能会将节点 N 分配给新的更高优先级的 Pod。
在这种情况下，调度程序会清除 Pod P 的 <code>nominatedNodeName</code>。
通过这样做，调度程序使 Pod P 有资格抢占另一个节点上的 Pod。</p><h3 id=limitations-of-preemption>抢占的限制</h3><h4 id=被抢占牺牲者的体面终止>被抢占牺牲者的体面终止</h4><p>当 Pod 被抢占时，牺牲者会得到他们的
<a href=/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>体面终止期</a>。
它们可以在体面终止期内完成工作并退出。如果它们不这样做就会被杀死。
这个体面终止期在调度程序抢占 Pod 的时间点和待处理的 Pod (P)
可以在节点 (N) 上调度的时间点之间划分出了一个时间跨度。
同时，调度器会继续调度其他待处理的 Pod。当牺牲者退出或被终止时，
调度程序会尝试在待处理队列中调度 Pod。
因此，调度器抢占牺牲者的时间点与 Pod P 被调度的时间点之间通常存在时间间隔。
为了最小化这个差距，可以将低优先级 Pod 的体面终止时间设置为零或一个小数字。</p><h4 id=支持-poddisruptionbudget-但不保证>支持 PodDisruptionBudget，但不保证</h4><p><a href=/zh-cn/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a>
(PDB) 允许多副本应用程序的所有者限制因自愿性质的干扰而同时终止的 Pod 数量。
Kubernetes 在抢占 Pod 时支持 PDB，但对 PDB 的支持是基于尽力而为原则的。
调度器会尝试寻找不会因被抢占而违反 PDB 的牺牲者，但如果没有找到这样的牺牲者，
抢占仍然会发生，并且即使违反了 PDB 约束也会删除优先级较低的 Pod。</p><h4 id=与低优先级-pod-之间的-pod-间亲和性>与低优先级 Pod 之间的 Pod 间亲和性</h4><p>只有当这个问题的答案是肯定的时，才考虑在一个节点上执行抢占操作：
“如果从此节点上删除优先级低于悬决 Pod 的所有 Pod，悬决 Pod 是否可以在该节点上调度？”</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>抢占并不一定会删除所有较低优先级的 Pod。
如果悬决 Pod 可以通过删除少于所有较低优先级的 Pod 来调度，
那么只有一部分较低优先级的 Pod 会被删除。
即便如此，上述问题的答案必须是肯定的。
如果答案是否定的，则不考虑在该节点上执行抢占。</div><p>如果悬决 Pod 与节点上的一个或多个较低优先级 Pod 具有 Pod 间<a class=glossary-tooltip title='调度程序用于确定在何处放置 Pod（亲和性）的规则。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity target=_blank aria-label=亲和性>亲和性</a>，
则在没有这些较低优先级 Pod 的情况下，无法满足 Pod 间亲和性规则。
在这种情况下，调度程序不会抢占节点上的任何 Pod。
相反，它寻找另一个节点。调度程序可能会找到合适的节点，
也可能不会。无法保证悬决 Pod 可以被调度。</p><p>我们针对此问题推荐的解决方案是仅针对同等或更高优先级的 Pod 设置 Pod 间亲和性。</p><h4 id=跨节点抢占>跨节点抢占</h4><p>假设正在考虑在一个节点 N 上执行抢占，以便可以在 N 上调度待处理的 Pod P。
只有当另一个节点上的 Pod 被抢占时，P 才可能在 N 上变得可行。
下面是一个例子：</p><ul><li>正在考虑将 Pod P 调度到节点 N 上。</li><li>Pod Q 正在与节点 N 位于同一区域的另一个节点上运行。</li><li>Pod P 与 Pod Q 具有 Zone 维度的反亲和（<code>topologyKey:topology.kubernetes.io/zone</code>）。</li><li>Pod P 与 Zone 中的其他 Pod 之间没有其他反亲和性设置。</li><li>为了在节点 N 上调度 Pod P，可以抢占 Pod Q，但调度器不会进行跨节点抢占。
因此，Pod P 将被视为在节点 N 上不可调度。</li></ul><p>如果将 Pod Q 从所在节点中移除，则不会违反 Pod 间反亲和性约束，
并且 Pod P 可能会被调度到节点 N 上。</p><p>如果有足够的需求，并且如果我们找到性能合理的算法，
我们可能会考虑在未来版本中添加跨节点抢占。</p><h2 id=troubleshooting>故障排除</h2><p>Pod 优先级和抢占可能会产生不必要的副作用。以下是一些潜在问题的示例以及处理这些问题的方法。</p><h3 id=pod-被不必要地抢占>Pod 被不必要地抢占</h3><p>抢占在资源压​​力较大时从集群中删除现有 Pod，为更高优先级的悬决 Pod 腾出空间。
如果你错误地为某些 Pod 设置了高优先级，这些无意的高优先级 Pod 可能会导致集群中出现抢占行为。
Pod 优先级是通过设置 Pod 规约中的 <code>priorityClassName</code> 字段来指定的。
优先级的整数值然后被解析并填充到 <code>podSpec</code> 的 <code>priority</code> 字段。</p><p>为了解决这个问题，你可以将这些 Pod 的 <code>priorityClassName</code> 更改为使用较低优先级的类，
或者将该字段留空。默认情况下，空的 <code>priorityClassName</code> 解析为零。</p><p>当 Pod 被抢占时，集群会为被抢占的 Pod 记录事件。只有当集群没有足够的资源用于 Pod 时，
才会发生抢占。在这种情况下，只有当悬决 Pod（抢占者）的优先级高于受害 Pod 时才会发生抢占。
当没有悬决 Pod，或者悬决 Pod 的优先级等于或低于牺牲者时，不得发生抢占。
如果在这种情况下发生抢占，请提出问题。</p><h3 id=有-pod-被抢占-但抢占者并没有被调度>有 Pod 被抢占，但抢占者并没有被调度</h3><p>当 Pod 被抢占时，它们会收到请求的体面终止期，默认为 30 秒。
如果受害 Pod 在此期限内没有终止，它们将被强制终止。
一旦所有牺牲者都离开，就可以调度抢占者 Pod。</p><p>在抢占者 Pod 等待牺牲者离开的同时，可能某个适合同一个节点的更高优先级的 Pod 被创建。
在这种情况下，调度器将调度优先级更高的 Pod 而不是抢占者。</p><p>这是预期的行为：具有较高优先级的 Pod 应该取代具有较低优先级的 Pod。</p><h3 id=优先级较高的-pod-在优先级较低的-pod-之前被抢占>优先级较高的 Pod 在优先级较低的 Pod 之前被抢占</h3><p>调度程序尝试查找可以运行悬决 Pod 的节点。如果没有找到这样的节点，
调度程序会尝试从任意节点中删除优先级较低的 Pod，以便为悬决 Pod 腾出空间。
如果具有低优先级 Pod 的节点无法运行悬决 Pod，
调度器可能会选择另一个具有更高优先级 Pod 的节点（与其他节点上的 Pod 相比）进行抢占。
牺牲者的优先级必须仍然低于抢占者 Pod。</p><p>当有多个节点可供执行抢占操作时，调度器会尝试选择具有一组优先级最低的 Pod 的节点。
但是，如果此类 Pod 具有 PodDisruptionBudget，当它们被抢占时，
则会违反 PodDisruptionBudget，那么调度程序可能会选择另一个具有更高优先级 Pod 的节点。</p><p>当存在多个节点抢占且上述场景均不适用时，调度器会选择优先级最低的节点。</p><h2 id=interactions-of-pod-priority-and-qos>Pod 优先级和服务质量之间的相互作用</h2><p>Pod 优先级和 <a class=glossary-tooltip title='QoS 类（Quality of Service Class）为 Kubernetes 提供了一种将集群中的 Pod 分为几个类并做出有关调度和驱逐决策的方法。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-qos-class' target=_blank aria-label='QoS 类'>QoS 类</a>
是两个正交特征，交互很少，并且对基于 QoS 类设置 Pod 的优先级没有默认限制。
调度器的抢占逻辑在选择抢占目标时不考虑 QoS。
抢占会考虑 Pod 优先级并尝试选择一组优先级最低的目标。
仅当移除优先级最低的 Pod 不足以让调度程序调度抢占式 Pod，
或者最低优先级的 Pod 受 PodDisruptionBudget 保护时，才会考虑优先级较高的 Pod。</p><p>kubelet 使用优先级来确定
<a href=/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/>节点压力驱逐</a> Pod 的顺序。
你可以使用 QoS 类来估计 Pod 最有可能被驱逐的顺序。kubelet 根据以下因素对 Pod 进行驱逐排名：</p><ol><li>对紧俏资源的使用是否超过请求值</li><li>Pod 优先级</li><li>相对于请求的资源使用量</li></ol><p>有关更多详细信息，请参阅
<a href=/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction>kubelet 驱逐时 Pod 的选择</a>。</p><p>当某 Pod 的资源用量未超过其请求时，kubelet 节点压力驱逐不会驱逐该 Pod。
如果优先级较低的 Pod 的资源使用量没有超过其请求，则不会被驱逐。
另一个优先级较高且资源使用量超过其请求的 Pod 可能会被驱逐。</p><h2 id=接下来>接下来</h2><ul><li>阅读有关将 ResourceQuota 与 PriorityClass 结合使用的信息：
<a href=/zh-cn/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default>默认限制优先级消费</a></li><li>了解 <a href=/zh-cn/docs/concepts/workloads/pods/disruptions/>Pod 干扰</a></li><li>了解 <a href=/zh-cn/docs/concepts/scheduling-eviction/api-eviction/>API 发起的驱逐</a></li><li>了解<a href=/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/>节点压力驱逐</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-78e0431b4b7516092662a7c289cbb304>10 - 节点压力驱逐</h1><p>节点压力驱逐是 <a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> 主动终止 Pod 以回收节点上资源的过程。</br></p><p><a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
监控集群节点的内存、磁盘空间和文件系统的 inode 等资源。
当这些资源中的一个或者多个达到特定的消耗水平，
kubelet 可以主动地使节点上一个或者多个 Pod 失效，以回收资源防止饥饿。</p><p>在节点压力驱逐期间，kubelet 将所选 Pod 的 <code>PodPhase</code> 设置为 <code>Failed</code>。这将终止 Pod。</p><p>节点压力驱逐不同于 <a href=/zh-cn/docs/concepts/scheduling-eviction/api-eviction/>API 发起的驱逐</a>。</p><p>kubelet 并不理会你配置的 <code>PodDisruptionBudget</code> 或者是 Pod 的 <code>terminationGracePeriodSeconds</code>。
如果你使用了<a href=#soft-eviction-thresholds>软驱逐条件</a>，kubelet 会考虑你所配置的
<code>eviction-max-pod-grace-period</code>。
如果你使用了<a href=#hard-eviction-thresholds>硬驱逐条件</a>，它使用 <code>0s</code> 宽限期来终止 Pod。</p><p>如果 Pod 是由替换失败 Pod 的<a class=glossary-tooltip title='工作负载是在 Kubernetes 上运行的应用程序。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/ target=_blank aria-label=工作负载>工作负载</a>资源
（例如 <a class=glossary-tooltip title='StatefulSet 用来管理某 Pod 集合的部署和扩缩，并为这些 Pod 提供持久存储和持久标识符。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>
或者 <a class=glossary-tooltip title=管理集群上的多副本应用。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>）管理，
则控制平面或 <code>kube-controller-manager</code> 会创建新的 Pod 来代替被驱逐的 Pod。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>kubelet 在终止最终用户 Pod 之前会尝试<a href=#reclaim-node-resources>回收节点级资源</a>。
例如，它会在磁盘资源不足时删除未使用的容器镜像。</div><p>kubelet 使用各种参数来做出驱逐决定，如下所示：</p><ul><li>驱逐信号</li><li>驱逐条件</li><li>监控间隔</li></ul><h3 id=eviction-signals>驱逐信号</h3><p>驱逐信号是特定资源在特定时间点的当前状态。
kubelet 使用驱逐信号，通过将信号与驱逐条件进行比较来做出驱逐决定，
驱逐条件是节点上应该可用资源的最小量。</p><p>kubelet 使用以下驱逐信号：</p><table><thead><tr><th>驱逐信号</th><th>描述</th></tr></thead><tbody><tr><td><code>memory.available</code></td><td><code>memory.available</code> := <code>node.status.capacity[memory]</code> - <code>node.stats.memory.workingSet</code></td></tr><tr><td><code>nodefs.available</code></td><td><code>nodefs.available</code> := <code>node.stats.fs.available</code></td></tr><tr><td><code>nodefs.inodesFree</code></td><td><code>nodefs.inodesFree</code> := <code>node.stats.fs.inodesFree</code></td></tr><tr><td><code>imagefs.available</code></td><td><code>imagefs.available</code> := <code>node.stats.runtime.imagefs.available</code></td></tr><tr><td><code>imagefs.inodesFree</code></td><td><code>imagefs.inodesFree</code> := <code>node.stats.runtime.imagefs.inodesFree</code></td></tr><tr><td><code>pid.available</code></td><td><code>pid.available</code> := <code>node.stats.rlimit.maxpid</code> - <code>node.stats.rlimit.curproc</code></td></tr></tbody></table><p>在上表中，<code>描述</code>列显示了 kubelet 如何获取信号的值。每个信号支持百分比值或者是字面值。
kubelet 计算相对于与信号有关的总量的百分比值。</p><p><code>memory.available</code> 的值来自 cgroupfs，而不是像 <code>free -m</code> 这样的工具。
这很重要，因为 <code>free -m</code> 在容器中不起作用，如果用户使用
<a href=/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>节点可分配资源</a>
这一功能特性，资源不足的判定是基于 cgroup 层次结构中的用户 Pod 所处的局部及 cgroup 根节点作出的。
这个<a href=/zh-cn/examples/admin/resource/memory-available.sh>脚本</a>
重现了 kubelet 为计算 <code>memory.available</code> 而执行的相同步骤。
kubelet 在其计算中排除了 inactive_file（即非活动 LRU 列表上基于文件来虚拟的内存的字节数），
因为它假定在压力下内存是可回收的。</p><p>kubelet 支持以下文件系统分区：</p><ol><li><code>nodefs</code>：节点的主要文件系统，用于本地磁盘卷、emptyDir、日志存储等。
例如，<code>nodefs</code> 包含 <code>/var/lib/kubelet/</code>。</li><li><code>imagefs</code>：可选文件系统，供容器运行时存储容器镜像和容器可写层。</li></ol><p>kubelet 会自动发现这些文件系统并忽略其他文件系统。kubelet 不支持其他配置。</p><p>一些 kubelet 垃圾收集功能已被弃用，以鼓励使用驱逐机制。</p><table><thead><tr><th>现有标志</th><th>新的标志</th><th>原因</th></tr></thead><tbody><tr><td><code>--image-gc-high-threshold</code></td><td><code>--eviction-hard</code> 或 <code>--eviction-soft</code></td><td>现有的驱逐信号可以触发镜像垃圾收集</td></tr><tr><td><code>--image-gc-low-threshold</code></td><td><code>--eviction-minimum-reclaim</code></td><td>驱逐回收具有相同的行为</td></tr><tr><td><code>--maximum-dead-containers</code></td><td>-</td><td>一旦旧的日志存储在容器的上下文之外就会被弃用</td></tr><tr><td><code>--maximum-dead-containers-per-container</code></td><td>-</td><td>一旦旧的日志存储在容器的上下文之外就会被弃用</td></tr><tr><td><code>--minimum-container-ttl-duration</code></td><td>-</td><td>一旦旧的日志存储在容器的上下文之外就会被弃用</td></tr></tbody></table><h3 id=eviction-thresholds>驱逐条件</h3><p>你可以为 kubelet 指定自定义驱逐条件，以便在作出驱逐决定时使用。</p><p>驱逐条件的形式为 <code>[eviction-signal][operator][quantity]</code>，其中：</p><ul><li><code>eviction-signal</code> 是要使用的<a href=#eviction-signals>驱逐信号</a>。</li><li><code>operator</code> 是你想要的<a href=https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators>关系运算符</a>，
比如 <code>&lt;</code>（小于）。</li><li><code>quantity</code> 是驱逐条件数量，例如 <code>1Gi</code>。
<code>quantity</code> 的值必须与 Kubernetes 使用的数量表示相匹配。
你可以使用文字值或百分比（<code>%</code>）。</li></ul><p>例如，如果一个节点的总内存为 10Gi 并且你希望在可用内存低于 1Gi 时触发驱逐，
则可以将驱逐条件定义为 <code>memory.available&lt;10%</code> 或 <code>memory.available&lt; 1G</code>。
你不能同时使用二者。</p><p>你可以配置软和硬驱逐条件。</p><h4 id=soft-eviction-thresholds>软驱逐条件</h4><p>软驱逐条件将驱逐条件与管理员所必须指定的宽限期配对。
在超过宽限期之前，kubelet 不会驱逐 Pod。
如果没有指定的宽限期，kubelet 会在启动时返回错误。</p><p>你可以既指定软驱逐条件宽限期，又指定 Pod 终止宽限期的上限，，给 kubelet 在驱逐期间使用。
如果你指定了宽限期的上限并且 Pod 满足软驱逐阈条件，则 kubelet 将使用两个宽限期中的较小者。
如果你没有指定宽限期上限，kubelet 会立即杀死被驱逐的 Pod，不允许其体面终止。</p><p>你可以使用以下标志来配置软驱逐条件：</p><ul><li><code>eviction-soft</code>：一组驱逐条件，如 <code>memory.available&lt;1.5Gi</code>，
如果驱逐条件持续时长超过指定的宽限期，可以触发 Pod 驱逐。</li><li><code>eviction-soft-grace-period</code>：一组驱逐宽限期，
如 <code>memory.available=1m30s</code>，定义软驱逐条件在触发 Pod 驱逐之前必须保持多长时间。</li><li><code>eviction-max-pod-grace-period</code>：在满足软驱逐条件而终止 Pod 时使用的最大允许宽限期（以秒为单位）。</li></ul><h4 id=hard-eviction-thresholds>硬驱逐条件</h4><p>硬驱逐条件没有宽限期。当达到硬驱逐条件时，
kubelet 会立即杀死 pod，而不会正常终止以回收紧缺的资源。</p><p>你可以使用 <code>eviction-hard</code> 标志来配置一组硬驱逐条件，
例如 <code>memory.available&lt;1Gi</code>。</p><p>kubelet 具有以下默认硬驱逐条件：</p><ul><li><code>memory.available&lt;100Mi</code></li><li><code>nodefs.available&lt;10%</code></li><li><code>imagefs.available&lt;15%</code></li><li><code>nodefs.inodesFree&lt;5%</code>（Linux 节点）</li></ul><p>只有在没有更改任何参数的情况下，硬驱逐阈值才会被设置成这些默认值。
如果你更改了任何参数的值，则其他参数的取值不会继承其默认值设置，而将被设置为零。
为了提供自定义值，你应该分别设置所有阈值。</p><h3 id=驱逐监测间隔>驱逐监测间隔</h3><p>kubelet 根据其配置的 <code>housekeeping-interval</code>（默认为 <code>10s</code>）评估驱逐条件。</p><h3 id=node-conditions>节点条件</h3><p>kubelet 报告节点状况以反映节点处于压力之下，因为满足硬或软驱逐条件，与配置的宽限期无关。</p><p>kubelet 根据下表将驱逐信号映射为节点状况：</p><table><thead><tr><th>节点条件</th><th>驱逐信号</th><th>描述</th></tr></thead><tbody><tr><td><code>MemoryPressure</code></td><td><code>memory.available</code></td><td>节点上的可用内存已满足驱逐条件</td></tr><tr><td><code>DiskPressure</code></td><td><code>nodefs.available</code>、<code>nodefs.inodesFree</code>、<code>imagefs.available</code> 或 <code>imagefs.inodesFree</code></td><td>节点的根文件系统或镜像文件系统上的可用磁盘空间和 inode 已满足驱逐条件</td></tr><tr><td><code>PIDPressure</code></td><td><code>pid.available</code></td><td>(Linux) 节点上的可用进程标识符已低于驱逐条件</td></tr></tbody></table><p>kubelet 根据配置的 <code>--node-status-update-frequency</code> 更新节点条件，默认为 <code>10s</code>。</p><h4 id=节点条件振荡>节点条件振荡</h4><p>在某些情况下，节点在软驱逐条件上下振荡，而没有保持定义的宽限期。
这会导致报告的节点条件在 <code>true</code> 和 <code>false</code> 之间不断切换，从而导致错误的驱逐决策。</p><p>为了防止振荡，你可以使用 <code>eviction-pressure-transition-period</code> 标志，
该标志控制 kubelet 在将节点条件转换为不同状态之前必须等待的时间。
过渡期的默认值为 <code>5m</code>。</p><h3 id=reclaim-node-resources>回收节点级资源</h3><p>kubelet 在驱逐最终用户 Pod 之前会先尝试回收节点级资源。</p><p>当报告 <code>DiskPressure</code> 节点状况时，kubelet 会根据节点上的文件系统回收节点级资源。</p><h4 id=有-imagefs>有 <code>imagefs</code></h4><p>如果节点有一个专用的 <code>imagefs</code> 文件系统供容器运行时使用，kubelet 会执行以下操作：</p><ul><li>如果 <code>nodefs</code> 文件系统满足驱逐条件，kubelet 垃圾收集死亡 Pod 和容器。</li><li>如果 <code>imagefs</code> 文件系统满足驱逐条件，kubelet 将删除所有未使用的镜像。</li></ul><h4 id=没有-imagefs>没有 <code>imagefs</code></h4><p>如果节点只有一个满足驱逐条件的 <code>nodefs</code> 文件系统，
kubelet 按以下顺序释放磁盘空间：</p><ol><li>对死亡的 Pod 和容器进行垃圾收集</li><li>删除未使用的镜像</li></ol><h3 id=kubelet-驱逐时-pod-的选择>kubelet 驱逐时 Pod 的选择</h3><p>如果 kubelet 回收节点级资源的尝试没有使驱逐信号低于条件，
则 kubelet 开始驱逐最终用户 Pod。</p><p>kubelet 使用以下参数来确定 Pod 驱逐顺序：</p><ol><li>Pod 的资源使用是否超过其请求</li><li><a href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级</a></li><li>Pod 相对于请求的资源使用情况</li></ol><p>因此，kubelet 按以下顺序排列和驱逐 Pod：</p><ol><li>首先考虑资源使用量超过其请求的 <code>BestEffort</code> 或 <code>Burstable</code> Pod。
这些 Pod 会根据它们的优先级以及它们的资源使用级别超过其请求的程度被逐出。</li><li>资源使用量少于请求量的 <code>Guaranteed</code> Pod 和 <code>Burstable</code> Pod 根据其优先级被最后驱逐。</li></ol><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>kubelet 不使用 Pod 的 QoS 类来确定驱逐顺序。
在回收内存等资源时，你可以使用 QoS 类来估计最可能的 Pod 驱逐顺序。
QoS 不适用于临时存储（EphemeralStorage）请求，
因此如果节点在 <code>DiskPressure</code> 下，则上述场景将不适用。</div><p>仅当 <code>Guaranteed</code> Pod 中所有容器都被指定了请求和限制并且二者相等时，才保证 Pod 不被驱逐。
这些 Pod 永远不会因为另一个 Pod 的资源消耗而被驱逐。
如果系统守护进程（例如 <code>kubelet</code> 和 <code>journald</code>）
消耗的资源比通过 <code>system-reserved</code> 或 <code>kube-reserved</code> 分配保留的资源多，
并且该节点只有 <code>Guaranteed</code> 或 <code>Burstable</code> Pod 使用的资源少于其上剩余的请求，
那么 kubelet 必须选择驱逐这些 Pod 中的一个以保持节点稳定性并减少资源匮乏对其他 Pod 的影响。
在这种情况下，它会选择首先驱逐最低优先级的 Pod。</p><p>当 kubelet 因 inode 或 PID 不足而驱逐 Pod 时，
它使用优先级来确定驱逐顺序，因为 inode 和 PID 没有请求。</p><p>kubelet 根据节点是否具有专用的 <code>imagefs</code> 文件系统对 Pod 进行不同的排序：</p><h4 id=有-imagefs-1>有 <code>imagefs</code></h4><p>如果 <code>nodefs</code> 触发驱逐，
kubelet 会根据 <code>nodefs</code> 使用情况（<code>本地卷 + 所有容器的日志</code>）对 Pod 进行排序。</p><p>如果 <code>imagefs</code> 触发驱逐，kubelet 会根据所有容器的可写层使用情况对 Pod 进行排序。</p><h4 id=没有-imagefs-1>没有 <code>imagefs</code></h4><p>如果 <code>nodefs</code> 触发驱逐，
kubelet 会根据磁盘总用量（<code>本地卷 + 日志和所有容器的可写层</code>）对 Pod 进行排序。</p><h3 id=minimum-eviction-reclaim>最小驱逐回收</h3><p>在某些情况下，驱逐 Pod 只会回收少量的紧俏资源。
这可能导致 kubelet 反复达到配置的驱逐条件并触发多次驱逐。</p><p>你可以使用 <code>--eviction-minimum-reclaim</code> 标志或
<a href=/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/>kubelet 配置文件</a>
为每个资源配置最小回收量。
当 kubelet 注意到某个资源耗尽时，它会继续回收该资源，直到回收到你所指定的数量为止。</p><p>例如，以下配置设置最小回收量：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>evictionHard</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>memory.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imagefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>evictionMinimumReclaim</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>memory.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imagefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>在这个例子中，如果 <code>nodefs.available</code> 信号满足驱逐条件，
kubelet 会回收资源，直到信号达到 <code>1Gi</code> 的条件，
然后继续回收至少 <code>500Mi</code> 直到信号达到 <code>1.5Gi</code>。</p><p>类似地，kubelet 会回收 <code>imagefs</code> 资源，直到 <code>imagefs.available</code> 信号达到 <code>102Gi</code>。</p><p>对于所有资源，默认的 <code>eviction-minimum-reclaim</code> 为 <code>0</code>。</p><h3 id=节点内存不足行为>节点内存不足行为</h3><p>如果节点在 kubelet 能够回收内存之前遇到内存不足（OOM）事件，
则节点依赖 <a href=https://lwn.net/Articles/391222/>oom_killer</a> 来响应。</p><p>kubelet 根据 Pod 的服务质量（QoS）为每个容器设置一个 <code>oom_score_adj</code> 值。</p><table><thead><tr><th>服务质量</th><th>oom_score_adj</th></tr></thead><tbody><tr><td><code>Guaranteed</code></td><td>-997</td></tr><tr><td><code>BestEffort</code></td><td>1000</td></tr><tr><td><code>Burstable</code></td><td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>kubelet 还将具有 <code>system-node-critical</code>
<a class=glossary-tooltip title='Pod 优先级表示一个 Pod 相对于其他 Pod 的重要性。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority target=_blank aria-label=优先级>优先级</a>
的 Pod 中的容器 <code>oom_score_adj</code> 值设为 <code>-997</code>。</div><p>如果 kubelet 在节点遇到 OOM 之前无法回收内存，
则 <code>oom_killer</code> 根据它在节点上使用的内存百分比计算 <code>oom_score</code>，
然后加上 <code>oom_score_adj</code> 得到每个容器有效的 <code>oom_score</code>。
然后它会杀死得分最高的容器。</p><p>这意味着低 QoS Pod 中相对于其调度请求消耗内存较多的容器，将首先被杀死。</p><p>与 Pod 驱逐不同，如果容器被 OOM 杀死，
<code>kubelet</code> 可以根据其 <code>RestartPolicy</code> 重新启动它。</p><h3 id=node-pressure-eviction-good-practices>最佳实践</h3><p>以下部分描述了驱逐配置的最佳实践。</p><h4 id=可调度的资源和驱逐策略>可调度的资源和驱逐策略</h4><p>当你为 kubelet 配置驱逐策略时，
你应该确保调度程序不会在 Pod 触发驱逐时对其进行调度，因为这类 Pod 会立即引起内存压力。</p><p>考虑以下场景：</p><ul><li>节点内存容量：<code>10Gi</code></li><li>操作员希望为系统守护进程（内核、<code>kubelet</code> 等）保留 10% 的内存容量</li><li>操作员希望在节点内存利用率达到 95% 以上时驱逐 Pod，以减少系统 OOM 的概率。</li></ul><p>为此，kubelet 启动设置如下：</p><pre tabindex=0><code>--eviction-hard=memory.available&lt;500Mi
--system-reserved=memory=1.5Gi
</code></pre><p>在此配置中，<code>--system-reserved</code> 标志为系统预留了 <code>1.5Gi</code> 的内存，
即 <code>总内存的 10% + 驱逐条件量</code>。</p><p>如果 Pod 使用的内存超过其请求值或者系统使用的内存超过 <code>1Gi</code>，
则节点可以达到驱逐条件，这使得 <code>memory.available</code> 信号低于 <code>500Mi</code> 并触发条件。</p><h3 id=daemonset>DaemonSet</h3><p>Pod 优先级是做出驱逐决定的主要因素。
如果你不希望 kubelet 驱逐属于 <code>DaemonSet</code> 的 Pod，
请在 Pod 规约中为这些 Pod 提供足够高的 <code>priorityClass</code>。
你还可以使用优先级较低的 <code>priorityClass</code> 或默认配置，
仅在有足够资源时才运行 <code>DaemonSet</code> Pod。</p><h3 id=已知问题>已知问题</h3><p>以下部分描述了与资源不足处理相关的已知问题。</p><h4 id=kubelet-可能不会立即观察到内存压力>kubelet 可能不会立即观察到内存压力</h4><p>默认情况下，kubelet 轮询 <code>cAdvisor</code> 以定期收集内存使用情况统计信息。
如果该轮询时间窗口内内存使用量迅速增加，kubelet 可能无法足够快地观察到 <code>MemoryPressure</code>，
但是 <code>OOMKiller</code> 仍将被调用。</p><p>你可以使用 <code>--kernel-memcg-notification</code>
标志在 kubelet 上启用 <code>memcg</code> 通知 API，以便在超过条件时立即收到通知。</p><p>如果你不是追求极端利用率，而是要采取合理的过量使用措施，
则解决此问题的可行方法是使用 <code>--kube-reserved</code> 和 <code>--system-reserved</code> 标志为系统分配内存。</p><h4 id=active-file-内存未被视为可用内存>active_file 内存未被视为可用内存</h4><p>在 Linux 上，内核跟踪活动 LRU 列表上的基于文件所虚拟的内存字节数作为 <code>active_file</code> 统计信息。
kubelet 将 <code>active_file</code> 内存区域视为不可回收。
对于大量使用块设备形式的本地存储（包括临时本地存储）的工作负载，
文件和块数据的内核级缓存意味着许多最近访问的缓存页面可能被计为 <code>active_file</code>。
如果这些内核块缓冲区中在活动 LRU 列表上有足够多，
kubelet 很容易将其视为资源用量过量并为节点设置内存压力污点，从而触发 Pod 驱逐。</p><p>更多细节请参见 <a href=https://github.com/kubernetes/kubernetes/issues/43916>https://github.com/kubernetes/kubernetes/issues/43916</a>。</p><p>你可以通过为可能执行 I/O 密集型活动的容器设置相同的内存限制和内存请求来应对该行为。
你将需要估计或测量该容器的最佳内存限制值。</p><h2 id=接下来>接下来</h2><ul><li>了解 <a href=/zh-cn/docs/concepts/scheduling-eviction/api-eviction/>API 发起的驱逐</a></li><li>了解 <a href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级和抢占</a></li><li>了解 <a href=/zh-cn/docs/tasks/run-application/configure-pdb/>PodDisruptionBudgets</a></li><li>了解<a href=/zh-cn/docs/tasks/configure-pod-container/quality-service-pod/>服务质量</a>（QoS）</li><li>查看<a href=/docs/reference/generated/kubernetes-api/v1.25/#create-eviction-pod-v1-core>驱逐 API</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b87723bf81b079042860f0ebd37b0a64>11 - API 发起的驱逐</h1><p>API 发起的驱逐是一个先调用
<a href=/docs/reference/generated/kubernetes-api/v1.25/#create-eviction-pod-v1-core>Eviction API</a>
创建 <code>Eviction</code> 对象，再由该对象体面地中止 Pod 的过程。</br></p><p>你可以通过直接调用 Eviction API 发起驱逐，也可以通过编程的方式使用
<a class=glossary-tooltip title='提供 Kubernetes API 服务的控制面组件。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API 服务器'>API 服务器</a>的客户端来发起驱逐，
比如 <code>kubectl drain</code> 命令。
此操作创建一个 <code>Eviction</code> 对象，该对象再驱动 API 服务器终止选定的 Pod。</p><p>API 发起的驱逐将遵从你的
<a href=/zh-cn/docs/tasks/run-application/configure-pdb/><code>PodDisruptionBudgets</code></a>
和 <a href=/zh-cn/docs/concepts/workloads/pods/pod-lifecycle#pod-termination><code>terminationGracePeriodSeconds</code></a>
配置。</p><p>使用 API 创建 Eviction 对象，就像对 Pod 执行策略控制的
<a href=/zh-cn/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod><code>DELETE</code> 操作</a></p><h2 id=调用-eviction-api>调用 Eviction API</h2><p>你可以使用 <a href=/zh-cn/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api>Kubernetes 语言客户端</a>
来访问 Kubernetes API 并创建 <code>Eviction</code> 对象。
要执行此操作，你应该用 POST 发出要尝试的请求，类似于下面的示例：</p><ul class="nav nav-tabs" id=eviction-example role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#eviction-example-0 role=tab aria-controls=eviction-example-0 aria-selected=true>policy/v1</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#eviction-example-1 role=tab aria-controls=eviction-example-1>policy/v1beta1</a></li></ul><div class=tab-content id=eviction-example><div id=eviction-example-0 class="tab-pane show active" role=tabpanel aria-labelledby=eviction-example-0><p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p><code>policy/v1</code> 版本的 Eviction 在 v1.22 以及更高的版本中可用，之前的发行版本使用 <code>policy/v1beta1</code> 版本。</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;policy/v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Eviction&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;quux&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></div><div id=eviction-example-1 class=tab-pane role=tabpanel aria-labelledby=eviction-example-1><p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>在 v1.22 版本废弃以支持 <code>policy/v1</code></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;policy/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Eviction&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;quux&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></div></div><p>或者，你可以通过使用 <code>curl</code> 或者 <code>wget</code> 来访问 API 以尝试驱逐操作，类似于以下示例：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -v -H <span style=color:#b44>&#39;Content-type: application/json&#39;</span> https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json
</span></span></code></pre></div><h2 id=api-发起驱逐的工作原理>API 发起驱逐的工作原理</h2><p>当你使用 API 来请求驱逐时，API 服务器将执行准入检查，并通过以下方式之一做出响应：</p><ul><li><code>200 OK</code>：允许驱逐，子资源 <code>Eviction</code> 被创建，并且 Pod 被删除，
类似于发送一个 <code>DELETE</code> 请求到 Pod 地址。</li><li><code>429 Too Many Requests</code>：当前不允许驱逐，因为配置了 <a class=glossary-tooltip title='Pod Disruption Budget 是这样一种对象：它保证在主动中断（ voluntary disruptions）时，多实例应用的  不会少于一定的数量。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-pod-disruption-budget' target=_blank aria-label=PodDisruptionBudget>PodDisruptionBudget</a>。
你可以稍后再尝试驱逐。你也可能因为 API 速率限制而看到这种响应。</li><li><code>500 Internal Server Error</code>：不允许驱逐，因为存在配置错误，
例如存在多个 PodDisruptionBudgets 引用同一个 Pod。</li></ul><p>如果你想驱逐的 Pod 不属于有 PodDisruptionBudget 的工作负载，
API 服务器总是返回 <code>200 OK</code> 并且允许驱逐。</p><p>如果 API 服务器允许驱逐，Pod 按照如下方式删除：</p><ol><li>API 服务器中的 <code>Pod</code> 资源会更新上删除时间戳，之后 API 服务器会认为此 <code>Pod</code> 资源将被终止。
此 <code>Pod</code> 资源还会标记上配置的宽限期。</li><li>本地运行状态的 Pod 所处的节点上的 <a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
注意到 <code>Pod</code> 资源被标记为终止，并开始优雅停止本地 Pod。</li><li>当 kubelet 停止 Pod 时，控制面从 <a class=glossary-tooltip title='端点负责记录与服务（Service）的选择器相匹配的 Pod 的 IP 地址。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-endpoint' target=_blank aria-label=Endpoint>Endpoint</a>
和 <a class=glossary-tooltip title='一种将网络端点与 Kubernetes 资源组合在一起的方法。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/services-networking/endpoint-slices/ target=_blank aria-label=EndpointSlice>EndpointSlice</a>
对象中移除该 Pod。因此，控制器不再将此 Pod 视为有用对象。</li><li>Pod 的宽限期到期后，kubelet 强制终止本地 Pod。</li><li>kubelet 告诉 API 服务器删除 <code>Pod</code> 资源。</li><li>API 服务器删除 <code>Pod</code> 资源。</li></ol><h2 id=解决驱逐被卡住的问题>解决驱逐被卡住的问题</h2><p>在某些情况下，你的应用可能进入中断状态，
在你干预之前，驱逐 API 总是返回 <code>429</code> 或 <code>500</code>。
例如，如果 ReplicaSet 为你的应用程序创建了 Pod，
但新的 Pod 没有进入 <code>Ready</code> 状态，就会发生这种情况。
在最后一个被驱逐的 Pod 有很长的终止宽限期的情况下，你可能也会注意到这种行为。</p><p>如果你注意到驱逐被卡住，请尝试以下解决方案之一：</p><ul><li>终止或暂停导致问题的自动化操作，重新启动操作之前，请检查被卡住的应用程序。</li><li>等待一段时间后，直接从集群控制平面删除 Pod，而不是使用 Eviction API。</li></ul><h2 id=接下来>接下来</h2><ul><li>了解如何使用 <a href=/zh-cn/docs/tasks/run-application/configure-pdb/>Pod 干扰预算</a> 保护你的应用。</li><li>了解<a href=/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/>节点压力引发的驱逐</a>。</li><li>了解 <a href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod 优先级和抢占</a>。</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/zh-cn/docs/home/>主页</a>
<a class=text-white href=/zh-cn/blog/>博客</a>
<a class=text-white href=/zh-cn/training/>培训</a>
<a class=text-white href=/zh-cn/partners/>合作伙伴</a>
<a class=text-white href=/zh-cn/community/>社区</a>
<a class=text-white href=/zh-cn/case-studies/>案例分析</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes 作者 | 文档发布基于 <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a> 授权许可</small><br><small class=text-white>Copyright &copy; 2023 Linux 基金会&reg;。保留所有权利。Linux 基金会已注册并使用商标。如需了解 Linux 基金会的商标列表，请访问<a href=https://www.linuxfoundation.org/trademark-usage class=light-text>商标使用页面</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>