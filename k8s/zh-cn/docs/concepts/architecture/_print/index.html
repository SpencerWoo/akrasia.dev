<!doctype html><html lang=zh-cn class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/architecture/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/architecture/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/architecture/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/architecture/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/architecture/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/architecture/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/architecture/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/architecture/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/architecture/><link rel=alternate hreflang=vi href=https://kubernetes.io/vi/docs/concepts/architecture/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/architecture/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/zh-cn/docs/concepts/architecture/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Kubernetes 架构 | Kubernetes</title><meta property="og:title" content="Kubernetes 架构"><meta property="og:description" content="Kubernetes 背后的架构概念。
"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/zh-cn/docs/concepts/architecture/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Kubernetes 架构"><meta itemprop=description content="Kubernetes 背后的架构概念。
"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes 架构"><meta name=twitter:description content="Kubernetes 背后的架构概念。
"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Kubernetes 背后的架构概念。
"><meta property="og:description" content="Kubernetes 背后的架构概念。
"><meta name=twitter:description content="Kubernetes 背后的架构概念。
"><meta property="og:url" content="https://kubernetes.io/zh-cn/docs/concepts/architecture/"><meta property="og:title" content="Kubernetes 架构"><meta name=twitter:title content="Kubernetes 架构"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/zh-cn/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/zh-cn/docs/>文档</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/blog/>Kubernetes 博客</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/training/>培训</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/partners/>合作伙伴</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/community/>社区</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/zh-cn/case-studies/>案例分析</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>版本列表</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/zh-cn/docs/concepts/architecture/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/zh-cn/docs/concepts/architecture/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/zh-cn/docs/concepts/architecture/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/zh-cn/docs/concepts/architecture/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/zh-cn/docs/concepts/architecture/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>中文 (Chinese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/architecture/>English</a>
<a class=dropdown-item href=/ko/docs/concepts/architecture/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/architecture/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/architecture/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/architecture/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/architecture/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/architecture/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/architecture/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/architecture/>Bahasa Indonesia</a>
<a class=dropdown-item href=/vi/docs/concepts/architecture/>Tiếng Việt (Vietnamese)</a>
<a class=dropdown-item href=/ru/docs/concepts/architecture/>Русский (Russian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>这是本节的多页打印视图。
<a href=# onclick="return print(),!1">点击此处打印</a>.</p><p><a href=/zh-cn/docs/concepts/architecture/>返回本页常规视图</a>.</p></div><h1 class=title>Kubernetes 架构</h1><div class=lead>Kubernetes 背后的架构概念。</div><ul><li>1: <a href=#pg-9ef2890698e773b6c0d24fd2c20146f5>节点</a></li><li>2: <a href=#pg-c0251def6da29b30afebfb04549f1703>节点与控制面之间的通信</a></li><li>3: <a href=#pg-ca8819042a505291540e831283da66df>控制器</a></li><li>4: <a href=#pg-bc804b02614d67025b4c788f1ca87fbc>云控制器管理器</a></li><li>5: <a href=#pg-c20ec7d296cc2c8668bb204c2af31180>关于 cgroup v2</a></li><li>6: <a href=#pg-c0ea5310f52e22c5de34dc84d9ab5e0d>容器运行时接口（CRI）</a></li><li>7: <a href=#pg-44a2e2e592af0846101e970aff9243e5>垃圾收集</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-9ef2890698e773b6c0d24fd2c20146f5>1 - 节点</h1><p>Kubernetes 通过将容器放入在节点（Node）上运行的 Pod 中来执行你的工作负载。
节点可以是一个虚拟机或者物理机器，取决于所在的集群配置。
每个节点包含运行 <a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> 所需的服务；
这些节点由 <a class=glossary-tooltip title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label=控制面>控制面</a> 负责管理。</p><p>通常集群中会有若干个节点；而在一个学习所用或者资源受限的环境中，你的集群中也可能只有一个节点。</p><p>节点上的<a href=/zh-cn/docs/concepts/overview/components/#node-components>组件</a>包括
<a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>、
<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>以及
<a class=glossary-tooltip title='kube-proxy 是集群中每个节点上运行的网络代理。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a>。</p><h2 id=management>管理</h2><p>向 <a class=glossary-tooltip title='提供 Kubernetes API 服务的控制面组件。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API 服务器'>API 服务器</a>添加节点的方式主要有两种：</p><ol><li>节点上的 <code>kubelet</code> 向控制面执行自注册；</li><li>你（或者别的什么人）手动添加一个 Node 对象。</li></ol><p>在你创建了 Node <a class=glossary-tooltip title='Kubernetes 系统中的实体，代表了集群的部分状态。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-objects target=_blank aria-label=对象>对象</a>或者节点上的
<code>kubelet</code> 执行了自注册操作之后，控制面会检查新的 Node 对象是否合法。
例如，如果你尝试使用下面的 JSON 对象来创建 Node 对象：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Node&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;10.240.79.157&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;my-first-k8s-node&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Kubernetes 会在内部创建一个 Node 对象作为节点的表示。Kubernetes 检查 <code>kubelet</code>
向 API 服务器注册节点时使用的 <code>metadata.name</code> 字段是否匹配。
如果节点是健康的（即所有必要的服务都在运行中），则该节点可以用来运行 Pod。
否则，直到该节点变为健康之前，所有的集群活动都会忽略该节点。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>Kubernetes 会一直保存着非法节点对应的对象，并持续检查该节点是否已经变得健康。
你，或者某个<a class=glossary-tooltip title='控制器通过 API 服务器监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/controller/ target=_blank aria-label=控制器>控制器</a>必须显式地删除该
Node 对象以停止健康检查操作。</div><p>Node 对象的名称必须是合法的
<a href=/zh-cn/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS 子域名</a>。</p><h3 id=node-name-uniqueness>节点名称唯一性</h3><p>节点的<a href=/zh-cn/docs/concepts/overview/working-with-objects/names#names>名称</a>用来标识 Node 对象。
没有两个 Node 可以同时使用相同的名称。 Kubernetes 还假定名字相同的资源是同一个对象。
就 Node 而言，隐式假定使用相同名称的实例会具有相同的状态（例如网络配置、根磁盘内容）
和类似节点标签这类属性。这可能在节点被更改但其名称未变时导致系统状态不一致。
如果某个 Node 需要被替换或者大量变更，需要从 API 服务器移除现有的 Node 对象，
之后再在更新之后重新将其加入。</p><h3 id=self-registration-of-nodes>节点自注册</h3><p>当 kubelet 标志 <code>--register-node</code> 为 true（默认）时，它会尝试向 API 服务注册自己。
这是首选模式，被绝大多数发行版选用。</p><p>对于自注册模式，kubelet 使用下列参数启动：</p><ul><li><code>--kubeconfig</code> - 用于向 API 服务器执行身份认证所用的凭据的路径。</li><li><code>--cloud-provider</code> - 与某<a class=glossary-tooltip title=一个提供云计算平台的组织。 data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-cloud-provider' target=_blank aria-label=云驱动>云驱动</a>
进行通信以读取与自身相关的元数据的方式。</li><li><code>--register-node</code> - 自动向 API 服务注册。</li><li><code>--register-with-taints</code> - 使用所给的<a class=glossary-tooltip title='污点是一种一个核心对象，包含三个必需的属性：key、value 和 effect。 污点会阻止在节点或节点组上调度 Pod。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=污点>污点</a>列表
（逗号分隔的 <code>&lt;key>=&lt;value>:&lt;effect></code>）注册节点。当 <code>register-node</code> 为 false 时无效。</li><li><code>--node-ip</code> - 节点 IP 地址。</li><li><code>--node-labels</code> - 在集群中注册节点时要添加的<a class=glossary-tooltip title=用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=标签>标签</a>。
（参见 <a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction 准入控制插件</a>所实施的标签限制）。</li><li><code>--node-status-update-frequency</code> - 指定 kubelet 向 API 服务器发送其节点状态的频率。</li></ul><p>当 <a href=/zh-cn/docs/reference/access-authn-authz/node/>Node 鉴权模式</a>和
<a href=/zh-cn/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction 准入插件</a>被启用后，
仅授权 kubelet 创建/修改自己的 Node 资源。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>正如<a href=#node-name-uniqueness>节点名称唯一性</a>一节所述，当 Node 的配置需要被更新时，
一种好的做法是重新向 API 服务器注册该节点。例如，如果 kubelet 重启时其 <code>--node-labels</code>
是新的值集，但同一个 Node 名称已经被使用，则所作变更不会起作用，
因为节点标签是在 Node 注册时完成的。</p><p>如果在 kubelet 重启期间 Node 配置发生了变化，已经被调度到某 Node 上的 Pod
可能会出现行为不正常或者出现其他问题，例如，已经运行的 Pod
可能通过污点机制设置了与 Node 上新设置的标签相排斥的规则，也有一些其他 Pod，
本来与此 Pod 之间存在不兼容的问题，也会因为新的标签设置而被调到同一节点。
节点重新注册操作可以确保节点上所有 Pod 都被排空并被正确地重新调度。</p></div><h3 id=manual-node-administration>手动节点管理</h3><p>你可以使用 <a class=glossary-tooltip title='kubectl 是用来和 Kubernetes 集群进行通信的命令行工具。' data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a>
来创建和修改 Node 对象。</p><p>如果你希望手动创建节点对象时，请设置 kubelet 标志 <code>--register-node=false</code>。</p><p>你可以修改 Node 对象（忽略 <code>--register-node</code> 设置）。
例如，你可以修改节点上的标签或并标记其为不可调度。</p><p>你可以结合使用 Node 上的标签和 Pod 上的选择算符来控制调度。
例如，你可以限制某 Pod 只能在符合要求的节点子集上运行。</p><p>如果标记节点为不可调度（unschedulable），将阻止新 Pod 调度到该 Node 之上，
但不会影响任何已经在其上的 Pod。
这是重启节点或者执行其他维护操作之前的一个有用的准备步骤。</p><p>要标记一个 Node 为不可调度，执行以下命令：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl cordon <span style=color:#b8860b>$NODENAME</span>
</span></span></code></pre></div><p>更多细节参考<a href=/zh-cn/docs/tasks/administer-cluster/safely-drain-node/>安全地腾空节点</a>。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>被 <a class=glossary-tooltip title='确保 Pod 的副本在集群中的一组节点上运行。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/controllers/daemonset/ target=_blank aria-label=DaemonSet>DaemonSet</a> 控制器创建的 Pod
能够容忍节点的不可调度属性。
DaemonSet 通常提供节点本地的服务，即使节点上的负载应用已经被腾空，
这些服务也仍需运行在节点之上。</div><h2 id=node-status>节点状态</h2><p>一个节点的状态包含以下信息:</p><ul><li><a href=#addresses>地址（Addresses）</a></li><li><a href=#condition>状况（Condition）</a></li><li><a href=#capacity>容量与可分配（Capacity）</a></li><li><a href=#info>信息（Info）</a></li></ul><p>你可以使用 <code>kubectl</code> 来查看节点状态和其他细节信息：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe node &lt;节点名称&gt;
</span></span></code></pre></div><p>下面对输出的每个部分进行详细描述。</p><h3 id=addresses>地址</h3><p>这些字段的用法取决于你的云服务商或者物理机配置。</p><ul><li>HostName：由节点的内核报告。可以通过 kubelet 的 <code>--hostname-override</code> 参数覆盖。</li><li>ExternalIP：通常是节点的可外部路由（从集群外可访问）的 IP 地址。</li><li>InternalIP：通常是节点的仅可在集群内部路由的 IP 地址。</li></ul><h3 id=condition>状况</h3><p><code>conditions</code> 字段描述了所有 <code>Running</code> 节点的状况。状况的示例包括：</p><table><caption style=display:none>节点状况及每种状况适用场景的描述</caption><thead><tr><th>节点状况</th><th>描述</th></tr></thead><tbody><tr><td><code>Ready</code></td><td>如节点是健康的并已经准备好接收 Pod 则为 <code>True</code>；<code>False</code> 表示节点不健康而且不能接收 Pod；<code>Unknown</code> 表示节点控制器在最近 <code>node-monitor-grace-period</code> 期间（默认 40 秒）没有收到节点的消息</td></tr><tr><td><code>DiskPressure</code></td><td><code>True</code> 表示节点存在磁盘空间压力，即磁盘可用量低, 否则为 <code>False</code></td></tr><tr><td><code>MemoryPressure</code></td><td><code>True</code> 表示节点存在内存压力，即节点内存可用量低，否则为 <code>False</code></td></tr><tr><td><code>PIDPressure</code></td><td><code>True</code> 表示节点存在进程压力，即节点上进程过多；否则为 <code>False</code></td></tr><tr><td><code>NetworkUnavailable</code></td><td><code>True</code> 表示节点网络配置不正确；否则为 <code>False</code></td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>如果使用命令行工具来打印已保护（Cordoned）节点的细节，其中的 Condition 字段可能包括
<code>SchedulingDisabled</code>。<code>SchedulingDisabled</code> 不是 Kubernetes API 中定义的
Condition，被保护起来的节点在其规约中被标记为不可调度（Unschedulable）。</div><p>在 Kubernetes API 中，节点的状况表示节点资源中<code>.status</code> 的一部分。
例如，以下 JSON 结构描述了一个健康节点：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;conditions&#34;</span><span>:</span> [
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;Ready&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;True&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;reason&#34;</span>: <span style=color:#b44>&#34;KubeletReady&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;message&#34;</span>: <span style=color:#b44>&#34;kubelet is posting ready status&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;lastHeartbeatTime&#34;</span>: <span style=color:#b44>&#34;2019-06-05T18:38:35Z&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;lastTransitionTime&#34;</span>: <span style=color:#b44>&#34;2019-06-05T11:41:27Z&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>如果 Ready 状况的 <code>status</code> 处于 <code>Unknown</code> 或者 <code>False</code> 状态的时间超过了
<code>pod-eviction-timeout</code> 值（一个传递给
<a class=glossary-tooltip title=主节点上运行控制器的组件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>
的参数），<a href=#node-controller>节点控制器</a>会对节点上的所有 Pod 触发
<a class=glossary-tooltip title='API 发起的驱逐是一个先调用 Eviction API 创建驱逐对象，再由该对象体面地中止 Pod 的过程。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/scheduling-eviction/api-eviction/ target=_blank aria-label='API 发起的驱逐'>API 发起的驱逐</a>。
默认的逐出超时时长为 <strong>5 分钟</strong>。</p><p>某些情况下，当节点不可达时，API 服务器不能和其上的 kubelet 通信。
删除 Pod 的决定不能传达给 kubelet，直到它重新建立和 API 服务器的连接为止。
与此同时，被计划删除的 Pod 可能会继续在游离的节点上运行。</p><p>节点控制器在确认 Pod 在集群中已经停止运行前，不会强制删除它们。
你可以看到可能在这些无法访问的节点上运行的 Pod 处于 <code>Terminating</code> 或者 <code>Unknown</code> 状态。
如果 kubernetes 不能基于下层基础设施推断出某节点是否已经永久离开了集群，
集群管理员可能需要手动删除该节点对象。
从 Kubernetes 删除节点对象将导致 API 服务器删除节点上所有运行的 Pod 对象并释放它们的名字。</p><p>当节点上出现问题时，Kubernetes 控制面会自动创建与影响节点的状况对应的
<a href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/>污点</a>。
调度器在将 Pod 指派到某 Node 时会考虑 Node 上的污点设置。
Pod 也可以设置<a class=glossary-tooltip title='一个核心对象，由三个必需的属性组成：key、value 和 effect。 容忍度允许将 Pod 调度到具有对应污点的节点或节点组上。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=容忍度>容忍度</a>，
以便能够在设置了特定污点的 Node 上运行。</p><p>进一步的细节可参阅<a href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition>根据状况为节点设置污点</a>。</p><h3 id=capacity>容量（Capacity）与可分配（Allocatable）</h3><p>这两个值描述节点上的可用资源：CPU、内存和可以调度到节点上的 Pod 的个数上限。</p><p><code>capacity</code> 块中的字段标示节点拥有的资源总量。
<code>allocatable</code> 块指示节点上可供普通 Pod 消耗的资源量。</p><p>可以在学习如何在节点上<a href=/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>预留计算资源</a>
的时候了解有关容量和可分配资源的更多信息。</p><h3 id=info>信息（Info）</h3><p>Info 指的是节点的一般信息，如内核版本、Kubernetes 版本（<code>kubelet</code> 和 <code>kube-proxy</code> 版本）、
容器运行时详细信息，以及节点使用的操作系统。
<code>kubelet</code> 从节点收集这些信息并将其发布到 Kubernetes API。</p><h2 id=heartbeats>心跳</h2><p>Kubernetes 节点发送的心跳帮助你的集群确定每个节点的可用性，并在检测到故障时采取行动。</p><p>对于节点，有两种形式的心跳:</p><ul><li>更新节点的 <code>.status</code></li><li><code>kube-node-lease</code> <a class=glossary-tooltip title='名字空间是 Kubernetes 用来支持隔离单个集群中的资源组的一种抽象。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=名字空间>名字空间</a>中的
<a href=/zh-cn/docs/reference/kubernetes-api/cluster-resources/lease-v1/>Lease（租约）</a>对象。
每个节点都有一个关联的 Lease 对象。</li></ul><p>与 Node 的 <code>.status</code> 更新相比，Lease 是一种轻量级资源。
使用 Lease 来表达心跳在大型集群中可以减少这些更新对性能的影响。</p><p>kubelet 负责创建和更新节点的 <code>.status</code>，以及更新它们对应的 Lease。</p><ul><li>当节点状态发生变化时，或者在配置的时间间隔内没有更新事件时，kubelet 会更新 <code>.status</code>。
<code>.status</code> 更新的默认间隔为 5 分钟（比节点不可达事件的 40 秒默认超时时间长很多）。</li><li><code>kubelet</code> 会创建并每 10 秒（默认更新间隔时间）更新 Lease 对象。
Lease 的更新独立于 Node 的 <code>.status</code> 更新而发生。
如果 Lease 的更新操作失败，kubelet 会采用指数回退机制，从 200 毫秒开始重试，
最长重试间隔为 7 秒钟。</li></ul><h2 id=node-controller>节点控制器</h2><p>节点<a class=glossary-tooltip title='控制器通过 API 服务器监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/controller/ target=_blank aria-label=控制器>控制器</a>是 Kubernetes 控制面组件，
管理节点的方方面面。</p><p>节点控制器在节点的生命周期中扮演多个角色。
第一个是当节点注册时为它分配一个 CIDR 区段（如果启用了 CIDR 分配）。</p><p>第二个是保持节点控制器内的节点列表与云服务商所提供的可用机器列表同步。
如果在云环境下运行，只要某节点不健康，节点控制器就会询问云服务是否节点的虚拟机仍可用。
如果不可用，节点控制器会将该节点从它的节点列表删除。</p><p>第三个是监控节点的健康状况。节点控制器负责：</p><ul><li>在节点不可达的情况下，在 Node 的 <code>.status</code> 中更新 <code>Ready</code> 状况。
在这种情况下，节点控制器将 NodeReady 状况更新为 <code>Unknown</code>。</li><li>如果节点仍然无法访问：对于不可达节点上的所有 Pod 触发
<a href=/zh-cn/docs/concepts/scheduling-eviction/api-eviction/>API 发起的逐出</a>操作。
默认情况下，节点控制器在将节点标记为 <code>Unknown</code> 后等待 5 分钟提交第一个驱逐请求。</li></ul><p>默认情况下，节点控制器每 5 秒检查一次节点状态，可以使用 <code>kube-controller-manager</code>
组件上的 <code>--node-monitor-period</code> 参数来配置周期。</p><h3 id=rate-limits-on-eviction>逐出速率限制</h3><p>大部分情况下，节点控制器把逐出速率限制在每秒 <code>--node-eviction-rate</code> 个（默认为 0.1）。
这表示它每 10 秒钟内至多从一个节点驱逐 Pod。</p><p>当一个可用区域（Availability Zone）中的节点变为不健康时，节点的驱逐行为将发生改变。
节点控制器会同时检查可用区域中不健康（<code>Ready</code> 状况为 <code>Unknown</code> 或 <code>False</code>）
的节点的百分比：</p><ul><li>如果不健康节点的比例超过 <code>--unhealthy-zone-threshold</code> （默认为 0.55），
驱逐速率将会降低。</li><li>如果集群较小（意即小于等于 <code>--large-cluster-size-threshold</code> 个节点 - 默认为 50），
驱逐操作将会停止。</li><li>否则驱逐速率将降为每秒 <code>--secondary-node-eviction-rate</code> 个（默认为 0.01）。</li></ul><p>在逐个可用区域中实施这些策略的原因是，
当一个可用区域可能从控制面脱离时其它可用区域可能仍然保持连接。
如果你的集群没有跨越云服务商的多个可用区域，那（整个集群）就只有一个可用区域。</p><p>跨多个可用区域部署你的节点的一个关键原因是当某个可用区域整体出现故障时，
工作负载可以转移到健康的可用区域。
因此，如果一个可用区域中的所有节点都不健康时，节点控制器会以正常的速率
<code>--node-eviction-rate</code> 进行驱逐操作。
在所有的可用区域都不健康（也即集群中没有健康节点）的极端情况下，
节点控制器将假设控制面与节点间的连接出了某些问题，它将停止所有驱逐动作
（如果故障后部分节点重新连接，节点控制器会从剩下不健康或者不可达节点中驱逐 Pod）。</p><p>节点控制器还负责驱逐运行在拥有 <code>NoExecute</code> 污点的节点上的 Pod，
除非这些 Pod 能够容忍此污点。
节点控制器还负责根据节点故障（例如节点不可访问或没有就绪）
为其添加<a class=glossary-tooltip title='污点是一种一个核心对象，包含三个必需的属性：key、value 和 effect。 污点会阻止在节点或节点组上调度 Pod。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=污点>污点</a>。
这意味着调度器不会将 Pod 调度到不健康的节点上。</p><h3 id=node-capacity>资源容量跟踪</h3><p>Node 对象会跟踪节点上资源的容量（例如可用内存和 CPU 数量）。
通过<a href=#self-registration-of-nodes>自注册</a>机制生成的 Node 对象会在注册期间报告自身容量。
如果你<a href=#manual-node-administration>手动</a>添加了 Node，
你就需要在添加节点时手动设置节点容量。</p><p>Kubernetes <a class=glossary-tooltip title='控制平面组件，负责监视新创建的、未指定运行节点的 Pod，选择节点让 Pod 在上面运行。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=调度器>调度器</a>
保证节点上有足够的资源供其上的所有 Pod 使用。
它会检查节点上所有容器的请求的总和不会超过节点的容量。
总的请求包括由 kubelet 启动的所有容器，但不包括由容器运行时直接启动的容器，
也不包括不受 <code>kubelet</code> 控制的其他进程。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>如果要为非 Pod 进程显式保留资源。
请参考<a href=/zh-cn/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved>为系统守护进程预留资源</a>。</div><h2 id=node-topology>节点拓扑</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.18 [beta]</code></div><p>如果启用了 <code>TopologyManager</code> <a href=/zh-cn/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>，
<code>kubelet</code> 可以在作出资源分配决策时使用拓扑提示。
参考<a href=/zh-cn/docs/tasks/administer-cluster/topology-manager/>控制节点上拓扑管理策略</a>了解详细信息。</p><h2 id=graceful-node-shutdown>节点体面关闭</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.21 [beta]</code></div><p>kubelet 会尝试检测节点系统关闭事件并终止在节点上运行的所有 Pod。</p><p>在节点终止期间，kubelet 保证 Pod 遵从常规的
<a href=/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>Pod 终止流程</a>。</p><p>节点体面关闭特性依赖于 systemd，因为它要利用
<a href=https://www.freedesktop.org/wiki/Software/systemd/inhibit/>systemd 抑制器锁</a>机制，
在给定的期限内延迟节点关闭。</p><p>节点体面关闭特性受 <code>GracefulNodeShutdown</code>
<a href=/zh-cn/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>控制，
在 1.21 版本中是默认启用的。</p><p>注意，默认情况下，下面描述的两个配置选项，<code>shutdownGracePeriod</code> 和
<code>shutdownGracePeriodCriticalPods</code> 都是被设置为 0 的，因此不会激活节点体面关闭功能。
要激活此功能特性，这两个 kubelet 配置选项要适当配置，并设置为非零值。</p><p>在体面关闭节点过程中，kubelet 分两个阶段来终止 Pod：</p><ol><li>终止在节点上运行的常规 Pod。</li><li>终止在节点上运行的<a href=/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>关键 Pod</a>。</li></ol><p>节点体面关闭的特性对应两个
<a href=/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/><code>KubeletConfiguration</code></a> 选项：</p><ul><li><code>shutdownGracePeriod</code>：<ul><li>指定节点应延迟关闭的总持续时间。此时间是 Pod 体面终止的时间总和，不区分常规 Pod
还是<a href=/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>关键 Pod</a>。</li></ul></li><li><code>shutdownGracePeriodCriticalPods</code>：<ul><li>在节点关闭期间指定用于终止<a href=/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>关键 Pod</a>
的持续时间。该值应小于 <code>shutdownGracePeriod</code>。</li></ul></li></ul><p>例如，如果设置了 <code>shutdownGracePeriod=30s</code> 和 <code>shutdownGracePeriodCriticalPods=10s</code>，
则 kubelet 将延迟 30 秒关闭节点。
在关闭期间，将保留前 20（30 - 10）秒用于体面终止常规 Pod，
而保留最后 10 秒用于终止<a href=/zh-cn/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>关键 Pod</a>。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>当 Pod 在正常节点关闭期间被驱逐时，它们会被标记为关闭。
运行 <code>kubectl get pods</code> 时，被驱逐的 Pod 的状态显示为 <code>Terminated</code>。
并且 <code>kubectl describe pod</code> 表示 Pod 因节点关闭而被驱逐：</p><pre tabindex=0><code>Reason:         Terminated
Message:        Pod was terminated in response to imminent node shutdown.
</code></pre></div><h2 id=non-graceful-node-shutdown>节点非体面关闭</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.24 [alpha]</code></div><p>节点关闭的操作可能无法被 kubelet 的节点关闭管理器检测到，
是因为该命令不会触发 kubelet 所使用的抑制锁定机制，或者是因为用户错误的原因，
即 ShutdownGracePeriod 和 ShutdownGracePeriodCriticalPod 配置不正确。
请参考以上<a href=#graceful-node-shutdown>节点体面关闭</a>部分了解更多详细信息。</p><p>当某节点关闭但 kubelet 的节点关闭管理器未检测到这一事件时，
在那个已关闭节点上、属于 StatefulSet 的 Pod 将停滞于终止状态，并且不能移动到新的运行节点上。
这是因为已关闭节点上的 kubelet 已不存在，亦无法删除 Pod，
因此 StatefulSet 无法创建同名的新 Pod。
如果 Pod 使用了卷，则 VolumeAttachments 不会从原来的已关闭节点上删除，
因此这些 Pod 所使用的卷也无法挂接到新的运行节点上。
所以，那些以 StatefulSet 形式运行的应用无法正常工作。
如果原来的已关闭节点被恢复，kubelet 将删除 Pod，新的 Pod 将被在不同的运行节点上创建。
如果原来的已关闭节点没有被恢复，那些在已关闭节点上的 Pod 将永远滞留在终止状态。</p><p>为了缓解上述情况，用户可以手动将具有 <code>NoExecute</code> 或 <code>NoSchedule</code> 效果的
<code>node.kubernetes.io/out-of-service</code> 污点添加到节点上，标记其无法提供服务。
如果在 <code>kube-controller-manager</code> 上启用了 <code>NodeOutOfServiceVolumeDetach</code>
<a href=/zh-cn/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>，
并且节点被通过污点标记为无法提供服务，如果节点 Pod 上没有设置对应的容忍度，
那么这样的 Pod 将被强制删除，并且该在节点上被终止的 Pod 将立即进行卷分离操作。
这样就允许那些在无法提供服务节点上的 Pod 能在其他节点上快速恢复。</p><p>在非体面关闭期间，Pod 分两个阶段终止：</p><ol><li>强制删除没有匹配的 <code>out-of-service</code> 容忍度的 Pod。</li><li>立即对此类 Pod 执行分离卷操作。</li></ol><div class="alert alert-info note callout" role=alert><strong>说明：</strong><ul><li>在添加 <code>node.kubernetes.io/out-of-service</code> 污点之前，应该验证节点已经处于关闭或断电状态（而不是在重新启动中）。</li><li>将 Pod 移动到新节点后，用户需要手动移除停止服务的污点，并且用户要检查关闭节点是否已恢复，因为该用户是最初添加污点的用户。</li></ul></div><h3 id=pod-priority-graceful-node-shutdown>基于 Pod 优先级的节点体面关闭</h3><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.23 [alpha]</code></div><p>为了在节点体面关闭期间提供更多的灵活性，尤其是处理关闭期间的 Pod 排序问题，
节点体面关闭机制能够关注 Pod 的 PriorityClass 设置，前提是你已经在集群中启用了此功能特性。
此功能特性允许集群管理员基于 Pod
的<a href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass>优先级类（Priority Class）</a>
显式地定义节点体面关闭期间 Pod 的处理顺序。</p><p>前文所述的<a href=#graceful-node-shutdown>节点体面关闭</a>特性能够分两个阶段关闭 Pod，
首先关闭的是非关键的 Pod，之后再处理关键 Pod。
如果需要显式地以更细粒度定义关闭期间 Pod 的处理顺序，需要一定的灵活度，
这时可以使用基于 Pod 优先级的体面关闭机制。</p><p>当节点体面关闭能够处理 Pod 优先级时，节点体面关闭的处理可以分为多个阶段，
每个阶段关闭特定优先级类的 Pod。kubelet 可以被配置为按确切的阶段处理 Pod，
且每个阶段可以独立设置关闭时间。</p><p>假设集群中存在以下自定义的 Pod
<a href=/zh-cn/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass>优先级类</a>。</p><table><thead><tr><th>Pod 优先级类名称</th><th>Pod 优先级类数值</th></tr></thead><tbody><tr><td><code>custom-class-a</code></td><td>100000</td></tr><tr><td><code>custom-class-b</code></td><td>10000</td></tr><tr><td><code>custom-class-c</code></td><td>1000</td></tr><tr><td><code>regular/unset</code></td><td>0</td></tr></tbody></table><p>在 <a href=/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration>kubelet 配置</a>中，
<code>shutdownGracePeriodByPodPriority</code> 可能看起来是这样：</p><table><thead><tr><th>Pod 优先级类数值</th><th>关闭期限</th></tr></thead><tbody><tr><td>100000</td><td>10 秒</td></tr><tr><td>10000</td><td>180 秒</td></tr><tr><td>1000</td><td>120 秒</td></tr><tr><td>0</td><td>60 秒</td></tr></tbody></table><p>对应的 kubelet 配置 YAML 将会是：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>shutdownGracePeriodByPodPriority</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>100000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>10000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>180</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>120</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>60</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>上面的表格表明，所有 <code>priority</code> 值大于等于 100000 的 Pod 会得到 10 秒钟期限停止，
所有 <code>priority</code> 值介于 10000 和 100000 之间的 Pod 会得到 180 秒钟期限停止，
所有 <code>priority</code> 值介于 1000 和 10000 之间的 Pod 会得到 120 秒钟期限停止，
所有其他 Pod 将获得 60 秒的时间停止。</p><p>用户不需要为所有的优先级类都设置数值。例如，你也可以使用下面这种配置：</p><table><thead><tr><th>Pod 优先级类数值</th><th>关闭期限</th></tr></thead><tbody><tr><td>100000</td><td>300 秒</td></tr><tr><td>1000</td><td>120 秒</td></tr><tr><td>0</td><td>60 秒</td></tr></tbody></table><p>在上面这个场景中，优先级类为 <code>custom-class-b</code> 的 Pod 会与优先级类为 <code>custom-class-c</code>
的 Pod 在关闭时按相同期限处理。</p><p>如果在特定的范围内不存在 Pod，则 kubelet 不会等待对应优先级范围的 Pod。
kubelet 会直接跳到下一个优先级数值范围进行处理。</p><p>如果此功能特性被启用，但没有提供配置数据，则不会出现排序操作。</p><p>使用此功能特性需要启用 <code>GracefulNodeShutdownBasedOnPodPriority</code>
<a href=/zh-cn/docs/reference/command-line-tools-reference/feature-gates/>特性门控</a>，
并将 <a href=/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/>kubelet 配置</a>
中的 <code>shutdownGracePeriodByPodPriority</code> 设置为期望的配置，
其中包含 Pod 的优先级类数值以及对应的关闭期限。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong> 在节点体面关闭期间考虑 Pod 优先级的能力是作为 Kubernetes v1.23 中的 Alpha 功能引入的。
在 Kubernetes 1.25 中该功能是 Beta 版，默认启用。</div><p>kubelet 子系统中会生成 <code>graceful_shutdown_start_time_seconds</code> 和
<code>graceful_shutdown_end_time_seconds</code> 度量指标以便监视节点关闭行为。</p><h2 id=swap-memory>交换内存管理</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.22 [alpha]</code></div><p>在 Kubernetes 1.22 之前，节点不支持使用交换内存，并且默认情况下，
如果在节点上检测到交换内存配置，kubelet 将无法启动。
在 1.22 以后，可以逐个节点地启用交换内存支持。</p><p>要在节点上启用交换内存，必须启用 kubelet 的 <code>NodeSwap</code> 特性门控，
同时使用 <code>--fail-swap-on</code> 命令行参数或者将 <code>failSwapOn</code>
<a href=/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration>配置</a>设置为 false。</p><div class="alert alert-danger warning callout" role=alert><strong>警告：</strong><p>当内存交换功能被启用后，Kubernetes 数据（如写入 tmpfs 的 Secret 对象的内容）可以被交换到磁盘。</div><p>用户还可以选择配置 <code>memorySwap.swapBehavior</code> 以指定节点使用交换内存的方式。例如:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>memorySwap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>swapBehavior</span>:<span style=color:#bbb> </span>LimitedSwap<span style=color:#bbb>
</span></span></span></code></pre></div><p>可用的 <code>swapBehavior</code> 的配置选项有：</p><ul><li><code>LimitedSwap</code>：Kubernetes 工作负载的交换内存会受限制。
不受 Kubernetes 管理的节点上的工作负载仍然可以交换。</li><li><code>UnlimitedSwap</code>：Kubernetes 工作负载可以使用尽可能多的交换内存请求，
一直到达到系统限制为止。</li></ul><p>如果启用了特性门控但是未指定 <code>memorySwap</code> 的配置，默认情况下 kubelet 将使用
<code>LimitedSwap</code> 设置。</p><p><code>LimitedSwap</code> 这种设置的行为取决于节点运行的是 v1 还是 v2 的控制组（也就是 <code>cgroups</code>）：</p><ul><li><strong>cgroupsv1:</strong> Kubernetes 工作负载可以使用内存和交换，上限为 Pod 的内存限制值（如果设置了的话）。</li><li><strong>cgroupsv2:</strong> Kubernetes 工作负载不能使用交换内存。</li></ul><p>如需更多信息以及协助测试和提供反馈，请参见
<a href=https://github.com/kubernetes/enhancements/issues/2400>KEP-2400</a>
及其<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md>设计提案</a>。</p><h2 id=接下来>接下来</h2><ul><li>进一步了解节点<a href=/zh-cn/docs/concepts/overview/components/#node-components>组件</a>。</li><li>阅读 <a href=/docs/reference/generated/kubernetes-api/v1.25/#node-v1-core>Node 的 API 定义</a>。</li><li>阅读架构设计文档中有关
<a href=https://git.k8s.io/design-proposals-archive/architecture/architecture.md#the-kubernetes-node>Node</a>
的章节。</li><li>了解<a href=/zh-cn/docs/concepts/scheduling-eviction/taint-and-toleration/>污点和容忍度</a>。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c0251def6da29b30afebfb04549f1703>2 - 节点与控制面之间的通信</h1><p>本文列举控制面节点（确切说是 API 服务器）和 Kubernetes 集群之间的通信路径。
目的是为了让用户能够自定义他们的安装，以实现对网络配置的加固，
使得集群能够在不可信的网络上（或者在一个云服务商完全公开的 IP 上）运行。</p><h2 id=node-to-control-plane>节点到控制面</h2><p>Kubernetes 采用的是中心辐射型（Hub-and-Spoke）API 模式。
所有从节点（或运行于其上的 Pod）发出的 API 调用都终止于 API 服务器。
其它控制面组件都没有被设计为可暴露远程服务。
API 服务器被配置为在一个安全的 HTTPS 端口（通常为 443）上监听远程连接请求，
并启用一种或多种形式的客户端<a href=/zh-cn/docs/reference/access-authn-authz/authentication/>身份认证</a>机制。
一种或多种客户端<a href=/zh-cn/docs/reference/access-authn-authz/authorization/>鉴权机制</a>应该被启用，
特别是在允许使用<a href=/zh-cn/docs/reference/access-authn-authz/authentication/#anonymous-requests>匿名请求</a>
或<a href=/zh-cn/docs/reference/access-authn-authz/authentication/#service-account-tokens>服务账户令牌</a>的时候。</p><p>应该使用集群的公共根证书开通节点，这样它们就能够基于有效的客户端凭据安全地连接 API 服务器。
一种好的方法是以客户端证书的形式将客户端凭据提供给 kubelet。
请查看 <a href=/zh-cn/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/>kubelet TLS 启动引导</a>
以了解如何自动提供 kubelet 客户端证书。</p><p>想要连接到 API 服务器的 Pod 可以使用服务账号安全地进行连接。
当 Pod 被实例化时，Kubernetes 自动把公共根证书和一个有效的持有者令牌注入到 Pod 里。
<code>kubernetes</code> 服务（位于 <code>default</code> 名字空间中）配置了一个虚拟 IP 地址，
用于（通过 kube-proxy）转发请求到 API 服务器的 HTTPS 末端。</p><p>控制面组件也通过安全端口与集群的 API 服务器通信。</p><p>这样，从集群节点和节点上运行的 Pod 到控制面的连接的缺省操作模式即是安全的，
能够在不可信的网络或公网上运行。</p><h2 id=control-plane-to-node>控制面到节点</h2><p>从控制面（API 服务器）到节点有两种主要的通信路径。
第一种是从 API 服务器到集群中每个节点上运行的 kubelet 进程。
第二种是从 API 服务器通过它的代理功能连接到任何节点、Pod 或者服务。</p><h3 id=api-server-to-kubelet>API 服务器到 kubelet</h3><p>从 API 服务器到 kubelet 的连接用于：</p><ul><li>获取 Pod 日志</li><li>挂接（通过 kubectl）到运行中的 Pod</li><li>提供 kubelet 的端口转发功能。</li></ul><p>这些连接终止于 kubelet 的 HTTPS 末端。
默认情况下，API 服务器不检查 kubelet 的服务证书。这使得此类连接容易受到中间人攻击，
在非受信网络或公开网络上运行也是 <strong>不安全的</strong>。</p><p>为了对这个连接进行认证，使用 <code>--kubelet-certificate-authority</code> 标志给
API 服务器提供一个根证书包，用于 kubelet 的服务证书。</p><p>如果无法实现这点，又要求避免在非受信网络或公共网络上进行连接，可在 API 服务器和
kubelet 之间使用 <a href=#ssh-tunnels>SSH 隧道</a>。</p><p>最后，应该启用
<a href=/zh-cn/docs/reference/access-authn-authz/kubelet-authn-authz/>Kubelet 认证/鉴权</a>
来保护 kubelet API。</p><h3 id=api-server-to-nodes-pods-and-services>API 服务器到节点、Pod 和服务</h3><p>从 API 服务器到节点、Pod 或服务的连接默认为纯 HTTP 方式，因此既没有认证，也没有加密。
这些连接可通过给 API URL 中的节点、Pod 或服务名称添加前缀 <code>https:</code> 来运行在安全的 HTTPS 连接上。
不过这些连接既不会验证 HTTPS 末端提供的证书，也不会提供客户端证书。
因此，虽然连接是加密的，仍无法提供任何完整性保证。
这些连接 <strong>目前还不能安全地</strong> 在非受信网络或公共网络上运行。</p><h3 id=ssh-tunnels>SSH 隧道</h3><p>Kubernetes 支持使用 SSH 隧道来保护从控制面到节点的通信路径。在这种配置下，
API 服务器建立一个到集群中各节点的 SSH 隧道（连接到在 22 端口监听的 SSH 服务器）
并通过这个隧道传输所有到 kubelet、节点、Pod 或服务的请求。
这一隧道保证通信不会被暴露到集群节点所运行的网络之外。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>SSH 隧道目前已被废弃。除非你了解个中细节，否则不应使用。
<a href=#konnectivity-service>Konnectivity 服务</a>是 SSH 隧道的替代方案。</div><h3 id=konnectivity-service>Konnectivity 服务</h3><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.18 [beta]</code></div><p>作为 SSH 隧道的替代方案，Konnectivity 服务提供 TCP 层的代理，以便支持从控制面到集群的通信。
Konnectivity 服务包含两个部分：Konnectivity 服务器和 Konnectivity 代理，
分别运行在控制面网络和节点网络中。
Konnectivity 代理建立并维持到 Konnectivity 服务器的网络连接。
启用 Konnectivity 服务之后，所有控制面到节点的通信都通过这些连接传输。</p><p>请浏览 <a href=/zh-cn/docs/tasks/extend-kubernetes/setup-konnectivity/>Konnectivity 服务任务</a>
在你的集群中配置 Konnectivity 服务。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ca8819042a505291540e831283da66df>3 - 控制器</h1><p>在机器人技术和自动化领域，控制回路（Control Loop）是一个非终止回路，用于调节系统状态。</p><p>这是一个控制环的例子：房间里的温度自动调节器。</p><p>当你设置了温度，告诉了温度自动调节器你的<strong>期望状态（Desired State）</strong>。
房间的实际温度是<strong>当前状态（Current State）</strong>。
通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。</p>在 Kubernetes 中，控制器通过监控<a class=glossary-tooltip title=一组工作机器，称为节点，会运行容器化应用程序。每个集群至少有一个工作节点。 data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=集群>集群</a>
的公共状态，并致力于将当前状态转变为期望的状态。<h2 id=controller-pattern>控制器模式</h2><p>一个控制器至少追踪一种类型的 Kubernetes 资源。这些
<a href=/zh-cn/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-objects>对象</a>
有一个代表期望状态的 <code>spec</code> 字段。
该资源的控制器负责确保其当前状态接近期望状态。</p><p>控制器可能会自行执行操作；在 Kubernetes 中更常见的是一个控制器会发送信息给
<a class=glossary-tooltip title='提供 Kubernetes API 服务的控制面组件。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API 服务器'>API 服务器</a>，这会有副作用。
具体可参看后文的例子。</p><h3 id=control-via-API-server>通过 API 服务器来控制</h3><p><a class=glossary-tooltip title='Job 是需要运行完成的确定性的或批量的任务。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Job>Job</a> 控制器是一个 Kubernetes 内置控制器的例子。
内置控制器通过和集群 API 服务器交互来管理状态。</p><p>Job 是一种 Kubernetes 资源，它运行一个或者多个 <a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>，
来执行一个任务然后停止。
（一旦<a href=/zh-cn/docs/concepts/scheduling-eviction/>被调度了</a>，对 <code>kubelet</code> 来说 Pod
对象就会变成了期望状态的一部分）。</p><p>在集群中，当 Job 控制器拿到新任务时，它会保证一组 Node 节点上的 <code>kubelet</code>
可以运行正确数量的 Pod 来完成工作。
Job 控制器不会自己运行任何的 Pod 或者容器。Job 控制器是通知 API 服务器来创建或者移除 Pod。
<a class=glossary-tooltip title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label=控制面>控制面</a>中的其它组件
根据新的消息作出反应（调度并运行新 Pod）并且最终完成工作。</p><p>创建新 Job 后，所期望的状态就是完成这个 Job。Job 控制器会让 Job 的当前状态不断接近期望状态：创建为 Job 要完成工作所需要的 Pod，使 Job 的状态接近完成。</p><p>控制器也会更新配置对象。例如：一旦 Job 的工作完成了，Job 控制器会更新 Job 对象的状态为 <code>Finished</code>。</p><p>（这有点像温度自动调节器关闭了一个灯，以此来告诉你房间的温度现在到你设定的值了）。</p><h3 id=direct-control>直接控制</h3><p>相比 Job 控制器，有些控制器需要对集群外的一些东西进行修改。</p><p>例如，如果你使用一个控制回路来保证集群中有足够的
<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>，那么控制器就需要当前集群外的
一些服务在需要时创建新节点。</p><p>和外部状态交互的控制器从 API 服务器获取到它想要的状态，然后直接和外部系统进行通信
并使当前状态更接近期望状态。</p><p>（实际上有一个<a href=https://github.com/kubernetes/autoscaler/>控制器</a>
可以水平地扩展集群中的节点。）</p><p>这里的重点是，控制器做出了一些变更以使得事物更接近你的期望状态，
之后将当前状态报告给集群的 API 服务器。
其他控制回路可以观测到所汇报的数据的这种变化并采取其各自的行动。</p><p>在温度计的例子中，如果房间很冷，那么某个控制器可能还会启动一个防冻加热器。
就 Kubernetes 集群而言，控制面间接地与 IP 地址管理工具、存储服务、云驱动
APIs 以及其他服务协作，通过<a href=/zh-cn/docs/concepts/extend-kubernetes/>扩展 Kubernetes</a>
来实现这点。</p><h2 id=desired-vs-current>期望状态与当前状态</h2><p>Kubernetes 采用了系统的云原生视图，并且可以处理持续的变化。</p><p>在任务执行时，集群随时都可能被修改，并且控制回路会自动修复故障。
这意味着很可能集群永远不会达到稳定状态。</p><p>只要集群中的控制器在运行并且进行有效的修改，整体状态的稳定与否是无关紧要的。</p><h2 id=design>设计</h2><p>作为设计原则之一，Kubernetes 使用了很多控制器，每个控制器管理集群状态的一个特定方面。
最常见的一个特定的控制器使用一种类型的资源作为它的期望状态，
控制器管理控制另外一种类型的资源向它的期望状态演化。
例如，Job 的控制器跟踪 Job 对象（以发现新的任务）和 Pod 对象（以运行 Job，然后查看任务何时完成）。
在这种情况下，新任务会创建 Job，而 Job 控制器会创建 Pod。</p><p>使用简单的控制器而不是一组相互连接的单体控制回路是很有用的。
控制器会失败，所以 Kubernetes 的设计正是考虑到了这一点。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>可以有多个控制器来创建或者更新相同类型的对象。
在后台，Kubernetes 控制器确保它们只关心与其控制资源相关联的资源。</p><p>例如，你可以创建 Deployment 和 Job；它们都可以创建 Pod。
Job 控制器不会删除 Deployment 所创建的 Pod，因为有信息
（<a class=glossary-tooltip title=用来为对象设置可标识的属性标记；这些标记对用户而言是有意义且重要的。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=标签>标签</a>）让控制器可以区分这些 Pod。</p></div><h2 id=running-controllers>运行控制器的方式</h2><p>Kubernetes 内置一组控制器，运行在 <a class=glossary-tooltip title=主节点上运行控制器的组件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a> 内。
这些内置的控制器提供了重要的核心功能。</p><p>Deployment 控制器和 Job 控制器是 Kubernetes 内置控制器的典型例子。
Kubernetes 允许你运行一个稳定的控制平面，这样即使某些内置控制器失败了，
控制平面的其他部分会接替它们的工作。</p><p>你会遇到某些控制器运行在控制面之外，用以扩展 Kubernetes。
或者，如果你愿意，你也可以自己编写新控制器。
你可以以一组 Pod 来运行你的控制器，或者运行在 Kubernetes 之外。
最合适的方案取决于控制器所要执行的功能是什么。</p><h2 id=接下来>接下来</h2><ul><li>阅读 <a href=/zh-cn/docs/concepts/overview/components/#control-plane-components>Kubernetes 控制平面组件</a></li><li>了解 <a href=/zh-cn/docs/concepts/overview/working-with-objects/kubernetes-objects/>Kubernetes 对象</a>
的一些基本知识</li><li>进一步学习 <a href=/zh-cn/docs/concepts/overview/kubernetes-api/>Kubernetes API</a></li><li>如果你想编写自己的控制器，请看 Kubernetes 的
<a href=/zh-cn/docs/concepts/extend-kubernetes/#extension-patterns>扩展模式</a>。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bc804b02614d67025b4c788f1ca87fbc>4 - 云控制器管理器</h1><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.11 [beta]</code></div><p>使用云基础设施技术，你可以在公有云、私有云或者混合云环境中运行 Kubernetes。
Kubernetes 的信条是基于自动化的、API 驱动的基础设施，同时避免组件间紧密耦合。</p><p><p>组件 cloud-controller-manager 是指云控制器管理器， 一个 Kubernetes <a class=glossary-tooltip title='控制平面是指容器编排层，它暴露 API 和接口来定义、部署容器和管理容器的生命周期。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label=控制平面>控制平面</a>组件，
嵌入了特定于云平台的控制逻辑。
云控制器管理器（Cloud Controller Manager）允许你将你的集群连接到云提供商的 API 之上，
并将与该云平台交互的组件同与你的集群交互的组件分离开来。</p></p><p>通过分离 Kubernetes 和底层云基础设置之间的互操作性逻辑，
<code>cloud-controller-manager</code> 组件使云提供商能够以不同于 Kubernetes 主项目的步调发布新特征。</p><p><code>cloud-controller-manager</code> 组件是基于一种插件机制来构造的，
这种机制使得不同的云厂商都能将其平台与 Kubernetes 集成。</p><h2 id=design>设计</h2><p><img src=/images/docs/components-of-kubernetes.svg alt="Kubernetes 组件"></p><p>云控制器管理器以一组多副本的进程集合的形式运行在控制面中，通常表现为 Pod
中的容器。每个 <code>cloud-controller-manager</code>
在同一进程中实现多个<a class=glossary-tooltip title='控制器通过 API 服务器监控集群的公共状态，并致力于将当前状态转变为期望的状态。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/controller/ target=_blank aria-label=控制器>控制器</a>。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>你也可以用 Kubernetes <a class=glossary-tooltip title='扩展 Kubernetes 功能的资源。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/cluster-administration/addons/ target=_blank aria-label=插件>插件</a>
的形式而不是控制面中的一部分来运行云控制器管理器。</div><h2 id=functions-of-the-ccm>云控制器管理器的功能</h2><p>云控制器管理器中的控制器包括：</p><h3 id=node-controller>节点控制器</h3><p>节点控制器负责在云基础设施中创建了新服务器时为之更新<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点（Node）>节点（Node）</a>对象。
节点控制器从云提供商获取当前租户中主机的信息。节点控制器执行以下功能：</p><ol><li>使用从云平台 API 获取的对应服务器的唯一标识符更新 Node 对象；</li><li>利用特定云平台的信息为 Node 对象添加注解和标签，例如节点所在的区域
（Region）和所具有的资源（CPU、内存等等）；</li><li>获取节点的网络地址和主机名；</li><li>检查节点的健康状况。如果节点无响应，控制器通过云平台 API
查看该节点是否已从云中禁用、删除或终止。如果节点已从云中删除，
则控制器从 Kubernetes 集群中删除 Node 对象。</li></ol><p>某些云驱动实现中，这些任务被划分到一个节点控制器和一个节点生命周期控制器中。</p><h3 id=route-controller>路由控制器</h3><p>Route 控制器负责适当地配置云平台中的路由，以便 Kubernetes 集群中不同节点上的容器之间可以相互通信。</p><p>取决于云驱动本身，路由控制器可能也会为 Pod 网络分配 IP 地址块。</p><h3 id=service-controller>服务控制器</h3><p><a class=glossary-tooltip title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/services-networking/service/ target=_blank aria-label=服务（Service）>服务（Service）</a>与受控的负载均衡器、
IP 地址、网络包过滤、目标健康检查等云基础设施组件集成。
服务控制器与云驱动的 API 交互，以配置负载均衡器和其他基础设施组件。
你所创建的 Service 资源会需要这些组件服务。</p><h2 id=authorization>鉴权</h2><p>本节分别讲述云控制器管理器为了完成自身工作而产生的对各类 API 对象的访问需求。</p><h3 id=authorization-node-controller>节点控制器</h3><p>节点控制器只操作 Node 对象。它需要读取和修改 Node 对象的完全访问权限。</p><p><code>v1/Node</code>：</p><ul><li>Get</li><li>List</li><li>Create</li><li>Update</li><li>Patch</li><li>Watch</li><li>Delete</li></ul><h3 id=authorization-route-controller>路由控制器</h3><p>路由控制器会监听 Node 对象的创建事件，并据此配置路由设施。
它需要读取 Node 对象的 Get 权限。</p><p><code>v1/Node</code>：</p><ul><li>Get</li></ul><h3 id=authorization-service-controller>服务控制器</h3><p>服务控制器监测 Service 对象的 Create、Update 和 Delete 事件，并配置对应服务的 Endpoints 对象
（对于 EndpointSlices，kube-controller-manager 按需对其进行管理）。</p><p>为了访问 Service 对象，它需要 List 和 Watch 访问权限。
为了更新 Service 对象，它需要 Patch 和 Update 访问权限。</p><p>为了能够配置 Service 对应的 Endpoints 资源，
它需要 Create、List、Get、Watch 和 Update 等访问权限。</p><p><code>v1/Service</code>：</p><ul><li>List</li><li>Get</li><li>Watch</li><li>Patch</li><li>Update</li></ul><h3 id=authorization-miscellaneous>其他</h3><p>在云控制器管理器的实现中，其核心部分需要创建 Event 对象的访问权限，
并创建 ServiceAccount 资源以保证操作安全性的权限。</p><p><code>v1/Event</code>：</p><ul><li>Create</li><li>Patch</li><li>Update</li></ul><p><code>v1/ServiceAccount</code>：</p><ul><li>Create</li></ul><p>用于云控制器管理器 <a class=glossary-tooltip title='管理授权决策，允许管理员通过 Kubernetes API 动态配置访问策略。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/reference/access-authn-authz/rbac/ target=_blank aria-label=RBAC>RBAC</a>
的 ClusterRole 如下例所示：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- events<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#39;*&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes/status<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- services<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- serviceaccounts<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- persistentvolumes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- endpoints<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=接下来>接下来</h2><p><a href=/zh-cn/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager>云控制器管理器的管理</a>
给出了运行和管理云控制器管理器的指南。</p><p>要升级 HA 控制平面以使用云控制器管理器，
请参见<a href=/zh-cn/docs/tasks/administer-cluster/controller-manager-leader-migration/>将复制的控制平面迁移以使用云控制器管理器</a>。</p><p>想要了解如何实现自己的云控制器管理器，或者对现有项目进行扩展么？</p><p>云控制器管理器使用 Go 语言的接口，从而使得针对各种云平台的具体实现都可以接入。
其中使用了在 <a href=https://github.com/kubernetes/cloud-provider>kubernetes/cloud-provider</a>
项目中 <a href=https://github.com/kubernetes/cloud-provider/blob/release-1.21/cloud.go#L42-L69><code>cloud.go</code></a>
文件所定义的 <code>CloudProvider</code> 接口。</p><p>本文中列举的共享控制器（节点控制器、路由控制器和服务控制器等）的实现以及其他一些生成具有
CloudProvider 接口的框架的代码，都是 Kubernetes 的核心代码。
特定于云驱动的实现虽不是 Kubernetes 核心成分，仍要实现 <code>CloudProvider</code> 接口。</p><p>关于如何开发插件的详细信息，
可参考<a href=/zh-cn/docs/tasks/administer-cluster/developing-cloud-controller-manager/>开发云控制器管理器</a>文档。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c20ec7d296cc2c8668bb204c2af31180>5 - 关于 cgroup v2</h1><p>在 Linux 上，<a class=glossary-tooltip title='一组具有可选资源隔离、审计和限制的 Linux 进程。' data-toggle=tooltip data-placement=top href='/zh-cn/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label=控制组>控制组</a>约束分配给进程的资源。</p><p><a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> 和底层容器运行时都需要对接 cgroup
来强制执行<a href=/zh-cn/docs/concepts/configuration/manage-resources-containers/>为 Pod 和容器管理资源</a>，
这包括为容器化工作负载配置 CPU/内存请求和限制。</p><p>Linux 中有两个 cgroup 版本：cgroup v1 和 cgroup v2。cgroup v2 是新一代的 <code>cgroup</code> API。</p><h2 id=cgroup-v2>什么是 cgroup v2？</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.25 [stable]</code></div><p>cgroup v2 是 Linux <code>cgroup</code> API 的下一个版本。cgroup v2 提供了一个具有增强资源管理能力的统一控制系统。</p><p>cgroup v2 对 cgroup v1 进行了多项改进，例如：</p><ul><li>API 中单个统一的层次结构设计</li><li>更安全的子树委派给容器</li><li>更新的功能特性，
例如<a href=https://www.kernel.org/doc/html/latest/accounting/psi.html>压力阻塞信息（Pressure Stall Information，PSI）</a></li><li>跨多个资源的增强资源分配管理和隔离<ul><li>统一核算不同类型的内存分配（网络内存、内核内存等）</li><li>考虑非即时资源变化，例如页面缓存回写</li></ul></li></ul><p>一些 Kubernetes 特性专门使用 cgroup v2 来增强资源管理和隔离。
例如，<a href=/blog/2021/11/26/qos-memory-resources/>MemoryQoS</a> 特性改进了内存 QoS 并依赖于 cgroup v2 原语。</p><h2 id=using-cgroupv2>使用 cgroup v2</h2><p>使用 cgroup v2 的推荐方法是使用一个默认启用 cgroup v2 的 Linux 发行版。</p><p>要检查你的发行版是否使用 cgroup v2，请参阅<a href=#check-cgroup-version>识别 Linux 节点上的 cgroup 版本</a>。</p><h3 id=requirements>要求</h3><p>cgroup v2 具有以下要求：</p><ul><li>操作系统发行版启用 cgroup v2</li><li>Linux 内核为 5.8 或更高版本</li><li>容器运行时支持 cgroup v2。例如：<ul><li><a href=https://containerd.io/>containerd</a> v1.4 和更高版本</li><li><a href=https://cri-o.io/>cri-o</a> v1.20 和更高版本</li></ul></li><li>kubelet 和容器运行时被配置为使用
<a href=/zh-cn/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver>systemd cgroup 驱动</a></li></ul><h3 id=linux-distribution-cgroup-v2-support>Linux 发行版 cgroup v2 支持</h3><p>有关使用 cgroup v2 的 Linux 发行版的列表，
请参阅 <a href=https://github.com/opencontainers/runc/blob/main/docs/cgroup-v2.md>cgroup v2 文档</a>。</p><ul><li>Container-Optimized OS（从 M97 开始）</li><li>Ubuntu（从 21.10 开始，推荐 22.04+）</li><li>Debian GNU/Linux（从 Debian 11 Bullseye 开始）</li><li>Fedora（从 31 开始）</li><li>Arch Linux（从 2021 年 4 月开始）</li><li>RHEL 和类似 RHEL 的发行版（从 9 开始）</li></ul><p>要检查你的发行版是否使用 cgroup v2，
请参阅你的发行版文档或遵循<a href=#check-cgroup-version>识别 Linux 节点上的 cgroup 版本</a>中的指示说明。</p><p>你还可以通过修改内核 cmdline 引导参数在你的 Linux 发行版上手动启用 cgroup v2。
如果你的发行版使用 GRUB，则应在 <code>/etc/default/grub</code> 下的 <code>GRUB_CMDLINE_LINUX</code>
中添加 <code>systemd.unified_cgroup_hierarchy=1</code>，
然后执行 <code>sudo update-grub</code>。不过，推荐的方法仍是使用一个默认已启用 cgroup v2 的发行版。</p><h3 id=migrating-cgroupv2>迁移到 cgroup v2</h3><p>要迁移到 cgroup v2，需确保满足<a href=#requirements>要求</a>，然后升级到一个默认启用 cgroup v2 的内核版本。</p><p>kubelet 能够自动检测操作系统是否运行在 cgroup v2 上并相应调整其操作，无需额外配置。</p><p>切换到 cgroup v2 时，用户体验应没有任何明显差异，除非用户直接在节点上或从容器内访问 cgroup 文件系统。</p><p>cgroup v2 使用一个与 cgroup v1 不同的 API，因此如果有任何应用直接访问 cgroup 文件系统，
则需要将这些应用更新为支持 cgroup v2 的版本。例如：</p><ul><li>一些第三方监控和安全代理可能依赖于 cgroup 文件系统。你要将这些代理更新到支持 cgroup v2 的版本。</li><li>如果以独立的 DaemonSet 的形式运行 <a href=https://github.com/google/cadvisor>cAdvisor</a> 以监控 Pod 和容器，
需将其更新到 v0.43.0 或更高版本。</li><li>如果你使用 JDK，推荐使用 JDK 11.0.16 及更高版本或 JDK 15 及更高版本，
以便<a href=https://bugs.openjdk.org/browse/JDK-8230305>完全支持 cgroup v2</a>。</li></ul><h2 id=check-cgroup-version>识别 Linux 节点上的 cgroup 版本</h2><p>cgroup 版本取决于正在使用的 Linux 发行版和操作系统上配置的默认 cgroup 版本。
要检查你的发行版使用的是哪个 cgroup 版本，请在该节点上运行 <code>stat -fc %T /sys/fs/cgroup/</code> 命令：</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>stat -fc %T /sys/fs/cgroup/
</span></span></code></pre></div><p>对于 cgroup v2，输出为 <code>cgroup2fs</code>。</p><p>对于 cgroup v1，输出为 <code>tmpfs</code>。</p><h2 id=接下来>接下来</h2><ul><li>进一步了解 <a href=https://man7.org/linux/man-pages/man7/cgroups.7.html>cgroups</a></li><li>进一步了解<a href=/zh-cn/docs/concepts/architecture/cri>容器运行时</a></li><li>进一步了解 <a href=/zh-cn/docs/setup/production-environment/container-runtimes#cgroup-drivers>cgroup 驱动</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c0ea5310f52e22c5de34dc84d9ab5e0d>6 - 容器运行时接口（CRI）</h1><p>CRI 是一个插件接口，它使 kubelet 能够使用各种容器运行时，无需重新编译集群组件。</p><p>你需要在集群中的每个节点上都有一个可以正常工作的<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>，
这样 <a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> 能启动
<a class=glossary-tooltip title='Pod 表示你的集群上一组正在运行的容器。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> 及其容器。</p><p><p>容器运行时接口（CRI）是 kubelet 和容器运行时之间通信的主要协议。</p></p><p>Kubernetes 容器运行时接口（Container Runtime Interface；CRI）定义了主要 <a href=https://grpc.io>gRPC</a> 协议，
用于<a href=/zh-cn/docs/concepts/overview/components/#node-components>集群组件</a>
<a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> 和
<a class=glossary-tooltip title=容器运行时是负责运行容器的软件。 data-toggle=tooltip data-placement=top href=/zh-cn/docs/setup/production-environment/container-runtimes target=_blank aria-label=容器运行时>容器运行时</a>。</p><h2 id=api>API</h2><div style=margin-top:10px;margin-bottom:10px><b>特性状态：</b> <code>Kubernetes v1.23 [stable]</code></div><p>当通过 gRPC 连接到容器运行时，kubelet 将充当客户端。运行时和镜像服务端点必须在容器运行时中可用，
可以使用 <code>--image-service-endpoint</code> 和 <code>--container-runtime-endpoint</code>
<a href=/zh-cn/docs/reference/command-line-tools-reference/kubelet>命令行标志</a>在 kubelet 中单独配置。</p><p>对 Kubernetes v1.25，kubelet 偏向于使用 CRI <code>v1</code> 版本。
如果容器运行时不支持 CRI 的 <code>v1</code> 版本，那么 kubelet 会尝试协商任何旧的其他支持版本。
如果 kubelet 无法协商支持的 CRI 版本，则 kubelet 放弃并且不会注册为节点。</p><h2 id=upgrading>升级</h2><p>升级 Kubernetes 时，kubelet 会尝试在组件重启时自动选择最新的 CRI 版本。
如果失败，则将如上所述进行回退。如果由于容器运行时已升级而需要 gRPC 重拨，
则容器运行时还必须支持最初选择的版本，否则重拨预计会失败。
这需要重新启动 kubelet。</p><h2 id=接下来>接下来</h2><ul><li>了解更多有关 CRI <a href=https://github.com/kubernetes/cri-api/blob/c75ef5b/pkg/apis/runtime/v1/api.proto>协议定义</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-44a2e2e592af0846101e970aff9243e5>7 - 垃圾收集</h1><p>垃圾收集（Garbage Collection）是 Kubernetes 用于清理集群资源的各种机制的统称。
垃圾收集允许系统清理如下资源：</p><ul><li><a href=/zh-cn/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection>终止的 Pod</a></li><li><a href=/zh-cn/docs/concepts/workloads/controllers/ttlafterfinished/>已完成的 Job</a></li><li><a href=#owners-dependents>不再存在属主引用的对象</a></li><li><a href=#containers-images>未使用的容器和容器镜像</a></li><li><a href=/zh-cn/docs/concepts/storage/persistent-volumes/#delete>动态制备的、StorageClass 回收策略为 Delete 的 PV 卷</a></li><li><a href=/zh-cn/docs/reference/access-authn-authz/certificate-signing-requests/#request-signing-process>阻滞或者过期的 CertificateSigningRequest (CSR)</a></li><li>在以下情形中删除了的<a class=glossary-tooltip title='Kubernetes 中的工作机器称作节点。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/architecture/nodes/ target=_blank aria-label=节点>节点</a>对象：<ul><li>当集群使用<a href=/zh-cn/docs/concepts/architecture/cloud-controller/>云控制器管理器</a>运行于云端时；</li><li>当集群使用类似于云控制器管理器的插件运行在本地环境中时。</li></ul></li><li><a href=/zh-cn/docs/concepts/architecture/nodes/#heartbeats>节点租约对象</a></li></ul><h2 id=owners-dependents>属主与依赖</h2><p>Kubernetes 中很多对象通过<a href=/zh-cn/docs/concepts/overview/working-with-objects/owners-dependents/><strong>属主引用</strong></a>
链接到彼此。属主引用（Owner Reference）可以告诉控制面哪些对象依赖于其他对象。
Kubernetes 使用属主引用来为控制面以及其他 API 客户端在删除某对象时提供一个清理关联资源的机会。
在大多数场合，Kubernetes 都是自动管理属主引用的。</p><p>属主关系与某些资源所使用的<a href=/zh-cn/docs/concepts/overview/working-with-objects/labels/>标签和选择算符</a>不同。
例如，考虑一个创建 <code>EndpointSlice</code> 对象的 <a class=glossary-tooltip title='将运行在一组 Pods 上的应用程序公开为网络服务的抽象方法。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>。
Service 使用<strong>标签</strong>来允许控制面确定哪些 <code>EndpointSlice</code> 对象被该 Service 使用。
除了标签，每个被 Service 托管的 <code>EndpointSlice</code> 对象还有一个属主引用属性。
属主引用可以帮助 Kubernetes 中的不同组件避免干预并非由它们控制的对象。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>根据设计，系统不允许出现跨名字空间的属主引用。名字空间作用域的依赖对象可以指定集群作用域或者名字空间作用域的属主。
名字空间作用域的属主<strong>必须</strong>存在于依赖对象所在的同一名字空间。
如果属主位于不同名字空间，则属主引用被视为不存在，而当检查发现所有属主都已不存在时，依赖对象会被删除。</p><p>集群作用域的依赖对象只能指定集群作用域的属主。
在 1.20 及更高版本中，如果一个集群作用域的依赖对象指定了某个名字空间作用域的类别作为其属主，
则该对象被视为拥有一个无法解析的属主引用，因而无法被垃圾收集处理。</p><p>在 1.20 及更高版本中，如果垃圾收集器检测到非法的跨名字空间 <code>ownerReference</code>，
或者某集群作用域的依赖对象的 <code>ownerReference</code> 引用某名字空间作用域的类别，
系统会生成一个警告事件，其原因为 <code>OwnerRefInvalidNamespace</code>，<code>involvedObject</code>
设置为非法的依赖对象。你可以通过运行
<code>kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace</code>
来检查是否存在这类事件。</p></div><h2 id=cascading-deletion>级联删除</h2><p>Kubernetes 会检查并删除那些不再拥有属主引用的对象，例如在你删除了 ReplicaSet
之后留下来的 Pod。当你删除某个对象时，你可以控制 Kubernetes 是否去自动删除该对象的依赖对象，
这个过程称为 <strong>级联删除（Cascading Deletion）</strong>。
级联删除有两种类型，分别如下：</p><ul><li>前台级联删除</li><li>后台级联删除</li></ul><p>你也可以使用 Kubernetes <a class=glossary-tooltip title='一个带有命名空间的键，告诉 Kubernetes 等到特定的条件被满足后， 再完全删除被标记为删除的资源。' data-toggle=tooltip data-placement=top href=/zh-cn/docs/concepts/overview/working-with-objects/finalizers/ target=_blank aria-label=Finalizers>Finalizers</a>
来控制垃圾收集机制如何以及何时删除包含属主引用的资源。</p><h3 id=foreground-deletion>前台级联删除</h3><p>在前台级联删除中，正在被你删除的属主对象首先进入 <strong>deletion in progress</strong> 状态。
在这种状态下，针对属主对象会发生以下事情：</p><ul><li>Kubernetes API 服务器将某对象的 <code>metadata.deletionTimestamp</code>
字段设置为该对象被标记为要删除的时间点。</li><li>Kubernetes API 服务器也会将 <code>metadata.finalizers</code> 字段设置为 <code>foregroundDeletion</code>。</li><li>在删除过程完成之前，通过 Kubernetes API 仍然可以看到该对象。</li></ul><p>当属主对象进入删除过程中状态后，控制器删除其依赖对象。控制器在删除完所有依赖对象之后，
删除属主对象。这时，通过 Kubernetes API 就无法再看到该对象。</p><p>在前台级联删除过程中，唯一可能阻止属主对象被删除的是那些带有
<code>ownerReference.blockOwnerDeletion=true</code> 字段的依赖对象。
参阅<a href=/zh-cn/docs/tasks/administer-cluster/use-cascading-deletion/#use-foreground-cascading-deletion>使用前台级联删除</a>
以了解进一步的细节。</p><h3 id=background-deletion>后台级联删除</h3><p>在后台级联删除过程中，Kubernetes 服务器立即删除属主对象，控制器在后台清理所有依赖对象。
默认情况下，Kubernetes 使用后台级联删除方案，除非你手动设置了要使用前台删除，
或者选择遗弃依赖对象。</p><p>参阅<a href=/zh-cn/docs/tasks/administer-cluster/use-cascading-deletion/#use-background-cascading-deletion>使用后台级联删除</a>
以了解进一步的细节。</p><h3 id=orphaned-dependents>被遗弃的依赖对象</h3><p>当 Kubernetes 删除某个属主对象时，被留下来的依赖对象被称作被遗弃的（Orphaned）对象。
默认情况下，Kubernetes 会删除依赖对象。要了解如何重载这种默认行为，可参阅
<a href=/zh-cn/docs/tasks/administer-cluster/use-cascading-deletion/#set-orphan-deletion-policy>删除属主对象和遗弃依赖对象</a>。</p><h2 id=containers-images>未使用容器和镜像的垃圾收集</h2><p><a class=glossary-tooltip title='一个在集群中每个节点上运行的代理。它保证容器都运行在 Pod 中。' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> 会每五分钟对未使用的镜像执行一次垃圾收集，
每分钟对未使用的容器执行一次垃圾收集。
你应该避免使用外部的垃圾收集工具，因为外部工具可能会破坏 kubelet
的行为，移除应该保留的容器。</p><p>要配置对未使用容器和镜像的垃圾收集选项，可以使用一个
<a href=/zh-cn/docs/tasks/administer-cluster/kubelet-config-file/>配置文件</a>，基于
<a href=/zh-cn/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration><code>KubeletConfiguration</code></a>
资源类型来调整与垃圾收集相关的 kubelet 行为。</p><h3 id=container-image-lifecycle>容器镜像生命周期</h3><p>Kubernetes 通过其<strong>镜像管理器（Image Manager）</strong> 来管理所有镜像的生命周期，
该管理器是 kubelet 的一部分，工作时与
<a class=glossary-tooltip title=帮助理解容器的资源用量与性能特征的工具。 data-toggle=tooltip data-placement=top href=https://github.com/google/cadvisor/ target=_blank aria-label=cadvisor>cadvisor</a> 协同。
kubelet 在作出垃圾收集决定时会考虑如下磁盘用量约束：</p><ul><li><code>HighThresholdPercent</code></li><li><code>LowThresholdPercent</code></li></ul><p>磁盘用量超出所配置的 <code>HighThresholdPercent</code> 值时会触发垃圾收集，
垃圾收集器会基于镜像上次被使用的时间来按顺序删除它们，首先删除的是最近未使用的镜像。
kubelet 会持续删除镜像，直到磁盘用量到达 <code>LowThresholdPercent</code> 值为止。</p><h3 id=container-image-garbage-collection>容器垃圾收集</h3><p>kubelet 会基于如下变量对所有未使用的容器执行垃圾收集操作，这些变量都是你可以定义的：</p><ul><li><code>MinAge</code>：kubelet 可以垃圾回收某个容器时该容器的最小年龄。设置为 <code>0</code>
表示禁止使用此规则。</li><li><code>MaxPerPodContainer</code>：每个 Pod 可以包含的已死亡的容器个数上限。设置为小于 <code>0</code>
的值表示禁止使用此规则。</li><li><code>MaxContainers</code>：集群中可以存在的已死亡的容器个数上限。设置为小于 <code>0</code>
的值意味着禁止应用此规则。</li></ul><p>除以上变量之外，kubelet 还会垃圾收集除无标识的以及已删除的容器，通常从最近未使用的容器开始。</p><p>当保持每个 Pod 的最大数量的容器（<code>MaxPerPodContainer</code>）会使得全局的已死亡容器个数超出上限
（<code>MaxContainers</code>）时，<code>MaxPerPodContainers</code> 和 <code>MaxContainers</code> 之间可能会出现冲突。
在这种情况下，kubelet 会调整 <code>MaxPerPodContainer</code> 来解决这一冲突。
最坏的情形是将 <code>MaxPerPodContainer</code> 降格为 <code>1</code>，并驱逐最近未使用的容器。
此外，当隶属于某已被删除的 Pod 的容器的年龄超过 <code>MinAge</code> 时，它们也会被删除。</p><div class="alert alert-info note callout" role=alert><strong>说明：</strong><p>kubelet 仅会回收由它所管理的容器。</div><h2 id=configuring-gc>配置垃圾收集</h2><p>你可以通过配置特定于管理资源的控制器来调整资源的垃圾收集行为。
下面的页面为你展示如何配置垃圾收集：</p><ul><li><a href=/zh-cn/docs/tasks/administer-cluster/use-cascading-deletion/>配置 Kubernetes 对象的级联删除</a></li><li><a href=/zh-cn/docs/concepts/workloads/controllers/ttlafterfinished/>配置已完成 Job 的清理</a></li></ul><h2 id=接下来>接下来</h2><ul><li>进一步了解 <a href=/zh-cn/docs/concepts/overview/working-with-objects/owners-dependents/>Kubernetes 对象的属主关系</a>。</li><li>进一步了解 Kubernetes <a href=/zh-cn/docs/concepts/overview/working-with-objects/finalizers/>finalizers</a>。</li><li>进一步了解 <a href=/zh-cn/docs/concepts/workloads/controllers/ttlafterfinished/>TTL 控制器</a> (Beta)，
该控制器负责清理已完成的 Job。</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/zh-cn/docs/home/>主页</a>
<a class=text-white href=/zh-cn/blog/>博客</a>
<a class=text-white href=/zh-cn/training/>培训</a>
<a class=text-white href=/zh-cn/partners/>合作伙伴</a>
<a class=text-white href=/zh-cn/community/>社区</a>
<a class=text-white href=/zh-cn/case-studies/>案例分析</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes 作者 | 文档发布基于 <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a> 授权许可</small><br><small class=text-white>Copyright &copy; 2023 Linux 基金会&reg;。保留所有权利。Linux 基金会已注册并使用商标。如需了解 Linux 基金会的商标列表，请访问<a href=https://www.linuxfoundation.org/trademark-usage class=light-text>商标使用页面</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>