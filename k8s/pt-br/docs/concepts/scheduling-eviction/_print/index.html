<!doctype html><html lang=pt-br class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/scheduling-eviction/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/pt-br/docs/concepts/scheduling-eviction/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Escalonamento | Kubernetes</title><meta property="og:title" content="Escalonamento"><meta property="og:description" content="No Kubernetes, agendamento refere-se a garantia de que os pods correspondam aos nós para que o kubelet possa executá-los. Remoção é o processo de falha proativa de um ou mais pods em nós com falta de recursos.
"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/pt-br/docs/concepts/scheduling-eviction/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Escalonamento"><meta itemprop=description content="No Kubernetes, agendamento refere-se a garantia de que os pods correspondam aos nós para que o kubelet possa executá-los. Remoção é o processo de falha proativa de um ou mais pods em nós com falta de recursos.
"><meta name=twitter:card content="summary"><meta name=twitter:title content="Escalonamento"><meta name=twitter:description content="No Kubernetes, agendamento refere-se a garantia de que os pods correspondam aos nós para que o kubelet possa executá-los. Remoção é o processo de falha proativa de um ou mais pods em nós com falta de recursos.
"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="No Kubernetes, agendamento refere-se a garantia de que os pods correspondam aos nós para que o kubelet possa executá-los. Remoção é o processo de falha proativa de um ou mais pods em nós com falta de recursos.
"><meta property="og:description" content="No Kubernetes, agendamento refere-se a garantia de que os pods correspondam aos nós para que o kubelet possa executá-los. Remoção é o processo de falha proativa de um ou mais pods em nós com falta de recursos.
"><meta name=twitter:description content="No Kubernetes, agendamento refere-se a garantia de que os pods correspondam aos nós para que o kubelet possa executá-los. Remoção é o processo de falha proativa de um ou mais pods em nós com falta de recursos.
"><meta property="og:url" content="https://kubernetes.io/pt-br/docs/concepts/scheduling-eviction/"><meta property="og:title" content="Escalonamento"><meta name=twitter:title content="Escalonamento"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/pt-br/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/pt-br/docs/>Documentação</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/partners/>Parceiros</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/community/>Comunidade</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/case-studies/>Casos de estudo</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versões</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/pt-br/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/pt-br/docs/concepts/scheduling-eviction/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/pt-br/docs/concepts/scheduling-eviction/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/pt-br/docs/concepts/scheduling-eviction/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/pt-br/docs/concepts/scheduling-eviction/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/pt-br/docs/concepts/scheduling-eviction/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Português (Portuguese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/scheduling-eviction/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/scheduling-eviction/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/scheduling-eviction/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/scheduling-eviction/>日本語 (Japanese)</a>
<a class=dropdown-item href=/id/docs/concepts/scheduling-eviction/>Bahasa Indonesia</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Essa é a versão completa de impressão dessa seção
<a href=# onclick="return print(),!1">Clique aqui para imprimir</a>.</p><p><a href=/pt-br/docs/concepts/scheduling-eviction/>Retornar à visualização normal</a>.</p></div><h1 class=title>Escalonamento</h1><div class=lead>No Kubernetes, agendamento refere-se a garantia de que os pods correspondam aos nós para que o kubelet possa executá-los. Remoção é o processo de falha proativa de um ou mais pods em nós com falta de recursos.</div><ul><li>1: <a href=#pg-ede4960b56a3529ee0bfe7c8fe2d09a5>Taints e Tolerâncias</a></li><li>2: <a href=#pg-598f36d691ab197f9d995784574b0a12>Escalonador do Kubernetes</a></li><li>3: <a href=#pg-da22fe2278df236f71efbe672f392677>Sobrecarga de Pod</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-ede4960b56a3529ee0bfe7c8fe2d09a5>1 - Taints e Tolerâncias</h1><p><a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity><em>Afinidade de nó</em></a>
é uma propriedade dos <a class=glossary-tooltip title='O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> que os <em>associa</em> a um conjunto de <a class=glossary-tooltip title='Um Nó é uma máquina de trabalho no Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nós>nós</a> (seja como uma preferência ou uma exigência). <em>Taints</em> são o oposto -- eles permitem que um nó repudie um conjunto de pods.</p><p><em>Tolerâncias</em> são aplicadas em pods e permitem, mas não exigem, que os pods sejam alocados em nós com <em>taints</em> correspondentes.</p><p>Taints e tolerâncias trabalham juntos para garantir que pods não sejam alocados em nós inapropriados. Um ou mais taints são aplicados em um nó; isso define que o nó não deve aceitar nenhum pod que não tolera essas taints.</p><h2 id=conceitos>Conceitos</h2><p>Você adiciona um taint a um nó utilizando <a href=/docs/reference/generated/kubectl/kubectl-commands#taint>kubectl taint</a>.
Por exemplo,</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</span></span></code></pre></div><p>define um taint no nó <code>node1</code>. O taint tem a chave <code>key1</code>, valor <code>value1</code> e o efeito <code>NoSchedule</code>.
Isso significa que nenhum pod conseguirá ser executado no nó <code>node1</code> a menos que possua uma tolerância correspondente.</p><p>Para remover o taint adicionado pelo comando acima, você pode executar:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule-
</span></span></code></pre></div><p>Você especifica uma tolerância para um pod na <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1#PodSpec>especificação do Pod</a>. Ambas as seguintes tolerâncias "correspondem" ao taint criado pelo <code>kubectl taint</code> acima, e assim um pod com qualquer uma delas poderia ser executado no <code>node1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Aqui está um exemplo de um pod que utiliza tolerâncias:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/pt-br/examples/pods/pod-with-toleration.yaml download=pods/pod-with-toleration.yaml><code>pods/pod-with-toleration.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-toleration-yaml")' title="Copy pods/pod-with-toleration.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-toleration-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example-key&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>O valor padrão de <code>operator</code> é <code>Equal</code>.</p><p>Uma tolerância "casa" um taint se as chaves e efeitos são os mesmos, e:</p><ul><li>o valor de <code>operator</code> é <code>Exists</code> (no caso nenhum <code>value</code> deve ser especificado), ou</li><li>o valor de <code>operator</code> é <code>Equal</code> e os valores de <code>value</code> são iguais.</li></ul><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Existem dois casos especiais:</p><p>Uma <code>key</code> vazia com o operador <code>Exists</code> "casa" todas as chaves, valores e efeitos, o que significa que o pod irá tolerar tudo.</p><p>Um <code>effect</code> vazio "casa" todos os efeitos com a chave <code>key1</code>.</p></div><p>O exemplo acima usou <code>effect</code> de <code>NoSchedule</code>. De forma alternativa, você pode usar <code>effect</code> de <code>PreferNoSchedule</code>.
Nesse efeito, o sistema <em>tentará</em> evitar que o pod seja alocado ao nó caso ele não tolere os taints definidos, contudo a alocação não será evitada de forma obrigatória. Pode-se dizer que o <code>PreferNoSchedule</code> é uma versão permissiva do <code>NoSchedule</code>. O terceiro tipo de <code>effect</code> é o <code>NoExecute</code> que será descrito posteriormente.</p><p>Você pode colocar múltiplos taints no mesmo nó e múltiplas tolerâncias no mesmo pod.
O jeito que o Kubernetes processa múltiplos taints e tolerâncias é como um filtro: começa com todos os taints de um nó, em seguida ignora aqueles para os quais o pod tem uma tolerância relacionada; os taints restantes que não foram ignorados indicam o efeito no pod. Mais especificamente,</p><ul><li>se existe pelo menos um taint não tolerado com o efeito <code>NoSchedule</code>, o Kubernetes não alocará o pod naquele nó</li><li>se existe um taint não tolerado com o efeito <code>NoSchedule</code>, mas existe pelo menos um taint não tolerado com o efeito <code>PreferNoSchedule</code>, o Kubernetes <em>tentará</em> não alocar o pod no nó</li><li>se existe pelo menos um taint não tolerado com o efeito <code>NoExecute</code>, o pod será expulso do nó (caso já esteja em execução) e não será alocado ao nó (caso ainda não esteja em execução).</li></ul><p>Por exemplo, imagine que você tem um nó com os seguintes taints</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</span></span><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoExecute
</span></span><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key2</span><span style=color:#666>=</span>value2:NoSchedule
</span></span></code></pre></div><p>E um pod com duas tolerâncias:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Nesse caso, o pod não será alocado ao nó porque não possui uma tolerância para o terceiro taint. Porém, se ele já estiver rodando no nó quando o taint foi adicionado, não será afetado e continuará rodando, tendo em vista que o terceiro taint é o único não tolerado pelo pod.</p><p>Normalmente, se um taint com o efeito <code>NoExecute</code> é adicionado a um nó, qualquer pod que não o tolere será expulso imediatamente e pods que o toleram nunca serão expulsos. Contudo, uma tolerância com efeito <code>NoExecute</code> pode especificar de forma opcional o campo <code>tolerationSeconds</code>, que determina quanto tempo o pod continuará alocado ao nó depois que o taint é adicionado. Por exemplo,</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>3600</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>significa que se esse pod está sendo executado e um taint correspondente é adicionado ao nó, o pod irá continuar rodando neste nó por 3600 segundos e depois será expulso. Se o taint for removido antes desse tempo acabar, o pod não será expulso.</p><h2 id=exemplos-de-casos-de-uso>Exemplos de Casos de Uso</h2><p>Taints e tolerâncias são um modo flexível de conduzir pods para <em>fora</em> dos nós ou expulsar pods que não deveriam estar sendo executados. Alguns casos de uso são</p><ul><li><p><strong>Nós Dedicados</strong>: Se você quiser dedicar um conjunto de nós para uso exclusivo de um conjunto específico de usuários, poderá adicionar um taint nesses nós. (digamos, <code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>) e em seguida adicionar uma tolerância correspondente para seus pods (isso seria feito mais facilmente com a escrita de um <a href=/docs/reference/access-authn-authz/admission-controllers/>controlador de admissão</a> customizado).
Os pods com tolerância terão sua execução permitida nos nós com taints (dedicados), assim como em qualquer outro nó no cluster. Se você quiser dedicar nós a esses pods <em>e garantir</em> que eles usem <em>apenas</em> os nós dedicados, precisará adicionar uma label similar ao taint para o mesmo conjunto de nós (por exemplo, <code>dedicated=groupName</code>), e o controle de admissão deverá adicionar uma afinidade de nó para exigir que os pods podem ser executados apenas nos nós definidos com a label <code>dedicated=groupName</code>.</p></li><li><p><strong>Nós com hardware especial</strong>: Em um cluster no qual um pequeno grupo de nós possui hardware especializado (por exemplo, GPUs), é desejável manter pods que não necessitem desse tipo de hardware fora desses nós, dessa forma o recurso estará disponível para pods que precisem do hardware especializado. Isso pode ser feito aplicando taints nos nós com o hardware especializado (por exemplo, <code>kubectl taint nodes nodename special=true:NoSchedule</code> or <code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>) e aplicando uma tolerância correspondente nos pods que usam o hardware especial. Assim como no caso de uso de nós dedicados, é provavelmente mais fácil aplicar as tolerâncias utilizando um <a href=/docs/reference/access-authn-authz/admission-controllers/>controlador de admissão</a>.
Por exemplo, é recomendado usar <a href=/docs/concepts/configuration/manage-resources-containers/#extended-resources>Extended Resources</a> para representar hardware especial, adicione um taint ao seus nós de hardware especializado com o nome do recurso estendido e execute o controle de admissão <a href=/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration>ExtendedResourceToleration</a>. Agora, tendo em vista que os nós estão marcados com um taint, nenhum pod sem a tolerância será executado neles. Porém, quando você submete um pod que requisita o recurso estendido, o controlador de admissão <code>ExtendedResourceToleration</code> irá adicionar automaticamente as tolerâncias necessárias ao pod que irá, por sua vez, ser alocado no nó com hardware especial. Isso garantirá que esses nós de hardware especial serão dedicados para os pods que requisitarem tal recurso e você não precisará adicionar manualmente as tolerâncias aos seus pods.</p></li><li><p><strong>Expulsões baseadas em Taint</strong>: Um comportamento de expulsão configurada por pod quando problemas existem em um nó, o qual será descrito na próxima seção.</p></li></ul><h2 id=expulsões-baseadas-em-taint>Expulsões baseadas em Taint</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code></div><p>O efeito de taint <code>NoExecute</code>, mencionado acima, afeta pods que já estão rodando no nó da seguinte forma</p><ul><li>pods que não toleram o taint são expulsos imediatamente</li><li>pods que toleram o taint sem especificar <code>tolerationSeconds</code> em sua especificação de tolerância, ficam alocados para sempre</li><li>pods que toleram o taint com um <code>tolerationSeconds</code> especificado, permanecem alocados pela quantidade de tempo definida</li></ul><p>O controlador de nó automaticamente adiciona um taint ao Nó quando certas condições se tornam verdadeiras. Os seguintes taints são embutidos:</p><ul><li><code>node.kubernetes.io/not-ready</code>: Nó não está pronto. Isso corresponde ao NodeCondition <code>Ready</code> com o valor "<code>False</code>".</li><li><code>node.kubernetes.io/unreachable</code>: Nó é inalcançável a partir do controlador de nó. Isso corresponde ao NodeCondition <code>Ready</code> com o valor "<code>Unknown</code>".</li><li><code>node.kubernetes.io/memory-pressure</code>: Nó possui pressão de memória.</li><li><code>node.kubernetes.io/disk-pressure</code>: Nó possui pressão de disco.</li><li><code>node.kubernetes.io/pid-pressure</code>: Nó possui pressão de PID.</li><li><code>node.kubernetes.io/network-unavailable</code>: A rede do nó está indisponível.</li><li><code>node.kubernetes.io/unschedulable</code>: Nó não é alocável.</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>: Quando o kubelet é iniciado com um provedor de nuvem "externo", esse taint é adicionado ao nó para que ele seja marcado como não utilizável. Após o controlador do cloud-controller-manager inicializar o nó, o kubelet remove esse taint.</li></ul><p>No caso de um nó estar prestes a ser expulso, o controlador de nó ou kubelet adicionam os taints relevantes com o efeito <code>NoExecute</code>. Se a condição de falha retorna ao normal, o kubelet ou controlador de nó podem remover esses taints.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> A camada de gerenciamento limita a taxa de adição de novos taints aos nós. Esse limite gerencia o número de expulsões que são disparadas quando muitos nós se tornam inalcançáveis ao mesmo tempo (por exemplo: se ocorre uma falha na rede).</div><p>Você pode especificar <code>tolerationSeconds</code> em um Pod para definir quanto tempo ele ficará alocado em um nó que está falhando ou está sem resposta.</p><p>Por exemplo, você talvez queira manter uma aplicação com vários estados salvos localmente alocado em um nó por um longo período na ocorrência de uma divisão na rede, esperando que essa divisão se recuperará e assim a expulsão do pod pode ser evitada.
A tolerância que você define para esse Pod poderia ficar assim:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;node.kubernetes.io/unreachable&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>6000</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>O Kubernetes automaticamente adiciona uma tolerância para <code>node.kubernetes.io/not-ready</code> e <code>node.kubernetes.io/unreachable</code> com <code>tolerationSeconds=300</code>, a menos que você, ou um controlador, defina essas tolerâncias explicitamente.</p><p>Essas tolerâncias adicionadas automaticamente significam que Pods podem continuar alocados aos Nós por 5 minutos após um desses problemas ser detectado.</p></div><p>Pods do tipo <a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> são criados com tolerâncias <code>NoExecute</code> sem a propriedade <code>tolerationSeconds</code> para os seguintes taints:</p><ul><li><code>node.kubernetes.io/unreachable</code></li><li><code>node.kubernetes.io/not-ready</code></li></ul><p>Isso garante que esses pods do DaemonSet nunca sejam expulsos por conta desses problemas.</p><h2 id=taints-por-condições-de-nó>Taints por condições de nó</h2><p>A camada de gerenciamento, usando o <a class=glossary-tooltip title='Um ciclo de controle que observa o estado partilhado do cluster através do API Server e efetua mudanças tentando mover o estado atual em direção ao estado desejado.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controlador>controlador</a> do nó, cria taints automaticamente com o efeito <code>NoSchedule</code> para <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions>condições de nó</a>.</p><p>O agendador verifica taints, não condições de nó, quando realiza suas decisões de agendamento. Isso garante que as condições de nó não afetem diretamente o agendamento.
Por exemplo, se a condição de nó <code>DiskPressure</code> está ativa, a camada de gerenciamento adiciona o taint <code>node.kubernetes.io/disk-pressure</code> e não aloca novos pods no nó afetado. Se a condição <code>MemoryPressure</code> está ativa, a camada de gerenciamento adiciona o taint <code>node.kubernetes.io/memory-pressure</code>.</p><p>Você pode ignorar condições de nó para pods recém-criados adicionando tolerâncias correspondentes. A camada de controle também adiciona a tolerância <code>node.kubernetes.io/memory-pressure</code> em pods que possuem uma <a class=glossary-tooltip title='QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction.' data-toggle=tooltip data-placement=top href='/pt-br/docs/reference/glossary/?all=true#term-qos-class' target=_blank aria-label='classe de QoS'>classe de QoS</a> diferente de <code>BestEffort</code>. Isso ocorre porque o Kubernetes trata pods nas classes de QoS <code>Guaranteed</code> ou <code>Burstable</code> (até mesmo pods sem requisitos de memória definidos) como se fossem capazes de lidar com pressão de memória, enquanto novos pods com <code>BestEffort</code> não são alocados no nó afetado.</p><p>O controlador DaemonSet adiciona automaticamente as seguintes tolerâncias de <code>NoSchedule</code> para todos os daemons, prevenindo que DaemonSets quebrem.</p><ul><li><code>node.kubernetes.io/memory-pressure</code></li><li><code>node.kubernetes.io/disk-pressure</code></li><li><code>node.kubernetes.io/pid-pressure</code> (1.14 ou superior)</li><li><code>node.kubernetes.io/unschedulable</code> (1.10 ou superior)</li><li><code>node.kubernetes.io/network-unavailable</code> (<em>somente rede do host</em>)</li></ul><p>Adicionando essas tolerâncias garante retro compatibilidade. Você também pode adicionar tolerâncias de forma arbitrária aos DaemonSets.</p><h2 id=próximos-passos>Próximos passos</h2><ul><li>Leia sobre <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a> e como você pode configurá-la</li><li>Leia sobre <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-598f36d691ab197f9d995784574b0a12>2 - Escalonador do Kubernetes</h1><p>No Kubernetes, <em>escalonamento</em> refere-se a garantir que os <a class=glossary-tooltip title='O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>
sejam correspondidos aos <a class=glossary-tooltip title='Um Nó é uma máquina de trabalho no Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a> para que o
<a class=glossary-tooltip title='Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=Kubelet>Kubelet</a> possa executá-los.</p><h2 id=escalonamento>Visão geral do Escalonamento</h2><p>Um escalonador observa Pods recém-criados que não possuem um Node atribuído.
Para cada Pod que o escalonador descobre, ele se torna responsável por
encontrar o melhor Node para execução do Pod. O escalonador chega a essa decisão de alocação levando em consideração os princípios de programação descritos abaixo.</p><p>Se você quiser entender por que os Pods são alocados em um Node específico
ou se planeja implementar um escalonador personalizado, esta página ajudará você a
aprender sobre escalonamento.</p><h2 id=kube-scheduler>kube-scheduler</h2><p><a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a>
é o escalonador padrão do Kubernetes e é executado como parte do
<a class=glossary-tooltip title='A camada de gerenciamento de contêiner que expõe a API e as interfaces para definir, implantar e gerenciar o ciclo de vida dos contêineres.' data-toggle=tooltip data-placement=top href='/pt-br/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>.
O kube-scheduler é projetado para que, se você quiser e precisar, possa
escrever seu próprio componente de escalonamento e usá-lo.</p><p>Para cada Pod recém-criado ou outros Pods não escalonados, o kube-scheduler
seleciona um Node ideal para execução. No entanto, todos os contêineres nos Pods
têm requisitos diferentes de recursos e cada Pod também possui requisitos diferentes.
Portanto, os Nodes existentes precisam ser filtrados de acordo com os requisitos de
escalonamento específicos.</p><p>Em um cluster, Nodes que atendem aos requisitos de escalonamento para um Pod
são chamados de Nodes <em>viáveis</em>. Se nenhum dos Nodes for adequado, o Pod
permanece não escalonado até que o escalonador possa alocá-lo.</p><p>O escalonador encontra Nodes viáveis para um Pod e, em seguida, executa um conjunto de
funções para pontuar os Nodes viáveis e escolhe um Node com a maior
pontuação entre os possíveis para executar o Pod. O escalonador então notifica
o servidor da API sobre essa decisão em um processo chamado <em>binding</em>.</p><p>Fatores que precisam ser levados em consideração para decisões de escalonamento incluem
requisitos individuais e coletivos de recursos,
restrições de hardware / software / política, especificações de afinidade e anti-afinidade,
localidade de dados, interferência entre cargas de trabalho e assim por diante.</p><h3 id=implementação-kube-scheduler>Seleção do Node no kube-scheduler</h3><p>O kube-scheduler seleciona um Node para o Pod em uma operação que consiste em duas etapas:</p><ol><li>Filtragem</li><li>Pontuação</li></ol><p>A etapa de <em>filtragem</em> localiza o conjunto de Nodes onde é possível
alocar o Pod. Por exemplo, o filtro PodFitsResources verifica se um Node
candidato possui recursos disponíveis suficientes para atender às solicitações
de recursos específicas de um Pod. Após esta etapa, a lista de Nodes contém
quaisquer Nodes adequados; frequentemente, haverá mais de um. Se a lista estiver vazia,
esse Pod (ainda) não é escalonável.</p><p>Na etapa de <em>pontuação</em>, o escalonador classifica os Nodes restantes para escolher
o mais adequado. O escalonador atribui uma pontuação a cada Node
que sobreviveu à filtragem, baseando essa pontuação nas regras de pontuação ativa.</p><p>Por fim, o kube-scheduler atribui o Pod ao Node com a classificação mais alta.
Se houver mais de um Node com pontuações iguais, o kube-scheduler seleciona
um deles aleatoriamente.</p><p>Existem duas maneiras suportadas de configurar o comportamento de filtragem e pontuação
do escalonador:</p><ol><li><p><a href=/docs/reference/scheduling/policies>Políticas de Escalonamento</a> permitem configurar <em>Predicados</em> para filtragem e <em>Prioridades</em> para pontuação.</p></li><li><p><a href=/docs/reference/scheduling/profiles>Perfis de Escalonamento</a> permitem configurar Plugins que implementam diferentes estágios de escalonamento, incluindo: <code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code>, e outros. Você também pode configurar o kube-scheduler para executar diferentes perfis.</p></li></ol><h2 id=próximos-passos>Próximos passos</h2><ul><li>Leia sobre <a href=/docs/concepts/scheduling/scheduler-perf-tuning/>ajuste de desempenho do escalonador</a></li><li>Leia sobre <a href=/docs/concepts/workloads/pods/pod-topology-spread-constraints/>restrições de propagação da topologia de pod</a></li><li>Leia a <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>documentação de referência</a> para o kube-scheduler</li><li>Aprenda como <a href=/docs/tasks/administer-cluster/configure-multiple-schedulers/>configurar vários escalonadores</a></li><li>Aprenda sobre <a href=/docs/tasks/administer-cluster/topology-manager/>políticas de gerenciamento de topologia</a></li><li>Aprenda sobre <a href=/docs/concepts/configuration/pod-overhead/>Pod Overhead</a></li><li>Saiba mais sobre o agendamento de pods que usam volumes em:<ul><li><a href=/docs/concepts/storage/storage-classes/#volume-binding-mode>Suporte de topologia de volume</a></li><li><a href=/docs/concepts/storage/storage-capacity/>Rastreamento de capacidade de armazenamento</a></li><li><a href=/docs/concepts/storage/storage-limits/>Limites de volumes específicos do nó</a></li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-da22fe2278df236f71efbe672f392677>3 - Sobrecarga de Pod</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code></div><p>Quando você executa um Pod num nó, o próprio Pod usa uma quantidade de recursos do sistema. Estes
recursos são adicionais aos recursos necessários para executar o(s) contêiner(s) dentro do Pod.
Sobrecarga de Pod, do inglês <em>Pod Overhead</em>, é uma funcionalidade que serve para contabilizar os recursos consumidos pela
infraestrutura do Pod para além das solicitações e limites do contêiner.</p><p>No Kubernetes, a sobrecarga de Pods é definido no tempo de
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks>admissão</a>
de acordo com a sobrecarga associada à
<a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a> do Pod.</p><p>Quando é ativada a Sobrecarga de Pod, a sobrecarga é considerada adicionalmente à soma das
solicitações de recursos do contêiner ao agendar um Pod. Semelhantemente, o <em>kubelet</em>
incluirá a sobrecarga do Pod ao dimensionar o cgroup do Pod e ao
executar a classificação de prioridade de migração do Pod em caso de <em>drain</em> do Node.</p><h2 id=set-up>Habilitando a Sobrecarga de Pod</h2><p>Terá de garantir que o <a href=/docs/reference/command-line-tools-reference/feature-gates/>Feature Gate</a>
<code>PodOverhead</code> esteja ativo (está ativo por padrão a partir da versão 1.18)
em todo o cluster, e uma <code>RuntimeClass</code> utilizada que defina o campo <code>overhead</code>.</p><h2 id=exemplo-de-uso>Exemplo de uso</h2><p>Para usar a funcionalidade PodOverhead, é necessário uma RuntimeClass que define o campo <code>overhead</code>.
Por exemplo, poderia usar a definição da RuntimeClass abaixo com um agente de execução de contêiner virtualizado
que use cerca de 120MiB por Pod para a máquina virtual e o sistema operacional convidado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>overhead</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podFixed</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;120Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>As cargas de trabalho que são criadas e que especificam o manipulador RuntimeClass <code>kata-fc</code> irão
usar a sobrecarga de memória e cpu em conta para os cálculos da quota de recursos, agendamento de nós,
assim como dimensionamento do cgroup do Pod.</p><p>Considere executar a seguinte carga de trabalho de exemplo, test-pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>stdin</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tty</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>1500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></span></span></code></pre></div><p>No tempo de admissão o <a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/>controlador de admissão</a> RuntimeClass
atualiza o <em>PodSpec</em> da carga de trabalho de forma a incluir o <code>overhead</code> como descrito na RuntimeClass. Se o <em>PodSpec</em> já tiver este campo definido
o Pod será rejeitado. No exemplo dado, como apenas o nome do RuntimeClass é especificado, o controlador de admissão muda o Pod de forma a
incluir um <code>overhead</code>.</p><p>Depois do controlador de admissão RuntimeClass, pode verificar o <em>PodSpec</em> atualizado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.overhead}&#39;</span>
</span></span></code></pre></div><p>A saída é:</p><pre tabindex=0><code>map[cpu:250m memory:120Mi]
</code></pre><p>Se for definido um <em>ResourceQuota</em>, a soma das requisições dos contêineres assim como o campo <code>overhead</code> são contados.</p><p>Quando o kube-scheduler está decidindo que nó deve executar um novo Pod, o agendador considera o <code>overhead</code> do pod,
assim como a soma de pedidos aos contêineres para esse <em>Pod</em>. Para este exemplo, o agendador adiciona as requisições e a sobrecarga, depois procura um nó com 2.25 CPU e 320 MiB de memória disponível.</p><p>Assim que um Pod é agendado a um nó, o kubelet nesse nó cria um novo <a class=glossary-tooltip title='Um grupo de processos do Linux com isolamento de recursos opcional, contagem e limites.' data-toggle=tooltip data-placement=top href='/pt-br/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label=cgroup>cgroup</a>
para o Pod. É dentro deste Pod que o agente de execução de contêiners subjacente vai criar contêineres.</p><p>Se o recurso tiver um limite definido para cada contêiner (<em>QoS</em> garantida ou <em>Burstrable QoS</em> com limites definidos),
o kubelet definirá um limite superior para o cgroup do Pod associado a esse recurso (cpu.cfs_quota_us para CPU
e memory.limit_in_bytes de memória). Este limite superior é baseado na soma dos limites do contêiner mais o <code>overhead</code>
definido no <em>PodSpec</em>.</p><p>Para CPU, se o Pod for QoS garantida ou <em>Burstrable QoS</em>, o kubelet vai definir <code>cpu.shares</code> baseado na soma dos
pedidos ao contêiner mais o <code>overhead</code> definido no <em>PodSpec</em>.</p><p>Olhando para o nosso exemplo, verifique as requisições ao contêiner para a carga de trabalho:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.containers[*].resources.limits}&#39;</span>
</span></span></code></pre></div><p>O total de requisições ao contêiner são 2000m CPU e 200MiB de memória:</p><pre tabindex=0><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre><p>Verifique isto comparado ao que é observado pelo nó:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe node | grep test-pod -B2
</span></span></code></pre></div><p>A saída mostra que 2250m CPU e 320MiB de memória são solicitados, que inclui <em>PodOverhead</em>:</p><pre tabindex=0><code>  Namespace                   Name                CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------                   ----                ------------  ----------   ---------------  -------------  ---
  default                     test-pod            2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre><h2 id=verificar-os-limites-cgroup-do-pod>Verificar os limites cgroup do Pod</h2><p>Verifique os cgroups de memória do Pod no nó onde a carga de trabalho está em execução. No seguinte exemplo, <a href=https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md><code>crictl</code></a>
é usado no nó, que fornece uma CLI para agentes de execução compatíveis com CRI. Isto é um
exemplo avançado para mostrar o comportamento do <em>PodOverhead</em>, e não é esperado que os usuários precisem verificar
cgroups diretamente no nó.</p><p>Primeiro, no nó em particular, determine o identificador do Pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># Execute no nó onde o Pod está agendado</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>POD_ID</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>sudo crictl pods --name test-pod -q<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div><p>A partir disto, pode determinar o caminho do cgroup para o <em>Pod</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># Execute no nó onde o Pod está agendado</span>
</span></span><span style=display:flex><span>sudo crictl inspectp -o<span style=color:#666>=</span>json <span style=color:#b8860b>$POD_ID</span> | grep cgroupsPath
</span></span></code></pre></div><p>O caminho do cgroup resultante inclui o contêiner <code>pause</code> do Pod. O cgroup no nível do Pod está um diretório acima.</p><pre tabindex=0><code>        &#34;cgroupsPath&#34;: &#34;/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a&#34;
</code></pre><p>Neste caso especifico, o caminho do cgroup do Pod é <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>. Verifique a configuração cgroup de nível do Pod para a memória:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># Execute no nó onde o Pod está agendado</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Mude também o nome do cgroup para combinar com o cgroup alocado ao Pod.</span>
</span></span><span style=display:flex><span> cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</span></span></code></pre></div><p>Isto é 320 MiB, como esperado:</p><pre tabindex=0><code>335544320
</code></pre><h3 id=observabilidade>Observabilidade</h3><p>Uma métrica <code>kube_pod_overhead</code> está disponível em <a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a>
para ajudar a identificar quando o <em>PodOverhead</em> está sendo utilizado e para ajudar a observar a estabilidade das cargas de trabalho
em execução com uma sobrecarga (<em>Overhead</em>) definida. Esta funcionalidade não está disponível na versão 1.9 do kube-state-metrics,
mas é esperado em uma próxima versão. Os usuários necessitarão entretanto construir o kube-state-metrics a partir do código fonte.</p><h2 id=próximos-passos>Próximos passos</h2><ul><li><a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a></li><li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>PodOverhead Design</a></li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/pt-br/docs/home/>Home</a>
<a class=text-white href=/pt-br/blog/>Blog</a>
<a class=text-white href=/pt-br/partners/>Parceiros</a>
<a class=text-white href=/pt-br/community/>Comunidade</a>
<a class=text-white href=/pt-br/case-studies/>Casos de estudo</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 Os autores do Kubernetes | Documentação Distribuída sob <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 A Fundação Linux &reg;. Todos os direitos reservados. A Linux Foundation tem marcas registradas e usa marcas registradas. Para uma lista de marcas registradas da The Linux Foundation, por favor, veja nossa <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Página de uso de marca registrada</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>