<!doctype html><html lang=pt-br class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/architecture/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/architecture/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/architecture/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/architecture/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/architecture/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/architecture/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/architecture/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/architecture/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/architecture/><link rel=alternate hreflang=vi href=https://kubernetes.io/vi/docs/concepts/architecture/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/architecture/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/pt-br/docs/concepts/architecture/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Arquitetura do Kubernetes | Kubernetes</title><meta property="og:title" content="Arquitetura do Kubernetes"><meta property="og:description" content="Orquestração de contêineres em nível de produção"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/pt-br/docs/concepts/architecture/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Arquitetura do Kubernetes"><meta itemprop=description content="Orquestração de contêineres em nível de produção"><meta name=twitter:card content="summary"><meta name=twitter:title content="Arquitetura do Kubernetes"><meta name=twitter:description content="Orquestração de contêineres em nível de produção"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/pt-br/docs/concepts/architecture/"><meta property="og:title" content="Arquitetura do Kubernetes"><meta name=twitter:title content="Arquitetura do Kubernetes"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/pt-br/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/pt-br/docs/>Documentação</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/partners/>Parceiros</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/community/>Comunidade</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/case-studies/>Casos de estudo</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versões</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/pt-br/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/pt-br/docs/concepts/architecture/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/pt-br/docs/concepts/architecture/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/pt-br/docs/concepts/architecture/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/pt-br/docs/concepts/architecture/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/pt-br/docs/concepts/architecture/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Português (Portuguese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/architecture/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/architecture/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/architecture/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/architecture/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/architecture/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/architecture/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/architecture/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/architecture/>Español (Spanish)</a>
<a class=dropdown-item href=/id/docs/concepts/architecture/>Bahasa Indonesia</a>
<a class=dropdown-item href=/vi/docs/concepts/architecture/>Tiếng Việt (Vietnamese)</a>
<a class=dropdown-item href=/ru/docs/concepts/architecture/>Русский (Russian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Essa é a versão completa de impressão dessa seção
<a href=# onclick="return print(),!1">Clique aqui para imprimir</a>.</p><p><a href=/pt-br/docs/concepts/architecture/>Retornar à visualização normal</a>.</p></div><h1 class=title>Arquitetura do Kubernetes</h1><ul><li>1: <a href=#pg-9ef2890698e773b6c0d24fd2c20146f5>Nós</a></li><li>2: <a href=#pg-c0251def6da29b30afebfb04549f1703>Comunicação entre Nó e Control Plane</a></li><li>3: <a href=#pg-bc804b02614d67025b4c788f1ca87fbc>Conceitos sobre Cloud Controller Manager</a></li><li>4: <a href=#pg-ca8819042a505291540e831283da66df>Controladores</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-9ef2890698e773b6c0d24fd2c20146f5>1 - Nós</h1><p>O Kubernetes executa sua carga de trabalho colocando contêineres em Pods para serem executados em <em>Nós</em>. Um nó pode ser uma máquina virtual ou física, dependendo do cluster. Cada nó é gerenciado pela <a class=glossary-tooltip title='A camada de gerenciamento de contêiner que expõe a API e as interfaces para definir, implantar e gerenciar o ciclo de vida dos contêineres.' data-toggle=tooltip data-placement=top href='/pt-br/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='camada de gerenciamento'>camada de gerenciamento</a> e contém os serviços necessários para executar <a class=glossary-tooltip title='O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>.</p><p>Normalmente, você tem vários nós em um cluster; em um ambiente de aprendizado ou limitado por recursos, você pode ter apenas um nó.</p><p>Os <a href=/docs/concepts/overview/components/#node-components>componentes</a> em um nó incluem o <a class=glossary-tooltip title='Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a>, um <a class=glossary-tooltip title='O agente de execução de contêiner é o software responsável por executar os contêineres.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='agente de execução de contêiner'>agente de execução de contêiner</a>, e o <a class=glossary-tooltip title='kube-proxy é um proxy de rede executado em cada nó do cluster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a>.</p><h2 id=administração>Administração</h2><p>Existem duas maneiras principais de adicionar Nós ao <a class=glossary-tooltip title='O componente da camada de gerenciamento que serve a API do Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='Servidor da API'>Servidor da API</a>:</p><ol><li>O kubelet em um nó se registra automaticamente na camada de gerenciamento</li><li>Você (ou outro usuário humano) adiciona manualmente um objeto Nó</li></ol><p>Depois de criar um <a class=glossary-tooltip title='A entity in the Kubernetes system, representing part of the state of your cluster.' data-toggle=tooltip data-placement=top href=https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-objects target=_blank aria-label=objeto>objeto</a> Nó, ou o kubelet em um nó se registra automaticamente, a camada de gerenciamento verifica se o novo objeto Nó é válido. Por exemplo, se você tentar criar um nó a partir do seguinte manifesto JSON:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Node&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;10.240.79.157&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;my-first-k8s-node&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>O Kubernetes cria um objeto nó internamente (a representação). O Kubernetes verifica se um kubelet se registrou no servidor da API que corresponde ao campo <code>metadata.name</code> do Nó. Se o nó estiver íntegro (ou seja, todos os serviços necessários estiverem em execução), ele será elegível para executar um Pod. Caso contrário, esse nó é ignorado para qualquer atividade de cluster até que se torne íntegro.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>O Kubernetes mantém o objeto nó inválido e continua verificando se ele se torna íntegro.</p><p>Você, ou um <a class=glossary-tooltip title='Um ciclo de controle que observa o estado partilhado do cluster através do API Server e efetua mudanças tentando mover o estado atual em direção ao estado desejado.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controlador>controlador</a>, deve excluir explicitamente o objeto Nó para interromper essa verificação de integridade.</p></div><p>O nome de um objeto nó deve ser um nome de <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>subdomínio válido de DNS</a>.</p><h3 id=singularidade-de-nome-do-nó>Singularidade de nome do nó</h3><p>O <a href=/docs/concepts/overview/working-with-objects/names#names>nome</a> identifica um nó. Dois nós não podem ter o mesmo nome ao mesmo tempo. O Kubernetes também assume que um recurso com o mesmo nome é o mesmo objeto. No caso de um nó, assume-se implicitamente que uma instância usando o mesmo nome terá o mesmo estado (por exemplo, configurações de rede, conteúdo do disco raiz) e atributos como label de nó. Isso pode levar a inconsistências se uma instância for modificada sem alterar seu nome. Se o nó precisar ser substituído ou atualizado significativamente, o objeto Nó existente precisa ser removido do servidor da API primeiro e adicionado novamente após a atualização.</p><h3 id=auto-registro-de-nós>Auto-registro de Nós</h3><p>Quando a opção <code>--register-node</code> do kubelet for verdadeira (padrão), o kubelet tentará se registrar no servidor da API. Este é o padrão preferido, usado pela maioria das distribuições.</p><p>Para auto-registro, o kubelet é iniciado com as seguintes opções:</p><ul><li><code>--kubeconfig</code> - O caminho das credenciais para se autenticar no servidor da API.</li><li><code>--cloud-provider</code> - Como comunicar com um <a class=glossary-tooltip title='Uma organização que oferece uma plataforma de computação em nuvem.' data-toggle=tooltip data-placement=top href='/pt-br/docs/reference/glossary/?all=true#term-cloud-provider' target=_blank aria-label='provedor de nuvem'>provedor de nuvem</a>
para ler metadados sobre si mesmo.</li><li><code>--register-node</code> - Registrar automaticamente no servidor da API.</li><li><code>--register-with-taints</code> - Registra o nó com a lista fornecida de <a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=taints>taints</a> (separadas por vírgula <code>&lt;key>=&lt;value>:&lt;effect></code>).</li></ul><p>Não funciona se o <code>register-node</code> for falso.</p><ul><li><code>--node-ip</code> - endereço IP do nó.</li><li><code>--node-labels</code> - <a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=Labels>Labels</a> a serem adicionados ao registrar o nó
no cluster (consulte as restrições de label impostas pelo <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>plug-in de admissão NodeRestriction</a>).</li><li><code>--node-status-update-frequency</code> - Especifica com que frequência o kubelet publica o status do nó no servidor da API.</li></ul><p>Quando o <a href=/docs/reference/access-authn-authz/node/>modo de autorização do nó</a> e o <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>plug-in de admissão NodeRestriction</a> estão ativados, os kubelets somente estarão autorizados a criar/modificar seu próprio recurso do nó.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Como mencionado na seção de <a href=#singularidade-de-nome-do-no>singularidade do nome do nó</a>, quando a configuração do nó precisa ser atualizada, é uma boa prática registrar novamente o nó no servidor da API. Por exemplo, se o kubelet estiver sendo reiniciado com o novo conjunto de <code>--node-labels</code>, mas o mesmo nome de nó for usado, a alteração não entrará em vigor, pois os labels estão sendo definidos no registro do Nó.</p><p>Pods já agendados no Nó podem ter um comportamento anormal ou causar problemas se a configuração do Nó for alterada na reinicialização do kubelet. Por exemplo, o Pod já em execução pode estar marcado diferente dos labels atribuídos ao Nó, enquanto outros Pods, que são incompatíveis com esse Pod, serão agendados com base nesse novo label. O novo registro do nó garante que todos os Pods sejam drenados e devidamente reiniciados.</p></div><h3 id=administração-manual-de-nós>Administração manual de nós</h3><p>Você pode criar e modificar objetos Nó usando o <a class=glossary-tooltip title='Uma ferramenta de linha de comando para se comunicar com um cluster Kubernetes.' data-toggle=tooltip data-placement=top href=/pt-br/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a>.</p><p>Quando você quiser manualmente criar objetos Nó, defina a opção do kubelet <code>--register-node=false</code>.</p><p>Você pode modificar os objetos Nó, independentemente da configuração de <code>--register-node</code>. Por exemplo, você pode definir labels em um nó existente ou marcá-lo como não disponível.</p><p>Você pode usar labels nos Nós em conjunto com seletores de nós nos Pods para controlar a disponibilidade. Por exemplo, você pode restringir um Pod a ser elegível apenas para ser executado em um subconjunto dos nós disponíveis.</p><p>Marcar um nó como não disponível impede que o escalonador coloque novos pods nesse nó, mas não afeta os Pods existentes no nó. Isso é útil como uma etapa preparatória antes da reinicialização de um nó ou outra manutenção.</p><p>Para marcar um nó como não disponível, execute:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl cordon <span style=color:#b8860b>$NODENAME</span>
</span></span></code></pre></div><p>Consulte <a href=/docs/tasks/administer-cluster/safely-drain-node/>Drenar um nó com segurança</a> para obter mais detalhes.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Os Pods que fazem parte de um <a class=glossary-tooltip title='Ensures a copy of a Pod is running across a set of nodes in a cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a> toleram ser executados em um nó não disponível. Os DaemonSets geralmente fornecem serviços locais de nós que devem ser executados em um Nó, mesmo que ele esteja sendo drenado de aplicativos de carga de trabalho.</div><h2 id=status-do-nó>Status do Nó</h2><p>O status de um nó contém as seguintes informações:</p><ul><li><a href=#addresses>Endereços</a></li><li><a href=#condition>Condições</a></li><li><a href=#capacity>Capacidade</a></li><li><a href=#info>Informação</a></li></ul><p>Você pode usar o <code>kubectl</code> para visualizar o status de um nó e outros detalhes:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe node &lt;insira-nome-do-nó-aqui&gt;
</span></span></code></pre></div><p>Cada seção da saída está descrita abaixo.</p><h3 id=endereços>Endereços</h3><p>O uso desses campos pode mudar dependendo do seu provedor de nuvem ou configuração dedicada.</p><ul><li>HostName: O nome do host relatado pelo <code>kernel</code> do nó. Pode ser substituído através do parâmetro kubelet <code>--hostname-override</code>.</li><li>ExternalIP: Geralmente, o endereço IP do nó que é roteável externamente (disponível fora do <code>cluster</code>).</li><li>InternalIP: Geralmente, o endereço IP do nó que é roteável somente dentro do <code>cluster</code>.</li></ul><h3 id=conditions>Condições</h3><p>O campo <code>conditions</code> descreve o status de todos os nós em execução. Exemplos de condições incluem:</p><table><caption style=display:none>Condições do nó e uma descrição de quando cada condição se aplica.</caption><thead><tr><th>Condições do nó</th><th>Descrição</th></tr></thead><tbody><tr><td><code>Ready</code></td><td><code>True</code> Se o nó estiver íntegro e pronto para aceitar pods, <code>False</code> se o nó não estiver íntegro e não estiver aceitando pods, e desconhecido <code>Unknown</code> se o controlador do nó tiver sem notícias do nó no último <code>node-monitor-grace-period</code> (o padrão é de 40 segundos)</td></tr><tr><td><code>DiskPressure</code></td><td><code>True</code> Se houver pressão sobre o tamanho do disco, ou seja, se a capacidade do disco for baixa; caso contrário <code>False</code></td></tr><tr><td><code>MemoryPressure</code></td><td><code>True</code> Se houver pressão na memória do nó, ou seja, se a memória do nó estiver baixa; caso contrário <code>False</code></td></tr><tr><td><code>PIDPressure</code></td><td><code>True</code> Se houver pressão sobre os processos, ou seja, se houver muitos processos no nó; caso contrário <code>False</code></td></tr><tr><td><code>NetworkUnavailable</code></td><td><code>True</code> Se a rede do nó não estiver configurada corretamente, caso contrário <code>False</code></td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Se você usar as ferramentas de linha de comando para mostrar os detalhes de um nó isolado, a <code>Condition</code> inclui <code>SchedulingDisabled</code>. <code>SchedulingDisabled</code> não é uma condição na API do Kubernetes; em vez disso, os nós isolados são marcados como <code>Unschedulable</code> em suas especificações.</div><p>Na API do Kubernetes, a condição de um nó é representada como parte do <code>.status</code> do recurso do nó. Por exemplo, a seguinte estrutura JSON descreve um nó íntegro:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;conditions&#34;</span><span>:</span> [
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;Ready&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;True&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;reason&#34;</span>: <span style=color:#b44>&#34;KubeletReady&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;message&#34;</span>: <span style=color:#b44>&#34;kubelet is posting ready status&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;lastHeartbeatTime&#34;</span>: <span style=color:#b44>&#34;2019-06-05T18:38:35Z&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;lastTransitionTime&#34;</span>: <span style=color:#b44>&#34;2019-06-05T11:41:27Z&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>Se o status da condição <code>Ready</code> permanecer desconhecido (<code>Unknown</code>) ou falso (<code>False</code>) por mais tempo do que o limite da remoção do pod (<code>pod-eviction-timeout</code>) (um argumento passado para o <a class=glossary-tooltip title='Componente da camada de gerenciamento que executa os processos de controle.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>), o <a href=#node-controller>controlador de nó</a> acionará o <a class=glossary-tooltip title='API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/api-eviction/ target=_blank aria-label='remoção iniciado pela API'>remoção iniciado pela API</a> para todos os Pods atribuídos a esse nó. A duração padrão do tempo limite da remoção é de <strong>cinco minutos</strong>. Em alguns casos, quando o nó está inacessível, o servidor da API não consegue se comunicar com o kubelet no nó. A decisão de excluir os pods não pode ser comunicada ao kubelet até que a comunicação com o servidor da API seja restabelecida. Enquanto isso, os pods agendados para exclusão podem continuar a ser executados no nó particionado.</p><p>O controlador de nós não força a exclusão dos pods até que seja confirmado que eles pararam de ser executados no cluster. Você pode ver os pods que podem estar sendo executados em um nó inacessível como estando no estado de terminando (<code>Terminating</code>) ou desconhecido (<code>Unknown</code>). Nos casos em que o Kubernetes não retirar da infraestrutura subjacente se um nó tiver deixado permanentemente um cluster, o administrador do cluster pode precisar excluir o objeto do nó manualmente. Excluir o objeto do nó do Kubernetes faz com que todos os objetos Pod em execução no nó sejam excluídos do servidor da API e libera seus nomes.</p><p>Quando ocorrem problemas nos nós, a camada de gerenciamento do Kubernetes cria automaticamente <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/><code>taints</code></a> que correspondem às condições que afetam o nó. O escalonador leva em consideração as <code>taints</code> do Nó ao atribuir um Pod a um Nó. Os Pods também podem ter <a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=tolerations>tolerations</a> que os permitem funcionar em um nó, mesmo que tenha uma <code>taint</code> específica.</p><p>Consulte <a href=/pt-br/docs/concepts/scheduling-eviction/taint-and-toleration/#taints-por-condi%C3%A7%C3%B5es-de-n%C3%B3>Nó Taint por Condição</a>
para mais detalhes.</p><h3 id=capacity>Capacidade e Alocável</h3><p>Descreve os recursos disponíveis no nó: CPU, memória e o número máximo de pods que podem ser agendados no nó.</p><p>Os campos no bloco de capacidade indicam a quantidade total de recursos que um nó possui. O bloco alocado indica a quantidade de recursos em um nó que está disponível para ser consumido por Pods normais.</p><p>Você pode ler mais sobre capacidade e recursos alocados enquanto aprende a <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>reservar recursos de computação</a> em um nó.</p><h3 id=info>Info</h3><p>Descreve informações gerais sobre o nó, como a versão do kernel, a versão do Kubernetes (versão do kubelet e kube-proxy), detalhes do tempo de execução do contêiner e qual sistema operacional o nó usa. O kubelet coleta essas informações do nó e as publica na API do Kubernetes.</p><h2 id=heartbeats>Heartbeats</h2><p>Os <code>Heartbeats</code>, enviados pelos nós do Kubernetes, ajudam seu cluster a determinar a disponibilidade de cada nó e a agir quando as falhas forem detectadas.</p><p>Para nós, existem duas formas de <code>heartbeats</code>:</p><ul><li>atualizações para o <code>.status</code> de um Nó</li><li>Objetos <a href=/docs/reference/kubernetes-api/cluster-resources/lease-v1/>Lease</a> dentro do <a class=glossary-tooltip title='Uma abstração utilizada pelo Kubernetes para suportar múltiplos clusters virtuais no mesmo cluster físico.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a> <code>kube-node-lease</code>. Cada nó tem um objeto de <code>Lease</code> associado.</li></ul><p>Em comparação com as atualizações no <code>.status</code> de um nó, um Lease é um recurso mais leve. O uso de Leases para <code>heartbeats</code> reduz o impacto no desempenho dessas atualizações para grandes clusters.</p><p>O kubelet é responsável por criar e atualizar o <code>.status</code> dos Nós e por atualizar suas Leases relacionadas.</p><ul><li>O kubelet atualiza o .status do nó quando há mudança de status ou se não houve atualização para um intervalo configurado. O intervalo padrão para atualizações .status para Nós é de 5 minutos, o que é muito maior do que o tempo limite padrão de 40 segundos para nós inacessíveis.</li><li>O kubelet cria e atualiza seu objeto <code>Lease</code> a cada 10 segundos (o intervalo de atualização padrão). As atualizações de Lease ocorrem independentemente das atualizações no <code>.status</code> do Nó. Se a atualização do <code>Lease</code> falhar, o kubelet voltará a tentativas, usando um recuo exponencial que começa em 200 milissegundos e limitado a 7 segundos.</li></ul><h2 id=controlador-de-nós>Controlador de Nós</h2><p>O <a class=glossary-tooltip title='Um ciclo de controle que observa o estado partilhado do cluster através do API Server e efetua mudanças tentando mover o estado atual em direção ao estado desejado.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controlador>controlador</a> de nós é um componente da camada de gerenciamento do Kubernetes que gerencia vários aspectos dos nós.</p><p>O controlador de nó tem várias funções na vida útil de um nó. O primeiro é atribuir um bloco CIDR ao nó quando ele é registrado (se a atribuição CIDR estiver ativada).</p><p>O segundo é manter a lista interna de nós do controlador de nós atualizada com a lista de máquinas disponíveis do provedor de nuvem. Ao ser executado em um ambiente de nuvem e sempre que um nó não é íntegro, o controlador de nó pergunta ao provedor de nuvem se a VM desse nó ainda está disponível. Caso contrário, o controlador de nós exclui o nó de sua lista de nós.</p><p>O terceiro é monitorar a saúde dos nós. O controlador do nó é responsável por:</p><ul><li>No caso de um nó se tornar inacessível, atualizar a condição NodeReady dentro do campo <code>.status</code> do nó. Nesse caso, o controlador do nó define a condição de pronto (<code>NodeReady</code>) como condição desconhecida (<code>ConditionUnknown</code>).</li><li>Se um nó permanecer inacessível: será iniciado a <a href=/docs/concepts/scheduling-eviction/api-eviction/>remoção pela API</a> para todos os Pods no nó inacessível. Por padrão, o controlador do nó espera 5 minutos entre marcar o nó como condição desconhecida (<code>ConditionUnknown</code>) e enviar a primeira solicitação de remoção.</li></ul><p>O controlador de nó verifica o estado de cada nó a cada <code>--node-monitor-period</code> segundos.</p><h3 id=limites-de-taxa-de-remoção>Limites de taxa de remoção</h3><p>Na maioria dos casos, o controlador de nós limita a taxa de remoção a <code>--node-eviction-rate</code> (0,1 por padrão) por segundo, o que significa que ele não removerá pods de mais de 1 nó por 10 segundos.</p><p>O comportamento de remoção do nó muda quando um nó em uma determinada zona de disponibilidade se torna não íntegro. O controlador de nós verifica qual porcentagem de nós na zona não são íntegras (a condição <code>NodeReady</code> é desconhecida <code>ConditionUnknown</code> ou falsa <code>ConditionFalse</code>) ao mesmo tempo:</p><ul><li>Se a fração de nós não íntegros for ao menos <code>--unhealthy-zone-threshold</code> (padrão 0,55), então a taxa de remoção será reduzida.</li><li>Se o cluster for pequeno (ou seja, tiver número de nós menor ou igual ao valor da opção <code>--large-cluster-size-threshold</code> - padrão 50), então as remoções serão interrompidas.</li><li>Caso contrário, a taxa de remoção é reduzida para <code>--secondary-node-eviction-rate</code> de nós secundários (padrão 0,01) por segundo.</li></ul><p>A razão pela qual essas políticas são implementadas por zona de disponibilidade é porque a camada de gerenciamento pode perder conexão com uma zona de disponibilidade, enquanto as outras permanecem conectadas. Se o seu cluster não abranger várias zonas de disponibilidade de provedores de nuvem, o mecanismo de remoção não levará em conta a indisponibilidade por zona.</p><p>Uma das principais razões para espalhar seus nós pelas zonas de disponibilidade é para que a carga de trabalho possa ser transferida para zonas íntegras quando uma zona inteira cair. Portanto, se todos os nós em uma zona não estiverem íntegros, o controlador do nó removerá na taxa normal de <code>--node-eviction-rate</code>. O caso especial é quando todas as zonas estiverem completamente insalubres (nenhum dos nós do cluster será íntegro). Nesse caso, o controlador do nó assume que há algum problema com a conectividade entre a camada de gerenciamento e os nós e não realizará nenhuma remoção. (Se houver uma interrupção e alguns nós reaparecerem, o controlador do nó expulsará os pods dos nós restantes que estiverem insalubres ou inacessíveis).</p><p>O controlador de nós também é responsável por remover pods em execução nos nós com <code>NoExecute</code> taints, a menos que esses pods tolerem essa taint. O controlador de nó também adiciona as <a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=taints>taints</a> correspondentes aos problemas de nó, como nó inacessível ou não pronto. Isso significa que o escalonador não colocará Pods em nós não íntegros.</p><h2 id=node-capacity>Rastreamento de capacidade de recursos</h2><p>Os objetos do nó rastreiam informações sobre a capacidade de recursos do nó: por exemplo, a quantidade de memória disponível e o número de CPUs. Os nós que se <a href=#self-registration-of-nodes>auto-registram</a> relatam sua capacidade durante o registro. Se você adicionar <a href=#manual-node-administration>manualmente</a> um nó, precisará definir as informações de capacidade do nó ao adicioná-lo.</p><p>O <a class=glossary-tooltip title='Componente da camada de gerenciamento que observa os pods recém-criados sem nenhum nó atribuído, e seleciona um nó para executá-los.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=escalonador>escalonador</a> do Kubernetes garante que haja recursos suficientes para todos os Pods em um nó. O escalonador verifica se a soma das solicitações de contêineres no nó não é maior do que a capacidade do nó. Essa soma de solicitações inclui todos os contêineres gerenciados pelo kubelet, mas exclui quaisquer contêineres iniciados diretamente pelo agente de execução de contêiner e também exclui quaisquer processos executados fora do controle do kubelet.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Se você quiser reservar explicitamente recursos para processos que não sejam do Pod, consulte <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved>reserva de recursos para daemons do sistema</a>.</div><h2 id=topologia-do-nó>Topologia do Nó</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.16 [alpha]</code></div><p>Se você ativou os [recursos]](/docs/reference/command-line-tools-reference/feature-gates/) de <code>TopologyManager</code>, o kubelet pode usar dicas da topologia ao tomar decisões de atribuição de recursos. Consulte <a href=/docs/tasks/administer-cluster/topology-manager/>Controle das Políticas de Gerenciamento de Topologia em um Nó</a> para obter mais informações.</p><h2 id=graceful-node-shutdown>Desligamento gracioso do nó</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code></div><p>O kubelet tenta detectar o desligamento do sistema do nó e encerra os pods em execução no nó.</p><p>O Kubelet garante que os pods sigam o processo normal de <a href=/docs/concepts/workloads/pods/>término do pod</a>pod-lifecycle/#pod-termination) durante o desligamento do nó.</p><p>O recurso de desligamento gradual do nó depende do systemd, pois aproveita os <a href=https://www.freedesktop.org/wiki/Software/systemd/inhibit/>bloqueios do inibidor do systemd</a> para atrasar o desligamento do nó com uma determinada duração.</p><p>O desligamento gradual do nó é controlado com <a href=/docs/reference/command-line-tools-reference/feature-gates/>recursos</a> <code>GracefulNodeShutdown</code>, que é ativado por padrão na versão 1.21.</p><p>Observe que, por padrão, ambas as opções de configuração descritas abaixo, <code>shutdownGracePeriod</code> and <code>shutdownGracePeriodCriticalPods</code> estão definidas como zero, não ativando assim a funcionalidade de desligamento gradual do nó. Para ativar o recurso, as duas configurações do kubelet devem ser configuradas adequadamente e definidas como valores diferentes de zero.</p><p>Durante um desligamento gradual, o kubelet encerra os pods em duas fases:</p><ol><li>Encerra os pods regulares em execução no nó.</li><li>Encerra os <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>pods críticos</a> em execução no nó.</li></ol><p>O recurso de desligamento gradual do nó é configurado com duas opções <a href=/docs/tasks/administer-cluster/kubelet-config-file/><code>KubeletConfiguration</code></a>:</p><ul><li><p><code>shutdownGracePeriod</code>:</p><ul><li>Especifica a duração total pela qual o nó deve atrasar o desligamento. Este é o período de carência total para o término dos pods regulares e os <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>críticos</a>.</li></ul></li><li><p><code>shutdownGracePeriodCriticalPods</code>:</p><ul><li>Especifica a duração utlizada para encerrar <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>pods críticos</a> durante um desligamento de nó. Este valor deve ser menor que <code>shutdownGracePeriod</code>.</li></ul></li></ul><p>Por exemplo, se <code>shutdownGracePeriod=30s</code> e <code>shutdownGracePeriodCriticalPods=10s</code>, o kubelet atrasará o desligamento do nó em 30 segundos. Durante o desligamento, os primeiros 20 (30-10) segundos seriam reservados para encerrar gradualmente os pods normais, e os últimos 10 segundos seriam reservados para encerrar <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>pods críticos</a>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Quando os pods forem removidos durante o desligamento gradual do nó, eles serão marcados como desligados. Executar o <code>kubectl get pods</code> para mostrar o status dos pods removidos como <code>Terminated</code>. E o <code>kubectl describe pod</code> indica que o pod foi removido por causa do desligamento do nó:</p><pre tabindex=0><code>Reason:         Terminated
Message:        Pod was terminated in response to imminent node shutdown.
</code></pre></div><h3 id=pod-priority-graceful-node-shutdown>Desligamento gradual do nó baseado em prioridade do Pod</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code></div><p>Para fornecer mais flexibilidade durante o desligamento gradual do nó em torno da ordem de pods durante o desligamento, o desligamento gradual do nó respeita a PriorityClass dos Pods, desde que você tenha ativado esse recurso em seu cluster. O recurso permite que o cluster defina explicitamente a ordem dos pods durante o desligamento gradual do nó com base em <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass>classes de prioridade</a>.</p><p>O recurso <a href=#graceful-node-shutdown>Desligamento Gradual do Nó</a>, conforme descrito acima, desliga pods em duas fases, pods não críticos, seguidos por pods críticos. Se for necessária flexibilidade adicional para definir explicitamente a ordem dos pods durante o desligamento de uma maneira mais granular, o desligamento gradual baseado na prioridade do pod pode ser usado.</p><p>Quando o desligamento gradual do nó respeita as prioridades do pod, isso torna possível fazer o desligamento gradual do nó em várias fases, cada fase encerrando uma classe de prioridade específica de pods. O kubelet pode ser configurado com as fases exatas e o tempo de desligamento por fase.</p><p>Assumindo as seguintes classes de prioridade de pod personalizadas em um cluster,</p><table><thead><tr><th>Nome das classes de prioridade</th><th>Valor das classes de prioridade</th></tr></thead><tbody><tr><td><code>custom-class-a</code></td><td>100000</td></tr><tr><td><code>custom-class-b</code></td><td>10000</td></tr><tr><td><code>custom-class-c</code></td><td>1000</td></tr><tr><td><code>regular/unset</code></td><td>0</td></tr></tbody></table><p>Na <a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration>configuração do kubelet</a>, as configurações para <code>shutdownGracePeriodByPodPriority</code> são semelhantes a:</p><table><thead><tr><th>Valor das classes de prioridade</th><th>Tempo de desligamento</th></tr></thead><tbody><tr><td>100000</td><td>10 segundos</td></tr><tr><td>10000</td><td>180 segundos</td></tr><tr><td>1000</td><td>120 segundos</td></tr><tr><td>0</td><td>60 segundos</td></tr></tbody></table><p>A configuração correspondente do YAML do kubelet seria:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>shutdownGracePeriodByPodPriority</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>100000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>10000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>180</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>120</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>60</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>A tabela acima implica que qualquer pod com valor <code>priority</code> >= 100000 terá apenas 10 segundos para parar qualquer pod com valor >= 10000 e &lt; 100000 e terá 180 segundos para parar, qualquer pod com valor >= 1000 e &lt; 10000 terá 120 segundos para parar. Finalmente, todos os outros pods terão 60 segundos para parar.</p><p>Não é preciso especificar valores correspondentes para todas as classes. Por exemplo, você pode usar estas configurações:</p><table><thead><tr><th>Valor das classes de prioridade</th><th>Tempo de desligamento</th></tr></thead><tbody><tr><td>100000</td><td>300 segundos</td></tr><tr><td>1000</td><td>120 segundos</td></tr><tr><td>0</td><td>60 segundos</td></tr></tbody></table><p>No caso acima, os pods com <code>custom-class-b</code> irão para o mesmo bucket que <code>custom-class-c</code> para desligamento.</p><p>Se não houver pods em um intervalo específico, o kubelet não irá espera por pods nesse intervalo de prioridades. Em vez disso, o kubelet pula imediatamente para o próximo intervalo de valores da classe de prioridade.</p><p>Se esse recurso estiver ativado e nenhuma configuração for fornecida, nenhuma ação de pedido será tomada.</p><p>O uso desse recurso requer ativar os recursos <code>GracefulNodeShutdownBasedOnPodPriority</code> e definir o <code>ShutdownGracePeriodByPodPriority</code> da configuração do kubelet para a configuração desejada, contendo os valores da classe de prioridade do pod e seus respectivos períodos de desligamento.</p><h2 id=swap-memory>Gerenciamento da memória swap</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code></div><p>Antes do Kubernetes 1.22, os nós não suportavam o uso de memória swap, e um kubelet, por padrão, não iniciaria se a troca fosse detectada em um nó. A partir de 1.22, o suporte a memória swap pode ser ativado por nó.</p><p>Para ativar a troca em um nó, o recursos <code>NodeSwap</code> deve estar ativado no kubelet, e a <a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration>configuração</a> de comando de linha <code>--fail-swap-on</code> ou <code>failSwapOn</code> deve ser definida como falsa.</p><div class="alert alert-danger warning callout" role=alert><strong>Aviso:</strong> Quando o recurso de memória swap está ativado, os dados do Kubernetes, como o conteúdo de objetos <code>Secret</code> que foram gravados no <code>tmpfs</code>, agora podem ser trocados para o disco.</div><p>Opcionalmente, um usuário também pode configurar <code>memorySwap.swapBehavior</code> para especificar como um nó usará memória swap. Por exemplo,</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>memorySwap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>swapBehavior</span>:<span style=color:#bbb> </span>LimitedSwap<span style=color:#bbb>
</span></span></span></code></pre></div><p>As opções de configuração disponíveis para <code>swapBehavior</code> são:</p><ul><li><code>LimitedSwap</code>: As cargas de trabalho do Kubernetes são limitadas na quantidade de troca que podem usar. Cargas de trabalho no nó não gerenciadas pelo Kubernetes ainda podem ser trocadas.</li><li><code>UnlimitedSwap</code>: As cargas de trabalho do Kubernetes podem usar tanta memória de swap quanto solicitarem, até o limite do sistema.</li></ul><p>Se a configuração do <code>memorySwap</code> não for especificada e o recurso estiver ativado, por padrão, o kubelet aplicará o mesmo comportamento que a configuração <code>LimitedSwap</code>.</p><p>O comportamento da configuração <code>LimitedSwap</code> depende se o nó estiver sendo executado com v1 ou v2 de grupos de controle (também conhecidos como "cgroups"):</p><ul><li><strong>cgroupsv1</strong>: As cargas de trabalho do Kubernetes podem usar qualquer combinação de memória e swap, até o limite de memória do pod, se definido.</li><li><strong>cgroupsv2</strong>: As cargas de trabalho do Kubernetes não podem usar memória swap.</li></ul><p>Para obter mais informações e para ajudar nos testes e fornecer feedback, consulte <a href=https://github.com/kubernetes/enhancements/issues/2400>KEP-2400</a> e sua <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md>proposta de design</a>.</p><h2 id=próximos-passos>Próximos passos</h2><ul><li>Saiba mais sobre <a href=/docs/concepts/overview/components/#node-components>componentes</a> que compõem um nó.</li><li>Leia a <a href=/docs/reference/generated/kubernetes-api/v1.25/#node-v1-core>definição da API para um Nó</a>.</li><li>Leia a seção <a href=https://git.k8s.io/community/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node>Nó</a> do documento de design de arquitetura.</li><li>Leia sobre <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>taints e tolerâncias</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c0251def6da29b30afebfb04549f1703>2 - Comunicação entre Nó e Control Plane</h1><p>Este documento cataloga os caminhos de comunicação entre o control plane (o
apiserver) e o cluster Kubernetes. A intenção é permitir que os usuários
personalizem sua instalação para proteger a configuração de rede
então o cluster pode ser executado em uma rede não confiável (ou em IPs totalmente públicos em um
provedor de nuvem).</p><h2 id=nó-para-o-control-plane>Nó para o Control Plane</h2><p>Todos os caminhos de comunicação do cluster para o control plane terminam no
apiserver (nenhum dos outros componentes do control plane são projetados para expor
Serviços remotos). Em uma implantação típica, o apiserver é configurado para escutar
conexões remotas em uma porta HTTPS segura (443) com uma ou mais clientes <a href=/docs/reference/access-authn-authz/authentication/>autenticação</a> habilitado.
Uma ou mais formas de <a href=/docs/reference/access-authn-authz/authorization/>autorização</a>
deve ser habilitado, especialmente se <a href=/docs/reference/access-authn-authz/authentication/#anonymous-requests>requisições anônimas</a>
ou <a href=/docs/reference/access-authn-authz/authentication/#service-account-tokens>tokens da conta de serviço</a>
são autorizados.</p><p>Os nós devem ser provisionados com o certificado root público para o cluster
de tal forma que eles podem se conectar de forma segura ao apiserver junto com o cliente válido
credenciais. Por exemplo, em uma implantação padrão do GKE, as credenciais do cliente
fornecidos para o kubelet estão na forma de um certificado de cliente. Vejo
<a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>bootstrapping TLS do kubelet</a>
para provisionamento automatizado de certificados de cliente kubelet.</p><p>Os pods que desejam se conectar ao apiserver podem fazê-lo com segurança, aproveitando
conta de serviço para que o Kubernetes injetará automaticamente o certificado raiz público
certificado e um token de portador válido no pod quando ele é instanciado.
O serviço <code>kubernetes</code> (no namespace <code>default</code>) é configurado com um IP virtual
endereço que é redirecionado (via kube-proxy) para o endpoint com HTTPS no
apiserver.</p><p>Os componentes do control plane também se comunicam com o apiserver do cluster através da porta segura.</p><p>Como resultado, o modo de operação padrão para conexões do cluster
(nodes e pods em execução nos Nodes) para o control plane é protegido por padrão
e pode passar por redes não confiáveis ​​e/ou públicas.</p><h2 id=control-plane-para-o-nó>Control Plane para o nó</h2><p>Existem dois caminhos de comunicação primários do control plane (apiserver) para os nós.
O primeiro é do apiserver para o processo do kubelet que é executado em
cada nó no cluster. O segundo é do apiserver para qualquer nó, pod,
ou serviço através da funcionalidade de proxy do apiserver.</p><h3 id=apiserver-para-o-kubelet>apiserver para o kubelet</h3><p>As conexões do apiserver ao kubelet são usadas para:</p><ul><li>Buscar logs para pods.</li><li>Anexar (através de kubectl) pods em execução.</li><li>Fornecer a funcionalidade de encaminhamento de porta do kubelet.</li></ul><p>Essas conexões terminam no endpoint HTTPS do kubelet. Por padrão,
o apiserver não verifica o certificado de serviço do kubelet,
o que torna a conexão sujeita a ataques man-in-the-middle, o que o torna
<strong>inseguro</strong> para passar por redes não confiáveis ​​e / ou públicas.</p><p>Para verificar essa conexão, use a flag <code>--kubelet-certificate-authority</code> para
fornecer o apiserver com um pacote de certificado raiz para usar e verificar o
certificado de serviço da kubelet.</p><p>Se isso não for possível, use o <a href=/docs/concepts/architecture/master-node-communication/#ssh-tunnels>SSH túnel</a>
entre o apiserver e kubelet se necessário para evitar a conexão ao longo de um
rede não confiável ou pública.</p><p>Finalmente, <a href=/docs/admin/kubelet-authentication-authorization/>Autenticação e/ou autorização do Kubelet</a>
deve ser ativado para proteger a API do kubelet.</p><h3 id=apiserver-para-nós-pods-e-serviços>apiserver para nós, pods e serviços</h3><p>As conexões a partir do apiserver para um nó, pod ou serviço padrão para simples
conexões HTTP não são autenticadas nem criptografadas. Eles
podem ser executados em uma conexão HTTPS segura prefixando <code>https:</code> no nó,
pod, ou nome do serviço no URL da API, mas eles não validarão o certificado
fornecido pelo ponto de extremidade HTTPS, nem fornece credenciais de cliente, enquanto
a conexão será criptografada, não fornecerá nenhuma garantia de integridade.
Estas conexões <strong>não são atualmente seguras</strong> para serem usados por redes não confiáveis ​​e/ou públicas.</p><h3 id=ssh-túnel>SSH Túnel</h3><p>O Kubernetes suporta túneis SSH para proteger os caminhos de comunicação do control plane para os nós. Nesta configuração, o apiserver inicia um túnel SSH para cada nó
no cluster (conectando ao servidor ssh escutando na porta 22) e passa
todo o tráfego destinado a um kubelet, nó, pod ou serviço através do túnel.
Este túnel garante que o tráfego não seja exposto fora da rede aos quais
os nós estão sendo executados.</p><p>Atualmente, os túneis SSH estão obsoletos, portanto, você não deve optar por usá-los, a menos que saiba o que está fazendo. O serviço Konnectivity é um substituto para este canal de comunicação.</p><h3 id=konnectivity-service>Konnectivity service</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code></div><p>Como uma substituição aos túneis SSH, o serviço Konnectivity fornece proxy de nível TCP para a comunicação do control plane para o cluster. O serviço Konnectivity consiste em duas partes: o servidor Konnectivity na rede control plane e os agentes Konnectivity na rede dos nós. Os agentes Konnectivity iniciam conexões com o servidor Konnectivity e mantêm as conexões de rede. Depois de habilitar o serviço Konnectivity, todo o tráfego do control plane para os nós passa por essas conexões.</p><p>Veja a <a href=docs/tasks/extend-kubernetes/setup-konnectivity/>tarefa do Konnectivity</a> para configurar o serviço Konnectivity no seu cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bc804b02614d67025b4c788f1ca87fbc>3 - Conceitos sobre Cloud Controller Manager</h1><p>O conceito do Cloud Controller Manager (CCM) (não confundir com o binário) foi originalmente criado para permitir que o código específico de provedor de nuvem e o núcleo do Kubernetes evoluíssem independentemente um do outro. O Cloud Controller Manager é executado junto com outros componentes principais, como o Kubernetes controller manager, o servidor de API e o scheduler. Também pode ser iniciado como um addon do Kubernetes, caso em que é executado em cima do Kubernetes.</p><p>O design do Cloud Controller Manager é baseado em um mecanismo de plug-in que permite que novos provedores de nuvem se integrem facilmente ao Kubernetes usando plug-ins. Existem planos para integrar novos provedores de nuvem no Kubernetes e para migrar provedores de nuvem que estão utilizando o modelo antigo para o novo modelo de CCM.</p><p>Este documento discute os conceitos por trás do Cloud Controller Manager e fornece detalhes sobre suas funções associadas.</p><p>Aqui está a arquitetura de um cluster Kubernetes sem o Cloud Controller Manager:</p><p><img src=/images/docs/pre-ccm-arch.png alt="Pre CCM Kube Arch"></p><h2 id=projeto-de-arquitetura-design>Projeto de Arquitetura (Design)</h2><p>No diagrama anterior, o Kubernetes e o provedor de nuvem são integrados através de vários componentes diferentes:</p><ul><li>Kubelet</li><li>Kubernetes controller manager</li><li>Kubernetes API server</li></ul><p>O CCM consolida toda a lógica que depende da nuvem dos três componentes anteriores para criar um único ponto de integração com a nuvem. A nova arquitetura com o CCM se parece com isso:</p><p><img src=/images/docs/post-ccm-arch.png alt="CCM Kube Arch"></p><h2 id=componentes-do-ccm>Componentes do CCM</h2><p>O CCM separa algumas das funcionalidades do KCM (Kubernetes Controller Manager) e o executa como um processo separado. Especificamente, isso elimina os controladores no KCM que dependem da nuvem. O KCM tem os seguintes loops de controlador dependentes de nuvem:</p><ul><li>Node controller</li><li>Volume controller</li><li>Route controller</li><li>Service controller</li></ul><p>Na versão 1.9, o CCM executa os seguintes controladores da lista anterior:</p><ul><li>Node controller</li><li>Route controller</li><li>Service controller</li></ul><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> O Volume Controller foi deliberadamente escolhido para não fazer parte do CCM. Devido à complexidade envolvida e devido aos esforços existentes para abstrair a lógica de volume específica do fornecedor, foi decidido que o Volume Controller não será movido para o CCM.</div><p>O plano original para suportar volumes usando o CCM era usar volumes Flex para suportar volumes plugáveis. No entanto, um esforço concorrente conhecido como CSI está sendo planejado para substituir o Flex.</p><p>Considerando essas dinâmicas, decidimos ter uma medida de intervalo intermediário até que o CSI esteja pronto.</p><h2 id=funções-do-ccm>Funções do CCM</h2><p>O CCM herda suas funções de componentes do Kubernetes que são dependentes de um provedor de nuvem. Esta seção é estruturada com base nesses componentes.</p><h3 id=1-kubernetes-controller-manager>1. Kubernetes Controller Manager</h3><p>A maioria das funções do CCM é derivada do KCM. Conforme mencionado na seção anterior, o CCM executa os seguintes ciclos de controle:</p><ul><li>Node Controller</li><li>Route Controller</li><li>Service Controller</li></ul><h4 id=node-controller>Node Controller</h4><p>O Node Controller é responsável por inicializar um nó obtendo informações sobre os nós em execução no cluster do provedor de nuvem. O Node Controller executa as seguintes funções:</p><ol><li>Inicializar um node com labels de região/zona específicos para a nuvem.</li><li>Inicialize um node com detalhes de instância específicos da nuvem, por exemplo, tipo e tamanho.</li><li>Obtenha os endereços de rede e o nome do host do node.</li><li>No caso de um node não responder, verifique a nuvem para ver se o node foi excluído da nuvem.
Se o node foi excluído da nuvem, exclua o objeto Node do Kubernetes.</li></ol><h4 id=route-controller>Route Controller</h4><p>O Route Controller é responsável por configurar as rotas na nuvem apropriadamente, de modo que os contêineres em diferentes nodes no cluster do Kubernetes possam se comunicar entre si. O Route Controller é aplicável apenas para clusters do Google Compute Engine.</p><h4 id=service-controller>Service controller</h4><p>O Service controller é responsável por ouvir os eventos de criação, atualização e exclusão do serviço. Com base no estado atual dos serviços no Kubernetes, ele configura os balanceadores de carga da nuvem (como o ELB, o Google LB ou o Oracle Cloud Infrastrucutre LB) para refletir o estado dos serviços no Kubernetes. Além disso, garante que os back-ends de serviço para balanceadores de carga da nuvem estejam atualizados.</p><h3 id=2-kubelet>2. Kubelet</h3><p>O Node Controller contém a funcionalidade dependente da nuvem do kubelet. Antes da introdução do CCM, o kubelet era responsável por inicializar um nó com detalhes específicos da nuvem, como endereços IP, rótulos de região / zona e informações de tipo de instância. A introdução do CCM mudou esta operação de inicialização do kubelet para o CCM.</p><p>Nesse novo modelo, o kubelet inicializa um nó sem informações específicas da nuvem. No entanto, ele adiciona uma marca (taint) ao nó recém-criado que torna o nó não programável até que o CCM inicialize o nó com informações específicas da nuvem. Em seguida, remove essa mancha (taint).</p><h2 id=mecanismo-de-plugins>Mecanismo de plugins</h2><p>O Cloud Controller Manager usa interfaces Go para permitir implementações de qualquer nuvem a ser conectada. Especificamente, ele usa a Interface CloudProvider definida<a href=https://github.com/kubernetes/cloud-provider/blob/9b77dc1c384685cb732b3025ed5689dd597a5971/cloud.go#L42-L62>aqui</a>.</p><p>A implementação dos quatro controladores compartilhados destacados acima, e algumas estruturas que ficam junto com a interface compartilhada do provedor de nuvem, permanecerão no núcleo do Kubernetes. Implementações específicas para provedores de nuvem serão construídas fora do núcleo e implementarão interfaces definidas no núcleo.</p><p>Para obter mais informações sobre o desenvolvimento de plug-ins, consulte<a href=/docs/tasks/administer-cluster/developing-cloud-controller-manager/>Desenvolvendo o Cloud Controller Manager</a>.</p><h2 id=autorização>Autorização</h2><p>Esta seção divide o acesso necessário em vários objetos da API pelo CCM para executar suas operações.</p><h3 id=node-controller-1>Node Controller</h3><p>O Node Controller só funciona com objetos Node. Ele requer acesso total para obter, listar, criar, atualizar, corrigir, assistir e excluir objetos Node.</p><p>v1/Node:</p><ul><li>Get</li><li>List</li><li>Create</li><li>Update</li><li>Patch</li><li>Watch</li><li>Delete</li></ul><h3 id=rote-controller>Rote Controller</h3><p>O Rote Controller escuta a criação do objeto Node e configura as rotas apropriadamente. Isso requer acesso a objetos Node.</p><p>v1/Node:</p><ul><li>Get</li></ul><h3 id=service-controller-1>Service Controller</h3><p>O Service Controller escuta eventos de criação, atualização e exclusão de objeto de serviço e, em seguida, configura pontos de extremidade para esses serviços de forma apropriada.</p><p>Para acessar os Serviços, é necessário listar e monitorar o acesso. Para atualizar os Serviços, ele requer patch e atualização de acesso.</p><p>Para configurar endpoints para os Serviços, é necessário acesso para criar, listar, obter, assistir e atualizar.</p><p>v1/Service:</p><ul><li>List</li><li>Get</li><li>Watch</li><li>Patch</li><li>Update</li></ul><h3 id=outros>Outros</h3><p>A implementação do núcleo do CCM requer acesso para criar eventos e, para garantir a operação segura, requer acesso para criar ServiceAccounts.</p><p>v1/Event:</p><ul><li>Create</li><li>Patch</li><li>Update</li></ul><p>v1/ServiceAccount:</p><ul><li>Create</li></ul><p>O RBAC ClusterRole para o CCM se parece com isso:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- events<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#39;*&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes/status<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- services<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- serviceaccounts<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- persistentvolumes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- endpoints<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=implementações-de-provedores-de-nuvem>Implementações de Provedores de Nuvem</h2><p>Os seguintes provedores de nuvem implementaram CCMs:</p><ul><li><a href=https://github.com/digitalocean/digitalocean-cloud-controller-manager>Digital Ocean</a></li><li><a href=https://github.com/oracle/oci-cloud-controller-manager>Oracle</a></li><li><a href=https://github.com/kubernetes/cloud-provider-azure>Azure</a></li><li><a href=https://github.com/kubernetes/cloud-provider-gcp>GCP</a></li><li><a href=https://github.com/kubernetes/cloud-provider-aws>AWS</a></li><li><a href=https://github.com/baidu/cloud-provider-baiducloud>BaiduCloud</a></li><li><a href=https://github.com/linode/linode-cloud-controller-manager>Linode</a></li></ul><h2 id=administração-de-cluster>Administração de Cluster</h2><p>Voce vai encontrar instruções completas para configurar e executar o CCM
<a href=/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager>aqui</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ca8819042a505291540e831283da66df>4 - Controladores</h1><p>Em robótica e automação um <em>control loop</em>, ou em português <em>ciclo de controle</em>, é
um ciclo não terminado que regula o estado de um sistema.</p><p>Um exemplo de ciclo de controle é um termostato de uma sala.</p><p>Quando você define a temperatura, isso indica ao termostato
sobre o seu <em>estado desejado</em>. A temperatura ambiente real é o
<em>estado atual</em>. O termostato atua de forma a trazer o estado atual
mais perto do estado desejado, ligando ou desligando o equipamento.</p>No Kubernetes, controladores são ciclos de controle que observam o estado do seu
<a class=glossary-tooltip title='Um conjunto de servidores de processamento, também chamados de nós, que executam aplicações containerizadas. Todo cluster possui ao menos um servidor de processamento (worker node).' data-toggle=tooltip data-placement=top href='/pt-br/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=cluster>cluster</a>, e então fazer ou requisitar
mudanças onde necessário.
Cada controlador tenta mover o estado atual do cluster mais perto do estado desejado.<h2 id=padrão-controlador-controller-pattern>Padrão Controlador (Controller pattern)</h2><p>Um controlador rastreia pelo menos um tipo de recurso Kubernetes.
Estes <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-objects>objetos</a>
têm um campo <em>spec</em> que representa o <em>estado desejado</em>.
O(s) controlador(es) para aquele recurso são responsáveis por trazer o <em>estado atual</em>
mais perto do <em>estado desejado</em>.</p><p>O controlador pode executar uma ação ele próprio, ou,
o que é mais comum, no Kubernetes, o controlador envia uma mensagem para o
<a class=glossary-tooltip title='O componente da camada de gerenciamento que serve a API do Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a> (servidor de API) que tem
efeitos colaterais úteis. Você vai ver exemplos disto abaixo.</p><h3 id=controlador-via-api-server>Controlador via API server</h3><p>O controlador <a class=glossary-tooltip title='Uma tarefa finita ou em lotes que executa até finalizar.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job target=_blank aria-label=Job>Job</a> é um exemplo de um
controlador Kubernetes embutido. Controladores embutidos gerem estados através da
interação com o <em>cluster API server</em>.</p><p><em>Job</em> é um recurso do Kubernetes que é executado em um
<em><a class=glossary-tooltip title='O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a></em>, ou talvez vários <em>Pods</em>, com o objetivo de
executar uma tarefa e depois parar.</p><p>(Uma vez <a href=/docs/concepts/scheduling/>agendado</a>, objetos <em>Pod</em> passam a fazer parte
do <em>estado desejado</em> para um kubelet.</p><p>Quando o controlador <em>Job</em> observa uma nova tarefa ele garante que,
algures no seu <em>cluster</em>, os kubelets num conjunto de nós (<em>Nodes</em>) estão correndo o número
correto de <em>Pods</em> para completar o trabalho.
O controlador <em>Job</em> não corre <em>Pods</em> ou <em>containers</em> ele próprio.
Em vez disso, o controlador <em>Job</em> informa o <em>API server</em> para criar ou remover <em>Pods</em>.
Outros componentes do plano de controle
(<a class=glossary-tooltip title='A camada de gerenciamento de contêiner que expõe a API e as interfaces para definir, implantar e gerenciar o ciclo de vida dos contêineres.' data-toggle=tooltip data-placement=top href='/pt-br/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>)
atuam na nova informação (existem novos <em>Pods</em> para serem agendados e executados),
e eventualmente o trabalho é feito.</p><p>Após ter criado um novo <em>Job</em>, o <em>estado desejado</em> é que esse Job seja completado.
O controlador <em>Job</em> faz com que o <em>estado atual</em> para esse <em>Job</em> esteja mais perto do seu
<em>estado desejado</em>: criando <em>Pods</em> que fazem o trabalho desejado para esse <em>Job</em> para que
o <em>Job</em> fique mais perto de ser completado.</p><p>Controladores também atualizam os objetos que os configuram.
Por exemplo: assim que o trabalho de um <em>Job</em> está completo,
o controlador <em>Job</em> atualiza esse objeto <em>Job</em> para o marcar como <code>Finished</code> (terminado).</p><p>(Isto é um pouco como alguns termostatos desligam uma luz para
indicar que a temperatura da sala está agora na temperatura que foi introduzida).</p><h3 id=controle-direto>Controle direto</h3><p>Em contraste com <em>Job</em>, alguns controladores necessitam de efetuar
mudanças fora do <em>cluster</em>.</p><p>Por exemplo, se usar um ciclo de controle para garantir que existem
<em><a class=glossary-tooltip title='Um Nó é uma máquina de trabalho no Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a></em> suficientes
no seu <em>cluster</em>, então esse controlador necessita de algo exterior ao
<em>cluster</em> atual para configurar novos <em>Nodes</em> quando necessário.</p><p>Controladores que interagem com estados externos encontram o seu estado desejado
a partir do <em>API server</em>, e então comunicam diretamente com o sistema externo para
trazer o <em>estado atual</em> mais próximo do desejado.</p><p>(Existe um controlador que escala horizontalmente nós no seu <em>cluster</em>.
Veja <a href=/docs/tasks/administer-cluster/cluster-management/#cluster-autoscaling>Escalamento automático do cluster</a>)</p><h2 id=desired-vs-current>Estado desejado versus atual</h2><p>Kubernetes tem uma visão <em>cloud-native</em> de sistemas e é capaz de manipular
mudanças constantes.</p><p>O seu <em>cluster</em> pode mudar em qualquer momento à medida que as ações acontecem e
os ciclos de controle corrigem falhas automaticamente. Isto significa que,
potencialmente, o seu <em>cluster</em> nunca atinge um estado estável.</p><p>Enquanto os controladores no seu <em>cluster</em> estiverem rodando e forem capazes de
fazer alterações úteis, não importa se o estado é estável ou se é instável.</p><h2 id=design>Design</h2><p>Como um princípio do seu desenho, o Kubernetes usa muitos controladores onde cada
um gerencia um aspecto particular do estado do <em>cluster</em>. Comumente, um particular
ciclo de controle (controlador) usa uma espécie de recurso como o seu <em>estado desejado</em>,
e tem uma espécie diferente de recurso que o mesmo gere para garantir que esse <em>estado desejado</em>
é cumprido.</p><p>É útil que haja controladores simples em vez de um conjunto monolítico de ciclos de controle
que estão interligados. Controladores podem falhar, então o Kubernetes foi desenhado para
permitir isso.</p><p>Por exemplo: um controlador de <em>Jobs</em> rastreia objetos <em>Job</em> (para
descobrir novos trabalhos) e objetos <em>Pod</em> (para correr o <em>Jobs</em>, e então
ver quando o trabalho termina). Neste caso outra coisa cria os <em>Jobs</em>,
enquanto o controlador <em>Job</em> cria <em>Pods</em>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Podem existir vários controladores que criam ou atualizam a mesma espécie (kind) de objeto.
Atrás das cortinas, os controladores do Kubernetes garantem que eles apenas tomam
atenção aos recursos ligados aos seus recursos controladores.</p><p>Por exemplo, você pode ter <em>Deployments</em> e <em>Jobs</em>; ambos criam <em>Pods</em>.
O controlador de <em>Job</em> não apaga os <em>Pods</em> que o seu <em>Deployment</em> criou,
porque existe informação (<a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a>)
que os controladores podem usar para diferenciar esses <em>Pods</em>.</p></div><h2 id=running-controllers>Formas de rodar controladores</h2><p>O Kubernetes vem com um conjunto de controladores embutidos que correm
dentro do <a class=glossary-tooltip title='Componente da camada de gerenciamento que executa os processos de controle.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>.
Estes controladores embutidos providenciam comportamentos centrais importantes.</p><p>O controlador <em>Deployment</em> e o controlador <em>Job</em> são exemplos de controladores
que veem como parte do próprio Kubernetes (controladores "embutidos").
O Kubernetes deixa você correr o plano de controle resiliente, para que se qualquer
um dos controladores embutidos falhar, outra parte do plano de controle assume
o trabalho.</p><p>Pode encontrar controladores fora do plano de controle, para extender o Kubernetes.
Ou, se quiser, pode escrever um novo controlador você mesmo.
Pode correr o seu próprio controlador como um conjunto de <em>Pods</em>,
ou externo ao Kubernetes. O que encaixa melhor vai depender no que esse
controlador faz em particular.</p><h2 id=próximos-passos>Próximos passos</h2><ul><li>Leia mais sobre o <a href=/docs/concepts/#kubernetes-control-plane>plano de controle do Kubernetes</a></li><li>Descubra alguns dos <a href=/docs/concepts/#kubernetes-objects>objetos Kubernetes</a> básicos.</li><li>Aprenda mais sobre <a href=/docs/concepts/overview/kubernetes-api/>API do Kubernetes</a></li><li>Se pretender escrever o seu próprio controlador, veja <a href=/docs/concepts/extend-kubernetes/extend-cluster/#extension-patterns>Padrões de Extensão</a></li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/pt-br/docs/home/>Home</a>
<a class=text-white href=/pt-br/blog/>Blog</a>
<a class=text-white href=/pt-br/partners/>Parceiros</a>
<a class=text-white href=/pt-br/community/>Comunidade</a>
<a class=text-white href=/pt-br/case-studies/>Casos de estudo</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 Os autores do Kubernetes | Documentação Distribuída sob <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 A Fundação Linux &reg;. Todos os direitos reservados. A Linux Foundation tem marcas registradas e usa marcas registradas. Para uma lista de marcas registradas da The Linux Foundation, por favor, veja nossa <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Página de uso de marca registrada</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>