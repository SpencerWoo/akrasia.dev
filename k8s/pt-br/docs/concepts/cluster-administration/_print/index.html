<!doctype html><html lang=pt-br class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/cluster-administration/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/cluster-administration/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/cluster-administration/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/cluster-administration/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/cluster-administration/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/cluster-administration/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/cluster-administration/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/cluster-administration/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/cluster-administration/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/pt-br/docs/concepts/cluster-administration/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Administração de Cluster | Kubernetes</title><meta property="og:title" content="Administração de Cluster"><meta property="og:description" content="Detalhes de baixo nível relevantes para criar ou administrar um cluster Kubernetes.
"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/pt-br/docs/concepts/cluster-administration/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Administração de Cluster"><meta itemprop=description content="Detalhes de baixo nível relevantes para criar ou administrar um cluster Kubernetes.
"><meta name=twitter:card content="summary"><meta name=twitter:title content="Administração de Cluster"><meta name=twitter:description content="Detalhes de baixo nível relevantes para criar ou administrar um cluster Kubernetes.
"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Detalhes de baixo nível relevantes para criar ou administrar um cluster Kubernetes.
"><meta property="og:description" content="Detalhes de baixo nível relevantes para criar ou administrar um cluster Kubernetes.
"><meta name=twitter:description content="Detalhes de baixo nível relevantes para criar ou administrar um cluster Kubernetes.
"><meta property="og:url" content="https://kubernetes.io/pt-br/docs/concepts/cluster-administration/"><meta property="og:title" content="Administração de Cluster"><meta name=twitter:title content="Administração de Cluster"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/pt-br/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/pt-br/docs/>Documentação</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/partners/>Parceiros</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/community/>Comunidade</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/pt-br/case-studies/>Casos de estudo</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versões</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/pt-br/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/pt-br/docs/concepts/cluster-administration/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/pt-br/docs/concepts/cluster-administration/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/pt-br/docs/concepts/cluster-administration/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/pt-br/docs/concepts/cluster-administration/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/pt-br/docs/concepts/cluster-administration/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Português (Portuguese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/cluster-administration/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/cluster-administration/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/cluster-administration/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/cluster-administration/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/cluster-administration/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/cluster-administration/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/cluster-administration/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/cluster-administration/>Español (Spanish)</a>
<a class=dropdown-item href=/id/docs/concepts/cluster-administration/>Bahasa Indonesia</a>
<a class=dropdown-item href=/ru/docs/concepts/cluster-administration/>Русский (Russian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Essa é a versão completa de impressão dessa seção
<a href=# onclick="return print(),!1">Clique aqui para imprimir</a>.</p><p><a href=/pt-br/docs/concepts/cluster-administration/>Retornar à visualização normal</a>.</p></div><h1 class=title>Administração de Cluster</h1><div class=lead>Detalhes de baixo nível relevantes para criar ou administrar um cluster Kubernetes.</div><ul><li>1: <a href=#pg-fb494ea3b1874bd753dcd11c3f35c2dc>Visão Geral da Administração de Cluster</a></li><li>2: <a href=#pg-2bf9a93ab5ba014fb6ff70b22c29d432>Certificates</a></li><li>3: <a href=#pg-d649067a69d8d5c7e71564b42b96909e>Conectividade do Cluster</a></li><li>4: <a href=#pg-c4b1e87a84441f8a90699a345ce48d68>Arquitetura de Log</a></li><li>5: <a href=#pg-5cc31ecfba86467f8884856412cfb6b2>Logs de Sistema</a></li><li>6: <a href=#pg-cbfd3654996eae9fcdef009f70fa83f0>Métricas para componentes do sistema Kubernetes</a></li><li>7: <a href=#pg-2e05a56491965ae320c2662590b2ca18>Configurando o Garbage Collection do kubelet</a></li><li>8: <a href=#pg-08e94e6a480e0d6b2de72d84a1b97617>Proxies no Kubernetes</a></li><li>9: <a href=#pg-85d633ae590aa20ec024f1b7af1d74fc>Instalando Complementos</a></li><li>10: <a href=#pg-31c9327d2332c585341b64ddafa19cdd>Prioridade e imparcialidade da API</a></li></ul><div class=content><p>A visão geral da administração do cluster é para qualquer pessoa que crie ou administre um cluster do Kubernetes.
É pressuposto alguma familiaridade com os <a href=/docs/concepts>conceitos</a> principais do Kubernetes.</p><h2 id=planejando-um-cluster>Planejando um cluster</h2><p>Consulte os guias em <a href=/docs/setup>Configuração</a> para exemplos de como planejar, instalar e configurar clusters Kubernetes. As soluções listadas neste artigo são chamadas de <em>distros</em>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Nem todas as distros são mantidas ativamente. Escolha distros que foram testadas com uma versão recente do Kubernetes.</div><p>Antes de escolher um guia, aqui estão algumas considerações:</p><ul><li>Você quer experimentar o Kubernetes em seu computador ou deseja criar um cluster de vários nós com alta disponibilidade? Escolha as distros mais adequadas ás suas necessidades.</li><li>Você vai usar um <strong>cluster Kubernetes gerenciado</strong> , como o <a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a>, ou <strong>vai hospedar seu próprio cluster</strong>?</li><li>Seu cluster será <strong>local</strong>, ou <strong>na nuvem (IaaS)</strong>? O Kubernetes não oferece suporte direto a clusters híbridos. Em vez disso, você pode configurar vários clusters.</li><li><strong>Se você estiver configurando o Kubernetes local</strong>, leve em consideração qual <a href=/docs/concepts/cluster-Administration/networking>modelo de rede</a> se encaixa melhor.</li><li>Você vai executar o Kubernetes em um hardware <strong>bare metal</strong> ou em <strong>máquinas virtuais? (VMs)</strong>?</li><li>Você <strong>deseja apenas executar um cluster</strong> ou espera <strong>participar ativamente do desenvolvimento do código do projeto Kubernetes</strong>? Se for a segunda opção,
escolha uma distro desenvolvida ativamente. Algumas distros usam apenas versão binária, mas oferecem uma maior variedade de opções.</li><li>Familiarize-se com os <a href=/docs/concepts/overview/components/>componentes</a> necessários para executar um cluster.</li></ul><h2 id=gerenciando-um-cluster>Gerenciando um cluster</h2><ul><li>Aprenda como <a href=/docs/concepts/architecture/nodes/>gerenciar nós</a>.</li><li>Aprenda a configurar e <a href=/docs/concepts/policy/resource-quotas/>gerenciar a quota de recursos</a> para clusters compartilhados.</li></ul><h2 id=protegendo-um-cluster>Protegendo um cluster</h2><ul><li><p><a href=/docs/tasks/administer-cluster/certificates/>Gerar Certificados</a> descreve os passos para gerar certificados usando diferentes cadeias de ferramentas.</p></li><li><p><a href=/docs/concepts/containers/container-environment/>Ambiente de Contêineres do Kubernetes</a> descreve o ambiente para contêineres gerenciados pelo kubelet em um nó Kubernetes.</p></li><li><p><a href=/docs/concepts/security/controlling-access>Controle de Acesso a API do Kubernetes</a> descreve como o Kubernetes implementa o controle de acesso para sua própria API.</p></li><li><p><a href=/docs/reference/access-authn-authz/authentication/>Autenticação</a> explica a autenticação no Kubernetes, incluindo as várias opções de autenticação.</p></li><li><p><a href=/docs/reference/access-authn-authz/authorization/>Autorização</a> é separado da autenticação e controla como as chamadas HTTP são tratadas.</p></li><li><p><a href=/docs/reference/access-authn-authz/admission-controllers/>Usando Controladores de Admissão</a> explica plugins que interceptam requisições para o servidor da API Kubernetes após
a autenticação e autorização.</p></li><li><p><a href=/docs/tasks/administer-cluster/sysctl-cluster/>usando Sysctl em um Cluster Kubernetes</a> descreve a um administrador como usar a ferramenta de linha de comando <code>sysctl</code> para
definir os parâmetros do kernel.</p></li><li><p><a href=/docs/tasks/debug-application-cluster/audit/>Auditoria</a> descreve como interagir com <em>logs</em> de auditoria do Kubernetes.</p></li></ul><h3 id=protegendo-o-kubelet>Protegendo o kubelet</h3><ul><li><a href=/docs/concepts/architecture/control-plane-node-communication/>Comunicação Control Plane-Nó</a></li><li><a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>TLS bootstrapping</a></li><li><a href=/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/>Autenticação/autorização do kubelet</a></li></ul><h2 id=serviços-opcionais-para-o-cluster>Serviços Opcionais para o Cluster</h2><ul><li><p><a href=/docs/concepts/services-networking/dns-pod-service/>Integração com DNS</a> descreve como resolver um nome DNS diretamente para um serviço Kubernetes.</p></li><li><p><a href=/docs/concepts/cluster-administration/logging/>Registro e Monitoramento da Atividade do Cluster</a> explica como funciona o <em>logging</em> no Kubernetes e como implementá-lo.</p></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-fb494ea3b1874bd753dcd11c3f35c2dc>1 - Visão Geral da Administração de Cluster</h1><p>A visão geral da administração de cluster é para qualquer um criando ou administrando um cluster Kubernetes. Assume-se que você tenha alguma familiaridade com os <a href=/docs/concepts/>conceitos</a> centrais do Kubernetes.</p><h2 id=planejando-um-cluster>Planejando um cluster</h2><p>Veja os guias em <a href=/docs/setup/>Setup</a> para exemplos de como planejar, iniciar e configurar clusters Kubernetes. As soluções listadas neste artigo são chamadas <em>distros</em>.</p><p>Antes de escolher um guia, aqui estão algumas considerações.</p><ul><li><p>Você quer experimentar o Kubernetes no seu computador, ou você quer construir um cluster de alta disponibilidade e multi-nós? Escolha as distros mais adequadas às suas necessidades.</p></li><li><p><strong>Se você esta projetando para alta-disponibilidade</strong>, saiba mais sobre configuração <a href=/docs/concepts/cluster-administration/federation/>clusters em múltiplas zonas</a>.</p></li><li><p>Você usará <strong>um cluster Kubernetes hospedado</strong>, como <a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a>, ou <strong>hospedará seu próprio cluster</strong>?</p></li><li><p>Seu cluster será <strong>on-premises</strong>, ou <strong>in the cloud (IaaS)</strong>? Kubernetes não suporta diretamente clusters híbridos. Em vez disso, você pode configurar vários clusters.</p></li><li><p><strong>Se você estiver configurando um Kubernetes on-premisess</strong>, considere qual <a href=/docs/concepts/cluster-administration/networking/>modelo de rede</a> melhor se adequa.</p></li><li><p>Você estará executando o Kubernetes em hardware <strong>"bare metal"</strong> ou em <strong>máquinas virtuais (VMs)</strong>?</p></li><li><p>Você <strong>quer apenas rodar um cluster</strong>, ou você espera fazer <strong>desenvolvimento ativo do código de projeto do Kubernetes</strong>? Se for a segunda opção, escolha uma distro mais ativa. Algumas distros fornecem apenas binários, mas oferecem uma maior variedade de opções.</p></li><li><p>Familiarize-se com os <a href=/docs/admin/cluster-components/>componentes</a> necessários para rodar um cluster.</p></li></ul><p>Nota: Nem todas as distros são ativamente mantidas. Escolha as distros que foram testadas com uma versão recente do Kubernetes.</p><h2 id=gerenciando-um-cluster>Gerenciando um cluster</h2><ul><li><p><a href=/docs/tasks/administer-cluster/cluster-management/>Gerenciando um cluster</a> descreve vários tópicos relacionados ao ciclo de vida de um cluster: criando um novo cluster, atualizando o nó mestre e os nós de trabalho do cluster, executando manutenção de nó (por exemplo, atualizações de kernel) e atualizando a versão da API do Kubernetes de um cluster em execução.</p></li><li><p>Aprender como <a href=/docs/concepts/nodes/node/>gerenciar um nó</a>.</p></li><li><p>Aprender como configurar e gerenciar o <a href=/docs/concepts/policy/resource-quotas/>recurso de quota</a> para um cluster compartilhado.</p></li></ul><h2 id=protegendo-um-cluster>Protegendo um cluster</h2><ul><li><p><a href=/docs/concepts/cluster-administration/certificates/>Certificados</a> descreve as etapas para gerar certificados usando diferentes ferramentas.</p></li><li><p><a href=/docs/concepts/containers/container-environment-variables/>Ambiente de Container Kubernetes</a> descreve o ambiente para contêineres gerenciados pelo Kubelet em um nó do Kubernetes.</p></li><li><p><a href=/docs/reference/access-authn-authz/controlling-access/>Controlando Acesso a API Kubernetes API</a> descreve como configurar
a permissão para usuários e contas de serviço.</p></li><li><p><a href=/docs/reference/access-authn-authz/authentication/>Autenticando</a> explica a autenticação no Kubernetes, incluindo as várias opções de autenticação.</p></li><li><p><a href=/docs/reference/access-authn-authz/authorization/>Autorização</a> é separada da autenticação e controla como as chamadas HTTP são tratadas.</p></li><li><p><a href=/docs/reference/access-authn-authz/admission-controllers/>Usando Controladores de Admissão</a> explica plug-ins que interceptam solicitações ao servidor da API do Kubernetes após autenticação e autorização.</p></li><li><p><a href=/docs/concepts/cluster-administration/sysctl-cluster/>Usando Sysctls em um Cluster Kubernetes</a> descreve a um administrador como usar a ferramenta de linha de comando <code>sysctl</code> para definir os parâmetros do kernel.</p></li><li><p><a href=/docs/tasks/debug-application-cluster/audit/>Auditando</a>
descreve como interagir com os logs de auditoria do Kubernetes.</p></li></ul><h3 id=protegendo-o-kubelet>Protegendo o kubelet</h3><ul><li><a href=/docs/concepts/architecture/master-node-communication/>Comunicação Master-Node</a></li><li><a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>TLS bootstrapping</a></li><li><a href=/docs/admin/kubelet-authentication-authorization/>Autenticação/Autorização Kubelet</a></li></ul><h2 id=serviços-opcionais-do-cluster>Serviços Opcionais do Cluster</h2><ul><li><p><a href=/docs/concepts/services-networking/dns-pod-service/>Integração DNS</a> descreve como resolver um nome DNS diretamente para um serviço do Kubernetes.</p></li><li><p><a href=/docs/concepts/cluster-administration/logging/>Logando e monitorando a atividade de cluster</a> explica como o log funciona no Kubernetes e como implementá-lo.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2bf9a93ab5ba014fb6ff70b22c29d432>2 - Certificates</h1><p>Ao usar um client para autenticação de certificado, você pode gerar certificados
manualmente através <code>easyrsa</code>, <code>openssl</code> ou <code>cfssl</code>.</p><h3 id=easyrsa>easyrsa</h3><p><strong>easyrsa</strong> pode gerar manualmente certificados para o seu cluster.</p><ol><li><p>Baixe, descompacte e inicialize a versão corrigida do easyrsa3.</p><pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz
tar xzf easy-rsa.tar.gz
cd easy-rsa-master/easyrsa3
./easyrsa init-pki
</code></pre></li><li><p>Gerar o CA. (<code>--batch</code> set automatic mode. <code>--req-cn</code> default CN to use.)</p><pre><code>./easyrsa --batch &quot;--req-cn=${MASTER_IP}@`date +%s`&quot; build-ca nopass
</code></pre></li><li><p>Gere o certificado e a chave do servidor.
O argumento <code>--subject-alt-name</code> define os possíveis IPs e nomes (DNS) que o servidor de API usará para ser acessado. O <code>MASTER_CLUSTER_IP</code> é geralmente o primeiro IP do serviço CIDR que é especificado como argumento em <code>--service-cluster-ip-range</code> para o servidor de API e o componente gerenciador do controlador. O argumento <code>--days</code> é usado para definir o número de dias após o qual o certificado expira.
O exemplo abaixo também assume que você está usando <code>cluster.local</code> como DNS de domínio padrão</p><pre><code>./easyrsa --subject-alt-name=&quot;IP:${MASTER_IP},&quot;\
&quot;IP:${MASTER_CLUSTER_IP},&quot;\
&quot;DNS:kubernetes,&quot;\
&quot;DNS:kubernetes.default,&quot;\
&quot;DNS:kubernetes.default.svc,&quot;\
&quot;DNS:kubernetes.default.svc.cluster,&quot;\
&quot;DNS:kubernetes.default.svc.cluster.local&quot; \
--days=10000 \
build-server-full server nopass
</code></pre></li><li><p>Copie <code>pki/ca.crt</code>, <code>pki/issued/server.crt</code>, e <code>pki/private/server.key</code> para o seu diretório.</p></li><li><p>Preencha e adicione os seguintes parâmetros aos parâmetros de inicialização do servidor de API:</p><pre><code>--client-ca-file=/yourdirectory/ca.crt
--tls-cert-file=/yourdirectory/server.crt
--tls-private-key-file=/yourdirectory/server.key
</code></pre></li></ol><h3 id=openssl>openssl</h3><p><strong>openssl</strong> pode gerar manualmente certificados para o seu cluster.</p><ol><li><p>Gere um ca.key com 2048bit:</p><pre><code>openssl genrsa -out ca.key 2048
</code></pre></li><li><p>De acordo com o ca.key, gere um ca.crt (use -days para definir o tempo efetivo do certificado):</p><pre><code> openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=${MASTER_IP}&quot; -days 10000 -out ca.crt
</code></pre></li><li><p>Gere um server.key com 2048bit:</p><pre><code>openssl genrsa -out server.key 2048
</code></pre></li><li><p>Crie um arquivo de configuração para gerar uma solicitação de assinatura de certificado (CSR - Certificate Signing Request). Certifique-se de substituir os valores marcados com colchetes angulares (por exemplo, <code>&lt;MASTER_IP></code>) com valores reais antes de salvá-lo em um arquivo (por exemplo, <code>csr.conf</code>). Note que o valor para o <code>MASTER_CLUSTER_IP</code> é o IP do cluster de serviços para o Servidor de API, conforme descrito na subseção anterior. O exemplo abaixo também assume que você está usando <code>cluster.local</code> como DNS de domínio padrão</p><pre><code>[ req ]
default_bits = 2048
prompt = no
default_md = sha256
req_extensions = req_ext
distinguished_name = dn

[ dn ]
C = &lt;country&gt;
ST = &lt;state&gt;
L = &lt;city&gt;
O = &lt;organization&gt;
OU = &lt;organization unit&gt;
CN = &lt;MASTER_IP&gt;

[ req_ext ]
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = kubernetes
DNS.2 = kubernetes.default
DNS.3 = kubernetes.default.svc
DNS.4 = kubernetes.default.svc.cluster
DNS.5 = kubernetes.default.svc.cluster.local
IP.1 = &lt;MASTER_IP&gt;
IP.2 = &lt;MASTER_CLUSTER_IP&gt;

[ v3_ext ]
authorityKeyIdentifier=keyid,issuer:always
basicConstraints=CA:FALSE
keyUsage=keyEncipherment,dataEncipherment
extendedKeyUsage=serverAuth,clientAuth
subjectAltName=@alt_names
</code></pre></li><li><p>Gere a solicitação de assinatura de certificado com base no arquivo de configuração:</p><pre><code>openssl req -new -key server.key -out server.csr -config csr.conf
</code></pre></li><li><p>Gere o certificado do servidor usando o ca.key, ca.crt e server.csr:</p><pre><code>openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \
-CAcreateserial -out server.crt -days 10000 \
-extensions v3_ext -extfile csr.conf
</code></pre></li><li><p>Veja o certificado:</p><pre><code>openssl x509  -noout -text -in ./server.crt
</code></pre></li></ol><p>Por fim, adicione os mesmos parâmetros nos parâmetros iniciais do Servidor de API.</p><h3 id=cfssl>cfssl</h3><p><strong>cfssl</strong> é outra ferramenta para geração de certificados.</p><ol><li><p>Baixe, descompacte e prepare as ferramentas de linha de comando, conforme mostrado abaixo. Observe que você pode precisar adaptar os comandos de exemplo abaixo com base na arquitetura do hardware e versão cfssl que você está usando.</p><pre><code>curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o cfssl
chmod +x cfssl
curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o cfssljson
chmod +x cfssljson
curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o cfssl-certinfo
chmod +x cfssl-certinfo
</code></pre></li><li><p>Crie um diretório para conter os artefatos e inicializar o cfssl:</p><pre><code>mkdir cert
cd cert
../cfssl print-defaults config &gt; config.json
../cfssl print-defaults csr &gt; csr.json
</code></pre></li><li><p>Crie um arquivo de configuração JSON para gerar o arquivo CA, por exemplo, <code>ca-config.json</code>:</p><pre><code>{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
          &quot;signing&quot;,
          &quot;key encipherment&quot;,
          &quot;server auth&quot;,
          &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
</code></pre></li><li><p>Crie um arquivo de configuração JSON para o CA - solicitação de assinatura de certificado (CSR - Certificate Signing Request), por exemplo, <code>ca-csr.json</code>. Certifique-se de substituir os valores marcados com colchetes angulares por valores reais que você deseja usar.</p><pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;:[{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre></li><li><p>Gere a chave CA (<code>ca-key.pem</code>) e o certificado (<code>ca.pem</code>):</p><pre><code>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</code></pre></li><li><p>Crie um arquivo de configuração JSON para gerar chaves e certificados para o Servidor de API, por exemplo, <code>server-csr.json</code>. Certifique-se de substituir os valores entre colchetes angulares por valores reais que você deseja usar. O <code>MASTER_CLUSTER_IP</code> é o IP do serviço do cluster para o servidor da API, conforme descrito na subseção anterior. O exemplo abaixo também assume que você está usando <code>cluster.local</code> como DNS de domínio padrão</p><pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;&lt;MASTER_IP&gt;&quot;,
    &quot;&lt;MASTER_CLUSTER_IP&gt;&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre></li><li><p>Gere a chave e o certificado para o Servidor de API, que são, por padrão, salvos nos arquivos <code>server-key.pem</code> e<code> server.pem</code> respectivamente:</p><pre><code>../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
--config=ca-config.json -profile=kubernetes \
server-csr.json | ../cfssljson -bare server
</code></pre></li></ol><h2 id=distribuindo-certificado-ca-auto-assinado>Distribuindo Certificado CA auto assinado</h2><p>Um nó cliente pode se recusar a reconhecer o certificado CA self-signed como válido.
Para uma implementação de não produção ou para uma instalação que roda atrás de um firewall, você pode distribuir certificados auto-assinados para todos os clientes e atualizar a lista de certificados válidos.</p><p>Em cada cliente, execute as seguintes operações:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
</span></span><span style=display:flex><span>sudo update-ca-certificates
</span></span></code></pre></div><pre tabindex=0><code>Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><h2 id=api-de-certificados>API de certificados</h2><p>Você pode usar a API <code>certificates.k8s.io</code> para provisionar
certificados x509 a serem usados ​​para autenticação conforme documentado
<a href=/docs/tasks/tls/managing-tls-in-a-cluster>aqui</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d649067a69d8d5c7e71564b42b96909e>3 - Conectividade do Cluster</h1><p>Conectividade é uma parte central do Kubernetes, mas pode ser desafiador
entender exatamente como é o seu funcionamento esperado. Existem 4 problemas
distintos em conectividade que devem ser tratados:</p><ol><li>Comunicações contêiner-para-contêiner altamente acopladas: Isso é resolvido
por <a class=glossary-tooltip title='O menor e mais simples objeto Kubernetes. Um Pod representa um conjunto de contêineres em execução no seu cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> e comunicações através do <code>localhost</code>.</li><li>Comunicações pod-para-pod: Esse é o foco primário desse documento.</li><li>Comunicações pod-para-serviço (<em>service</em>): Isso é tratado em <a href=/docs/concepts/services-networking/service/>Services</a>.</li><li>Comunicações Externas-para-serviços: Isso é tratado em <a href=/docs/concepts/services-networking/service/>services</a>.</li></ol><p>Kubernetes é basicamente o compartilhamento de máquinas entre aplicações. Tradicionalmente,
compartilhar máquinas requer a garantia de que duas aplicações não tentem utilizar
as mesmas portas. Coordenar a alocação de portas entre múltiplos desenvolvedores é
muito dificil de fazer em escala e expõe os usuários a problemas em nível do cluster e
fora de seu controle.</p><p>A alocação dinâmica de portas traz uma série de complicações para o sistema - toda
aplicação deve obter suas portas através de flags de configuração, os servidores de API
devem saber como inserir números dinämicos de portas nos blocos de configuração, serviços
precisam saber como buscar um ao outro, etc. Ao invés de lidar com isso, o Kubernetes
faz de uma maneira diferente.</p><h2 id=o-modelo-de-conectividade-e-rede-do-kubernetes>O modelo de conectividade e rede do Kubernetes</h2><p>Todo <code>Pod</code> obtém seu próprio endereço IP. Isso significa que vocë não precisa
criar links explícitos entre os <code>Pods</code> e vocë quase nunca terá que lidar com o
mapeamento de portas de contêineres para portas do host. Isso cria um modelo simples,
retro-compatível onde os <code>Pods</code> podem ser tratados muito mais como VMs ou hosts
físicos da perspectiva de alocação de portas, nomes, descobrimento de serviços
(<em>service discovery</em>), balanceamento de carga, configuração de aplicações e migrações.</p><p>O Kubernetes impõe os seguintes requisitos fundamentais para qualquer implementação de
rede (exceto qualquer política de segmentação intencional):</p><ul><li>pods em um nó podem se comunicar com todos os pods em todos os nós sem usar <em>NAT</em>.</li><li>agentes em um nó (por exemplo o kubelet ou um serviço local) podem se comunicar com
todos os Pods naquele nó.</li></ul><p>Nota: Para as plataformas que suportam <code>Pods</code> executando na rede do host (como o Linux):</p><ul><li>pods alocados na rede do host de um nó podem se comunicar com todos os pods
em todos os nós sem <em>NAT</em>.</li></ul><p>Esse modelo não só é menos complexo, mas é principalmente compatível com o
desejo do Kubernetes de permitir a portabilidade com baixo esforço de aplicações
de VMs para contêineres. Se a sua aplicação executava anteriormente em uma VM, sua VM
possuía um IP e podia se comunicar com outras VMs no seu projeto. Esse é o mesmo
modelo básico.</p><p>Os endereços de IP no Kubernetes existem no escopo do <code>Pod</code> - contêineres em um <code>Pod</code>
compartilham o mesmo <em>network namespace</em> - incluíndo seu endereço de IP e MAC.
Isso significa que contêineres que compõem um <code>Pod</code> podem se comunicar entre eles
através do endereço <code>localhost</code> e respectivas portas. Isso também significa que
contêineres em um mesmo <code>Pod</code> devem coordenar a alocação e uso de portas, o que não
difere do modelo de processos rodando dentro de uma mesma VM. Isso é chamado de
modelo "IP-por-pod".</p><p>Como isso é implementado é um detalhe do agente de execução de contêiner em uso.</p><p>É possível solicitar uma porta no nó que será encaminhada para seu <code>Pod</code> (chamado
de <em>portas do host</em>), mas isso é uma operação muito específica. Como esse encaminhamento
é implementado é um detalhe do agente de execução do contêiner. O <code>Pod</code> mesmo
desconhece a existência ou não de portas do host.</p><h2 id=como-implementar-o-modelo-de-conectividade-do-kubernetes>Como implementar o modelo de conectividade do Kubernetes</h2><p>Existe um número de formas de implementar esse modelo de conectividade. Esse
documento não é um estudo exaustivo desses vários métodos, mas pode servir como
uma introdução de várias tecnologias e serve como um ponto de início.</p><p>A conectividade no Kubernetes é fornecida através de plugins de
<a class=glossary-tooltip title='Plugins Container network interface (CNI) são um tipo de plugin de Rede em conformidade com a especificação appc/CNI.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label=CNIs>CNIs</a></p><p>As seguintes opções estão organizadas alfabeticamente e não implicam preferência por
qualquer solução.</p><div class="alert alert-secondary callout third-party-content" role=alert><strong>Nota:</strong>
Esta seção tem links para projetos de terceiros que fornecem a funcionalidade exigida pelo Kubernetes. Os autores do projeto Kubernetes não são responsáveis por esses projetos. Esta página obedece as <a href=https://github.com/cncf/foundation/blob/master/website-guidelines.md target=_blank>diretrizes de conteúdo do site CNCF</a>, listando os itens em ordem alfabética. Para adicionar um projeto a esta lista, leia o <a href=/docs/contribute/style/content-guide/#third-party-content>guia de conteúdo</a> antes de enviar sua alteração.</div><h3 id=antrea>Antrea</h3><p>O projeto <a href=https://github.com/vmware-tanzu/antrea>Antrea</a> é uma solução de
conectividade para Kubernetes que pretende ser nativa. Ela utiliza o Open vSwitch
na camada de conectividade de dados. O Open vSwitch é um switch virtual de alta
performance e programável que suporta Linux e Windows. O Open vSwitch permite
ao Antrea implementar políticas de rede do Kubernetes (<em>NetworkPolicies</em>) de
uma forma muito performática e eficiente.</p><p>Graças à característica programável do Open vSwitch, o Antrea consegue implementar
uma série de funcionalidades de rede e segurança.</p><h3 id=aws-vpc-cni-para-kubernetes>AWS VPC CNI para Kubernetes</h3><p>O <a href=https://github.com/aws/amazon-vpc-cni-k8s>AWS VPC CNI</a> oferece conectividade
com o AWS Virtual Private Cloud (VPC) para clusters Kubernetes. Esse plugin oferece
alta performance e disponibilidade e baixa latência. Adicionalmente, usuários podem
aplicar as melhores práticas de conectividade e segurança existentes no AWS VPC
para a construção de clusters Kubernetes. Isso inclui possibilidade de usar o
<em>VPC flow logs</em>, políticas de roteamento da VPC e grupos de segurança para isolamento
de tráfego.</p><p>O uso desse plugin permite aos Pods no Kubernetes ter o mesmo endereço de IP dentro do
pod como se eles estivessem dentro da rede do VPC. O CNI (Container Network Interface)
aloca um <em>Elastic Networking Interface</em> (ENI) para cada nó do Kubernetes e usa uma
faixa de endereços IP secundário de cada ENI para os Pods no nó. O CNI inclui
controles para pré alocação dos ENIs e endereços IP para um início mais rápido dos
pods e permite clusters com até 2,000 nós.</p><p>Adicionalmente, esse CNI pode ser utilizado junto com o <a href=https://docs.aws.amazon.com/eks/latest/userguide/calico.html>Calico</a>
para a criação de políticas de rede (<em>NetworkPolicies</em>). O projeto AWS VPC CNI
tem código fonte aberto com a <a href=https://github.com/aws/amazon-vpc-cni-k8s>documentação no Github</a>.</p><h3 id=azure-cni-para-o-kubernetes>Azure CNI para o Kubernetes</h3><p><a href=https://docs.microsoft.com/en-us/azure/virtual-network/container-networking-overview>Azure CNI</a> é um
plugin de <a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>código fonte aberto</a>
que integra os Pods do Kubernetes com uma rede virtual da Azure (também conhecida como VNet)
provendo performance de rede similar à de máquinas virtuais no ambiente. Os Pods
podem se comunicar com outras VNets e com ambientes <em>on-premises</em> com o uso de
funcionalidades da Azure, e também podem ter clientes com origem dessas redes.
Os Pods podem acessar serviços da Azure, como armazenamento e SQL, que são
protegidos por <em>Service Endpoints</em> e <em>Private Link</em>. Você pode utilizar as políticas
de segurança e roteamento para filtrar o tráfico do Pod. O plugin associa IPs da VNet
para os Pods utilizando um pool de IPs secundário pré-configurado na interface de rede
do nó Kubernetes.</p><p>O Azure CNI está disponível nativamente no <a href=https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni>Azure Kubernetes Service (AKS)</a>.</p><h3 id=calico>Calico</h3><p><a href=https://docs.projectcalico.org/>Calico</a> é uma solução de conectividade e
segurança para contêineres, máquinas virtuais e serviços nativos em hosts. O
Calico suporta múltiplas camadas de conectividade/dados, como por exemplo:
uma camada Linux eBPF nativa, uma camada de conectividade baseada em conceitos
padrão do Linux e uma camada baseada no HNS do Windows. O calico provê uma
camada completa de conectividade e rede, mas também pode ser usado em conjunto com
<a href=https://docs.projectcalico.org/networking/determine-best-networking#calico-compatible-cni-plugins-and-cloud-provider-integrations>CNIs de provedores de nuvem</a>
para permitir a criação de políticas de rede.</p><h3 id=cilium>Cilium</h3><p><a href=https://github.com/cilium/cilium>Cilium</a> é um software de código fonte aberto
para prover conectividade e segurança entre contêineres de aplicação. O Cilium
pode lidar com tráfego na camada de aplicação (ex. HTTP) e pode forçar políticas
de rede nas camadas L3-L7 usando um modelo de segurança baseado em identidade e
desacoplado do endereçamento de redes, podendo inclusive ser utilizado com outros
plugins CNI.</p><h3 id=flannel>Flannel</h3><p><a href=https://github.com/coreos/flannel#flannel>Flannel</a> é uma camada muito simples
de conectividade que satisfaz os requisitos do Kubernetes. Muitas pessoas
reportaram sucesso em utilizar o Flannel com o Kubernetes.</p><h3 id=google-compute-engine-gce>Google Compute Engine (GCE)</h3><p>Para os scripts de configuração do Google Compute Engine, <a href=https://cloud.google.com/vpc/docs/routes>roteamento
avançado</a> é usado para associar
para cada VM uma sub-rede (o padrão é <code>/24</code> - 254 IPs). Qualquer tráfico direcionado
para aquela sub-rede será roteado diretamente para a VM pela rede do GCE. Isso é
adicional ao IP principal associado à VM, que é mascarado para o acesso à Internet.
Uma <em>brige</em> Linux (chamada <code>cbr0</code>) é configurada para existir naquela sub-rede, e é
configurada no docker através da opção <code>--bridge</code>.</p><p>O Docker é iniciado com:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>DOCKER_OPTS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--bridge=cbr0 --iptables=false --ip-masq=false&#34;</span>
</span></span></code></pre></div><p>Essa <em>bridge</em> é criada pelo Kubelet (controlada pela opção <code>--network-plugin=kubenet</code>)
de acordo com a informação <code>.spec.podCIDR</code> do Nó.</p><p>O Docker irá agora alocar IPs do bloco <code>cbr-cidr</code>. Contêineres podem alcançar
outros contêineres e nós através da interface <code>cbr0</code>. Esses IPs são todos roteáveis
dentro da rede do projeto do GCE.</p><p>O GCE mesmo não sabe nada sobre esses IPs, então não irá mascará-los quando tentarem
se comunicar com a internet. Para permitir isso uma regra de IPTables é utilizada para
mascarar o tráfego para IPs fora da rede do projeto do GCE (no exemplo abaixo, 10.0.0.0/8):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>iptables -t nat -A POSTROUTING ! -d 10.0.0.0/8 -o eth0 -j MASQUERADE
</span></span></code></pre></div><p>Por fim, o encaminhamento de IP deve ser habilitado no Kernel de forma a processar
os pacotes vindos dos contêineres:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sysctl net.ipv4.ip_forward<span style=color:#666>=</span><span style=color:#666>1</span>
</span></span></code></pre></div><p>O resultado disso tudo é que <code>Pods</code> agora podem alcançar outros <code>Pods</code> e podem também
se comunicar com a Internet.</p><h3 id=kube-router>Kube-router</h3><p><a href=https://github.com/cloudnativelabs/kube-router>Kube-router</a> é uma solução construída
que visa prover alta performance e simplicidade operacional. Kube-router provê um
proxy de serviços baseado no <a href=https://www.linuxvirtualserver.org/software/ipvs.html>LVS/IPVS</a>,
uma solução de comunicação pod-para-pod baseada em encaminhamento de pacotes Linux e sem camadas
adicionais, e funcionalidade de políticas de redes baseadas no IPTables/IPSet.</p><h3 id=redes-l2-e-bridges-linux>Redes L2 e bridges Linux</h3><p>Se você tem uma rede L2 "burra", como um switch em um ambiente "bare-metal",
você deve conseguir fazer algo similar ao ambiente GCE explicado acima.
Note que essas instruções foram testadas casualmente - parece funcionar, mas
não foi propriamente testado. Se você conseguir usar essa técnica e aperfeiçoar
o processo, por favor nos avise!!</p><p>Siga a parte <em>"With Linux Bridge devices"</em> desse
<a href=https://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/>tutorial super bacana</a> do
Lars Kellogg-Stedman.</p><h3 id=multus>Multus (Plugin multi redes)</h3><p>Multus é um plugin Multi CNI para
suportar a funcionalidade multi redes do Kubernetes usando objetos baseados em <a class=glossary-tooltip title='Código customizado que define um recurso a ser adicionado ao seu servidor de API Kubernetes sem a necessidade de construir um servidor customizado.' data-toggle=tooltip data-placement=top href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ target=_blank aria-label=CRDs>CRDs</a>.</p><p>Multus suporta todos os <a href=https://github.com/containernetworking/plugins>plugins referência</a> (ex. <a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel>Flannel</a>,
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/dhcp>DHCP</a>,
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/macvlan>Macvlan</a>)
que implementam a especificação de CNI e plugins de terceiros
(ex. <a href=https://github.com/projectcalico/cni-plugin>Calico</a>, <a href=https://github.com/weaveworks/weave>Weave</a>,
<a href=https://github.com/cilium/cilium>Cilium</a>, <a href=https://github.com/contiv/netplugin>Contiv</a>).
Adicionalmente, Multus suporta cargas de trabalho no Kubernetes que necessitem de funcionalidades como
<a href=https://github.com/hustcat/sriov-cni>SRIOV</a>, <a href=https://github.com/Intel-Corp/sriov-cni>DPDK</a>,
<a href=https://github.com/intel/vhost-user-net-plugin>OVS-DPDK & VPP</a>.</p><h3 id=ovn-open-virtual-networking>OVN (Open Virtual Networking)</h3><p>OVN é uma solução de virtualização de redes de código aberto desenvolvido pela
comunidade Open vSwitch. Permite a criação de switches lógicos, roteadores lógicos,
listas de acesso, balanceadores de carga e mais, para construir diferences topologias
de redes virtuais. Esse projeto possui um plugin específico para o Kubernetes e a
documentação em <a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a>.</p><h2 id=próximos-passos>Próximos passos</h2><p>Design inicial do modelo de conectividade do Kubernetes e alguns planos futuros
estão descritos com maiores detalhes no
<a href=https://git.k8s.io/design-proposals-archive/network/networking.md>documento de design de redes</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c4b1e87a84441f8a90699a345ce48d68>4 - Arquitetura de Log</h1><p>Os logs de aplicativos e sistemas podem ajudá-lo a entender o que está acontecendo dentro do seu cluster. Os logs são particularmente úteis para depurar problemas e monitorar a atividade do cluster. A maioria das aplicações modernas possui algum tipo de mecanismo de logs; como tal, a maioria dos mecanismos de contêineres também é projetada para suportar algum tipo de log. O método de log mais fácil e abrangente para aplicações em contêiner é gravar nos fluxos de saída e erro padrão.</p><p>No entanto, a funcionalidade nativa fornecida por um mecanismo de contêiner ou tempo de execução geralmente não é suficiente para uma solução completa de log. Por exemplo, se um contêiner travar, um pod for despejado ou um nó morrer, geralmente você ainda desejará acessar os logs do aplicativo. Dessa forma, os logs devem ter armazenamento e ciclo de vida separados, independentemente de nós, pods ou contêineres. Este conceito é chamado <em>cluster-level-logging</em>. O log no nível de cluster requer um back-end separado para armazenar, analisar e consultar logs. O kubernetes não fornece uma solução de armazenamento nativa para dados de log, mas você pode integrar muitas soluções de log existentes no cluster do Kubernetes.</p><p>As arquiteturas de log no nível de cluster são descritas no pressuposto de que um back-end de log esteja presente dentro ou fora do cluster. Se você não estiver interessado em ter o log no nível do cluster, ainda poderá encontrar a descrição de como os logs são armazenados e manipulados no nó para serem úteis.</p><h2 id=log-básico-no-kubernentes>Log básico no Kubernentes</h2><p>Nesta seção, você pode ver um exemplo de log básico no Kubernetes que gera dados para o fluxo de saída padrão(standard output stream). Esta demostração usa uma <a href=/examples/debug/counter-pod.yaml>especificação de pod</a> com um contêiner que grava algum texto na saída padrão uma vez por segundo.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/pt-br/examples/debug/counter-pod.yaml download=debug/counter-pod.yaml><code>debug/counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("debug-counter-pod-yaml")' title="Copy debug/counter-pod.yaml to clipboard"></img></div><div class=includecode id=debug-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c,<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:#b44>&#39;i=0; while true; do echo &#34;$i: $(date)&#34;; i=$((i+1)); sleep 1; done&#39;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Para executar este pod, use o seguinte comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</span></span></code></pre></div><p>A saída será:</p><pre tabindex=0><code>pod/counter created
</code></pre><p>Para buscar os logs, use o comando <code>kubectl logs</code>, da seguinte maneira:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter
</span></span></code></pre></div><p>A saída será:</p><pre tabindex=0><code>0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><p>Você pode usar <code>kubectl logs</code> para recuperar logs de uma instanciação anterior de um contêiner com o sinalizador <code>--previous</code>, caso o contêiner tenha falhado. Se o seu pod tiver vários contêineres, você deverá especificar quais logs do contêiner você deseja acessar anexando um nome de contêiner ao comando. Veja a <a href=/docs/reference/generated/kubectl/kubectl-commands#logs>documentação do <code>kubectl logs</code></a> para mais destalhes.</p><h2 id=logs-no-nível-do-nó>Logs no nível do Nó</h2><p><img src=/images/docs/user-guide/logging/logging-node-level.png alt="Log no nível do nó"></p><p>Tudo o que um aplicativo em contêiner grava no <code>stdout</code> e <code>stderr</code> é tratado e redirecionado para algum lugar por dentro do mecanismo de contêiner. Por exemplo, o mecanismo de contêiner do Docker redireciona esses dois fluxos para <a href=https://docs.docker.com/engine/admin/logging/overview>um driver de log</a>, configurado no Kubernetes para gravar em um arquivo no formato json.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> O driver de log json do Docker trata cada linha como uma mensagem separada. Ao usar o driver de log do Docker, não há suporte direto para mensagens de várias linhas. Você precisa lidar com mensagens de várias linhas no nível do agente de log ou superior.</div><p>Por padrão, se um contêiner reiniciar, o kubelet manterá um contêiner terminado com seus logs. Se um pod for despejado do nó, todos os contêineres correspondentes também serão despejados, juntamente com seus logs.</p><p>Uma consideração importante no log no nível do nó está implementado a rotação de log, para que os logs não consumam todo o armazenamento disponível no nó. Atualmente, o Kubernentes não é responsável pela rotação de logs, mas uma ferramenta de deployment deve configurar uma solução para resolver isso.
Por exemplo, nos clusters do Kubernetes, implementados pelo script <code>kube-up.sh</code>, existe uma ferramenta <a href=https://linux.die.net/man/8/logrotate><code>logrotate</code></a> configurada para executar a cada hora. Você pode configurar um tempo de execução do contêiner para girar os logs do aplicativo automaticamente, por exemplo, usando o <code>log-opt</code> do Docker.
No script <code>kube-up.sh</code>, a última abordagem é usada para imagem COS no GCP, e a anterior é usada em qualquer outro ambiente. Nos dois casos por padrão, a rotação é configurada para ocorrer quando o arquivo de log exceder 10MB.</p><p>Como exemplo, você pode encontrar informações detalhadas sobre como o <code>kube-up.sh</code> define o log da imagem COS no GCP no <a href=https://github.com/kubernetes/kubernetes/blob/master/cluster/gce/gci/configure-helper.sh>script</a> correspondente.</p><p>Quando você executa <a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code></a> como no exemplo de log básico acima, o kubelet no nó lida com a solicitação e lê diretamente do arquivo de log, retornando o conteúdo na resposta.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Atualmente, se algum sistema externo executou a rotação, apenas o conteúdo do arquivo de log mais recente estará disponível através de <code>kubectl logs</code>. Por exemplo, se houver um arquivo de 10MB, o <code>logrotate</code> executa a rotação e existem dois arquivos, um com 10MB de tamanho e um vazio, o <code>kubectl logs</code> retornará uma resposta vazia.</div><h3 id=logs-de-componentes-do-sistema>Logs de componentes do sistema</h3><p>Existem dois tipos de componentes do sistema: aqueles que são executados em um contêiner e aqueles que não são executados em um contêiner. Por exemplo:</p><ul><li>O scheduler Kubernetes e o kube-proxy são executados em um contêiner.</li><li>O tempo de execução do kubelet e do contêiner, por exemplo, Docker, não é executado em contêineres.</li></ul><p>Nas máquinas com systemd, o tempo de execução do kubelet e do container é gravado no journald. Se systemd não estiver presente, eles gravam em arquivos <code>.log</code> no diretório <code>/var/log</code>.
Os componentes do sistema dentro dos contêineres sempre gravam no diretório <code>/var/log</code>, ignorando o mecanismo de log padrão. Eles usam a biblioteca de logs <a href=https://github.com/kubernetes/klog>klog</a>. Você pode encontrar as convenções para a gravidade do log desses componentes nos <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>documentos de desenvolvimento sobre log</a>.</p><p>Da mesma forma que os logs de contêiner, os logs de componentes do sistema no diretório <code>/var/log</code> devem ser rotacionados. Nos clusters do Kubernetes criados pelo script <code>kube-up.sh</code>, esses logs são configurados para serem rotacionados pela ferramenta <code>logrotate</code> diariamente ou quando o tamanho exceder 100MB.</p><h2 id=arquiteturas-de-log-no-nível-de-cluster>Arquiteturas de log no nível de cluster</h2><p>Embora o Kubernetes não forneça uma solução nativa para o log em nível de cluster, há várias abordagens comuns que você pode considerar. Aqui estão algumas opções:</p><ul><li>Use um agente de log no nível do nó que seja executado em todos os nós.</li><li>Inclua um contêiner sidecar dedicado para efetuar logging em um pod de aplicativo.</li><li>Envie logs diretamente para um back-end de dentro de um aplicativo.</li></ul><h3 id=usando-um-agente-de-log-de-nó>Usando um agente de log de nó</h3><p><img src=/images/docs/user-guide/logging/logging-with-node-agent.png alt="Usando um agente de log no nível do nó"></p><p>Você pode implementar o log em nível de cluster incluindo um <em>agente de log em nível de nó</em> em cada nó. O agente de log é uma ferramenta dedicada que expõe logs ou envia logs para um back-end. Geralmente, o agente de log é um contêiner que tem acesso a um diretório com arquivos de log de todos os contêineres de aplicativos nesse nó.</p><p>Como o agente de log deve ser executado em todos os nós, é comum implementá-lo como uma réplica do DaemonSet, um pod de manifesto ou um processo nativo dedicado no nó. No entanto, as duas últimas abordagens são obsoletas e altamente desencorajadas.</p><p>O uso de um agente de log no nível do nó é a abordagem mais comum e incentivada para um cluster Kubernetes, porque ele cria apenas um agente por nó e não requer alterações nos aplicativos em execução no nó. No entanto, o log no nível do nó <em>funciona apenas para a saída padrão dos aplicativos e o erro padrão</em>.</p><p>O Kubernetes não especifica um agente de log, mas dois agentes de log opcionais são fornecidos com a versão Kubernetes: <a href=/docs/user-guide/logging/stackdriver>Stackdriver Logging</a> para uso com o Google Cloud Platform e <a href=/docs/user-guide/logging/elasticsearch>Elasticsearch</a>. Você pode encontrar mais informações e instruções nos documentos dedicados. Ambos usam <a href=http://www.fluentd.org/>fluentd</a> com configuração customizada como um agente no nó.</p><h3 id=usando-um-contêiner-sidecar-com-o-agente-de-log>Usando um contêiner sidecar com o agente de log</h3><p>Você pode usar um contêiner sidecar de uma das seguintes maneiras:</p><ul><li>O container sidecar transmite os logs do aplicativo para seu próprio <code>stdout</code>.</li><li>O contêiner do sidecar executa um agente de log, configurado para selecionar logs de um contêiner de aplicativo.</li></ul><h4 id=streaming-sidecar-conteiner>Streaming sidecar conteiner</h4><p><img src=/images/docs/user-guide/logging/logging-with-streaming-sidecar.png alt="Conteiner sidecar com um streaming container"></p><p>Fazendo com que seus contêineres de sidecar fluam para seus próprios <code>stdout</code> e <code>stderr</code>, você pode tirar proveito do kubelet e do agente de log que já executam em cada nó. Os contêineres sidecar lêem logs de um arquivo, socket ou journald. Cada contêiner sidecar individual imprime o log em seu próprio <code>stdout</code> ou <code>stderr</code> stream.</p><p>Essa abordagem permite separar vários fluxos de logs de diferentes partes do seu aplicativo, algumas das quais podem não ter suporte para gravar em <code>stdout</code> ou <code>stderr</code>. A lógica por trás do redirecionamento de logs é mínima, portanto dificilmente representa uma sobrecarga significativa. Além disso, como <code>stdout</code> e <code>stderr</code> são manipulados pelo kubelet, você pode usar ferramentas internas como o <code>kubectl logs</code>.</p><p>Considere o seguinte exemplo. Um pod executa um único contêiner e grava em dois arquivos de log diferentes, usando dois formatos diferentes. Aqui está um arquivo de configuração para o Pod:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/pt-br/examples/admin/logging/two-files-counter-pod.yaml download=admin/logging/two-files-counter-pod.yaml><code>admin/logging/two-files-counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-yaml")' title="Copy admin/logging/two-files-counter-pod.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Seria uma bagunça ter entradas de log de diferentes formatos no mesmo fluxo de logs, mesmo se você conseguisse redirecionar os dois componentes para o fluxo <code>stdout</code> do contêiner. Em vez disso, você pode introduzir dois contêineres sidecar. Cada contêiner sidecar pode direcionar um arquivo de log específico de um volume compartilhado e depois redirecionar os logs para seu próprio fluxo <code>stdout</code>.</p><p>Aqui está um arquivo de configuração para um pod que possui dois contêineres sidecar:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/pt-br/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml download=admin/logging/two-files-counter-pod-streaming-sidecar.yaml><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-streaming-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-streaming-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -f /var/log/1.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -f /var/log/2.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Agora, quando você executa este pod, é possível acessar cada fluxo de log separadamente, executando os seguintes comandos:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-1
</span></span></code></pre></div><pre tabindex=0><code>0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-2
</span></span></code></pre></div><pre tabindex=0><code>Mon Jan  1 00:00:00 UTC 2001 INFO 0
Mon Jan  1 00:00:01 UTC 2001 INFO 1
Mon Jan  1 00:00:02 UTC 2001 INFO 2
...
</code></pre><p>O agente no nível do nó instalado em seu cluster coleta esses fluxos de logs automaticamente sem nenhuma configuração adicional. Se desejar, você pode configurar o agente para analisar as linhas de log, dependendo do contêiner de origem.</p><p>Observe que, apesar do baixo uso da CPU e da memória (ordem de alguns milicores por CPU e ordem de vários megabytes de memória), gravar logs em um arquivo e depois transmiti-los para o <code>stdout</code> pode duplicar o uso do disco. Se você tem um aplicativo que grava em um único arquivo, geralmente é melhor definir <code>/dev/stdout</code> como destino, em vez de implementar a abordagem de contêiner de transmissão no sidecar.</p><p>Os contêineres sidecar também podem ser usados para rotacionar arquivos de log que não podem ser rotacionados pelo próprio aplicativo. Um exemplo dessa abordagem é um pequeno contêiner executando <code>logrotate</code> periodicamente.
No entanto, é recomendável usar o <code>stdout</code> e o <code>stderr</code> diretamente e deixar as políticas de rotação e retenção no kubelet.</p><h4 id=contêiner-sidecar-com-um-agente-de-log>Contêiner sidecar com um agente de log</h4><p><img src=/images/docs/user-guide/logging/logging-with-sidecar-agent.png alt="Contêiner sidecar com um agente de log"></p><p>Se o agente de log no nível do nó não for flexível o suficiente para sua situação, você poderá criar um contêiner secundário com um agente de log separado que você configurou especificamente para executar com seu aplicativo.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> O uso de um agente de log em um contêiner sidecar pode levar a um consumo significativo de recursos. Além disso, você não poderá acessar esses logs usando o comando <code>kubectl logs</code>, porque eles não são controlados pelo kubelet.</div><p>Como exemplo, você pode usar o <a href=/docs/tasks/debug-application-cluster/logging-stackdriver/>Stackdriver</a>, que usa fluentd como um agente de log. Aqui estão dois arquivos de configuração que você pode usar para implementar essa abordagem. O primeiro arquivo contém um <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMap</a> para configurar o fluentd.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/pt-br/examples/admin/logging/fluentd-sidecar-config.yaml download=admin/logging/fluentd-sidecar-config.yaml><code>admin/logging/fluentd-sidecar-config.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-fluentd-sidecar-config-yaml")' title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard"></img></div><div class=includecode id=admin-logging-fluentd-sidecar-config-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fluentd.conf</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/1.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/1.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format1
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/2.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/2.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format2
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;match **&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type google_cloud
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/match&gt;</span><span style=color:#bbb>    
</span></span></span></code></pre></div></div></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> A configuração do fluentd está além do escopo deste artigo. Para obter informações sobre como configurar o fluentd, consulte a <a href=http://docs.fluentd.org/>documentação oficial do fluentd</a>.</div><p>O segundo arquivo descreve um pod que possui um contêiner sidecar rodando fluentemente.
O pod monta um volume onde o fluentd pode coletar seus dados de configuração.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/pt-br/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml download=admin/logging/two-files-counter-pod-agent-sidecar.yaml><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-agent-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-agent-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-agent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/fluentd-gcp:1.30<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>FLUENTD_ARGS<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>-c /etc/fluentd-config/fluentd.conf<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Depois de algum tempo, você pode encontrar mensagens de log na interface do Stackdriver.</p><p>Lembre-se de que este é apenas um exemplo e você pode realmente substituir o fluentd por qualquer agente de log, lendo de qualquer fonte dentro de um contêiner de aplicativo.</p><h3 id=expondo-logs-diretamente-do-aplicativo>Expondo logs diretamente do aplicativo</h3><p><img src=/images/docs/user-guide/logging/logging-from-application.png alt="Expondo logs diretamente do aplicativo"></p><p>Você pode implementar o log no nível do cluster, expondo ou enviando logs diretamente de todos os aplicativos; no entanto, a implementação desse mecanismo de log está fora do escopo do Kubernetes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5cc31ecfba86467f8884856412cfb6b2>5 - Logs de Sistema</h1><p>Logs de componentes do sistema armazenam eventos que acontecem no cluster, sendo muito úteis para depuração. Seus níveis de detalhe podem ser ajustados para mais ou para menos. Podendo se ater, por exemplo, a mostrar apenas os erros que ocorrem no componente, ou chegando a mostrar cada passo de um evento. (Como acessos HTTP, mudanças no estado dos pods, ações dos controllers, ou decisões do scheduler).</p><h2 id=klog>Klog</h2><p><a href=https://github.com/kubernetes/klog>Klog</a> é a biblioteca de logs do Kubernetes. Responsável por gerar as mensagens de log para os componentes do sistema.</p><p>Para mais informações acerca da sua configuração, veja a documentação da <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/>ferramenta de linha de comando</a>.</p><p>Um exemplo do formato padrão dos logs da biblioteca:</p><pre tabindex=0><code>I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]
</code></pre><h3 id=logs-estruturados>Logs Estruturados</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [alpha]</code></div><div class="alert alert-danger warning callout" role=alert><strong>Aviso:</strong><p>A migração pro formato de logs estruturados é um processo em andamento. Nem todos os logs estão dessa forma na versão atual. Sendo assim, para realizar o processamento de arquivos de log, você também precisa lidar com logs não estruturados.</p><p>A formatação e serialização dos logs ainda estão sujeitas a alterações.</p></div><p>A estruturação dos logs trás uma estrutura uniforme para as mensagens de log, permitindo a extração programática de informações. Logs estruturados podem ser armazenados e processados com menos esforço e custo. Esse formato é totalmente retrocompatível e é habilitado por padrão.</p><p>Formato dos logs estruturados:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#b44>&lt;klog header&gt; &#34;&lt;message&gt;&#34; &lt;key1&gt;</span><span style=color:#666>=</span><span style=color:#b44>&#34;&lt;value1&gt;&#34; &lt;key2&gt;=&#34;&lt;value2&gt;&#34; ...</span>
</span></span></code></pre></div><p>Exemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#b44>I1025 00:15:15.525108       1 controller_utils.go:116] &#34;Pod status updated&#34; pod</span><span style=color:#666>=</span><span style=color:#b44>&#34;kube-system/kubedns&#34; status=&#34;ready&#34;</span>
</span></span></code></pre></div><h3 id=logs-em-formato-json>Logs em formato JSON</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [alpha]</code></div><div class="alert alert-danger warning callout" role=alert><strong>Aviso:</strong><p>Algumas opções da biblioteca klog ainda não funcionam com os logs em formato JSON. Para ver uma lista completa de quais são estas, veja a documentação da <a href=/docs/reference/command-line-tools-reference/>ferramenta de linha de comando</a>.</p><p>Nem todos os logs estarão garantidamente em formato JSON (como por exemplo durante o início de processos). Sendo assim, se você pretende realizar o processamento dos logs, seu código deverá saber tratar também linhas que não são JSON.</p><p>O nome dos campos e a serialização JSON ainda estão sujeitos a mudanças.</p></div><p>A opção <code>--logging-format=json</code> muda o formato dos logs, do formato padrão da klog para JSON. Abaixo segue um exemplo de um log em formato JSON (identado):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;ts&#34;</span>: <span style=color:#666>1580306777.04728</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;v&#34;</span>: <span style=color:#666>4</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;msg&#34;</span>: <span style=color:#b44>&#34;Pod status updated&#34;</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;pod&#34;</span>:{
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;nginx-1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
</span></span><span style=display:flex><span>   },
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;ready&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Chaves com significados especiais:</p><ul><li><code>ts</code> - Data e hora no formato Unix (obrigatório, float)</li><li><code>v</code> - Nível de detalhe (obrigatório, int, padrão 0)</li><li><code>err</code> - Mensagem de erro (opcional, string)</li><li><code>msg</code> - Mensagem (obrigatório, string)</li></ul><p>Lista dos componentes que suportam o formato JSON atualmente:</p><ul><li><a class=glossary-tooltip title='Componente da camada de gerenciamento que executa os processos de controle.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li><li><a class=glossary-tooltip title='O componente da camada de gerenciamento que serve a API do Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li><li><a class=glossary-tooltip title='Componente da camada de gerenciamento que observa os pods recém-criados sem nenhum nó atribuído, e seleciona um nó para executá-los.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li><li><a class=glossary-tooltip title='Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a></li></ul><h3 id=limpeza-dos-logs>Limpeza dos Logs</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code></div><div class="alert alert-danger warning callout" role=alert><strong>Aviso:</strong> A funcionalidade de limpeza dos logs pode causar impactos significativos na performance, sendo portanto contraindicada em produção.</div><p>A opção <code>--experimental-logging-sanitization</code> habilita o filtro de limpeza dos logs.
Quando habilitado, esse filtro inspeciona todos os argumentos dos logs, procurando por campos contendo dados sensíveis (como senhas, chaves e tokens). Tais campos não serão expostos nas mensagens de log.</p><p>Lista dos componentes que suportam a limpeza de logs atualmente:</p><ul><li><a class=glossary-tooltip title='Componente da camada de gerenciamento que executa os processos de controle.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li><li><a class=glossary-tooltip title='O componente da camada de gerenciamento que serve a API do Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li><li><a class=glossary-tooltip title='Componente da camada de gerenciamento que observa os pods recém-criados sem nenhum nó atribuído, e seleciona um nó para executá-los.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li><li><a class=glossary-tooltip title='Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a></li></ul><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> O filtro de limpeza dos logs não impede a exposição de dados sensíveis nos logs das aplicações em execução.</div><h3 id=nível-de-detalhe-dos-logs>Nível de detalhe dos logs</h3><p>A opção <code>-v</code> controla o nível de detalhe dos logs. Um valor maior aumenta o número de eventos registrados, começando a registrar também os eventos menos importantes. Similarmente, um valor menor restringe os logs apenas aos eventos mais importantes. O valor padrão 0 registra apenas eventos críticos.</p><h3 id=localização-dos-logs>Localização dos Logs</h3><p>Existem dois tipos de componentes do sistema: aqueles que são executados em um contêiner e aqueles que não são. Por exemplo:</p><ul><li>O <a href=https://kubernetes.io/pt-br/docs/concepts/overview/components/#kube-scheduler>Kubernetes scheduler</a> e o <a href=https://kubernetes.io/pt-br/docs/concepts/overview/components/#kube-proxy>kube-proxy</a> são executados em um contêiner.</li><li>O <a href=https://kubernetes.io/pt-br/docs/concepts/overview/components/#kubelet>kubelet</a> e os <a href=https://kubernetes.io/pt-br/docs/concepts/overview/components/#container-runtime>agentes de execução</a>, como o Docker por exemplo, não são executados em contêineres.</li></ul><p>Em máquinas com systemd, o kubelet e os agentes de execução gravam os logs no journald.
Em outros casos, eles escrevem os logs em arquivos <code>.log</code> no diretório <code>/var/log</code>.
Já os componentes executados dentro de contêineres, sempre irão escrever os logs em arquivos <code>.log</code>
no diretório <code>/var/log</code>, ignorando o mecanismo padrão de log.</p><p>De forma similar aos logs de contêiner, os logs de componentes do sistema no diretório <code>/var/log</code> devem ser rotacionados.
Nos clusters Kubernetes criados com o script <code>kube-up.sh</code>, a rotação dos logs é configurada pela ferramenta <code>logrotate</code>. Essa ferramenta rotaciona os logs diariamente
ou quando o tamanho do arquivo excede 100MB.</p><h2 id=próximos-passos>Próximos passos</h2><ul><li>Leia sobre <a href=/pt-br/docs/concepts/cluster-administration/logging/>Arquitetura de Logs do Kubernetes</a></li><li>Leia sobre <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging>Logs Estruturados</a></li><li>Leia sobre <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>Convenções sobre os níveis de logs</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cbfd3654996eae9fcdef009f70fa83f0>6 - Métricas para componentes do sistema Kubernetes</h1><p>Métricas dos componentes do sistema podem dar uma visão melhor do que acontece internamente. Métricas são particularmente úteis para construir <em>dashboards</em> e alertas.</p><p>Componentes do Kubernetes emitem métricas no <a href=https://prometheus.io/docs/instrumenting/exposition_formats/>formato Prometheus</a>. Esse formato é um texto simples estruturado, projetado para que pessoas e máquinas possam lê-lo.</p><h2 id=métricas-no-kubernetes>Métricas no Kubernetes</h2><p>Na maioria dos casos, as métricas estão disponíveis no <em>endpoint</em> <code>/metrics</code> do servidor HTTP. Para componentes que não expõem o <em>endpoint</em> por padrão, ele pode ser ativado usando a <em>flag</em> <code>--bind-address</code>.</p><p>Exemplos desses componentes:</p><ul><li><a class=glossary-tooltip title='Componente da camada de gerenciamento que executa os processos de controle.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li><li><a class=glossary-tooltip title='kube-proxy é um proxy de rede executado em cada nó do cluster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a></li><li><a class=glossary-tooltip title='O componente da camada de gerenciamento que serve a API do Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li><li><a class=glossary-tooltip title='Componente da camada de gerenciamento que observa os pods recém-criados sem nenhum nó atribuído, e seleciona um nó para executá-los.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li><li><a class=glossary-tooltip title='Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a></li></ul><p>Em um ambiente de produção, você pode querer configurar o <a href=https://prometheus.io/>Servidor Prometheus</a> ou algum outro coletor de métricas e disponibilizá-las em algum tipo de banco de dados temporais.</p><p>Observe que o <a class=glossary-tooltip title='Um agente que é executado em cada node no cluster. Ele garante que os contêineres estejam sendo executados em um pod.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kubelet target=_blank aria-label=kubelet>kubelet</a> também expõe métricas nos <em>endpoints</em> <code>/metrics/cadvisor</code>, <code>/metrics/resource</code> e <code>/metrics/probes</code>. Essas métricas não possuem o mesmo ciclo de vida.</p><p>Se o seu <em>cluster</em> usa <a class=glossary-tooltip title='Gerencia decisões de autorização, permitindo que os administradores configurem dinamicamente políticas de acesso por meio da API do Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/rbac/ target=_blank aria-label=RBAC>RBAC</a>, ler as métricas requer autorização por meio de um usuário, grupo ou <em>ServiceAccount</em> com um <em>ClusterRole</em> que conceda o acesso ao <code>/metrics</code>.</p><p>Por exemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prometheus<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>nonResourceURLs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/metrics&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- get<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=ciclo-de-vida-da-métrica>Ciclo de vida da métrica</h2><p>Métrica alfa → Métrica estável → Métrica ultrapassada → Métrica oculta → Métrica excluída</p><p>A métrica alfa não tem garantias de estabilidade. Essas métricas podem ser modificadas ou deletadas a qualquer momento.</p><p>Métricas estáveis possuem a garantia de que não serão alteradas. Isso significa:</p><ul><li>Uma métrica estável sem uma assinatura ultrapassada não será deletada ou renomeada</li><li>O tipo de uma métrica estável não será modificado</li></ul><p>As métricas ultrapassadas estão programadas para exclusão, mas ainda estão disponíveis para uso.
Essas métricas incluem uma anotação sobre a versão em que se tornarão ultrapassadas.</p><p>Por exemplo:</p><ul><li><p>Antes de se tornar ultrapassado</p><pre tabindex=0><code># HELP some_counter isso conta coisas
# TYPE some_counter contador
some_counter 0
</code></pre></li><li><p>Depois de se tornar ultrapassado</p><pre tabindex=0><code># HELP some_counter (obsoleto desde 1.15.0) isso conta coisas
# TYPE some_counter contador
some_counter 0
</code></pre></li></ul><p>Métricas ocultas não são mais publicadas para extração, mas ainda estão disponíveis para uso. Para usar uma métrica oculta, por favor consulte a seção <a href=#mostrar-m%C3%A9tricas-ocultas>mostrar métricas ocultas</a>.</p><p>Métricas excluídas não estão mais disponíveis e não podem mais ser usadas.</p><h2 id=mostrar-métricas-ocultas>Mostrar métricas ocultas</h2><p>Como descrito anteriormente, administradores podem habilitar métricas ocultas por meio de uma <em>flag</em> de linha de comando em um binário específico. Isso pode ser usado como uma saída de emergência para os administradores caso percam a migração das métricas ultrapassadas na última versão.</p><p>A <em>flag</em> <code>show-hidden-metrics-for-version</code> usa uma versão para a qual você deseja mostrar métricas ultrapassadas nessa versão. A versão é expressada como x.y, onde x é a versão principal e y a versão secundária. A versão de <em>patch</em> não é necessária mesmo que uma métrica possa ser descontinuada em uma versão de <em>patch</em>, o motivo é que a política de descontinuação de métricas é executada na versão secundária.</p><p>A <em>flag</em> só pode usar a versão secundária anterior como seu valor. Todas as métricas ocultas no anterior serão emitidas se os administradores definirem a versão anterior como <code>show-hidden-metrics-for-version</code>. A versão muito antiga não é permitida porque viola a política de métricas ultrapassadas.</p><p>Utilize a métrica <code>A</code> como exemplo, assumindo que <code>A</code> está obsoleto em 1.n. De acordo com a política de métricas ultrapassadas, podemos chegar à seguinte conclusão:</p><ul><li>Na versão <code>1.n</code>, a métrica está ultrapassada, e pode ser emitida por padrão.</li><li>Na versão <code>1.n+1</code>, a métrica está oculta por padrão e pode ser emitida via linha de comando <code>show-hidden-metrics-for-version=1.n</code>.</li><li>Na versão <code>1.n+2</code>, a métrica deve ser removida do código fonte. Não há mais <em>escape hatch</em>.</li></ul><p>Se você está atualizando da versão <code>1.12</code> para <code>1.13</code>, mas ainda depende da métrica <code>A</code> ultrapassada em <code>1.12</code>, você deve definir métricas ocultas via linha de comando: <code>--show-hidden-metrics=1.12</code> e lembre-se de remover essa dependência de métrica antes de atualizar para <code>1.14</code>.</p><h2 id=desativar-métricas-do-accelerator>Desativar métricas do <em>accelerator</em></h2><p>O kubelet coleta métricas do <em>accelerator</em> por meio do cAdvisor. Para coletar essas métricas, para <em>accelerator</em> como as GPUs NVIDIA, o kubelet mantinha uma alça aberta no driver. Isso significava que, para realizar alterações na infraestrutura (por exemplo, atualizar o <em>driver</em>), um administrador do <em>cluster</em> precisa interromper o agente kubelet.</p><p>A responsabilidade de colear métricas do <em>accelerator</em> agora pertence ao fornecedor, e não ao kubelet. Os fornecedores devem providenciar um contêiner que colete métricas e as exponha ao serviço de métricas (por exemplo, Prometheus).</p><p>O <a href=/docs/reference/command-line-tools-reference/feature-gates/><code>DisableAcceleratorUsageMetrics</code> <em>feature gate</em></a> desabilita as métricas coletadas pelo kubelet, com uma <a href=https://github.com/kubernetes/enhancements/tree/411e51027db842355bd489691af897afc1a41a5e/keps/sig-node/1867-disable-accelerator-usage-metrics#graduation-criteria><em>timeline</em> para habilitar esse recurso por padrão</a>.</p><h2 id=métricas-de-componentes>Métricas de componentes</h2><h3 id=métricas-do-kube-controller-manager>Métricas do <em>kube-controller-manager</em></h3><p>As métricas do <em>controller manager</em> fornecem informações importantes sobre o desempenho e a integridade do <em>controller manager</em>.
Essas métricas incluem métricas comuns do agente de execução da linguagem Go, tais como a quantidade de <em>go_routine</em> e métricas específicas do <em>controller</em>, como latência de requisições etcd ou latência da <em>API</em> dos provedores de serviços de nuvem (AWS, GCE, OpenStack), que podem ser usadas para medir a integridade de um <em>cluster</em>.</p><p>A partir do Kubernetes 1.7, métricas detalhadas de provedores de serviços de nuvem estão disponíveis para operações de armazenamento para o GCE, AWS, Vsphere e OpenStack.
Essas métricas podem ser usadas para monitorar a integridade das operações de volumes persistentes.</p><p>Por exemplo, para o GCE as seguintes métricas são chamadas:</p><pre tabindex=0><code>cloudprovider_gce_api_request_duration_seconds { request = &#34;instance_list&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_insert&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_delete&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;attach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;detach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;list_disk&#34;}
</code></pre><h3 id=métricas-do-kube-scheduler>Métricas do <em>kube-scheduler</em></h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code></div><p>O <em>scheduler</em> expõe métricas opcionais que relatam os recursos solicitados e os limites desejados de todos os <em>pods</em> em execução. Essas métricas podem ser usadas para criar <em>dashboards</em> de planejamento de capacidade, avaliar os limites de agendamentos atuais ou históricos, identificar rapidamente cargas de trabalho que não podem ser agendadas devido à falta de recursos e comparar o uso atual com a solicitação do <em>pod</em>.</p><p>O <em>kube-scheduler</em> identifica as requisições de <a href=/docs/concepts/configuration/manage-resources-containers/>recursos e limites</a> configurado para cada <em>Pod</em>; quando uma requisição ou limite é diferente de zero o <em>kube-scheduler</em> relata uma <em>timeseries</em> de métricas. Essa <em>timeseries</em> é etiquetada por:</p><ul><li><em>namespace</em></li><li>nome do <em>pod</em></li><li>o nó onde o <em>pod</em> está programado ou uma <em>string</em> vazia caso ainda não esteja programado</li><li>prioridade</li><li>o <em>scheduler</em> atribuído para esse <em>pod</em></li><li>o nome do recurso (por exemplo, <code>cpu</code>)</li><li>a unidade do recurso, se conhecida (por exemplo, <code>cores</code>)</li></ul><p>Uma vez que o <em>pod</em> alcança um estado de conclusão (sua <code>restartPolicy</code> está como <code>Never</code> ou <code>onFailure</code> e está na fase de <code>Succeeded</code> ou <code>Failed</code>, ou foi deletado e todos os contêineres tem um estado de terminado), a série não é mais relatada já que o <em>scheduler</em> agora está livre para agendar a execução de outros <em>pods</em>. As duas métricas são chamadas de <code>kube_pod_resource_request</code> e <code>kube_pod_resource_limit</code>.</p><p>As métricas são expostas no <em>endpoint</em> HTTP <code>/metrics/resources</code> e requerem a mesma autorização que o <em>endpoint</em> <code>/metrics</code> no <em>scheduler</em>. Você deve usar a <em>flag</em> <code>--show-hidden-metrics-for-version=1.20</code> para expor essas métricas de estabilidade alfa.</p><h2 id=desativando-métricas>Desativando métricas</h2><p>Você pode desativar explicitamente as métricas via linha de comando utilizando a <em>flag</em> <code>--disabled-metrics</code>. Isso pode ser desejado se, por exemplo, uma métrica estiver causando um problema de desempenho. A entrada é uma lista de métricas desabilitadas (ou seja, <code>--disabled-metrics=metric1,metric2</code>).</p><h2 id=aplicação-de-cardinalidade-de-métrica>Aplicação de cardinalidade de métrica</h2><p>As métricas com dimensões sem limites podem causar problemas de memória nos componentes que elas instrumentam. Para limitar a utilização de recursos você pode usar a opção de linha de comando <code>--allow-label-value</code> para dinamicamente configurar uma lista de permissões de valores de <em>label</em> para uma métrica.</p><p>No estágio alfa, a <em>flag</em> pode receber apenas uma série de mapeamentos como lista de permissões de <em>labels</em> para uma métrica.
Cada mapeamento tem o formato <code>&lt;metric_name>,&lt;label_name>=&lt;allowed_labels></code> onde <code>&lt;allowed_labels></code> é uma lista separada por vírgulas de nomes aceitáveis para a <em>label</em>.</p><p>O formato geral se parece com:
<code>--allow-label-value &lt;metric_name>,&lt;label_name>='&lt;allow_value1>, &lt;allow_value2>...', &lt;metric_name2>,&lt;label_name>='&lt;allow_value1>, &lt;allow_value2>...', ...</code>.</p><p>Por exemplo:
<code>--allow-label-value number_count_metric,odd_number='1,3,5', number_count_metric,even_number='2,4,6', date_gauge_metric,weekend='Saturday,Sunday'</code></p><h2 id=próximos-passos>Próximos passos</h2><ul><li>Leia sobre o <a href=https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format>formato de texto do Prometheus</a> para métricas</li><li>Veja a lista de <a href=https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml>métricas estáveis ​​do Kubernetes</a></li><li>Leia sobre a <a href=/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior>Política de suspensão de uso do Kubernetes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2e05a56491965ae320c2662590b2ca18>7 - Configurando o Garbage Collection do kubelet</h1><p>O Garbage collection(Coleta de lixo) é uma função útil do kubelet que limpa imagens e contêineres não utilizados. O kubelet executará o garbage collection para contêineres a cada minuto e para imagens a cada cinco minutos.</p><p>Ferramentas externas de garbage collection não são recomendadas, pois podem potencialmente interromper o comportamento do kubelet removendo os contêineres que existem.</p><h2 id=coleta-de-imagens>Coleta de imagens</h2><p>O Kubernetes gerencia o ciclo de vida de todas as imagens através do imageManager, com a cooperação do cadvisor.</p><p>A política para o garbage collection de imagens leva dois fatores em consideração:
<code>HighThresholdPercent</code> e <code>LowThresholdPercent</code>. Uso do disco acima do limite acionará o garbage collection. O garbage collection excluirá as imagens que foram menos usadas recentemente até que o nível fique abaixo do limite.</p><h2 id=coleta-de-container>Coleta de container</h2><p>A política para o garbage collection de contêineres considera três variáveis definidas pelo usuário. <code>MinAge</code> é a idade mínima em que um contêiner pode ser coletado. <code>MaxPerPodContainer</code> é o número máximo de contêineres mortos que todo par de pod (UID, container name) pode ter. <code>MaxContainers</code> é o número máximo de contêineres mortos totais. Essas variáveis podem ser desabilitadas individualmente, definindo <code>MinAge</code> como zero e definindo <code>MaxPerPodContainer</code> e <code>MaxContainers</code> respectivamente para menor que zero.</p><p>O Kubelet atuará em contêineres não identificados, excluídos ou fora dos limites definidos pelos sinalizadores mencionados. Os contêineres mais antigos geralmente serão removidos primeiro. <code>MaxPerPodContainer</code> e <code>MaxContainer</code> podem potencialmente conflitar entre si em situações em que a retenção do número máximo de contêineres por pod (<code>MaxPerPodContainer</code>) estaria fora do intervalo permitido de contêineres globais mortos (<code>MaxContainers</code>). O <code>MaxPerPodContainer</code> seria ajustado nesta situação: O pior cenário seria fazer o downgrade do <code>MaxPerPodContainer</code> para 1 e remover os contêineres mais antigos. Além disso, os contêineres pertencentes a pods que foram excluídos são removidos assim que se tornem mais antigos que <code>MinAge</code>.</p><p>Os contêineres que não são gerenciados pelo kubelet não estão sujeitos ao garbage collection de contêiner.</p><h2 id=configurações-do-usuário>Configurações do usuário</h2><p>Os usuários podem ajustar os seguintes limites para ajustar o garbage collection da imagem com os seguintes sinalizadores do kubelet:</p><ol><li><code>image-gh-high-threshold</code>, a porcentagem de uso de disco que aciona o garbage collection da imagem. O padrão é 85%.</li><li><code>image-gc-low-threshold</code>, a porcentagem de uso de disco com o qual o garbage collection da imagem tenta liberar. O padrão é 80%.</li></ol><p>Também permitimos que os usuários personalizem a política do garbagem collection através dos seguintes sinalizadores do kubelet:</p><ol><li><code>minimum-container-ttl-duration</code>, idade mínima para um contêiner finalizado antes de ser colectado. O padrão é 0 minuto, o que significa que todo contêiner finalizado será coletado como lixo.</li><li><code>maximum-dead-containers-per-container</code>, número máximo de instâncias antigas a serem retidas por contêiner. O padrão é 1.</li><li><code>maximum-dead-containers</code>, número máximo de instâncias antigas de contêineres para retenção global. O padrão é -1, o que significa que não há limite global.</li></ol><p>Os contêineres podem ser potencialmente coletados como lixo antes que sua utilidade expire. Esses contêineres podem conter logs e outros dados que podem ser úteis para solucionar problemas. Um valor suficientemente grande para <code>maximum-dead-containers-per-container</code> é altamente recomendado para permitir que pelo menos 1 contêiner morto seja retido por contêiner esperado. Um valor maior para <code>maximum-dead-containers</code> também é recomendados por um motivo semelhante.
Consulte <a href=https://github.com/kubernetes/kubernetes/issues/13287>esta issue</a> para obter mais detalhes.</p><h2 id=descontinuado>Descontinuado</h2><p>Alguns recursos do Garbage Collection neste documento serão substituídos pelo kubelet eviction no futuro.</p><p>Incluindo:</p><table><thead><tr><th>Flag Existente</th><th>Nova Flag</th><th>Fundamentação</th></tr></thead><tbody><tr><td><code>--image-gc-high-threshold</code></td><td><code>--eviction-hard</code> ou <code>--eviction-soft</code></td><td>os sinais existentes de despejo podem acionar o garbage collection da imagem</td></tr><tr><td><code>--image-gc-low-threshold</code></td><td><code>--eviction-minimum-reclaim</code></td><td>recuperações de despejo atinge o mesmo comportamento</td></tr><tr><td><code>--maximum-dead-containers</code></td><td></td><td>descontinuado quando os logs antigos forem armazenados fora do contexto do contêiner</td></tr><tr><td><code>--maximum-dead-containers-per-container</code></td><td></td><td>descontinuado quando os logs antigos forem armazenados fora do contexto do contêiner</td></tr><tr><td><code>--minimum-container-ttl-duration</code></td><td></td><td>descontinuado quando os logs antigos forem armazenados fora do contexto do contêiner</td></tr><tr><td><code>--low-diskspace-threshold-mb</code></td><td><code>--eviction-hard</code> ou <code>eviction-soft</code></td><td>O despejo generaliza os limites do disco para outros recursos</td></tr><tr><td><code>--outofdisk-transition-frequency</code></td><td><code>--eviction-pressure-transition-period</code></td><td>O despejo generaliza a transição da pressão do disco para outros recursos</td></tr></tbody></table><h2 id=próximos-passos>Próximos passos</h2><p>Consulte <a href=/docs/tasks/administer-cluster/out-of-resource/>Configurando a Manipulação de Recursos Insuficientes</a> para mais detalhes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-08e94e6a480e0d6b2de72d84a1b97617>8 - Proxies no Kubernetes</h1><p>Esta página descreve o uso de proxies com Kubernetes.</p><h2 id=proxies>Proxies</h2><p>Existem vários tipos diferentes de proxies que você pode encontrar usando Kubernetes:</p><ol><li>O <a href=/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api>kubectl proxy</a>:</li></ol><p>Quando o kubectl proxy é utilizado ocorre o seguinte:
- executa na máquina do usuário ou em um pod
- redireciona/encapsula conexões direcionadas ao localhost para o servidor de API
- a comunicação entre o cliente e o o proxy usa HTTP
- a comunicação entre o proxy e o servidor de API usa HTTPS
- o proxy localiza o servidor de API do cluster
- o proxy adiciona os cabeçalhos de comunicação.</p><ol><li><p>O <a href=/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>apiserver proxy</a>:</p><ul><li>é um bastion server, construído no servidor de API</li><li>conecta um usuário fora do cluster com os IPs do cluster que não podem ser acessados de outra forma</li><li>executa dentro do processo do servidor de API</li><li>cliente para proxy usa HTTPS (ou HTTP se o servidor de API for configurado)</li><li>proxy para o destino pode usar HTTP ou HTTPS conforme escolhido pelo proxy usando as informações disponíveis</li><li>pode ser usado para acessar um Nó, Pod ou serviço</li><li>faz balanceamento de carga quando usado para acessar um Service.</li></ul></li><li><p>O <a href=/docs/concepts/services-networking/service/#ips-and-vips>kube proxy</a>:</p><ul><li>executa em todos os Nós</li><li>atua como proxy para UDP, TCP e SCTP</li><li>não aceita HTTP</li><li>provém balanceamento de carga</li><li>apenas é usado para acessar serviços.</li></ul></li><li><p>Um Proxy/Balanceador de carga na frente de servidores de API(s):</p><ul><li>a existência e a implementação de tal elemento varia de cluster para cluster, por exemplo nginx</li><li>fica entre todos os clientes e um ou mais serviços</li><li>atua como balanceador de carga se existe mais de um servidor de API.</li></ul></li><li><p>Balanceadores de carga da nuvem em serviços externos:</p><ul><li>são fornecidos por algum provedor de nuvem (e.x AWS ELB, Google Cloud Load Balancer)</li><li>são criados automaticamente quando o serviço de Kubernetes tem o tipo <code>LoadBalancer</code></li><li>geralmente suportam apenas UDP/TCP</li><li>O suporte ao SCTP fica por conta da implementação do balanceador de carga da provedora de nuvem</li><li>a implementação varia de acordo com o provedor de cloud.</li></ul></li></ol><p>Os usuários de Kubernetes geralmente não precisam se preocupar com outras coisas além dos dois primeiros tipos. O
administrador do cluster tipicamente garante que os últimos tipos serão configurados corretamente.</p><h2 id=redirecionamento-de-requisições>Redirecionamento de requisições</h2><p>Os proxies substituíram as capacidades de redirecionamento. O redirecionamento foi depreciado.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-85d633ae590aa20ec024f1b7af1d74fc>9 - Instalando Complementos</h1><div class="alert alert-secondary callout third-party-content" role=alert><strong>Nota:</strong>
Esta seção tem links para projetos de terceiros que fornecem a funcionalidade exigida pelo Kubernetes. Os autores do projeto Kubernetes não são responsáveis por esses projetos. Esta página obedece as <a href=https://github.com/cncf/foundation/blob/master/website-guidelines.md target=_blank>diretrizes de conteúdo do site CNCF</a>, listando os itens em ordem alfabética. Para adicionar um projeto a esta lista, leia o <a href=/docs/contribute/style/content-guide/#third-party-content>guia de conteúdo</a> antes de enviar sua alteração.</div><p>Complementos estendem as funcionalidades do Kubernetes.</p><p>Esta página lista alguns dos complementos disponíveis e links com suas respectivas instruções de instalação.</p><h2 id=rede-e-política-de-rede>Rede e Política de Rede</h2><ul><li><a href=https://www.github.com/noironetworks/aci-containers>ACI</a> fornece rede integrada de contêineres e segurança de rede com a Cisco ACI.</li><li><a href=https://antrea.io/>Antrea</a> opera nas camadas 3 e 4 do modelo de rede OSI para fornecer serviços de rede e de segurança para o Kubernetes, aproveitando o Open vSwitch como camada de dados de rede.</li><li><a href=https://docs.projectcalico.org/latest/introduction/>Calico</a> é um provedor de serviços de rede e de políticas de rede. Este complemento suporta um conjunto flexível de opções de rede, de modo a permitir a escolha da opção mais eficiente para um dado caso de uso, incluindo redes <em>overlay</em> (sobrepostas) e não-<em>overlay</em>, com ou sem o uso do protocolo BGP. Calico usa o mesmo mecanismo para aplicar políticas de rede a hosts, pods, e aplicações na camada de <em>service mesh</em> (quando Istio e Envoy estão instalados).</li><li><a href=https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel>Canal</a> une Flannel e Calico, fornecendo rede e política de rede.</li><li><a href=https://github.com/cilium/cilium>Cilium</a> é um plug-in de rede de camada 3 e de políticas de rede que pode aplicar políticas HTTP/API/camada 7 de forma transparente. Tanto o modo de roteamento quanto o de sobreposição/encapsulamento são suportados. Este plug-in também consegue operar no topo de outros plug-ins CNI.</li><li><a href=https://github.com/cni-genie/CNI-Genie>CNI-Genie</a> permite que o Kubernetes se conecte facilmente a uma variedade de plug-ins CNI, como Calico, Canal, Flannel, Romana ou Weave.</li><li><a href=https://contivpp.io/>Contiv</a> oferece serviços de rede configuráveis para diferentes casos de uso (camada 3 nativa usando BGP, <em>overlay</em> (sobreposição) usando vxlan, camada 2 clássica e Cisco-SDN/ACI) e também um <em>framework</em> rico de políticas de rede. O projeto Contiv é totalmente <a href=http://github.com/contiv>open source</a>. O <a href=http://github.com/contiv/install>instalador</a> fornece opções de instalação com ou sem kubeadm.</li><li><a href=http://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a> é uma plataforma open source baseada no <a href=https://tungsten.io>Tungsten Fabric</a> que oferece virtualização de rede multi-nuvem e gerenciamento de políticas de rede. O Contrail e o Tungsten Fabric são integrados a sistemas de orquestração de contêineres, como Kubernetes, OpenShift, OpenStack e Mesos, e fornecem modos de isolamento para cargas de trabalho executando em máquinas virtuais, contêineres/pods e servidores físicos.</li><li><a href=https://github.com/flannel-io/flannel#deploying-flannel-manually>Flannel</a> é um provedor de redes <em>overlay</em> (sobrepostas) que pode ser usado com o Kubernetes.</li><li><a href=https://github.com/ZTE/Knitter/>Knitter</a> é um plug-in para suporte de múltiplas interfaces de rede em Pods do Kubernetes.</li><li><a href=https://github.com/k8snetworkplumbingwg/multus-cni>Multus</a> é um plugin para suporte a várias interfaces de rede em Pods no Kubernetes. Este plug-in pode agir como um "meta-plug-in", ou um plug-in CNI que se comunica com múltiplos outros plug-ins CNI (por exemplo, Calico, Cilium, Contiv, Flannel), além das cargas de trabalho baseadas em SRIOV, DPDK, OVS-DPDK e VPP no Kubernetes.</li><li><a href=https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html>NSX-T</a> Container Plug-in (NCP) fornece integração entre o VMware NSX-T e sistemas de orquestração de contêineres como o Kubernetes. Além disso, oferece também integração entre o NSX-T e as plataformas CaaS/PaaS baseadas em contêiner, como o Pivotal Container Service (PKS) e o OpenShift.</li><li><a href=https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst>Nuage</a> é uma plataforma de rede definida por software que fornece serviços de rede baseados em políticas entre os Pods do Kubernetes e os ambientes não-Kubernetes, com visibilidade e monitoramento de segurança.</li><li><a href=https://github.com/ovn-org/ovn-kubernetes/>OVN-Kubernetes</a> é um provedor de rede para o Kubernetes baseado no <a href=https://github.com/ovn-org/ovn/>OVN (Open Virtual Network)</a>, uma implementação de redes virtuais que surgiu através do projeto Open vSwitch (OVS). O OVN-Kubernetes fornece uma implementação de rede baseada em <em>overlay</em> (sobreposição) para o Kubernetes, incluindo uma implementação baseada em OVS para serviços de balanceamento de carga e políticas de rede.</li><li><a href=https://github.com/opnfv/ovn4nfv-k8s-plugin>OVN4NFV-K8S-Plugin</a> é um plug-in controlador CNI baseado no OVN (Open Virtual Network) que fornece serviços de rede <em>cloud native</em>, como <em>Service Function Chaining</em> (SFC), redes <em>overlay</em> (sobrepostas) OVN múltiplas, criação dinâmica de subredes, criação dinâmica de redes virtuais, provedor de rede VLAN e provedor de rede direto, e é plugável a outros plug-ins multi-rede. Ideal para cargas de trabalho que utilizam computação de borda <em>cloud native</em> em redes multi-cluster.</li><li><a href=https://github.com/romana/romana>Romana</a> é uma solução de rede de camada 3 para redes de pods que também suporta a <a href=/pt-br/docs/concepts/services-networking/network-policies/>API NetworkPolicy</a>. Detalhes da instalação do complemento Kubeadm disponíveis <a href=https://github.com/romana/romana/tree/master/containerize>aqui</a>.</li><li><a href=https://www.weave.works/docs/net/latest/kube-addon/>Weave Net</a> fornece rede e política de rede, funciona em ambos os lados de uma partição de rede e não requer um banco de dados externo.</li></ul><h2 id=descoberta-de-serviço>Descoberta de Serviço</h2><ul><li><a href=https://coredns.io>CoreDNS</a> é um servidor DNS flexível e extensível que pode ser <a href=https://github.com/coredns/deployment/tree/master/kubernetes>instalado</a> como o serviço de DNS dentro do cluster para ser utilizado por pods.</li></ul><h2 id=visualização-amp-controle>Visualização & Controle</h2><ul><li><a href=https://github.com/kubernetes/dashboard#kubernetes-dashboard>Dashboard</a> é uma interface web para gestão do Kubernetes.</li><li><a href=https://www.weave.works/documentation/scope-latest-installing/#k8s>Weave Scope</a> é uma ferramenta gráfica para visualizar contêineres, pods, serviços, entre outros objetos do cluster. Pode ser utilizado com uma <a href=https://cloud.weave.works/>conta Weave Cloud</a>. Como alternativa, é possível hospedar a interface do usuário por conta própria.</li></ul><h2 id=infraestrutura>Infraestrutura</h2><ul><li><a href=https://kubevirt.io/user-guide/#/installation/installation>KubeVirt</a> é um complemento para executar máquinas virtuais no Kubernetes. É geralmente executado em clusters em máquina física.</li></ul><h2 id=complementos-legados>Complementos Legados</h2><p>Existem vários outros complementos documentados no diretório <a href=https://git.k8s.io/kubernetes/cluster/addons>cluster/addons</a> que não são mais utilizados.</p><p>Projetos bem mantidos devem ser listados aqui. PRs são bem-vindos!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-31c9327d2332c585341b64ddafa19cdd>10 - Prioridade e imparcialidade da API</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code></div><p>Controlar o comportamento do servidor da API Kubernetes em uma situação de sobrecarga
é uma tarefa chave para administradores de cluster. O <a class=glossary-tooltip title='O componente da camada de gerenciamento que serve a API do Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a> tem alguns controles disponíveis
(ou seja, as <em>flags</em> <code>--max-requests-inflight</code> e <code>--max-mutating-requests-inflight</code>)
para limitar a quantidade de trabalho pendente que será aceito,
evitando que uma grande quantidade de solicitações de entrada sobrecarreguem, e
potencialmente travando o servidor da API, mas essas <em>flags</em> não são suficientes para garantir
que as solicitações mais importantes cheguem em um período de alto tráfego.</p><p>O recurso de prioridade e imparcialidade da API (do inglês <em>API Priority and Fairness</em>, APF) é uma alternativa que melhora
as limitações mencionadas acima. A APF classifica
e isola os pedidos de uma forma mais refinada. Também introduz
uma quantidade limitada de filas, para que nenhuma solicitação seja rejeitada nos casos
de sobrecargas muito breves. As solicitações são despachadas das filas usando uma
técnica de filas justa para que, por exemplo, um
<a class=glossary-tooltip title='Um ciclo de controle que observa o estado partilhado do cluster através do API Server e efetua mudanças tentando mover o estado atual em direção ao estado desejado.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> não precise
negar as outras requisições (mesmo no mesmo nível de prioridade).</p><p>Esse recurso foi projetado para funcionar bem com controladores padrão, que
usam informantes e reagem a falhas de solicitações da API com exponencial
back-off, e outros clientes que também funcionam desta forma.</p><div class="alert alert-warning caution callout" role=alert><strong>Cuidado:</strong> Solicitações classificadas como "de longa duração" — principalmente <em>watches</em> — não são
sujeitas ao filtro da prioridade e imparcialidade da API. Isso também é verdade para
a <em>flag</em> <code>--max-requests-inflight</code> sem o recurso da APF ativado.</div><h2 id=ativando-desativando-a-prioridade-e-imparcialidade-da-api>Ativando/Desativando a prioridade e imparcialidade da API</h2><p>O recurso de prioridade e imparcialidade da API é controlado por um feature gate
e está habilitado por padrão. Veja <a href=/docs/reference/command-line-tools-reference/feature-gates/>Portões de Recurso</a>
para uma explicação geral dos portões de recursos e como habilitar e
desativá-los. O nome da porta de recurso para APF é
"APIPriorityAndFairness". Este recurso também envolve um <a class=glossary-tooltip title='Um conjunto de caminhos relacionados da API Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning target=_blank aria-label='API Group'>API Group</a> com: (a) um
Versão <code>v1alpha1</code>, desabilitada por padrão, e (b) <code>v1beta1</code> e
Versões <code>v1beta2</code>, habilitadas por padrão. Você pode desativar o feature gate
e versões beta do grupo de APIs adicionando a seguinte
<em>flag</em> para sua invocação <code>kube-apiserver</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kube-apiserver <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--feature-gates<span style=color:#666>=</span><span style=color:#b8860b>APIPriorityAndFairness</span><span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--runtime-config<span style=color:#666>=</span>flowcontrol.apiserver.k8s.io/v1beta1<span style=color:#666>=</span>false,flowcontrol.apiserver.k8s.io/v1beta2<span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span> <span style=color:#080;font-style:italic># …and other flags as usual</span>
</span></span></code></pre></div><p>Como alternativa, você pode habilitar a versão v1alpha1 do grupo de APIs
com <code>--runtime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true</code>.</p><p>A <em>flag</em> <code>--enable-priority-and-fairness=false</code> desabilitará o
recurso de prioridade e imparcialidade da API, mesmo que outras <em>flags</em> o tenha ativado.</p><h2 id=conceitos>Conceitos</h2><p>Existem vários recursos distintos envolvidos na APF.
As solicitações recebidas são classificadas por atributos da solicitação usando
<em>FlowSchemas</em> e atribuídos a níveis de prioridade. Os níveis de prioridade adicionam um grau de
isolamento mantendo limites de simultaneidade separados, para que as solicitações atribuídas
a diferentes níveis de prioridade não travem outros. Dentro de um nível de prioridade,
um algoritmo de <em>fair queuing</em> impede que solicitações de diferentes <em>flows</em> fiquem sem energia
entre si, e permite que os pedidos sejam enfileirados para evitar que um alto tráfego
cause falhas nas solicitações quando a carga média é aceitavelmente baixa.</p><h3 id=níveis-de-prioridade>Níveis de prioridade</h3><p>Sem o APF ativado, a simultaneidade geral no servidor de API é limitada pelo
<code>kube-apiserver</code> as <em>flags</em> <code>--max-requests-inflight</code> e
<code>--max-mutating-requests-inflight</code>. Com o APF ativado, os limites de simultaneidade
definidos por esses sinalizadores são somados e, em seguida, a soma é dividida entre um
conjunto configurável de <em>níveis de prioridade</em>. Cada solicitação recebida é atribuída a um
nível de prioridade único, e cada nível de prioridade só despachará tantos
solicitações simultâneas conforme sua configuração permite.</p><p>A configuração padrão, por exemplo, inclui níveis de prioridade separados para
solicitações de eleição de líder, solicitações de controladores integrados e solicitações de
<em>Pods</em>. Isso significa que um <em>pod</em> mal-comportado que inunda o servidor da API com
solicitações não podem impedir a eleição do líder ou ações dos controladores integrados
de ter sucesso.</p><h3 id=enfileiramento>Enfileiramento</h3><p>Mesmo dentro de um nível de prioridade pode haver um grande número de fontes distintas de
tráfego. Em uma situação de sobrecarga, é importante evitar um fluxo de
pedidos de outros serviços (em particular, no caso relativamente comum de um
único cliente buggy inundando o kube-apiserver com solicitações, esse cliente buggy
idealmente não teria muito impacto em outros clientes). Isto é
tratadas pelo uso de um algoritmo de <em>fair queuing</em> para processar solicitações que são atribuídas
ao mesmo nível de prioridade. Cada solicitação é atribuída a um <em>flow</em>, identificado pelo
nome do FlowSchema correspondente mais um <em>flow distincter</em> — que
é o usuário solicitante, o namespace do recurso de destino ou nada — e o
sistema tenta dar peso aproximadamente igual a solicitações em diferentes
fluxos do mesmo nível de prioridade.
Para habilitar o tratamento distinto de instâncias distintas, os controladores que
muitas instâncias devem ser autenticadas com nomes de usuário distintos</p><p>Depois de classificar uma solicitação em um fluxo, a APF
pode então atribuir a solicitação a uma fila. Esta atribuição usa
uma técnica conhecida como <a class=glossary-tooltip title='A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.' data-toggle=tooltip data-placement=top href='/pt-br/docs/reference/glossary/?all=true#term-shuffle-sharding' target=_blank aria-label='shuffle sharding'>shuffle sharding</a>, que faz uso relativamente eficiente de
filas para isolar fluxos de baixa intensidade de fluxos de alta intensidade.</p><p>Os detalhes do algoritmo de enfileiramento são ajustáveis ​​para cada nível de prioridade e
permitem que os administradores troquem o uso de memória, justiça (a propriedade que
fluxos independentes irão progredir quando o tráfego total exceder a capacidade),
tolerância para tráfego e a latência adicionada induzida pelo enfileiramento.</p><h3 id=solicitações-de-isenção>Solicitações de isenção</h3><p>Alguns pedidos são considerados suficientemente importantes para que não estejam sujeitos a
qualquer uma das limitações impostas por este recurso. Estas isenções impedem uma
configuração de controle de fluxo mal configurada de desabilitar totalmente um servidor da API.</p><h2 id=recursos>Recursos</h2><p>A API de controle de fluxo envolve dois tipos de recursos.
<a href=/docs/reference/generated/kubernetes-api/v1.25/#prioritylevelconfiguration-v1beta2-flowcontrol-apiserver-k8s-io>PriorityLevelConfigurations</a>
define as classes de isolamento disponíveis, a parte da concorrência disponível
que cada um pode tratar e permite o ajuste fino do comportamento das filas.
<a href=/docs/reference/generated/kubernetes-api/v1.25/#flowschema-v1beta2-flowcontrol-apiserver-k8s-io>FlowSchemas</a>
são usados ​​para classificar solicitações de entrada individuais, correspondendo cada uma a um
único PriorityLevelConfiguration. Há também uma versão <code>v1alpha1</code>
do mesmo grupo de APIs e tem os mesmos tipos com a mesma sintaxe e
semântica.</p><h3 id=prioritylevelconfiguration>PriorityLevelConfiguration</h3><p>Um PriorityLevelConfiguration representa uma única classe de isolamento. Cada
PriorityLevelConfiguration tem um limite independente no número de solicitações de pendências
e limitações no número de solicitações enfileiradas.</p><p>Os limites de simultaneidade para PriorityLevelConfigurations não são especificados no número absoluto
de solicitações, mas sim em "compartilhamentos de simultaneidade". A simultaneidade limite total
para o servidor da API é distribuído entre os PriorityLevelConfigurations existentes
em proporção com esses compartilhamentos. Isso permite um
administrador de cluster aumentar ou diminuir a quantidade total de tráfego para um
servidor reiniciando <code>kube-apiserver</code> com um valor diferente para
<code>--max-requests-inflight</code> (ou <code>--max-mutating-requests-inflight</code>), e todos os
PriorityLevelConfigurations verá sua simultaneidade máxima permitida aumentar (ou
abaixar) pela mesma proporção.</p><div class="alert alert-warning caution callout" role=alert><strong>Cuidado:</strong> Com o recurso prioridade e imparcialidade ativado, o limite total de simultaneidade para
o servidor é definido como a soma de <code>--max-requests-inflight</code> e
<code>--max-mutating-requests-inflight</code>. Já não há distinção
entre solicitações mutantes e não mutantes; se você quiser tratá-las
separadamente para um determinado recurso, faça FlowSchemas separados que correspondam ao
verbos mutantes e não mutantes, respectivamente.</div><p>Quando o volume de solicitações de entrada atribuídas a um único
PriorityLevelConfiguration é maior do que o permitido por seu nível de simultaneidade, o
O campo <code>type</code> de sua especificação determina o que acontecerá com solicitações extras.
Um tipo de 'Reject' significa que o excesso de tráfego será imediatamente rejeitado com
um erro HTTP 429 (Too Many Requests). Um tipo de <code>Queue</code> significa que as solicitações
acima do limite será enfileirado, com as técnicas de
<em>shuffle sharding</em> e <em>fair queuing</em> usadas
para equilibrar o progresso entre os fluxos de solicitação.</p><p>A configuração de enfileiramento permite ajustar o algoritmo de <em>fair queuing</em> para um
nível de prioridade. Os detalhes do algoritmo podem ser lidos no
<a href=#whats-next>proposta de melhoria</a>, mas resumindo:</p><ul><li><p>Aumentar as 'filas' reduz a taxa de colisões entre diferentes fluxos,
o custo do aumento do uso de memória. Um valor de 1 aqui efetivamente desabilita a
lógica de <em>fair queuing</em>, mas ainda permite que as solicitações sejam enfileiradas.</p></li><li><p>Aumentar o <code>queueLengthLimit</code> permite que tráfegos maiores sejam
sustentados sem deixar de lado nenhum pedido, ao custo de aumento
latência e uso de memória.</p></li><li><p>Alterar <code>handSize</code> permite ajustar a probabilidade de colisões entre
fluxos diferentes e a simultaneidade geral disponível para um único fluxo em um
situação de sobrecarga.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Um 'handSize' maior torna menos provável que dois fluxos individuais colidam
(e, portanto, um bloqueie a solicitação do outro), mas é mais provável que
um pequeno número de fluxos pode dominar o apiserver. Um <code>handSize</code> maior também
aumenta potencialmente a quantidade de latência que um único fluxo de alto tráfego
pode causar. O número máximo de solicitações enfileiradas possíveis de um
fluxo único é <code>handSize * queueLengthLimit</code>.</div></li></ul><p>A seguir está uma tabela mostrando uma coleção interessante de configurações do
<em>shuffle sharding</em>, mostrando para cada uma a probabilidade de que um
determinado rato (fluxo de baixa intensidade) é esmagado pelos elefantes (fluxo de alta intensidade) para
uma coleção ilustrativa de números de elefantes. Veja
<a href=https://play.golang.org/p/Gi0PLgVHiUg>https://play.golang.org/p/Gi0PLgVHiUg</a> , que calcula esta tabela.</p><table><caption style=display:none>Example Shuffle Sharding Configurations</caption><thead><tr><th>HandSize</th><th>Filas</th><th>1 elefante</th><th>4 elefantes</th><th>16 elefantes</th></tr></thead><tbody><tr><td>12</td><td>32</td><td>4.428838398950118e-09</td><td>0.11431348830099144</td><td>0.9935089607656024</td></tr><tr><td>10</td><td>32</td><td>1.550093439632541e-08</td><td>0.0626479840223545</td><td>0.9753101519027554</td></tr><tr><td>10</td><td>64</td><td>6.601827268370426e-12</td><td>0.00045571320990370776</td><td>0.49999929150089345</td></tr><tr><td>9</td><td>64</td><td>3.6310049976037345e-11</td><td>0.00045501212304112273</td><td>0.4282314876454858</td></tr><tr><td>8</td><td>64</td><td>2.25929199850899e-10</td><td>0.0004886697053040446</td><td>0.35935114681123076</td></tr><tr><td>8</td><td>128</td><td>6.994461389026097e-13</td><td>3.4055790161620863e-06</td><td>0.02746173137155063</td></tr><tr><td>7</td><td>128</td><td>1.0579122850901972e-11</td><td>6.960839379258192e-06</td><td>0.02406157386340147</td></tr><tr><td>7</td><td>256</td><td>7.597695465552631e-14</td><td>6.728547142019406e-08</td><td>0.0006709661542533682</td></tr><tr><td>6</td><td>256</td><td>2.7134626662687968e-12</td><td>2.9516464018476436e-07</td><td>0.0008895654642000348</td></tr><tr><td>6</td><td>512</td><td>4.116062922897309e-14</td><td>4.982983350480894e-09</td><td>2.26025764343413e-05</td></tr><tr><td>6</td><td>1024</td><td>6.337324016514285e-16</td><td>8.09060164312957e-11</td><td>4.517408062903668e-07</td></tr></tbody></table><h3 id=flowschema>FlowSchema</h3><p>Um FlowSchema corresponde a algumas solicitações de entrada e as atribui a um
nível de prioridade. Cada solicitação de entrada é testada em relação a cada
FlowSchema, por sua vez, começando com aqueles com valores numericamente mais baixos ---
que consideramos ser o logicamente mais alto --- <code>matchingPrecedence</code> e
trabalhando adiante. A primeira correspondência ganha.</p><div class="alert alert-warning caution callout" role=alert><strong>Cuidado:</strong> Somente o primeiro FlowSchema correspondente para uma determinada solicitação é importante. Se vários
FlowSchemas correspondem a uma única solicitação de entrada, ela será atribuída com base na
com o maior em <code>matchingPrecedence</code>. Se vários FlowSchemas com igual
<code>matchingPrecedence</code> corresponde ao mesmo pedido, aquele com menor
<code>name</code> lexicográfico vencerá, mas é melhor não confiar nisso e, em vez disso,
certifique-se de que dois FlowSchemas não tenham o mesmo <code>matchingPrecedence</code>.</div><p>Um FlowSchema corresponde a uma determinada solicitação se pelo menos uma de suas <code>regras</code>
são correspondidas. Uma regra corresponde se pelo menos um de seus <code>assuntos</code> <em>e</em> pelo menos
uma de suas <code>resourceRules</code> ou <code>nonResourceRules</code> (dependendo se a
solicitação de entrada é para um recurso ou URL de não-recurso) corresponde à solicitação.</p><p>Para o campo <code>name</code> em assuntos, e os campos <code>verbs</code>, <code>apiGroups</code>, <code>resources</code>,
<code>namespaces</code> e <code>nonResourceURLs</code> de regras de recursos e não recursos,
o <em>wildcard</em> <code>*</code> pode ser especificado para corresponder a todos os valores do campo fornecido,
efetivamente removendo-o de consideração.</p><p>O <code>distinguisherMethod.type</code> de um FlowSchema determina como as solicitações correspondentes a esse
esquema será separado em fluxos. Pode ser
ou <code>ByUser</code>, caso em que um usuário solicitante não poderá ser bloqueado por outros,
ou <code>ByNamespace</code>, caso em que solicitações de recursos
em um namespace não será capaz de privar os pedidos de recursos em outros
namespaces de capacidade, ou pode estar em branco (ou <code>distinguisherMethod</code> pode ser
omitido inteiramente), caso em que todas as solicitações correspondidas por este FlowSchema serão
considerados parte de um único fluxo. A escolha correta para um determinado FlowSchema
depende do recurso e do seu ambiente específico.</p><h2 id=padrões>Padrões</h2><p>Cada kube-apiserver mantém dois tipos de objetos de configuração APF:
obrigatória e sugerida.</p><h3 id=objetos-de-configuração-obrigatórios>Objetos de configuração obrigatórios</h3><p>Os quatro objetos de configuração obrigatórios refletem no
comportamento do <em>guardrail</em> embutido. Este é o comportamento que os servidores tinham antes
desses objetos existirem e, quando esses objetos existem, suas especificações refletem
esse comportamento. Os quatro objetos obrigatórios são os seguintes.</p><ul><li><p>O nível de prioridade obrigatório <code>exempt</code> é usado para solicitações que são
não sujeito a controle de fluxo: eles sempre serão despachados
imediatamente. O FlowSchema obrigatório <code>exempt</code> classifica todos
solicitações do grupo <code>system:masters</code> para este nível de prioridade.
Você pode definir outros FlowSchemas que direcionam outras solicitações
a este nível de prioridade, se apropriado.</p></li><li><p>O nível de prioridade obrigatório <code>catch-all</code> é usado em combinação com
o FlowSchema <code>catch-all</code> obrigatório para garantir que todas as solicitações
recebam algum tipo de classificação. Normalmente você não deve confiar
nesta configuração catch-all, e deve criar seu próprio FlowSchema catch-all
e PriorityLevelConfiguration (ou use o
nível de prioridade <code>global-default</code> que é instalado por padrão) como
apropriado. Como não se espera que seja usado normalmente, o
o nível de prioridade obrigatório <code>catch-all</code> tem uma simultaneidade muito pequena
compartilha e não enfileira solicitações.</p></li></ul><h3 id=objetos-de-configuração-sugeridos>Objetos de configuração sugeridos</h3><p>Os FlowSchemas e PriorityLevelConfigurations sugeridos constituem uma
configuração padrão razoável. Você pode modificá-los e/ou criar
objetos de configuração adicionais, se desejar. Se o seu cluster tiver a
probabilidade de experimentar carga pesada, então você deve considerar qual
configuração funcionará melhor.</p><p>A configuração sugerida agrupa as solicitações em seis níveis de prioridade:</p><ul><li><p>O nível de prioridade <code>node-high</code> é para atualizações de integridade dos nós.</p></li><li><p>O nível de prioridade <code>system</code> é para solicitações não relacionadas à integridade do
grupo <code>system:nodes</code>, ou seja, Kubelets, que deve ser capaz de contatar
o servidor de API para que as cargas de trabalho possam ser agendadas
eles.</p></li><li><p>O nível de prioridade <code>leader-election</code> é para solicitações de eleição de líder de
controladores embutidos (em particular, solicitações para <code>endpoints</code>, <code>configmaps</code>,
ou <code>leases</code> vindo do <code>system:kube-controller-manager</code> ou
usuários <code>system:kube-scheduler</code> e contas de serviço no namespace <code>kube-system</code>).
Estes são importantes para isolar de outro tráfego porque as falhas
na eleição do líder fazem com que seus controladores falhem e reiniciem, o que por sua vez
causa tráfego mais caro à medida que os novos controladores sincronizam seus informantes.</p></li><li><p>O nível de prioridade <code>workload-high</code> é para outras solicitações de controladores built-in.</p></li><li><p>O nível de prioridade <code>workload-low</code> é para solicitações de qualquer outra conta de serviço,
que normalmente incluirá todas as solicitações de controladores em execução
<em>Pods</em>.</p></li><li><p>O nível de prioridade <code>global-default</code> trata de todos os outros tráfegos, por exemplo,
comandos <code>kubectl</code> interativos executados por usuários não privilegiados.</p></li></ul><p>Os FlowSchemas sugeridos servem para direcionar as solicitações para os
níveis de prioridade acima, e não são enumerados aqui.</p><h3 id=manutenção-dos-objetos-de-configuração-obrigatórios-e-sugeridos>Manutenção dos Objetos de Configuração Obrigatórios e Sugeridos</h3><p>Cada <code>kube-apiserver</code> mantém independentemente os requisitos obrigatórios e
objetos de configuração sugeridos, usando comportamento inicial e periódico.
Assim, em uma situação com uma mistura de servidores de diferentes versões
pode haver <em>thrashing</em> desde que servidores diferentes tenham
opiniões sobre o conteúdo adequado desses objetos.</p><p>Para os objetos de configuração obrigatórios, a manutenção consiste em
garantir que o objeto existe e, se existir, tem a especificação adequada.
O servidor se recusa a permitir uma criação ou atualização com uma especificação que é
inconsistente com o comportamento do guarda-corpo do servidor.</p><p>A manutenção de objetos de configuração sugeridos é projetada para permitir
que suas especificações sejam substituídas. A exclusão, por outro lado, não é
respeitada: a manutenção restaurará o objeto. Se você não quer um
objeto de configuração sugerido, então você precisa mantê-lo por perto, mas defina
sua especificação para ter consequências mínimas. Manutenção de objetos
sugeridos também é projetada para suportar a migração automática quando uma nova
versão do <code>kube-apiserver</code> é lançada, embora potencialmente com
<em>thrashing</em> enquanto há uma população mista de servidores.</p><p>A manutenção de um objeto de configuração sugerido consiste em cria-lo
--- com a especificação sugerida pelo servidor --- se o objeto não
existir. OTOH, se o objeto já existir, o comportamento de manutenção
depende se os <code>kube-apiservers</code> ou os usuários controlam o
objeto. No primeiro caso, o servidor garante que a especificação do objeto
é o que o servidor sugere; no último caso, a especificação é deixada
sozinho.</p><p>A questão de quem controla o objeto é respondida primeiro olhando
para uma anotação com a chave <code>apf.kubernetes.io/autoupdate-spec</code>. Se
existe tal anotação e seu valor é <code>true</code> então o
kube-apiservers controlam o objeto. Se houver tal anotação
e seu valor for <code>false</code>, os usuários controlarão o objeto. Se
nenhuma dessas condições é satisfeita entaão a <code>metadata.generation</code> do
objeto é consultado. Se for 1, o kube-apiservers controla
o objeto. Caso contrário, os usuários controlam o objeto. Essas regras foram
introduzido na versão 1.22 e sua consideração de
<code>metadata.generation</code> é para migrar do mais simples
comportamento anterior. Usuários que desejam controlar um objeto de configuração sugerido
deve definir sua anotação <code>apf.kubernetes.io/autoupdate-spec</code>
para 'falso'.</p><p>A manutenção de um objeto de configuração obrigatório ou sugerido também
inclui garantir que ele tenha uma anotação <code>apf.kubernetes.io/autoupdate-spec</code>
que reflete com precisão se os kube-apiservers
controlam o objeto.</p><p>A manutenção também inclui a exclusão de objetos que não são obrigatórios
nem sugeridos, mas são anotados
<code>apf.kubernetes.io/autoupdate-spec=true</code>.</p><h2 id=isenção-de-simultaneidade-da-verificação-de-integridade>Isenção de simultaneidade da verificação de integridade</h2><p>A configuração sugerida não dá nenhum tratamento especial a checagem de saúde das requisições
verifique solicitações em kube-apiservers de seus kubelets locais --- que
tendem a usar a porta segura, mas não fornecem credenciais. Com o
configuração sugerida, essas solicitações são atribuídas ao <code>global-default</code>
FlowSchema e o nível de prioridade "global-default" correspondente,
onde outro tráfego pode bloqueá-los.</p><p>Se você adicionar o seguinte FlowSchema adicional, isso isenta aquelas
solicitações de limitação de taxa.</p><div class="alert alert-warning caution callout" role=alert><strong>Cuidado:</strong> Fazer essa alteração também permite que qualquer parte hostil envie
solicitações de verificação de integridade que correspondam a este FlowSchema, em qualquer volume.
Se você tiver um filtro de tráfego da Web ou outro mecanismo de segurança externa semelhante
para proteger o servidor de API do seu cluster do trafego geral de internet,
você pode configurar regras para bloquear qualquer solicitação de verificação de integridade
que se originam de fora do seu cluster.</div><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/pt-br/examples/priority-and-fairness/health-for-strangers.yaml download=priority-and-fairness/health-for-strangers.yaml><code>priority-and-fairness/health-for-strangers.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("priority-and-fairness-health-for-strangers-yaml")' title="Copy priority-and-fairness/health-for-strangers.yaml to clipboard"></img></div><div class=includecode id=priority-and-fairness-health-for-strangers-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>flowcontrol.apiserver.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>FlowSchema<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>health-for-strangers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>matchingPrecedence</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityLevelConfiguration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>exempt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>nonResourceRules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>nonResourceURLs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/healthz&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/livez&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/readyz&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Group<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>group</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:unauthenticated<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h2 id=diagnóstico>Diagnóstico</h2><p>Cada resposta HTTP de um servidor da API com o recurso de prioridade e justiça
ativado tem dois cabeçalhos extras: <code>X-Kubernetes-PF-FlowSchema-UID</code> e
<code>X-Kubernetes-PF-PriorityLevel-UID</code>, observando o esquema de fluxo que corresponde à solicitação
e o nível de prioridade ao qual foi atribuído, respectivamente. Os nomes dos objetos da API
não são incluídos nesses cabeçalhos caso o usuário solicitante não
tenha permissão para visualizá-los, então ao depurar você pode usar um comando como</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get flowschemas -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span><span style=display:flex><span>kubectl get prioritylevelconfigurations -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span></code></pre></div><p>para obter um mapeamento de UIDs de nomes para FlowSchemas e
PriorityLevelConfigurations.</p><h2 id=observabilidade>Observabilidade</h2><h3 id=metricas>Metricas</h3><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Nas versões do Kubernetes anteriores à v1.20, as <em>labels</em> <code>flow_schema</code> e
<code>priority_level</code> foram nomeados de forma inconsistente como <code>flowSchema</code> e <code>priorityLevel</code>,
respectivamente. Se você estiver executando versões do Kubernetes v1.19 ou anteriores, você
deve consultar a documentação da sua versão.</div><p>Quando você ativa o APF, o kube-apiserver
exporta métricas adicionais. Monitorá-los pode ajudá-lo a determinar se a sua
configuração está limitando indevidamente o tráfego importante, ou encontrar
cargas de trabalho mal comportadas que podem estar prejudicando a integridade do sistema.</p><ul><li><p><code>apiserver_flowcontrol_rejected_requests_total</code> é um vetor de contador
(cumulativo desde o início do servidor) de solicitações que foram rejeitadas,
dividido pelos rótulos <code>flow_schema</code> (indicando aquele que
correspondeu ao pedido), <code>priority_level</code> (indicando aquele para o qual
a solicitação foi atribuída) e <code>reason</code>. A <em>label</em> <code>reason</code> pode
ter um dos seguintes valores:</p><ul><li><code>queue-full</code>, indicando que muitos pedidos já foram enfileirados,</li><li><code>concurrency-limit</code>, indicando que o
PriorityLevelConfiguration está configurado para rejeitar em vez de
enfileirar solicitações em excesso ou</li><li><code>time-out</code>, indicando que a solicitação ainda estava na fila
quando seu limite de tempo de fila expirou.</li></ul></li><li><p><code>apiserver_flowcontrol_dispatched_requests_total</code> é um vetor contador
(cumulativo desde o início do servidor) de solicitações que começaram
executando, dividido pelos rótulos <code>flow_schema</code> (indicando o
um que corresponda à solicitação) e <code>priority_level</code> (indicando o
aquele ao qual o pedido foi atribuído).</p></li><li><p><code>apiserver_current_inqueue_requests</code> é um vetor de medidor de
limites máximos do número de solicitações enfileiradas, agrupadas por uma
<em>label</em> chamado <code>request_kind</code> cujo valor é <code>mutating</code> ou <code>readOnly</code>.
Essas marcas d'água altas descrevem o maior número visto em uma
segunda janela concluída recentemente. Estes complementam o mais antigo
vetor medidor <code>apiserver_current_inflight_requests</code> que contém o
marca d'água alta da última janela de número de solicitações sendo ativamente
servido.</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_request_count_samples</code> é um
vetor de histograma de observações do número atual de
solicitações, divididas pelos rótulos <code>phase</code> (que assume o
valores <code>waiting</code> e <code>executing</code>) e <code>request_kind</code> (que assume
os valores <code>mutating</code> e <code>readOnly</code>). As observações são feitas
periodicamente a uma taxa elevada.</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_request_count_watermarks</code> é um
vetor de histograma de marcas d'água altas ou baixas do número de
solicitações divididas pelos rótulos <code>phase</code> (que assume o
valores <code>waiting</code> e <code>executing</code>) e <code>request_kind</code> (que assume
os valores <code>mutating</code> e <code>readOnly</code>); o rótulo <code>mark</code> assume
valores <code>high</code> e <code>low</code>. As marcas d'água são acumuladas ao longo de
janelas delimitadas pelos tempos em que uma observação foi adicionada a
<code>apiserver_flowcontrol_read_vs_write_request_count_samples</code>. Esses
marcas d'água mostram o intervalo de valores que ocorreram entre as amostras.</p></li><li><p><code>apiserver_flowcontrol_current_inqueue_requests</code> é um vetor de medidor
mantendo o número instantâneo de solicitações enfileiradas (não em execução),
dividido pelos rótulos <code>priority_level</code> e <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_current_executing_requests</code> é um vetor de medidor
segurando o número instantâneo de execução (não esperando em uma
queue), divididas pelos rótulos <code>priority_level</code> e
<code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_request_concurrency_in_use</code> é um vetor de medidor
ocupando o número instantâneo de assentos ocupados, diferenciados pelas
<em>labels</em> <code>priority_level</code> e <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_count_samples</code> é um
vetor de histograma de observações do número atual de
solicitações divididas pelas <em>labels</em> <code>phase</code> (que assume o
valores <code>waiting</code> e <code>executing</code>) e <code>priority_level</code>. Cada
histograma obtém observações feitas periodicamente, até a última
atividade do tipo relevante. As observações são feitas em nota alta.</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_count_watermarks</code> é um
vetor de histograma de marcas d'água altas ou baixas do número de
solicitações divididas pelas <em>labels</em> <code>phase</code> (que assume o
valores <code>waiting</code> e <code>executing</code>) e <code>priority_level</code>; a <em>label</em>
<code>mark</code> assume valores <code>high</code> e <code>low</code>. As marcas da água são
acumulada em janelas delimitadas pelos tempos em que uma observação
foi adicionado a
<code>apiserver_flowcontrol_priority_level_request_count_samples</code>. Esses
marcas d'água mostram o intervalo de valores que ocorreram entre as amostras.</p></li><li><p><code>apiserver_flowcontrol_request_queue_length_after_enqueue</code> é um
vetor de histograma de comprimentos de fila para as filas, dividido pelas
<em>labels</em> <code>priority_level</code> e <code>flow_schema</code>, conforme mostrado pelas
solicitações enfileiradas. Cada solicitação enfileirada contribui com uma
amostra para seu histograma, relatando o comprimento da fila imediatamente
depois que o pedido foi adicionado. Observe que isso produz diferentes
estatísticas do que uma pesquisa imparcial faria.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Um valor discrepante em um histograma aqui significa que é provável que um único fluxo
(ou seja, solicitações de um usuário ou de um namespace, dependendo da
configuração) está inundando o servidor de API e sendo limitado. Por contraste,
se o histograma de um nível de prioridade mostrar que todas as filas para essa prioridade
são mais longos do que os de outros níveis de prioridade, pode ser apropriado
aumentar os compartilhamentos de simultaneidade desse PriorityLevelConfiguration.</div></li><li><p><code>apiserver_flowcontrol_request_concurrency_limit</code> é um vetor de medidor
mantendo o limite de simultaneidade calculado (com base no limite total de simultaneidade do servidor da API
e na simultaneidade de PriorityLevelConfigurations share), divididos pela <em>label</em> <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_request_wait_duration_seconds</code> é um vetor de histograma
de quanto tempo as solicitações ficaram na fila, divididas pelas <em>labels</em>
<code>flow_schema</code> (indicando qual corresponde à solicitação),
<code>priority_level</code> (indicando aquele para o qual o pedido foi
atribuído) e <code>execute</code> (indicando se a solicitação foi iniciada
executando).</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Como cada FlowSchema sempre atribui solicitações a um único
PriorityLevelConfiguration, você pode adicionar os histogramas para todos os
FlowSchemas para um nível de prioridade para obter o histograma efetivo para
solicitações atribuídas a esse nível de prioridade.</div></li><li><p><code>apiserver_flowcontrol_request_execution_seconds</code> é um vetor de histograma
de quanto tempo as solicitações levaram para realmente serem executadas, divididas pelas
<em>labels</em> <code>flow_schema</code> (indicando qual corresponde à solicitação)
e <code>priority_level</code> (indicando aquele para o qual o pedido foi
atribuído).</p></li></ul><h3 id=debug-endpoints><em>Debug endpoints</em></h3><p>Quando você ativa A APF, o <code>kube-apiserver</code>
serve os seguintes caminhos adicionais em suas portas HTTP[S].</p><ul><li><p><code>/debug/api_priority_and_fairness/dump_priority_levels</code> - uma lista de
todos os níveis de prioridade e o estado atual de cada um. Você pode buscar assim:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_priority_levels
</span></span></code></pre></div><p>A saída é parecido com isto:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, ActiveQueues, IsIdle, IsQuiescing, WaitingRequests, ExecutingRequests,
workload-low,      0,            true,   false,       0,               0,
global-default,    0,            true,   false,       0,               0,
exempt,            &lt;none&gt;,       &lt;none&gt;, &lt;none&gt;,      &lt;none&gt;,          &lt;none&gt;,
catch-all,         0,            true,   false,       0,               0,
system,            0,            true,   false,       0,               0,
leader-election,   0,            true,   false,       0,               0,
workload-high,     0,            true,   false,       0,               0,
</code></pre></li><li><p><code>/debug/api_priority_and_fairness/dump_queues</code> - uma listagem de todas as
filas e seu estado atual. Você pode buscar assim:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_queues
</span></span></code></pre></div><p>A saída é parecido com isto:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, Index,  PendingRequests, ExecutingRequests, VirtualStart,
workload-high,     0,      0,               0,                 0.0000,
workload-high,     1,      0,               0,                 0.0000,
workload-high,     2,      0,               0,                 0.0000,
...
leader-election,   14,     0,               0,                 0.0000,
leader-election,   15,     0,               0,                 0.0000,
</code></pre></li><li><p><code>/debug/api_priority_and_fairness/dump_requests</code> - uma lista de todos os pedidos
que estão atualmente esperando em uma fila. Você pode buscar assim:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_requests
</span></span></code></pre></div><p>A saída é parecido com isto:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,
exempt,            &lt;none&gt;,         &lt;none&gt;,     &lt;none&gt;,              &lt;none&gt;,                &lt;none&gt;,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:26:57.179170694Z,
</code></pre><p>Além das solicitações enfileiradas, a saída inclui uma linha fantasma
para cada nível de prioridade isento de limitação.</p><p>Você pode obter uma lista mais detalhada com um comando como este:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw <span style=color:#b44>&#39;/debug/api_priority_and_fairness/dump_requests?includeRequestDetails=1&#39;</span>
</span></span></code></pre></div><p>A saída é parecido com isto:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,                     UserName,              Verb,   APIPath,                                                     Namespace, Name,   APIVersion, Resource, SubResource,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:31:03.583823404Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
system,            system-nodes,   12,         1,                   system:node:127.0.0.1, 2020-07-23T15:31:03.594555947Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
</code></pre></li></ul><h2 id=próximos-passos>Próximos passos</h2><p>Para obter informações básicas sobre detalhes de design para prioridade e justiça da API, consulte
a <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness>proposta de aprimoramento</a>.
Você pode fazer sugestões e solicitações de recursos por meio do <a href=https://github.com/kubernetes/community/tree/master/sig-api-machinery>SIG API Machinery</a>
ou do <a href=https://kubernetes.slack.com/messages/api-priority-and-fairness>canal do slack</a>.</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/pt-br/docs/home/>Home</a>
<a class=text-white href=/pt-br/blog/>Blog</a>
<a class=text-white href=/pt-br/partners/>Parceiros</a>
<a class=text-white href=/pt-br/community/>Comunidade</a>
<a class=text-white href=/pt-br/case-studies/>Casos de estudo</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 Os autores do Kubernetes | Documentação Distribuída sob <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 A Fundação Linux &reg;. Todos os direitos reservados. A Linux Foundation tem marcas registradas e usa marcas registradas. Para uma lista de marcas registradas da The Linux Foundation, por favor, veja nossa <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Página de uso de marca registrada</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>