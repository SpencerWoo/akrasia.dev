<!doctype html><html lang=ja class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/setup/production-environment/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/setup/production-environment/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/production-environment/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/setup/production-environment/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/production-environment/><link rel=alternate hreflang=hi href=https://kubernetes.io/hi/docs/setup/production-environment/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/production-environment/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/ja/docs/setup/production-environment/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>プロダクション環境 | Kubernetes</title><meta property="og:title" content="プロダクション環境"><meta property="og:description" content="プロダクショングレードのコンテナ管理基盤"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/ja/docs/setup/production-environment/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="プロダクション環境"><meta itemprop=description content="プロダクショングレードのコンテナ管理基盤"><meta name=twitter:card content="summary"><meta name=twitter:title content="プロダクション環境"><meta name=twitter:description content="プロダクショングレードのコンテナ管理基盤"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/ja/docs/setup/production-environment/"><meta property="og:title" content="プロダクション環境"><meta name=twitter:title content="プロダクション環境"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/ja/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/ja/docs/>ドキュメント</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/blog/>Blogs</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/training/>トレーニング</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/partners/>パートナー</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/community/>コミュニティ</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/case-studies/>ケーススタディ</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>バージョン</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/ja/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/ja/docs/setup/production-environment/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/ja/docs/setup/production-environment/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/ja/docs/setup/production-environment/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/ja/docs/setup/production-environment/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/ja/docs/setup/production-environment/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>日本語 (Japanese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/setup/production-environment/>English</a>
<a class=dropdown-item href=/zh-cn/docs/setup/production-environment/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/setup/production-environment/>한국어 (Korean)</a>
<a class=dropdown-item href=/fr/docs/setup/production-environment/>Français (French)</a>
<a class=dropdown-item href=/id/docs/setup/production-environment/>Bahasa Indonesia</a>
<a class=dropdown-item href=/hi/docs/setup/production-environment/>हिन्दी (Hindi)</a>
<a class=dropdown-item href=/uk/docs/setup/production-environment/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>これは、このセクションの複数ページの印刷可能なビューです。
<a href=# onclick="return print(),!1">印刷するには、ここをクリックしてください</a>.</p><p><a href=/ja/docs/setup/production-environment/>このページの通常のビューに戻る</a>.</p></div><h1 class=title>プロダクション環境</h1><ul><li>1: <a href=#pg-a77d3feb6e6d9978f32fa14622642e9a>CRIのインストール</a></li><li>2: <a href=#pg-00e1646f68aeb89f9722cf6f6cfcad94>Kubernetesをデプロイツールでインストールする</a></li><ul><li>2.1: <a href=#pg-a16f59f325a17cdeed324d5c889f7f73>kubeadmを使ってクラスターを構築する</a></li><ul><li>2.1.1: <a href=#pg-29e59491dd6118b23072dfe9ebb93323>kubeadmのインストール</a></li><li>2.1.2: <a href=#pg-c3689df4b0c61a998e79d91a865aa244>kubeadmのトラブルシューティング</a></li><li>2.1.3: <a href=#pg-134ed1f6142a98e6ac681a1ba4920e53>kubeadmを使用したクラスターの作成</a></li><li>2.1.4: <a href=#pg-4c656c5eda3e1c06ad1aedebdc04a211>kubeadmを使ったコントロールプレーンの設定のカスタマイズ</a></li><li>2.1.5: <a href=#pg-015edbc7cc688d31b1d1edce7c186135>高可用性トポロジーのためのオプション</a></li><li>2.1.6: <a href=#pg-3941d5c3409342219bf7e03128b8ecb6>kubeadmを使用した高可用性クラスターの作成</a></li><li>2.1.7: <a href=#pg-8160424c22d24f7d2d63c521e107dbf8>kubeadmを使用した高可用性etcdクラスターの作成</a></li><li>2.1.8: <a href=#pg-07709e71de6b4ac2573041c31213dbeb>kubeadmを使用したクラスター内の各kubeletの設定</a></li><li>2.1.9: <a href=#pg-ed857e09999827b013ee9062dc9c59bb>コントロールプレーンをセルフホストするようにkubernetesクラスターを構成する</a></li></ul><li>2.2: <a href=#pg-478acca1934b6d89a0bc00fb25bfe5b6>kopsを使ったAWS上でのKubernetesのインストール</a></li><li>2.3: <a href=#pg-f8b4964187fe973644e06ee629eff1de>kubesprayを使ったオンプレミス/クラウドプロバイダへのKubernetesのインストール</a></li></ul><li>3: <a href=#pg-e2eb3029b668b1713d0dc8bea296ba9c>ターンキークラウドソリューション</a></li><ul><li>3.1: <a href=#pg-20c20ee4c93be1062165131aff27cff5>Alibaba CloudでKubernetesを動かす</a></li><li>3.2: <a href=#pg-4db51d554f14646b6af380916c827aa0>AWS EC2上でKubernetesを動かす</a></li><li>3.3: <a href=#pg-66b96ad245efc180060593fa01df4dde>Azure 上で Kubernetes を動かす</a></li><li>3.4: <a href=#pg-bed0528b13f56089ee19400212edf55d>Google Compute Engine上でKubernetesを動かす</a></li><li>3.5: <a href=#pg-ba2691e5872ead2141b1e8fd7d29ee21>IBM Cloud Privateを使ってマルチクラウドでKubernetesを動かす</a></li></ul><li>4: <a href=#pg-1b751cdddc397a65edb7bcf703bc0414>オンプレミスVM</a></li><ul><li>4.1: <a href=#pg-83a1f15a6fa96fcdca228c4a70f392b3>Cloudstack</a></li><li>4.2: <a href=#pg-89a44f01ba43ac7fda5e20024b9d6cea>DC/OS上のKubernetes</a></li><li>4.3: <a href=#pg-7f608a89334fa86add74d0d6ba0beedf>oVirt</a></li></ul><li>5: <a href=#pg-acce7e24090fea04715a7a516ba3e69b>Windows in Kubernetes</a></li><ul><li>5.1: <a href=#pg-a307d413f1f7430fced233023087e2a1>KubernetesのWindowsサポート概要</a></li><li>5.2: <a href=#pg-3a51e66c5de55f9093a8dc55742006d3>KubernetesでWindowsコンテナをスケジュールするためのガイド</a></li></ul></ul><div class=content></div></div><div class=td-content><h1 id=pg-a77d3feb6e6d9978f32fa14622642e9a>1 - CRIのインストール</h1><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.6 [stable]</code></div>Podのコンテナを実行するために、Kubernetesはコンテナランタイムを使用します。
様々なランタイムのインストール手順は次のとおりです。</p><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong><p>コンテナ実行時にruncがシステムファイルディスクリプターを扱える脆弱性が見つかりました。
悪意のあるコンテナがこの脆弱性を利用してruncのバイナリを上書きし、
コンテナホストシステム上で任意のコマンドを実行する可能性があります。</p><p>この問題の更なる情報は<a href=https://access.redhat.com/security/cve/cve-2019-5736>CVE-2019-5736</a>を参照してください。</p></div><h3 id=適用性>適用性</h3><div class="alert alert-info note callout" role=alert><strong>備考:</strong> このドキュメントはLinuxにCRIをインストールするユーザーのために書かれています。
他のオペレーティングシステムの場合、プラットフォーム固有のドキュメントを見つけてください。</div><p>このガイドでは全てのコマンドを <code>root</code> で実行します。
例として、コマンドに <code>sudo</code> を付けたり、 <code>root</code> になってそのユーザーでコマンドを実行します。</p><h3 id=cgroupドライバー>Cgroupドライバー</h3><p>systemdがLinuxのディストリビューションのinitシステムとして選択されている場合、
initプロセスが作成され、rootコントロールグループ(<code>cgroup</code>)を使い、cgroupマネージャーとして行動します。
systemdはcgroupと密接に統合されており、プロセスごとにcgroupを割り当てます。
<code>cgroupfs</code> を使うように、あなたのコンテナランライムとkubeletを設定することができます。
systemdと一緒に <code>cgroupfs</code> を使用するということは、2つの異なるcgroupマネージャーがあることを意味します。</p><p>コントロールグループはプロセスに割り当てられるリソースを制御するために使用されます。
単一のcgroupマネージャーは、割り当てられているリソースのビューを単純化し、
デフォルトでは使用可能なリソースと使用中のリソースについてより一貫性のあるビューになります。
2つのマネージャーがある場合、それらのリソースについて2つのビューが得られます。
kubeletとDockerに <code>cgroupfs</code> を使用し、ノード上で実行されている残りのプロセスに <code>systemd</code> を使用するように設定されたノードが、
リソース圧迫下で不安定になる場合があります。</p><p>コンテナランタイムとkubeletがcgroupドライバーとしてsystemdを使用するように設定を変更することでシステムは安定します。
以下のDocker設定の <code>native.cgroupdriver=systemd</code> オプションに注意してください。</p><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> すでにクラスターに組み込まれているノードのcgroupドライバーを変更することは非常におすすめしません。
kubeletが一方のcgroupドライバーを使用してPodを作成した場合、コンテナランタイムを別のもう一方のcgroupドライバーに変更すると、そのような既存のPodのPodサンドボックスを再作成しようとするとエラーが発生する可能性があります。
kubeletを再起動しても問題は解決しないでしょう。
ワークロードからノードを縮退させ、クラスターから削除して再び組み込むことを推奨します。</div><h2 id=docker>Docker</h2><p>それぞれのマシンに対してDockerをインストールします。
バージョン19.03.11が推奨されていますが、1.13.1、17.03、17.06、17.09、18.06、18.09についても動作が確認されています。
Kubernetesのリリースノートにある、Dockerの動作確認済み最新バージョンについてもご確認ください。</p><p>システムへDockerをインストールするには、次のコマンドを実行します。</p><ul class="nav nav-tabs" id=tab-cri-docker-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-cri-docker-installation-0 role=tab aria-controls=tab-cri-docker-installation-0 aria-selected=true>Ubuntu 16.04+</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-docker-installation-1 role=tab aria-controls=tab-cri-docker-installation-1>CentOS/RHEL 7.4+</a></li></ul><div class=tab-content id=tab-cri-docker-installation><div id=tab-cri-docker-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-cri-docker-installation-0><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># (Install Docker CE)</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>## リポジトリをセットアップ</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>### HTTPS越しのリポジトリの使用をaptに許可するために、パッケージをインストール</span>
</span></span><span style=display:flex><span>apt-get update <span style=color:#666>&amp;&amp;</span> apt-get install -y <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  apt-transport-https ca-certificates curl software-properties-common gnupg2
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># Docker公式のGPG鍵を追加:</span>
</span></span><span style=display:flex><span>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># Dockerのaptレポジトリを追加:</span>
</span></span><span style=display:flex><span>add-apt-repository <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  <span style=color:#b44>&#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
</span></span></span><span style=display:flex><span><span style=color:#b44>  </span><span style=color:#a2f;font-weight:700>$(</span>lsb_release -cs<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44> \
</span></span></span><span style=display:flex><span><span style=color:#b44>  stable&#34;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># Docker CEのインストール</span>
</span></span><span style=display:flex><span>apt-get update <span style=color:#666>&amp;&amp;</span> apt-get install -y <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>containerd.io<span style=color:#666>=</span>1.2.13-2 <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>docker-ce<span style=color:#666>=</span>5:19.03.11~3-0~ubuntu-<span style=color:#a2f;font-weight:700>$(</span>lsb_release -cs<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>docker-ce-cli<span style=color:#666>=</span>5:19.03.11~3-0~ubuntu-<span style=color:#a2f;font-weight:700>$(</span>lsb_release -cs<span style=color:#a2f;font-weight:700>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># デーモンをセットアップ</span>
</span></span><span style=display:flex><span>cat &gt; /etc/docker/daemon.json <span style=color:#b44>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#b44>{
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;log-driver&#34;: &#34;json-file&#34;,
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;log-opts&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#b44>    &#34;max-size&#34;: &#34;100m&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>  },
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;storage-driver&#34;: &#34;overlay2&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>}
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>mkdir -p /etc/systemd/system/docker.service.d
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># dockerを再起動</span>
</span></span><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart docker
</span></span></code></pre></div></div><div id=tab-cri-docker-installation-1 class=tab-pane role=tabpanel aria-labelledby=tab-cri-docker-installation-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># (Docker CEのインストール)</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>## リポジトリをセットアップ</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>### 必要なパッケージのインストール</span>
</span></span><span style=display:flex><span>yum install -y yum-utils device-mapper-persistent-data lvm2
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic>## Dockerリポジトリの追加</span>
</span></span><span style=display:flex><span>yum-config-manager --add-repo <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  https://download.docker.com/linux/centos/docker-ce.repo
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic>## Docker CEのインストール</span>
</span></span><span style=display:flex><span>yum update -y <span style=color:#666>&amp;&amp;</span> yum install -y <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  containerd.io-1.2.13 <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  docker-ce-19.03.11 <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  docker-ce-cli-19.03.11
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic>## /etc/docker ディレクトリを作成</span>
</span></span><span style=display:flex><span>mkdir /etc/docker
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># デーモンをセットアップ</span>
</span></span><span style=display:flex><span>cat &gt; /etc/docker/daemon.json <span style=color:#b44>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#b44>{
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;log-driver&#34;: &#34;json-file&#34;,
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;log-opts&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#b44>    &#34;max-size&#34;: &#34;100m&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>  },
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;storage-driver&#34;: &#34;overlay2&#34;,
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;storage-opts&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#b44>    &#34;overlay2.override_kernel_check=true&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>  ]
</span></span></span><span style=display:flex><span><span style=color:#b44>}
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>mkdir -p /etc/systemd/system/docker.service.d
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># dockerを再起動</span>
</span></span><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart docker
</span></span></code></pre></div></div></div><p>ブート時にDockerサービスを開始させたい場合は、以下のコマンドを入力してください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo systemctl <span style=color:#a2f>enable</span> docker
</span></span></code></pre></div><p>詳細については、<a href=https://docs.docker.com/engine/installation/>Dockerの公式インストールガイド</a>を参照してください。</p><h2 id=cri-o>CRI-O</h2><p>このセクションでは、CRIランタイムとして<code>CRI-O</code>を利用するために必要な手順について説明します。</p><p>システムへCRI-Oをインストールするためには以下のコマンドを利用します:</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> CRI-OのメジャーとマイナーバージョンはKubernetesのメジャーとマイナーバージョンと一致しなければなりません。
詳細は<a href=https://github.com/cri-o/cri-o>CRI-O互換性表</a>を参照してください。</div><h3 id=事前準備>事前準備</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>modprobe overlay
</span></span><span style=display:flex><span>modprobe br_netfilter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 必要なカーネルパラメータの設定をします。これらの設定値は再起動後も永続化されます。</span>
</span></span><span style=display:flex><span>cat &gt; /etc/sysctl.d/99-kubernetes-cri.conf <span style=color:#b44>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-iptables  = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.ipv4.ip_forward                 = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sysctl --system
</span></span></code></pre></div><ul class="nav nav-tabs" id=tab-cri-cri-o-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-cri-cri-o-installation-0 role=tab aria-controls=tab-cri-cri-o-installation-0 aria-selected=true>Debian</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-1 role=tab aria-controls=tab-cri-cri-o-installation-1>Ubuntu</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-2 role=tab aria-controls=tab-cri-cri-o-installation-2>CentOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-3 role=tab aria-controls=tab-cri-cri-o-installation-3>openSUSE Tumbleweed</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-cri-o-installation-4 role=tab aria-controls=tab-cri-cri-o-installation-4>Fedora</a></li></ul><div class=tab-content id=tab-cri-cri-o-installation><div id=tab-cri-cri-o-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-cri-cri-o-installation-0><p><p>CRI-Oを以下のOSにインストールするには、環境変数$OSを以下の表の適切なフィールドに設定します。</p><table><thead><tr><th>Operating system</th><th>$OS</th></tr></thead><tbody><tr><td>Debian Unstable</td><td><code>Debian_Unstable</code></td></tr><tr><td>Debian Testing</td><td><code>Debian_Testing</code></td></tr></tbody></table><br>そして、`$VERSION`にKubernetesのバージョンに合わせたCRI-Oのバージョンを設定します。例えば、CRI-O 1.18をインストールしたい場合は、`VERSION=1.18` を設定します。インストールを特定のリリースに固定することができます。バージョン 1.18.3をインストールするには、`VERSION=1.18:1.18.3` を設定します。<br><p>以下を実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/</span><span style=color:#b8860b>$OS</span><span style=color:#b44>/ /&#34;</span> &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/</span><span style=color:#b8860b>$VERSION</span><span style=color:#b44>/</span><span style=color:#b8860b>$OS</span><span style=color:#b44>/ /&#34;</span> &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>.list
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>/<span style=color:#b8860b>$OS</span>/Release.key | apt-key add -
</span></span><span style=display:flex><span>curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style=color:#b8860b>$OS</span>/Release.key | apt-key add -
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>apt-get update
</span></span><span style=display:flex><span>apt-get install cri-o cri-o-runc
</span></span></code></pre></div></div><div id=tab-cri-cri-o-installation-1 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-1><p><p>CRI-Oを以下のOSにインストールするには、環境変数$OSを以下の表の適切なフィールドに設定します。</p><table><thead><tr><th>Operating system</th><th>$OS</th></tr></thead><tbody><tr><td>Ubuntu 20.04</td><td><code>xUbuntu_20.04</code></td></tr><tr><td>Ubuntu 19.10</td><td><code>xUbuntu_19.10</code></td></tr><tr><td>Ubuntu 19.04</td><td><code>xUbuntu_19.04</code></td></tr><tr><td>Ubuntu 18.04</td><td><code>xUbuntu_18.04</code></td></tr></tbody></table><br>次に、`$VERSION`をKubernetesのバージョンと一致するCRI-Oのバージョンに設定します。例えば、CRI-O 1.18をインストールしたい場合は、`VERSION=1.18` を設定します。インストールを特定のリリースに固定することができます。バージョン 1.18.3 をインストールするには、`VERSION=1.18:1.18.3` を設定します。<br><p>以下を実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/</span><span style=color:#b8860b>$OS</span><span style=color:#b44>/ /&#34;</span> &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/</span><span style=color:#b8860b>$VERSION</span><span style=color:#b44>/</span><span style=color:#b8860b>$OS</span><span style=color:#b44>/ /&#34;</span> &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>.list
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>curl -L https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>/<span style=color:#b8860b>$OS</span>/Release.key | apt-key add -
</span></span><span style=display:flex><span>curl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style=color:#b8860b>$OS</span>/Release.key | apt-key add -
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>apt-get update
</span></span><span style=display:flex><span>apt-get install cri-o cri-o-runc
</span></span></code></pre></div></div><div id=tab-cri-cri-o-installation-2 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-2><p><p>CRI-Oを以下のOSにインストールするには、環境変数$OSを以下の表の適切なフィールドに設定します。</p><table><thead><tr><th>Operating system</th><th>$OS</th></tr></thead><tbody><tr><td>Centos 8</td><td><code>CentOS_8</code></td></tr><tr><td>Centos 8 Stream</td><td><code>CentOS_8_Stream</code></td></tr><tr><td>Centos 7</td><td><code>CentOS_7</code></td></tr></tbody></table><br>次に、`$VERSION`をKubernetesのバージョンと一致するCRI-Oのバージョンに設定します。例えば、CRI-O 1.18 をインストールしたい場合は、`VERSION=1.18` を設定します。インストールを特定のリリースに固定することができます。バージョン 1.18.3 をインストールするには、`VERSION=1.18:1.18.3` を設定します。<br><p>以下を実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable.repo https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/<span style=color:#b8860b>$OS</span>/devel:kubic:libcontainers:stable.repo
</span></span><span style=display:flex><span>curl -L -o /etc/yum.repos.d/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>.repo https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>/<span style=color:#b8860b>$OS</span>/devel:kubic:libcontainers:stable:cri-o:<span style=color:#b8860b>$VERSION</span>.repo
</span></span><span style=display:flex><span>yum install cri-o
</span></span></code></pre></div></div><div id=tab-cri-cri-o-installation-3 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-3><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span> sudo zypper install cri-o
</span></span></code></pre></div></div><div id=tab-cri-cri-o-installation-4 class=tab-pane role=tabpanel aria-labelledby=tab-cri-cri-o-installation-4><p><p>$VERSIONには、Kubernetesのバージョンと一致するCRI-Oのバージョンを設定します。例えば、CRI-O 1.18をインストールしたい場合は、$VERSION=1.18を設定します。
以下のコマンドで、利用可能なバージョンを見つけることができます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>dnf module list cri-o
</span></span></code></pre></div><p>CRI-OはFedoraの特定のリリースにピン留めすることをサポートしていません。</p><p>以下を実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>dnf module <span style=color:#a2f>enable</span> cri-o:<span style=color:#b8860b>$VERSION</span>
</span></span><span style=display:flex><span>dnf install cri-o
</span></span></code></pre></div></div></div><h3 id=cri-oの起動>CRI-Oの起動</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl start crio
</span></span></code></pre></div><p>詳細については、<a href=https://github.com/kubernetes-sigs/cri-o#getting-started>CRI-Oインストールガイド</a>を参照してください。</p><h2 id=containerd>Containerd</h2><p>このセクションでは、CRIランタイムとして<code>containerd</code>を利用するために必要な手順について説明します。</p><p>システムへContainerdをインストールするためには次のコマンドを実行します。</p><h3 id=必要な設定の追加>必要な設定の追加</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat &gt; /etc/modules-load.d/containerd.conf <span style=color:#b44>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#b44>overlay
</span></span></span><span style=display:flex><span><span style=color:#b44>br_netfilter
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>modprobe overlay
</span></span><span style=display:flex><span>modprobe br_netfilter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 必要なカーネルパラメータの設定をします。これらの設定値は再起動後も永続化されます。</span>
</span></span><span style=display:flex><span>cat &gt; /etc/sysctl.d/99-kubernetes-cri.conf <span style=color:#b44>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-iptables  = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.ipv4.ip_forward                 = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sysctl --system
</span></span></code></pre></div><h3 id=containerdのインストール>containerdのインストール</h3><ul class="nav nav-tabs" id=tab-cri-containerd-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tab-cri-containerd-installation-0 role=tab aria-controls=tab-cri-containerd-installation-0 aria-selected=true>Ubuntu 16.04</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tab-cri-containerd-installation-1 role=tab aria-controls=tab-cri-containerd-installation-1>CentOS/RHEL 7.4+</a></li></ul><div class=tab-content id=tab-cri-containerd-installation><div id=tab-cri-containerd-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=tab-cri-containerd-installation-0><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># (containerdのインストール)</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>## リポジトリの設定</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>### HTTPS越しのリポジトリの使用をaptに許可するために、パッケージをインストール</span>
</span></span><span style=display:flex><span>apt-get update <span style=color:#666>&amp;&amp;</span> apt-get install -y apt-transport-https ca-certificates curl software-properties-common
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic>## Docker公式のGPG鍵を追加</span>
</span></span><span style=display:flex><span>curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic>## Dockerのaptリポジトリの追加</span>
</span></span><span style=display:flex><span>add-apt-repository <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>    <span style=color:#b44>&#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
</span></span></span><span style=display:flex><span><span style=color:#b44>    </span><span style=color:#a2f;font-weight:700>$(</span>lsb_release -cs<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44> \
</span></span></span><span style=display:flex><span><span style=color:#b44>    stable&#34;</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic>## containerdのインストール</span>
</span></span><span style=display:flex><span>apt-get update <span style=color:#666>&amp;&amp;</span> apt-get install -y containerd.io
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># containerdの設定</span>
</span></span><span style=display:flex><span>mkdir -p /etc/containerd
</span></span><span style=display:flex><span>containerd config default | sudo tee /etc/containerd/config.toml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># containerdの再起動</span>
</span></span><span style=display:flex><span>systemctl restart containerd
</span></span></code></pre></div></div><div id=tab-cri-containerd-installation-1 class=tab-pane role=tabpanel aria-labelledby=tab-cri-containerd-installation-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># (containerdのインストール)</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>## リポジトリの設定</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>### 必要なパッケージのインストール</span>
</span></span><span style=display:flex><span>yum install -y yum-utils device-mapper-persistent-data lvm2
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic>## Dockerのリポジトリの追加</span>
</span></span><span style=display:flex><span>yum-config-manager <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>    --add-repo <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>    https://download.docker.com/linux/centos/docker-ce.repo
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic>## containerdのインストール</span>
</span></span><span style=display:flex><span>yum update -y <span style=color:#666>&amp;&amp;</span> yum install -y containerd.io
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic>## containerdの設定</span>
</span></span><span style=display:flex><span>mkdir -p /etc/containerd
</span></span><span style=display:flex><span>containerd config default | sudo tee /etc/containerd/config.toml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># containerdの再起動</span>
</span></span><span style=display:flex><span>systemctl restart containerd
</span></span></code></pre></div></div></div><h3 id=systemd>systemd</h3><p><code>systemd</code>のcgroupドライバーを使うには、<code>/etc/containerd/config.toml</code>内で<code>plugins.cri.systemd_cgroup = true</code>を設定してください。
kubeadmを使う場合は<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E3%83%9E%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%8E%E3%83%BC%E3%83%89%E3%81%AEkubelet%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E4%BD%BF%E7%94%A8%E3%81%95%E3%82%8C%E3%82%8Bcgroup%E3%83%89%E3%83%A9%E3%82%A4%E3%83%90%E3%83%BC%E3%81%AE%E8%A8%AD%E5%AE%9A>kubeletのためのcgroupドライバー</a>を手動で設定してください。</p><h2 id=その他のcriランタイム-frakti>その他のCRIランタイム: frakti</h2><p>詳細については<a href=https://github.com/kubernetes/frakti#quickstart>Fraktiのクイックスタートガイド</a>を参照してください。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-00e1646f68aeb89f9722cf6f6cfcad94>2 - Kubernetesをデプロイツールでインストールする</h1></div><div class=td-content><h1 id=pg-a16f59f325a17cdeed324d5c889f7f73>2.1 - kubeadmを使ってクラスターを構築する</h1></div><div class=td-content><h1 id=pg-29e59491dd6118b23072dfe9ebb93323>2.1.1 - kubeadmのインストール</h1><img src=/images/kubeadm-stacked-color.png align=right width=150px><p>このページでは<code>kubeadm</code>コマンドをインストールする方法を示します。このインストール処理実行後にkubeadmを使用してクラスターを作成する方法については、<a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>kubeadmを使用したシングルマスタークラスターの作成</a>を参照してください。</p><h2 id=始める前に>始める前に</h2><ul><li>次のいずれかが動作しているマシンが必要です<ul><li>Ubuntu 16.04+</li><li>Debian 9+</li><li>CentOS 7</li><li>Red Hat Enterprise Linux (RHEL) 7</li><li>Fedora 25+</li><li>HypriotOS v1.0.1+</li><li>Container Linux (tested with 1800.6.0)</li></ul></li><li>1台あたり2GB以上のメモリ(2GBの場合、アプリ用のスペースはほとんどありません)</li><li>2コア以上のCPU</li><li>クラスター内のすべてのマシン間で通信可能なネットワーク(パブリックネットワークでもプライベートネットワークでも構いません)</li><li>ユニークなhostname、MACアドレス、とproduct_uuidが各ノードに必要です。詳細は<a href=#MAC%E3%82%A2%E3%83%89%E3%83%AC%E3%82%B9%E3%81%A8product_uuid%E3%81%8C%E5%85%A8%E3%81%A6%E3%81%AE%E3%83%8E%E3%83%BC%E3%83%89%E3%81%A7%E3%83%A6%E3%83%8B%E3%83%BC%E3%82%AF%E3%81%A7%E3%81%82%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%A4%9C%E8%A8%BC>ここ</a>を参照してください。</li><li>マシン内の特定のポートが開いていること。詳細は<a href=#%E5%BF%85%E9%A0%88%E3%83%9D%E3%83%BC%E3%83%88%E3%81%AE%E7%A2%BA%E8%AA%8D>ここ</a>を参照してください。</li><li>Swapがオフであること。kubeletが正常に動作するためにはswapは<strong>必ず</strong>オフでなければなりません。</li></ul><h2 id=macアドレスとproduct-uuidが全てのノードでユニークであることの検証>MACアドレスとproduct_uuidが全てのノードでユニークであることの検証</h2><ul><li>ネットワークインターフェースのMACアドレスは<code>ip link</code>もしくは<code>ifconfig -a</code>コマンドで取得できます。</li><li>product_uuidは<code>sudo cat /sys/class/dmi/id/product_uuid</code>コマンドで確認できます。</li></ul><p>ハードウェアデバイスではユニークなアドレスが割り当てられる可能性が非常に高いですが、VMでは同じになることがあります。Kubernetesはこれらの値を使用して、クラスター内のノードを一意に識別します。これらの値が各ノードに固有ではない場合、インストール処理が<a href=https://github.com/kubernetes/kubeadm/issues/31>失敗</a>することもあります。</p><h2 id=ネットワークアダプタの確認>ネットワークアダプタの確認</h2><p>複数のネットワークアダプターがあり、Kubernetesコンポーネントにデフォルトで到達できない場合、IPルートを追加して、Kubernetesクラスターのアドレスが適切なアダプターを経由するように設定することをお勧めします。</p><h2 id=iptablesがブリッジを通過するトラフィックを処理できるようにする>iptablesがブリッジを通過するトラフィックを処理できるようにする</h2><p>Linuxノードのiptablesがブリッジを通過するトラフィックを正確に処理する要件として、<code>net.bridge.bridge-nf-call-iptables</code>を<code>sysctl</code>の設定ファイルで1に設定してください。例えば以下のようにします。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-iptables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>sysctl --system
</span></span></code></pre></div><p>この手順の前に<code>br_netfilter</code>モジュールがロードされていることを確認してください。<code>lsmod | grep br_netfilter</code>を実行することで確認できます。明示的にロードするには<code>modprobe br_netfilter</code>を実行してください。</p><p>詳細は<a href=https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>ネットワークプラグインの要件</a>を参照してください。</p><h2 id=iptablesがnftablesバックエンドを使用しないようにする>iptablesがnftablesバックエンドを使用しないようにする</h2><p>Linuxでは、カーネルのiptablesサブシステムの最新の代替品としてnftablesが利用できます。<code>iptables</code>ツールは互換性レイヤーとして機能し、iptablesのように動作しますが、実際にはnftablesを設定します。このnftablesバックエンドは現在のkubeadmパッケージと互換性がありません。(ファイアウォールルールが重複し、<code>kube-proxy</code>を破壊するためです。)</p><p>もしあなたのシステムの<code>iptables</code>ツールがnftablesバックエンドを使用している場合、これらの問題を避けるために<code>iptables</code>ツールをレガシーモードに切り替える必要があります。これは、少なくともDebian 10(Buster)、Ubuntu 19.04、Fedora 29、およびこれらのディストリビューションの新しいリリースでのデフォルトです。RHEL 8はレガシーモードへの切り替えをサポートしていないため、現在のkubeadmパッケージと互換性がありません。</p><ul class="nav nav-tabs" id=iptables-legacy role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#iptables-legacy-0 role=tab aria-controls=iptables-legacy-0 aria-selected=true>DebianまたはUbuntu</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#iptables-legacy-1 role=tab aria-controls=iptables-legacy-1>Fedora</a></li></ul><div class=tab-content id=iptables-legacy><div id=iptables-legacy-0 class="tab-pane show active" role=tabpanel aria-labelledby=iptables-legacy-0><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># レガシーバイナリがインストールされていることを確認してください</span>
</span></span><span style=display:flex><span>sudo apt-get install -y iptables arptables ebtables
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># レガシーバージョンに切り替えてください。</span>
</span></span><span style=display:flex><span>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy
</span></span><span style=display:flex><span>sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy
</span></span><span style=display:flex><span>sudo update-alternatives --set arptables /usr/sbin/arptables-legacy
</span></span><span style=display:flex><span>sudo update-alternatives --set ebtables /usr/sbin/ebtables-legacy
</span></span></code></pre></div></div><div id=iptables-legacy-1 class=tab-pane role=tabpanel aria-labelledby=iptables-legacy-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>update-alternatives --set iptables /usr/sbin/iptables-legacy
</span></span></code></pre></div></div></div><h2 id=必須ポートの確認>必須ポートの確認</h2><h3 id=コントロールプレーンノード>コントロールプレーンノード</h3><table><thead><tr><th>プロトコル</th><th>通信の向き</th><th>ポート範囲</th><th>目的</th><th>使用者</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>6443*</td><td>Kubernetes API server</td><td>全て</td></tr><tr><td>TCP</td><td>Inbound</td><td>2379-2380</td><td>etcd server client API</td><td>kube-apiserver、etcd</td></tr><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>Kubelet API</td><td>自身、コントロールプレーン</td></tr><tr><td>TCP</td><td>Inbound</td><td>10251</td><td>kube-scheduler</td><td>自身</td></tr><tr><td>TCP</td><td>Inbound</td><td>10252</td><td>kube-controller-manager</td><td>自身</td></tr></tbody></table><h3 id=ワーカーノード>ワーカーノード</h3><table><thead><tr><th>プロトコル</th><th>通信の向き</th><th>ポート範囲</th><th>目的</th><th>使用者</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>Kubelet API</td><td>自身、コントロールプレーン</td></tr><tr><td>TCP</td><td>Inbound</td><td>30000-32767</td><td>NodePort Service†</td><td>全て</td></tr></tbody></table><p>† <a href=/ja/docs/concepts/services-networking/service/>NodePort Service</a>のデフォルトのポートの範囲</p><p>*の項目は書き換え可能です。そのため、あなたが指定したカスタムポートも開いていることを確認する必要があります。</p><p>etcdポートはコントロールプレーンノードに含まれていますが、独自のetcdクラスターを外部またはカスタムポートでホストすることもできます。</p><p>使用するPodネットワークプラグイン(以下を参照)のポートも開く必要があります。これは各Podネットワークプラグインによって異なるため、必要なポートについてはプラグインのドキュメントを参照してください。</p><h2 id=installing-runtime>ランタイムのインストール</h2><p>Podのコンテナを実行するために、Kubernetesは<a class=glossary-tooltip title=コンテナランタイムは、コンテナの実行を担当するソフトウェアです。 data-toggle=tooltip data-placement=top href=/ja/docs/setup/production-environment/container-runtimes target=_blank aria-label=コンテナランタイム>コンテナランタイム</a>を使用します。</p><ul class="nav nav-tabs" id=container-runtime role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#container-runtime-0 role=tab aria-controls=container-runtime-0 aria-selected=true>Linuxノード</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#container-runtime-1 role=tab aria-controls=container-runtime-1>その他のOS</a></li></ul><div class=tab-content id=container-runtime><div id=container-runtime-0 class="tab-pane show active" role=tabpanel aria-labelledby=container-runtime-0><p><p>デフォルトでは、Kubernetesは選択されたコンテナランタイムと通信するために<a class=glossary-tooltip title='An API for container runtimes to integrate with kubelet' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#container-runtime target=_blank aria-label='Container Runtime Interface'>Container Runtime Interface</a> (CRI)を使用します。</p><p>ランタイムを指定しない場合、kubeadmはよく知られたUnixドメインソケットのリストをスキャンすることで、インストールされたコンテナランタイムの検出を試みます。
次の表がコンテナランタイムと関連するソケットのパスリストです。</p><table><caption style=display:none>コンテナランタイムとソケットパス</caption><thead><tr><th>ランタイム</th><th>Unixドメインソケットのパス</th></tr></thead><tbody><tr><td>Docker</td><td><code>/var/run/docker.sock</code></td></tr><tr><td>containerd</td><td><code>/run/containerd/containerd.sock</code></td></tr><tr><td>CRI-O</td><td><code>/var/run/crio/crio.sock</code></td></tr></tbody></table><br>Dockerとcontainerdの両方が同時に検出された場合、Dockerが優先されます。Docker 18.09にはcontainerdが同梱されており、両方が検出可能であるため、この仕様が必要です。他の2つ以上のランタイムが検出された場合、kubeadmは適切なエラーメッセージで終了します。<p>kubeletは、組み込まれた<code>dockershim</code>CRIを通してDockerと連携します。</p><p>詳細は、<a href=/ja/docs/setup/production-environment/container-runtimes/>コンテナランタイム</a>を参照してください。</p></div><div id=container-runtime-1 class=tab-pane role=tabpanel aria-labelledby=container-runtime-1><p><p>デフォルトでは、kubeadmは<a class=glossary-tooltip title=Dockerは、コンテナとして知られる、オペレーティングシステムレベルでの仮想化を提供するソフトウェア技術です。 data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a>をコンテナランタイムとして使用します。
kubeletは、組み込まれた<code>dockershim</code>CRIを通してDockerと連携します。</p><p>詳細は、<a href=/ja/docs/setup/production-environment/container-runtimes/>コンテナランタイム</a>を参照してください。</p></div></div><h2 id=kubeadm-kubelet-kubectlのインストール>kubeadm、kubelet、kubectlのインストール</h2><p>以下のパッケージをマシン上にインストールしてください</p><ul><li><p><code>kubeadm</code>: クラスターを起動するコマンドです。</p></li><li><p><code>kubelet</code>: クラスター内のすべてのマシンで実行されるコンポーネントです。
Podやコンテナの起動などを行います。</p></li><li><p><code>kubectl</code>: クラスターにアクセスするためのコマンドラインツールです。</p></li></ul><p>kubeadmは<code>kubelet</code>や<code>kubectl</code>をインストールまたは管理<strong>しない</strong>ため、kubeadmにインストールするKubernetesコントロールプレーンのバージョンと一致させる必要があります。そうしないと、予期しないバグのある動作につながる可能性のあるバージョン差異(version skew)が発生するリスクがあります。ただし、kubeletとコントロールプレーン間のマイナーバージョン差異(minor version skew)は_1つ_サポートされていますが、kubeletバージョンがAPIサーバーのバージョンを超えることはできません。たとえば、1.7.0を実行するkubeletは1.8.0 APIサーバーと完全に互換性がありますが、その逆はできません。</p><p><code>kubectl</code>のインストールに関する詳細情報は、<a href=/ja/docs/tasks/tools/install-kubectl/>kubectlのインストールおよびセットアップ</a>を参照してください。</p><div class="alert alert-danger warning callout" role=alert><strong>警告:</strong> これらの手順はシステムアップグレードによるすべてのKubernetesパッケージの更新を除きます。これはkubeadmとKubernetesが<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>アップグレードにおける特別な注意</a>を必要とするからです。</div><p>バージョン差異(version skew)に関しては下記を参照してください。</p><ul><li>Kubernetes <a href=/ja/docs/setup/release/version-skew-policy/>Kubernetesバージョンとバージョンスキューサポートポリシー</a></li><li>Kubeadm-specific <a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy>バージョン互換ポリシー</a></li></ul><ul class="nav nav-tabs" id=k8s-install role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-0 role=tab aria-controls=k8s-install-0 aria-selected=true>Ubuntu、Debian、またはHypriotOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-1 role=tab aria-controls=k8s-install-1>CentOS、RHEL、またはFedora</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-2 role=tab aria-controls=k8s-install-2>Container Linux</a></li></ul><div class=tab-content id=k8s-install><div id=k8s-install-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-0><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo apt-get update <span style=color:#666>&amp;&amp;</span> sudo apt-get install -y apt-transport-https curl
</span></span><span style=display:flex><span>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span></span><span style=display:flex><span><span style=color:#b44>deb https://apt.kubernetes.io/ kubernetes-xenial main
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install -y kubelet kubeadm kubectl
</span></span><span style=display:flex><span>sudo apt-mark hold kubelet kubeadm kubectl
</span></span></code></pre></div></div><div id=k8s-install-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span></span></span><span style=display:flex><span><span style=color:#b44>[kubernetes]
</span></span></span><span style=display:flex><span><span style=color:#b44>name=Kubernetes
</span></span></span><span style=display:flex><span><span style=color:#b44>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span></span></span><span style=display:flex><span><span style=color:#b44>enabled=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#b44>repo_gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># SELinuxをpermissiveモードに設定する(効果的に無効化する)</span>
</span></span><span style=display:flex><span>setenforce <span style=color:#666>0</span>
</span></span><span style=display:flex><span>sed -i <span style=color:#b44>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>yum install -y kubelet kubeadm kubectl --disableexcludes<span style=color:#666>=</span>kubernetes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div><p><strong>Note:</strong></p><ul><li><code>setenforce 0</code>および<code>sed ...</code>を実行することによりSELinuxをpermissiveモードに設定し、効果的に無効化できます。
これはコンテナがホストのファイルシステムにアクセスするために必要です。例えば、Podのネットワークに必要とされます。
kubeletにおけるSELinuxのサポートが改善されるまでは、これを実行しなければなりません。</li></ul></div><div id=k8s-install-2 class=tab-pane role=tabpanel aria-labelledby=k8s-install-2><p><p>CNIプラグインをインストールする(ほとんどのPodのネットワークに必要です):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.8.2&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>mkdir -p /opt/cni/bin
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cni-plugins-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tgz&#34;</span> | tar -C /opt/cni/bin -xz
</span></span></code></pre></div><p>crictlをインストールする (kubeadm / Kubelet Container Runtime Interface (CRI)に必要です)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.22.0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/crictl-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tar.gz&#34;</span> | sudo tar -C <span style=color:#b8860b>$DOWNLOAD_DIR</span> -xz
</span></span></code></pre></div><p><code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code>をインストールし<code>kubelet</code>をsystemd serviceに登録します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>RELEASE</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>curl -sSL https://dl.k8s.io/release/stable.txt<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>mkdir -p /opt/bin
</span></span><span style=display:flex><span><span style=color:#a2f>cd</span> /opt/bin
</span></span><span style=display:flex><span>curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span>/bin/linux/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span>/<span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>
</span></span><span style=display:flex><span>chmod +x <span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/kubernetes/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/build/debs/kubelet.service&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:/opt/bin:g&#34;</span> &gt; /etc/systemd/system/kubelet.service
</span></span><span style=display:flex><span>mkdir -p /etc/systemd/system/kubelet.service.d
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/kubernetes/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/build/debs/10-kubeadm.conf&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:/opt/bin:g&#34;</span> &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</span></span></code></pre></div><p><code>kubelet</code>を有効化し起動します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div></div></div><p>kubeadmが何をすべきか指示するまで、kubeletはクラッシュループで数秒ごとに再起動します。</p><h2 id=コントロールプレーンノードのkubeletによって使用されるcgroupドライバーの設定>コントロールプレーンノードのkubeletによって使用されるcgroupドライバーの設定</h2><p>Dockerを使用した場合、kubeadmは自動的にkubelet向けのcgroupドライバーを検出し、それを実行時に<code>/var/lib/kubelet/kubeadm-flags.env</code>ファイルに設定します。</p><p>もしあなたが異なるCRIを使用している場合、<code>/etc/default/kubelet</code>(CentOS、RHEL、Fedoraでは<code>/etc/sysconfig/kubelet</code>)ファイル内の<code>cgroup-driver</code>の値を以下のように変更する必要があります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>KUBELET_EXTRA_ARGS</span><span style=color:#666>=</span>--cgroup-driver<span style=color:#666>=</span>&lt;value&gt;
</span></span></code></pre></div><p>このファイルは、kubeletの追加のユーザー定義引数を取得するために、<code>kubeadm init</code>および<code>kubeadm join</code>によって使用されます。</p><p>CRIのcgroupドライバーが<code>cgroupfs</code>でない場合に<strong>のみ</strong>それを行う必要があることに注意してください。なぜなら、これはすでにkubeletのデフォルト値であるためです。</p><p>kubeletをリスタートする方法:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div><p>CRI-Oやcontainerdといった他のコンテナランタイムのcgroup driverは実行中に自動的に検出されます。</p><h2 id=トラブルシュート>トラブルシュート</h2><p>kubeadmで問題が発生した場合は、<a href=/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>トラブルシューティング</a>を参照してください。</p><h2 id=次の項目>次の項目</h2><ul><li><a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>kubeadmを使用したシングルコントロールプレーンクラスターの作成</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c3689df4b0c61a998e79d91a865aa244>2.1.2 - kubeadmのトラブルシューティング</h1><p>どのプログラムでもそうですが、kubeadmのインストールや実行でエラーが発生することがあります。このページでは、一般的な失敗例をいくつか挙げ、問題を理解して解決するための手順を示しています。</p><p>本ページに問題が記載されていない場合は、以下の手順を行ってください:</p><ul><li><p>問題がkubeadmのバグによるものと思った場合:</p><ul><li><a href=https://github.com/kubernetes/kubeadm/issues>github.com/kubernetes/kubeadm</a>にアクセスして、既存のIssueを探してください。</li><li>Issueがない場合は、テンプレートにしたがって<a href=https://github.com/kubernetes/kubeadm/issues/new>新しくIssueを立ててください</a>。</li></ul></li><li><p>kubeadmがどのように動作するかわからない場合は、<a href=http://slack.k8s.io/>Slack</a>の#kubeadmチャンネルで質問するか、<a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a>で質問をあげてください。その際は、他の方が助けを出しやすいように<code>#kubernetes</code>や<code>#kubeadm</code>といったタグをつけてください。</p></li></ul><h2 id=rbacがないため-v1-18ノードをv1-17クラスタに結合できない>RBACがないため、v1.18ノードをv1.17クラスタに結合できない</h2><p>v1.18では、同名のノードが既に存在する場合にクラスタ内のノードに参加しないようにする機能を追加しました。これには、ブートストラップトークンユーザがNodeオブジェクトをGETできるようにRBACを追加する必要がありました。</p><p>しかし、これによりv1.18の<code>kubeadm join</code>がkubeadm v1.17で作成したクラスタに参加できないという問題が発生します。</p><p>この問題を回避するには、次の2つの方法があります。</p><ul><li><p>kubeadm v1.18を用いて、コントロールプレーンノード上で<code>kubeadm init phase bootstrap-token</code>を実行します。
これには、ブートストラップトークンの残りのパーミッションも同様に有効にすることに注意してください。</p></li><li><p><code>kubectl apply -f ...</code>を使って以下のRBACを手動で適用します。</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRoleBinding<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>roleRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Group<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:bootstrappers:kubeadm:default-node-token<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=インストール中に-ebtables-もしくは他の似たような実行プログラムが見つからない>インストール中に<code>ebtables</code>もしくは他の似たような実行プログラムが見つからない</h2><p><code>kubeadm init</code>の実行中に以下のような警告が表示された場合は、以降に記載するやり方を行ってください。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ebtables not found in system path
</span></span><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ethtool not found in system path
</span></span></code></pre></div><p>このような場合、ノード上に<code>ebtables</code>, <code>ethtool</code>などの実行ファイルがない可能性があります。これらをインストールするには、以下のコマンドを実行します。</p><ul><li>Ubuntu/Debianユーザーは、<code>apt install ebtables ethtool</code>を実行してください。</li><li>CentOS/Fedoraユーザーは、<code>yum install ebtables ethtool</code>を実行してください。</li></ul><h2 id=インストール中にkubeadmがコントロールプレーンを待ち続けて止まる>インストール中にkubeadmがコントロールプレーンを待ち続けて止まる</h2><p>以下のを出力した後に<code>kubeadm init</code>が止まる場合は、<code>kubeadm init</code>を実行してください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#666>[</span>apiclient<span style=color:#666>]</span> Created API client, waiting <span style=color:#a2f;font-weight:700>for</span> the control plane to become ready
</span></span></code></pre></div><p>これはいくつかの問題が原因となっている可能性があります。最も一般的なのは:</p><ul><li><p>ネットワーク接続の問題が挙げられます。続行する前に、お使いのマシンがネットワークに完全に接続されていることを確認してください。</p></li><li><p>kubeletのデフォルトのcgroupドライバの設定がDockerで使用されているものとは異なっている場合も考えられます。
システムログファイル(例: <code>/var/log/message</code>)をチェックするか、<code>journalctl -u kubelet</code>の出力を調べてください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>error: failed to run Kubelet: failed to create kubelet:
</span></span><span style=display:flex><span>misconfiguration: kubelet cgroup driver: <span style=color:#b44>&#34;systemd&#34;</span> is different from docker cgroup driver: <span style=color:#b44>&#34;cgroupfs&#34;</span>
</span></span></code></pre></div><p>以上のようなエラーが現れていた場合、cgroupドライバの問題を解決するには、以下の2つの方法があります:</p></li></ul><ol><li><p><a href=/ja/docs/setup/independent/install-kubeadm/#installing-docker>ここ</a>の指示に従ってDockerを再度インストールします。</p></li><li><p>Dockerのcgroupドライバに合わせてkubeletの設定を手動で変更します。その際は、<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-master-node>マスターノード上でkubeletが使用するcgroupドライバを設定する</a>を参照してください。</p></li></ol><ul><li>control plane Dockerコンテナがクラッシュループしたり、ハングしたりしています。これは<code>docker ps</code>を実行し、<code>docker logs</code>を実行して各コンテナを調査することで確認できます。</li></ul><h2 id=管理コンテナを削除する時にkubeadmが止まる>管理コンテナを削除する時にkubeadmが止まる</h2><p>Dockerが停止して、Kubernetesで管理されているコンテナを削除しないと、以下のようなことが起こる可能性があります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo kubeadm reset
</span></span><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> Running pre-flight checks
</span></span><span style=display:flex><span><span style=color:#666>[</span>reset<span style=color:#666>]</span> Stopping the kubelet service
</span></span><span style=display:flex><span><span style=color:#666>[</span>reset<span style=color:#666>]</span> Unmounting mounted directories in <span style=color:#b44>&#34;/var/lib/kubelet&#34;</span>
</span></span><span style=display:flex><span><span style=color:#666>[</span>reset<span style=color:#666>]</span> Removing kubernetes-managed containers
</span></span><span style=display:flex><span><span style=color:#666>(</span>block<span style=color:#666>)</span>
</span></span></code></pre></div><p>考えられる解決策は、Dockerサービスを再起動してから<code>kubeadm reset</code>を再実行することです:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo systemctl restart docker.service
</span></span><span style=display:flex><span>sudo kubeadm reset
</span></span></code></pre></div><p>dockerのログを調べるのも有効な場合があります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>journalctl -u docker
</span></span></code></pre></div><h2 id=podの状態が-runcontainererror-crashloopbackoff-または-error-となる>Podの状態が<code>RunContainerError</code>、<code>CrashLoopBackOff</code>、または<code>Error</code>となる</h2><p><code>kubeadm init</code>の直後には、これらの状態ではPodは存在しないはずです。</p><ul><li><code>kubeadm init</code>の <em>直後</em> にこれらの状態のいずれかにPodがある場合は、kubeadmのリポジトリにIssueを立ててください。ネットワークソリューションをデプロイするまでは<code>coredns</code>(または<code>kube-dns</code>)は<code>Pending</code>状態でなければなりません。</li><li>ネットワークソリューションをデプロイしても<code>coredns</code>(または<code>kube-dns</code>)に何も起こらない場合にRunContainerError<code>、</code>CrashLoopBackOff<code>、</code>Error`の状態でPodが表示された場合は、インストールしたPodネットワークソリューションが壊れている可能性が高いです。より多くのRBACの特権を付与するか、新しいバージョンを使用する必要があるかもしれません。PodネットワークプロバイダのイシュートラッカーにIssueを出して、そこで問題をトリアージしてください。</li><li>1.12.1よりも古いバージョンのDockerをインストールした場合は、<code>systemd</code>で<code>dockerd</code>を起動する際に<code>MountFlags=slave</code>オプションを削除して<code>docker</code>を再起動してください。マウントフラグは<code>/usr/lib/systemd/system/docker.service</code>で確認できます。MountFlagsはKubernetesがマウントしたボリュームに干渉し、Podsを<code>CrashLoopBackOff</code>状態にすることがあります。このエラーは、Kubernetesが<code>var/run/secrets/kubernetes.io/serviceaccount</code>ファイルを見つけられない場合に発生します。</li></ul><h2 id=coredns-もしくは-kube-dns-が-pending-状態でスタックする><code>coredns</code>(もしくは<code>kube-dns</code>)が<code>Pending</code>状態でスタックする</h2><p>kubeadmはネットワークプロバイダに依存しないため、管理者は選択した<a href=/docs/concepts/cluster-administration/addons/>Podネットワークソリューションをインストール</a>をする必要があります。CoreDNSを完全にデプロイする前にPodネットワークをインストールする必要があります。したがって、ネットワークがセットアップされる前の <code>Pending</code>状態になります。</p><h2 id=hostport-サービスが動かない><code>HostPort</code>サービスが動かない</h2><p><code>HostPort</code>と<code>HostIP</code>の機能は、ご使用のPodネットワークプロバイダによって利用可能です。Podネットワークソリューションの作者に連絡して、<code>HostPort</code>と<code>HostIP</code>機能が利用可能かどうかを確認してください。</p><p>Calico、Canal、FlannelのCNIプロバイダは、HostPortをサポートしていることが確認されています。</p><p>詳細については、[CNI portmap documentation] (<a href=https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md>https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md</a>) を参照してください。</p><p>ネットワークプロバイダが portmap CNI プラグインをサポートしていない場合は、<a href=/ja/docs/concepts/services-networking/service/#nodeport>NodePortサービス</a>を使用するか、<code>HostNetwork=true</code>を使用してください。</p><h2 id=サービスip経由でpodにアクセスすることができない>サービスIP経由でPodにアクセスすることができない</h2><ul><li><p>多くのネットワークアドオンは、PodがサービスIPを介して自分自身にアクセスできるようにする<a href=/ja/docs/tasks/debug-application-cluster/debug-service/#a-pod-cannot-reach-itself-via-service-ip>ヘアピンモード</a>を有効にしていません。これは<a href=https://github.com/containernetworking/cni/issues/476>CNI</a>に関連する問題です。ヘアピンモードのサポート状況については、ネットワークアドオンプロバイダにお問い合わせください。</p></li><li><p>VirtualBoxを使用している場合(直接またはVagrant経由)は、<code>hostname -i</code>がルーティング可能なIPアドレスを返すことを確認する必要があります。デフォルトでは、最初のインターフェースはルーティング可能でないホスト専用のネットワークに接続されています。これを回避するには<code>/etc/hosts</code>を修正する必要があります。例としてはこの<a href=https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11>Vagrantfile</a>を参照してください。</p></li></ul><h2 id=tls証明書のエラー>TLS証明書のエラー</h2><p>以下のエラーは、証明書の不一致の可能性を示しています。</p><pre tabindex=0><code class=language-none data-lang=none># kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &#34;crypto/rsa: verification error&#34; while trying to verify candidate authority certificate &#34;kubernetes&#34;)
</code></pre><ul><li><p><code>HOME/.kube/config</code>ファイルに有効な証明書が含まれていることを確認し、必要に応じて証明書を再生成します。kubeconfigファイル内の証明書はbase64でエンコードされています。証明書をデコードするには<code>base64 --decode</code>コマンドを、証明書情報を表示するには<code>openssl x509 -text -noout</code>コマンドを用いてください。</p></li><li><p>環境変数<code>KUBECONFIG</code>の設定を解除するには以下のコマンドを実行するか:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>unset</span> KUBECONFIG
</span></span></code></pre></div><p>設定をデフォルトの<code>KUBECONFIG</code>の場所に設定します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div></li><li><p>もう一つの回避策は、既存の<code>kubeconfig</code>を"admin"ユーザに上書きすることです:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>mv  <span style=color:#b8860b>$HOME</span>/.kube <span style=color:#b8860b>$HOME</span>/.kube.bak
</span></span><span style=display:flex><span>mkdir <span style=color:#b8860b>$HOME</span>/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div></li></ul><h2 id=vagrant内でpodネットワークとしてflannelを使用する時のデフォルトnic>Vagrant内でPodネットワークとしてflannelを使用する時のデフォルトNIC</h2><p>以下のエラーは、Podネットワークに何か問題があったことを示している可能性を示しています:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Error from server <span style=color:#666>(</span>NotFound<span style=color:#666>)</span>: the server could not find the requested resource
</span></span></code></pre></div><ul><li><p>Vagrant内のPodネットワークとしてflannelを使用している場合は、flannelのデフォルトのインターフェース名を指定する必要があります。</p><p>Vagrantは通常、2つのインターフェースを全てのVMに割り当てます。1つ目は全てのホストにIPアドレス<code>10.0.2.15</code>が割り当てられており、NATされる外部トラフィックのためのものです。</p><p>これは、ホストの最初のインターフェイスをデフォルトにしているflannelの問題につながるかもしれません。これは、すべてのホストが同じパブリックIPアドレスを持っていると考えます。これを防ぐには、2番目のインターフェイスが選択されるように <code>--iface eth1</code>フラグをflannelに渡してください。</p></li></ul><h2 id=公開されていないipがコンテナに使われている>公開されていないIPがコンテナに使われている</h2><p>状況によっては、<code>kubectl logs</code>や<code>kubectl run</code>コマンドが以下のようなエラーを返すことがあります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
</span></span></code></pre></div><ul><li><p>これには、おそらくマシンプロバイダのポリシーによって、一見同じサブネット上の他のIPと通信できないIPをKubernetesが使用している可能性があります。</p></li><li><p>DigitalOceanはパブリックIPとプライベートIPを<code>eth0</code>に割り当てていますが、<code>kubelet</code>はパブリックIPではなく、ノードの<code>InternalIP</code>として後者を選択します。</p><p><code>ifconfig</code>ではエイリアスIPアドレスが表示されないため、<code>ifconfig</code>の代わりに<code>ip addr show</code>を使用してこのシナリオをチェックしてください。あるいは、DigitalOcean専用のAPIエンドポイントを使用して、ドロップレットからアンカーIPを取得することもできます:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
</span></span></code></pre></div><p>回避策としては、<code>--node-ip</code>を使ってどのIPを使うかを<code>kubelet</code>に伝えることです。DigitalOceanを使用する場合、オプションのプライベートネットワークを使用したい場合は、パブリックIP（<code>eth0</code>に割り当てられている）かプライベートIP（<code>eth1</code>に割り当てられている）のどちらかを指定します。これにはkubeadm <code>NodeRegistrationOptions</code>構造体の <a href=https://github.com/kubernetes/kubernetes/blob/release-1.13/cmd/kubeadm/app/apis/kubeadm/v1beta1/types.go><code>KubeletExtraArgs</code>セクション</a> が利用できます。</p><p><code>kubelet</code>を再起動してください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li></ul><h2 id=coredns-のpodが-crashloopbackoff-もしくは-error-状態になる><code>coredns</code>のPodが<code>CrashLoopBackOff</code>もしくは<code>Error</code>状態になる</h2><p>SELinuxを実行しているノードで古いバージョンのDockerを使用している場合、<code>coredns</code> Podが起動しないということが起きるかもしれません。この問題を解決するには、以下のオプションのいずれかを試してみてください:</p><ul><li><p><a href=/ja/docs/setup/independent/install-kubeadm/#installing-docker>新しいDockerのバージョン</a>にアップグレードする。</p></li><li><p><a href=https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux>SELinuxを無効化する</a>。</p></li><li><p><code>coredns</code>を変更して、<code>allowPrivilegeEscalation</code>を<code>true</code>に設定:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kube-system get deployment coredns -o yaml | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  sed <span style=color:#b44>&#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g&#39;</span> | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  kubectl apply -f -
</span></span></code></pre></div><p>CoreDNSに<code>CrashLoopBackOff</code>が発生する別の原因は、KubernetesにデプロイされたCoreDNS Podがループを検出したときに発生します。CoreDNSがループを検出して終了するたびに、KubernetesがCoreDNS Podを再起動しようとするのを避けるために、<a href=https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters>いくつかの回避策</a>が用意されています。</p><div class="alert alert-danger warning callout" role=alert><strong>警告:</strong> SELinuxを無効にするか<code>allowPrivilegeEscalation</code>を<code>true</code>に設定すると、クラスタのセキュリティが損なわれる可能性があります。</div><h2 id=etcdのpodが継続的に再起動する>etcdのpodが継続的に再起動する</h2><p>以下のエラーが発生した場合は:</p><pre tabindex=0><code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &#34;process_linux.go:110: decoding init error from pipe caused \&#34;read parent: connection reset by peer\&#34;&#34;
</code></pre><p>この問題は、CentOS 7をDocker 1.13.1.84で実行した場合に表示されます。このバージョンのDockerでは、kubeletがetcdコンテナに実行されないようにすることができます。</p><p>この問題を回避するには、以下のいずれかのオプションを選択します:</p><ul><li>1.13.1-75のような以前のバージョンのDockerにロールバックする</li></ul><pre tabindex=0><code>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
</code></pre><ul><li>18.06のような最新の推奨バージョンをインストールする:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span>yum install docker-ce-18.06.1.ce-3.el7.x86_64
</span></span></code></pre></div><h2 id=コンマで区切られた値のリストを-component-extra-args-フラグ内の引数に渡すことができない>コンマで区切られた値のリストを<code>--component-extra-args</code>フラグ内の引数に渡すことができない</h2><p><code>-component-extra-args</code>のような<code>kubeadm init</code>フラグを使うと、kube-apiserverのようなコントロールプレーンコンポーネントにカスタム引数を渡すことができます。しかし、このメカニズムは値の解析に使われる基本的な型 (<code>mapStringString</code>) のために制限されています。</p><p>もし、<code>--apiserver-extra-args "enable-admission plugins=LimitRanger,NamespaceExists"</code>のようにカンマで区切られた複数の値をサポートする引数を渡した場合、このフラグは<code>flag: malformed pair, expect string=string</code>で失敗します。これは<code>--apiserver-extra-args</code>の引数リストが<code>key=value</code>のペアを期待しており、この場合<code>NamespacesExists</code>は値を欠いたキーとみなされるためです。</p><p>別の方法として、<code>key=value</code>のペアを以下のように分離してみることもできます:
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"</code>しかし、この場合は、キー<code>enable-admission-plugins</code>は<code>NamespaceExists</code>の値しか持ちません。既知の回避策としては、kubeadm<a href=/ja/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#apiserver-flags>設定ファイル</a>を使用することが挙げられます。</p><h2 id=cloud-controller-managerによってノードが初期化される前にkube-proxyがスケジューリングされる>cloud-controller-managerによってノードが初期化される前にkube-proxyがスケジューリングされる</h2><p>クラウドプロバイダのシナリオでは、クラウドコントローラマネージャがノードアドレスを初期化する前に、kube-proxyが新しいワーカーノードでスケジューリングされてしまうことがあります。これにより、kube-proxyがノードのIPアドレスを正しく拾えず、ロードバランサを管理するプロキシ機能に悪影響を及ぼします。</p><p>kube-proxy Podsでは以下のようなエラーが発生します:</p><pre tabindex=0><code>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
</code></pre><p>既知の解決策は、初期のガード条件が緩和されるまで他のノードから離しておき、条件に関係なくコントロールプレーンノード上でスケジューリングできるように、キューブプロキシDaemonSetにパッチを当てることです:</p><pre tabindex=0><code>kubectl -n kube-system patch ds kube-proxy -p=&#39;{ &#34;spec&#34;: { &#34;template&#34;: { &#34;spec&#34;: { &#34;tolerations&#34;: [ { &#34;key&#34;: &#34;CriticalAddonsOnly&#34;, &#34;operator&#34;: &#34;Exists&#34; }, { &#34;effect&#34;: &#34;NoSchedule&#34;, &#34;key&#34;: &#34;node-role.kubernetes.io/master&#34; } ] } } } }&#39;
</code></pre><p>Tこの問題のトラッキング問題は<a href=https://github.com/kubernetes/kubeadm/issues/1027>こちら</a>。</p><h2 id=kubeadmの設定をマーシャリングする際-noderegistration-taintsフィールドが省略される>kubeadmの設定をマーシャリングする際、NodeRegistration.Taintsフィールドが省略される</h2><p><em>注意: この<a href=https://github.com/kubernetes/kubeadm/issues/1358>Issue</a>は、kubeadmタイプをマーシャルするツール(YAML設定ファイルなど)にのみ適用されます。これはkubeadm API v1beta2で修正される予定です。</em></p><p>デフォルトでは、kubeadmはコントロールプレーンノードに<code>node-role.kubernetes.io/master:NoSchedule</code>のテイントを適用します。kubeadmがコントロールプレーンノードに影響を与えないようにし、<code>InitConfiguration.NodeRegistration.Taints</code>を空のスライスに設定すると、マーシャリング時にこのフィールドは省略されます。フィールドが省略された場合、kubeadmはデフォルトのテイントを適用します。</p><p>少なくとも2つの回避策があります:</p><ol><li><p>空のスライスの代わりに<code>node-role.kubernetes.io/master:PreferNoSchedule</code>テイントを使用します。他のノードに容量がない限り、<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>Podsはマスター上でスケジュールされます</a>。</p></li><li><p>kubeadm init終了後のテイントの除去:</p></li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl taint nodes NODE_NAME node-role.kubernetes.io/master:NoSchedule-
</span></span></code></pre></div><h2 id=ノード-usr-mounted-read-only-に-usr-が読み取り専用でマウントされる>ノード{#usr-mounted-read-only}に<code>/usr</code>が読み取り専用でマウントされる</h2><p>Fedora CoreOSなどのLinuxディストリビューションでは、ディレクトリ<code>/usr</code>が読み取り専用のファイルシステムとしてマウントされます。 <a href=https://github.com/kubernetes/community/blob/ab55d85/contributors/devel/sig-storage/flexvolume.md>flex-volumeサポート</a>では、kubeletやkube-controller-managerのようなKubernetesコンポーネントはデフォルトで<code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code>のパスを使用していますが、この機能を動作させるためにはflex-volumeディレクトリは <em>書き込み可能</em> な状態でなければなりません。</p><p>この問題を回避するには、kubeadm<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2>設定ファイル</a>を使用してflex-volumeディレクトリを設定します。</p><p>プライマリコントロールプレーンノード（<code>kubeadm init</code>で作成されたもの）上で、<code>--config</code>で以下のファイルを渡します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>flex-volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>ノードをジョインするには:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>あるいは、<code>/usr</code>マウントを書き込み可能にするために <code>/etc/fstab</code>を変更することもできますが、これはLinuxディストリビューションの設計原理を変更していることに注意してください。</p><h2 id=kubeadm-upgrade-plan-が-context-deadline-exceeded-エラーメッセージを表示する><code>kubeadm upgrade plan</code>が<code>context deadline exceeded</code>エラーメッセージを表示する</h2><p>このエラーメッセージは、外部etcdを実行している場合に<code>kubeadm</code>でKubernetesクラスタをアップグレードする際に表示されます。これは致命的なバグではなく、古いバージョンのkubeadmが外部etcdクラスタのバージョンチェックを行うために発生します。<code>kubeadm upgrade apply ...</code>で進めることができます。</p><p>この問題はバージョン1.19で修正されます。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-134ed1f6142a98e6ac681a1ba4920e53>2.1.3 - kubeadmを使用したクラスターの作成</h1><p><img src=/images/kubeadm-stacked-color.png align=right width=150px>ベストプラクティスに準拠した実用最小限のKubernetesクラスターを作成します。実際、<code>kubeadm</code>を使用すれば、<a href=https://kubernetes.io/blog/2017/10/software-conformance-certification>Kubernetes Conformance tests</a>に通るクラスターをセットアップすることができます。<code>kubeadm</code>は、<a href=/docs/reference/access-authn-authz/bootstrap-tokens/>ブートストラップトークン</a>やクラスターのアップグレードなどのその他のクラスターのライフサイクルの機能もサポートします。</p><p><code>kubeadm</code>ツールは、次のようなときに適しています。</p><ul><li>新しいユーザーが初めてKubernetesを試すためのシンプルな方法が必要なとき。</li><li>既存のユーザーがクラスターのセットアップを自動化し、アプリケーションをテストする方法が必要なとき。</li><li>より大きなスコープで、他のエコシステムやインストーラーツールのビルディングブロックが必要なとき。</li></ul><p><code>kubeadm</code>は、ラップトップ、クラウドのサーバー群、Raspberry Piなどの様々なマシンにインストールして使えます。クラウドとオンプレミスのどちらにデプロイする場合でも、<code>kubeadm</code>はAnsibleやTerraformなどのプロビジョニングシステムに統合できます。</p><h2 id=始める前に>始める前に</h2><p>このガイドを進めるには、以下の環境が必要です。</p><ul><li>UbuntuやCentOSなど、deb/rpmパッケージと互換性のあるLinux OSが動作している1台以上のマシンがあること。</li><li>マシンごとに2GiB以上のRAMが搭載されていること。それ以下の場合、アプリ実行用のメモリーがほとんど残りません。</li><li>コントロールプレーンノードとして使用するマシンには、最低でも2CPU以上あること。</li><li>クラスター内の全マシン間に完全なネットワーク接続があること。パブリックネットワークとプライベートネットワークのいずれでも使えます。</li></ul><p>また、新しいクラスターで使いたいKubernetesのバージョンをデプロイできるバージョンの<code>kubeadm</code>を使用する必要もあります。</p><p><a href=/ja/docs/setup/release/version-skew-policy/#supported-versions>Kubernetesのバージョンとバージョンスキューポリシー</a>は、<code>kubeadm</code>にもKubernetes全体と同じように当てはまります。Kubernetesと<code>kubeadm</code>がサポートするバージョンを理解するには、上記のポリシーを確認してください。このページは、Kubernetes v1.25向けに書かれています。</p><p>kubeadmツールの全体の機能の状態は、一般利用可能(GA)です。一部のサブ機能はまだ活発に開発が行われています。クラスター作成の実装は、ツールの進化に伴ってわずかに変わるかもしれませんが、全体の実装は非常に安定しているはずです。</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>kubeadm alpha</code>以下のすべてのコマンドは、定義通り、アルファレベルでサポートされています。</div><h2 id=目的>目的</h2><ul><li>シングルコントロールプレーンのKubernetesクラスターをインストールする</li><li>クラスター上にPodネットワークをインストールして、Podがお互いに通信できるようにする</li></ul><h2 id=手順>手順</h2><h3 id=ホストへのkubeadmのインストール>ホストへのkubeadmのインストール</h3><p>「<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>kubeadmのインストール</a>」を読んでください。</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong><p>すでにkubeadmがインストール済みである場合は、最新バージョンのkubeadmを取得するために<code>apt-get update && apt-get upgrade</code>や<code>yum update</code>を実行してください。</p><p>アップグレード中、kubeletが数秒ごとに再起動します。これは、kubeadmがkubeletにするべきことを伝えるまで、crashloopの状態で待機するためです。このcrashloopは期待通りの通常の動作です。コントロールプレーンの初期化が完了すれば、kubeletは正常に動作します。</p></div><h3 id=コントロールプレーンノードの初期化>コントロールプレーンノードの初期化</h3><p>コントロールプレーンノードとは、<a class=glossary-tooltip title=一貫性、高可用性を持ったキーバリューストアで、Kubernetesの全てのクラスター情報の保存場所として利用されています。 data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>(クラスターのデータベース)や<a class=glossary-tooltip title='Kubernetes APIを提供するコントロールプレーンのコンポーネントです。' data-toggle=tooltip data-placement=top href=/ja/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=APIサーバー>APIサーバー</a>(<a class=glossary-tooltip title='A command line tool for communicating with a Kubernetes cluster.' data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a>コマンドラインツールが通信する相手)などのコントロールプレーンのコンポーネントが実行されるマシンです。</p><ol><li>(推奨)シングルコントロールプレーンの<code>kubeadm</code>クラスターを高可用性クラスターにアップグレードする予定がある場合、<code>--control-plane-endpoint</code>を指定して、すべてのコントロールプレーンノードとエンドポイントを共有する必要があります。エンドポイントにはDNSネームやロードバランサーのIPアドレスが使用できます。</li><li>Podネットワークアドオンを選んで、<code>kubeadm init</code>に引数を渡す必要があるかどうか確認してください。選んだサードパーティーのプロバイダーによっては、<code>--pod-network-cidr</code>をプロバイダー固有の値に設定する必要がある場合があります。詳しくは、<a href=#pod-network>Podネットワークアドオンのインストール</a>を参照してください。</li><li>(オプション)バージョン1.14から、<code>kubeadm</code>はよく知られたドメインソケットのパスリストを用いて、Linux上のコンテナランタイムの検出を試みます。異なるコンテナランタイムを使用する場合やプロビジョニングするノードに2つ以上のランタイムがインストールされている場合、<code>kubeadm init</code>に<code>--cri-socket</code>引数を指定してください。詳しくは、<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime>ランタイムのインストール</a>を読んでください。</li><li>(オプション)明示的に指定しない限り、<code>kubeadm</code>はデフォルトゲートウェイに関連付けられたネットワークインターフェイスを使用して、この特定のコントロールプレーンノードのAPIサーバーのadvertise addressを設定します。異なるネットワークインターフェイスを使用するには、<code>kubeadm init</code>に<code>--apiserver-advertise-address=&lt;ip-address></code>引数を指定してください。IPv6アドレスを使用するIPv6 Kubernetesクラスターをデプロイするには、たとえば<code>--apiserver-advertise-address=fd00::101</code>のように、IPv6アドレスを指定する必要があります。</li><li>(オプション)<code>kubeadm init</code>を実行する前に<code>kubeadm config images pull</code>を実行して、gcr.ioコンテナイメージレジストリに接続できるかどうかを確認します。</li></ol><p>コントロールプレーンノードを初期化するには、次のコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init &lt;args&gt;
</span></span></code></pre></div><h3 id=apiserver-advertise-addressとcontrolplaneendpointに関する検討>apiserver-advertise-addressとControlPlaneEndpointに関する検討</h3><p><code>--apiserver-advertise-address</code>は、この特定のコントロールプレーンノードのAPIサーバーへのadvertise addressを設定するために使えますが、<code>--control-plane-endpoint</code>は、すべてのコントロールプレーンノード共有のエンドポイントを設定するために使えます。</p><p><code>--control-plane-endpoint</code>はIPアドレスと、IPアドレスへマッピングできるDNS名を使用できます。利用可能なソリューションをそうしたマッピングの観点から評価するには、ネットワーク管理者に相談してください。</p><p>以下にマッピングの例を示します。</p><pre tabindex=0><code>192.168.0.102 cluster-endpoint
</code></pre><p>ここでは、<code>192.168.0.102</code>がこのノードのIPアドレスであり、<code>cluster-endpoint</code>がこのIPアドレスへとマッピングされるカスタムDNSネームです。このように設定することで、<code>--control-plane-endpoint=cluster-endpoint</code>を<code>kubeadm init</code>に渡せるようになり、<code>kubeadm join</code>にも同じDNSネームを渡せます。後で<code>cluster-endpoint</code>を修正して、高可用性が必要なシナリオでロードバランサーのアドレスを指すようにすることができます。</p><p>kubeadmでは、<code>--control-plane-endpoint</code>を渡さずに構築したシングルコントロールプレーンのクラスターを高可用性クラスターに切り替えることはサポートされていません。</p><h3 id=詳細な情報>詳細な情報</h3><p><code>kubeadm init</code>の引数のより詳細な情報は、<a href=/docs/reference/setup-tools/kubeadm/kubeadm/>kubeadmリファレンスガイド</a>を参照してください。</p><p>設定オプションの全リストは、<a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>設定ファイルのドキュメント</a>で確認できます。</p><p>コントロールプレーンコンポーネントやetcdサーバーのliveness probeへのオプションのIPv6の割り当てなど、コントロールプレーンのコンポーネントをカスタマイズしたい場合は、<a href=/ja/docs/setup/production-environment/tools/kubeadm/control-plane-flags/>カスタムの引数</a>に示されている方法で各コンポーネントに追加の引数を与えてください。</p><p><code>kubeadm init</code>を再び実行する場合は、初めに<a href=#tear-down>クラスターの破壊</a>を行う必要があります。</p><p>もし異なるアーキテクチャのノードをクラスターにjoinさせたい場合は、デプロイしたDaemonSetがそのアーキテクチャ向けのコンテナイメージをサポートしているか確認してください。</p><p>初めに<code>kubeadm init</code>は、マシンがKubernetesを実行する準備ができているかを確認する、一連の事前チェックを行います。これらの事前チェックはエラー発生時には警告を表示して終了します。次に、<code>kubeadm init</code>はクラスターのコントロールプレーンのコンポーネントをダウンロードしてインストールします。これには数分掛かるかもしれません。出力は次のようになります。</p><pre tabindex=0><code class=language-none data-lang=none>[init] Using Kubernetes version: vX.Y.Z
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[kubelet-start] Writing kubelet environment file with flags to file &#34;/var/lib/kubelet/kubeadm-flags.env&#34;
[kubelet-start] Writing kubelet configuration to file &#34;/var/lib/kubelet/config.yaml&#34;
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder &#34;/etc/kubernetes/pki&#34;
[certs] Generating &#34;etcd/ca&#34; certificate and key
[certs] Generating &#34;etcd/server&#34; certificate and key
[certs] etcd/server serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating &#34;etcd/healthcheck-client&#34; certificate and key
[certs] Generating &#34;etcd/peer&#34; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating &#34;apiserver-etcd-client&#34; certificate and key
[certs] Generating &#34;ca&#34; certificate and key
[certs] Generating &#34;apiserver&#34; certificate and key
[certs] apiserver serving cert is signed for DNS names [kubeadm-cp kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4]
[certs] Generating &#34;apiserver-kubelet-client&#34; certificate and key
[certs] Generating &#34;front-proxy-ca&#34; certificate and key
[certs] Generating &#34;front-proxy-client&#34; certificate and key
[certs] Generating &#34;sa&#34; key and public key
[kubeconfig] Using kubeconfig folder &#34;/etc/kubernetes&#34;
[kubeconfig] Writing &#34;admin.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;kubelet.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;controller-manager.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;scheduler.conf&#34; kubeconfig file
[control-plane] Using manifest folder &#34;/etc/kubernetes/manifests&#34;
[control-plane] Creating static Pod manifest for &#34;kube-apiserver&#34;
[control-plane] Creating static Pod manifest for &#34;kube-controller-manager&#34;
[control-plane] Creating static Pod manifest for &#34;kube-scheduler&#34;
[etcd] Creating static Pod manifest for local etcd in &#34;/etc/kubernetes/manifests&#34;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &#34;/etc/kubernetes/manifests&#34;. This can take up to 4m0s
[apiclient] All control plane components are healthy after 31.501735 seconds
[uploadconfig] storing the configuration used in ConfigMap &#34;kubeadm-config&#34; in the &#34;kube-system&#34; Namespace
[kubelet] Creating a ConfigMap &#34;kubelet-config-X.Y&#34; in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information &#34;/var/run/dockershim.sock&#34; to the Node API object &#34;kubeadm-cp&#34; as an annotation
[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the label &#34;node-role.kubernetes.io/master=&#39;&#39;&#34;
[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: &lt;token&gt;
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the &#34;cluster-info&#34; ConfigMap in the &#34;kube-public&#34; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  /ja/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><p>kubectlをroot以外のユーザーでも実行できるようにするには、次のコマンドを実行します。これらのコマンドは、<code>kubectl init</code>の出力の中にも書かれています。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p <span style=color:#b8860b>$HOME</span>/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div><p>あなたが<code>root</code>ユーザーである場合は、代わりに次のコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div><p><code>kubeadm init</code>が出力した<code>kubeadm join</code>コマンドをメモしておいてください。<a href=#join-nodes>クラスターにノードを追加する</a>ために、このコマンドが必要になります。</p><p>トークンは、コントロールプレーンノードと追加ノードの間の相互認証に使用します。ここに含まれるトークンには秘密の情報が含まれます。このトークンを知っていれば、誰でもクラスターに認証済みノードを追加できてしまうため、取り扱いには注意してください。<code>kubeadm token</code>コマンドを使用すると、これらのトークンの一覧、作成、削除ができます。詳しくは<a href=/docs/reference/setup-tools/kubeadm/kubeadm-token/>kubeadmリファレンスガイド</a>を読んでください。</p><h3 id=pod-network>Podネットワークアドオンのインストール</h3><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong><p>このセクションには、ネットワークのセットアップとデプロイの順序に関する重要な情報が書かれています。先に進む前に以下のすべてのアドバイスを熟読してください。</p><p><strong>Pod同士が通信できるようにするには、<a class=glossary-tooltip title='Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label='Container Network Interface'>Container Network Interface</a>(CNI)をベースとするPodネットワークアドオンをデプロイしなければなりません。ネットワークアドオンをインストールする前には、Cluster DNS(CoreDNS)は起動しません。</strong></p><ul><li><p>Podネットワークがホストネットワークと決して重ならないように気をつけてください。もし重なると、様々な問題が起こってしまう可能性があります。(ネットワークプラグインが優先するPodネットワークとホストのネットワークの一部が衝突することが分かった場合、適切な代わりのCIDRを考える必要があります。そして、<code>kubeadm init</code>の実行時に<code>--pod-network-cidr</code>にそのCIDRを指定し、ネットワークプラグインのYAMLでは代わりにそのCIDRを使用してください)</p></li><li><p>デフォルトでは、<code>kubeadm</code>は<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>(role based access control)の使用を強制します。PodネットワークプラグインがRBACをサポートしていて、またそのデプロイに使用するマニフェストもRBACをサポートしていることを確認してください。</p></li><li><p>クラスターでIPv6を使用したい場合、デュアルスタック、IPv6のみのシングルスタックのネットワークのいずれであっても、PodネットワークプラグインがIPv6をサポートしていることを確認してください。IPv6のサポートは、CNIの<a href=https://github.com/containernetworking/cni/releases/tag/v0.6.0>v0.6.0</a>で追加されました。</p></li></ul></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> 現在、Calicoはkubeadmプロジェクトがe2eテストを実施している唯一のCNIプラグインです。
もしCNIプラグインに関する問題を見つけた場合、kubeadmやkubernetesではなく、そのCNIプラグインの課題管理システムへ問題を報告してください。</div><p>CNIを使用するKubernetes Podネットワークを提供する外部のプロジェクトがいくつかあります。一部のプロジェクトでは、<a href=/ja/docs/concepts/services-networking/network-policies/>ネットワークポリシー</a>もサポートしています。</p><p><a href=/ja/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>Kubernetesのネットワークモデル</a>を実装したアドオンの一覧も確認してください。</p><p>Podネットワークアドオンをインストールするには、コントロールプレーンノード上またはkubeconfigクレデンシャルを持っているノード上で、次のコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f &lt;add-on.yaml&gt;
</span></span></code></pre></div><p>インストールできるPodネットワークは、クラスターごとに1つだけです。</p><p>Podネットワークがインストールされたら、<code>kubectl get pods --all-namespaces</code>の出力結果でCoreDNS Podが<code>Running</code>状態であることをチェックすることで、ネットワークが動作していることを確認できます。そして、一度CoreDNS Podが動作すれば、続けてノードを追加できます。</p><p>もしネットワークやCoreDNSが<code>Running</code>状態にならない場合は、<code>kubeadm</code>の<a href=/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>トラブルシューティングガイド</a>をチェックしてください。</p><h3 id=コントロールプレーンノードの隔離>コントロールプレーンノードの隔離</h3><p>デフォルトでは、セキュリティ上の理由により、クラスターはコントロールプレーンノードにPodをスケジューリングしません。たとえば、開発用のKubernetesシングルマシンのクラスターなどで、Podをコントロールプレーンノードにスケジューリングしたい場合は、次のコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl taint nodes --all node-role.kubernetes.io/master-
</span></span></code></pre></div><p>出力は次のようになります。</p><pre tabindex=0><code>node &#34;test-01&#34; untainted
taint &#34;node-role.kubernetes.io/master:&#34; not found
taint &#34;node-role.kubernetes.io/master:&#34; not found
</code></pre><p>このコマンドは、コントロールプレーンノードを含むすべてのノードから<code>node-role.kubernetes.io/master</code>taintを削除します。その結果、スケジューラーはどこにでもPodをスケジューリングできるようになります。</p><h3 id=join-nodes>ノードの追加</h3><p>ノードは、ワークロード(コンテナやPodなど)が実行される場所です。新しいノードをクラスターに追加するためには、各マシンに対して、以下の手順を実行してください。</p><ul><li>マシンへSSHする</li><li>rootになる(例: <code>sudo su -</code>)</li><li><code>kubeadm init</code>実行時に出力されたコマンドを実行する。たとえば、次のようなコマンドです。</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</span></span></code></pre></div><p>トークンがわからない場合は、コントロールプレーンノードで次のコマンドを実行すると取得できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token list
</span></span></code></pre></div><p>出力は次のようになります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
</span></span></span><span style=display:flex><span><span style=color:#888>8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                   signing          token generated by     bootstrappers:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                    &#39;kubeadm init&#39;.        kubeadm:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                                           default-node-token
</span></span></span></code></pre></div><p>デフォルトでは、トークンは24時間後に有効期限が切れます。もし現在のトークンの有効期限が切れた後にクラスターにノードを参加させたい場合は、コントロールプレーンノードで次のコマンドを実行することで、新しいトークンを生成できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token create
</span></span></code></pre></div><p>このコマンドの出力は次のようになります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>5didvk.d09sbcov8ph2amjw
</span></span></span></code></pre></div><p>もし<code>--discovery-token-ca-cert-hash</code>の値がわからない場合は、コントロールプレーンノード上で次のコマンドチェーンを実行することで取得できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>   openssl dgst -sha256 -hex | sed <span style=color:#b44>&#39;s/^.* //&#39;</span>
</span></span></code></pre></div><p>出力は次のようになります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> IPv6タプルを<code>&lt;control-plane-host>:&lt;control-plane-port></code>と指定するためには、IPv6アドレスを角括弧で囲みます。たとえば、<code>[fd00::101]:2073</code>のように書きます。</div><p>出力は次のようになります。</p><pre tabindex=0><code>[preflight] Running pre-flight checks

... (joinワークフローのログ出力) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run &#39;kubectl get nodes&#39; on control-plane to see this machine join.
</code></pre><p>数秒後、コントロールプレーンノード上で<code>kubectl get nodes</code>を実行すると、出力内にこのノードが表示されるはずです。</p><h3 id=オプション-コントロールプレーンノード以外のマシンからのクラスター操作>(オプション)コントロールプレーンノード以外のマシンからのクラスター操作</h3><p>他のコンピューター(例: ラップトップ)上のkubectlがクラスターと通信できるようにするためには、次のようにして、administratorのkubeconfigファイルをコントロールプレーンノードからそのコンピューター上にコピーする必要があります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf get nodes
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong><p>上の例では、rootユーザーに対するSSH接続が有効であることを仮定しています。もしそうでない場合は、<code>admin.conf</code>ファイルを誰か他のユーザーからアクセスできるようにコピーした上で、代わりにそのユーザーを使って<code>scp</code>してください。</p><p><code>admin.conf</code>ファイルはユーザーにクラスターに対する <em>特権ユーザー</em> の権限を与えます。そのため、このファイルを使うのは控えめにしなければなりません。通常のユーザーには、明示的に許可した権限を持つユニークなクレデンシャルを生成することを推奨します。これには、<code>kubeadm alpha kubeconfig user --client-name &lt;CN></code>コマンドが使えます。このコマンドを実行すると、KubeConfigファイルがSTDOUTに出力されるので、ファイルに保存してユーザーに配布します。その後、<code>kubectl create (cluster)rolebinding</code>コマンドを使って権限を付与します。</p></div><h3 id=オプション-apiサーバーをlocalhostへプロキシする>(オプション)APIサーバーをlocalhostへプロキシする</h3><p>クラスターの外部からAPIサーバーに接続したいときは、次のように<code>kubectl proxy</code>コマンドが使えます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf proxy
</span></span></code></pre></div><p>これで、ローカルの<code>http://localhost:8001/api/v1</code>からAPIサーバーにアクセスできるようになります。</p><h2 id=tear-down>クリーンアップ</h2><p>テストのためにクラスターに破棄可能なサーバーを使用した場合、サーバーのスイッチをオフにすれば、以降のクリーンアップの作業は必要ありません。クラスターのローカルの設定を削除するには、<code>kubectl config delete-cluster</code>を実行します。</p><p>しかし、もしよりきれいにクラスターのプロビジョンをもとに戻したい場合は、初めに<a href=/docs/reference/generated/kubectl/kubectl-commands#drain>ノードのdrain</a>を行い、ノードが空になっていることを確認した後、ノードの設定を削除する必要があります。</p><h3 id=ノードの削除>ノードの削除</h3><p>適切なクレデンシャルを使用してコントロールプレーンノードに削除することを伝えます。次のコマンドを実行してください。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets
</span></span></code></pre></div><p>ノードが削除される前に、<code>kubeadm</code>によってインストールされた状態をリセットします。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm reset
</span></span></code></pre></div><p>リセットプロセスでは、iptablesのルールやIPVS tablesのリセットやクリーンアップは行われません。iptablesをリセットしたい場合は、次のように手動でコマンドを実行する必要があります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>iptables -F <span style=color:#666>&amp;&amp;</span> iptables -t nat -F <span style=color:#666>&amp;&amp;</span> iptables -t mangle -F <span style=color:#666>&amp;&amp;</span> iptables -X
</span></span></code></pre></div><p>IPVS tablesをリセットしたい場合は、次のコマンドを実行する必要があります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -C
</span></span></code></pre></div><p>ノードを削除します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete node &lt;node name&gt;
</span></span></code></pre></div><p>クラスターのセットアップを最初から始めたいときは、<code>kubeadm init</code>や<code>kubeadm join</code>を適切な引数を付けて実行すればいいだけです。</p><h3 id=コントロールプレーンのクリーンアップ>コントロールプレーンのクリーンアップ</h3><p>コントロールホスト上で<code>kubeadm reset</code>を実行すると、ベストエフォートでのクリーンアップが実行できます。</p><p>このサブコマンドとオプションに関するより詳しい情報は、<a href=/docs/reference/setup-tools/kubeadm/kubeadm-reset/><code>kubeadm reset</code></a>リファレンスドキュメントを読んでください。</p><h2 id=whats-next>次の手順</h2><ul><li><a href=https://github.com/heptio/sonobuoy>Sonobuoy</a>を使用してクラスターが適切に動作しているか検証する。</li><li><a id=lifecycle><code>kubeadm</code>を使用したクラスターをアップグレードする方法について、<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>kubeadmクラスターをアップグレードする</a>を読む。</li><li><code>kubeadm</code>の高度な利用方法について<a href=/docs/reference/setup-tools/kubeadm/kubeadm>kubeadmリファレンスドキュメント</a>で学ぶ。</li><li>Kubernetesの<a href=/ja/docs/concepts/>コンセプト</a>や<a href=/ja/docs/reference/kubectl/overview/><code>kubectl</code></a>についてもっと学ぶ。</li><li>Podネットワークアドオンのより完全なリストを<a href=/ja/docs/concepts/cluster-administration/networking/>クラスターのネットワーク</a>で確認する。</li><li><a id=other-addons>ロギング、モニタリング、ネットワークポリシー、仮想化、Kubernetesクラスターの制御のためのツールなど、その他のアドオンについて、<a href=/ja/docs/concepts/cluster-administration/addons/>アドオンのリスト</a>で確認する。</li><li>クラスターイベントやPod内で実行中のアプリケーションから送られるログをクラスターがハンドリングする方法を設定する。関係する要素の概要を理解するために、<a href=/docs/concepts/cluster-administration/logging/>ロギングのアーキテクチャ</a>を読んでください。</li></ul><h3 id=feedback>フィードバック</h3><ul><li>バグを見つけた場合は、<a href=https://github.com/kubernetes/kubeadm/issues>kubeadm GitHub issue tracker</a>で報告してください。</li><li>サポートを受けたい場合は、<a href=https://kubernetes.slack.com/messages/kubeadm/>#kubeadm</a>Slackチャンネルを訪ねてください。</li><li>General SIG Cluster Lifecycle development Slackチャンネル:
<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle/>#sig-cluster-lifecycle</a></li><li>SIG Cluster Lifecycle <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme>SIG information</a></li><li>SIG Cluster Lifecycleメーリングリスト:
<a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle>kubernetes-sig-cluster-lifecycle</a></li></ul><h2 id=version-skew-policy>バージョン互換ポリシー</h2><p>バージョンv1.25の<code>kubeadm</code>ツールは、バージョンv1.25またはv1.24のコントロールプレーンを持つクラスターをデプロイできます。また、バージョンv1.25の<code>kubeadm</code>は、バージョンv1.24のkubeadmで構築されたクラスターをアップグレートできます。</p><p>未来を見ることはできないため、kubeadm CLI v1.25はv1.26をデプロイできないかもしれません。</p><p>例: <code>kubeadm</code> v1.8は、v1.7とv1.8のクラスターをデプロイでき、v1.7のkubeadmで構築されたクラスターをv1.8にアップグレートできます。</p><p>kubeletとコントロールプレーンの間や、他のKubernetesコンポーネント間のバージョンの差異に関する詳しい情報は、以下の資料を確認してください。</p><ul><li>Kubernetes<a href=/ja/docs/setup/release/version-skew-policy/>バージョンスキューサポートポリシー</a></li><li>Kubeadm特有の<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl>インストールガイド</a></li></ul><h2 id=limitations>制限事項</h2><h3 id=resilience>クラスターのレジリエンス</h3><p>ここで作られたクラスターは、1つのコントロールプレーンノードと、その上で動作する1つのetcdデータベースしか持ちません。つまり、コントロールプレーンノードが故障した場合、クラスターのデータは失われ、クラスターを最初から作り直す必要があるかもしれないということです。</p><p>対処方法:</p><ul><li><p>定期的に<a href=https://coreos.com/etcd/docs/latest/admin_guide.html>etcdをバックアップ</a>する。kubeadmが設定するetcdのデータディレクトリは、コントロールプレーンノードの<code>/var/lib/etcd</code>にあります。</p></li><li><p>複数のコントロールプレーンノードを使用する。<a href=/ja/docs/setup/production-environment/tools/kubeadm/ha-topology/>高可用性トポロジーのオプション</a>では、<a href=/ja/docs/setup/production-environment/tools/kubeadm/high-availability/>より高い可用性</a>を提供するクラスターのトポロジーの選択について説明してます。</p></li></ul><h3 id=multi-platform>プラットフォームの互換性</h3><p>kubeadmのdeb/rpmパッケージおよびバイナリは、<a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md>multi-platform proposal</a>に従い、amd64、arm(32ビット)、arm64、ppc64le、およびs390x向けにビルドされています。</p><p>マルチプラットフォームのコントロールプレーンおよびアドオン用のコンテナイメージも、v1.12からサポートされています。</p><p>すべてのプラットフォーム向けのソリューションを提供しているネットワークプロバイダーは一部のみです。それぞれのプロバイダーが選択したプラットフォームをサポートしているかどうかを確認するには、前述のネットワークプロバイダーのリストを参照してください。</p><h2 id=troubleshooting>トラブルシューティング</h2><p>kubeadmに関する問題が起きたときは、<a href=/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>トラブルシューティングドキュメント</a>を確認してください。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c656c5eda3e1c06ad1aedebdc04a211>2.1.4 - kubeadmを使ったコントロールプレーンの設定のカスタマイズ</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.12 [stable]</code></div><p>kubeadmの<code>ClusterConfiguration</code>オブジェクトはAPIServer、ControllerManager、およびSchedulerのようなコントロールプレーンの構成要素に渡されたデフォルトのフラグを上書きすることができる <code>extraArgs</code>の項目があります。
その構成要素は次の項目で定義されています。</p><ul><li><code>apiServer</code></li><li><code>controllerManager</code></li><li><code>scheduler</code></li></ul><p><code>extraArgs</code> の項目は <code>キー: 値</code> のペアです。コントロールプレーンの構成要素のフラグを上書きするには:</p><ol><li>設定内容に適切な項目を追加</li><li>フラグを追加して項目を上書き</li><li><code>--config &lt;任意の設定YAMLファイル></code>で<code>kubeadm init</code>を実行</li></ol><p>各設定項目のより詳細な情報は<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm#ClusterConfiguration>APIリファレンスのページ</a>を参照してください。</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>kubeadm config print init-defaults</code>を実行し、選択したファイルに出力を保存することで、デフォルト値で<code>ClusterConfiguration</code>オブジェクトを生成できます。</div><h2 id=apiserverフラグ>APIServerフラグ</h2><p>詳細は<a href=/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserverのリファレンスドキュメント</a>を参照してください。</p><p>使用例:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>advertise-address</span>:<span style=color:#bbb> </span><span style=color:#666>192.168.0.103</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>anonymous-auth</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;false&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>enable-admission-plugins</span>:<span style=color:#bbb> </span>AlwaysPullImages,DefaultStorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>audit-log-path</span>:<span style=color:#bbb> </span>/home/johndoe/audit.log<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=controllermanagerフラグ>ControllerManagerフラグ</h2><p>詳細は<a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-managerのリファレンスドキュメント</a>を参照してください。</p><p>使用例:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-key-file</span>:<span style=color:#bbb> </span>/home/johndoe/keys/ca.key<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deployment-controller-sync-period</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;50&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=schedulerフラグ>Schedulerフラグ</h2><p>詳細は<a href=/docs/reference/command-line-tools-reference/kube-scheduler/>kube-schedulerのリファレンスドキュメント</a>を参照してください。</p><p>使用例:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduler</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>config</span>:<span style=color:#bbb> </span>/home/johndoe/schedconfig.yaml<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubeconfig</span>:<span style=color:#bbb> </span>/home/johndoe/kubeconfig.yaml<span style=color:#bbb>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-015edbc7cc688d31b1d1edce7c186135>2.1.5 - 高可用性トポロジーのためのオプション</h1><p>このページでは、高可用性(HA)Kubernetesクラスターのトポロジーを設定するための2つのオプションについて説明します。</p><p>HAクラスターは次の方法で設定できます。</p><ul><li>積層コントロールプレーンノードを使用する方法。こちらの場合、etcdノードはコントロールプレーンノードと同じ場所で動作します。</li><li>外部のetcdノードを使用する方法。こちらの場合、etcdがコントロールプレーンとは分離されたノードで動作します。</li></ul><p>HAクラスターをセットアップする前に、各トポロジーの利点と欠点について注意深く考慮する必要があります。</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> kubeadmは、etcdクラスターを静的にブートストラップします。
詳細については、etcd<a href=https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static>クラスタリングガイド</a>をご覧ください。</div><h2 id=積層etcdトポロジー>積層etcdトポロジー</h2><p>積層HAクラスターは、コントロールプレーンのコンポーネントを実行する、kubeadmで管理されたノードで構成されるクラスターの上に、etcdにより提供される分散データストレージクラスターがあるような<a href=https://en.wikipedia.org/wiki/Network_topology>トポロジー</a>です。</p><p>各コントロールプレーンノードは、<code>kube-apiserver</code>、<code>kube-scheduler</code>、および<code>kube-controller-manager</code>を実行します。<code>kube-apiserver</code> はロードバランサーを用いてワーカーノードに公開されます。</p><p>各コントロールプレーンノードはローカルのetcdメンバーを作り、このetcdメンバーはそのノードの<code>kube-apiserver</code>とだけ通信します。ローカルの<code>kube-controller-manager</code>と<code>kube-scheduler</code>のインスタンスも同様です。</p><p>このトポロジーは、同じノード上のコントロールプレーンとetcdのメンバーを結合します。外部のetcdノードを使用するクラスターよりはセットアップがシンプルで、レプリケーションの管理もシンプルです。</p><p>しかし、積層クラスターには、結合による故障のリスクがあります。1つのノードがダウンすると、etcdメンバーとコントロールプレーンのインスタンスの両方が失われ、冗長性が損なわれます。より多くのコントロールプレーンノードを追加することで、このリスクは緩和できます。</p><p>そのため、HAクラスターのためには、最低でも3台の積層コントロールプレーンノードを実行しなければなりません。</p><p>これがkubeadmのデフォルトのトポロジーです。<code>kubeadm init</code>や<code>kubeadm join --control-place</code>を実行すると、ローカルのetcdメンバーがコントロールプレーンノード上に自動的に作成されます。</p><p><img src=/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg alt=積層etcdトポロジー></p><h2 id=外部のetcdトポロジー>外部のetcdトポロジー</h2><p>外部のetcdを持つHAクラスターは、コントロールプレーンコンポーネントを実行するノードで構成されるクラスターの外部に、etcdにより提供される分散データストレージクラスターがあるような<a href=https://en.wikipedia.org/wiki/Network_topology>トポロジー</a>です。</p><p>積層etcdトポロジーと同様に、外部のetcdトポロジーにおける各コントロールプレーンノードは、<code>kube-apiserver</code>、<code>kube-scheduler</code>、および<code>kube-controller-manager</code>のインスタンスを実行します。そして、<code>kube-apiserver</code>は、ロードバランサーを使用してワーカーノードに公開されます。しかし、etcdメンバーは異なるホスト上で動作しており、各etcdホストは各コントロールプレーンノードの<code>kube-api-server</code>と通信します。</p><p>このトポロジーは、コントロールプレーンとetcdメンバーを疎結合にします。そのため、コントロールプレーンインスタンスまたはetcdメンバーを失うことによる影響は少なく、積層HAトポロジーほどクラスターの冗長性に影響しないHAセットアップが実現します。</p><p>しかし、このトポロジーでは積層HAトポロジーの2倍の数のホストを必要とします。このトポロジーのHAクラスターのためには、最低でもコントロールプレーンのために3台のホストが、etcdノードのために3台のホストがそれぞれ必要です。</p><p><img src=/images/kubeadm/kubeadm-ha-topology-external-etcd.svg alt=外部のetcdトポロジー></p><h2 id=次の項目>次の項目</h2><ul><li><a href=/ja/docs/setup/production-environment/tools/kubeadm/high-availability/>kubeadmを使用した高可用性クラスターの作成</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3941d5c3409342219bf7e03128b8ecb6>2.1.6 - kubeadmを使用した高可用性クラスターの作成</h1><p>このページでは、kubeadmを使用して、高可用性クラスターを作成する、2つの異なるアプローチを説明します:</p><ul><li>積層コントロールプレーンノードを使う方法。こちらのアプローチは、必要なインフラストラクチャーが少ないです。etcdのメンバーと、コントロールプレーンノードは同じ場所に置かれます。</li><li>外部のetcdクラスターを使う方法。こちらのアプローチには、より多くのインフラストラクチャーが必要です。コントロールプレーンノードと、etcdのメンバーは分離されます。</li></ul><p>先へ進む前に、どちらのアプローチがアプリケーションの要件と、環境に適合するか、慎重に検討してください。<a href=/ja/docs/setup/production-environment/tools/kubeadm/ha-topology/>こちらの比較</a>が、それぞれの利点/欠点について概説しています。</p><p>高可用性クラスターの作成で問題が発生した場合は、kueadmの<a href=https://github.com/kubernetes/kubeadm/issues/new>issue tracker</a>でフィードバックを提供してください。</p><p><a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>高可用性クラスターのアップグレード</a>も参照してください。</p><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> このページはクラウド上でクラスターを構築することには対応していません。ここで説明されているどちらのアプローチも、クラウド上で、LoadBalancerタイプのServiceオブジェクトや、動的なPersistentVolumeを利用して動かすことはできません。</div><h2 id=始める前に>始める前に</h2><p>どちらの方法でも、以下のインフラストラクチャーが必要です:</p><ul><li>master用に、<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%A7%8B%E3%82%81%E3%82%8B%E5%89%8D%E3%81%AB>kubeadmの最小要件</a>を満たす3台のマシン</li><li>worker用に、<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%A7%8B%E3%82%81%E3%82%8B%E5%89%8D%E3%81%AB>kubeadmの最小要件</a>を満たす3台のマシン</li><li>クラスター内のすべてのマシン間がフルにネットワーク接続可能であること(パブリック、もしくはプライベートネットワーク)</li><li>すべてのマシンにおいて、sudo権限</li><li>あるデバイスから、システム内のすべてのノードに対しSSH接続できること</li><li><code>kubeadm</code>と<code>kubelet</code>がすべてのマシンにインストールされていること。 <code>kubectl</code>は任意です。</li></ul><p>外部etcdクラスターには、以下も必要です:</p><ul><li>etcdメンバー用に、追加で3台のマシン</li></ul><h2 id=両手順における最初のステップ>両手順における最初のステップ</h2><h3 id=kube-apiserver用にロードバランサーを作成>kube-apiserver用にロードバランサーを作成</h3><div class="alert alert-info note callout" role=alert><strong>備考:</strong> ロードバランサーには多くの設定項目があります。以下の例は、一選択肢に過ぎません。あなたのクラスター要件には、異なった設定が必要かもしれません。</div><ol><li><p>DNSで解決される名前で、kube-apiserver用ロードバランサーを作成する。</p><ul><li><p>クラウド環境では、コントロールプレーンノードをTCPフォワーディングロードバランサーの後ろに置かなければなりません。このロードバランサーはターゲットリストに含まれる、すべての健全なコントロールプレーンノードにトラフィックを分配します。apiserverへのヘルスチェックはkube-apiserverがリッスンするポート(デフォルト値: <code>:6443</code>)に対する、TCPチェックです。</p></li><li><p>クラウド環境では、IPアドレスを直接使うことは推奨されません。</p></li><li><p>ロードバランサーは、apiserverポートで、全てのコントロールプレーンノードと通信できなければなりません。また、リスニングポートに対する流入トラフィックも許可されていなければなりません。</p></li><li><p>ロードバランサーのアドレスは、常にkubeadmの<code>ControlPlaneEndpoint</code>のアドレスと一致することを確認してください。</p></li><li><p>詳細は<a href=https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#options-for-software-load-balancing>Options for Software Load Balancing</a>をご覧ください。</p></li></ul></li><li><p>ロードバランサーに、最初のコントロールプレーンノードを追加し、接続をテストする:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>nc -v LOAD_BALANCER_IP PORT
</span></span></code></pre></div><ul><li>apiserverはまだ動いていないので、接続の拒否は想定通りです。しかし、タイムアウトしたのであれば、ロードバランサーはコントロールプレーンノードと通信できなかったことを意味します。もし、タイムアウトが起きたら、コントロールプレーンノードと通信できるように、ロードバランサーを再設定してください。</li></ul></li><li><p>残りのコントロールプレーンノードを、ロードバランサーのターゲットグループに追加します。</p></li></ol><h2 id=積層コントロールプレーンとetcdノード>積層コントロールプレーンとetcdノード</h2><h3 id=最初のコントロールプレーンノードの手順>最初のコントロールプレーンノードの手順</h3><ol><li><p>最初のコントロールプレーンノードを初期化します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm init --control-plane-endpoint <span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span> --upload-certs
</span></span></code></pre></div><ul><li><code>--kubernetes-version</code>フラグで使用するKubernetesのバージョンを設定できます。kubeadm、kubelet、kubectl、Kubernetesのバージョンを一致させることが推奨されます。</li><li><code>--control-plane-endpoint</code>フラグは、ロードバランサーのIPアドレスまたはDNS名と、ポートが設定される必要があります。</li><li><code>--upload-certs</code>フラグは全てのコントロールプレーンノードで共有する必要がある証明書をクラスターにアップロードするために使用されます。代わりに、コントロールプレーンノード間で手動あるいは自動化ツールを使用して証明書をコピーしたい場合は、このフラグを削除し、以下の<a href=#manual-certs>証明書の手動配布</a>のセクションを参照してください。</li></ul><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>kubeadm init</code>の<code>--config</code>フラグと<code>--certificate-key</code>フラグは混在させることはできないため、<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2>kubeadm configuration</a>を使用する場合は<code>certificateKey</code>フィールドを適切な場所に追加する必要があります(<code>InitConfiguration</code>と<code>JoinConfiguration: controlPlane</code>の配下)。</div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> いくつかのCNIネットワークプラグインはPodのIPのCIDRの指定など追加の設定を必要としますが、必要としないプラグインもあります。<a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>CNIネットワークドキュメント</a>を参照してください。PodにCIDRを設定するには、<code>ClusterConfiguration</code>の<code>networking</code>オブジェクトに<code>podSubnet: 192.168.0.0/16</code>フィールドを設定してください。</div><ul><li>このような出力がされます:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>...
</span></span><span style=display:flex><span>You can now join any number of control-plane node by running the following <span style=color:#a2f>command</span> on each as a root:
</span></span><span style=display:flex><span>    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
</span></span><span style=display:flex><span>As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style=display:flex><span>    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><ul><li><p>この出力をテキストファイルにコピーします。あとで、他のコントロールプレーンノードとワーカーノードをクラスターに参加させる際に必要です。</p></li><li><p><code>--upload-certs</code>フラグを<code>kubeadm init</code>で使用すると、プライマリコントロールプレーンの証明書が暗号化されて、<code>kubeadm-certs</code> Secretにアップロードされます。</p></li><li><p>証明書を再アップロードして新しい復号キーを生成するには、すでにクラスターに参加しているコントロールプレーンノードで次のコマンドを使用します:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm init phase upload-certs --upload-certs
</span></span></code></pre></div><ul><li>また、後で<code>join</code>で使用できるように、<code>init</code>中にカスタムした<code>--certificate-key</code>を指定することもできます。このようなキーを生成するには、次のコマンドを使用します:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubeadm alpha certs certificate-key
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>kubeadm-certs</code>のSecretと復号キーは2時間で期限切れとなります。</div><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> コマンド出力に記載されているように、証明書キーはクラスターの機密データへのアクセスを提供します。秘密にしてください！</div></li><li><p>使用するCNIプラグインを適用します:<br><a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>こちらの手順に従い</a>CNIプロバイダーをインストールします。該当する場合は、kubeadmの設定で指定されたPodのCIDRに対応していることを確認してください。</p><p>Weave Netを使用する場合の例:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div></li><li><p>以下のコマンドを入力し、コンポーネントのPodが起動するのを確認します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl get pod -n kube-system -w
</span></span></code></pre></div></li></ol><h3 id=残りのコントロールプレーンノードの手順>残りのコントロールプレーンノードの手順</h3><div class="alert alert-info note callout" role=alert><strong>備考:</strong> kubeadmバージョン1.15以降、複数のコントロールプレーンノードを並行してクラスターに参加させることができます。
このバージョンの前は、最初のノードの初期化が完了した後でのみ、新しいコントロールプレーンノードを順番にクラスターに参加させる必要があります。</div><p>追加のコントロールプレーンノード毎に、以下の手順を行います。</p><ol><li><p><code>kubeadm init</code>を最初のノードで実行した際に取得したjoinコマンドを使って、新しく追加するコントロールプレーンノードで<code>kubeadm join</code>を開始します。このようなコマンドになるはずです:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span></code></pre></div><ul><li><code>--control-plane</code>フラグによって、<code>kubeadm join</code>の実行は新しいコントロールプレーンを作成します。</li><li><code>-certificate-key ...</code>を指定したキーを使って、クラスターの<code>kubeadm-certs</code> Secretからダウンロードされたコントロールプレーンの証明書が復号されます。</li></ul></li></ol><h2 id=外部のetcdノード>外部のetcdノード</h2><p>外部のetcdノードを使ったクラスターの設定は、積層etcdの場合と似ていますが、最初にetcdを設定し、kubeadmの設定ファイルにetcdの情報を渡す必要があります。</p><h3 id=etcdクラスターの構築>etcdクラスターの構築</h3><ol><li><p><a href=/ja/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>こちらの手順</a>にしたがって、etcdクラスターを構築してください。</p></li><li><p><a href=#manual-certs>こちらの手順</a>にしたがって、SSHを構築してください。</p></li><li><p>以下のファイルをクラスター内の任意のetcdノードから最初のコントロールプレーンノードにコピーしてください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#666>=</span><span style=color:#b44>&#34;ubuntu@10.0.0.7&#34;</span>
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span></code></pre></div><ul><li><code>CONTROL_PLANE</code>の値を、最初のコントロールプレーンノードの<code>user@host</code>で置き換えます。</li></ul></li></ol><h3 id=最初のコントロールプレーンノードの構築>最初のコントロールプレーンノードの構築</h3><ol><li><p>以下の内容で、<code>kubeadm-config.yaml</code>という名前の設定ファイルを作成します:</p><pre><code>apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: &quot;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&quot;
etcd:
    external:
        endpoints:
        - https://ETCD_0_IP:2379
        - https://ETCD_1_IP:2379
        - https://ETCD_2_IP:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
</code></pre><div class="alert alert-info note callout" role=alert><strong>備考:</strong> ここで、積層etcdと外部etcdの違いは、外部etcdの構成では<code>etcd</code>の<code>external</code>オブジェクトにetcdのエンドポイントが記述された設定ファイルが必要です。積層etcdトポロジーの場合、これは自動で管理されます。</div><ul><li><p>テンプレート内の以下の変数を、クラスターに合わせて適切な値に置き換えます:</p><ul><li><code>LOAD_BALANCER_DNS</code></li><li><code>LOAD_BALANCER_PORT</code></li><li><code>ETCD_0_IP</code></li><li><code>ETCD_1_IP</code></li><li><code>ETCD_2_IP</code></li></ul></li></ul></li></ol><p>以下の手順は、積層etcdの構築と同様です。</p><ol><li><p><code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code>をこのノードで実行します。</p></li><li><p>表示されたjoinコマンドを、あとで使うためにテキストファイルに書き込みます。</p></li><li><p>使用するCNIプラグインを適用します。以下はWeave CNIの場合です:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div></li></ol><h3 id=残りのコントロールプレーンノードの手順-1>残りのコントロールプレーンノードの手順</h3><p>手順は、積層etcd構築の場合と同じです:</p><ul><li>最初のコントロールプレーンノードが完全に初期化されているのを確認します。</li><li>テキストファイルに保存したjoinコマンドを使って、それぞれのコントロールプレーンノードをクラスターへ参加させます。コントロールプレーンノードは1台ずつクラスターへ参加させるのを推奨します。</li><li><code>--certificate-key</code>で指定する復号キーは、デフォルトで2時間で期限切れになることを忘れないでください。</li></ul><h2 id=コントロールプレーン起動後の共通タスク>コントロールプレーン起動後の共通タスク</h2><h3 id=workerのインストール>workerのインストール</h3><p><code>kubeadm init</code>コマンドから返されたコマンドを利用して、workerノードをクラスターに参加させることが可能です。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><h2 id=manual-certs>証明書の手動配布</h2><p><code>--upload-certs</code>フラグを指定して<code>kubeadm init</code>を実行しない場合、プライマリコントロールプレーンノードから他のコントロールプレーンノードへ証明書を手動でコピーする必要があります。</p><p>コピーを行うには多くの方法があります。次の例では<code>ssh</code>と<code>scp</code>を使用しています。</p><p>1台のマシンから全てのノードをコントロールしたいのであれば、SSHが必要です。</p><ol><li><p>システム内の全ての他のノードにアクセスできるメインデバイスで、ssh-agentを有効にします</p><pre tabindex=0><code>eval $(ssh-agent)
</code></pre></li><li><p>SSHの秘密鍵を、セッションに追加します:</p><pre tabindex=0><code>ssh-add ~/.ssh/path_to_private_key
</code></pre></li><li><p>正常に接続できることを確認するために、ノード間でSSHします。</p><ul><li><p>ノードにSSHする際は、必ず<code>-A</code>フラグをつけます:</p><pre tabindex=0><code>ssh -A 10.0.0.7
</code></pre></li><li><p>ノードでsudoするときは、SSHフォワーディングが動くように、環境変数を引き継ぎます:</p><pre tabindex=0><code>sudo -E -s
</code></pre></li></ul></li><li><p>全てのノードでSSHを設定したら、<code>kubeadm init</code>を実行した後、最初のコントロールノードプレーンノードで次のスクリプトを実行します。このスクリプトは、最初のコントロールプレーンノードから残りのコントロールプレーンノードへ証明書ファイルをコピーします:</p><p>次の例の、<code>CONTROL_PLANE_IPS</code>を他のコントロールプレーンノードのIPアドレスに置き換えます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 環境に合わせる</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#666>=</span><span style=color:#b44>&#34;10.0.0.7 10.0.0.8&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> host in <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#b68;font-weight:700>}</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.pub <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.crt
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic># 外部のetcdノード使用時はこちらのコマンドを実行</span>
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.key
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> 上のリストにある証明書だけをコピーしてください。kubeadmが、参加するコントロールプレーンノード用に、残りの証明書と必要なSANの生成を行います。間違って全ての証明書をコピーしてしまったら、必要なSANがないため、追加ノードの作成は失敗するかもしれません。</div></li><li><p>次に、クラスターに参加させる残りの各コントロールプレーンノードで<code>kubeadm join</code>を実行する前に次のスクリプトを実行する必要があります。このスクリプトは、前の手順でコピーした証明書をホームディレクトリから<code>/etc/kubernetes/pki</code>へ移動します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 環境に合わせる</span>
</span></span><span style=display:flex><span>mkdir -p /etc/kubernetes/pki/etcd
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.pub /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 外部のetcdノード使用時はこちらのコマンドを実行</span>
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</span></span></code></pre></div></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-8160424c22d24f7d2d63c521e107dbf8>2.1.7 - kubeadmを使用した高可用性etcdクラスターの作成</h1><div class="alert alert-info note callout" role=alert><strong>備考:</strong> While kubeadm is being used as the management tool for external etcd nodes
in this guide, please note that kubeadm does not plan to support certificate rotation
or upgrades for such nodes. The long term plan is to empower the tool
<a href=https://github.com/kubernetes-sigs/etcdadm>etcdadm</a> to manage these
aspects.</div><p>Kubeadm defaults to running a single member etcd cluster in a static pod managed
by the kubelet on the control plane node. This is not a high availability setup
as the etcd cluster contains only one member and cannot sustain any members
becoming unavailable. This task walks through the process of creating a high
availability etcd cluster of three members that can be used as an external etcd
when using kubeadm to set up a kubernetes cluster.</p><h2 id=始める前に>始める前に</h2><ul><li>Three hosts that can talk to each other over ports 2379 and 2380. This
document assumes these default ports. However, they are configurable through
the kubeadm config file.</li><li>Each host must <a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>have docker, kubelet, and kubeadm installed</a>.</li><li>Each host should have access to the Kubernetes container image registry (<code>k8s.gcr.io</code>) or list/pull the required etcd image using <code>kubeadm config images list/pull</code>. This guide will setup etcd instances as <a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a> managed by a kubelet.</li><li>Some infrastructure to copy files between hosts. For example <code>ssh</code> and <code>scp</code>
can satisfy this requirement.</li></ul><h2 id=クラスターの構築>クラスターの構築</h2><p>The general approach is to generate all certs on one node and only distribute
the <em>necessary</em> files to the other nodes.</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> kubeadm contains all the necessary cryptographic machinery to generate
the certificates described below; no other cryptographic tooling is required for
this example.</div><ol><li><p>Configure the kubelet to be a service manager for etcd.</p><p>Since etcd was created first, you must override the service priority by creating a new unit file
that has higher precedence than the kubeadm-provided kubelet unit file.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>[Service]
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=
</span></span></span><span style=display:flex><span><span style=color:#b44>#  Replace &#34;systemd&#34; with the cgroup driver of your container runtime. The default value in the kubelet is &#34;cgroupfs&#34;.
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
</span></span></span><span style=display:flex><span><span style=color:#b44>Restart=always
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li><li><p>Create configuration files for kubeadm.</p><p>Generate one kubeadm configuration file for each host that will have an etcd
member running on it using the following script.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#080;font-style:italic># Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST0</span><span style=color:#666>=</span>10.0.0.6
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST1</span><span style=color:#666>=</span>10.0.0.7
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST2</span><span style=color:#666>=</span>10.0.0.8
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Create temp directories to store files that will end up on other hosts.</span>
</span></span><span style=display:flex><span>mkdir -p /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ETCDHOSTS</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAMES</span><span style=color:#666>=(</span><span style=color:#b44>&#34;infra0&#34;</span> <span style=color:#b44>&#34;infra1&#34;</span> <span style=color:#b44>&#34;infra2&#34;</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span>!ETCDHOSTS[@]<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCDHOSTS</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAME</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAMES</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta2&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: ClusterConfiguration
</span></span></span><span style=display:flex><span><span style=color:#b44>etcd:
</span></span></span><span style=display:flex><span><span style=color:#b44>    local:
</span></span></span><span style=display:flex><span><span style=color:#b44>        serverCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        peerCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        extraArgs:
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster-state: new
</span></span></span><span style=display:flex><span><span style=color:#b44>            name: ${NAME}
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            advertise-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-advertise-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div></li><li><p>Generate the certificate authority</p><p>If you already have a CA then the only action that is copying the CA's <code>crt</code> and
<code>key</code> file to <code>/etc/kubernetes/pki/etcd/ca.crt</code> and
<code>/etc/kubernetes/pki/etcd/ca.key</code>. After those files have been copied,
proceed to the next step, "Create certificates for each member".</p><p>If you do not already have a CA then run this command on <code>$HOST0</code> (where you
generated the configuration files for kubeadm).</p><pre tabindex=0><code>kubeadm init phase certs etcd-ca
</code></pre><p>This creates two files</p><ul><li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li><li><code>/etc/kubernetes/pki/etcd/ca.key</code></li></ul></li><li><p>Create certificates for each member</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># cleanup non-reusable certificates</span>
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># No need to move the certs because they are for HOST0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># clean up certs that should not be copied off this host</span>
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span></code></pre></div></li><li><p>Copy certificates and kubeadm configs</p><p>The certificates have been generated and now they must be moved to their
respective hosts.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>scp -r /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>/* <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>:
</span></span><span style=display:flex><span>ssh <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>USER@HOST $ sudo -Es
</span></span><span style=display:flex><span>root@HOST $ chown -R root:root pki
</span></span><span style=display:flex><span>root@HOST $ mv pki /etc/kubernetes/
</span></span></code></pre></div></li><li><p>Ensure all expected files exist</p><p>The complete list of required files on <code>$HOST0</code> is:</p><pre tabindex=0><code>/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>On <code>$HOST1</code>:</p><pre tabindex=0><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>On <code>$HOST2</code></p><pre tabindex=0><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre></li><li><p>Create the static pod manifests</p><p>Now that the certificates and configs are in place it's time to create the
manifests. On each host run the <code>kubeadm</code> command to generate a static manifest
for etcd.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>root@HOST0 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>root@HOST1 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>root@HOST2 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span></code></pre></div></li><li><p>Optional: Check the cluster health</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>docker run --rm -it <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--net host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCD_TAG</span><span style=color:#b68;font-weight:700>}</span> etcdctl <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--cert /etc/kubernetes/pki/etcd/peer.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--key /etc/kubernetes/pki/etcd/peer.key <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--cacert /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--endpoints https://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>:2379 endpoint health --cluster
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST0 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 16.283339ms
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST1 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 19.44402ms
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST2 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 35.926451ms
</span></span></code></pre></div><ul><li>Set <code>${ETCD_TAG}</code> to the version tag of your etcd image. For example <code>3.4.3-0</code>. To see the etcd image and tag that kubeadm uses execute <code>kubeadm config images list --kubernetes-version ${K8S_VERSION}</code>, where <code>${K8S_VERSION}</code> is for example <code>v1.17.0</code></li><li>Set <code>${HOST0}</code>to the IP address of the host you are testing.</li></ul></li></ol><h2 id=次の項目>次の項目</h2><p>Once you have a working 3 member etcd cluster, you can continue setting up a
highly available control plane using the <a href=/ja/docs/setup/production-environment/tools/kubeadm/high-availability/>external etcd method with
kubeadm</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-07709e71de6b4ac2573041c31213dbeb>2.1.8 - kubeadmを使用したクラスター内の各kubeletの設定</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.11 [stable]</code></div><p>kubeadm CLIツールのライフサイクルは、Kubernetesクラスター内の各ノード上で稼働するデーモンである<a href=/docs/reference/command-line-tools-reference/kubelet>kubelet</a>から分離しています。kubeadm CLIツールはKubernetesを初期化またはアップグレードする際にユーザーによって実行されます。一方で、kubeletは常にバックグラウンドで稼働しています。</p><p>kubeletはデーモンのため、何らかのinitシステムやサービスマネージャーで管理する必要があります。DEBパッケージやRPMパッケージからkubeletをインストールすると、systemdはkubeletを管理するように設定されます。代わりに別のサービスマネージャーを使用することもできますが、手動で設定する必要があります。</p><p>いくつかのkubeletの設定は、クラスターに含まれる全てのkubeletで同一である必要があります。一方で、特定のマシンの異なる特性(OS、ストレージ、ネットワークなど)に対応するために、kubeletごとに設定が必要なものもあります。手動で設定を管理することも可能ですが、kubeadmは<a href=#configure-kubelets-using-kubeadm>一元的な設定管理</a>のための<code>KubeletConfiguration</code>APIを提供しています。</p><h2 id=kubeletの設定パターン>Kubeletの設定パターン</h2><p>以下のセクションでは、kubeadmを使用したkubeletの設定パターンについて説明します。これは手動で各Nodeの設定を管理するよりも簡易に行うことができます。</p><h3 id=propagating-cluster-level-configuration-to-each-kubelet>各kubeletにクラスターレベルの設定を配布</h3><p><code>kubeadm init</code>および<code>kubeadm join</code>コマンドを使用すると、kubeletにデフォルト値を設定することができます。興味深い例として、異なるCRIランタイムを使用したり、Serviceが使用するデフォルトのサブネットを設定したりすることができます。</p><p>Serviceが使用するデフォルトのサブネットとして<code>10.96.0.0/12</code>を設定する必要がある場合は、<code>--service-cidr</code>パラメーターを渡します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init --service-cidr 10.96.0.0/12
</span></span></code></pre></div><p>これによってServiceの仮想IPはこのサブネットから割り当てられるようになりました。また、<code>--cluster-dns</code>フラグを使用し、kubeletが用いるDNSアドレスを設定する必要もあります。この設定はクラスター内の全てのマネージャーとNode上で同一である必要があります。kubeletは、<strong>kubeletのComponentConfig</strong>と呼ばれる、バージョン管理と構造化されたAPIオブジェクトを提供します。これはkubelet内のほとんどのパラメーターを設定し、その設定をクラスター内で稼働中の各kubeletへ適用することを可能にします。以下の例のように、キャメルケースのキーに値のリストとしてクラスターDNS IPアドレスなどのフラグを指定することができます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusterDNS</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:#666>10.96.0.10</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>ComponentConfigの詳細については、<a href=#configure-kubelets-using-kubeadm>このセクション</a>をご覧ください</p><h3 id=providing-instance-specific-configuration-details>インスタンス固有の設定内容を適用</h3><p>いくつかのホストでは、ハードウェア、オペレーティングシステム、ネットワーク、その他ホスト固有のパラメータの違いのため、特定のkubeletの設定を必要とします。以下にいくつかの例を示します。</p><ul><li>DNS解決ファイルへのパスは<code>--resolv-conf</code>フラグで指定することができますが、オペレーティングシステムや<code>systemd-resolved</code>を使用するかどうかによって異なる場合があります。このパスに誤りがある場合、そのNode上でのDNS解決は失敗します。</li><li>クラウドプロバイダーを使用していない場合、Node APIオブジェクト<code>.metadata.name</code>はデフォルトでマシンのホスト名に設定されます。異なるNode名を指定する必要がある場合には、<code>--hostname-override</code>フラグによってこの挙動を書き換えることができます。</li><li>現在のところ、kubletはCRIランタイムが使用するcgroupドライバを自動で検知することができませんが、kubeletの稼働を保証するためには、<code>--cgroup-driver</code>の値はCRIランタイムが使用するcgroupドライバに一致していなければなりません。</li><li>クラスターが使用するCRIランタイムによっては、異なるフラグを指定する必要があるかもしれません。例えば、Dockerを使用している場合には、<code>--network-plugin=cni</code>のようなフラグを指定する必要があります。外部のランタイムを使用している場合には、<code>--container-runtime=remote</code>と指定し、<code>--container-runtime-endpoint=&lt;path></code>のようにCRIエンドポイントを指定する必要があります。</li></ul><p>これらのフラグは、systemdなどのサービスマネージャー内のkubeletの設定によって指定することができます。</p><h2 id=configure-kubelets-using-kubeadm>kubeadmを使用したkubeletの設定</h2><p><code>kubeadm ... --config some-config-file.yaml</code>のように、カスタムの<code>KubeletConfiguration</code>APIオブジェクトを設定ファイルを介して渡すことで、kubeadmによって起動されるkubeletに設定を反映することができます。</p><p><code>kubeadm config print init-defaults --component-configs KubeletConfiguration</code>を実行することによって、この構造体の全てのデフォルト値を確認することができます。</p><p>また、各フィールドの詳細については、<a href=https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration>kubelet ComponentConfigに関するAPIリファレンス</a>を参照してください。</p><h3 id=kubeadm-init-実行時の流れ><code>kubeadm init</code>実行時の流れ</h3><p><code>kubeadm init</code>を実行した場合、kubeletの設定は<code>/var/lib/kubelet/config.yaml</code>に格納され、クラスターのConfigMapにもアップロードされます。ConfigMapは<code>kubelet-config-1.X</code>という名前で、<code>X</code>は初期化するKubernetesのマイナーバージョンを表します。またこの設定ファイルは、クラスタ内の全てのkubeletのために、クラスター全体設定の基準と共に<code>/etc/kubernetes/kubelet.conf</code>にも書き込まれます。この設定ファイルは、kubeletがAPIサーバと通信するためのクライアント証明書を指し示します。これは、<a href=#propagating-cluster-level-configuration-to-each-kubelet>各kubeletにクラスターレベルの設定を配布</a>することの必要性を示しています。</p><p>二つ目のパターンである、<a href=#providing-instance-specific-configuration-details>インスタンス固有の設定内容を適用</a>するために、kubeadmは環境ファイルを<code>/var/lib/kubelet/kubeadm-flags.env</code>へ書き出します。このファイルは以下のように、kubelet起動時に渡されるフラグのリストを含んでいます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>KUBELET_KUBEADM_ARGS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--flag1=value1 --flag2=value2 ...&#34;</span>
</span></span></code></pre></div><p>kubelet起動時に渡されるフラグに加えて、このファイルはcgroupドライバーや異なるCRIランタイムソケットを使用するかどうか(<code>--cri-socket</code>)といった動的なパラメータも含みます。</p><p>これら二つのファイルがディスク上に格納されると、systemdを使用している場合、kubeadmは以下の二つのコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>リロードと再起動に成功すると、通常の<code>kubeadm init</code>のワークフローが続きます。</p><h3 id=kubeadm-join-実行時の流れ><code>kubeadm join</code>実行時の流れ</h3><p><code>kubeadm join</code>を実行した場合、kubeadmはBootstrap Token証明書を使用してTLS bootstrapを行い、ConfigMap<code>kubelet-config-1.X</code>をダウンロードするために必要なクレデンシャルを取得し、<code>/var/lib/kubelet/config.yaml</code>へ書き込みます。動的な環境ファイルは、<code>kubeadm init</code>の場合と全く同様の方法で生成されます。</p><p>次に、<code>kubeadm</code>は、kubeletに新たな設定を読み込むために、以下の二つのコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>kubeletが新たな設定を読み込むと、kubeadmは、KubeConfigファイル<code>/etc/kubernetes/bootstrap-kubelet.conf</code>を書き込みます。これは、CA証明書とBootstrap Tokenを含みます。これらはkubeletがTLS Bootstrapを行い<code>/etc/kubernetes/kubelet.conf</code>に格納されるユニークなクレデンシャルを取得するために使用されます。ファイルが書き込まれると、kubeletはTLS Bootstrapを終了します。</p><h2 id=the-kubelet-drop-in-file-for-systemd>kubelet用のsystemdファイル</h2><p><code>kubeadm</code>には、systemdがどのようにkubeletを実行するかを指定した設定ファイルが同梱されています。
kubeadm CLIコマンドは決してこのsystemdファイルには触れないことに注意してください。</p><p>kubeadmの<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf>DEBパッケージ</a>または<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf>RPMパッケージ</a>によってインストールされたこの設定ファイルは、<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>に書き込まれ、systemdで使用されます。基本的な<code>kubelet.service</code>(<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service>RPM用</a>または、 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service>DEB用</a>)を拡張します。</p><pre tabindex=0><code class=language-none data-lang=none>[Service]
Environment=&#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
--kubeconfig=/etc/kubernetes/kubelet.conf&#34;
Environment=&#34;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&#34;
# This is a file that &#34;kubeadm init&#34; and &#34;kubeadm join&#34; generate at runtime, populating
the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably,
# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.
# KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
</code></pre><p>このファイルは、kubeadmがkubelet用に管理する全ファイルが置かれるデフォルトの場所を指定します。</p><ul><li>TLS Bootstrapに使用するKubeConfigファイルは<code>/etc/kubernetes/bootstrap-kubelet.conf</code>ですが、<code>/etc/kubernetes/kubelet.conf</code>が存在しない場合にのみ使用します。</li><li>ユニークなkublet識別子を含むKubeConfigファイルは<code>/etc/kubernetes/kubelet.conf</code>です。</li><li>kubeletのComponentConfigを含むファイルは<code>/var/lib/kubelet/config.yaml</code>です。</li><li><code>KUBELET_KUBEADM_ARGS</code>を含む動的な環境ファイルは<code>/var/lib/kubelet/kubeadm-flags.env</code>から取得します。</li><li><code>KUBELET_EXTRA_ARGS</code>によるユーザー定義のフラグの上書きを格納できるファイルは<code>/etc/default/kubelet</code>(DEBの場合)、または<code>/etc/sysconfig/kubelet</code>(RPMの場合)から取得します。<code>KUBELET_EXTRA_ARGS</code>はフラグの連なりの最後に位置し、優先度が最も高いです。</li></ul><h2 id=kubernetesバイナリとパッケージの内容>Kubernetesバイナリとパッケージの内容</h2><p>Kubernetesに同梱されるDEB、RPMのパッケージは以下の通りです。</p><table><thead><tr><th>パッケージ名</th><th>説明</th></tr></thead><tbody><tr><td><code>kubeadm</code></td><td><code>/usr/bin/kubeadm</code>CLIツールと、<a href=#the-kubelet-drop-in-file-for-systemd>kubelet用のsystemdファイル</a>をインストールします。</td></tr><tr><td><code>kubelet</code></td><td>kubeletバイナリを<code>/usr/bin</code>に、CNIバイナリを<code>/opt/cni/bin</code>にインストールします。</td></tr><tr><td><code>kubectl</code></td><td><code>/usr/bin/kubectl</code>バイナリをインストールします。</td></tr><tr><td><code>cri-tools</code></td><td><code>/usr/bin/crictl</code>バイナリを<a href=https://github.com/kubernetes-incubator/cri-tools>cri-tools gitリポジトリ</a>からインストールします。</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-ed857e09999827b013ee9062dc9c59bb>2.1.9 - コントロールプレーンをセルフホストするようにkubernetesクラスターを構成する</h1><h3 id=self-hosting>コントロールプレーンのセルフホスティング</h3><p>kubeadmを使用すると、セルフホスト型のKubernetesコントロールプレーンを実験的に作成できます。これはAPIサーバー、コントローラーマネージャー、スケジューラーなどの主要コンポーネントは、静的ファイルを介してkubeletで構成された<a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a>ではなく、Kubernetes APIを介して構成された<a href=/ja/docs/concepts/workloads/controllers/daemonset/>DaemonSet pods</a>として実行されることを意味します。</p><p>セルフホスト型クラスターを作成する場合は<a href=/docs/reference/setup-tools/kubeadm/kubeadm-alpha/#cmd-selfhosting>kubeadm alpha selfhosting pivot</a>を参照してください。</p><h4 id=警告>警告</h4><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> この機能により、クラスターがサポートされていない状態になり、kubeadmがクラスターを管理できなくなります。これには<code>kubeadm upgrade</code>が含まれます。</div><ol><li><p>1.8以降のセルフホスティングには、いくつかの重要な制限があります。特に、セルフホスト型クラスターは、手動の介入なしにコントロールプレーンのNode再起動から回復することはできません。</p></li><li><p>デフォルトでは、セルフホスト型のコントロールプレーンのPodは、<a href=/docs/concepts/storage/volumes/#hostpath><code>hostPath</code></a>ボリュームからロードされた資格情報に依存しています。最初の作成を除いて、これらの資格情報はkubeadmによって管理されません。</p></li><li><p>コントロールプレーンのセルフホストされた部分にはetcdが含まれていませんが、etcdは静的Podとして実行されます。</p></li></ol><h4 id=プロセス>プロセス</h4><p>セルフホスティングのブートストラッププロセスは、<a href=https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md#optional-self-hosting>kubeadm design
document</a>に記載されています。</p><p>要約すると、<code>kubeadm alpha selfhosting</code>は次のように機能します。</p><ol><li><p>静的コントロールプレーンのブートストラップが起動し、正常になるのを待ちます。これは<code>kubeadm init</code>のセルフホスティングを使用しないプロセスと同じです。</p></li><li><p>静的コントロールプレーンのPodのマニフェストを使用して、セルフホスト型コントロールプレーンを実行する一連のDaemonSetのマニフェストを構築します。また、必要に応じてこれらのマニフェストを変更します。たとえば、シークレット用の新しいボリュームを追加します。</p></li><li><p><code>kube-system</code>のネームスペースにDaemonSetを作成し、Podの結果が起動されるのを待ちます。</p></li><li><p>セルフホスト型のPodが操作可能になると、関連する静的Podが削除され、kubeadmは次のコンポーネントのインストールに進みます。これによりkubeletがトリガーされて静的Podが停止します。</p></li><li><p>元の静的なコントロールプレーンが停止すると、新しいセルフホスト型コントロールプレーンはリスニングポートにバインドしてアクティブになります。</p></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-478acca1934b6d89a0bc00fb25bfe5b6>2.2 - kopsを使ったAWS上でのKubernetesのインストール</h1><p>This quickstart shows you how to easily install a Kubernetes cluster on AWS.
It uses a tool called <a href=https://github.com/kubernetes/kops><code>kops</code></a>.</p><p>kops is an automated provisioning system:</p><ul><li>Fully automated installation</li><li>Uses DNS to identify clusters</li><li>Self-healing: everything runs in Auto-Scaling Groups</li><li>Multiple OS support (Debian, Ubuntu 16.04 supported, CentOS & RHEL, Amazon Linux and CoreOS) - see the <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/images.md>images.md</a></li><li>High-Availability support - see the <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/high_availability.md>high_availability.md</a></li><li>Can directly provision, or generate terraform manifests - see the <a href=https://github.com/kubernetes/kops/blob/master/docs/terraform.md>terraform.md</a></li></ul><h2 id=始める前に>始める前に</h2><ul><li><p>You must have <a href=/docs/tasks/tools/install-kubectl/>kubectl</a> installed.</p></li><li><p>You must <a href=https://github.com/kubernetes/kops#installing>install</a> <code>kops</code> on a 64-bit (AMD64 and Intel 64) device architecture.</p></li><li><p>You must have an <a href=https://docs.aws.amazon.com/polly/latest/dg/setting-up.html>AWS account</a>, generate <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>IAM keys</a> and <a href=https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration>configure</a> them. The IAM user will need <a href=https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user>adequate permissions</a>.</p></li></ul><h2 id=クラスタの作成>クラスタの作成</h2><h3 id=1-5-kopsのインストール>(1/5) kopsのインストール</h3><h4 id=インストール>インストール</h4><p>Download kops from the <a href=https://github.com/kubernetes/kops/releases>releases page</a> (it is also easy to build from source):</p><ul class="nav nav-tabs" id=kops-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#kops-installation-0 role=tab aria-controls=kops-installation-0 aria-selected=true>macOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#kops-installation-1 role=tab aria-controls=kops-installation-1>Linux</a></li></ul><div class=tab-content id=kops-installation><div id=kops-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=kops-installation-0><p><p>Download the latest release with the command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-darwin-amd64
</span></span></code></pre></div><p>To download a specific version, replace the following portion of the command with the specific kops version.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</span></span></code></pre></div><p>For example, to download kops version v1.20.0 type:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-darwin-amd64
</span></span></code></pre></div><p>Make the kops binary executable.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>chmod +x kops-darwin-amd64
</span></span></code></pre></div><p>Move the kops binary in to your PATH.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo mv kops-darwin-amd64 /usr/local/bin/kops
</span></span></code></pre></div><p>You can also install kops using <a href=https://brew.sh/>Homebrew</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</span></span></code></pre></div></div><div id=kops-installation-1 class=tab-pane role=tabpanel aria-labelledby=kops-installation-1><p><p>Download the latest release with the command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-linux-amd64
</span></span></code></pre></div><p>To download a specific version of kops, replace the following portion of the command with the specific kops version.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</span></span></code></pre></div><p>For example, to download kops version v1.20.0 type:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64
</span></span></code></pre></div><p>Make the kops binary executable</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>chmod +x kops-linux-amd64
</span></span></code></pre></div><p>Move the kops binary in to your PATH.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo mv kops-linux-amd64 /usr/local/bin/kops
</span></span></code></pre></div><p>You can also install kops using <a href=https://docs.brew.sh/Homebrew-on-Linux>Homebrew</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</span></span></code></pre></div></div></div><h3 id=2-5-クラスタ用のroute53ドメインの作成>(2/5) クラスタ用のroute53ドメインの作成</h3><p>kops uses DNS for discovery, both inside the cluster and outside, so that you can reach the kubernetes API server
from clients.</p><p>kops has a strong opinion on the cluster name: it should be a valid DNS name. By doing so you will
no longer get your clusters confused, you can share clusters with your colleagues unambiguously,
and you can reach them without relying on remembering an IP address.</p><p>You can, and probably should, use subdomains to divide your clusters. As our example we will use
<code>useast1.dev.example.com</code>. The API server endpoint will then be <code>api.useast1.dev.example.com</code>.</p><p>A Route53 hosted zone can serve subdomains. Your hosted zone could be <code>useast1.dev.example.com</code>,
but also <code>dev.example.com</code> or even <code>example.com</code>. kops works with any of these, so typically
you choose for organization reasons (e.g. you are allowed to create records under <code>dev.example.com</code>,
but not under <code>example.com</code>).</p><p>Let's assume you're using <code>dev.example.com</code> as your hosted zone. You create that hosted zone using
the <a href=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html>normal process</a>, or
with a command such as <code>aws route53 create-hosted-zone --name dev.example.com --caller-reference 1</code>.</p><p>You must then set up your NS records in the parent domain, so that records in the domain will resolve. Here,
you would create NS records in <code>example.com</code> for <code>dev</code>. If it is a root domain name you would configure the NS
records at your domain registrar (e.g. <code>example.com</code> would need to be configured where you bought <code>example.com</code>).</p><p>This step is easy to mess up (it is the #1 cause of problems!) You can double-check that
your cluster is configured correctly if you have the dig tool by running:</p><p><code>dig NS dev.example.com</code></p><p>You should see the 4 NS records that Route53 assigned your hosted zone.</p><h3 id=3-5-クラスタの状態を保存するs3バケットの作成>(3/5) クラスタの状態を保存するS3バケットの作成</h3><p>kops lets you manage your clusters even after installation. To do this, it must keep track of the clusters
that you have created, along with their configuration, the keys they are using etc. This information is stored
in an S3 bucket. S3 permissions are used to control access to the bucket.</p><p>Multiple clusters can use the same S3 bucket, and you can share an S3 bucket between your colleagues that
administer the same clusters - this is much easier than passing around kubecfg files. But anyone with access
to the S3 bucket will have administrative access to all your clusters, so you don't want to share it beyond
the operations team.</p><p>So typically you have one S3 bucket for each ops team (and often the name will correspond
to the name of the hosted zone above!)</p><p>In our example, we chose <code>dev.example.com</code> as our hosted zone, so let's pick <code>clusters.dev.example.com</code> as
the S3 bucket name.</p><ul><li><p>Export <code>AWS_PROFILE</code> (if you need to select a profile for the AWS CLI to work)</p></li><li><p>Create the S3 bucket using <code>aws s3 mb s3://clusters.dev.example.com</code></p></li><li><p>You can <code>export KOPS_STATE_STORE=s3://clusters.dev.example.com</code> and then kops will use this location by default.
We suggest putting this in your bash profile or similar.</p></li></ul><h3 id=4-5-クラスタ設定の構築>(4/5) クラスタ設定の構築</h3><p>Run <code>kops create cluster</code> to create your cluster configuration:</p><p><code>kops create cluster --zones=us-east-1c useast1.dev.example.com</code></p><p>kops will create the configuration for your cluster. Note that it <em>only</em> creates the configuration, it does
not actually create the cloud resources - you'll do that in the next step with a <code>kops update cluster</code>. This
give you an opportunity to review the configuration or change it.</p><p>It prints commands you can use to explore further:</p><ul><li>List your clusters with: <code>kops get cluster</code></li><li>Edit this cluster with: <code>kops edit cluster useast1.dev.example.com</code></li><li>Edit your node instance group: <code>kops edit ig --name=useast1.dev.example.com nodes</code></li><li>Edit your master instance group: <code>kops edit ig --name=useast1.dev.example.com master-us-east-1c</code></li></ul><p>If this is your first time using kops, do spend a few minutes to try those out! An instance group is a
set of instances, which will be registered as kubernetes nodes. On AWS this is implemented via auto-scaling-groups.
You can have several instance groups, for example if you wanted nodes that are a mix of spot and on-demand instances, or
GPU and non-GPU instances.</p><h3 id=5-5-awsにクラスタを作成>(5/5) AWSにクラスタを作成</h3><p>Run "kops update cluster" to create your cluster in AWS:</p><p><code>kops update cluster useast1.dev.example.com --yes</code></p><p>That takes a few seconds to run, but then your cluster will likely take a few minutes to actually be ready.
<code>kops update cluster</code> will be the tool you'll use whenever you change the configuration of your cluster; it
applies the changes you have made to the configuration to your cluster - reconfiguring AWS or kubernetes as needed.</p><p>For example, after you <code>kops edit ig nodes</code>, then <code>kops update cluster --yes</code> to apply your configuration, and
sometimes you will also have to <code>kops rolling-update cluster</code> to roll out the configuration immediately.</p><p>Without <code>--yes</code>, <code>kops update cluster</code> will show you a preview of what it is going to do. This is handy
for production clusters!</p><h3 id=他のアドオンの参照>他のアドオンの参照</h3><p>See the <a href=/ja/docs/concepts/cluster-administration/addons/>list of add-ons</a> to explore other add-ons, including tools for logging, monitoring, network policy, visualization, and control of your Kubernetes cluster.</p><h2 id=クリーンアップ>クリーンアップ</h2><ul><li>To delete your cluster: <code>kops delete cluster useast1.dev.example.com --yes</code></li></ul><h2 id=次の項目>次の項目</h2><ul><li>Learn more about Kubernetes <a href=/docs/concepts/>concepts</a> and <a href=/docs/reference/kubectl/overview/><code>kubectl</code></a>.</li><li>Learn more about <code>kops</code> <a href=https://kops.sigs.k8s.io/>advanced usage</a> for tutorials, best practices and advanced configuration options.</li><li>Follow <code>kops</code> community discussions on Slack: <a href=https://github.com/kubernetes/kops#other-ways-to-communicate-with-the-contributors>community discussions</a></li><li>Contribute to <code>kops</code> by addressing or raising an issue <a href=https://github.com/kubernetes/kops/issues>GitHub Issues</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f8b4964187fe973644e06ee629eff1de>2.3 - kubesprayを使ったオンプレミス/クラウドプロバイダへのKubernetesのインストール</h1><p>This quickstart helps to install a Kubernetes cluster hosted on GCE, Azure, OpenStack, AWS, vSphere, Equinix Metal (formerly Packet), Oracle Cloud Infrastructure (Experimental) or Baremetal with <a href=https://github.com/kubernetes-sigs/kubespray>Kubespray</a>.</p><p>Kubespray is a composition of <a href=https://docs.ansible.com/>Ansible</a> playbooks, <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md>inventory</a>, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks. Kubespray provides:</p><ul><li>a highly available cluster</li><li>composable attributes</li><li>support for most popular Linux distributions<ul><li>Ubuntu 16.04, 18.04, 20.04, 22.04</li><li>CentOS/RHEL/Oracle Linux 7, 8</li><li>Debian Buster, Jessie, Stretch, Wheezy</li><li>Fedora 34, 35</li><li>Fedora CoreOS</li><li>openSUSE Leap 15</li><li>Flatcar Container Linux by Kinvolk</li></ul></li><li>continuous integration tests</li></ul><p>To choose a tool which best fits your use case, read <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md>this comparison</a> to
<a href=/docs/reference/setup-tools/kubeadm/kubeadm/>kubeadm</a> and <a href=/ja/docs/setup/production-environment/tools/kops/>kops</a>.</p><h2 id=クラスタの作成>クラスタの作成</h2><h3 id=1-5-下地の要件の確認>(1/5) 下地の要件の確認</h3><p>Provision servers with the following <a href=https://github.com/kubernetes-sigs/kubespray#requirements>requirements</a>:</p><ul><li><strong>Ansible v2.11 and python-netaddr are installed on the machine that will run Ansible commands</strong></li><li><strong>Jinja 2.11 (or newer) is required to run the Ansible Playbooks</strong></li><li>The target servers must have access to the Internet in order to pull docker images. Otherwise, additional configuration is required (<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md>See Offline Environment</a>)</li><li>The target servers are configured to allow <strong>IPv4 forwarding</strong></li><li><strong>Your ssh key must be copied</strong> to all the servers in your inventory</li><li><strong>Firewalls are not managed by kubespray</strong>. You'll need to implement appropriate rules as needed. You should disable your firewall in order to avoid any issues during deployment</li><li>If kubespray is run from a non-root user account, correct privilege escalation method should be configured in the target servers and the <code>ansible_become</code> flag or command parameters <code>--become</code> or <code>-b</code> should be specified</li></ul><p>Kubespray provides the following utilities to help provision your environment:</p><ul><li><a href=https://www.terraform.io/>Terraform</a> scripts for the following cloud providers:<ul><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws>AWS</a></li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/openstack>OpenStack</a></li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/packet>Packet</a></li></ul></li></ul><h3 id=2-5-インベントリファイルの用意>(2/5) インベントリファイルの用意</h3><p>After you provision your servers, create an <a href=https://docs.ansible.com/ansible/latest/network/getting_started/first_inventory.html>inventory file for Ansible</a>. You can do this manually or via a dynamic inventory script. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#building-your-own-inventory>Building your own inventory</a>".</p><h3 id=3-5-クラスタ作成の計画>(3/5) クラスタ作成の計画</h3><p>Kubespray provides the ability to customize many aspects of the deployment:</p><ul><li>Choice deployment mode: kubeadm or non-kubeadm</li><li>CNI (networking) plugins</li><li>DNS configuration</li><li>Choice of control plane: native/binary or containerized</li><li>Component versions</li><li>Calico route reflectors</li><li>Component runtime options<ul><li><a class=glossary-tooltip title=Dockerは、コンテナとして知られる、オペレーティングシステムレベルでの仮想化を提供するソフトウェア技術です。 data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a></li><li><a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></li><li><a class=glossary-tooltip title='A lightweight container runtime specifically for Kubernetes' data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></li></ul></li><li>Certificate generation methods</li></ul><p>Kubespray customizations can be made to a <a href=https://docs.ansible.com/ansible/playbooks_variables.html>variable file</a>. If you are just getting started with Kubespray, consider using the Kubespray defaults to deploy your cluster and explore Kubernetes.</p><h3 id=4-5-クラスタのデプロイ>(4/5) クラスタのデプロイ</h3><p>Next, deploy your cluster:</p><p>Cluster deployment using <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#starting-custom-deployment>ansible-playbook</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ansible-playbook -i your/inventory/inventory.ini cluster.yml -b -v <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --private-key<span style=color:#666>=</span>~/.ssh/private_key
</span></span></code></pre></div><p>Large deployments (100+ nodes) may require <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md>specific adjustments</a> for best results.</p><h3 id=5-5-デプロイの確認>(5/5) デプロイの確認</h3><p>Kubespray provides a way to verify inter-pod connectivity and DNS resolve with <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md>Netchecker</a>. Netchecker ensures the netchecker-agents pods can resolve DNS requests and ping each over within the default namespace. Those pods mimic similar behavior of the rest of the workloads and serve as cluster health indicators.</p><h2 id=クラスタの操作>クラスタの操作</h2><p>Kubespray provides additional playbooks to manage your cluster: <em>scale</em> and <em>upgrade</em>.</p><h3 id=クラスタのスケール>クラスタのスケール</h3><p>You can add worker nodes from your cluster by running the scale playbook. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#adding-nodes>Adding nodes</a>".
You can remove worker nodes from your cluster by running the remove-node playbook. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#remove-nodes>Remove nodes</a>".</p><h3 id=クラスタのアップグレード>クラスタのアップグレード</h3><p>You can upgrade your cluster by running the upgrade-cluster playbook. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md>Upgrades</a>".</p><h2 id=クリーンアップ>クリーンアップ</h2><p>You can reset your nodes and wipe out all components installed with Kubespray via the <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml>reset playbook</a>.</p><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> When running the reset playbook, be sure not to accidentally target your production cluster!</div><h2 id=フィードバック>フィードバック</h2><ul><li>Slack Channel: <a href=https://kubernetes.slack.com/messages/kubespray/>#kubespray</a> (You can get your invite <a href=https://slack.k8s.io/>here</a>)</li><li><a href=https://github.com/kubernetes-sigs/kubespray/issues>GitHub Issues</a></li></ul><h2 id=次の項目>次の項目</h2><p>Check out planned work on Kubespray's <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/roadmap.md>roadmap</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-e2eb3029b668b1713d0dc8bea296ba9c>3 - ターンキークラウドソリューション</h1></div><div class=td-content><h1 id=pg-20c20ee4c93be1062165131aff27cff5>3.1 - Alibaba CloudでKubernetesを動かす</h1><h2 id=alibaba-cloud-container-service>Alibaba Cloud Container Service</h2><p><a href=https://www.alibabacloud.com/product/container-service>Alibaba Cloud Container Service</a>はAlibaba Cloud ECSインスタンスのクラスター上もしくはサーバーレスの形態でDockerアプリケーションを起動して管理します。著名なオープンソースのコンテナオーケストレーターであるDocker SwarmおよびKubernetesをサポートしています。</p><p>クラスターの構築と管理を簡素化するために、<a href=https://www.alibabacloud.com/product/kubernetes>Alibaba Cloud Container ServiceのためのKubernetesサポート</a>を使用します。<a href=https://www.alibabacloud.com/help/doc-detail/86737.htm>Kubernetes walk-through</a>に従ってすぐに始めることができ、中国語の<a href=https://yq.aliyun.com/teams/11/type_blog-cid_200-page_1>Alibaba CloudにおけるKubernetesサポートのためのチュートリアル</a>もあります。</p><p>カスタムバイナリもしくはオープンソースKubernetesを使用する場合は、以下の手順に従って下さい。</p><h2 id=構築のカスタム>構築のカスタム</h2><p><a href=https://github.com/AliyunContainerService/kubernetes>Alibaba Cloudプロバイダーが実装されたKubernetesのソースコード</a>はオープンソースであり、GitHubから入手可能です。</p><p>さらなる情報は英語の<a href=https://www.alibabacloud.com/forum/read-830>Kubernetesのクイックデプロイメント - Alibaba CloudのVPC環境</a>をご覧下さい。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4db51d554f14646b6af380916c827aa0>3.2 - AWS EC2上でKubernetesを動かす</h1><p>このページでは、AWS上でKubernetesクラスターをインストールする方法について説明します。</p><h2 id=始める前に>始める前に</h2><p>AWS上でKubernetesクラスターを作成するには、AWSからアクセスキーIDおよびシークレットアクセスキーを入手する必要があります。</p><h3 id=サポートされているプロダクショングレードのツール>サポートされているプロダクショングレードのツール</h3><ul><li><p><a href=https://docs.conjure-up.io/stable/en/cni/k8s-and-aws>conjure-up</a>はUbuntu上でネイティブなAWSインテグレーションを用いてKubernetesクラスターを作成するオープンソースのインストーラーです。</p></li><li><p><a href=https://github.com/kubernetes/kops>Kubernetes Operations</a> - プロダクショングレードなKubernetesのインストール、アップグレード、管理が可能です。AWS上のDebian、Ubuntu、CentOS、RHELをサポートしています。</p></li><li><p><a href=https://github.com/kubernetes-incubator/kube-aws>kube-aws</a> EC2、CloudFormation、Auto Scalingを使用して、<a href=https://www.flatcar-linux.org/>Flatcar Linux</a>ノードでKubernetesクラスターを作成および管理します。</p></li><li><p><a href=https://github.com/kubermatic/kubeone>KubeOne</a>は可用性の高いKubernetesクラスターを作成、アップグレード、管理するための、オープンソースのライフサイクル管理ツールです。</p></li></ul><h2 id=クラスターの始まり>クラスターの始まり</h2><h3 id=コマンドライン管理ツール-kubectl>コマンドライン管理ツール: kubectl</h3><p>クラスターの起動スクリプトによってワークステーション上に<code>kubernetes</code>ディレクトリが作成されます。もしくは、Kubernetesの最新リリースを<a href=https://github.com/kubernetes/kubernetes/releases>こちら</a>からダウンロードすることも可能です。</p><p>次に、kubectlにアクセスするために適切なバイナリフォルダーを<code>PATH</code>へ追加します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># macOS</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>PATH</span><span style=color:#666>=</span>&lt;path/to/kubernetes-directory&gt;/platforms/darwin/amd64:<span style=color:#b8860b>$PATH</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Linux</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>PATH</span><span style=color:#666>=</span>&lt;path/to/kubernetes-directory&gt;/platforms/linux/amd64:<span style=color:#b8860b>$PATH</span>
</span></span></code></pre></div><p>ツールに関する最新のドキュメントページはこちらです: <a href=/docs/reference/kubectl/kubectl/>kubectl manual</a></p><p>デフォルトでは、<code>kubectl</code>はクラスターの起動中に生成された<code>kubeconfig</code>ファイルをAPIに対する認証に使用します。
詳細な情報は、<a href=/ja/docs/tasks/access-application-cluster/configure-access-multiple-clusters/>kubeconfig files</a>を参照してください。</p><h3 id=例>例</h3><p>新しいクラスターを試すには、<a href=/ja/docs/tasks/run-application/run-stateless-application-deployment/>簡単なnginxの例</a>を参照してください。</p><p>"Guestbook"アプリケーションは、Kubernetesを始めるもう一つのポピュラーな例です: <a href=https://github.com/kubernetes/examples/tree/master/guestbook/>guestbookの例</a></p><p>より完全なアプリケーションについては、<a href=https://github.com/kubernetes/examples/tree/master/>examplesディレクトリ</a>を参照してください。</p><h2 id=クラスターのスケーリング>クラスターのスケーリング</h2><p><code>kubectl</code>を使用したノードの追加および削除はサポートしていません。インストール中に作成された<a href=https://docs.aws.amazon.com/autoscaling/latest/userguide/as-manual-scaling.html>Auto Scaling Group</a>内の'Desired'および'Max'プロパティを手動で調整することで、ノード数をスケールさせることができます。</p><h2 id=クラスターの解体>クラスターの解体</h2><p>クラスターのプロビジョニングに使用した環境変数がexportされていることを確認してから、<code>kubernetes</code>ディレクトリ内で以下のスクリプトを実行してください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cluster/kube-down.sh
</span></span></code></pre></div><h2 id=サポートレベル>サポートレベル</h2><table><thead><tr><th>IaaS プロバイダー</th><th>構成管理</th><th>OS</th><th>ネットワーク</th><th>ドキュメント</th><th>適合</th><th>サポートレベル</th></tr></thead><tbody><tr><td>AWS</td><td>kops</td><td>Debian</td><td>k8s (VPC)</td><td><a href=https://github.com/kubernetes/kops>docs</a></td><td></td><td>Community (<a href=https://github.com/justinsb>@justinsb</a>)</td></tr><tr><td>AWS</td><td>CoreOS</td><td>CoreOS</td><td>flannel</td><td>-</td><td></td><td>Community</td></tr><tr><td>AWS</td><td>Juju</td><td>Ubuntu</td><td>flannel, calico, canal</td><td>-</td><td>100%</td><td>Commercial, Community</td></tr><tr><td>AWS</td><td>KubeOne</td><td>Ubuntu, CoreOS, CentOS</td><td>canal, weavenet</td><td><a href=https://github.com/kubermatic/kubeone>docs</a></td><td>100%</td><td>Commercial, Community</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-66b96ad245efc180060593fa01df4dde>3.3 - Azure 上で Kubernetes を動かす</h1><h2 id=azure-kubernetes-service-aks>Azure Kubernetes Service (AKS)</h2><p><a href=https://azure.microsoft.com/ja-jp/services/kubernetes-service/>Azure Kubernetes Service</a>は、Kubernetesクラスターのためのシンプルなデプロイ機能を提供します。</p><p>Azure Kubernetes Serviceを利用してAzure上にKubernetesクラスターをデプロイする例:</p><p><strong><a href=https://docs.microsoft.com/ja-jp/azure/aks/intro-kubernetes>Microsoft Azure Kubernetes Service</a></strong></p><h2 id=デプロイのカスタマイズ-aks-engine>デプロイのカスタマイズ: AKS-Engine</h2><p>Azure Kubernetes Serviceのコア部分は<strong>オープンソース</strong>であり、コミュニティのためにGitHub上で公開され、利用およびコントリビュートすることができます: <strong><a href=https://github.com/Azure/aks-engine>AKS-Engine</a></strong>。レガシーな <a href=https://github.com/Azure/acs-engine>ACS-Engine</a> のコードベースはAKS-engineのために廃止となりました。</p><p>AKS-Engineは、Azure Kubernetes Serviceが公式にサポートしている機能を超えてデプロイをカスタマイズしたい場合に適した選択肢です。
既存の仮想ネットワークへのデプロイや、複数のagent poolを利用するなどのカスタマイズをすることができます。
コミュニティによるAKS-Engineへのコントリビュートが、Azure Kubernetes Serviceに組み込まれる場合もあります。</p><p>AKS-Engineへの入力は、Kubernetesクラスターを記述するapimodelのJSONファイルです。これはAzure Kubernetes Serviceを使用してクラスターを直接デプロイするために使用されるAzure Resource Manager (ARM) のテンプレート構文と似ています。
処理結果はARMテンプレートとして出力され、ソース管理に組み込んだり、AzureにKubernetesクラスターをデプロイするために使うことができます。</p><p><strong><a href=https://github.com/Azure/aks-engine/blob/master/docs/tutorials/README.md>AKS-Engine Kubernetes Tutorial</a></strong> を参照して始めることができます。</p><h2 id=azure上でcoreos-tectonicを動かす>Azure上でCoreOS Tectonicを動かす</h2><p>Azureで利用できるCoreOS Tectonic Installerは<strong>オープンソース</strong>であり、コミュニティのためにGitHub上で公開され、利用およびコントリビュートすることができます: <strong><a href=https://github.com/coreos/tectonic-installer>Tectonic Installer</a></strong>.</p><p>Tectonic Installerは、 <a href=https://www.terraform.io/docs/providers/azurerm/>Hashicorp が提供する Terraform</a>のAzure Resource Manager(ARM)プロバイダーを用いてクラスターをカスタマイズしたい場合に適した選択肢です。
これを利用することにより、Terraformと親和性の高いツールを使用してカスタマイズしたり連携したりすることができます。</p><p><a href=https://coreos.com/tectonic/docs/latest/install/azure/azure-terraform.html>Tectonic Installer for Azure Guide</a>を参照して、すぐに始めることができます。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bed0528b13f56089ee19400212edf55d>3.4 - Google Compute Engine上でKubernetesを動かす</h1><p>The example below creates a Kubernetes cluster with 3 worker node Virtual Machines and a master Virtual Machine (i.e. 4 VMs in your cluster). This cluster is set up and controlled from your workstation (or wherever you find convenient).</p><h2 id=始める前に>始める前に</h2><p>If you want a simplified getting started experience and GUI for managing clusters, please consider trying <a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a> for hosted cluster installation and management.</p><p>For an easy way to experiment with the Kubernetes development environment, click the button below
to open a Google Cloud Shell with an auto-cloned copy of the Kubernetes source repo.</p><p><a href="https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/kubernetes/kubernetes&page=editor&open_in_editor=README.md"><img src=https://gstatic.com/cloudssh/images/open-btn.png alt="Open in Cloud Shell"></a></p><p>If you want to use custom binaries or pure open source Kubernetes, please continue with the instructions below.</p><h3 id=前提条件>前提条件</h3><ol><li>You need a Google Cloud Platform account with billing enabled. Visit the <a href=https://console.cloud.google.com>Google Developers Console</a> for more details.</li><li>Install <code>gcloud</code> as necessary. <code>gcloud</code> can be installed as a part of the <a href=https://cloud.google.com/sdk/>Google Cloud SDK</a>.</li><li>Enable the <a href=https://console.developers.google.com/apis/api/replicapool.googleapis.com/overview>Compute Engine Instance Group Manager API</a> in the <a href=https://console.developers.google.com/apis/library>Google Cloud developers console</a>.</li><li>Make sure that gcloud is set to use the Google Cloud Platform project you want. You can check the current project using <code>gcloud config list project</code> and change it via <code>gcloud config set project &lt;project-id></code>.</li><li>Make sure you have credentials for GCloud by running <code>gcloud auth login</code>.</li><li>(Optional) In order to make API calls against GCE, you must also run <code>gcloud auth application-default login</code>.</li><li>Make sure you can start up a GCE VM from the command line. At least make sure you can do the <a href=https://cloud.google.com/compute/docs/instances/#startinstancegcloud>Create an instance</a> part of the GCE Quickstart.</li><li>Make sure you can SSH into the VM without interactive prompts. See the <a href=https://cloud.google.com/compute/docs/instances/#sshing>Log in to the instance</a> part of the GCE Quickstart.</li></ol><h2 id=クラスターの起動>クラスターの起動</h2><p>You can install a client and start a cluster with either one of these commands (we list both in case only one is installed on your machine):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -sS https://get.k8s.io | bash
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>wget -q -O - https://get.k8s.io | bash
</span></span></code></pre></div><p>Once this command completes, you will have a master VM and four worker VMs, running as a Kubernetes cluster.</p><p>By default, some containers will already be running on your cluster. Containers like <code>fluentd</code> provide <a href=/docs/concepts/cluster-administration/logging/>logging</a>, while <code>heapster</code> provides <a href=https://releases.k8s.io/master/cluster/addons/cluster-monitoring/README.md>monitoring</a> services.</p><p>The script run by the commands above creates a cluster with the name/prefix "kubernetes". It defines one specific cluster config, so you can't run it more than once.</p><p>Alternately, you can download and install the latest Kubernetes release from <a href=https://github.com/kubernetes/kubernetes/releases>this page</a>, then run the <code>&lt;kubernetes>/cluster/kube-up.sh</code> script to start the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>cd</span> kubernetes
</span></span><span style=display:flex><span>cluster/kube-up.sh
</span></span></code></pre></div><p>If you want more than one cluster running in your project, want to use a different name, or want a different number of worker nodes, see the <code>&lt;kubernetes>/cluster/gce/config-default.sh</code> file for more fine-grained configuration before you start up your cluster.</p><p>If you run into trouble, please see the section on <a href=/ja/docs/setup/production-environment/turnkey/gce/#troubleshooting>troubleshooting</a>, post to the
<a href=https://discuss.kubernetes.io>Kubernetes Forum</a>, or come ask questions on <code>#gke</code> Slack channel.</p><p>The next few steps will show you:</p><ol><li>How to set up the command line client on your workstation to manage the cluster</li><li>Examples of how to use the cluster</li><li>How to delete the cluster</li><li>How to start clusters with non-default options (like larger clusters)</li></ol><h2 id=ワークステーション上でのkubernetesコマンドラインツールのインストール>ワークステーション上でのKubernetesコマンドラインツールのインストール</h2><p>The cluster startup script will leave you with a running cluster and a <code>kubernetes</code> directory on your workstation.</p><p>The <a href=/docs/reference/kubectl/kubectl/>kubectl</a> tool controls the Kubernetes cluster
manager. It lets you inspect your cluster resources, create, delete, and update
components, and much more. You will use it to look at your new cluster and bring
up example apps.</p><p>You can use <code>gcloud</code> to install the <code>kubectl</code> command-line tool on your workstation:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>gcloud components install kubectl
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> The kubectl version bundled with <code>gcloud</code> may be older than the one
The <a href=/ja/docs/reference/kubectl/kubectl/>kubectl</a> tool controls the Kubernetes cluster
document to see how you can set up the latest <code>kubectl</code> on your workstation.</div><h2 id=クラスターの始まり>クラスターの始まり</h2><h3 id=クラスターの様子を見る>クラスターの様子を見る</h3><p>Once <code>kubectl</code> is in your path, you can use it to look at your cluster. E.g., running:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --all-namespaces services
</span></span></code></pre></div><p>should show a set of <a href=/docs/concepts/services-networking/service/>services</a> that look something like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAMESPACE     NAME          TYPE             CLUSTER_IP       EXTERNAL_IP       PORT<span style=color:#666>(</span>S<span style=color:#666>)</span>        AGE
</span></span><span style=display:flex><span>default       kubernetes    ClusterIP        10.0.0.1         &lt;none&gt;            443/TCP        1d
</span></span><span style=display:flex><span>kube-system   kube-dns      ClusterIP        10.0.0.2         &lt;none&gt;            53/TCP,53/UDP  1d
</span></span><span style=display:flex><span>kube-system   kube-ui       ClusterIP        10.0.0.3         &lt;none&gt;            80/TCP         1d
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Similarly, you can take a look at the set of <a href=/ja/docs/concepts/workloads/pods/>pods</a> that were created during cluster startup.
You can do this via the</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --all-namespaces pods
</span></span></code></pre></div><p>command.</p><p>You'll see a list of pods that looks something like this (the name specifics will be different):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAMESPACE     NAME                                           READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>kube-system   coredns-5f4fbb68df-mc8z8                       1/1       Running   <span style=color:#666>0</span>          15m
</span></span><span style=display:flex><span>kube-system   fluentd-cloud-logging-kubernetes-minion-63uo   1/1       Running   <span style=color:#666>0</span>          14m
</span></span><span style=display:flex><span>kube-system   fluentd-cloud-logging-kubernetes-minion-c1n9   1/1       Running   <span style=color:#666>0</span>          14m
</span></span><span style=display:flex><span>kube-system   fluentd-cloud-logging-kubernetes-minion-c4og   1/1       Running   <span style=color:#666>0</span>          14m
</span></span><span style=display:flex><span>kube-system   fluentd-cloud-logging-kubernetes-minion-ngua   1/1       Running   <span style=color:#666>0</span>          14m
</span></span><span style=display:flex><span>kube-system   kube-ui-v1-curt1                               1/1       Running   <span style=color:#666>0</span>          15m
</span></span><span style=display:flex><span>kube-system   monitoring-heapster-v5-ex4u3                   1/1       Running   <span style=color:#666>1</span>          15m
</span></span><span style=display:flex><span>kube-system   monitoring-influx-grafana-v1-piled             2/2       Running   <span style=color:#666>0</span>          15m
</span></span></code></pre></div><p>Some of the pods may take a few seconds to start up (during this time they'll show <code>Pending</code>), but check that they all show as <code>Running</code> after a short period.</p><h3 id=いくつかの例の実行>いくつかの例の実行</h3><p>Then, see <a href=/ja/docs/tasks/run-application/run-stateless-application-deployment/>a simple nginx example</a> to try out your new cluster.</p><p>For more complete applications, please look in the <a href=https://github.com/kubernetes/examples/tree/master/>examples directory</a>. The <a href=https://github.com/kubernetes/examples/tree/master/guestbook/>guestbook example</a> is a good "getting started" walkthrough.</p><h2 id=クラスターの解体>クラスターの解体</h2><p>To remove/delete/teardown the cluster, use the <code>kube-down.sh</code> script.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>cd</span> kubernetes
</span></span><span style=display:flex><span>cluster/kube-down.sh
</span></span></code></pre></div><p>Likewise, the <code>kube-up.sh</code> in the same directory will bring it back up. You do not need to rerun the <code>curl</code> or <code>wget</code> command: everything needed to setup the Kubernetes cluster is now on your workstation.</p><h2 id=カスタマイズ>カスタマイズ</h2><p>The script above relies on Google Storage to stage the Kubernetes release. It
then will start (by default) a single master VM along with 3 worker VMs. You
can tweak some of these parameters by editing <code>kubernetes/cluster/gce/config-default.sh</code>
You can view a transcript of a successful cluster creation
<a href=https://gist.github.com/satnam6502/fc689d1b46db9772adea>here</a>.</p><h2 id=トラブルシューティング>トラブルシューティング</h2><h3 id=プロジェクトの設定>プロジェクトの設定</h3><p>You need to have the Google Cloud Storage API, and the Google Cloud Storage
JSON API enabled. It is activated by default for new projects. Otherwise, it
can be done in the Google Cloud Console. See the <a href=https://cloud.google.com/storage/docs/json_api/>Google Cloud Storage JSON
API Overview</a> for more
details.</p><p>Also ensure that-- as listed in the <a href=#%E5%89%8D%E6%8F%90%E6%9D%A1%E4%BB%B6>Prerequisites section</a>-- you've enabled the <code>Compute Engine Instance Group Manager API</code>, and can start up a GCE VM from the command line as in the <a href=https://cloud.google.com/compute/docs/quickstart>GCE Quickstart</a> instructions.</p><h3 id=クラスター初期化のハング>クラスター初期化のハング</h3><p>If the Kubernetes startup script hangs waiting for the API to be reachable, you can troubleshoot by SSHing into the master and node VMs and looking at logs such as <code>/var/log/startupscript.log</code>.</p><p><strong>Once you fix the issue, you should run <code>kube-down.sh</code> to cleanup</strong> after the partial cluster creation, before running <code>kube-up.sh</code> to try again.</p><h3 id=ssh>SSH</h3><p>If you're having trouble SSHing into your instances, ensure the GCE firewall
isn't blocking port 22 to your VMs. By default, this should work but if you
have edited firewall rules or created a new non-default network, you'll need to
expose it: <code>gcloud compute firewall-rules create default-ssh --network=&lt;network-name> --description "SSH allowed from anywhere" --allow tcp:22</code></p><p>Additionally, your GCE SSH key must either have no passcode or you need to be
using <code>ssh-agent</code>.</p><h3 id=ネットワーク>ネットワーク</h3><p>The instances must be able to connect to each other using their private IP. The
script uses the "default" network which should have a firewall rule called
"default-allow-internal" which allows traffic on any port on the private IPs.
If this rule is missing from the default network or if you change the network
being used in <code>cluster/config-default.sh</code> create a new rule with the following
field values:</p><ul><li>Source Ranges: <code>10.0.0.0/8</code></li><li>Allowed Protocols and Port: <code>tcp:1-65535;udp:1-65535;icmp</code></li></ul><h2 id=サポートレベル>サポートレベル</h2><table><thead><tr><th>IaaS Provider</th><th>Config. Mgmt</th><th>OS</th><th>Networking</th><th>Docs</th><th>Conforms</th><th>Support Level</th></tr></thead><tbody><tr><td>GCE</td><td>Saltstack</td><td>Debian</td><td>GCE</td><td><a href=/ja/docs/setup/production-environment/turnkey/gce/>docs</a></td><td></td><td>Project</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-ba2691e5872ead2141b1e8fd7d29ee21>3.5 - IBM Cloud Privateを使ってマルチクラウドでKubernetesを動かす</h1><p>IBM® Cloud Private is a turnkey cloud solution and an on-premises turnkey cloud solution. IBM Cloud Private delivers pure upstream Kubernetes with the typical management components that are required to run real enterprise workloads. These workloads include health management, log management, audit trails, and metering for tracking usage of workloads on the platform.</p><p>IBM Cloud Private is available in a community edition and a fully supported enterprise edition. The community edition is available at no charge from <a href=https://hub.docker.com/r/ibmcom/icp-inception/>Docker Hub</a>. The enterprise edition supports high availability topologies and includes commercial support from IBM for Kubernetes and the IBM Cloud Private management platform. If you want to try IBM Cloud Private, you can use either the hosted trial, the tutorial, or the self-guided demo. You can also try the free community edition. For details, see <a href=https://www.ibm.com/cloud/private/get-started>Get started with IBM Cloud Private</a>.</p><p>For more information, explore the following resources:</p><ul><li><a href=https://www.ibm.com/cloud/private>IBM Cloud Private</a></li><li><a href=https://github.com/ibm-cloud-architecture/refarch-privatecloud>Reference architecture for IBM Cloud Private</a></li><li><a href=https://www.ibm.com/support/knowledgecenter/SSBS6K/product_welcome_cloud_private.html>IBM Cloud Private documentation</a></li></ul><h2 id=ibm-cloud-privateとterraform>IBM Cloud PrivateとTerraform</h2><p>The following modules are available where you can deploy IBM Cloud Private by using Terraform:</p><ul><li>AWS: <a href=https://github.com/ibm-cloud-architecture/terraform-icp-aws>Deploy IBM Cloud Private to AWS</a></li><li>Azure: <a href=https://github.com/ibm-cloud-architecture/terraform-icp-azure>Deploy IBM Cloud Private to Azure</a></li><li>IBM Cloud: <a href=https://github.com/ibm-cloud-architecture/terraform-icp-ibmcloud>Deploy IBM Cloud Private cluster to IBM Cloud</a></li><li>OpenStack: <a href=https://github.com/ibm-cloud-architecture/terraform-icp-openstack>Deploy IBM Cloud Private to OpenStack</a></li><li>Terraform module: <a href=https://github.com/ibm-cloud-architecture/terraform-module-icp-deploy>Deploy IBM Cloud Private on any supported infrastructure vendor</a></li><li>VMware: <a href=https://github.com/ibm-cloud-architecture/terraform-icp-vmware>Deploy IBM Cloud Private to VMware</a></li></ul><h2 id=aws上でのibm-cloud-private>AWS上でのIBM Cloud Private</h2><p>You can deploy an IBM Cloud Private cluster on Amazon Web Services (AWS) using Terraform.</p><p>IBM Cloud Private can also run on the AWS cloud platform by using Terraform. To deploy IBM Cloud Private in an AWS EC2 environment, see <a href=https://github.com/ibm-cloud-architecture/terraform-icp-aws>Installing IBM Cloud Private on AWS</a>.</p><h2 id=azure上でのibm-cloud-private>Azure上でのIBM Cloud Private</h2><p>You can enable Microsoft Azure as a cloud provider for IBM Cloud Private deployment and take advantage of all the IBM Cloud Private features on the Azure public cloud. For more information, see <a href=https://www.ibm.com/support/knowledgecenter/SSBS6K_3.2.0/supported_environments/azure_overview.html>IBM Cloud Private on Azure</a>.</p><h2 id=red-hat-openshiftを用いたibm-cloud-private>Red Hat OpenShiftを用いたIBM Cloud Private</h2><p>You can deploy IBM certified software containers that are running on IBM Cloud Private onto Red Hat OpenShift.</p><p>Integration capabilities:</p><ul><li>Supports Linux® 64-bit platform in offline-only installation mode</li><li>Single-master configuration</li><li>Integrated IBM Cloud Private cluster management console and catalog</li><li>Integrated core platform services, such as monitoring, metering, and logging</li><li>IBM Cloud Private uses the OpenShift image registry</li></ul><p>For more information see, <a href=https://www.ibm.com/support/knowledgecenter/SSBS6K_3.2.0/supported_environments/openshift/overview.html>IBM Cloud Private on OpenShift</a>.</p><h2 id=virtualbox上でのibm-cloud-private>VirtualBox上でのIBM Cloud Private</h2><p>To install IBM Cloud Private to a VirtualBox environment, see <a href=https://github.com/ibm-cloud-architecture/refarch-privatecloud-virtualbox>Installing IBM Cloud Private on VirtualBox</a>.</p><h2 id=vmware上でのibm-cloud-private>VMware上でのIBM Cloud Private</h2><p>You can install IBM Cloud Private on VMware with either Ubuntu or RHEL images. For details, see the following projects:</p><ul><li><a href=https://github.com/ibm-cloud-architecture/refarch-privatecloud/blob/master/Installing_ICp_on_prem_ubuntu.md>Installing IBM Cloud Private with Ubuntu</a></li><li><a href=https://github.com/ibm-cloud-architecture/refarch-privatecloud/tree/master/icp-on-rhel>Installing IBM Cloud Private with Red Hat Enterprise</a></li></ul><p>The IBM Cloud Private Hosted service automatically deploys IBM Cloud Private Hosted on your VMware vCenter Server instances. This service brings the power of microservices and containers to your VMware environment on IBM Cloud. With this service, you can extend the same familiar VMware and IBM Cloud Private operational model and tools from on-premises into the IBM Cloud.</p><p>For more information, see <a href="https://cloud.ibm.com/docs/vmwaresolutions?topic=vmwaresolutions-icp_overview">IBM Cloud Private Hosted service</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1b751cdddc397a65edb7bcf703bc0414>4 - オンプレミスVM</h1></div><div class=td-content><h1 id=pg-83a1f15a6fa96fcdca228c4a70f392b3>4.1 - Cloudstack</h1><p><a href=https://cloudstack.apache.org/>CloudStack</a> is a software to build public and private clouds based on hardware virtualization principles (traditional IaaS). To deploy Kubernetes on CloudStack there are several possibilities depending on the Cloud being used and what images are made available. CloudStack also has a vagrant plugin available, hence Vagrant could be used to deploy Kubernetes either using the existing shell provisioner or using new Salt based recipes.</p><p><a href=https://coreos.com>CoreOS</a> templates for CloudStack are built <a href=https://stable.release.core-os.net/amd64-usr/current/>nightly</a>. CloudStack operators need to <a href=https://docs.cloudstack.apache.org/projects/cloudstack-administration/en/latest/templates.html>register</a> this template in their cloud before proceeding with these Kubernetes deployment instructions.</p><p>This guide uses a single <a href=https://github.com/apachecloudstack/k8s>Ansible playbook</a>, which is completely automated and can deploy Kubernetes on a CloudStack based Cloud using CoreOS images. The playbook, creates an ssh key pair, creates a security group and associated rules and finally starts coreOS instances configured via cloud-init.</p><h2 id=前提条件>前提条件</h2><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo apt-get install -y python-pip libssl-dev
</span></span><span style=display:flex><span>sudo pip install cs
</span></span><span style=display:flex><span>sudo pip install sshpubkeys
</span></span><span style=display:flex><span>sudo apt-get install software-properties-common
</span></span><span style=display:flex><span>sudo apt-add-repository ppa:ansible/ansible
</span></span><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install ansible
</span></span></code></pre></div><p>On CloudStack server you also have to install libselinux-python :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>yum install libselinux-python
</span></span></code></pre></div><p><a href=https://github.com/exoscale/cs><em>cs</em></a> is a python module for the CloudStack API.</p><p>Set your CloudStack endpoint, API keys and HTTP method used.</p><p>You can define them as environment variables: <code>CLOUDSTACK_ENDPOINT</code>, <code>CLOUDSTACK_KEY</code>, <code>CLOUDSTACK_SECRET</code> and <code>CLOUDSTACK_METHOD</code>.</p><p>Or create a <code>~/.cloudstack.ini</code> file:</p><pre tabindex=0><code class=language-none data-lang=none>[cloudstack]
endpoint = &lt;your cloudstack api endpoint&gt;
key = &lt;your api access key&gt;
secret = &lt;your api secret key&gt;
method = post
</code></pre><p>We need to use the http POST method to pass the <em>large</em> userdata to the coreOS instances.</p><h3 id=playbookのクローン>playbookのクローン</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>git clone https://github.com/apachecloudstack/k8s
</span></span><span style=display:flex><span><span style=color:#a2f>cd</span> kubernetes-cloudstack
</span></span></code></pre></div><h3 id=kubernetesクラスターの作成>Kubernetesクラスターの作成</h3><p>You simply need to run the playbook.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ansible-playbook k8s.yml
</span></span></code></pre></div><p>Some variables can be edited in the <code>k8s.yml</code> file.</p><pre tabindex=0><code class=language-none data-lang=none>vars:
  ssh_key: k8s
  k8s_num_nodes: 2
  k8s_security_group_name: k8s
  k8s_node_prefix: k8s2
  k8s_template: &lt;templatename&gt;
  k8s_instance_type: &lt;serviceofferingname&gt;
</code></pre><p>This will start a Kubernetes master node and a number of compute nodes (by default 2).
The <code>instance_type</code> and <code>template</code> are specific, edit them to specify your CloudStack cloud specific template and instance type (i.e. service offering).</p><p>Check the tasks and templates in <code>roles/k8s</code> if you want to modify anything.</p><p>Once the playbook as finished, it will print out the IP of the Kubernetes master:</p><pre tabindex=0><code class=language-none data-lang=none>TASK: [k8s | debug msg=&#39;k8s master IP is {{ k8s_master.default_ip }}&#39;] ********
</code></pre><p>SSH to it using the key that was created and using the <em>core</em> user.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ssh -i ~/.ssh/id_rsa_k8s core@&lt;master IP&gt;
</span></span></code></pre></div><p>And you can list the machines in your cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>fleetctl list-machines
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>MACHINE        IP             METADATA
a017c422...    &lt;node #1 IP&gt;   role=node
ad13bf84...    &lt;master IP&gt;    role=master
e9af8293...    &lt;node #2 IP&gt;   role=node
</code></pre><h2 id=サポートレベル>サポートレベル</h2><table><thead><tr><th>IaaS Provider</th><th>Config. Mgmt</th><th>OS</th><th>Networking</th><th>Docs</th><th>Conforms</th><th>Support Level</th></tr></thead><tbody><tr><td>CloudStack</td><td>Ansible</td><td>CoreOS</td><td>flannel</td><td><a href=/docs/setup/production-environment/on-premises-vm/cloudstack/>docs</a></td><td></td><td>Community (<a href=https://github.com/ltupin/>@Guiques</a>)</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-89a44f01ba43ac7fda5e20024b9d6cea>4.2 - DC/OS上のKubernetes</h1><p>Mesosphereは<a href=https://mesosphere.com/product/>DC/OS</a>上にKubernetesを構築するための簡単な選択肢を提供します。それは</p><ul><li>純粋なアップストリームのKubernetes</li><li>シングルクリッククラスター構築</li><li>デフォルトで高可用であり安全</li><li>Kubernetesが高速なデータプラットフォーム(例えばAkka、Cassandra、Kafka、Spark)と共に稼働</li></ul><p>です。</p><h2 id=公式mesosphereガイド>公式Mesosphereガイド</h2><p>DC/OS入門の正規のソースは<a href=https://github.com/mesosphere/dcos-kubernetes-quickstart>クイックスタートリポジトリ</a>にあります。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7f608a89334fa86add74d0d6ba0beedf>4.3 - oVirt</h1><p>oVirt is a virtual datacenter manager that delivers powerful management of multiple virtual machines on multiple hosts. Using KVM and libvirt, oVirt can be installed on Fedora, CentOS, or Red Hat Enterprise Linux hosts to set up and manage your virtual data center.</p><h2 id=ovirtクラウドプロバイダーによる構築>oVirtクラウドプロバイダーによる構築</h2><p>The oVirt cloud provider allows to easily discover and automatically add new VM instances as nodes to your Kubernetes cluster.
At the moment there are no community-supported or pre-loaded VM images including Kubernetes but it is possible to <a href=https://ovedou.blogspot.it/2014/03/importing-glance-images-as-ovirt.html>import</a> or <a href=https://www.ovirt.org/documentation/quickstart/quickstart-guide/#create-virtual-machines>install</a> Project Atomic (or Fedora) in a VM to <a href=https://www.ovirt.org/documentation/quickstart/quickstart-guide/#using-templates>generate a template</a>. Any other distribution that includes Kubernetes may work as well.</p><p>It is mandatory to <a href=https://www.ovirt.org/documentation/how-to/guest-agent/install-the-guest-agent-in-fedora/>install the ovirt-guest-agent</a> in the guests for the VM ip address and hostname to be reported to ovirt-engine and ultimately to Kubernetes.</p><p>Once the Kubernetes template is available it is possible to start instantiating VMs that can be discovered by the cloud provider.</p><h2 id=ovirtクラウドプロバイダーの使用>oVirtクラウドプロバイダーの使用</h2><p>The oVirt Cloud Provider requires access to the oVirt REST-API to gather the proper information, the required credential should be specified in the <code>ovirt-cloud.conf</code> file:</p><pre tabindex=0><code class=language-none data-lang=none>[connection]
uri = https://localhost:8443/ovirt-engine/api
username = admin@internal
password = admin
</code></pre><p>In the same file it is possible to specify (using the <code>filters</code> section) what search query to use to identify the VMs to be reported to Kubernetes:</p><pre tabindex=0><code class=language-none data-lang=none>[filters]
# Search query used to find nodes
vms = tag=kubernetes
</code></pre><p>In the above example all the VMs tagged with the <code>kubernetes</code> label will be reported as nodes to Kubernetes.</p><p>The <code>ovirt-cloud.conf</code> file then must be specified in kube-controller-manager:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kube-controller-manager ... --cloud-provider<span style=color:#666>=</span>ovirt --cloud-config<span style=color:#666>=</span>/path/to/ovirt-cloud.conf ...
</span></span></code></pre></div><h2 id=ovirtクラウドプロバイダーのスクリーンキャスト>oVirtクラウドプロバイダーのスクリーンキャスト</h2><p>This short screencast demonstrates how the oVirt Cloud Provider can be used to dynamically add VMs to your Kubernetes cluster.</p><p><a href="https://www.youtube.com/watch?v=JyyST4ZKne8"><img src=https://img.youtube.com/vi/JyyST4ZKne8/0.jpg alt=Screencast></a></p><h2 id=サポートレベル>サポートレベル</h2><table><thead><tr><th>IaaS Provider</th><th>Config. Mgmt</th><th>OS</th><th>Networking</th><th>Docs</th><th>Conforms</th><th>Support Level</th></tr></thead><tbody><tr><td>oVirt</td><td></td><td></td><td></td><td><a href=/docs/setup/production-environment/on-premises-vm/ovirt/>docs</a></td><td></td><td>Community (<a href=https://github.com/simon3z>@simon3z</a>)</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-acce7e24090fea04715a7a516ba3e69b>5 - Windows in Kubernetes</h1></div><div class=td-content><h1 id=pg-a307d413f1f7430fced233023087e2a1>5.1 - KubernetesのWindowsサポート概要</h1><p>Windowsアプリケーションは、多くの組織で実行されているサービスやアプリケーションの大部分を占めています。<a href=https://aka.ms/windowscontainers>Windowsコンテナ</a>は、プロセスとパッケージの依存関係を一つにまとめる最新の方法を提供し、DevOpsプラクティスの使用とWindowsアプリケーションのクラウドネイティブパターンの追求を容易にします。Kubernetesは事実上、標準的なコンテナオーケストレータになりました。Kubernetes 1.14のリリースでは、Kubernetesクラスター内のWindowsノードでWindowsコンテナをスケジューリングする本番環境サポートが含まれたので、Windowsアプリケーションの広大なエコシステムにおいて、Kubernetesを有効的に活用できます。WindowsベースのアプリケーションとLinuxベースのアプリケーションに投資している組織は、ワークロードを管理する個別のオーケストレーターが不要となるため、オペレーティングシステムに関係なくアプリケーション全体の運用効率が向上します。</p><h2 id=kubernetesのwindowsコンテナ>KubernetesのWindowsコンテナ</h2><p>KubernetesでWindowsコンテナのオーケストレーションを有効にする方法は、既存のLinuxクラスターにWindowsノードを含めるだけです。Kubernetesの<a class=glossary-tooltip title='一番小さく一番シンプルな Kubernetes のオブジェクト。Pod とはクラスターで動作しているいくつかのコンテナのまとまりです。' data-toggle=tooltip data-placement=top href=/ja/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>でWindowsコンテナをスケジュールすることは、Linuxベースのコンテナをスケジュールするのと同じくらいシンプルで簡単です。</p><p>Windowsコンテナを実行するには、Kubernetesクラスターに複数のオペレーティングシステムを含める必要があります。コントロールプレーンノードはLinux、ワーカーノードはワークロードのニーズに応じてWindowsまたはLinuxで実行します。Windows Server 2019は、サポートされている唯一のWindowsオペレーティングシステムであり、Windows (kubelet、<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/containerd>コンテナランタイム</a>、kube-proxyを含む)で<a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node>Kubernetesノード</a>を有効にします。Windowsディストリビューションチャンネルの詳細については、<a href=https://docs.microsoft.com/en-us/windows-server/get-started/servicing-channels-comparison>Microsoftのドキュメント</a>を参照してください。</p><p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <a href=/ja/docs/concepts/overview/components/>マスターコンポーネント</a>を含むKubernetesコントロールプレーンは、Linuxで実行し続けます。WindowsのみのKubernetesクラスターを導入する計画はありません。</div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> このドキュメントでは、Windowsコンテナについて説明する場合、プロセス分離のWindowsコンテナを意味します。<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container>Hyper-V分離</a>のWindowsコンテナは、将来リリースが計画されています。</div></p><h2 id=サポートされている機能と制限>サポートされている機能と制限</h2><h3 id=サポートされている機能>サポートされている機能</h3><h4 id=コンピュート>コンピュート</h4><p>APIとkubectlの観点から見ると、WindowsコンテナはLinuxベースのコンテナとほとんど同じように動作します。ただし、制限セクションで概説されている主要な機能には、いくつかの顕著な違いがあります。</p><p>オペレーティングシステムのバージョンから始めましょう。KubernetesのWindowsオペレーティングシステムのサポートについては、次の表を参照してください。単一の混成Kubernetesクラスターは、WindowsとLinuxの両方のワーカーノードを持つことができます。WindowsコンテナはWindowsノードで、LinuxコンテナはLinuxノードでスケジュールする必要があります。</p><table><thead><tr><th>Kubernetes バージョン</th><th>ホストOS バージョン (Kubernetes ノード)</th><th></th><th></th></tr></thead><tbody><tr><td></td><td><em>Windows Server 1709</em></td><td><em>Windows Server 1803</em></td><td><em>Windows Server 1809/Windows Server 2019</em></td></tr><tr><td><em>Kubernetes v1.14</em></td><td>サポートされていません</td><td>サポートされていません</td><td>Windows Server containers Builds 17763.* と Docker EE-basic 18.09 がサポートされています</td></tr></tbody></table><p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> すべてのWindowsユーザーがアプリのオペレーティングシステムを頻繁に更新することは望んでいません。アプリケーションのアップグレードは、クラスターに新しいノードをアップグレードまたは導入することを要求する必要があります。Kubernetesで実行されているコンテナのオペレーティングシステムをアップグレードすることを選択したユーザーには、新しいオペレーティングシステムバージョンのサポート追加時に、ガイダンスと段階的な指示を提供します。このガイダンスには、クラスターノードと共にアプリケーションをアップグレードするための推奨アップグレード手順が含まれます。Windowsノードは、現在のLinuxノードと同じように、Kubernetes<a href=/ja/docs/setup/release/version-skew-policy/>バージョンスキューポリシー</a>(ノードからコントロールプレーンのバージョン管理)に準拠しています。</div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> Windows Serverホストオペレーティングシステムには、<a href=https://www.microsoft.com/en-us/cloud-platform/windows-server-pricing>Windows Server</a>ライセンスが適用されます。Windowsコンテナイメージには、<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/images-eula>Windowsコンテナの追加ライセンス条項</a>ライセンスが提供されます。</div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> プロセス分離のWindowsコンテナには、<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/version-compatibility>ホストOSのバージョンはコンテナのベースイメージのOSバージョンと一致する必要がある</a>という厳格な互換性ルールがあります。KubernetesでHyper-V分離のWindowsコンテナをサポートする際には、制限と互換性ルールが変更されます。</div></p><p>Kubernetesの主要な要素は、WindowsでもLinuxと同じように機能します。このセクションでは、主要なワークロードイネーブラーのいくつかと、それらがWindowsにどのようにマップされるかについて説明します。</p><ul><li><p><a href=/ja/docs/concepts/workloads/pods/>Pods</a></p><p>Podは、Kubernetesにおける最も基本的な構成要素です。人間が作成またはデプロイするKubernetesオブジェクトモデルの中で最小かつ最もシンプルな単位です。WindowsとLinuxのコンテナを同じPodにデプロイすることはできません。Pod内のすべてのコンテナは、各ノードが特定のプラットフォームとアーキテクチャを表す単一のノードにスケジュールされます。次のPod機能、プロパティ、およびイベントがWindowsコンテナでサポートされています。:</p><ul><li>プロセス分離とボリューム共有を備えたPodごとの単一または複数のコンテナ</li><li>Podステータスフィールド</li><li>ReadinessとLiveness Probe</li><li>postStartとpreStopコンテナのライフサイクルイベント</li><li>環境変数またはボリュームとしてのConfigMap、 Secrets</li><li>EmptyDir</li><li>名前付きパイプホストマウント</li><li>リソース制限</li></ul></li><li><p><a href=/ja/docs/concepts/workloads/controllers/>Controllers</a></p><p>Kubernetesコントローラは、Podの望ましい状態を処理します。次のワークロードコントローラーは、Windowsコンテナでサポートされています。:</p><ul><li>ReplicaSet</li><li>ReplicationController</li><li>Deployments</li><li>StatefulSets</li><li>DaemonSet</li><li>Job</li><li>CronJob</li></ul></li><li><p><a href=/ja/docs/concepts/services-networking/service/>Services</a></p><p>Kubernetes Serviceは、Podの論理セットとPodにアクセスするためのポリシーを定義する抽象概念です。マイクロサービスと呼ばれることもあります。オペレーティングシステム間の接続にServiceを使用できます。WindowsでのServiceは、次のタイプ、プロパティと機能を利用できます。:</p><ul><li>サービス環境変数</li><li>NodePort</li><li>ClusterIP</li><li>LoadBalancer</li><li>ExternalName</li><li>Headless services</li></ul></li></ul><p>Pod、Controller、Serviceは、KubernetesでWindowsワークロードを管理するための重要な要素です。ただし、それだけでは、動的なクラウドネイティブ環境でWindowsワークロードの適切なライフサイクル管理を可能にするのに十分ではありません。次の機能のサポートを追加しました：</p><ul><li>Podとコンテナのメトリクス</li><li>Horizontal Pod Autoscalerサポート</li><li>kubectl Exec</li><li>リソースクォータ</li><li>Schedulerのプリエンプション</li></ul><h4 id=コンテナランタイム>コンテナランタイム</h4><h5 id=docker-ee>Docker EE</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><p>Docker EE-basic 18.09+は、Kubernetesを実行しているWindows Server 2019 / 1809ノードに推奨されるコンテナランタイムです。kubeletに含まれるdockershimコードで動作します。</p><h5 id=cri-containerd>CRI-ContainerD</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [alpha]</code></div><p>ContainerDはLinux上のKubernetesで動作するOCI準拠のランタイムです。Kubernetes v1.18では、Windows上での<a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=ContainerD>ContainerD</a>のサポートが追加されています。Windows上でのContainerDの進捗状況は<a href=https://github.com/kubernetes/enhancements/issues/1001>enhancements#1001</a>で確認できます。</p><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong><p>Kubernetes v1.18におけるWindows上でのContainerDは以下の既知の欠点があります:</p><ul><li>ContainerDは公式リリースではWindowsをサポートしていません。すなわち、Kubernetesでのすべての開発はアクティブなContainerD開発ブランチに対して行われています。本番環境へのデプロイは常に、完全にテストされセキュリティ修正をサポートした公式リリースを利用するべきです。</li><li>ContainerDを利用した場合、Group Managed Service Accountsは実装されていません。詳細は<a href=https://github.com/containerd/cri/issues/1276>containerd/cri#1276</a>を参照してください。</li></ul></div><h4 id=永続ストレージ>永続ストレージ</h4><p>Kubernetes<a href=/docs/concepts/storage/volumes/>ボリューム</a>を使用すると、データの永続性とPodボリュームの共有要件を備えた複雑なアプリケーションをKubernetesにデプロイできます。特定のストレージバックエンドまたはプロトコルに関連付けられた永続ボリュームの管理には、ボリュームのプロビジョニング/プロビジョニング解除/サイズ変更、Kubernetesノードへのボリュームのアタッチ/デタッチ、およびデータを永続化する必要があるPod内の個別のコンテナへのボリュームのマウント/マウント解除などのアクションが含まれます。特定のストレージバックエンドまたはプロトコルに対してこれらのボリューム管理アクションを実装するコードは、Kubernetesボリューム<a href=/docs/concepts/storage/volumes/#types-of-volumes>プラグイン</a>の形式で出荷されます。次の幅広いクラスのKubernetesボリュームプラグインがWindowsでサポートされています。:</p><h5 id=in-treeボリュームプラグイン>In-treeボリュームプラグイン</h5><p>In-treeボリュームプラグインに関連付けられたコードは、コアKubernetesコードベースの一部として提供されます。In-treeボリュームプラグインのデプロイでは、追加のスクリプトをインストールしたり、個別のコンテナ化されたプラグインコンポーネントをデプロイしたりする必要はありません。これらのプラグインは、ストレージバックエンドでのボリュームのプロビジョニング/プロビジョニング解除とサイズ変更、Kubernetesノードへのボリュームのアタッチ/アタッチ解除、Pod内の個々のコンテナーへのボリュームのマウント/マウント解除を処理できます。次のIn-treeプラグインは、Windowsノードをサポートしています。:</p><ul><li><a href=/docs/concepts/storage/volumes/#awselasticblockstore>awsElasticBlockStore</a></li><li><a href=/docs/concepts/storage/volumes/#azuredisk>azureDisk</a></li><li><a href=/docs/concepts/storage/volumes/#azurefile>azureFile</a></li><li><a href=/docs/concepts/storage/volumes/#gcepersistentdisk>gcePersistentDisk</a></li><li><a href=/docs/concepts/storage/volumes/#vspherevolume>vsphereVolume</a></li></ul><h5 id=flexvolume-plugins>FlexVolume Plugins</h5><p><a href=/docs/concepts/storage/volumes/#flexVolume>FlexVolume</a>プラグインに関連付けられたコードは、ホストに直接デプロイする必要があるout-of-treeのスクリプトまたはバイナリとして出荷されます。FlexVolumeプラグインは、Kubernetesノードとの間のボリュームのアタッチ/デタッチ、およびPod内の個々のコンテナとの間のボリュームのマウント/マウント解除を処理します。FlexVolumeプラグインに関連付けられた永続ボリュームのプロビジョニング/プロビジョニング解除は、通常FlexVolumeプラグインとは別の外部プロビジョニング担当者を通じて処理できます。次のFlexVolume<a href=https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows>プラグイン</a>は、Powershellスクリプトとしてホストにデプロイされ、Windowsノードをサポートします:</p><ul><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd>SMB</a></li><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd>iSCSI</a></li></ul><h5 id=csiプラグイン>CSIプラグイン</h5><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.16 [alpha]</code></div><p><a class=glossary-tooltip title=コンテナストレージインターフェイス(CSI)はストレージシステムをコンテナに公開するための標準インターフェイスを定義します。 data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a>プラグインに関連付けられたコードは、通常、コンテナイメージとして配布され、DaemonSetやStatefulSetなどの標準のKubernetesコンポーネントを使用してデプロイされるout-of-treeのスクリプトおよびバイナリとして出荷されます。CSIプラグインは、ボリュームのプロビジョニング/プロビジョニング解除/サイズ変更、Kubernetesノードへのボリュームのアタッチ/ボリュームからのデタッチ、Pod内の個々のコンテナへのボリュームのマウント/マウント解除、バックアップ/スナップショットとクローニングを使用した永続データのバックアップ/リストアといった、Kubernetesの幅広いボリューム管理アクションを処理します。CSIプラグインは通常、ノードプラグイン（各ノードでDaemonSetとして実行される）とコントローラープラグインで構成されます。</p><p>CSIノードプラグイン（特に、ブロックデバイスまたは共有ファイルシステムとして公開された永続ボリュームに関連付けられているプラ​​グイン）は、ディスクデバイスのスキャン、ファイルシステムのマウントなど、さまざまな特権操作を実行する必要があります。これらの操作は、ホストオペレーティングシステムごとに異なります。Linuxワーカーノードの場合、コンテナ化されたCSIノードプラグインは通常、特権コンテナとしてデプロイされます。Windowsワーカーノードの場合、コンテナ化されたCSIノードプラグインの特権操作は、<a href=https://github.com/kubernetes-csi/csi-proxy>csi-proxy</a>を使用してサポートされます。各Windowsノードにプリインストールされている。詳細については、展開するCSIプラグインの展開ガイドを参照してください。</p><h4 id=ネットワーキング>ネットワーキング</h4><p>Windowsコンテナのネットワークは、<a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>CNIプラグイン</a>を通じて公開されます。Windowsコンテナは、ネットワークに関して仮想マシンと同様に機能します。各コンテナには、Hyper-V仮想スイッチ(vSwitch)に接続されている仮想ネットワークアダプター(vNIC)があります。Host Network Service(HNS)とHost Compute Service(HCS)は連携してコンテナを作成し、コンテナvNICをネットワークに接続します。HCSはコンテナの管理を担当するのに対し、HNSは次のようなネットワークリソースの管理を担当します。:</p><ul><li>仮想ネットワーク(vSwitchの作成を含む)</li><li>エンドポイント/vNIC</li><li>名前空間</li><li>ポリシー(パケットのカプセル化、負荷分散ルール、ACL、NATルールなど)</li></ul><p>次のServiceタイプがサポートされています。:</p><ul><li>NodePort</li><li>ClusterIP</li><li>LoadBalancer</li><li>ExternalName</li></ul><p>Windowsは、L2bridge、L2tunnel、Overlay、Transparent、NATの5つの異なるネットワークドライバー/モードをサポートしています。WindowsとLinuxのワーカーノードを持つ異種クラスターでは、WindowsとLinuxの両方で互換性のあるネットワークソリューションを選択する必要があります。以下のツリー外プラグインがWindowsでサポートされており、各CNIをいつ使用するかに関する推奨事項があります。:</p><table><thead><tr><th>ネットワークドライバー</th><th>説明</th><th>コンテナパケットの変更</th><th>ネットワークプラグイン</th><th>ネットワークプラグインの特性</th></tr></thead><tbody><tr><td>L2bridge</td><td>コンテナは外部のvSwitchに接続されます。コンテナはアンダーレイネットワークに接続されますが、物理ネットワークはコンテナのMACを上り/下りで書き換えるため、MACを学習する必要はありません。コンテナ間トラフィックは、コンテナホスト内でブリッジされます。</td><td>MACはホストのMACに書き換えられ、IPは変わりません。</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-bridge>win-bridge</a>、<a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a>、Flannelホストゲートウェイは、win-bridgeを使用します。</td><td>win-bridgeはL2bridgeネットワークモードを使用して、コンテナをホストのアンダーレイに接続して、最高のパフォーマンスを提供します。ノード間接続にはユーザー定義ルート(UDR)が必要です。</td></tr><tr><td>L2Tunnel</td><td>これはl2bridgeの特殊なケースですが、Azureでのみ使用されます。すべてのパケットは、SDNポリシーが適用されている仮想化ホストに送信されます。</td><td>MACが書き換えられ、IPがアンダーレイネットワークで表示されます。</td><td><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a></td><td>Azure-CNIを使用すると、コンテナをAzure vNETと統合し、<a href=https://azure.microsoft.com/en-us/services/virtual-network/>Azure Virtual Networkが提供</a>する一連の機能を活用できます。たとえば、Azureサービスに安全に接続するか、Azure NSGを使用します。<a href=https://docs.microsoft.com/en-us/azure/aks/concepts-network#azure-cni-advanced-networking>azure-cniのいくつかの例</a>を参照してください。</td></tr><tr><td>オーバーレイ(KubernetesのWindows用のオーバーレイネットワークは <em>アルファ</em> 段階です)</td><td>コンテナには、外部のvSwitchに接続されたvNICが付与されます。各オーバーレイネットワークは、カスタムIPプレフィックスで定義された独自のIPサブネットを取得します。オーバーレイネットワークドライバーは、VXLANを使用してカプセル化します。</td><td>外部ヘッダーでカプセル化されます。</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-overlay>Win-overlay</a>、Flannel VXLAN (win-overlayを使用)</td><td>win-overlayは、仮想コンテナーネットワークをホストのアンダーレイから分離する必要がある場合に使用する必要があります(セキュリティ上の理由など)。データセンター内のIPが制限されている場合に、(異なるVNIDタグを持つ)異なるオーバーレイネットワークでIPを再利用できるようにします。このオプションには、Windows Server 2019で<a href=https://support.microsoft.com/help/4489899>KB4489899</a>が必要です。</td></tr><tr><td>透過的(<a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a>の特別な使用例)</td><td>外部のvSwitchが必要です。コンテナは外部のvSwitchに接続され、論理ネットワーク(論理スイッチおよびルーター)を介したPod内通信を可能にします。</td><td>パケットは、<a href=https://datatracker.ietf.org/doc/draft-gross-geneve/>GENEVE</a>または<a href=https://datatracker.ietf.org/doc/draft-davie-stt/>STT</a>トンネリングを介してカプセル化され、同じホスト上にないポッドに到達します。パケットは、ovnネットワークコントローラーによって提供されるトンネルメタデータ情報を介して転送またはドロップされます。NATは南北通信のために行われます。</td><td><a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a></td><td><a href=https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib>ansible経由でデプロイ</a>します。分散ACLは、Kubernetesポリシーを介して適用できます。 IPAMをサポートします。負荷分散は、kube-proxyなしで実現できます。 NATは、ip​​tables/netshを使用せずに行われます。</td></tr><tr><td>NAT(<em>Kubernetesでは使用されません</em>)</td><td>コンテナには、内部のvSwitchに接続されたvNICが付与されます。DNS/DHCPは、<a href=https://blogs.technet.microsoft.com/virtualization/2016/05/25/windows-nat-winnat-capabilities-and-limitations/>WinNAT</a>と呼ばれる内部コンポーネントを使用して提供されます。</td><td>MACおよびIPはホストMAC/IPに書き換えられます。</td><td><a href=https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat>nat</a></td><td>完全を期すためにここに含まれています。</td></tr></tbody></table><p>上で概説したように、<a href=https://github.com/coreos/flannel>Flannel</a> CNI<a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel>メタプラグイン</a>は、<a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>VXLANネットワークバックエンド</a>(<strong>アルファサポート</strong>、win-overlayへのデリゲート)および<a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw>ホストゲートウェイネットワークバックエンド</a>(安定したサポート、win-bridgeへのデリゲート)を介して<a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel#windows-support-experimental>Windows</a>でもサポートされます。このプラグインは、参照CNIプラグイン(win-overlay、win-bridge)の1つへの委任をサポートし、WindowsのFlannelデーモン(Flanneld)と連携して、ノードのサブネットリースの自動割り当てとHNSネットワークの作成を行います。このプラグインは、独自の構成ファイル(cni.conf)を読み取り、FlannelDで生成されたsubnet.envファイルからの環境変数と統合します。次に、ネットワークプラミング用の参照CNIプラグインの1つに委任し、ノード割り当てサブネットを含む正しい構成をIPAMプラグイン(ホストローカルなど)に送信します。</p><p>Node、Pod、およびServiceオブジェクトの場合、TCP/UDPトラフィックに対して次のネットワークフローがサポートされます。:</p><ul><li>Pod -> Pod (IP)</li><li>Pod -> Pod (Name)</li><li>Pod -> Service (Cluster IP)</li><li>Pod -> Service (PQDN、ただし、「.」がない場合のみ)</li><li>Pod -> Service (FQDN)</li><li>Pod -> External (IP)</li><li>Pod -> External (DNS)</li><li>Node -> Pod</li><li>Pod -> Node</li></ul><p>Windowsでは、次のIPAMオプションがサポートされています。</p><ul><li><a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local>ホストローカル</a></li><li>HNS IPAM (受信トレイプラットフォームIPAM、これはIPAMが設定されていない場合のフォールバック)</li><li><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md>Azure-vnet-ipam</a>(azure-cniのみ)</li></ul><h3 id=制限>制限</h3><h4 id=コントロールプレーン>コントロールプレーン</h4><p>Windowsは、Kubernetesアーキテクチャとコンポーネントマトリックスのワーカーノードとしてのみサポートされています。つまり、Kubernetesクラスタには常にLinuxマスターノード、0以上のLinuxワーカーノード、0以上のWindowsワーカーノードが含まれている必要があります。</p><h4 id=コンピュート-1>コンピュート</h4><h5 id=リソース管理とプロセス分離>リソース管理とプロセス分離</h5><p>Linux cgroupsは、Linuxのリソースを制御するPodの境界として使用されます。コンテナは、ネットワーク、プロセス、およびファイルシステムを分離するのために、その境界内に作成されます。cgroups APIを使用して、cpu/io/memoryの統計を収集できます。対照的に、Windowsはシステムネームスペースフィルターを備えたコンテナごとのジョブオブジェクトを使用して、コンテナ内のすべてのプロセスを格納し、ホストからの論理的な分離を提供します。ネームスペースフィルタリングを行わずにWindowsコンテナを実行する方法はありません。これは、ホストの環境ではシステム特権を主張できないため、Windowsでは特権コンテナを使用できないことを意味します。セキュリティアカウントマネージャー(SAM)が独立しているため、コンテナはホストからIDを引き受けることができません。</p><h5 id=オペレーティングシステムの制限>オペレーティングシステムの制限</h5><p>Windowsには厳密な互換性ルールがあり、ホストOSのバージョンとコンテナのベースイメージOSのバージョンは、一致する必要があります。Windows Server 2019のコンテナオペレーティングシステムを備えたWindowsコンテナのみがサポートされます。Hyper-V分離のコンテナは、Windowsコンテナのイメージバージョンに下位互換性を持たせることは、将来のリリースで計画されています。</p><h5 id=機能制限>機能制限</h5><ul><li>TerminationGracePeriod：実装されていません</li><li>単一ファイルのマッピング：CRI-ContainerDで実装されます</li><li>終了メッセージ：CRI-ContainerDで実装されます</li><li>特権コンテナ：現在Windowsコンテナではサポートされていません</li><li>HugePages：現在Windowsコンテナではサポートされていません</li><li>既存のノード問題を検出する機能はLinux専用であり、特権コンテナが必要です。一般的に、特権コンテナはサポートされていないため、これがWindowsで使用されることは想定していません。</li><li>ネームスペース共有については、すべての機能がサポートされているわけではありません（詳細については、APIセクションを参照してください）</li></ul><h5 id=メモリ予約と処理>メモリ予約と処理</h5><p>Windowsには、Linuxのようなメモリ不足のプロセスキラーはありません。Windowsは常に全ユーザーモードのメモリ割り当てを仮想として扱い、ページファイルは必須です。正味の効果は、WindowsはLinuxのようなメモリ不足の状態にはならず、メモリ不足（OOM）終了の影響を受ける代わりにページをディスクに処理します。メモリが過剰にプロビジョニングされ、物理メモリのすべてが使い果たされると、ページングによってパフォーマンスが低下する可能性があります。</p><p>2ステップのプロセスで、メモリ使用量を妥当な範囲内に保つことが可能です。まず、kubeletパラメータ<code>--kubelet-reserve</code>や<code>--system-reserve</code>を使用して、ノード（コンテナ外）でのメモリ使用量を明確にします。これにより、<a href=/ja/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>NodeAllocatable</a>)が削減されます。ワークロードをデプロイするときは、コンテナにリソース制限をかけます（制限のみを設定するか、制限が要求と等しくなければなりません）。これにより、NodeAllocatableも差し引かれ、ノードのリソースがフルな状態になるとSchedulerがPodを追加できなくなります。</p><p>過剰なプロビジョニングを回避するためのベストプラクティスは、Windows、Docker、およびKubernetesのプロセスに対応するために、最低2GBのメモリを予約したシステムでkubeletを構成することです。</p><p>フラグの振舞いについては、次のような異なる動作をします。:</p><ul><li><code>--kubelet-reserve</code>、<code>--system-reserve</code>、および<code>--eviction-hard</code>フラグはノードの割り当て可能数を更新します</li><li><code>--enforce-node-allocable</code>を使用した排除は実装されていません</li><li><code>--eviction-hard</code>および<code>--eviction-soft</code>を使用した排除は実装されていません</li><li>MemoryPressureの制約は実装されていません</li><li>kubeletによって実行されるOOMを排除することはありません</li><li>Windowsノードで実行されているKubeletにはメモリ制限がありません。<code>--kubelet-reserve</code>と<code>--system-reserve</code>は、ホストで実行されているkubeletまたはプロセスに制限を設定しません。これは、ホスト上のkubeletまたはプロセスが、NodeAllocatableとSchedulerの外でメモリリソース不足を引き起こす可能性があることを意味します。</li></ul><h4 id=ストレージ>ストレージ</h4><p>Windowsには、コンテナレイヤーをマウントして、NTFSに基づいて複製されたファイルシステムを作るためのレイヤー構造のファイルシステムドライバーがあります。コンテナ内のすべてのファイルパスは、そのコンテナの環境内だけで決められます。</p><ul><li>ボリュームマウントは、コンテナ内のディレクトリのみを対象にすることができ、個別のファイルは対象にできません</li><li>ボリュームマウントは、ファイルまたはディレクトリをホストファイルシステムに投影することはできません</li><li>WindowsレジストリとSAMデータベースには常に書き込みアクセスが必要であるため、読み取り専用ファイルシステムはサポートされていません。ただし、読み取り専用ボリュームはサポートされています</li><li>ボリュームのユーザーマスクと権限は使用できません。SAMはホストとコンテナ間で共有されないため、それらの間のマッピングはありません。すべての権限はコンテナの環境内で決められます</li></ul><p>その結果、次のストレージ機能はWindowsノードではサポートされません。</p><ul><li>ボリュームサブパスのマウント。Windowsコンテナにマウントできるのはボリューム全体だけです。</li><li>シークレットのサブパスボリュームのマウント</li><li>ホストマウントプロジェクション</li><li>DefaultMode（UID/GID依存関係による）</li><li>読み取り専用のルートファイルシステム。マップされたボリュームは引き続き読み取り専用をサポートします</li><li>ブロックデバイスマッピング</li><li>記憶媒体としてのメモリ</li><li>uui/guid、ユーザーごとのLinuxファイルシステム権限などのファイルシステム機能</li><li>NFSベースのストレージ/ボリュームのサポート</li><li>マウントされたボリュームの拡張（resizefs）</li></ul><h4 id=ネットワーキング-1>ネットワーキング</h4><p>Windowsコンテナネットワーキングは、Linuxネットワーキングとはいくつかの重要な実装方法の違いがあります。<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture>Microsoft documentation for Windows Container Networking</a>には、追加の詳細と背景があります。</p><p>Windowsホストネットワーキングサービスと仮想スイッチはネームスペースを実装して、Podまたはコンテナの必要に応じて仮想NICを作成できます。ただし、DNS、ルート、メトリックなどの多くの構成は、Linuxのような/etc/...ファイルではなく、Windowsレジストリデータベースに保存されます。コンテナのWindowsレジストリはホストのレジストリとは別であるため、ホストからコンテナへの/etc/resolv.confのマッピングなどの概念は、Linuxの場合と同じ効果をもたらしません。これらは、そのコンテナの環境で実行されるWindows APIを使用して構成する必要があります。したがって、CNIの実装は、ファイルマッピングに依存する代わりにHNSを呼び出して、ネットワークの詳細をPodまたはコンテナに渡す必要があります。</p><p>次のネットワーク機能はWindowsノードではサポートされていません</p><ul><li>ホストネットワーキングモードはWindows Podでは使用できません</li><li>ノード自体からのローカルNodePortアクセスは失敗します（他のノードまたは外部クライアントで機能）</li><li>ノードからのService VIPへのアクセスは、Windows Serverの将来のリリースで利用可能になる予定です</li><li>kube-proxyのオーバーレイネットワーキングサポートはアルファリリースです。さらに、<a href=https://support.microsoft.com/en-us/help/4482887/windows-10-update-kb4482887>KB4482887</a>がWindows Server 2019にインストールされている必要があります</li><li>ローカルトラフィックポリシーとDSRモード</li><li>l2bridge、l2tunnel、またはオーバーレイネットワークに接続されたWindowsコンテナは、IPv6スタックを介した通信をサポートしていません。これらのネットワークドライバーがIPv6アドレスを使用できるようにするために必要な機能として、優れたWindowsプラットフォームの機能があり、それに続いて、kubelet、kube-proxy、およびCNIプラグインといったKubernetesの機能があります。</li><li>win-overlay、win-bridge、およびAzure-CNIプラグインを介したICMPプロトコルを使用したアウトバウンド通信。具体的には、Windowsデータプレーン(<a href=https://www.microsoft.com/en-us/research/project/azure-virtual-filtering-platform/>VFP</a>)は、ICMPパケットの置き換えをサポートしていません。これの意味は：<ul><li>同じネットワーク内の宛先に向けられたICMPパケット（pingを介したPod間通信など）は期待どおりに機能し、制限はありません</li><li>TCP/UDPパケットは期待どおりに機能し、制限はありません</li><li>リモートネットワーク（Podからping経由の外部インターネット通信など）を通過するように指示されたICMPパケットは置き換えできないため、ソースにルーティングされません。</li><li>TCP/UDPパケットは引き続き置き換えできるため、<code>ping &lt;destination></code>を<code>curl &lt;destination></code>に置き換えることで、外部への接続をデバッグできます。</li></ul></li></ul><p>これらの機能はKubernetes v1.15で追加されました。</p><ul><li><code>kubectl port-forward</code></li></ul><h5 id=cniプラグイン>CNIプラグイン</h5><ul><li>Windowsリファレンスネットワークプラグインのwin-bridgeとwin-overlayは、<a href=https://github.com/containernetworking/cni/blob/master/SPEC.md>CNI仕様</a>v0.4.0において「CHECK」実装がないため、今のところ実装されていません。</li><li>Flannel VXLAN CNIについては、Windowsで次の制限があります。:</li></ul><ol><li>Node-podの直接間接続は設計上不可能です。Flannel<a href=https://github.com/coreos/flannel/pull/1096>PR 1096</a>を使用するローカルPodでのみ可能です</li><li>VNI 4096とUDPポート4789の使用に制限されています。VNIの制限は現在取り組んでおり、将来のリリースで解決される予定です（オープンソースのflannelの変更）。これらのパラメーターの詳細については、公式の<a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>Flannel VXLAN</a>バックエンドのドキュメントをご覧ください。</li></ol><h5 id=dns-limitations>DNS</h5><ul><li>ClusterFirstWithHostNetは、DNSでサポートされていません。Windowsでは、FQDNとしてすべての名前を「.」で扱い、PQDNでの名前解決はスキップします。</li><li>Linuxでは、PQDNで名前解決しようとするときに使用するDNSサフィックスリストがあります。Windowsでは、1つのDNSサフィックスしかありません。これは、そのPodのNamespaceに関連付けられているDNSサフィックスです（たとえば、mydns.svc.cluster.local）。Windowsでは、そのサフィックスだけで名前解決可能なFQDNおよびServiceまたはNameでの名前解決ができます。たとえば、defaultのNamespaceで生成されたPodには、DNSサフィックス<strong>default.svc.cluster.local</strong>が付けられます。WindowsのPodでは、<strong>kubernetes.default.svc.cluster.local</strong>と<strong>kubernetes</strong>の両方を名前解決できますが、<strong>kubernetes.default</strong>や<strong>kubernetes.default.svc</strong>のような中間での名前解決はできません。</li><li>Windowsでは、複数のDNSリゾルバーを使用できます。これらには少し異なる動作が付属しているため、ネームクエリの解決には<code>Resolve-DNSName</code>ユーティリティを使用することをお勧めします。</li></ul><h5 id=セキュリティ>セキュリティ</h5><p>Secretはノードのボリュームに平文テキストで書き込まれます（Linuxのtmpfs/in-memoryの比較として）。これはカスタマーが2つのことを行う必要があります</p><ol><li>ファイルACLを使用してSecretファイルの場所を保護する</li><li><a href=https://docs.microsoft.com/en-us/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server>BitLocker</a>を使って、ボリュームレベルの暗号化を使用する</li></ol><p><a href=/docs/concepts/policy/pod-security-policy/#users-and-groups>RunAsUser</a>は、現在Windowsではサポートされていません。回避策は、コンテナをパッケージ化する前にローカルアカウントを作成することです。RunAsUsername機能は、将来のリリースで追加される可能性があります。</p><p>SELinux、AppArmor、Seccomp、特性（POSIX機能）のような、Linux固有のPodセキュリティ環境の権限はサポートされていません。</p><p>さらに、既に述べたように特権付きコンテナは、Windowsにおいてサポートされていません。</p><h4 id=api>API</h4><p>ほとんどのKubernetes APIがWindowsでも機能することに違いはありません。そのわずかな違いはOSとコンテナランタイムの違いによるものです。特定の状況では、PodやコンテナなどのワークロードAPIの一部のプロパティが、Linuxで実装されているが、Windowsでは実行できないことを前提に設計されています。</p><p>高いレベルで、これらOSのコンセプトに違いがります。:</p><ul><li>ID - Linuxでは、Integer型として表されるuserID（UID）とgroupID（GID）を使用します。ユーザー名とグループ名は正規ではありません - それらは、UID+GIDの背後にある<code>/etc/groups</code>または<code>/etc/passwd</code>の単なるエイリアスです。Windowsは、Windows Security Access Manager（SAM）データベースに格納されているより大きなバイナリセキュリティ識別子（SID）を使用します。このデータベースは、ホストとコンテナ間、またはコンテナ間で共有されません。</li><li>ファイル権限 - Windowsは、権限とUID+GIDのビットマスクではなく、SIDに基づくアクセス制御リストを使用します</li><li>ファイルパス - Windowsの規則では、<code>/</code>ではなく<code>\</code>を使用します。Go IOライブラリは通常両方を受け入れ、それを機能させるだけですが、コンテナ内で解釈されるパスまたはコマンドラインを設定する場合、<code>\</code>が必要になる場合があります。</li><li>シグナル - Windowsのインタラクティブなアプリは終了を異なる方法で処理し、次の1つ以上を実装できます。:<ul><li>UIスレッドは、WM_CLOSEを含む明確に定義されたメッセージを処理します</li><li>コンソールアプリは、コントロールハンドラーを使用してctrl-cまたはctrl-breakを処理します</li><li>サービスは、SERVICE_CONTROL_STOP制御コードを受け入れることができるサービスコントロールハンドラー関数を登録します。</li></ul></li></ul><p>終了コードは、0が成功、0以外が失敗の場合と同じ規則に従います。特定のエラーコードは、WindowsとLinuxで異なる場合があります。ただし、Kubernetesのコンポーネント（kubelet、kube-proxy）から渡される終了コードは変更されていません。</p><h5 id=v1-container>V1.Container</h5><ul><li>V1.Container.ResourceRequirements.limits.cpuおよびV1.Container.ResourceRequirements.limits.memory - Windowsは、CPU割り当てにハード制限を使用しません。代わりに、共有システムが使用されます。ミリコアに基づく既存のフィールドは、Windowsスケジューラーによって追従される相対共有にスケーリングされます。<a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/helpers_windows.go>参照: kuberuntime/helpers_windows.go</a>、<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/resource-controls>参照: resource controls in Microsoft docs</a><ul><li>Huge Pagesは、Windowsコンテナランタイムには実装されてないので、使用できません。コンテナに対して設定できない<a href=https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support>ユーザー特権を主張</a>する必要があります。</li></ul></li><li>V1.Container.ResourceRequirements.requests.cpuおよびV1.Container.ResourceRequirements.requests.memory - リクエストはノードの利用可能なリソースから差し引かれるので、ノードのオーバープロビジョニングを回避するために使用できます。ただし、過剰にプロビジョニングされたノードのリソースを保証するために使用することはできません。オペレーターが完全にプロビジョニングし過ぎないようにする場合は、ベストプラクティスとしてこれらをすべてのコンテナに適用する必要があります。</li><li>V1.Container.SecurityContext.allowPrivilegeEscalation - Windowsでは使用できません、接続されている機能はありません</li><li>V1.Container.SecurityContext.Capabilities - POSIX機能はWindowsでは実装されていません</li><li>V1.Container.SecurityContext.privileged - Windowsでは特権コンテナをサポートしていません</li><li>V1.Container.SecurityContext.procMount - Windowsでは/procファイルシステムがありません</li><li>V1.Container.SecurityContext.readOnlyRootFilesystem - Windowsでは使用できません、レジストリおよびシステムプロセスがコンテナ内で実行するには、書き込みアクセスが必要です</li><li>V1.Container.SecurityContext.runAsGroup - Windowsでは使用できません、GIDのサポートもありません</li><li>V1.Container.SecurityContext.runAsNonRoot - Windowsではrootユーザーが存在しません。最も近いものは、ノードに存在しないIDであるContainerAdministratorです。</li><li>V1.Container.SecurityContext.runAsUser - Windowsでは使用できません。intとしてのUIDはサポートされていません。</li><li>V1.Container.SecurityContext.seLinuxOptions - Windowsでは使用できません、SELinuxがありません</li><li>V1.Container.terminationMessagePath - これは、Windowsが単一ファイルのマッピングをサポートしないという点でいくつかの制限があります。デフォルト値は/dev/termination-logであり、デフォルトではWindowsに存在しないため動作します。</li></ul><h5 id=v1-pod>V1.Pod</h5><ul><li>V1.Pod.hostIPC、v1.pod.hostpid - Windowsではホストのネームスペースを共有することはできません</li><li>V1.Pod.hostNetwork - ホストのネットワークを共有するためのWindows OSサポートはありません</li><li>V1.Pod.dnsPolicy - ClusterFirstWithHostNet - Windowsではホストネットワーキングがサポートされていないため、サポートされていません。</li><li>V1.Pod.podSecurityContext - 以下のV1.PodSecurityContextを参照</li><li>V1.Pod.shareProcessNamespace - これはベータ版の機能であり、Windowsに実装されていないLinuxのNamespace機能に依存しています。Windowsでは、プロセスのネームスペースまたはコンテナのルートファイルシステムを共有できません。共有できるのはネットワークだけです。</li><li>V1.Pod.terminationGracePeriodSeconds - これはWindowsのDockerに完全には実装されていません。<a href=https://github.com/moby/moby/issues/25982>リファレンス</a>を参照してください。今日の動作では、ENTRYPOINTプロセスにCTRL_SHUTDOWN_EVENTが送信され、Windowsではデフォルトで5秒待機し、最後に通常のWindowsシャットダウン動作を使用してすべてのプロセスをシャットダウンします。5秒のデフォルトは、実際にはWindowsレジストリー<a href=https://github.com/moby/moby/issues/25982#issuecomment-426441183>コンテナ内</a>にあるため、コンテナ作成時にオーバーライドできます。</li><li>V1.Pod.volumeDevices - これはベータ機能であり、Windowsには実装されていません。Windowsでは、rawブロックデバイスをPodに接続できません。</li><li>V1.Pod.volumes-EmptyDir、Secret、ConfigMap、HostPath - すべて動作し、TestGridにテストがあります<ul><li>V1.emptyDirVolumeSource - ノードのデフォルトのメディアはWindowsのディスクです。Windowsでは、RAMディスクが組み込まれていないため、メモリはサポートされていません。</li></ul></li><li>V1.VolumeMount.mountPropagation - mount propagationは、Windowsではサポートされていません。</li></ul><h5 id=v1-podsecuritycontext>V1.PodSecurityContext</h5><p>Windowsでは、PodSecurityContextフィールドはどれも機能しません。これらは参照用にここにリストされています。</p><ul><li>V1.PodSecurityContext.SELinuxOptions - SELinuxは、Windowsでは使用できません</li><li>V1.PodSecurityContext.RunAsUser - UIDを提供しますが、Windowsでは使用できません</li><li>V1.PodSecurityContext.RunAsGroup - GIDを提供しますが、Windowsでは使用できません</li><li>V1.PodSecurityContext.RunAsNonRoot - Windowsにはrootユーザーがありません。最も近いものは、ノードに存在しないIDであるContainerAdministratorです。</li><li>V1.PodSecurityContext.SupplementalGroups - GIDを提供しますが、Windowsでは使用できません</li><li>V1.PodSecurityContext.Sysctls - これらはLinuxのsysctlインターフェースの一部です。Windowsには同等のものはありません。</li></ul><h2 id=troubleshooting>ヘルプとトラブルシューティングを学ぶ</h2><p>Kubernetesクラスターのトラブルシューティングの主なヘルプソースは、この<a href=/docs/tasks/debug-application-cluster/troubleshooting/>セクション</a>から始める必要があります。このセクションには、いくつか追加的な、Windows固有のトラブルシューティングヘルプが含まれています。ログは、Kubernetesにおけるトラブルシューティング問題の重要な要素です。他のコントリビューターからトラブルシューティングの支援を求めるときは、必ずそれらを含めてください。SIG-Windows<a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs>ログ収集に関するコントリビュートガイド</a>の指示に従ってください。</p><ol><li><p>start.ps1が正常に完了したことをどのように確認できますか？</p><p>ノード上でkubelet、kube-proxy、および（ネットワーキングソリューションとしてFlannelを選択した場合）flanneldホストエージェントプロセスが実行され、実行ログが個別のPowerShellウィンドウに表示されます。これに加えて、WindowsノードがKubernetesクラスターで「Ready」として表示されているはずです。</p></li><li><p>Kubernetesノードのプロセスをサービスとしてバックグラウンドで実行するように構成できますか？</p><p>Kubeletとkube-proxyは、ネイティブのWindowsサービスとして実行するように既に構成されています、障害（例えば、プロセスのクラッシュ）が発生した場合にサービスを自動的に再起動することにより、復元性を提供します。これらのノードコンポーネントをサービスとして構成するには、2つのオプションがあります。</p><ol><li><p>ネイティブWindowsサービスとして</p><p>Kubeletとkube-proxyは、<code>sc.exe</code>を使用してネイティブのWindowsサービスとして実行できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span><span style=color:#080;font-style:italic># 2つの個別のコマンドでkubeletおよびkube-proxyのサービスを作成する</span>
</span></span><span style=display:flex><span>sc.exe create &lt;component_name&gt; binPath= <span style=color:#b44>&#34;&lt;path_to_binary&gt; --service &lt;other_args&gt;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 引数にスペースが含まれている場合は、エスケープする必要があることに注意してください。</span>
</span></span><span style=display:flex><span>sc.exe create kubelet binPath= <span style=color:#b44>&#34;C:\kubelet.exe --service --hostname-override &#39;minion&#39; &lt;other_args&gt;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># サービスを開始する</span>
</span></span><span style=display:flex><span><span style=color:#a2f>Start-Service</span> kubelet
</span></span><span style=display:flex><span><span style=color:#a2f>Start-Service</span> <span style=color:#a2f>kube-proxy</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># サービスを停止する</span>
</span></span><span style=display:flex><span><span style=color:#a2f>Stop-Service</span> kubelet (-Force)
</span></span><span style=display:flex><span><span style=color:#a2f>Stop-Service</span> <span style=color:#a2f>kube-proxy</span> (-Force)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># サービスの状態を問い合わせる</span>
</span></span><span style=display:flex><span><span style=color:#a2f>Get-Service</span> kubelet
</span></span><span style=display:flex><span><span style=color:#a2f>Get-Service</span> <span style=color:#a2f>kube-proxy</span>
</span></span></code></pre></div></li><li><p>nssm.exeの使用</p><p>また、<a href=https://nssm.cc/>nssm.exe</a>などの代替サービスマネージャーを使用して、これらのプロセス（flanneld、kubelet、kube-proxy）をバックグラウンドで実行することもできます。この<a href=https://github.com/Microsoft/SDN/tree/master/Kubernetes/flannel/register-svc.ps1>サンプルスクリプト</a>を使用すると、nssm.exeを利用してkubelet、kube-proxy、flanneld.exeを登録し、Windowsサービスとしてバックグラウンドで実行できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span><span style=color:#a2f>register-svc</span>.ps1 -NetworkMode &lt;Network mode&gt; -ManagementIP &lt;Windows Node IP&gt; -ClusterCIDR &lt;Cluster subnet&gt; -KubeDnsServiceIP &lt;<span style=color:#a2f>Kube-dns</span> Service IP&gt; -LogDir &lt;Directory to place logs&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># NetworkMode      = ネットワークソリューションとして選択されたネットワークモードl2bridge（flannel host-gw、これもデフォルト値）またはoverlay（flannel vxlan）</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># ManagementIP     = Windowsノードに割り当てられたIPアドレス。 ipconfigを使用してこれを見つけることができます</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># ClusterCIDR      = クラスターのサブネット範囲。（デフォルト値 10.244.0.0/16）</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># KubeDnsServiceIP = Kubernetes DNSサービスIP（デフォルト値 10.96.0.10）</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># LogDir           = kubeletおよびkube-proxyログがそれぞれの出力ファイルにリダイレクトされるディレクトリ（デフォルト値 C:\k）</span>
</span></span></code></pre></div><p>上記のスクリプトが適切でない場合は、次の例を使用してnssm.exeを手動で構成できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span><span style=color:#080;font-style:italic># flanneld.exeを登録する</span>
</span></span><span style=display:flex><span>nssm install flanneld C:\flannel\flanneld.exe
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set </span>flanneld AppParameters --kubeconfig<span style=color:#666>-file</span>=c:\k\config --iface=&lt;ManagementIP&gt; --ip-masq=<span style=color:#666>1</span> --kube-subnet-mgr=<span style=color:#666>1</span>
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set </span>flanneld AppEnvironmentExtra NODE_NAME=&lt;hostname&gt;
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set </span>flanneld AppDirectory C:\flannel
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>start </span>flanneld
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># kubelet.exeを登録</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># マイクロソフトは、mcr.microsoft.com/k8s/core/pause:1.2.0としてポーズインフラストラクチャコンテナをリリース</span>
</span></span><span style=display:flex><span>nssm install kubelet C:\k\kubelet.exe
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set </span>kubelet AppParameters --hostname-override=&lt;hostname&gt; --v=<span style=color:#666>6</span> --pod-infra-container-image=mcr.microsoft.com/k8s/core/pause<span>:</span><span style=color:#666>1.2</span>.0 --resolv-conf=<span style=color:#b44>&#34;&#34;</span> --allow-privileged=true --enable-debugging-handlers --cluster-dns=&lt;<span style=color:#a2f>DNS-service</span>-IP&gt; --cluster-domain=cluster.local --kubeconfig=c:\k\config --hairpin-mode=<span style=color:#a2f>promiscuous-bridge</span> --image-pull-progress-deadline=<span style=color:#666>20m</span> --cgroups-per-qos=false  --log-dir=&lt;log directory&gt; --logtostderr=false --enforce-node-allocatable=<span style=color:#b44>&#34;&#34;</span> --network-plugin=cni --cni-bin-dir=c:\k\cni --cni-conf-dir=c:\k\cni\config
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set </span>kubelet AppDirectory C:\k
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>start </span>kubelet
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># kube-proxy.exeを登録する (l2bridge / host-gw)</span>
</span></span><span style=display:flex><span>nssm install <span style=color:#a2f>kube-proxy</span> C:\k\<span style=color:#a2f>kube-proxy</span>.exe
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set kube-proxy</span> AppDirectory c:\k
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set kube-proxy</span> AppParameters --v=<span style=color:#666>4</span> --proxy-mode=kernelspace --hostname-override=&lt;hostname&gt;--kubeconfig=c:\k\config --enable-dsr=false --log-dir=&lt;log directory&gt; --logtostderr=false
</span></span><span style=display:flex><span>nssm.exe <span style=color:#a2f>set kube-proxy</span> AppEnvironmentExtra KUBE_NETWORK=cbr0
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set kube-proxy</span> DependOnService kubelet
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>start kube-proxy</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># kube-proxy.exeを登録する (overlay / vxlan)</span>
</span></span><span style=display:flex><span>nssm install <span style=color:#a2f>kube-proxy</span> C:\k\<span style=color:#a2f>kube-proxy</span>.exe
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set kube-proxy</span> AppDirectory c:\k
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set kube-proxy</span> AppParameters --v=<span style=color:#666>4</span> --proxy-mode=kernelspace --feature-gates=<span style=color:#b44>&#34;WinOverlay=true&#34;</span> --hostname-override=&lt;hostname&gt; --kubeconfig=c:\k\config --network-name=vxlan0 --source-vip=&lt;<span style=color:#a2f>source-vip</span>&gt; --enable-dsr=false --log-dir=&lt;log directory&gt; --logtostderr=false
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set kube-proxy</span> DependOnService kubelet
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>start kube-proxy</span>
</span></span></code></pre></div><p>最初のトラブルシューティングでは、<a href=https://nssm.cc/>nssm.exe</a>で次のフラグを使用して、stdoutおよびstderrを出力ファイルにリダイレクトできます。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>nssm <span style=color:#a2f>set </span>&lt;Service Name&gt; AppStdout C:\k\mysvc.log
</span></span><span style=display:flex><span>nssm <span style=color:#a2f>set </span>&lt;Service Name&gt; AppStderr C:\k\mysvc.log
</span></span></code></pre></div><p>詳細については、公式の<a href=https://nssm.cc/usage>nssmの使用法</a>のドキュメントを参照してください。</p></li></ol></li><li><p>Windows Podにネットワーク接続がありません</p><p>仮想マシンを使用している場合は、すべてのVMネットワークアダプターでMACスプーフィングが有効になっていることを確認してください。</p></li><li><p>Windows Podが外部リソースにpingできません</p><p>現在、Windows Podには、ICMPプロトコル用にプログラムされた送信ルールはありません。ただし、TCP/UDPはサポートされています。クラスター外のリソースへの接続を実証する場合は、<code>ping &lt;IP></code>に対応する<code>curl &lt;IP></code>コマンドに置き換えてください。</p><p>それでも問題が解決しない場合は、<a href=https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf>cni.conf</a>のネットワーク構成に値する可能性があるので、いくつかの特別な注意が必要です。この静的ファイルはいつでも編集できます。構成の更新は、新しく作成されたすべてのKubernetesリソースに適用されます。</p><p>Kubernetesのネットワーキング要件の1つ(参照<a href=/ja/docs/concepts/cluster-administration/networking/>Kubernetesモデル</a>)は、内部でNATを使用せずにクラスター通信を行うためのものです。この要件を遵守するために、すべての通信に<a href=https://github.com/Microsoft/SDN/blob/master/Kubernetes/flannel/l2bridge/cni/config/cni.conf#L20>ExceptionList</a>があり、アウトバウンドNATが発生しないようにします。ただし、これは、クエリしようとしている外部IPをExceptionListから除外する必要があることも意味します。そうして初めて、Windows PodからのトラフィックがSNAT処理され、外部からの応答を受信できるようになります。この点で、<code>cni.conf</code>のExceptionListは次のようになります。:</p><pre tabindex=0><code class=language-conf data-lang=conf>&#34;ExceptionList&#34;: [
                &#34;10.244.0.0/16&#34;,  # クラスターのサブネット
                &#34;10.96.0.0/12&#34;,   # Serviceのサブネット
                &#34;10.127.130.0/24&#34; # 管理 (ホスト) のサブネット
            ]
</code></pre></li><li><p>WindowsノードがNodePort Serviceにアクセスできません</p><p>ノード自体からのローカルNodePortアクセスは失敗します。これは既知の制限です。NodePortアクセスは、他のノードまたは外部クライアントから行えます。</p></li><li><p>コンテナのvNICとHNSエンドポイントが削除されています</p><p>この問題は、<code>hostname-override</code>パラメータが<a href=/docs/reference/command-line-tools-reference/kube-proxy/>kube-proxy</a>に渡されない場合に発生する可能性があります。これを解決するには、ユーザーは次のようにホスト名をkube-proxyに渡す必要があります。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span>C:\k\<span style=color:#a2f>kube-proxy</span>.exe --hostname-override=$(hostname)
</span></span></code></pre></div></li><li><p>flannelを使用すると、クラスターに再参加した後、ノードに問題が発生します</p><p>以前に削除されたノードがクラスターに再参加するときはいつも、flannelDは新しいPodサブネットをノードに割り当てようとします。ユーザーは、次のパスにある古いPodサブネット構成ファイルを削除する必要があります。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span><span style=color:#a2f>Remove-Item</span> C:\k\SourceVip.json
</span></span><span style=display:flex><span><span style=color:#a2f>Remove-Item</span> C:\k\SourceVipRequest.json
</span></span></code></pre></div></li><li><p><code>start.ps1</code>を起動した後、flanneldが「ネットワークが作成されるのを待っています」と表示されたままになります</p><p>この<a href=https://github.com/coreos/flannel/issues/1066>調査中の問題</a>に関する多数の報告があります。最も可能性が高いのは、flannelネットワークの管理IPが設定されるタイミングの問題です。回避策は、単純にstart.ps1を再起動するか、次のように手動で再起動することです。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span><span style=color:#a2f>PS </span>C:&gt; [<span style=color:#800>Environment</span>]::SetEnvironmentVariable(<span style=color:#b44>&#34;NODE_NAME&#34;</span>, <span style=color:#b44>&#34;&lt;Windows_Worker_Hostname&gt;&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#a2f>PS </span>C:&gt; C:\flannel\flanneld.exe --kubeconfig<span style=color:#666>-file</span>=c:\k\config --iface=&lt;Windows_Worker_Node_IP&gt; --ip-masq=<span style=color:#666>1</span> --kube-subnet-mgr=<span style=color:#666>1</span>
</span></span></code></pre></div></li><li><p><code>/run/flannel/subnet.env</code>がないため、Windows Podを起動できません</p><p>これは、Flannelが正しく起動しなかったことを示しています。 flanneld.exeの再起動を試みるか、Kubernetesマスターの<code>/run/flannel/subnet.env</code>からWindowsワーカーノードの<code>C:\run\flannel\subnet.env</code>に手動でファイルをコピーすることができます。「FLANNEL_SUBNET」行を別の番号に変更します。たとえば、ノードサブネット10.244.4.1/24が必要な場合は以下となります。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-env data-lang=env><span style=display:flex><span><span style=color:#b8860b>FLANNEL_NETWORK</span><span style=color:#666>=</span>10.244.0.0/16
</span></span><span style=display:flex><span><span style=color:#b8860b>FLANNEL_SUBNET</span><span style=color:#666>=</span>10.244.4.1/24
</span></span><span style=display:flex><span><span style=color:#b8860b>FLANNEL_MTU</span><span style=color:#666>=</span><span style=color:#666>1500</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>FLANNEL_IPMASQ</span><span style=color:#666>=</span><span style=color:#a2f>true</span>
</span></span></code></pre></div></li><li><p>WindowsノードがService IPを使用してServiceにアクセスできない</p><p>これは、Windows上の現在のネットワークスタックの既知の制限です。ただし、Windows PodはService IPにアクセスできます。</p></li><li><p>kubeletの起動時にネットワークアダプターが見つかりません</p><p>WindowsネットワーキングスタックがKubernetesネットワーキングを動かすには、仮想アダプターが必要です。次のコマンドを実行しても結果が返されない場合（管理シェルで）、仮想ネットワークの作成（Kubeletが機能するために必要な前提条件）に失敗したことになります。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span><span style=color:#a2f>Get-HnsNetwork</span> | ? Name <span style=color:#666>-ieq</span> <span style=color:#b44>&#34;cbr0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f>Get-NetAdapter</span> | ? Name <span style=color:#666>-Like</span> <span style=color:#b44>&#34;vEthernet (Ethernet*&#34;</span>
</span></span></code></pre></div><p>ホストのネットワークアダプターが「イーサネット」ではない場合、多くの場合、start.ps1スクリプトの<a href=https://github.com/microsoft/SDN/blob/master/Kubernetes/flannel/start.ps1#L6>InterfaceName</a>パラメーターを修正する価値があります。そうでない場合は<code>start-kubelet.ps1</code>スクリプトの出力結果を調べて、仮想ネットワークの作成中にエラーがないか確認します。</p></li><li><p>Podが「Container Creating」と表示されたまま動かなくなったり、何度も再起動を繰り返します</p><p>PauseイメージがOSバージョンと互換性があることを確認してください。<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/deploying-resources>説明</a>では、OSとコンテナの両方がバージョン1803であると想定しています。それ以降のバージョンのWindowsを使用している場合は、Insiderビルドなどでは、それに応じてイメージを調整する必要があります。イメージについては、Microsoftの<a href=https://hub.docker.com/u/microsoft/>Dockerレジストリ</a>を参照してください。いずれにしても、PauseイメージのDockerfileとサンプルサービスの両方で、イメージに:latestのタグが付けられていると想定しています。</p><p>Kubernetes v1.14以降、MicrosoftはPauseインフラストラクチャコンテナを<code>mcr.microsoft.com/k8s/core/pause:1.2.0</code>でリリースしています。</p></li><li><p>DNS名前解決が正しく機能していない</p><p>この<a href=#dns-limitations>セクション</a>でDNSの制限を確認してください。</p></li><li><p><code>kubectl port-forward</code>が「ポート転送を実行できません:wincatが見つかりません」で失敗します</p><p>これはKubernetes 1.15、およびPauseインフラストラクチャコンテナ<code>mcr.microsoft.com/k8s/core/pause:1.2.0</code>で実装されました。必ずこれらのバージョン以降を使用してください。
独自のPauseインフラストラクチャコンテナを構築する場合は、必ず<a href=https://github.com/kubernetes-sigs/sig-windows-tools/tree/master/cmd/wincat>wincat</a>を含めてください。</p></li><li><p>Windows Serverノードがプロキシの背後にあるため、Kubernetesのインストールが失敗します</p><p>プロキシの背後にある場合は、次のPowerShell環境変数を定義する必要があります。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-PowerShell data-lang=PowerShell><span style=display:flex><span>[<span style=color:#800>Environment</span>]::SetEnvironmentVariable(<span style=color:#b44>&#34;HTTP_PROXY&#34;</span>, <span style=color:#b44>&#34;http://proxy.example.com:80/&#34;</span>, [<span style=color:#800>EnvironmentVariableTarget</span>]::Machine)
</span></span><span style=display:flex><span>[<span style=color:#800>Environment</span>]::SetEnvironmentVariable(<span style=color:#b44>&#34;HTTPS_PROXY&#34;</span>, <span style=color:#b44>&#34;http://proxy.example.com:443/&#34;</span>, [<span style=color:#800>EnvironmentVariableTarget</span>]::Machine)
</span></span></code></pre></div></li><li><p><code>pause</code>コンテナとは何ですか</p><p>Kubernetes Podでは、インフラストラクチャまたは「pause」コンテナが最初に作成され、コンテナエンドポイントをホストします。インフラストラクチャやワーカーコンテナなど、同じPodに属するコンテナは、共通のネットワークネームスペースとエンドポイント（同じIPとポートスペース）を共有します。Pauseコンテナは、ネットワーク構成を失うことなくクラッシュまたは再起動するワーカーコンテナに対応するために必要です。</p><p>「pause」（インフラストラクチャ）イメージは、Microsoft Container Registry（MCR）でホストされています。<code>docker pull mcr.microsoft.com/k8s/core/pause:1.2.0</code>を使用してアクセスできます。詳細については、<a href=https://github.com/kubernetes-sigs/windows-testing/blob/master/images/pause/Dockerfile>DOCKERFILE</a>をご覧ください。</p></li></ol><h3 id=さらなる調査>さらなる調査</h3><p>これらの手順で問題が解決しない場合は、次の方法で、KubernetesのWindowsノードでWindowsコンテナを実行する際のヘルプを利用できます。:</p><ul><li>StackOverflow <a href=https://stackoverflow.com/questions/tagged/windows-server-container>Windows Server Container</a>トピック</li><li>Kubernetesオフィシャルフォーラム <a href=https://discuss.kubernetes.io/>discuss.kubernetes.io</a></li><li>Kubernetes Slack <a href=https://kubernetes.slack.com/messages/sig-windows>#SIG-Windows Channel</a></li></ul><h2 id=issueとfeatureリクエストの報告>IssueとFeatureリクエストの報告</h2><p>バグのようなものがある場合、またはFeatureリクエストを行う場合は、<a href=https://github.com/kubernetes/kubernetes/issues>GitHubのIssueシステム</a>を使用してください。<a href=https://github.com/kubernetes/kubernetes/issues/new/choose>GitHub</a>でIssueを開いて、SIG-Windowsに割り当てることができます。以前に報告された場合は、まずIssueリストを検索し、Issueについての経験をコメントして、追加のログを加える必要があります。SIG-Windows Slackは、チケットを作成する前に、初期サポートとトラブルシューティングのアイデアを得るための素晴らしい手段でもあります。</p><p>バグを報告する場合は、問題の再現方法に関する次のような詳細情報を含めてください。:</p><ul><li>Kubernetesのバージョン: kubectlのバージョン</li><li>環境の詳細: クラウドプロバイダー、OSのディストリビューション、選択したネットワーキングと構成、およびDockerのバージョン</li><li>問題を再現するための詳細な手順</li><li><a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs>関連するログ</a></li><li><code>/sig windows</code>でIssueにコメントして、Issueにsig/windowsのタグを付けて、SIG-Windowsメンバーが気付くようにします</li></ul><h2 id=次の項目>次の項目</h2><p>ロードマップには多くの機能があります。高レベルの簡略リストを以下に示しますが、<a href=https://github.com/orgs/kubernetes/projects/8>ロードマッププロジェクト</a>を見て、<a href=https://github.com/kubernetes/community/blob/master/sig-windows/>貢献すること</a>によってWindowsサポートを改善することをお勧めします。</p><h3 id=hyper-v分離>Hyper-V分離</h3><p>Hyper-V分離はKubernetesで以下のWindowsコンテナのユースケースを実現するために必要です。</p><ul><li>Pod間のハイパーバイザーベースの分離により、セキュリティを強化</li><li>下位互換性により、コンテナの再構築を必要とせずにノードで新しいWindows Serverバージョンを実行</li><li>Podの特定のCPU/NUMA設定</li><li>メモリの分離と予約</li></ul><p>既存のHyper-V分離サポートは、v1.10の試験的な機能であり、上記のCRI-ContainerD機能とRuntimeClass機能を優先して将来廃止される予定です。現在の機能を使用してHyper-V分離コンテナを作成するには、kubeletのフィーチャーゲートを<code>HyperVContainer=true</code>で開始し、Podにアノテーション<code>experimental.windows.kubernetes.io/isolation-type=hyperv</code>を含める必要があります。実験的リリースでは、この機能はPodごとに1つのコンテナに制限されています。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>experimental.windows.kubernetes.io/isolation-type</span>:<span style=color:#bbb> </span>hyperv<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>microsoft/iis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=kubeadmとクラスターapiを使用したデプロイ>kubeadmとクラスターAPIを使用したデプロイ</h3><p>Kubeadmは、ユーザーがKubernetesクラスターをデプロイするための事実上の標準になりつつあります。kubeadmのWindowsノードのサポートは進行中ですが、ガイドはすでに<a href=/ja/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/>ここ</a>で利用可能です。Windowsノードが適切にプロビジョニングされるように、クラスターAPIにも投資しています。</p><h3 id=その他の主な機能>その他の主な機能</h3><ul><li>グループ管理サービスアカウントのベータサポート</li><li>その他のCNI</li><li>その他のストレージプラグイン</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3a51e66c5de55f9093a8dc55742006d3>5.2 - KubernetesでWindowsコンテナをスケジュールするためのガイド</h1><p>Windowsアプリケーションは、多くの組織で実行されるサービスとアプリケーションの大部分を占めます。このガイドでは、KubernetesでWindowsコンテナを構成してデプロイする手順について説明します。</p><h2 id=目的>目的</h2><ul><li>WindowsノードでWindowsコンテナを実行するサンプルのDeploymentを構成します</li><li>(オプション)Group Managed Service Accounts(GMSA)を使用してPodのActive Directory IDを構成します</li></ul><h2 id=始める前に>始める前に</h2><ul><li><a href=/ja/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes>Windows Serverを実行するマスターノードとワーカーノード</a>を含むKubernetesクラスターを作成します</li><li>Kubernetes上にServiceとワークロードを作成してデプロイすることは、LinuxコンテナとWindowsコンテナ共に、ほぼ同じように動作することに注意してください。クラスターとのインタフェースとなる<a href=/docs/reference/kubectl/overview/>Kubectlコマンド</a>も同じです。Windowsコンテナをすぐに体験できる例を以下セクションに用意しています。</li></ul><h2 id=はじめに-windowsコンテナのデプロイ>はじめに:Windowsコンテナのデプロイ</h2><p>WindowsコンテナをKubernetesにデプロイするには、最初にサンプルアプリケーションを作成する必要があります。以下のYAMLファイルの例では、簡単なウェブサーバーアプリケーションを作成しています。以下の内容で<code>win-webserver.yaml</code>という名前のサービススペックを作成します。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># このサービスが提供するポート</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>NodePort<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windowswebserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- powershell.exe<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- -command<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:#b44>&#34;&lt;#code used from https://gist.github.com/19WAS85/5424431#&gt; ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add(&#39;http://*:80/&#39;) ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host(&#39;Listening at http://*:80/&#39;) ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host &#39;&#39; ;Write-Host(&#39;&gt; {0}&#39; -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count = $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header=&#39;&lt;html&gt;&lt;body&gt;&lt;H1&gt;Windows Container Web Server&lt;/H1&gt;&#39; ;$$callerCountsString=&#39;&#39; ;$$callerCounts.Keys | % { $$callerCountsString=&#39;&lt;p&gt;IP {0} callerCount {1} &#39; -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer=&#39;&lt;/body&gt;&lt;/html&gt;&#39; ;$$content=&#39;{0}{1}{2}&#39; -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host(&#39;&lt; {0}&#39; -f $$responseStatus)  } ; &#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> ポートマッピングもサポートされていますが、この例では簡単にするために、コンテナポート80がサービスに直接公開されています。</div><ol><li><p>すべてのノードが正常であることを確認します。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes
</span></span></code></pre></div></li><li><p>Serviceをデプロイして、Podの更新を確認します。:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f win-webserver.yaml
</span></span><span style=display:flex><span>kubectl get pods -o wide -w
</span></span></code></pre></div><p>Serviceが正しくデプロイされると、両方のPodがReadyとして表示されます。watch状態のコマンドを終了するには、Ctrl + Cを押します。</p></li><li><p>デプロイが成功したことを確認します。検証するために行うこと:</p><ul><li>WindowsノードのPodごとの2つのコンテナに<code>docker ps</code>します</li><li>Linuxマスターからリストされた2つのPodに<code>kubectl get pods</code>します</li><li>ネットワークを介したノードとPod間通信、LinuxマスターからのPod IPのポート80に向けて<code>curl</code>して、ウェブサーバーの応答をチェックします</li><li>docker execまたはkubectl execを使用したPod間通信、Pod間(および複数のWindowsノードがある場合はホスト間)へのpingします</li><li>ServiceからPodへの通信、Linuxマスターおよび個々のPodからの仮想Service IP(<code>kubectl get services</code>で表示される)に<code>curl</code>します</li><li>サービスディスカバリ、Kubernetesの<a href=/ja/docs/concepts/services-networking/dns-pod-service/#services>default DNS suffix</a>と共にService名に<code>curl</code>します</li><li>Inbound connectivity, <code>curl</code> the NodePort from the Linux master or machines outside of the cluster</li><li>インバウンド接続、Linuxマスターまたはクラスター外のマシンからNodePortに<code>curl</code>します</li><li>アウトバウンド接続、kubectl execを使用したPod内からの外部IPに<code>curl</code>します</li></ul></li></ol><div class="alert alert-info note callout" role=alert><strong>備考:</strong> 今のところ、Windowsネットワークスタックのプラットフォーム制限のため、Windowsコンテナホストは、ホストされているサービスのIPにアクセスできません。Service IPにアクセスできるのは、Windows Podだけです。</div><h2 id=可観測性>可観測性</h2><h3 id=ワークロードからのログキャプチャ>ワークロードからのログキャプチャ</h3><p>ログは可観測性の重要な要素です。これにより、ユーザーはワークロードの運用面に関する洞察を得ることができ、問題のトラブルシューティングの主要な要素になります。WindowsコンテナとWindowsコンテナ内のワークロードの動作はLinuxコンテナとは異なるため、ユーザーはログの収集に苦労し、運用の可視性が制限されていました。たとえば、Windowsワークロードは通常、ETW(Windowsのイベントトレース)にログを記録するか、アプリケーションイベントログにエントリをプッシュするように構成されます。Microsoftのオープンソースツールである<a href=https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor>LogMonitor</a>は、Windowsコンテナ内の構成されたログソースを監視するための推奨方法です。LogMonitorは、イベントログ、ETWプロバイダー、カスタムアプリケーションログのモニタリングをサポートしており、それらをSTDOUTにパイプして、<code>kubectl logs &lt;pod></code>で使用できます。</p><p>LogMonitor GitHubページの指示に従って、バイナリと構成ファイルをすべてのコンテナにコピーして、LogMonitorがログをSTDOUTにプッシュするために必要なエントリーポイントを追加します。</p><h2 id=構成可能なコンテナのユーザー名の使用>構成可能なコンテナのユーザー名の使用</h2><p>Kubernetes v1.16以降、Windowsコンテナは、イメージのデフォルトとは異なるユーザー名でエントリーポイントとプロセスを実行するように構成できます。これが達成される方法は、Linuxコンテナで行われる方法とは少し異なります。詳しくは<a href=/docs/tasks/configure-pod-container/configure-runasusername/>こちら</a>.</p><h2 id=group-managed-service-accountsによるワークロードidの管理>Group Managed Service AccountsによるワークロードIDの管理</h2><p>Kubernetes v1.14以降、Windowsコンテナワークロードは、Group Managed Service Accounts(GMSA)を使用するように構成できます。Group Managed Service Accountsは、自動パスワード管理、簡略化されたサービスプリンシパル名（SPN）管理、および複数のサーバー間で他の管理者に管理を委任する機能を提供する特定の種類のActive Directoryアカウントです。GMSAで構成されたコンテナは、GMSAで構成されたIDを保持しながら、外部Active Directoryドメインリソースにアクセスできます。Windowsコンテナ用のGMSAの構成と使用の詳細は<a href=/docs/tasks/configure-pod-container/configure-gmsa/>こちら</a>。</p><h2 id=taintsとtolerations>TaintsとTolerations</h2><p>今日のユーザーは、LinuxとWindowsのワークロードをそれぞれのOS固有のノードで維持するために、Taintsとノードセレクターのいくつかの組み合わせを使用する必要があります。これはおそらくWindowsユーザーにのみ負担をかけます。推奨されるアプローチの概要を以下に示します。主な目標の1つは、このアプローチによって既存のLinuxワークロードの互換性が損なわれないようにすることです。</p><h3 id=os固有のワークロードが適切なコンテナホストに確実に到達するようにする>OS固有のワークロードが適切なコンテナホストに確実に到達するようにする</h3><p>ユーザーは、TaintsとTolerationsを使用して、Windowsコンテナを適切なホストでスケジュールできるようにすることができます。現在、すべてのKubernetesノードには次のデフォルトラベルがあります。:</p><ul><li>kubernetes.io/os = [windows|linux]</li><li>kubernetes.io/arch = [amd64|arm64|...]</li></ul><p>Podの仕様で<code>"kubernetes.io/os": windows</code>のようなnodeSelectorが指定されていない場合、PodをWindowsまたはLinuxの任意のホストでスケジュールすることができます。WindowsコンテナはWindowsでのみ実行でき、LinuxコンテナはLinuxでのみ実行できるため、これは問題になる可能性があります。ベストプラクティスは、nodeSelectorを使用することです。</p><p>ただし、多くの場合、ユーザーには既存の多数のLinuxコンテナのdeployment、およびコミュニティHelmチャートのような既成構成のエコシステムやOperatorのようなプログラム的にPodを生成するケースがあることを理解しています。このような状況では、nodeSelectorsを追加するための構成変更をためらう可能性があります。代替策は、Taintsを使用することです。kubeletは登録中にTaintsを設定できるため、Windowsだけで実行する時に自動的にTaintを追加するように簡単に変更できます。</p><p>例:<code>--register-with-taints='os=windows:NoSchedule'</code></p><p>すべてのWindowsノードにTaintを追加することにより、それらには何もスケジュールされません（既存のLinuxPodを含む）。Windows PodがWindowsノードでスケジュールされるためには、nodeSelectorがWindowsを選択することと、適切にマッチするTolerationが必要です。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;os&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=同じクラスター内の複数windowsバージョンの管理>同じクラスター内の複数Windowsバージョンの管理</h3><p>各Podで使用されるWindows Serverのバージョンは、ノードのバージョンと一致している必要があります。
同じクラスター内で複数のWindows Serverバージョンを使用したい場合は、追加のノードラベルとnodeSelectorsを設定する必要があります。</p><p>Kubernetes 1.17では、これを簡単するために新しいラベル<code>node.kubernetes.io/windows-build</code>が自動的に追加されます。古いバージョンを実行している場合は、このラベルをWindowsノードに手動で追加することをお勧めします。</p><p>このラベルは、互換性のために一致する必要があるWindowsのメジャー、マイナー、およびビルド番号を反映しています。以下は、Windows Serverの各バージョンで現在使用されている値です。</p><table><thead><tr><th>製品番号 　　</th><th>ビルド番号</th></tr></thead><tbody><tr><td>Windows Server 2019</td><td>10.0.17763</td></tr><tr><td>Windows Server version 1809</td><td>10.0.17763</td></tr><tr><td>Windows Server version 1903</td><td>10.0.18362</td></tr></tbody></table><h3 id=runtimeclassによる簡素化>RuntimeClassによる簡素化</h3><p><a href=https://kubernetes.io/ja/docs/concepts/containers/runtime-class/>RuntimeClass</a>は、TaintsとTolerationsを使用するプロセスを簡略化するために使用できます。クラスター管理者は、これらのTaintsとTolerationsをカプセル化するために使用する<code>RuntimeClass</code>オブジェクトを作成できます。</p><ol><li>このファイルを<code>runtimeClasses.yml</code>に保存します。これには、Windows OS、アーキテクチャ、およびバージョンに適切な<code>nodeSelector</code>が含まれています。</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;docker&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduling</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;windows&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/arch</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;amd64&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>os<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Equal<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><ol><li>クラスター管理者として使用する<code>kubectl create -f runtimeClasses.yml</code>を実行します</li><li>Podの仕様に応じて<code>runtimeClassName: windows-2019</code>を追加します</li></ol><p>例:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>800Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>.1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>300Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span></code></pre></div></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/ja/docs/home/>ホーム</a>
<a class=text-white href=/ja/blog/>Blogs</a>
<a class=text-white href=/ja/training/>トレーニング</a>
<a class=text-white href=/ja/partners/>パートナー</a>
<a class=text-white href=/ja/community/>コミュニティ</a>
<a class=text-white href=/ja/case-studies/>ケーススタディ</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>