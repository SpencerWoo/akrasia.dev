<!doctype html><html lang=ja class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/production-environment/tools/kubeadm/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/setup/production-environment/tools/kubeadm/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/production-environment/tools/kubeadm/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/production-environment/tools/kubeadm/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>kubeadmを使ってクラスターを構築する | Kubernetes</title><meta property="og:title" content="kubeadmを使ってクラスターを構築する"><meta property="og:description" content="プロダクショングレードのコンテナ管理基盤"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="kubeadmを使ってクラスターを構築する"><meta itemprop=description content="プロダクショングレードのコンテナ管理基盤"><meta name=twitter:card content="summary"><meta name=twitter:title content="kubeadmを使ってクラスターを構築する"><meta name=twitter:description content="プロダクショングレードのコンテナ管理基盤"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/"><meta property="og:title" content="kubeadmを使ってクラスターを構築する"><meta name=twitter:title content="kubeadmを使ってクラスターを構築する"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/ja/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/ja/docs/>ドキュメント</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/blog/>Blogs</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/training/>トレーニング</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/partners/>パートナー</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/community/>コミュニティ</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/case-studies/>ケーススタディ</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>バージョン</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/ja/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/ja/docs/setup/production-environment/tools/kubeadm/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>日本語 (Japanese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/setup/production-environment/tools/kubeadm/>English</a>
<a class=dropdown-item href=/zh-cn/docs/setup/production-environment/tools/kubeadm/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/setup/production-environment/tools/kubeadm/>한국어 (Korean)</a>
<a class=dropdown-item href=/fr/docs/setup/production-environment/tools/kubeadm/>Français (French)</a>
<a class=dropdown-item href=/id/docs/setup/production-environment/tools/kubeadm/>Bahasa Indonesia</a>
<a class=dropdown-item href=/uk/docs/setup/production-environment/tools/kubeadm/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>これは、このセクションの複数ページの印刷可能なビューです。
<a href=# onclick="return print(),!1">印刷するには、ここをクリックしてください</a>.</p><p><a href=/ja/docs/setup/production-environment/tools/kubeadm/>このページの通常のビューに戻る</a>.</p></div><h1 class=title>kubeadmを使ってクラスターを構築する</h1><ul><li>1: <a href=#pg-29e59491dd6118b23072dfe9ebb93323>kubeadmのインストール</a></li><li>2: <a href=#pg-c3689df4b0c61a998e79d91a865aa244>kubeadmのトラブルシューティング</a></li><li>3: <a href=#pg-134ed1f6142a98e6ac681a1ba4920e53>kubeadmを使用したクラスターの作成</a></li><li>4: <a href=#pg-4c656c5eda3e1c06ad1aedebdc04a211>kubeadmを使ったコントロールプレーンの設定のカスタマイズ</a></li><li>5: <a href=#pg-015edbc7cc688d31b1d1edce7c186135>高可用性トポロジーのためのオプション</a></li><li>6: <a href=#pg-3941d5c3409342219bf7e03128b8ecb6>kubeadmを使用した高可用性クラスターの作成</a></li><li>7: <a href=#pg-8160424c22d24f7d2d63c521e107dbf8>kubeadmを使用した高可用性etcdクラスターの作成</a></li><li>8: <a href=#pg-07709e71de6b4ac2573041c31213dbeb>kubeadmを使用したクラスター内の各kubeletの設定</a></li><li>9: <a href=#pg-ed857e09999827b013ee9062dc9c59bb>コントロールプレーンをセルフホストするようにkubernetesクラスターを構成する</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-29e59491dd6118b23072dfe9ebb93323>1 - kubeadmのインストール</h1><img src=/images/kubeadm-stacked-color.png align=right width=150px><p>このページでは<code>kubeadm</code>コマンドをインストールする方法を示します。このインストール処理実行後にkubeadmを使用してクラスターを作成する方法については、<a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>kubeadmを使用したシングルマスタークラスターの作成</a>を参照してください。</p><h2 id=始める前に>始める前に</h2><ul><li>次のいずれかが動作しているマシンが必要です<ul><li>Ubuntu 16.04+</li><li>Debian 9+</li><li>CentOS 7</li><li>Red Hat Enterprise Linux (RHEL) 7</li><li>Fedora 25+</li><li>HypriotOS v1.0.1+</li><li>Container Linux (tested with 1800.6.0)</li></ul></li><li>1台あたり2GB以上のメモリ(2GBの場合、アプリ用のスペースはほとんどありません)</li><li>2コア以上のCPU</li><li>クラスター内のすべてのマシン間で通信可能なネットワーク(パブリックネットワークでもプライベートネットワークでも構いません)</li><li>ユニークなhostname、MACアドレス、とproduct_uuidが各ノードに必要です。詳細は<a href=#MAC%E3%82%A2%E3%83%89%E3%83%AC%E3%82%B9%E3%81%A8product_uuid%E3%81%8C%E5%85%A8%E3%81%A6%E3%81%AE%E3%83%8E%E3%83%BC%E3%83%89%E3%81%A7%E3%83%A6%E3%83%8B%E3%83%BC%E3%82%AF%E3%81%A7%E3%81%82%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%A4%9C%E8%A8%BC>ここ</a>を参照してください。</li><li>マシン内の特定のポートが開いていること。詳細は<a href=#%E5%BF%85%E9%A0%88%E3%83%9D%E3%83%BC%E3%83%88%E3%81%AE%E7%A2%BA%E8%AA%8D>ここ</a>を参照してください。</li><li>Swapがオフであること。kubeletが正常に動作するためにはswapは<strong>必ず</strong>オフでなければなりません。</li></ul><h2 id=macアドレスとproduct-uuidが全てのノードでユニークであることの検証>MACアドレスとproduct_uuidが全てのノードでユニークであることの検証</h2><ul><li>ネットワークインターフェースのMACアドレスは<code>ip link</code>もしくは<code>ifconfig -a</code>コマンドで取得できます。</li><li>product_uuidは<code>sudo cat /sys/class/dmi/id/product_uuid</code>コマンドで確認できます。</li></ul><p>ハードウェアデバイスではユニークなアドレスが割り当てられる可能性が非常に高いですが、VMでは同じになることがあります。Kubernetesはこれらの値を使用して、クラスター内のノードを一意に識別します。これらの値が各ノードに固有ではない場合、インストール処理が<a href=https://github.com/kubernetes/kubeadm/issues/31>失敗</a>することもあります。</p><h2 id=ネットワークアダプタの確認>ネットワークアダプタの確認</h2><p>複数のネットワークアダプターがあり、Kubernetesコンポーネントにデフォルトで到達できない場合、IPルートを追加して、Kubernetesクラスターのアドレスが適切なアダプターを経由するように設定することをお勧めします。</p><h2 id=iptablesがブリッジを通過するトラフィックを処理できるようにする>iptablesがブリッジを通過するトラフィックを処理できるようにする</h2><p>Linuxノードのiptablesがブリッジを通過するトラフィックを正確に処理する要件として、<code>net.bridge.bridge-nf-call-iptables</code>を<code>sysctl</code>の設定ファイルで1に設定してください。例えば以下のようにします。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; /etc/sysctl.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-iptables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>sysctl --system
</span></span></code></pre></div><p>この手順の前に<code>br_netfilter</code>モジュールがロードされていることを確認してください。<code>lsmod | grep br_netfilter</code>を実行することで確認できます。明示的にロードするには<code>modprobe br_netfilter</code>を実行してください。</p><p>詳細は<a href=https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>ネットワークプラグインの要件</a>を参照してください。</p><h2 id=iptablesがnftablesバックエンドを使用しないようにする>iptablesがnftablesバックエンドを使用しないようにする</h2><p>Linuxでは、カーネルのiptablesサブシステムの最新の代替品としてnftablesが利用できます。<code>iptables</code>ツールは互換性レイヤーとして機能し、iptablesのように動作しますが、実際にはnftablesを設定します。このnftablesバックエンドは現在のkubeadmパッケージと互換性がありません。(ファイアウォールルールが重複し、<code>kube-proxy</code>を破壊するためです。)</p><p>もしあなたのシステムの<code>iptables</code>ツールがnftablesバックエンドを使用している場合、これらの問題を避けるために<code>iptables</code>ツールをレガシーモードに切り替える必要があります。これは、少なくともDebian 10(Buster)、Ubuntu 19.04、Fedora 29、およびこれらのディストリビューションの新しいリリースでのデフォルトです。RHEL 8はレガシーモードへの切り替えをサポートしていないため、現在のkubeadmパッケージと互換性がありません。</p><ul class="nav nav-tabs" id=iptables-legacy role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#iptables-legacy-0 role=tab aria-controls=iptables-legacy-0 aria-selected=true>DebianまたはUbuntu</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#iptables-legacy-1 role=tab aria-controls=iptables-legacy-1>Fedora</a></li></ul><div class=tab-content id=iptables-legacy><div id=iptables-legacy-0 class="tab-pane show active" role=tabpanel aria-labelledby=iptables-legacy-0><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># レガシーバイナリがインストールされていることを確認してください</span>
</span></span><span style=display:flex><span>sudo apt-get install -y iptables arptables ebtables
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># レガシーバージョンに切り替えてください。</span>
</span></span><span style=display:flex><span>sudo update-alternatives --set iptables /usr/sbin/iptables-legacy
</span></span><span style=display:flex><span>sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy
</span></span><span style=display:flex><span>sudo update-alternatives --set arptables /usr/sbin/arptables-legacy
</span></span><span style=display:flex><span>sudo update-alternatives --set ebtables /usr/sbin/ebtables-legacy
</span></span></code></pre></div></div><div id=iptables-legacy-1 class=tab-pane role=tabpanel aria-labelledby=iptables-legacy-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>update-alternatives --set iptables /usr/sbin/iptables-legacy
</span></span></code></pre></div></div></div><h2 id=必須ポートの確認>必須ポートの確認</h2><h3 id=コントロールプレーンノード>コントロールプレーンノード</h3><table><thead><tr><th>プロトコル</th><th>通信の向き</th><th>ポート範囲</th><th>目的</th><th>使用者</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>6443*</td><td>Kubernetes API server</td><td>全て</td></tr><tr><td>TCP</td><td>Inbound</td><td>2379-2380</td><td>etcd server client API</td><td>kube-apiserver、etcd</td></tr><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>Kubelet API</td><td>自身、コントロールプレーン</td></tr><tr><td>TCP</td><td>Inbound</td><td>10251</td><td>kube-scheduler</td><td>自身</td></tr><tr><td>TCP</td><td>Inbound</td><td>10252</td><td>kube-controller-manager</td><td>自身</td></tr></tbody></table><h3 id=ワーカーノード>ワーカーノード</h3><table><thead><tr><th>プロトコル</th><th>通信の向き</th><th>ポート範囲</th><th>目的</th><th>使用者</th></tr></thead><tbody><tr><td>TCP</td><td>Inbound</td><td>10250</td><td>Kubelet API</td><td>自身、コントロールプレーン</td></tr><tr><td>TCP</td><td>Inbound</td><td>30000-32767</td><td>NodePort Service†</td><td>全て</td></tr></tbody></table><p>† <a href=/ja/docs/concepts/services-networking/service/>NodePort Service</a>のデフォルトのポートの範囲</p><p>*の項目は書き換え可能です。そのため、あなたが指定したカスタムポートも開いていることを確認する必要があります。</p><p>etcdポートはコントロールプレーンノードに含まれていますが、独自のetcdクラスターを外部またはカスタムポートでホストすることもできます。</p><p>使用するPodネットワークプラグイン(以下を参照)のポートも開く必要があります。これは各Podネットワークプラグインによって異なるため、必要なポートについてはプラグインのドキュメントを参照してください。</p><h2 id=installing-runtime>ランタイムのインストール</h2><p>Podのコンテナを実行するために、Kubernetesは<a class=glossary-tooltip title=コンテナランタイムは、コンテナの実行を担当するソフトウェアです。 data-toggle=tooltip data-placement=top href=/ja/docs/setup/production-environment/container-runtimes target=_blank aria-label=コンテナランタイム>コンテナランタイム</a>を使用します。</p><ul class="nav nav-tabs" id=container-runtime role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#container-runtime-0 role=tab aria-controls=container-runtime-0 aria-selected=true>Linuxノード</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#container-runtime-1 role=tab aria-controls=container-runtime-1>その他のOS</a></li></ul><div class=tab-content id=container-runtime><div id=container-runtime-0 class="tab-pane show active" role=tabpanel aria-labelledby=container-runtime-0><p><p>デフォルトでは、Kubernetesは選択されたコンテナランタイムと通信するために<a class=glossary-tooltip title='An API for container runtimes to integrate with kubelet' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#container-runtime target=_blank aria-label='Container Runtime Interface'>Container Runtime Interface</a> (CRI)を使用します。</p><p>ランタイムを指定しない場合、kubeadmはよく知られたUnixドメインソケットのリストをスキャンすることで、インストールされたコンテナランタイムの検出を試みます。
次の表がコンテナランタイムと関連するソケットのパスリストです。</p><table><caption style=display:none>コンテナランタイムとソケットパス</caption><thead><tr><th>ランタイム</th><th>Unixドメインソケットのパス</th></tr></thead><tbody><tr><td>Docker</td><td><code>/var/run/docker.sock</code></td></tr><tr><td>containerd</td><td><code>/run/containerd/containerd.sock</code></td></tr><tr><td>CRI-O</td><td><code>/var/run/crio/crio.sock</code></td></tr></tbody></table><br>Dockerとcontainerdの両方が同時に検出された場合、Dockerが優先されます。Docker 18.09にはcontainerdが同梱されており、両方が検出可能であるため、この仕様が必要です。他の2つ以上のランタイムが検出された場合、kubeadmは適切なエラーメッセージで終了します。<p>kubeletは、組み込まれた<code>dockershim</code>CRIを通してDockerと連携します。</p><p>詳細は、<a href=/ja/docs/setup/production-environment/container-runtimes/>コンテナランタイム</a>を参照してください。</p></div><div id=container-runtime-1 class=tab-pane role=tabpanel aria-labelledby=container-runtime-1><p><p>デフォルトでは、kubeadmは<a class=glossary-tooltip title=Dockerは、コンテナとして知られる、オペレーティングシステムレベルでの仮想化を提供するソフトウェア技術です。 data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a>をコンテナランタイムとして使用します。
kubeletは、組み込まれた<code>dockershim</code>CRIを通してDockerと連携します。</p><p>詳細は、<a href=/ja/docs/setup/production-environment/container-runtimes/>コンテナランタイム</a>を参照してください。</p></div></div><h2 id=kubeadm-kubelet-kubectlのインストール>kubeadm、kubelet、kubectlのインストール</h2><p>以下のパッケージをマシン上にインストールしてください</p><ul><li><p><code>kubeadm</code>: クラスターを起動するコマンドです。</p></li><li><p><code>kubelet</code>: クラスター内のすべてのマシンで実行されるコンポーネントです。
Podやコンテナの起動などを行います。</p></li><li><p><code>kubectl</code>: クラスターにアクセスするためのコマンドラインツールです。</p></li></ul><p>kubeadmは<code>kubelet</code>や<code>kubectl</code>をインストールまたは管理<strong>しない</strong>ため、kubeadmにインストールするKubernetesコントロールプレーンのバージョンと一致させる必要があります。そうしないと、予期しないバグのある動作につながる可能性のあるバージョン差異(version skew)が発生するリスクがあります。ただし、kubeletとコントロールプレーン間のマイナーバージョン差異(minor version skew)は_1つ_サポートされていますが、kubeletバージョンがAPIサーバーのバージョンを超えることはできません。たとえば、1.7.0を実行するkubeletは1.8.0 APIサーバーと完全に互換性がありますが、その逆はできません。</p><p><code>kubectl</code>のインストールに関する詳細情報は、<a href=/ja/docs/tasks/tools/install-kubectl/>kubectlのインストールおよびセットアップ</a>を参照してください。</p><div class="alert alert-danger warning callout" role=alert><strong>警告:</strong> これらの手順はシステムアップグレードによるすべてのKubernetesパッケージの更新を除きます。これはkubeadmとKubernetesが<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>アップグレードにおける特別な注意</a>を必要とするからです。</div><p>バージョン差異(version skew)に関しては下記を参照してください。</p><ul><li>Kubernetes <a href=/ja/docs/setup/release/version-skew-policy/>Kubernetesバージョンとバージョンスキューサポートポリシー</a></li><li>Kubeadm-specific <a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy>バージョン互換ポリシー</a></li></ul><ul class="nav nav-tabs" id=k8s-install role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-0 role=tab aria-controls=k8s-install-0 aria-selected=true>Ubuntu、Debian、またはHypriotOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-1 role=tab aria-controls=k8s-install-1>CentOS、RHEL、またはFedora</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-2 role=tab aria-controls=k8s-install-2>Container Linux</a></li></ul><div class=tab-content id=k8s-install><div id=k8s-install-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-0><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo apt-get update <span style=color:#666>&amp;&amp;</span> sudo apt-get install -y apt-transport-https curl
</span></span><span style=display:flex><span>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span></span><span style=display:flex><span><span style=color:#b44>deb https://apt.kubernetes.io/ kubernetes-xenial main
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install -y kubelet kubeadm kubectl
</span></span><span style=display:flex><span>sudo apt-mark hold kubelet kubeadm kubectl
</span></span></code></pre></div></div><div id=k8s-install-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
</span></span></span><span style=display:flex><span><span style=color:#b44>[kubernetes]
</span></span></span><span style=display:flex><span><span style=color:#b44>name=Kubernetes
</span></span></span><span style=display:flex><span><span style=color:#b44>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
</span></span></span><span style=display:flex><span><span style=color:#b44>enabled=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#b44>repo_gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># SELinuxをpermissiveモードに設定する(効果的に無効化する)</span>
</span></span><span style=display:flex><span>setenforce <span style=color:#666>0</span>
</span></span><span style=display:flex><span>sed -i <span style=color:#b44>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>yum install -y kubelet kubeadm kubectl --disableexcludes<span style=color:#666>=</span>kubernetes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div><p><strong>Note:</strong></p><ul><li><code>setenforce 0</code>および<code>sed ...</code>を実行することによりSELinuxをpermissiveモードに設定し、効果的に無効化できます。
これはコンテナがホストのファイルシステムにアクセスするために必要です。例えば、Podのネットワークに必要とされます。
kubeletにおけるSELinuxのサポートが改善されるまでは、これを実行しなければなりません。</li></ul></div><div id=k8s-install-2 class=tab-pane role=tabpanel aria-labelledby=k8s-install-2><p><p>CNIプラグインをインストールする(ほとんどのPodのネットワークに必要です):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.8.2&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>mkdir -p /opt/cni/bin
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cni-plugins-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tgz&#34;</span> | tar -C /opt/cni/bin -xz
</span></span></code></pre></div><p>crictlをインストールする (kubeadm / Kubelet Container Runtime Interface (CRI)に必要です)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.22.0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/crictl-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tar.gz&#34;</span> | sudo tar -C <span style=color:#b8860b>$DOWNLOAD_DIR</span> -xz
</span></span></code></pre></div><p><code>kubeadm</code>、<code>kubelet</code>、<code>kubectl</code>をインストールし<code>kubelet</code>をsystemd serviceに登録します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>RELEASE</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>curl -sSL https://dl.k8s.io/release/stable.txt<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>mkdir -p /opt/bin
</span></span><span style=display:flex><span><span style=color:#a2f>cd</span> /opt/bin
</span></span><span style=display:flex><span>curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span>/bin/linux/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span>/<span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>
</span></span><span style=display:flex><span>chmod +x <span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/kubernetes/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/build/debs/kubelet.service&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:/opt/bin:g&#34;</span> &gt; /etc/systemd/system/kubelet.service
</span></span><span style=display:flex><span>mkdir -p /etc/systemd/system/kubelet.service.d
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/kubernetes/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/build/debs/10-kubeadm.conf&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:/opt/bin:g&#34;</span> &gt; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</span></span></code></pre></div><p><code>kubelet</code>を有効化し起動します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div></div></div><p>kubeadmが何をすべきか指示するまで、kubeletはクラッシュループで数秒ごとに再起動します。</p><h2 id=コントロールプレーンノードのkubeletによって使用されるcgroupドライバーの設定>コントロールプレーンノードのkubeletによって使用されるcgroupドライバーの設定</h2><p>Dockerを使用した場合、kubeadmは自動的にkubelet向けのcgroupドライバーを検出し、それを実行時に<code>/var/lib/kubelet/kubeadm-flags.env</code>ファイルに設定します。</p><p>もしあなたが異なるCRIを使用している場合、<code>/etc/default/kubelet</code>(CentOS、RHEL、Fedoraでは<code>/etc/sysconfig/kubelet</code>)ファイル内の<code>cgroup-driver</code>の値を以下のように変更する必要があります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>KUBELET_EXTRA_ARGS</span><span style=color:#666>=</span>--cgroup-driver<span style=color:#666>=</span>&lt;value&gt;
</span></span></code></pre></div><p>このファイルは、kubeletの追加のユーザー定義引数を取得するために、<code>kubeadm init</code>および<code>kubeadm join</code>によって使用されます。</p><p>CRIのcgroupドライバーが<code>cgroupfs</code>でない場合に<strong>のみ</strong>それを行う必要があることに注意してください。なぜなら、これはすでにkubeletのデフォルト値であるためです。</p><p>kubeletをリスタートする方法:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div><p>CRI-Oやcontainerdといった他のコンテナランタイムのcgroup driverは実行中に自動的に検出されます。</p><h2 id=トラブルシュート>トラブルシュート</h2><p>kubeadmで問題が発生した場合は、<a href=/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>トラブルシューティング</a>を参照してください。</p><h2 id=次の項目>次の項目</h2><ul><li><a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>kubeadmを使用したシングルコントロールプレーンクラスターの作成</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c3689df4b0c61a998e79d91a865aa244>2 - kubeadmのトラブルシューティング</h1><p>どのプログラムでもそうですが、kubeadmのインストールや実行でエラーが発生することがあります。このページでは、一般的な失敗例をいくつか挙げ、問題を理解して解決するための手順を示しています。</p><p>本ページに問題が記載されていない場合は、以下の手順を行ってください:</p><ul><li><p>問題がkubeadmのバグによるものと思った場合:</p><ul><li><a href=https://github.com/kubernetes/kubeadm/issues>github.com/kubernetes/kubeadm</a>にアクセスして、既存のIssueを探してください。</li><li>Issueがない場合は、テンプレートにしたがって<a href=https://github.com/kubernetes/kubeadm/issues/new>新しくIssueを立ててください</a>。</li></ul></li><li><p>kubeadmがどのように動作するかわからない場合は、<a href=http://slack.k8s.io/>Slack</a>の#kubeadmチャンネルで質問するか、<a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a>で質問をあげてください。その際は、他の方が助けを出しやすいように<code>#kubernetes</code>や<code>#kubeadm</code>といったタグをつけてください。</p></li></ul><h2 id=rbacがないため-v1-18ノードをv1-17クラスタに結合できない>RBACがないため、v1.18ノードをv1.17クラスタに結合できない</h2><p>v1.18では、同名のノードが既に存在する場合にクラスタ内のノードに参加しないようにする機能を追加しました。これには、ブートストラップトークンユーザがNodeオブジェクトをGETできるようにRBACを追加する必要がありました。</p><p>しかし、これによりv1.18の<code>kubeadm join</code>がkubeadm v1.17で作成したクラスタに参加できないという問題が発生します。</p><p>この問題を回避するには、次の2つの方法があります。</p><ul><li><p>kubeadm v1.18を用いて、コントロールプレーンノード上で<code>kubeadm init phase bootstrap-token</code>を実行します。
これには、ブートストラップトークンの残りのパーミッションも同様に有効にすることに注意してください。</p></li><li><p><code>kubectl apply -f ...</code>を使って以下のRBACを手動で適用します。</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRoleBinding<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>roleRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Group<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:bootstrappers:kubeadm:default-node-token<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=インストール中に-ebtables-もしくは他の似たような実行プログラムが見つからない>インストール中に<code>ebtables</code>もしくは他の似たような実行プログラムが見つからない</h2><p><code>kubeadm init</code>の実行中に以下のような警告が表示された場合は、以降に記載するやり方を行ってください。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ebtables not found in system path
</span></span><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ethtool not found in system path
</span></span></code></pre></div><p>このような場合、ノード上に<code>ebtables</code>, <code>ethtool</code>などの実行ファイルがない可能性があります。これらをインストールするには、以下のコマンドを実行します。</p><ul><li>Ubuntu/Debianユーザーは、<code>apt install ebtables ethtool</code>を実行してください。</li><li>CentOS/Fedoraユーザーは、<code>yum install ebtables ethtool</code>を実行してください。</li></ul><h2 id=インストール中にkubeadmがコントロールプレーンを待ち続けて止まる>インストール中にkubeadmがコントロールプレーンを待ち続けて止まる</h2><p>以下のを出力した後に<code>kubeadm init</code>が止まる場合は、<code>kubeadm init</code>を実行してください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#666>[</span>apiclient<span style=color:#666>]</span> Created API client, waiting <span style=color:#a2f;font-weight:700>for</span> the control plane to become ready
</span></span></code></pre></div><p>これはいくつかの問題が原因となっている可能性があります。最も一般的なのは:</p><ul><li><p>ネットワーク接続の問題が挙げられます。続行する前に、お使いのマシンがネットワークに完全に接続されていることを確認してください。</p></li><li><p>kubeletのデフォルトのcgroupドライバの設定がDockerで使用されているものとは異なっている場合も考えられます。
システムログファイル(例: <code>/var/log/message</code>)をチェックするか、<code>journalctl -u kubelet</code>の出力を調べてください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>error: failed to run Kubelet: failed to create kubelet:
</span></span><span style=display:flex><span>misconfiguration: kubelet cgroup driver: <span style=color:#b44>&#34;systemd&#34;</span> is different from docker cgroup driver: <span style=color:#b44>&#34;cgroupfs&#34;</span>
</span></span></code></pre></div><p>以上のようなエラーが現れていた場合、cgroupドライバの問題を解決するには、以下の2つの方法があります:</p></li></ul><ol><li><p><a href=/ja/docs/setup/independent/install-kubeadm/#installing-docker>ここ</a>の指示に従ってDockerを再度インストールします。</p></li><li><p>Dockerのcgroupドライバに合わせてkubeletの設定を手動で変更します。その際は、<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-master-node>マスターノード上でkubeletが使用するcgroupドライバを設定する</a>を参照してください。</p></li></ol><ul><li>control plane Dockerコンテナがクラッシュループしたり、ハングしたりしています。これは<code>docker ps</code>を実行し、<code>docker logs</code>を実行して各コンテナを調査することで確認できます。</li></ul><h2 id=管理コンテナを削除する時にkubeadmが止まる>管理コンテナを削除する時にkubeadmが止まる</h2><p>Dockerが停止して、Kubernetesで管理されているコンテナを削除しないと、以下のようなことが起こる可能性があります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo kubeadm reset
</span></span><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> Running pre-flight checks
</span></span><span style=display:flex><span><span style=color:#666>[</span>reset<span style=color:#666>]</span> Stopping the kubelet service
</span></span><span style=display:flex><span><span style=color:#666>[</span>reset<span style=color:#666>]</span> Unmounting mounted directories in <span style=color:#b44>&#34;/var/lib/kubelet&#34;</span>
</span></span><span style=display:flex><span><span style=color:#666>[</span>reset<span style=color:#666>]</span> Removing kubernetes-managed containers
</span></span><span style=display:flex><span><span style=color:#666>(</span>block<span style=color:#666>)</span>
</span></span></code></pre></div><p>考えられる解決策は、Dockerサービスを再起動してから<code>kubeadm reset</code>を再実行することです:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo systemctl restart docker.service
</span></span><span style=display:flex><span>sudo kubeadm reset
</span></span></code></pre></div><p>dockerのログを調べるのも有効な場合があります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>journalctl -u docker
</span></span></code></pre></div><h2 id=podの状態が-runcontainererror-crashloopbackoff-または-error-となる>Podの状態が<code>RunContainerError</code>、<code>CrashLoopBackOff</code>、または<code>Error</code>となる</h2><p><code>kubeadm init</code>の直後には、これらの状態ではPodは存在しないはずです。</p><ul><li><code>kubeadm init</code>の <em>直後</em> にこれらの状態のいずれかにPodがある場合は、kubeadmのリポジトリにIssueを立ててください。ネットワークソリューションをデプロイするまでは<code>coredns</code>(または<code>kube-dns</code>)は<code>Pending</code>状態でなければなりません。</li><li>ネットワークソリューションをデプロイしても<code>coredns</code>(または<code>kube-dns</code>)に何も起こらない場合にRunContainerError<code>、</code>CrashLoopBackOff<code>、</code>Error`の状態でPodが表示された場合は、インストールしたPodネットワークソリューションが壊れている可能性が高いです。より多くのRBACの特権を付与するか、新しいバージョンを使用する必要があるかもしれません。PodネットワークプロバイダのイシュートラッカーにIssueを出して、そこで問題をトリアージしてください。</li><li>1.12.1よりも古いバージョンのDockerをインストールした場合は、<code>systemd</code>で<code>dockerd</code>を起動する際に<code>MountFlags=slave</code>オプションを削除して<code>docker</code>を再起動してください。マウントフラグは<code>/usr/lib/systemd/system/docker.service</code>で確認できます。MountFlagsはKubernetesがマウントしたボリュームに干渉し、Podsを<code>CrashLoopBackOff</code>状態にすることがあります。このエラーは、Kubernetesが<code>var/run/secrets/kubernetes.io/serviceaccount</code>ファイルを見つけられない場合に発生します。</li></ul><h2 id=coredns-もしくは-kube-dns-が-pending-状態でスタックする><code>coredns</code>(もしくは<code>kube-dns</code>)が<code>Pending</code>状態でスタックする</h2><p>kubeadmはネットワークプロバイダに依存しないため、管理者は選択した<a href=/docs/concepts/cluster-administration/addons/>Podネットワークソリューションをインストール</a>をする必要があります。CoreDNSを完全にデプロイする前にPodネットワークをインストールする必要があります。したがって、ネットワークがセットアップされる前の <code>Pending</code>状態になります。</p><h2 id=hostport-サービスが動かない><code>HostPort</code>サービスが動かない</h2><p><code>HostPort</code>と<code>HostIP</code>の機能は、ご使用のPodネットワークプロバイダによって利用可能です。Podネットワークソリューションの作者に連絡して、<code>HostPort</code>と<code>HostIP</code>機能が利用可能かどうかを確認してください。</p><p>Calico、Canal、FlannelのCNIプロバイダは、HostPortをサポートしていることが確認されています。</p><p>詳細については、[CNI portmap documentation] (<a href=https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md>https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md</a>) を参照してください。</p><p>ネットワークプロバイダが portmap CNI プラグインをサポートしていない場合は、<a href=/ja/docs/concepts/services-networking/service/#nodeport>NodePortサービス</a>を使用するか、<code>HostNetwork=true</code>を使用してください。</p><h2 id=サービスip経由でpodにアクセスすることができない>サービスIP経由でPodにアクセスすることができない</h2><ul><li><p>多くのネットワークアドオンは、PodがサービスIPを介して自分自身にアクセスできるようにする<a href=/ja/docs/tasks/debug-application-cluster/debug-service/#a-pod-cannot-reach-itself-via-service-ip>ヘアピンモード</a>を有効にしていません。これは<a href=https://github.com/containernetworking/cni/issues/476>CNI</a>に関連する問題です。ヘアピンモードのサポート状況については、ネットワークアドオンプロバイダにお問い合わせください。</p></li><li><p>VirtualBoxを使用している場合(直接またはVagrant経由)は、<code>hostname -i</code>がルーティング可能なIPアドレスを返すことを確認する必要があります。デフォルトでは、最初のインターフェースはルーティング可能でないホスト専用のネットワークに接続されています。これを回避するには<code>/etc/hosts</code>を修正する必要があります。例としてはこの<a href=https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11>Vagrantfile</a>を参照してください。</p></li></ul><h2 id=tls証明書のエラー>TLS証明書のエラー</h2><p>以下のエラーは、証明書の不一致の可能性を示しています。</p><pre tabindex=0><code class=language-none data-lang=none># kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &#34;crypto/rsa: verification error&#34; while trying to verify candidate authority certificate &#34;kubernetes&#34;)
</code></pre><ul><li><p><code>HOME/.kube/config</code>ファイルに有効な証明書が含まれていることを確認し、必要に応じて証明書を再生成します。kubeconfigファイル内の証明書はbase64でエンコードされています。証明書をデコードするには<code>base64 --decode</code>コマンドを、証明書情報を表示するには<code>openssl x509 -text -noout</code>コマンドを用いてください。</p></li><li><p>環境変数<code>KUBECONFIG</code>の設定を解除するには以下のコマンドを実行するか:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>unset</span> KUBECONFIG
</span></span></code></pre></div><p>設定をデフォルトの<code>KUBECONFIG</code>の場所に設定します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div></li><li><p>もう一つの回避策は、既存の<code>kubeconfig</code>を"admin"ユーザに上書きすることです:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>mv  <span style=color:#b8860b>$HOME</span>/.kube <span style=color:#b8860b>$HOME</span>/.kube.bak
</span></span><span style=display:flex><span>mkdir <span style=color:#b8860b>$HOME</span>/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div></li></ul><h2 id=vagrant内でpodネットワークとしてflannelを使用する時のデフォルトnic>Vagrant内でPodネットワークとしてflannelを使用する時のデフォルトNIC</h2><p>以下のエラーは、Podネットワークに何か問題があったことを示している可能性を示しています:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Error from server <span style=color:#666>(</span>NotFound<span style=color:#666>)</span>: the server could not find the requested resource
</span></span></code></pre></div><ul><li><p>Vagrant内のPodネットワークとしてflannelを使用している場合は、flannelのデフォルトのインターフェース名を指定する必要があります。</p><p>Vagrantは通常、2つのインターフェースを全てのVMに割り当てます。1つ目は全てのホストにIPアドレス<code>10.0.2.15</code>が割り当てられており、NATされる外部トラフィックのためのものです。</p><p>これは、ホストの最初のインターフェイスをデフォルトにしているflannelの問題につながるかもしれません。これは、すべてのホストが同じパブリックIPアドレスを持っていると考えます。これを防ぐには、2番目のインターフェイスが選択されるように <code>--iface eth1</code>フラグをflannelに渡してください。</p></li></ul><h2 id=公開されていないipがコンテナに使われている>公開されていないIPがコンテナに使われている</h2><p>状況によっては、<code>kubectl logs</code>や<code>kubectl run</code>コマンドが以下のようなエラーを返すことがあります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
</span></span></code></pre></div><ul><li><p>これには、おそらくマシンプロバイダのポリシーによって、一見同じサブネット上の他のIPと通信できないIPをKubernetesが使用している可能性があります。</p></li><li><p>DigitalOceanはパブリックIPとプライベートIPを<code>eth0</code>に割り当てていますが、<code>kubelet</code>はパブリックIPではなく、ノードの<code>InternalIP</code>として後者を選択します。</p><p><code>ifconfig</code>ではエイリアスIPアドレスが表示されないため、<code>ifconfig</code>の代わりに<code>ip addr show</code>を使用してこのシナリオをチェックしてください。あるいは、DigitalOcean専用のAPIエンドポイントを使用して、ドロップレットからアンカーIPを取得することもできます:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
</span></span></code></pre></div><p>回避策としては、<code>--node-ip</code>を使ってどのIPを使うかを<code>kubelet</code>に伝えることです。DigitalOceanを使用する場合、オプションのプライベートネットワークを使用したい場合は、パブリックIP（<code>eth0</code>に割り当てられている）かプライベートIP（<code>eth1</code>に割り当てられている）のどちらかを指定します。これにはkubeadm <code>NodeRegistrationOptions</code>構造体の <a href=https://github.com/kubernetes/kubernetes/blob/release-1.13/cmd/kubeadm/app/apis/kubeadm/v1beta1/types.go><code>KubeletExtraArgs</code>セクション</a> が利用できます。</p><p><code>kubelet</code>を再起動してください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li></ul><h2 id=coredns-のpodが-crashloopbackoff-もしくは-error-状態になる><code>coredns</code>のPodが<code>CrashLoopBackOff</code>もしくは<code>Error</code>状態になる</h2><p>SELinuxを実行しているノードで古いバージョンのDockerを使用している場合、<code>coredns</code> Podが起動しないということが起きるかもしれません。この問題を解決するには、以下のオプションのいずれかを試してみてください:</p><ul><li><p><a href=/ja/docs/setup/independent/install-kubeadm/#installing-docker>新しいDockerのバージョン</a>にアップグレードする。</p></li><li><p><a href=https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux>SELinuxを無効化する</a>。</p></li><li><p><code>coredns</code>を変更して、<code>allowPrivilegeEscalation</code>を<code>true</code>に設定:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kube-system get deployment coredns -o yaml | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  sed <span style=color:#b44>&#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g&#39;</span> | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  kubectl apply -f -
</span></span></code></pre></div><p>CoreDNSに<code>CrashLoopBackOff</code>が発生する別の原因は、KubernetesにデプロイされたCoreDNS Podがループを検出したときに発生します。CoreDNSがループを検出して終了するたびに、KubernetesがCoreDNS Podを再起動しようとするのを避けるために、<a href=https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters>いくつかの回避策</a>が用意されています。</p><div class="alert alert-danger warning callout" role=alert><strong>警告:</strong> SELinuxを無効にするか<code>allowPrivilegeEscalation</code>を<code>true</code>に設定すると、クラスタのセキュリティが損なわれる可能性があります。</div><h2 id=etcdのpodが継続的に再起動する>etcdのpodが継続的に再起動する</h2><p>以下のエラーが発生した場合は:</p><pre tabindex=0><code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &#34;process_linux.go:110: decoding init error from pipe caused \&#34;read parent: connection reset by peer\&#34;&#34;
</code></pre><p>この問題は、CentOS 7をDocker 1.13.1.84で実行した場合に表示されます。このバージョンのDockerでは、kubeletがetcdコンテナに実行されないようにすることができます。</p><p>この問題を回避するには、以下のいずれかのオプションを選択します:</p><ul><li>1.13.1-75のような以前のバージョンのDockerにロールバックする</li></ul><pre tabindex=0><code>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
</code></pre><ul><li>18.06のような最新の推奨バージョンをインストールする:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span>yum install docker-ce-18.06.1.ce-3.el7.x86_64
</span></span></code></pre></div><h2 id=コンマで区切られた値のリストを-component-extra-args-フラグ内の引数に渡すことができない>コンマで区切られた値のリストを<code>--component-extra-args</code>フラグ内の引数に渡すことができない</h2><p><code>-component-extra-args</code>のような<code>kubeadm init</code>フラグを使うと、kube-apiserverのようなコントロールプレーンコンポーネントにカスタム引数を渡すことができます。しかし、このメカニズムは値の解析に使われる基本的な型 (<code>mapStringString</code>) のために制限されています。</p><p>もし、<code>--apiserver-extra-args "enable-admission plugins=LimitRanger,NamespaceExists"</code>のようにカンマで区切られた複数の値をサポートする引数を渡した場合、このフラグは<code>flag: malformed pair, expect string=string</code>で失敗します。これは<code>--apiserver-extra-args</code>の引数リストが<code>key=value</code>のペアを期待しており、この場合<code>NamespacesExists</code>は値を欠いたキーとみなされるためです。</p><p>別の方法として、<code>key=value</code>のペアを以下のように分離してみることもできます:
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"</code>しかし、この場合は、キー<code>enable-admission-plugins</code>は<code>NamespaceExists</code>の値しか持ちません。既知の回避策としては、kubeadm<a href=/ja/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#apiserver-flags>設定ファイル</a>を使用することが挙げられます。</p><h2 id=cloud-controller-managerによってノードが初期化される前にkube-proxyがスケジューリングされる>cloud-controller-managerによってノードが初期化される前にkube-proxyがスケジューリングされる</h2><p>クラウドプロバイダのシナリオでは、クラウドコントローラマネージャがノードアドレスを初期化する前に、kube-proxyが新しいワーカーノードでスケジューリングされてしまうことがあります。これにより、kube-proxyがノードのIPアドレスを正しく拾えず、ロードバランサを管理するプロキシ機能に悪影響を及ぼします。</p><p>kube-proxy Podsでは以下のようなエラーが発生します:</p><pre tabindex=0><code>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
</code></pre><p>既知の解決策は、初期のガード条件が緩和されるまで他のノードから離しておき、条件に関係なくコントロールプレーンノード上でスケジューリングできるように、キューブプロキシDaemonSetにパッチを当てることです:</p><pre tabindex=0><code>kubectl -n kube-system patch ds kube-proxy -p=&#39;{ &#34;spec&#34;: { &#34;template&#34;: { &#34;spec&#34;: { &#34;tolerations&#34;: [ { &#34;key&#34;: &#34;CriticalAddonsOnly&#34;, &#34;operator&#34;: &#34;Exists&#34; }, { &#34;effect&#34;: &#34;NoSchedule&#34;, &#34;key&#34;: &#34;node-role.kubernetes.io/master&#34; } ] } } } }&#39;
</code></pre><p>Tこの問題のトラッキング問題は<a href=https://github.com/kubernetes/kubeadm/issues/1027>こちら</a>。</p><h2 id=kubeadmの設定をマーシャリングする際-noderegistration-taintsフィールドが省略される>kubeadmの設定をマーシャリングする際、NodeRegistration.Taintsフィールドが省略される</h2><p><em>注意: この<a href=https://github.com/kubernetes/kubeadm/issues/1358>Issue</a>は、kubeadmタイプをマーシャルするツール(YAML設定ファイルなど)にのみ適用されます。これはkubeadm API v1beta2で修正される予定です。</em></p><p>デフォルトでは、kubeadmはコントロールプレーンノードに<code>node-role.kubernetes.io/master:NoSchedule</code>のテイントを適用します。kubeadmがコントロールプレーンノードに影響を与えないようにし、<code>InitConfiguration.NodeRegistration.Taints</code>を空のスライスに設定すると、マーシャリング時にこのフィールドは省略されます。フィールドが省略された場合、kubeadmはデフォルトのテイントを適用します。</p><p>少なくとも2つの回避策があります:</p><ol><li><p>空のスライスの代わりに<code>node-role.kubernetes.io/master:PreferNoSchedule</code>テイントを使用します。他のノードに容量がない限り、<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>Podsはマスター上でスケジュールされます</a>。</p></li><li><p>kubeadm init終了後のテイントの除去:</p></li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl taint nodes NODE_NAME node-role.kubernetes.io/master:NoSchedule-
</span></span></code></pre></div><h2 id=ノード-usr-mounted-read-only-に-usr-が読み取り専用でマウントされる>ノード{#usr-mounted-read-only}に<code>/usr</code>が読み取り専用でマウントされる</h2><p>Fedora CoreOSなどのLinuxディストリビューションでは、ディレクトリ<code>/usr</code>が読み取り専用のファイルシステムとしてマウントされます。 <a href=https://github.com/kubernetes/community/blob/ab55d85/contributors/devel/sig-storage/flexvolume.md>flex-volumeサポート</a>では、kubeletやkube-controller-managerのようなKubernetesコンポーネントはデフォルトで<code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code>のパスを使用していますが、この機能を動作させるためにはflex-volumeディレクトリは <em>書き込み可能</em> な状態でなければなりません。</p><p>この問題を回避するには、kubeadm<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2>設定ファイル</a>を使用してflex-volumeディレクトリを設定します。</p><p>プライマリコントロールプレーンノード（<code>kubeadm init</code>で作成されたもの）上で、<code>--config</code>で以下のファイルを渡します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>flex-volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>ノードをジョインするには:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>あるいは、<code>/usr</code>マウントを書き込み可能にするために <code>/etc/fstab</code>を変更することもできますが、これはLinuxディストリビューションの設計原理を変更していることに注意してください。</p><h2 id=kubeadm-upgrade-plan-が-context-deadline-exceeded-エラーメッセージを表示する><code>kubeadm upgrade plan</code>が<code>context deadline exceeded</code>エラーメッセージを表示する</h2><p>このエラーメッセージは、外部etcdを実行している場合に<code>kubeadm</code>でKubernetesクラスタをアップグレードする際に表示されます。これは致命的なバグではなく、古いバージョンのkubeadmが外部etcdクラスタのバージョンチェックを行うために発生します。<code>kubeadm upgrade apply ...</code>で進めることができます。</p><p>この問題はバージョン1.19で修正されます。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-134ed1f6142a98e6ac681a1ba4920e53>3 - kubeadmを使用したクラスターの作成</h1><p><img src=/images/kubeadm-stacked-color.png align=right width=150px>ベストプラクティスに準拠した実用最小限のKubernetesクラスターを作成します。実際、<code>kubeadm</code>を使用すれば、<a href=https://kubernetes.io/blog/2017/10/software-conformance-certification>Kubernetes Conformance tests</a>に通るクラスターをセットアップすることができます。<code>kubeadm</code>は、<a href=/docs/reference/access-authn-authz/bootstrap-tokens/>ブートストラップトークン</a>やクラスターのアップグレードなどのその他のクラスターのライフサイクルの機能もサポートします。</p><p><code>kubeadm</code>ツールは、次のようなときに適しています。</p><ul><li>新しいユーザーが初めてKubernetesを試すためのシンプルな方法が必要なとき。</li><li>既存のユーザーがクラスターのセットアップを自動化し、アプリケーションをテストする方法が必要なとき。</li><li>より大きなスコープで、他のエコシステムやインストーラーツールのビルディングブロックが必要なとき。</li></ul><p><code>kubeadm</code>は、ラップトップ、クラウドのサーバー群、Raspberry Piなどの様々なマシンにインストールして使えます。クラウドとオンプレミスのどちらにデプロイする場合でも、<code>kubeadm</code>はAnsibleやTerraformなどのプロビジョニングシステムに統合できます。</p><h2 id=始める前に>始める前に</h2><p>このガイドを進めるには、以下の環境が必要です。</p><ul><li>UbuntuやCentOSなど、deb/rpmパッケージと互換性のあるLinux OSが動作している1台以上のマシンがあること。</li><li>マシンごとに2GiB以上のRAMが搭載されていること。それ以下の場合、アプリ実行用のメモリーがほとんど残りません。</li><li>コントロールプレーンノードとして使用するマシンには、最低でも2CPU以上あること。</li><li>クラスター内の全マシン間に完全なネットワーク接続があること。パブリックネットワークとプライベートネットワークのいずれでも使えます。</li></ul><p>また、新しいクラスターで使いたいKubernetesのバージョンをデプロイできるバージョンの<code>kubeadm</code>を使用する必要もあります。</p><p><a href=/ja/docs/setup/release/version-skew-policy/#supported-versions>Kubernetesのバージョンとバージョンスキューポリシー</a>は、<code>kubeadm</code>にもKubernetes全体と同じように当てはまります。Kubernetesと<code>kubeadm</code>がサポートするバージョンを理解するには、上記のポリシーを確認してください。このページは、Kubernetes v1.25向けに書かれています。</p><p>kubeadmツールの全体の機能の状態は、一般利用可能(GA)です。一部のサブ機能はまだ活発に開発が行われています。クラスター作成の実装は、ツールの進化に伴ってわずかに変わるかもしれませんが、全体の実装は非常に安定しているはずです。</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>kubeadm alpha</code>以下のすべてのコマンドは、定義通り、アルファレベルでサポートされています。</div><h2 id=目的>目的</h2><ul><li>シングルコントロールプレーンのKubernetesクラスターをインストールする</li><li>クラスター上にPodネットワークをインストールして、Podがお互いに通信できるようにする</li></ul><h2 id=手順>手順</h2><h3 id=ホストへのkubeadmのインストール>ホストへのkubeadmのインストール</h3><p>「<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>kubeadmのインストール</a>」を読んでください。</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong><p>すでにkubeadmがインストール済みである場合は、最新バージョンのkubeadmを取得するために<code>apt-get update && apt-get upgrade</code>や<code>yum update</code>を実行してください。</p><p>アップグレード中、kubeletが数秒ごとに再起動します。これは、kubeadmがkubeletにするべきことを伝えるまで、crashloopの状態で待機するためです。このcrashloopは期待通りの通常の動作です。コントロールプレーンの初期化が完了すれば、kubeletは正常に動作します。</p></div><h3 id=コントロールプレーンノードの初期化>コントロールプレーンノードの初期化</h3><p>コントロールプレーンノードとは、<a class=glossary-tooltip title=一貫性、高可用性を持ったキーバリューストアで、Kubernetesの全てのクラスター情報の保存場所として利用されています。 data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>(クラスターのデータベース)や<a class=glossary-tooltip title='Kubernetes APIを提供するコントロールプレーンのコンポーネントです。' data-toggle=tooltip data-placement=top href=/ja/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=APIサーバー>APIサーバー</a>(<a class=glossary-tooltip title='A command line tool for communicating with a Kubernetes cluster.' data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a>コマンドラインツールが通信する相手)などのコントロールプレーンのコンポーネントが実行されるマシンです。</p><ol><li>(推奨)シングルコントロールプレーンの<code>kubeadm</code>クラスターを高可用性クラスターにアップグレードする予定がある場合、<code>--control-plane-endpoint</code>を指定して、すべてのコントロールプレーンノードとエンドポイントを共有する必要があります。エンドポイントにはDNSネームやロードバランサーのIPアドレスが使用できます。</li><li>Podネットワークアドオンを選んで、<code>kubeadm init</code>に引数を渡す必要があるかどうか確認してください。選んだサードパーティーのプロバイダーによっては、<code>--pod-network-cidr</code>をプロバイダー固有の値に設定する必要がある場合があります。詳しくは、<a href=#pod-network>Podネットワークアドオンのインストール</a>を参照してください。</li><li>(オプション)バージョン1.14から、<code>kubeadm</code>はよく知られたドメインソケットのパスリストを用いて、Linux上のコンテナランタイムの検出を試みます。異なるコンテナランタイムを使用する場合やプロビジョニングするノードに2つ以上のランタイムがインストールされている場合、<code>kubeadm init</code>に<code>--cri-socket</code>引数を指定してください。詳しくは、<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime>ランタイムのインストール</a>を読んでください。</li><li>(オプション)明示的に指定しない限り、<code>kubeadm</code>はデフォルトゲートウェイに関連付けられたネットワークインターフェイスを使用して、この特定のコントロールプレーンノードのAPIサーバーのadvertise addressを設定します。異なるネットワークインターフェイスを使用するには、<code>kubeadm init</code>に<code>--apiserver-advertise-address=&lt;ip-address></code>引数を指定してください。IPv6アドレスを使用するIPv6 Kubernetesクラスターをデプロイするには、たとえば<code>--apiserver-advertise-address=fd00::101</code>のように、IPv6アドレスを指定する必要があります。</li><li>(オプション)<code>kubeadm init</code>を実行する前に<code>kubeadm config images pull</code>を実行して、gcr.ioコンテナイメージレジストリに接続できるかどうかを確認します。</li></ol><p>コントロールプレーンノードを初期化するには、次のコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init &lt;args&gt;
</span></span></code></pre></div><h3 id=apiserver-advertise-addressとcontrolplaneendpointに関する検討>apiserver-advertise-addressとControlPlaneEndpointに関する検討</h3><p><code>--apiserver-advertise-address</code>は、この特定のコントロールプレーンノードのAPIサーバーへのadvertise addressを設定するために使えますが、<code>--control-plane-endpoint</code>は、すべてのコントロールプレーンノード共有のエンドポイントを設定するために使えます。</p><p><code>--control-plane-endpoint</code>はIPアドレスと、IPアドレスへマッピングできるDNS名を使用できます。利用可能なソリューションをそうしたマッピングの観点から評価するには、ネットワーク管理者に相談してください。</p><p>以下にマッピングの例を示します。</p><pre tabindex=0><code>192.168.0.102 cluster-endpoint
</code></pre><p>ここでは、<code>192.168.0.102</code>がこのノードのIPアドレスであり、<code>cluster-endpoint</code>がこのIPアドレスへとマッピングされるカスタムDNSネームです。このように設定することで、<code>--control-plane-endpoint=cluster-endpoint</code>を<code>kubeadm init</code>に渡せるようになり、<code>kubeadm join</code>にも同じDNSネームを渡せます。後で<code>cluster-endpoint</code>を修正して、高可用性が必要なシナリオでロードバランサーのアドレスを指すようにすることができます。</p><p>kubeadmでは、<code>--control-plane-endpoint</code>を渡さずに構築したシングルコントロールプレーンのクラスターを高可用性クラスターに切り替えることはサポートされていません。</p><h3 id=詳細な情報>詳細な情報</h3><p><code>kubeadm init</code>の引数のより詳細な情報は、<a href=/docs/reference/setup-tools/kubeadm/kubeadm/>kubeadmリファレンスガイド</a>を参照してください。</p><p>設定オプションの全リストは、<a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>設定ファイルのドキュメント</a>で確認できます。</p><p>コントロールプレーンコンポーネントやetcdサーバーのliveness probeへのオプションのIPv6の割り当てなど、コントロールプレーンのコンポーネントをカスタマイズしたい場合は、<a href=/ja/docs/setup/production-environment/tools/kubeadm/control-plane-flags/>カスタムの引数</a>に示されている方法で各コンポーネントに追加の引数を与えてください。</p><p><code>kubeadm init</code>を再び実行する場合は、初めに<a href=#tear-down>クラスターの破壊</a>を行う必要があります。</p><p>もし異なるアーキテクチャのノードをクラスターにjoinさせたい場合は、デプロイしたDaemonSetがそのアーキテクチャ向けのコンテナイメージをサポートしているか確認してください。</p><p>初めに<code>kubeadm init</code>は、マシンがKubernetesを実行する準備ができているかを確認する、一連の事前チェックを行います。これらの事前チェックはエラー発生時には警告を表示して終了します。次に、<code>kubeadm init</code>はクラスターのコントロールプレーンのコンポーネントをダウンロードしてインストールします。これには数分掛かるかもしれません。出力は次のようになります。</p><pre tabindex=0><code class=language-none data-lang=none>[init] Using Kubernetes version: vX.Y.Z
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[kubelet-start] Writing kubelet environment file with flags to file &#34;/var/lib/kubelet/kubeadm-flags.env&#34;
[kubelet-start] Writing kubelet configuration to file &#34;/var/lib/kubelet/config.yaml&#34;
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder &#34;/etc/kubernetes/pki&#34;
[certs] Generating &#34;etcd/ca&#34; certificate and key
[certs] Generating &#34;etcd/server&#34; certificate and key
[certs] etcd/server serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating &#34;etcd/healthcheck-client&#34; certificate and key
[certs] Generating &#34;etcd/peer&#34; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kubeadm-cp localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating &#34;apiserver-etcd-client&#34; certificate and key
[certs] Generating &#34;ca&#34; certificate and key
[certs] Generating &#34;apiserver&#34; certificate and key
[certs] apiserver serving cert is signed for DNS names [kubeadm-cp kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4]
[certs] Generating &#34;apiserver-kubelet-client&#34; certificate and key
[certs] Generating &#34;front-proxy-ca&#34; certificate and key
[certs] Generating &#34;front-proxy-client&#34; certificate and key
[certs] Generating &#34;sa&#34; key and public key
[kubeconfig] Using kubeconfig folder &#34;/etc/kubernetes&#34;
[kubeconfig] Writing &#34;admin.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;kubelet.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;controller-manager.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;scheduler.conf&#34; kubeconfig file
[control-plane] Using manifest folder &#34;/etc/kubernetes/manifests&#34;
[control-plane] Creating static Pod manifest for &#34;kube-apiserver&#34;
[control-plane] Creating static Pod manifest for &#34;kube-controller-manager&#34;
[control-plane] Creating static Pod manifest for &#34;kube-scheduler&#34;
[etcd] Creating static Pod manifest for local etcd in &#34;/etc/kubernetes/manifests&#34;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &#34;/etc/kubernetes/manifests&#34;. This can take up to 4m0s
[apiclient] All control plane components are healthy after 31.501735 seconds
[uploadconfig] storing the configuration used in ConfigMap &#34;kubeadm-config&#34; in the &#34;kube-system&#34; Namespace
[kubelet] Creating a ConfigMap &#34;kubelet-config-X.Y&#34; in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information &#34;/var/run/dockershim.sock&#34; to the Node API object &#34;kubeadm-cp&#34; as an annotation
[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the label &#34;node-role.kubernetes.io/master=&#39;&#39;&#34;
[mark-control-plane] Marking the node kubeadm-cp as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: &lt;token&gt;
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the &#34;cluster-info&#34; ConfigMap in the &#34;kube-public&#34; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  /ja/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><p>kubectlをroot以外のユーザーでも実行できるようにするには、次のコマンドを実行します。これらのコマンドは、<code>kubectl init</code>の出力の中にも書かれています。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p <span style=color:#b8860b>$HOME</span>/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div><p>あなたが<code>root</code>ユーザーである場合は、代わりに次のコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div><p><code>kubeadm init</code>が出力した<code>kubeadm join</code>コマンドをメモしておいてください。<a href=#join-nodes>クラスターにノードを追加する</a>ために、このコマンドが必要になります。</p><p>トークンは、コントロールプレーンノードと追加ノードの間の相互認証に使用します。ここに含まれるトークンには秘密の情報が含まれます。このトークンを知っていれば、誰でもクラスターに認証済みノードを追加できてしまうため、取り扱いには注意してください。<code>kubeadm token</code>コマンドを使用すると、これらのトークンの一覧、作成、削除ができます。詳しくは<a href=/docs/reference/setup-tools/kubeadm/kubeadm-token/>kubeadmリファレンスガイド</a>を読んでください。</p><h3 id=pod-network>Podネットワークアドオンのインストール</h3><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong><p>このセクションには、ネットワークのセットアップとデプロイの順序に関する重要な情報が書かれています。先に進む前に以下のすべてのアドバイスを熟読してください。</p><p><strong>Pod同士が通信できるようにするには、<a class=glossary-tooltip title='Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label='Container Network Interface'>Container Network Interface</a>(CNI)をベースとするPodネットワークアドオンをデプロイしなければなりません。ネットワークアドオンをインストールする前には、Cluster DNS(CoreDNS)は起動しません。</strong></p><ul><li><p>Podネットワークがホストネットワークと決して重ならないように気をつけてください。もし重なると、様々な問題が起こってしまう可能性があります。(ネットワークプラグインが優先するPodネットワークとホストのネットワークの一部が衝突することが分かった場合、適切な代わりのCIDRを考える必要があります。そして、<code>kubeadm init</code>の実行時に<code>--pod-network-cidr</code>にそのCIDRを指定し、ネットワークプラグインのYAMLでは代わりにそのCIDRを使用してください)</p></li><li><p>デフォルトでは、<code>kubeadm</code>は<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>(role based access control)の使用を強制します。PodネットワークプラグインがRBACをサポートしていて、またそのデプロイに使用するマニフェストもRBACをサポートしていることを確認してください。</p></li><li><p>クラスターでIPv6を使用したい場合、デュアルスタック、IPv6のみのシングルスタックのネットワークのいずれであっても、PodネットワークプラグインがIPv6をサポートしていることを確認してください。IPv6のサポートは、CNIの<a href=https://github.com/containernetworking/cni/releases/tag/v0.6.0>v0.6.0</a>で追加されました。</p></li></ul></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> 現在、Calicoはkubeadmプロジェクトがe2eテストを実施している唯一のCNIプラグインです。
もしCNIプラグインに関する問題を見つけた場合、kubeadmやkubernetesではなく、そのCNIプラグインの課題管理システムへ問題を報告してください。</div><p>CNIを使用するKubernetes Podネットワークを提供する外部のプロジェクトがいくつかあります。一部のプロジェクトでは、<a href=/ja/docs/concepts/services-networking/network-policies/>ネットワークポリシー</a>もサポートしています。</p><p><a href=/ja/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>Kubernetesのネットワークモデル</a>を実装したアドオンの一覧も確認してください。</p><p>Podネットワークアドオンをインストールするには、コントロールプレーンノード上またはkubeconfigクレデンシャルを持っているノード上で、次のコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f &lt;add-on.yaml&gt;
</span></span></code></pre></div><p>インストールできるPodネットワークは、クラスターごとに1つだけです。</p><p>Podネットワークがインストールされたら、<code>kubectl get pods --all-namespaces</code>の出力結果でCoreDNS Podが<code>Running</code>状態であることをチェックすることで、ネットワークが動作していることを確認できます。そして、一度CoreDNS Podが動作すれば、続けてノードを追加できます。</p><p>もしネットワークやCoreDNSが<code>Running</code>状態にならない場合は、<code>kubeadm</code>の<a href=/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>トラブルシューティングガイド</a>をチェックしてください。</p><h3 id=コントロールプレーンノードの隔離>コントロールプレーンノードの隔離</h3><p>デフォルトでは、セキュリティ上の理由により、クラスターはコントロールプレーンノードにPodをスケジューリングしません。たとえば、開発用のKubernetesシングルマシンのクラスターなどで、Podをコントロールプレーンノードにスケジューリングしたい場合は、次のコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl taint nodes --all node-role.kubernetes.io/master-
</span></span></code></pre></div><p>出力は次のようになります。</p><pre tabindex=0><code>node &#34;test-01&#34; untainted
taint &#34;node-role.kubernetes.io/master:&#34; not found
taint &#34;node-role.kubernetes.io/master:&#34; not found
</code></pre><p>このコマンドは、コントロールプレーンノードを含むすべてのノードから<code>node-role.kubernetes.io/master</code>taintを削除します。その結果、スケジューラーはどこにでもPodをスケジューリングできるようになります。</p><h3 id=join-nodes>ノードの追加</h3><p>ノードは、ワークロード(コンテナやPodなど)が実行される場所です。新しいノードをクラスターに追加するためには、各マシンに対して、以下の手順を実行してください。</p><ul><li>マシンへSSHする</li><li>rootになる(例: <code>sudo su -</code>)</li><li><code>kubeadm init</code>実行時に出力されたコマンドを実行する。たとえば、次のようなコマンドです。</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</span></span></code></pre></div><p>トークンがわからない場合は、コントロールプレーンノードで次のコマンドを実行すると取得できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token list
</span></span></code></pre></div><p>出力は次のようになります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
</span></span></span><span style=display:flex><span><span style=color:#888>8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                   signing          token generated by     bootstrappers:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                    &#39;kubeadm init&#39;.        kubeadm:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                                           default-node-token
</span></span></span></code></pre></div><p>デフォルトでは、トークンは24時間後に有効期限が切れます。もし現在のトークンの有効期限が切れた後にクラスターにノードを参加させたい場合は、コントロールプレーンノードで次のコマンドを実行することで、新しいトークンを生成できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token create
</span></span></code></pre></div><p>このコマンドの出力は次のようになります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>5didvk.d09sbcov8ph2amjw
</span></span></span></code></pre></div><p>もし<code>--discovery-token-ca-cert-hash</code>の値がわからない場合は、コントロールプレーンノード上で次のコマンドチェーンを実行することで取得できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>   openssl dgst -sha256 -hex | sed <span style=color:#b44>&#39;s/^.* //&#39;</span>
</span></span></code></pre></div><p>出力は次のようになります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> IPv6タプルを<code>&lt;control-plane-host>:&lt;control-plane-port></code>と指定するためには、IPv6アドレスを角括弧で囲みます。たとえば、<code>[fd00::101]:2073</code>のように書きます。</div><p>出力は次のようになります。</p><pre tabindex=0><code>[preflight] Running pre-flight checks

... (joinワークフローのログ出力) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run &#39;kubectl get nodes&#39; on control-plane to see this machine join.
</code></pre><p>数秒後、コントロールプレーンノード上で<code>kubectl get nodes</code>を実行すると、出力内にこのノードが表示されるはずです。</p><h3 id=オプション-コントロールプレーンノード以外のマシンからのクラスター操作>(オプション)コントロールプレーンノード以外のマシンからのクラスター操作</h3><p>他のコンピューター(例: ラップトップ)上のkubectlがクラスターと通信できるようにするためには、次のようにして、administratorのkubeconfigファイルをコントロールプレーンノードからそのコンピューター上にコピーする必要があります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf get nodes
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong><p>上の例では、rootユーザーに対するSSH接続が有効であることを仮定しています。もしそうでない場合は、<code>admin.conf</code>ファイルを誰か他のユーザーからアクセスできるようにコピーした上で、代わりにそのユーザーを使って<code>scp</code>してください。</p><p><code>admin.conf</code>ファイルはユーザーにクラスターに対する <em>特権ユーザー</em> の権限を与えます。そのため、このファイルを使うのは控えめにしなければなりません。通常のユーザーには、明示的に許可した権限を持つユニークなクレデンシャルを生成することを推奨します。これには、<code>kubeadm alpha kubeconfig user --client-name &lt;CN></code>コマンドが使えます。このコマンドを実行すると、KubeConfigファイルがSTDOUTに出力されるので、ファイルに保存してユーザーに配布します。その後、<code>kubectl create (cluster)rolebinding</code>コマンドを使って権限を付与します。</p></div><h3 id=オプション-apiサーバーをlocalhostへプロキシする>(オプション)APIサーバーをlocalhostへプロキシする</h3><p>クラスターの外部からAPIサーバーに接続したいときは、次のように<code>kubectl proxy</code>コマンドが使えます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf proxy
</span></span></code></pre></div><p>これで、ローカルの<code>http://localhost:8001/api/v1</code>からAPIサーバーにアクセスできるようになります。</p><h2 id=tear-down>クリーンアップ</h2><p>テストのためにクラスターに破棄可能なサーバーを使用した場合、サーバーのスイッチをオフにすれば、以降のクリーンアップの作業は必要ありません。クラスターのローカルの設定を削除するには、<code>kubectl config delete-cluster</code>を実行します。</p><p>しかし、もしよりきれいにクラスターのプロビジョンをもとに戻したい場合は、初めに<a href=/docs/reference/generated/kubectl/kubectl-commands#drain>ノードのdrain</a>を行い、ノードが空になっていることを確認した後、ノードの設定を削除する必要があります。</p><h3 id=ノードの削除>ノードの削除</h3><p>適切なクレデンシャルを使用してコントロールプレーンノードに削除することを伝えます。次のコマンドを実行してください。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets
</span></span></code></pre></div><p>ノードが削除される前に、<code>kubeadm</code>によってインストールされた状態をリセットします。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm reset
</span></span></code></pre></div><p>リセットプロセスでは、iptablesのルールやIPVS tablesのリセットやクリーンアップは行われません。iptablesをリセットしたい場合は、次のように手動でコマンドを実行する必要があります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>iptables -F <span style=color:#666>&amp;&amp;</span> iptables -t nat -F <span style=color:#666>&amp;&amp;</span> iptables -t mangle -F <span style=color:#666>&amp;&amp;</span> iptables -X
</span></span></code></pre></div><p>IPVS tablesをリセットしたい場合は、次のコマンドを実行する必要があります。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -C
</span></span></code></pre></div><p>ノードを削除します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete node &lt;node name&gt;
</span></span></code></pre></div><p>クラスターのセットアップを最初から始めたいときは、<code>kubeadm init</code>や<code>kubeadm join</code>を適切な引数を付けて実行すればいいだけです。</p><h3 id=コントロールプレーンのクリーンアップ>コントロールプレーンのクリーンアップ</h3><p>コントロールホスト上で<code>kubeadm reset</code>を実行すると、ベストエフォートでのクリーンアップが実行できます。</p><p>このサブコマンドとオプションに関するより詳しい情報は、<a href=/docs/reference/setup-tools/kubeadm/kubeadm-reset/><code>kubeadm reset</code></a>リファレンスドキュメントを読んでください。</p><h2 id=whats-next>次の手順</h2><ul><li><a href=https://github.com/heptio/sonobuoy>Sonobuoy</a>を使用してクラスターが適切に動作しているか検証する。</li><li><a id=lifecycle><code>kubeadm</code>を使用したクラスターをアップグレードする方法について、<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>kubeadmクラスターをアップグレードする</a>を読む。</li><li><code>kubeadm</code>の高度な利用方法について<a href=/docs/reference/setup-tools/kubeadm/kubeadm>kubeadmリファレンスドキュメント</a>で学ぶ。</li><li>Kubernetesの<a href=/ja/docs/concepts/>コンセプト</a>や<a href=/ja/docs/reference/kubectl/overview/><code>kubectl</code></a>についてもっと学ぶ。</li><li>Podネットワークアドオンのより完全なリストを<a href=/ja/docs/concepts/cluster-administration/networking/>クラスターのネットワーク</a>で確認する。</li><li><a id=other-addons>ロギング、モニタリング、ネットワークポリシー、仮想化、Kubernetesクラスターの制御のためのツールなど、その他のアドオンについて、<a href=/ja/docs/concepts/cluster-administration/addons/>アドオンのリスト</a>で確認する。</li><li>クラスターイベントやPod内で実行中のアプリケーションから送られるログをクラスターがハンドリングする方法を設定する。関係する要素の概要を理解するために、<a href=/docs/concepts/cluster-administration/logging/>ロギングのアーキテクチャ</a>を読んでください。</li></ul><h3 id=feedback>フィードバック</h3><ul><li>バグを見つけた場合は、<a href=https://github.com/kubernetes/kubeadm/issues>kubeadm GitHub issue tracker</a>で報告してください。</li><li>サポートを受けたい場合は、<a href=https://kubernetes.slack.com/messages/kubeadm/>#kubeadm</a>Slackチャンネルを訪ねてください。</li><li>General SIG Cluster Lifecycle development Slackチャンネル:
<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle/>#sig-cluster-lifecycle</a></li><li>SIG Cluster Lifecycle <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme>SIG information</a></li><li>SIG Cluster Lifecycleメーリングリスト:
<a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle>kubernetes-sig-cluster-lifecycle</a></li></ul><h2 id=version-skew-policy>バージョン互換ポリシー</h2><p>バージョンv1.25の<code>kubeadm</code>ツールは、バージョンv1.25またはv1.24のコントロールプレーンを持つクラスターをデプロイできます。また、バージョンv1.25の<code>kubeadm</code>は、バージョンv1.24のkubeadmで構築されたクラスターをアップグレートできます。</p><p>未来を見ることはできないため、kubeadm CLI v1.25はv1.26をデプロイできないかもしれません。</p><p>例: <code>kubeadm</code> v1.8は、v1.7とv1.8のクラスターをデプロイでき、v1.7のkubeadmで構築されたクラスターをv1.8にアップグレートできます。</p><p>kubeletとコントロールプレーンの間や、他のKubernetesコンポーネント間のバージョンの差異に関する詳しい情報は、以下の資料を確認してください。</p><ul><li>Kubernetes<a href=/ja/docs/setup/release/version-skew-policy/>バージョンスキューサポートポリシー</a></li><li>Kubeadm特有の<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl>インストールガイド</a></li></ul><h2 id=limitations>制限事項</h2><h3 id=resilience>クラスターのレジリエンス</h3><p>ここで作られたクラスターは、1つのコントロールプレーンノードと、その上で動作する1つのetcdデータベースしか持ちません。つまり、コントロールプレーンノードが故障した場合、クラスターのデータは失われ、クラスターを最初から作り直す必要があるかもしれないということです。</p><p>対処方法:</p><ul><li><p>定期的に<a href=https://coreos.com/etcd/docs/latest/admin_guide.html>etcdをバックアップ</a>する。kubeadmが設定するetcdのデータディレクトリは、コントロールプレーンノードの<code>/var/lib/etcd</code>にあります。</p></li><li><p>複数のコントロールプレーンノードを使用する。<a href=/ja/docs/setup/production-environment/tools/kubeadm/ha-topology/>高可用性トポロジーのオプション</a>では、<a href=/ja/docs/setup/production-environment/tools/kubeadm/high-availability/>より高い可用性</a>を提供するクラスターのトポロジーの選択について説明してます。</p></li></ul><h3 id=multi-platform>プラットフォームの互換性</h3><p>kubeadmのdeb/rpmパッケージおよびバイナリは、<a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md>multi-platform proposal</a>に従い、amd64、arm(32ビット)、arm64、ppc64le、およびs390x向けにビルドされています。</p><p>マルチプラットフォームのコントロールプレーンおよびアドオン用のコンテナイメージも、v1.12からサポートされています。</p><p>すべてのプラットフォーム向けのソリューションを提供しているネットワークプロバイダーは一部のみです。それぞれのプロバイダーが選択したプラットフォームをサポートしているかどうかを確認するには、前述のネットワークプロバイダーのリストを参照してください。</p><h2 id=troubleshooting>トラブルシューティング</h2><p>kubeadmに関する問題が起きたときは、<a href=/ja/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>トラブルシューティングドキュメント</a>を確認してください。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c656c5eda3e1c06ad1aedebdc04a211>4 - kubeadmを使ったコントロールプレーンの設定のカスタマイズ</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.12 [stable]</code></div><p>kubeadmの<code>ClusterConfiguration</code>オブジェクトはAPIServer、ControllerManager、およびSchedulerのようなコントロールプレーンの構成要素に渡されたデフォルトのフラグを上書きすることができる <code>extraArgs</code>の項目があります。
その構成要素は次の項目で定義されています。</p><ul><li><code>apiServer</code></li><li><code>controllerManager</code></li><li><code>scheduler</code></li></ul><p><code>extraArgs</code> の項目は <code>キー: 値</code> のペアです。コントロールプレーンの構成要素のフラグを上書きするには:</p><ol><li>設定内容に適切な項目を追加</li><li>フラグを追加して項目を上書き</li><li><code>--config &lt;任意の設定YAMLファイル></code>で<code>kubeadm init</code>を実行</li></ol><p>各設定項目のより詳細な情報は<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm#ClusterConfiguration>APIリファレンスのページ</a>を参照してください。</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>kubeadm config print init-defaults</code>を実行し、選択したファイルに出力を保存することで、デフォルト値で<code>ClusterConfiguration</code>オブジェクトを生成できます。</div><h2 id=apiserverフラグ>APIServerフラグ</h2><p>詳細は<a href=/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserverのリファレンスドキュメント</a>を参照してください。</p><p>使用例:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>advertise-address</span>:<span style=color:#bbb> </span><span style=color:#666>192.168.0.103</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>anonymous-auth</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;false&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>enable-admission-plugins</span>:<span style=color:#bbb> </span>AlwaysPullImages,DefaultStorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>audit-log-path</span>:<span style=color:#bbb> </span>/home/johndoe/audit.log<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=controllermanagerフラグ>ControllerManagerフラグ</h2><p>詳細は<a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-managerのリファレンスドキュメント</a>を参照してください。</p><p>使用例:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-key-file</span>:<span style=color:#bbb> </span>/home/johndoe/keys/ca.key<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deployment-controller-sync-period</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;50&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=schedulerフラグ>Schedulerフラグ</h2><p>詳細は<a href=/docs/reference/command-line-tools-reference/kube-scheduler/>kube-schedulerのリファレンスドキュメント</a>を参照してください。</p><p>使用例:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduler</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>config</span>:<span style=color:#bbb> </span>/home/johndoe/schedconfig.yaml<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubeconfig</span>:<span style=color:#bbb> </span>/home/johndoe/kubeconfig.yaml<span style=color:#bbb>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-015edbc7cc688d31b1d1edce7c186135>5 - 高可用性トポロジーのためのオプション</h1><p>このページでは、高可用性(HA)Kubernetesクラスターのトポロジーを設定するための2つのオプションについて説明します。</p><p>HAクラスターは次の方法で設定できます。</p><ul><li>積層コントロールプレーンノードを使用する方法。こちらの場合、etcdノードはコントロールプレーンノードと同じ場所で動作します。</li><li>外部のetcdノードを使用する方法。こちらの場合、etcdがコントロールプレーンとは分離されたノードで動作します。</li></ul><p>HAクラスターをセットアップする前に、各トポロジーの利点と欠点について注意深く考慮する必要があります。</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> kubeadmは、etcdクラスターを静的にブートストラップします。
詳細については、etcd<a href=https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static>クラスタリングガイド</a>をご覧ください。</div><h2 id=積層etcdトポロジー>積層etcdトポロジー</h2><p>積層HAクラスターは、コントロールプレーンのコンポーネントを実行する、kubeadmで管理されたノードで構成されるクラスターの上に、etcdにより提供される分散データストレージクラスターがあるような<a href=https://en.wikipedia.org/wiki/Network_topology>トポロジー</a>です。</p><p>各コントロールプレーンノードは、<code>kube-apiserver</code>、<code>kube-scheduler</code>、および<code>kube-controller-manager</code>を実行します。<code>kube-apiserver</code> はロードバランサーを用いてワーカーノードに公開されます。</p><p>各コントロールプレーンノードはローカルのetcdメンバーを作り、このetcdメンバーはそのノードの<code>kube-apiserver</code>とだけ通信します。ローカルの<code>kube-controller-manager</code>と<code>kube-scheduler</code>のインスタンスも同様です。</p><p>このトポロジーは、同じノード上のコントロールプレーンとetcdのメンバーを結合します。外部のetcdノードを使用するクラスターよりはセットアップがシンプルで、レプリケーションの管理もシンプルです。</p><p>しかし、積層クラスターには、結合による故障のリスクがあります。1つのノードがダウンすると、etcdメンバーとコントロールプレーンのインスタンスの両方が失われ、冗長性が損なわれます。より多くのコントロールプレーンノードを追加することで、このリスクは緩和できます。</p><p>そのため、HAクラスターのためには、最低でも3台の積層コントロールプレーンノードを実行しなければなりません。</p><p>これがkubeadmのデフォルトのトポロジーです。<code>kubeadm init</code>や<code>kubeadm join --control-place</code>を実行すると、ローカルのetcdメンバーがコントロールプレーンノード上に自動的に作成されます。</p><p><img src=/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg alt=積層etcdトポロジー></p><h2 id=外部のetcdトポロジー>外部のetcdトポロジー</h2><p>外部のetcdを持つHAクラスターは、コントロールプレーンコンポーネントを実行するノードで構成されるクラスターの外部に、etcdにより提供される分散データストレージクラスターがあるような<a href=https://en.wikipedia.org/wiki/Network_topology>トポロジー</a>です。</p><p>積層etcdトポロジーと同様に、外部のetcdトポロジーにおける各コントロールプレーンノードは、<code>kube-apiserver</code>、<code>kube-scheduler</code>、および<code>kube-controller-manager</code>のインスタンスを実行します。そして、<code>kube-apiserver</code>は、ロードバランサーを使用してワーカーノードに公開されます。しかし、etcdメンバーは異なるホスト上で動作しており、各etcdホストは各コントロールプレーンノードの<code>kube-api-server</code>と通信します。</p><p>このトポロジーは、コントロールプレーンとetcdメンバーを疎結合にします。そのため、コントロールプレーンインスタンスまたはetcdメンバーを失うことによる影響は少なく、積層HAトポロジーほどクラスターの冗長性に影響しないHAセットアップが実現します。</p><p>しかし、このトポロジーでは積層HAトポロジーの2倍の数のホストを必要とします。このトポロジーのHAクラスターのためには、最低でもコントロールプレーンのために3台のホストが、etcdノードのために3台のホストがそれぞれ必要です。</p><p><img src=/images/kubeadm/kubeadm-ha-topology-external-etcd.svg alt=外部のetcdトポロジー></p><h2 id=次の項目>次の項目</h2><ul><li><a href=/ja/docs/setup/production-environment/tools/kubeadm/high-availability/>kubeadmを使用した高可用性クラスターの作成</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3941d5c3409342219bf7e03128b8ecb6>6 - kubeadmを使用した高可用性クラスターの作成</h1><p>このページでは、kubeadmを使用して、高可用性クラスターを作成する、2つの異なるアプローチを説明します:</p><ul><li>積層コントロールプレーンノードを使う方法。こちらのアプローチは、必要なインフラストラクチャーが少ないです。etcdのメンバーと、コントロールプレーンノードは同じ場所に置かれます。</li><li>外部のetcdクラスターを使う方法。こちらのアプローチには、より多くのインフラストラクチャーが必要です。コントロールプレーンノードと、etcdのメンバーは分離されます。</li></ul><p>先へ進む前に、どちらのアプローチがアプリケーションの要件と、環境に適合するか、慎重に検討してください。<a href=/ja/docs/setup/production-environment/tools/kubeadm/ha-topology/>こちらの比較</a>が、それぞれの利点/欠点について概説しています。</p><p>高可用性クラスターの作成で問題が発生した場合は、kueadmの<a href=https://github.com/kubernetes/kubeadm/issues/new>issue tracker</a>でフィードバックを提供してください。</p><p><a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>高可用性クラスターのアップグレード</a>も参照してください。</p><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> このページはクラウド上でクラスターを構築することには対応していません。ここで説明されているどちらのアプローチも、クラウド上で、LoadBalancerタイプのServiceオブジェクトや、動的なPersistentVolumeを利用して動かすことはできません。</div><h2 id=始める前に>始める前に</h2><p>どちらの方法でも、以下のインフラストラクチャーが必要です:</p><ul><li>master用に、<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%A7%8B%E3%82%81%E3%82%8B%E5%89%8D%E3%81%AB>kubeadmの最小要件</a>を満たす3台のマシン</li><li>worker用に、<a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#%E5%A7%8B%E3%82%81%E3%82%8B%E5%89%8D%E3%81%AB>kubeadmの最小要件</a>を満たす3台のマシン</li><li>クラスター内のすべてのマシン間がフルにネットワーク接続可能であること(パブリック、もしくはプライベートネットワーク)</li><li>すべてのマシンにおいて、sudo権限</li><li>あるデバイスから、システム内のすべてのノードに対しSSH接続できること</li><li><code>kubeadm</code>と<code>kubelet</code>がすべてのマシンにインストールされていること。 <code>kubectl</code>は任意です。</li></ul><p>外部etcdクラスターには、以下も必要です:</p><ul><li>etcdメンバー用に、追加で3台のマシン</li></ul><h2 id=両手順における最初のステップ>両手順における最初のステップ</h2><h3 id=kube-apiserver用にロードバランサーを作成>kube-apiserver用にロードバランサーを作成</h3><div class="alert alert-info note callout" role=alert><strong>備考:</strong> ロードバランサーには多くの設定項目があります。以下の例は、一選択肢に過ぎません。あなたのクラスター要件には、異なった設定が必要かもしれません。</div><ol><li><p>DNSで解決される名前で、kube-apiserver用ロードバランサーを作成する。</p><ul><li><p>クラウド環境では、コントロールプレーンノードをTCPフォワーディングロードバランサーの後ろに置かなければなりません。このロードバランサーはターゲットリストに含まれる、すべての健全なコントロールプレーンノードにトラフィックを分配します。apiserverへのヘルスチェックはkube-apiserverがリッスンするポート(デフォルト値: <code>:6443</code>)に対する、TCPチェックです。</p></li><li><p>クラウド環境では、IPアドレスを直接使うことは推奨されません。</p></li><li><p>ロードバランサーは、apiserverポートで、全てのコントロールプレーンノードと通信できなければなりません。また、リスニングポートに対する流入トラフィックも許可されていなければなりません。</p></li><li><p>ロードバランサーのアドレスは、常にkubeadmの<code>ControlPlaneEndpoint</code>のアドレスと一致することを確認してください。</p></li><li><p>詳細は<a href=https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#options-for-software-load-balancing>Options for Software Load Balancing</a>をご覧ください。</p></li></ul></li><li><p>ロードバランサーに、最初のコントロールプレーンノードを追加し、接続をテストする:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>nc -v LOAD_BALANCER_IP PORT
</span></span></code></pre></div><ul><li>apiserverはまだ動いていないので、接続の拒否は想定通りです。しかし、タイムアウトしたのであれば、ロードバランサーはコントロールプレーンノードと通信できなかったことを意味します。もし、タイムアウトが起きたら、コントロールプレーンノードと通信できるように、ロードバランサーを再設定してください。</li></ul></li><li><p>残りのコントロールプレーンノードを、ロードバランサーのターゲットグループに追加します。</p></li></ol><h2 id=積層コントロールプレーンとetcdノード>積層コントロールプレーンとetcdノード</h2><h3 id=最初のコントロールプレーンノードの手順>最初のコントロールプレーンノードの手順</h3><ol><li><p>最初のコントロールプレーンノードを初期化します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm init --control-plane-endpoint <span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span> --upload-certs
</span></span></code></pre></div><ul><li><code>--kubernetes-version</code>フラグで使用するKubernetesのバージョンを設定できます。kubeadm、kubelet、kubectl、Kubernetesのバージョンを一致させることが推奨されます。</li><li><code>--control-plane-endpoint</code>フラグは、ロードバランサーのIPアドレスまたはDNS名と、ポートが設定される必要があります。</li><li><code>--upload-certs</code>フラグは全てのコントロールプレーンノードで共有する必要がある証明書をクラスターにアップロードするために使用されます。代わりに、コントロールプレーンノード間で手動あるいは自動化ツールを使用して証明書をコピーしたい場合は、このフラグを削除し、以下の<a href=#manual-certs>証明書の手動配布</a>のセクションを参照してください。</li></ul><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>kubeadm init</code>の<code>--config</code>フラグと<code>--certificate-key</code>フラグは混在させることはできないため、<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2>kubeadm configuration</a>を使用する場合は<code>certificateKey</code>フィールドを適切な場所に追加する必要があります(<code>InitConfiguration</code>と<code>JoinConfiguration: controlPlane</code>の配下)。</div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> いくつかのCNIネットワークプラグインはPodのIPのCIDRの指定など追加の設定を必要としますが、必要としないプラグインもあります。<a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>CNIネットワークドキュメント</a>を参照してください。PodにCIDRを設定するには、<code>ClusterConfiguration</code>の<code>networking</code>オブジェクトに<code>podSubnet: 192.168.0.0/16</code>フィールドを設定してください。</div><ul><li>このような出力がされます:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>...
</span></span><span style=display:flex><span>You can now join any number of control-plane node by running the following <span style=color:#a2f>command</span> on each as a root:
</span></span><span style=display:flex><span>    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
</span></span><span style=display:flex><span>As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style=display:flex><span>    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><ul><li><p>この出力をテキストファイルにコピーします。あとで、他のコントロールプレーンノードとワーカーノードをクラスターに参加させる際に必要です。</p></li><li><p><code>--upload-certs</code>フラグを<code>kubeadm init</code>で使用すると、プライマリコントロールプレーンの証明書が暗号化されて、<code>kubeadm-certs</code> Secretにアップロードされます。</p></li><li><p>証明書を再アップロードして新しい復号キーを生成するには、すでにクラスターに参加しているコントロールプレーンノードで次のコマンドを使用します:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm init phase upload-certs --upload-certs
</span></span></code></pre></div><ul><li>また、後で<code>join</code>で使用できるように、<code>init</code>中にカスタムした<code>--certificate-key</code>を指定することもできます。このようなキーを生成するには、次のコマンドを使用します:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubeadm alpha certs certificate-key
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>kubeadm-certs</code>のSecretと復号キーは2時間で期限切れとなります。</div><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> コマンド出力に記載されているように、証明書キーはクラスターの機密データへのアクセスを提供します。秘密にしてください！</div></li><li><p>使用するCNIプラグインを適用します:<br><a href=/ja/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>こちらの手順に従い</a>CNIプロバイダーをインストールします。該当する場合は、kubeadmの設定で指定されたPodのCIDRに対応していることを確認してください。</p><p>Weave Netを使用する場合の例:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div></li><li><p>以下のコマンドを入力し、コンポーネントのPodが起動するのを確認します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl get pod -n kube-system -w
</span></span></code></pre></div></li></ol><h3 id=残りのコントロールプレーンノードの手順>残りのコントロールプレーンノードの手順</h3><div class="alert alert-info note callout" role=alert><strong>備考:</strong> kubeadmバージョン1.15以降、複数のコントロールプレーンノードを並行してクラスターに参加させることができます。
このバージョンの前は、最初のノードの初期化が完了した後でのみ、新しいコントロールプレーンノードを順番にクラスターに参加させる必要があります。</div><p>追加のコントロールプレーンノード毎に、以下の手順を行います。</p><ol><li><p><code>kubeadm init</code>を最初のノードで実行した際に取得したjoinコマンドを使って、新しく追加するコントロールプレーンノードで<code>kubeadm join</code>を開始します。このようなコマンドになるはずです:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span></code></pre></div><ul><li><code>--control-plane</code>フラグによって、<code>kubeadm join</code>の実行は新しいコントロールプレーンを作成します。</li><li><code>-certificate-key ...</code>を指定したキーを使って、クラスターの<code>kubeadm-certs</code> Secretからダウンロードされたコントロールプレーンの証明書が復号されます。</li></ul></li></ol><h2 id=外部のetcdノード>外部のetcdノード</h2><p>外部のetcdノードを使ったクラスターの設定は、積層etcdの場合と似ていますが、最初にetcdを設定し、kubeadmの設定ファイルにetcdの情報を渡す必要があります。</p><h3 id=etcdクラスターの構築>etcdクラスターの構築</h3><ol><li><p><a href=/ja/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>こちらの手順</a>にしたがって、etcdクラスターを構築してください。</p></li><li><p><a href=#manual-certs>こちらの手順</a>にしたがって、SSHを構築してください。</p></li><li><p>以下のファイルをクラスター内の任意のetcdノードから最初のコントロールプレーンノードにコピーしてください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#666>=</span><span style=color:#b44>&#34;ubuntu@10.0.0.7&#34;</span>
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span></code></pre></div><ul><li><code>CONTROL_PLANE</code>の値を、最初のコントロールプレーンノードの<code>user@host</code>で置き換えます。</li></ul></li></ol><h3 id=最初のコントロールプレーンノードの構築>最初のコントロールプレーンノードの構築</h3><ol><li><p>以下の内容で、<code>kubeadm-config.yaml</code>という名前の設定ファイルを作成します:</p><pre><code>apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: &quot;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&quot;
etcd:
    external:
        endpoints:
        - https://ETCD_0_IP:2379
        - https://ETCD_1_IP:2379
        - https://ETCD_2_IP:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
</code></pre><div class="alert alert-info note callout" role=alert><strong>備考:</strong> ここで、積層etcdと外部etcdの違いは、外部etcdの構成では<code>etcd</code>の<code>external</code>オブジェクトにetcdのエンドポイントが記述された設定ファイルが必要です。積層etcdトポロジーの場合、これは自動で管理されます。</div><ul><li><p>テンプレート内の以下の変数を、クラスターに合わせて適切な値に置き換えます:</p><ul><li><code>LOAD_BALANCER_DNS</code></li><li><code>LOAD_BALANCER_PORT</code></li><li><code>ETCD_0_IP</code></li><li><code>ETCD_1_IP</code></li><li><code>ETCD_2_IP</code></li></ul></li></ul></li></ol><p>以下の手順は、積層etcdの構築と同様です。</p><ol><li><p><code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code>をこのノードで実行します。</p></li><li><p>表示されたjoinコマンドを、あとで使うためにテキストファイルに書き込みます。</p></li><li><p>使用するCNIプラグインを適用します。以下はWeave CNIの場合です:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div></li></ol><h3 id=残りのコントロールプレーンノードの手順-1>残りのコントロールプレーンノードの手順</h3><p>手順は、積層etcd構築の場合と同じです:</p><ul><li>最初のコントロールプレーンノードが完全に初期化されているのを確認します。</li><li>テキストファイルに保存したjoinコマンドを使って、それぞれのコントロールプレーンノードをクラスターへ参加させます。コントロールプレーンノードは1台ずつクラスターへ参加させるのを推奨します。</li><li><code>--certificate-key</code>で指定する復号キーは、デフォルトで2時間で期限切れになることを忘れないでください。</li></ul><h2 id=コントロールプレーン起動後の共通タスク>コントロールプレーン起動後の共通タスク</h2><h3 id=workerのインストール>workerのインストール</h3><p><code>kubeadm init</code>コマンドから返されたコマンドを利用して、workerノードをクラスターに参加させることが可能です。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><h2 id=manual-certs>証明書の手動配布</h2><p><code>--upload-certs</code>フラグを指定して<code>kubeadm init</code>を実行しない場合、プライマリコントロールプレーンノードから他のコントロールプレーンノードへ証明書を手動でコピーする必要があります。</p><p>コピーを行うには多くの方法があります。次の例では<code>ssh</code>と<code>scp</code>を使用しています。</p><p>1台のマシンから全てのノードをコントロールしたいのであれば、SSHが必要です。</p><ol><li><p>システム内の全ての他のノードにアクセスできるメインデバイスで、ssh-agentを有効にします</p><pre tabindex=0><code>eval $(ssh-agent)
</code></pre></li><li><p>SSHの秘密鍵を、セッションに追加します:</p><pre tabindex=0><code>ssh-add ~/.ssh/path_to_private_key
</code></pre></li><li><p>正常に接続できることを確認するために、ノード間でSSHします。</p><ul><li><p>ノードにSSHする際は、必ず<code>-A</code>フラグをつけます:</p><pre tabindex=0><code>ssh -A 10.0.0.7
</code></pre></li><li><p>ノードでsudoするときは、SSHフォワーディングが動くように、環境変数を引き継ぎます:</p><pre tabindex=0><code>sudo -E -s
</code></pre></li></ul></li><li><p>全てのノードでSSHを設定したら、<code>kubeadm init</code>を実行した後、最初のコントロールノードプレーンノードで次のスクリプトを実行します。このスクリプトは、最初のコントロールプレーンノードから残りのコントロールプレーンノードへ証明書ファイルをコピーします:</p><p>次の例の、<code>CONTROL_PLANE_IPS</code>を他のコントロールプレーンノードのIPアドレスに置き換えます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 環境に合わせる</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#666>=</span><span style=color:#b44>&#34;10.0.0.7 10.0.0.8&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> host in <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#b68;font-weight:700>}</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.pub <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.crt
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic># 外部のetcdノード使用時はこちらのコマンドを実行</span>
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.key
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> 上のリストにある証明書だけをコピーしてください。kubeadmが、参加するコントロールプレーンノード用に、残りの証明書と必要なSANの生成を行います。間違って全ての証明書をコピーしてしまったら、必要なSANがないため、追加ノードの作成は失敗するかもしれません。</div></li><li><p>次に、クラスターに参加させる残りの各コントロールプレーンノードで<code>kubeadm join</code>を実行する前に次のスクリプトを実行する必要があります。このスクリプトは、前の手順でコピーした証明書をホームディレクトリから<code>/etc/kubernetes/pki</code>へ移動します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># 環境に合わせる</span>
</span></span><span style=display:flex><span>mkdir -p /etc/kubernetes/pki/etcd
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.pub /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># 外部のetcdノード使用時はこちらのコマンドを実行</span>
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</span></span></code></pre></div></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-8160424c22d24f7d2d63c521e107dbf8>7 - kubeadmを使用した高可用性etcdクラスターの作成</h1><div class="alert alert-info note callout" role=alert><strong>備考:</strong> While kubeadm is being used as the management tool for external etcd nodes
in this guide, please note that kubeadm does not plan to support certificate rotation
or upgrades for such nodes. The long term plan is to empower the tool
<a href=https://github.com/kubernetes-sigs/etcdadm>etcdadm</a> to manage these
aspects.</div><p>Kubeadm defaults to running a single member etcd cluster in a static pod managed
by the kubelet on the control plane node. This is not a high availability setup
as the etcd cluster contains only one member and cannot sustain any members
becoming unavailable. This task walks through the process of creating a high
availability etcd cluster of three members that can be used as an external etcd
when using kubeadm to set up a kubernetes cluster.</p><h2 id=始める前に>始める前に</h2><ul><li>Three hosts that can talk to each other over ports 2379 and 2380. This
document assumes these default ports. However, they are configurable through
the kubeadm config file.</li><li>Each host must <a href=/ja/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>have docker, kubelet, and kubeadm installed</a>.</li><li>Each host should have access to the Kubernetes container image registry (<code>k8s.gcr.io</code>) or list/pull the required etcd image using <code>kubeadm config images list/pull</code>. This guide will setup etcd instances as <a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a> managed by a kubelet.</li><li>Some infrastructure to copy files between hosts. For example <code>ssh</code> and <code>scp</code>
can satisfy this requirement.</li></ul><h2 id=クラスターの構築>クラスターの構築</h2><p>The general approach is to generate all certs on one node and only distribute
the <em>necessary</em> files to the other nodes.</p><div class="alert alert-info note callout" role=alert><strong>備考:</strong> kubeadm contains all the necessary cryptographic machinery to generate
the certificates described below; no other cryptographic tooling is required for
this example.</div><ol><li><p>Configure the kubelet to be a service manager for etcd.</p><p>Since etcd was created first, you must override the service priority by creating a new unit file
that has higher precedence than the kubeadm-provided kubelet unit file.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>[Service]
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=
</span></span></span><span style=display:flex><span><span style=color:#b44>#  Replace &#34;systemd&#34; with the cgroup driver of your container runtime. The default value in the kubelet is &#34;cgroupfs&#34;.
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd
</span></span></span><span style=display:flex><span><span style=color:#b44>Restart=always
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li><li><p>Create configuration files for kubeadm.</p><p>Generate one kubeadm configuration file for each host that will have an etcd
member running on it using the following script.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#080;font-style:italic># Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST0</span><span style=color:#666>=</span>10.0.0.6
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST1</span><span style=color:#666>=</span>10.0.0.7
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST2</span><span style=color:#666>=</span>10.0.0.8
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Create temp directories to store files that will end up on other hosts.</span>
</span></span><span style=display:flex><span>mkdir -p /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ETCDHOSTS</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAMES</span><span style=color:#666>=(</span><span style=color:#b44>&#34;infra0&#34;</span> <span style=color:#b44>&#34;infra1&#34;</span> <span style=color:#b44>&#34;infra2&#34;</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span>!ETCDHOSTS[@]<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCDHOSTS</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAME</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAMES</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta2&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: ClusterConfiguration
</span></span></span><span style=display:flex><span><span style=color:#b44>etcd:
</span></span></span><span style=display:flex><span><span style=color:#b44>    local:
</span></span></span><span style=display:flex><span><span style=color:#b44>        serverCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        peerCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        extraArgs:
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster-state: new
</span></span></span><span style=display:flex><span><span style=color:#b44>            name: ${NAME}
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            advertise-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-advertise-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div></li><li><p>Generate the certificate authority</p><p>If you already have a CA then the only action that is copying the CA's <code>crt</code> and
<code>key</code> file to <code>/etc/kubernetes/pki/etcd/ca.crt</code> and
<code>/etc/kubernetes/pki/etcd/ca.key</code>. After those files have been copied,
proceed to the next step, "Create certificates for each member".</p><p>If you do not already have a CA then run this command on <code>$HOST0</code> (where you
generated the configuration files for kubeadm).</p><pre tabindex=0><code>kubeadm init phase certs etcd-ca
</code></pre><p>This creates two files</p><ul><li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li><li><code>/etc/kubernetes/pki/etcd/ca.key</code></li></ul></li><li><p>Create certificates for each member</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># cleanup non-reusable certificates</span>
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># No need to move the certs because they are for HOST0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># clean up certs that should not be copied off this host</span>
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span></code></pre></div></li><li><p>Copy certificates and kubeadm configs</p><p>The certificates have been generated and now they must be moved to their
respective hosts.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>scp -r /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>/* <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>:
</span></span><span style=display:flex><span>ssh <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>USER@HOST $ sudo -Es
</span></span><span style=display:flex><span>root@HOST $ chown -R root:root pki
</span></span><span style=display:flex><span>root@HOST $ mv pki /etc/kubernetes/
</span></span></code></pre></div></li><li><p>Ensure all expected files exist</p><p>The complete list of required files on <code>$HOST0</code> is:</p><pre tabindex=0><code>/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>On <code>$HOST1</code>:</p><pre tabindex=0><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>On <code>$HOST2</code></p><pre tabindex=0><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre></li><li><p>Create the static pod manifests</p><p>Now that the certificates and configs are in place it's time to create the
manifests. On each host run the <code>kubeadm</code> command to generate a static manifest
for etcd.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>root@HOST0 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>root@HOST1 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>root@HOST2 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span></code></pre></div></li><li><p>Optional: Check the cluster health</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>docker run --rm -it <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--net host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-v /etc/kubernetes:/etc/kubernetes k8s.gcr.io/etcd:<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCD_TAG</span><span style=color:#b68;font-weight:700>}</span> etcdctl <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--cert /etc/kubernetes/pki/etcd/peer.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--key /etc/kubernetes/pki/etcd/peer.key <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--cacert /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--endpoints https://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>:2379 endpoint health --cluster
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST0 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 16.283339ms
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST1 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 19.44402ms
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST2 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 35.926451ms
</span></span></code></pre></div><ul><li>Set <code>${ETCD_TAG}</code> to the version tag of your etcd image. For example <code>3.4.3-0</code>. To see the etcd image and tag that kubeadm uses execute <code>kubeadm config images list --kubernetes-version ${K8S_VERSION}</code>, where <code>${K8S_VERSION}</code> is for example <code>v1.17.0</code></li><li>Set <code>${HOST0}</code>to the IP address of the host you are testing.</li></ul></li></ol><h2 id=次の項目>次の項目</h2><p>Once you have a working 3 member etcd cluster, you can continue setting up a
highly available control plane using the <a href=/ja/docs/setup/production-environment/tools/kubeadm/high-availability/>external etcd method with
kubeadm</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-07709e71de6b4ac2573041c31213dbeb>8 - kubeadmを使用したクラスター内の各kubeletの設定</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.11 [stable]</code></div><p>kubeadm CLIツールのライフサイクルは、Kubernetesクラスター内の各ノード上で稼働するデーモンである<a href=/docs/reference/command-line-tools-reference/kubelet>kubelet</a>から分離しています。kubeadm CLIツールはKubernetesを初期化またはアップグレードする際にユーザーによって実行されます。一方で、kubeletは常にバックグラウンドで稼働しています。</p><p>kubeletはデーモンのため、何らかのinitシステムやサービスマネージャーで管理する必要があります。DEBパッケージやRPMパッケージからkubeletをインストールすると、systemdはkubeletを管理するように設定されます。代わりに別のサービスマネージャーを使用することもできますが、手動で設定する必要があります。</p><p>いくつかのkubeletの設定は、クラスターに含まれる全てのkubeletで同一である必要があります。一方で、特定のマシンの異なる特性(OS、ストレージ、ネットワークなど)に対応するために、kubeletごとに設定が必要なものもあります。手動で設定を管理することも可能ですが、kubeadmは<a href=#configure-kubelets-using-kubeadm>一元的な設定管理</a>のための<code>KubeletConfiguration</code>APIを提供しています。</p><h2 id=kubeletの設定パターン>Kubeletの設定パターン</h2><p>以下のセクションでは、kubeadmを使用したkubeletの設定パターンについて説明します。これは手動で各Nodeの設定を管理するよりも簡易に行うことができます。</p><h3 id=propagating-cluster-level-configuration-to-each-kubelet>各kubeletにクラスターレベルの設定を配布</h3><p><code>kubeadm init</code>および<code>kubeadm join</code>コマンドを使用すると、kubeletにデフォルト値を設定することができます。興味深い例として、異なるCRIランタイムを使用したり、Serviceが使用するデフォルトのサブネットを設定したりすることができます。</p><p>Serviceが使用するデフォルトのサブネットとして<code>10.96.0.0/12</code>を設定する必要がある場合は、<code>--service-cidr</code>パラメーターを渡します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init --service-cidr 10.96.0.0/12
</span></span></code></pre></div><p>これによってServiceの仮想IPはこのサブネットから割り当てられるようになりました。また、<code>--cluster-dns</code>フラグを使用し、kubeletが用いるDNSアドレスを設定する必要もあります。この設定はクラスター内の全てのマネージャーとNode上で同一である必要があります。kubeletは、<strong>kubeletのComponentConfig</strong>と呼ばれる、バージョン管理と構造化されたAPIオブジェクトを提供します。これはkubelet内のほとんどのパラメーターを設定し、その設定をクラスター内で稼働中の各kubeletへ適用することを可能にします。以下の例のように、キャメルケースのキーに値のリストとしてクラスターDNS IPアドレスなどのフラグを指定することができます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusterDNS</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:#666>10.96.0.10</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>ComponentConfigの詳細については、<a href=#configure-kubelets-using-kubeadm>このセクション</a>をご覧ください</p><h3 id=providing-instance-specific-configuration-details>インスタンス固有の設定内容を適用</h3><p>いくつかのホストでは、ハードウェア、オペレーティングシステム、ネットワーク、その他ホスト固有のパラメータの違いのため、特定のkubeletの設定を必要とします。以下にいくつかの例を示します。</p><ul><li>DNS解決ファイルへのパスは<code>--resolv-conf</code>フラグで指定することができますが、オペレーティングシステムや<code>systemd-resolved</code>を使用するかどうかによって異なる場合があります。このパスに誤りがある場合、そのNode上でのDNS解決は失敗します。</li><li>クラウドプロバイダーを使用していない場合、Node APIオブジェクト<code>.metadata.name</code>はデフォルトでマシンのホスト名に設定されます。異なるNode名を指定する必要がある場合には、<code>--hostname-override</code>フラグによってこの挙動を書き換えることができます。</li><li>現在のところ、kubletはCRIランタイムが使用するcgroupドライバを自動で検知することができませんが、kubeletの稼働を保証するためには、<code>--cgroup-driver</code>の値はCRIランタイムが使用するcgroupドライバに一致していなければなりません。</li><li>クラスターが使用するCRIランタイムによっては、異なるフラグを指定する必要があるかもしれません。例えば、Dockerを使用している場合には、<code>--network-plugin=cni</code>のようなフラグを指定する必要があります。外部のランタイムを使用している場合には、<code>--container-runtime=remote</code>と指定し、<code>--container-runtime-endpoint=&lt;path></code>のようにCRIエンドポイントを指定する必要があります。</li></ul><p>これらのフラグは、systemdなどのサービスマネージャー内のkubeletの設定によって指定することができます。</p><h2 id=configure-kubelets-using-kubeadm>kubeadmを使用したkubeletの設定</h2><p><code>kubeadm ... --config some-config-file.yaml</code>のように、カスタムの<code>KubeletConfiguration</code>APIオブジェクトを設定ファイルを介して渡すことで、kubeadmによって起動されるkubeletに設定を反映することができます。</p><p><code>kubeadm config print init-defaults --component-configs KubeletConfiguration</code>を実行することによって、この構造体の全てのデフォルト値を確認することができます。</p><p>また、各フィールドの詳細については、<a href=https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration>kubelet ComponentConfigに関するAPIリファレンス</a>を参照してください。</p><h3 id=kubeadm-init-実行時の流れ><code>kubeadm init</code>実行時の流れ</h3><p><code>kubeadm init</code>を実行した場合、kubeletの設定は<code>/var/lib/kubelet/config.yaml</code>に格納され、クラスターのConfigMapにもアップロードされます。ConfigMapは<code>kubelet-config-1.X</code>という名前で、<code>X</code>は初期化するKubernetesのマイナーバージョンを表します。またこの設定ファイルは、クラスタ内の全てのkubeletのために、クラスター全体設定の基準と共に<code>/etc/kubernetes/kubelet.conf</code>にも書き込まれます。この設定ファイルは、kubeletがAPIサーバと通信するためのクライアント証明書を指し示します。これは、<a href=#propagating-cluster-level-configuration-to-each-kubelet>各kubeletにクラスターレベルの設定を配布</a>することの必要性を示しています。</p><p>二つ目のパターンである、<a href=#providing-instance-specific-configuration-details>インスタンス固有の設定内容を適用</a>するために、kubeadmは環境ファイルを<code>/var/lib/kubelet/kubeadm-flags.env</code>へ書き出します。このファイルは以下のように、kubelet起動時に渡されるフラグのリストを含んでいます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>KUBELET_KUBEADM_ARGS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--flag1=value1 --flag2=value2 ...&#34;</span>
</span></span></code></pre></div><p>kubelet起動時に渡されるフラグに加えて、このファイルはcgroupドライバーや異なるCRIランタイムソケットを使用するかどうか(<code>--cri-socket</code>)といった動的なパラメータも含みます。</p><p>これら二つのファイルがディスク上に格納されると、systemdを使用している場合、kubeadmは以下の二つのコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>リロードと再起動に成功すると、通常の<code>kubeadm init</code>のワークフローが続きます。</p><h3 id=kubeadm-join-実行時の流れ><code>kubeadm join</code>実行時の流れ</h3><p><code>kubeadm join</code>を実行した場合、kubeadmはBootstrap Token証明書を使用してTLS bootstrapを行い、ConfigMap<code>kubelet-config-1.X</code>をダウンロードするために必要なクレデンシャルを取得し、<code>/var/lib/kubelet/config.yaml</code>へ書き込みます。動的な環境ファイルは、<code>kubeadm init</code>の場合と全く同様の方法で生成されます。</p><p>次に、<code>kubeadm</code>は、kubeletに新たな設定を読み込むために、以下の二つのコマンドを実行します。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>kubeletが新たな設定を読み込むと、kubeadmは、KubeConfigファイル<code>/etc/kubernetes/bootstrap-kubelet.conf</code>を書き込みます。これは、CA証明書とBootstrap Tokenを含みます。これらはkubeletがTLS Bootstrapを行い<code>/etc/kubernetes/kubelet.conf</code>に格納されるユニークなクレデンシャルを取得するために使用されます。ファイルが書き込まれると、kubeletはTLS Bootstrapを終了します。</p><h2 id=the-kubelet-drop-in-file-for-systemd>kubelet用のsystemdファイル</h2><p><code>kubeadm</code>には、systemdがどのようにkubeletを実行するかを指定した設定ファイルが同梱されています。
kubeadm CLIコマンドは決してこのsystemdファイルには触れないことに注意してください。</p><p>kubeadmの<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf>DEBパッケージ</a>または<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf>RPMパッケージ</a>によってインストールされたこの設定ファイルは、<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code>に書き込まれ、systemdで使用されます。基本的な<code>kubelet.service</code>(<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service>RPM用</a>または、 <a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service>DEB用</a>)を拡張します。</p><pre tabindex=0><code class=language-none data-lang=none>[Service]
Environment=&#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
--kubeconfig=/etc/kubernetes/kubelet.conf&#34;
Environment=&#34;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&#34;
# This is a file that &#34;kubeadm init&#34; and &#34;kubeadm join&#34; generate at runtime, populating
the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably,
# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.
# KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
</code></pre><p>このファイルは、kubeadmがkubelet用に管理する全ファイルが置かれるデフォルトの場所を指定します。</p><ul><li>TLS Bootstrapに使用するKubeConfigファイルは<code>/etc/kubernetes/bootstrap-kubelet.conf</code>ですが、<code>/etc/kubernetes/kubelet.conf</code>が存在しない場合にのみ使用します。</li><li>ユニークなkublet識別子を含むKubeConfigファイルは<code>/etc/kubernetes/kubelet.conf</code>です。</li><li>kubeletのComponentConfigを含むファイルは<code>/var/lib/kubelet/config.yaml</code>です。</li><li><code>KUBELET_KUBEADM_ARGS</code>を含む動的な環境ファイルは<code>/var/lib/kubelet/kubeadm-flags.env</code>から取得します。</li><li><code>KUBELET_EXTRA_ARGS</code>によるユーザー定義のフラグの上書きを格納できるファイルは<code>/etc/default/kubelet</code>(DEBの場合)、または<code>/etc/sysconfig/kubelet</code>(RPMの場合)から取得します。<code>KUBELET_EXTRA_ARGS</code>はフラグの連なりの最後に位置し、優先度が最も高いです。</li></ul><h2 id=kubernetesバイナリとパッケージの内容>Kubernetesバイナリとパッケージの内容</h2><p>Kubernetesに同梱されるDEB、RPMのパッケージは以下の通りです。</p><table><thead><tr><th>パッケージ名</th><th>説明</th></tr></thead><tbody><tr><td><code>kubeadm</code></td><td><code>/usr/bin/kubeadm</code>CLIツールと、<a href=#the-kubelet-drop-in-file-for-systemd>kubelet用のsystemdファイル</a>をインストールします。</td></tr><tr><td><code>kubelet</code></td><td>kubeletバイナリを<code>/usr/bin</code>に、CNIバイナリを<code>/opt/cni/bin</code>にインストールします。</td></tr><tr><td><code>kubectl</code></td><td><code>/usr/bin/kubectl</code>バイナリをインストールします。</td></tr><tr><td><code>cri-tools</code></td><td><code>/usr/bin/crictl</code>バイナリを<a href=https://github.com/kubernetes-incubator/cri-tools>cri-tools gitリポジトリ</a>からインストールします。</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-ed857e09999827b013ee9062dc9c59bb>9 - コントロールプレーンをセルフホストするようにkubernetesクラスターを構成する</h1><h3 id=self-hosting>コントロールプレーンのセルフホスティング</h3><p>kubeadmを使用すると、セルフホスト型のKubernetesコントロールプレーンを実験的に作成できます。これはAPIサーバー、コントローラーマネージャー、スケジューラーなどの主要コンポーネントは、静的ファイルを介してkubeletで構成された<a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a>ではなく、Kubernetes APIを介して構成された<a href=/ja/docs/concepts/workloads/controllers/daemonset/>DaemonSet pods</a>として実行されることを意味します。</p><p>セルフホスト型クラスターを作成する場合は<a href=/docs/reference/setup-tools/kubeadm/kubeadm-alpha/#cmd-selfhosting>kubeadm alpha selfhosting pivot</a>を参照してください。</p><h4 id=警告>警告</h4><div class="alert alert-warning caution callout" role=alert><strong>注意:</strong> この機能により、クラスターがサポートされていない状態になり、kubeadmがクラスターを管理できなくなります。これには<code>kubeadm upgrade</code>が含まれます。</div><ol><li><p>1.8以降のセルフホスティングには、いくつかの重要な制限があります。特に、セルフホスト型クラスターは、手動の介入なしにコントロールプレーンのNode再起動から回復することはできません。</p></li><li><p>デフォルトでは、セルフホスト型のコントロールプレーンのPodは、<a href=/docs/concepts/storage/volumes/#hostpath><code>hostPath</code></a>ボリュームからロードされた資格情報に依存しています。最初の作成を除いて、これらの資格情報はkubeadmによって管理されません。</p></li><li><p>コントロールプレーンのセルフホストされた部分にはetcdが含まれていませんが、etcdは静的Podとして実行されます。</p></li></ol><h4 id=プロセス>プロセス</h4><p>セルフホスティングのブートストラッププロセスは、<a href=https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.9.md#optional-self-hosting>kubeadm design
document</a>に記載されています。</p><p>要約すると、<code>kubeadm alpha selfhosting</code>は次のように機能します。</p><ol><li><p>静的コントロールプレーンのブートストラップが起動し、正常になるのを待ちます。これは<code>kubeadm init</code>のセルフホスティングを使用しないプロセスと同じです。</p></li><li><p>静的コントロールプレーンのPodのマニフェストを使用して、セルフホスト型コントロールプレーンを実行する一連のDaemonSetのマニフェストを構築します。また、必要に応じてこれらのマニフェストを変更します。たとえば、シークレット用の新しいボリュームを追加します。</p></li><li><p><code>kube-system</code>のネームスペースにDaemonSetを作成し、Podの結果が起動されるのを待ちます。</p></li><li><p>セルフホスト型のPodが操作可能になると、関連する静的Podが削除され、kubeadmは次のコンポーネントのインストールに進みます。これによりkubeletがトリガーされて静的Podが停止します。</p></li><li><p>元の静的なコントロールプレーンが停止すると、新しいセルフホスト型コントロールプレーンはリスニングポートにバインドしてアクティブになります。</p></li></ol></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/ja/docs/home/>ホーム</a>
<a class=text-white href=/ja/blog/>Blogs</a>
<a class=text-white href=/ja/training/>トレーニング</a>
<a class=text-white href=/ja/partners/>パートナー</a>
<a class=text-white href=/ja/community/>コミュニティ</a>
<a class=text-white href=/ja/case-studies/>ケーススタディ</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>