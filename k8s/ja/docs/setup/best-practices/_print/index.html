<!doctype html><html lang=ja class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/setup/best-practices/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/setup/best-practices/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/best-practices/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/best-practices/><link rel=alternate hreflang=hi href=https://kubernetes.io/hi/docs/setup/best-practices/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/best-practices/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/ja/docs/setup/best-practices/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>ベストプラクティス | Kubernetes</title><meta property="og:title" content="ベストプラクティス"><meta property="og:description" content="プロダクショングレードのコンテナ管理基盤"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/ja/docs/setup/best-practices/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="ベストプラクティス"><meta itemprop=description content="プロダクショングレードのコンテナ管理基盤"><meta name=twitter:card content="summary"><meta name=twitter:title content="ベストプラクティス"><meta name=twitter:description content="プロダクショングレードのコンテナ管理基盤"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/ja/docs/setup/best-practices/"><meta property="og:title" content="ベストプラクティス"><meta name=twitter:title content="ベストプラクティス"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/ja/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/ja/docs/>ドキュメント</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/blog/>Blogs</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/training/>トレーニング</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/partners/>パートナー</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/community/>コミュニティ</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/ja/case-studies/>ケーススタディ</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>バージョン</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/ja/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/ja/docs/setup/best-practices/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/ja/docs/setup/best-practices/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/ja/docs/setup/best-practices/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/ja/docs/setup/best-practices/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/ja/docs/setup/best-practices/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>日本語 (Japanese)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/setup/best-practices/>English</a>
<a class=dropdown-item href=/zh-cn/docs/setup/best-practices/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/setup/best-practices/>한국어 (Korean)</a>
<a class=dropdown-item href=/id/docs/setup/best-practices/>Bahasa Indonesia</a>
<a class=dropdown-item href=/hi/docs/setup/best-practices/>हिन्दी (Hindi)</a>
<a class=dropdown-item href=/uk/docs/setup/best-practices/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>これは、このセクションの複数ページの印刷可能なビューです。
<a href=# onclick="return print(),!1">印刷するには、ここをクリックしてください</a>.</p><p><a href=/ja/docs/setup/best-practices/>このページの通常のビューに戻る</a>.</p></div><h1 class=title>ベストプラクティス</h1><ul><li>1: <a href=#pg-970615c97499e3651fd3a98e0387cefc>複数のゾーンで動かす</a></li><li>2: <a href=#pg-c797ee17120176c685455db89ae091a9>大規模クラスターの構築</a></li><li>3: <a href=#pg-f89867de1d34943f1524f67a241f5cc9>ノードのセットアップの検証</a></li><li>4: <a href=#pg-0394f813094b7a35058dffe5b8bacd20>PKI証明書とその要件</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-970615c97499e3651fd3a98e0387cefc>1 - 複数のゾーンで動かす</h1><p>This page describes how to run a cluster in multiple zones.</p><h2 id=始めに>始めに</h2><p>Kubernetes 1.2より、複数のゾーンにおいて単一のクラスターを運用するサポートが追加されました(GCEでは単純に"ゾーン"，AWSは"アベイラビリティゾーン"と呼びますが、ここでは"ゾーン"とします)。
これは、より範囲の広いCluster Federationの軽量バージョンです(以前は<a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multicluster/federation.md>"Ubernetes"</a>の愛称で言及されていました)。
完全なCluster Federationでは、異なるリージョンやクラウドプロバイダー(あるいはオンプレミスデータセンター)内の独立したKubernetesクラスターをまとめることが可能になります。しかしながら、多くのユーザーは単に1つのクラウドプロバイダーの複数のゾーンでより可用性の高いKubernetesクラスターを運用したいと考えており、バージョン1.2におけるマルチゾーンサポート(以前は"Ubernetes Lite"の愛称で使用されていました)ではこれが可能になります。</p><p>マルチゾーンサポートは故意に限定されています: 1つのKubernetesクラスターは複数のゾーンで運用することができますが、同じリージョン(あるいはクラウドプロバイダー)のみです。現在はGCEとAWSのみが自動的にサポートされています(他のクラウドプロバイダーやベアメタル環境においても、単にノードやボリュームに追加する適切なラベルを用意して同様のサポートを追加することは容易ではありますが)。</p><h2 id=機能性>機能性</h2><p>ノードが開始された時、kubeletは自動的にそれらにゾーン情報を付したラベルを追加します。</p><p>Kubernetesはレプリケーションコントローラーやサービス内のPodをシングルゾーンクラスターにおけるノードにデプロイします(障害の影響を減らすため)。マルチゾーンクラスターでは、このデプロイの挙動はゾーンを跨いで拡張されます(障害の影響を減らすため)(これは<code>SelectorSpreadPriority</code>によって可能になります)。これはベストエフォートな配置であり、つまりもしクラスターのゾーンが異種である(例:異なる数のノード，異なるタイプのノードや異なるPodのリソース要件)場合、これはゾーンを跨いだPodのデプロイを完璧に防ぐことができます。必要であれば、同種のゾーン(同一の数及びタイプのノード)を利用して不平等なデプロイの可能性を減らすことができます。</p><p>永続ボリュームが作成されると、<code>PersistentVolumeLabel</code>アドミッションコントローラーがそれらにゾーンラベルを付与します。スケジューラーは<code>VolumeZonePredicate</code>を通じて与えられたボリュームを請求するPodがそのボリュームと同じゾーンにのみ配置されることを保証します、これはボリュームはゾーンを跨いでアタッチすることができないためです。</p><h2 id=制限>制限</h2><p>マルチゾーンサポートにはいくつか重要な制限があります:</p><ul><li><p>異なるゾーンはネットワーク内においてお互いに近接して位置していることが想定されているため、いかなるzone-aware routingも行われません。特に、トラフィックはゾーンを跨いだサービスを通じて行き来するため(サービスをサポートするいくつかのPodがクライアントと同じゾーンに存在していても)、これは追加のレイテンシやコストを生むかもしれません。</p></li><li><p>Volume zone-affinityは<code>PersistentVolume</code>と共に動作し、例えばPodのスペックにおいてEBSボリュームを直接指定しても動作しません。</p></li><li><p>クラスターはクラウドやリージョンを跨げません(この機能はフルフェデレーションサポートが必要です)。</p></li></ul><p>*ノードは複数のゾーンに存在しますが、kube-upは現在デフォルトではシングルマスターノードでビルドします。サービスは高可用性でありゾーンの障害に耐えることができますが、コントロールプレーンは単一のゾーンに配置されます。高可用性コントロールプレーンを必要とするユーザーは<a href=/ja/docs/setup/production-environment/tools/kubeadm/high-availability/>高可用性</a>の説明を参照してください。</p><h3 id=ボリュームの制限>ボリュームの制限</h3><p>以下の制限は<a href=/docs/concepts/storage/storage-classes/#volume-binding-mode>topology-aware volume binding</a>に記載されています。</p><ul><li><p>動的なプロビジョニングを使用する際のStatefulSetボリュームゾーンのデプロイは、現在Podのアフィニティあるいはアンチアフィニティと互換性がありません。</p></li><li><p>StatefulSetの名前がダッシュ("-")を含む場合、ボリュームゾーンのデプロイはゾーンを跨いだストレージの均一な分配を提供しない可能性があります。</p></li><li><p>DeploymentやPodのスペックにおいて複数のPVCを指定すると、StorageClassは特定の1つのゾーンに割り当てる必要があります、あるいはPVは特定のゾーンに静的にプロビジョンされる必要があります。もう一つの解決方法として、StatefulSetを使用すると、レプリカに対する全てのボリュームが同じゾーンにプロビジョンされます。</p></li></ul><h2 id=全体の流れ>全体の流れ</h2><p>GCEとAWSの両方にマルチゾーンのクラスターをセットアップし使用する手順について説明します。そのために、フルクラスターを用意し(<code>MULTIZONE=true</code>と指定する)、<code>kube-up</code>を再び実行して追加のゾーンにノードを追加します(<code>KUBE_USE_EXISTING_MASTER=true</code>と指定する)。</p><h3 id=クラスターの立ち上げ>クラスターの立ち上げ</h3><p>通常と同様にクラスターを作成します、しかし複数のゾーンを管理するためにMULTIZONEをクラスターに設定します。ノードをus-central1-aに作成します。</p><p>GCE:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -sS https://get.k8s.io | <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-a <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> bash
</span></span></code></pre></div><p>AWS:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -sS https://get.k8s.io | <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2a <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> bash
</span></span></code></pre></div><p>このステップは通常と同様にクラスターを立ち上げ、1つのゾーンで動作しています(しかし、<code>MULTIZONE=true</code>によりマルチゾーン能力は有効になっています)。</p><h3 id=ノードはラベルが付与される>ノードはラベルが付与される</h3><p>ノードを見てください。それらがゾーン情報と共にラベルされているのが分かります。
それら全ては今のところ<code>us-central1-a</code> (GCE)あるいは<code>us-west-2a</code> (AWS)にあります。ラベルは<code>topology.kubernetes.io/region</code>がリージョンに、<code>topology.kubernetes.io/zone</code>はゾーンに付けられています:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get nodes --show-labels
</span></span></code></pre></div><p>結果は以下のようになります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                     STATUS                     ROLES    AGE   VERSION          LABELS
</span></span><span style=display:flex><span>kubernetes-master        Ready,SchedulingDisabled   &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-1,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-master
</span></span><span style=display:flex><span>kubernetes-minion-87j9   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-87j9
</span></span><span style=display:flex><span>kubernetes-minion-9vlv   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-9vlv
</span></span><span style=display:flex><span>kubernetes-minion-a12q   Ready                      &lt;none&gt;   6m    v1.13.0          beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-a12q
</span></span></code></pre></div><h3 id=2つ目のゾーンにさらにノードを追加>2つ目のゾーンにさらにノードを追加</h3><p>それでは、現存のマスターを再利用し、現存のクラスターの異なるゾーン(us-central1-bかus-west-2b)にもう1つのノードのセットを追加しましょう。
kube-upを再び実行します．しかし<code>KUBE_USE_EXISTING_MASTER=true</code>を指定することでkube-upは新しいマスターを作成せず、代わりに以前作成したものを再利用します。</p><p>GCE:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-b <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> kubernetes/cluster/kube-up.sh
</span></span></code></pre></div><p>AWSではマスターの内部IPアドレスに加えて追加のサブネット用のネットワークCIDRを指定する必要があります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2b <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> <span style=color:#b8860b>KUBE_SUBNET_CIDR</span><span style=color:#666>=</span>172.20.1.0/24 <span style=color:#b8860b>MASTER_INTERNAL_IP</span><span style=color:#666>=</span>172.20.0.9 kubernetes/cluster/kube-up.sh
</span></span></code></pre></div><p>ノードをもう1度見てください。更なる3つのノードがus-central1-bに起動し、タグ付けられているはずです:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get nodes --show-labels
</span></span></code></pre></div><p>結果は以下のようになります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                     STATUS                     ROLES    AGE   VERSION           LABELS
</span></span><span style=display:flex><span>kubernetes-master        Ready,SchedulingDisabled   &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-1,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-master
</span></span><span style=display:flex><span>kubernetes-minion-281d   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-b,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-281d
</span></span><span style=display:flex><span>kubernetes-minion-87j9   Ready                      &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-87j9
</span></span><span style=display:flex><span>kubernetes-minion-9vlv   Ready                      &lt;none&gt;   16m   v1.13.0           beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-9vlv
</span></span><span style=display:flex><span>kubernetes-minion-a12q   Ready                      &lt;none&gt;   17m   v1.13.0           beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-a12q
</span></span><span style=display:flex><span>kubernetes-minion-pp2f   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-b,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-pp2f
</span></span><span style=display:flex><span>kubernetes-minion-wf8i   Ready                      &lt;none&gt;   2m    v1.13.0           beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-b,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-wf8i
</span></span></code></pre></div><h3 id=ボリュームのアフィニティ>ボリュームのアフィニティ</h3><p>動的ボリュームを使用してボリュームを作成します(PersistentVolumeのみがゾーンアフィニティに対してサポートされています):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f - <span style=color:#b44>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#b44>{
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;apiVersion&#34;: &#34;v1&#34;,
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;kind&#34;: &#34;PersistentVolumeClaim&#34;,
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;metadata&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#b44>    &#34;name&#34;: &#34;claim1&#34;,
</span></span></span><span style=display:flex><span><span style=color:#b44>    &#34;annotations&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#b44>        &#34;volume.alpha.kubernetes.io/storage-class&#34;: &#34;foo&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    }
</span></span></span><span style=display:flex><span><span style=color:#b44>  },
</span></span></span><span style=display:flex><span><span style=color:#b44>  &#34;spec&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#b44>    &#34;accessModes&#34;: [
</span></span></span><span style=display:flex><span><span style=color:#b44>      &#34;ReadWriteOnce&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    ],
</span></span></span><span style=display:flex><span><span style=color:#b44>    &#34;resources&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#b44>      &#34;requests&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#b44>        &#34;storage&#34;: &#34;5Gi&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>      }
</span></span></span><span style=display:flex><span><span style=color:#b44>    }
</span></span></span><span style=display:flex><span><span style=color:#b44>  }
</span></span></span><span style=display:flex><span><span style=color:#b44>}
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>備考:</strong> バージョン1.3以降のKubernetesは設定したゾーンを跨いでPVクレームを分配します。
バージョン1.2では動的永続ボリュームは常にクラスターのマスターがあるゾーンに作成されます。
(ここではus-central1-a / us-west-2a); このイシューは
(<a href=https://github.com/kubernetes/kubernetes/issues/23330>#23330</a>)
にバージョン1.3以降で記載されています。</div><p>それでは、KubernetesがPVが作成されたゾーン及びリージョンを自動的にラベルしているか確認しましょう。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pv --show-labels
</span></span></code></pre></div><p>結果は以下のようになります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME           CAPACITY   ACCESSMODES   RECLAIM POLICY   STATUS    CLAIM            STORAGECLASS    REASON    AGE       LABELS
</span></span><span style=display:flex><span>pv-gce-mj4gm   5Gi        RWO           Retain           Bound     default/claim1   manual                    46s       topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a
</span></span></code></pre></div><p>では永続ボリュームクレームを使用するPodを作成します。
GCE PD / AWS EBSボリュームはゾーンを跨いでアタッチできないため、これはこのPodがボリュームと同じゾーンにのみ作成されることを意味します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>kubectl apply -f - &lt;&lt;EOF<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myfrontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/var/www/html&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>claim1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>EOF<span style=color:#bbb>
</span></span></span></code></pre></div><p>一般的にゾーンを跨いだアタッチはクラウドプロバイダーによって許可されていないため、Podは自動的にボリュームと同じゾーンに作成されることに注意してください:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pod mypod | grep Node
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Node:        kubernetes-minion-9vlv/10.240.0.5
</span></span></code></pre></div><p>ノードのラベルをチェックします:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get node kubernetes-minion-9vlv --show-labels
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                     STATUS    AGE    VERSION          LABELS
</span></span><span style=display:flex><span>kubernetes-minion-9vlv   Ready     22m    v1.6.0+fff5156   beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-9vlv
</span></span></code></pre></div><h3 id=podがゾーンをまたがって配置される>Podがゾーンをまたがって配置される</h3><p>レプリケーションコントローラーやサービス内のPodは自動的にゾーンに跨いでデプロイされます。まず、3つ目のゾーンに更なるノードを立ち上げましょう:</p><p>GCE:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-f <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> kubernetes/cluster/kube-up.sh
</span></span></code></pre></div><p>AWS:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2c <span style=color:#b8860b>NUM_NODES</span><span style=color:#666>=</span><span style=color:#666>3</span> <span style=color:#b8860b>KUBE_SUBNET_CIDR</span><span style=color:#666>=</span>172.20.2.0/24 <span style=color:#b8860b>MASTER_INTERNAL_IP</span><span style=color:#666>=</span>172.20.0.9 kubernetes/cluster/kube-up.sh
</span></span></code></pre></div><p>3つのゾーンにノードがあることを確認します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get nodes --show-labels
</span></span></code></pre></div><p>シンプルなWebアプリケーションを動作する、3つのRCを持つguestbook-goの例を作成します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>find kubernetes/examples/guestbook-go/ -name <span style=color:#b44>&#39;*.json&#39;</span> | xargs -I <span style=color:#666>{}</span> kubectl apply -f <span style=color:#666>{}</span>
</span></span></code></pre></div><p>Podは3つの全てのゾーンにデプロイされているはずです:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pod -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>guestbook | grep Node
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Node:        kubernetes-minion-9vlv/10.240.0.5
</span></span><span style=display:flex><span>Node:        kubernetes-minion-281d/10.240.0.8
</span></span><span style=display:flex><span>Node:        kubernetes-minion-olsh/10.240.0.11
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get node kubernetes-minion-9vlv kubernetes-minion-281d kubernetes-minion-olsh --show-labels
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                     STATUS    ROLES    AGE    VERSION          LABELS
</span></span><span style=display:flex><span>kubernetes-minion-9vlv   Ready     &lt;none&gt;   34m    v1.13.0          beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-a,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-9vlv
</span></span><span style=display:flex><span>kubernetes-minion-281d   Ready     &lt;none&gt;   20m    v1.13.0          beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-b,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-281d
</span></span><span style=display:flex><span>kubernetes-minion-olsh   Ready     &lt;none&gt;   3m     v1.13.0          beta.kubernetes.io/instance-type<span style=color:#666>=</span>n1-standard-2,topology.kubernetes.io/region<span style=color:#666>=</span>us-central1,topology.kubernetes.io/zone<span style=color:#666>=</span>us-central1-f,kubernetes.io/hostname<span style=color:#666>=</span>kubernetes-minion-olsh
</span></span></code></pre></div><p>ロードバランサーはクラスター内の全てのゾーンにデプロイされています; guestbook-goの例は負荷分散サービスのサンプルを含みます:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe service guestbook | grep LoadBalancer.Ingress
</span></span></code></pre></div><p>結果は以下のようになります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>LoadBalancer Ingress:   130.211.126.21
</span></span></code></pre></div><p>IPの上に設定します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>IP</span><span style=color:#666>=</span>130.211.126.21
</span></span></code></pre></div><p>IPをcurlを通じて探索します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -s http://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>IP</span><span style=color:#b68;font-weight:700>}</span>:3000/env | grep HOSTNAME
</span></span></code></pre></div><p>結果は以下のようになります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-44sep&#34;</span>,
</span></span></code></pre></div><p>再び、複数回探索します:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#666>(</span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>`</span>seq 20<span style=color:#b44>`</span>; <span style=color:#a2f;font-weight:700>do</span> curl -s http://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>IP</span><span style=color:#b68;font-weight:700>}</span>:3000/env | grep HOSTNAME; <span style=color:#a2f;font-weight:700>done</span><span style=color:#666>)</span>  | sort | uniq
</span></span></code></pre></div><p>結果は以下のようになります:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-44sep&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-hum5n&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#b44>&#34;HOSTNAME&#34;</span>: <span style=color:#b44>&#34;guestbook-ppm40&#34;</span>,
</span></span></code></pre></div><p>ロードバランサーは、たとえPodが複数のゾーンに存在していても、全てのPodをターゲットします。</p><h3 id=クラスターの停止>クラスターの停止</h3><p>終了したら、クリーンアップします:</p><p>GCE:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-f kubernetes/cluster/kube-down.sh
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-b kubernetes/cluster/kube-down.sh
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>gce <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>us-central1-a kubernetes/cluster/kube-down.sh
</span></span></code></pre></div><p>AWS:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2c kubernetes/cluster/kube-down.sh
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_USE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2b kubernetes/cluster/kube-down.sh
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBERNETES_PROVIDER</span><span style=color:#666>=</span>aws <span style=color:#b8860b>KUBE_AWS_ZONE</span><span style=color:#666>=</span>us-west-2a kubernetes/cluster/kube-down.sh
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c797ee17120176c685455db89ae091a9>2 - 大規模クラスターの構築</h1><p>クラスターはKubernetesのエージェントが動作する(物理もしくは仮想の)<a class=glossary-tooltip title=ノードはKubernetesのワーカーマシンです。 data-toggle=tooltip data-placement=top href=/ja/docs/concepts/architecture/nodes/ target=_blank aria-label=ノード>ノード</a>の集合で、<a class=glossary-tooltip title=コンテナのライフサイクルを定義、展開、管理するためのAPIとインターフェイスを公開するコンテナオーケストレーションレイヤーです。 data-toggle=tooltip data-placement=top href='/ja/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label=コントロールプレーン>コントロールプレーン</a>によって管理されます。
Kubernetes v1.25 では、最大5000ノードから構成されるクラスターをサポートします。
具体的には、Kubernetesは次の基準を <em>全て</em> 満たす構成に対して適用できるように設計されています。</p><ul><li>1ノードにつきPodが110個以上存在しない</li><li>5000ノード以上存在しない</li><li>Podの総数が150000個以上存在しない</li><li>コンテナの総数が300000個以上存在しない</li></ul><p>ノードを追加したり削除したりすることによって、クラスターをスケールできます。
これを行う方法は、クラスターがどのようにデプロイされたかに依存します。</p><h2 id=クォータの問題>クラウドプロバイダーのリソースクォータ</h2><p>クラウドプロバイダーのクォータの問題に遭遇することを避けるため、多数のノードを使ったクラスターを作成するときには次のようなことを考慮してください。</p><ul><li>次のようなクラウドリソースの増加をリクエストする<ul><li>コンピューターインスタンス</li><li>CPU</li><li>ストレージボリューム</li><li>使用中のIPアドレス</li><li>パケットフィルタリングのルールセット</li><li>ロードバランサーの数</li><li>ネットワークサブネット</li><li>ログストリーム</li></ul></li><li>クラウドプロバイダーによる新しいインスタンスの作成に対するレート制限のため、バッチで新しいノードを立ち上げるようなクラスターのスケーリング操作を通すためには、バッチ間ですこし休止を入れます。</li></ul><h2 id=コントロールプレーンのコンポーネント>コントロールプレーンのコンポーネント</h2><p>大きなクラスターでは、十分な計算とその他のリソースを持ったコントロールプレーンが必要になります。</p><p>特に故障ゾーンあたり1つまたは2つのコントロールプレーンインスタンスを動かす場合、最初に垂直方向にインスタンスをスケールし、垂直方向のスケーリングの効果が低下するポイントに達したら水平方向にスケールします。</p><p>フォールトトレランスを備えるために、1つの故障ゾーンに対して最低1インスタンスを動かすべきです。
Kubernetesノードは、同一故障ゾーン内のコントロールプレーンエンドポイントに対して自動的にトラフィックが向かないようにします。
しかし、クラウドプロバイダーはこれを実現するための独自の機構を持っているかもしれません。</p><p>例えばマネージドなロードバランサーを使うと、故障ゾーン <em>A</em> にあるkubeletやPodから発生したトラフィックを、同じく故障ゾーン <em>A</em> にあるコントロールプレーンホストに対してのみ送るように設定します。もし1つのコントロールプレーンホストまたは故障ゾーン <em>A</em> のエンドポイントがオフラインになった場合、ゾーン <em>A</em> にあるノードについてすべてのコントロールプレーンのトラフィックはゾーンを跨いで送信されます。それぞれのゾーンで複数のコントロールプレーンホストを動作させることは、結果としてほとんどありません。</p><h2 id=etcdストレージ>etcdストレージ</h2><p>大きなクラスターの性能を向上させるために、他の専用のetcdインスタンスにイベントオブジェクトを保存できます。</p><p>クラスターを作るときに、(カスタムツールを使って)以下のようなことができます。</p><ul><li>追加のetcdインスタンスを起動または設定する</li><li>イベントを保存するために<a class=glossary-tooltip title='Kubernetes APIを提供するコントロールプレーンのコンポーネントです。' data-toggle=tooltip data-placement=top href=/ja/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=APIサーバ>APIサーバ</a>を設定する</li></ul><p>大きなクラスターのためにetcdを設定・管理する詳細については、<a href=/docs/tasks/administer-cluster/configure-upgrade-etcd/>Operating etcd clusters for Kubernetes</a>または<a href=/ja/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>kubeadmを使用した高可用性etcdクラスターの作成</a>を見てください。</p><h2 id=アドオンのリソース>アドオンのリソース</h2><p>Kubernetesの<a href=/ja/docs/concepts/configuration/manage-resources-containers/>リソース制限</a>は、メモリリークの影響やPodやコンテナが他のコンポーネントに与える他の影響を最小化することに役立ちます。
これらのリソース制限は、アプリケーションのワークロードに適用するのと同様に、<a class=glossary-tooltip title='Resources that extend the functionality of Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/cluster-administration/addons/ target=_blank aria-label=アドオン>アドオン</a>のリソースにも適用されます。</p><p>例えば、ロギングコンポーネントに対してCPUやメモリ制限を設定できます。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-cloud-logging<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>fluent/fluentd-kubernetes-daemonset:v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span></code></pre></div><p>アドオンのデフォルト制限は、アドオンを小～中規模のKubernetesクラスターで動作させたときの経験から得られたデータに基づきます。
大規模のクラスターで動作させる場合は、アドオンはデフォルト制限よりも多くのリソースを消費することが多いです。
これらの値を調整せずに大規模のクラスターをデプロイした場合、メモリー制限に達し続けるため、アドオンが継続的に停止されるかもしれません。
あるいは、CPUのタイムスライス制限により性能がでない状態で動作するかもしれません。</p><p>クラスターのアドオンのリソース制限に遭遇しないために、多くのノードで構成されるクラスターを構築する場合は次のことを考慮します。</p><ul><li>いくつかのアドオンは垂直方向にスケールします - クラスターに1つのレプリカ、もしくは故障ゾーン全体にサービングされるものがあります。このようなアドオンでは、クラスターをスケールアウトしたときにリクエストと制限を増やす必要があります。</li><li>数多くのアドオンは、水平方向にスケールします - より多くのPod数を動作させることで性能を向上できます - ただし、とても大きなクラスターではCPUやメモリの制限も少し引き上げる必要があるかもしれません。VerticalPodAutoscalerは、提案されたリクエストや制限の数値を提供する <code>_recommender_</code> モードで動作可能です。</li><li>いくつかのアドオンは<a class=glossary-tooltip title=Podのコピーがクラスター内の一連のNodeに渡って実行されることを保証します。 data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a>によって制御され、1ノードに1つ複製される形で動作します: 例えばノードレベルのログアグリゲーターです。水平方向にスケールするアドオンの場合と同様に、CPUやメモリ制限を少し引き上げる必要があるかもしれません。</li></ul><h2 id=次の項目>次の項目</h2><p><code>VerticalPodAutoscaler</code> は、リソースのリクエストやPodの制限についての管理を手助けするためにクラスターへデプロイ可能なカスタムリソースです。
<code>VerticalPodAutoscaler</code> やクラスターで致命的なアドオンを含むクラスターコンポーネントをスケールする方法についてさらに知りたい場合は<a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme>Vertical Pod Autoscaler</a>をご覧ください。</p><p><a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme>cluster autoscaler</a>は、クラスターで要求されるリソース水準を満たす正確なノード数で動作できるよう、いくつかのクラウドプロバイダーと統合されています。</p><p><a href=https://github.com/kubernetes/autoscaler/tree/master/addon-resizer#readme>addon resizer</a>は、クラスターのスケールが変化したときにアドオンの自動的なリサイズをお手伝いします。</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f89867de1d34943f1524f67a241f5cc9>3 - ノードのセットアップの検証</h1><h2 id=ノード適合テスト>ノード適合テスト</h2><p><em>ノード適合テスト</em> は、システムの検証とノードに対する機能テストを提供するコンテナ型のテストフレームワークです。このテストは、ノードがKubernetesの最小要件を満たしているかどうかを検証するもので、テストに合格したノードはKubernetesクラスタに参加する資格があることになります。</p><h2 id=制約>制約</h2><p>Kubernetesのバージョン1.5ではノード適合テストには以下の制約があります:</p><ul><li>ノード適合テストはコンテナのランタイムとしてDockerのみをサポートします。</li></ul><h2 id=ノードの前提条件>ノードの前提条件</h2><p>適合テストを実行するにはノードは通常のKubernetesノードと同じ前提条件を満たしている必要があります。 最低でもノードに以下のデーモンがインストールされている必要があります:</p><ul><li>コンテナランタイム (Docker)</li><li>Kubelet</li></ul><h2 id=ノード適合テストの実行>ノード適合テストの実行</h2><p>ノード適合テストを実行するには、以下の手順に従います:</p><ol><li>Kubeletをlocalhostに指定します(<code>--api-servers="http://localhost:8080"</code>)、
このテストフレームワークはKubeletのテストにローカルマスターを起動するため、Kubeletをローカルホストに設定します(<code>--api-servers="http://localhost:8080"</code>)。他にも配慮するべきKubeletフラグがいくつかあります:</li></ol><ul><li><code>--pod-cidr</code>: <code>kubenet</code>を利用している場合は、Kubeletに任意のCIDR(例: <code>--pod-cidr=10.180.0.0/24</code>)を指定する必要があります。</li><li><code>--cloud-provider</code>: <code>--cloud-provider=gce</code>を指定している場合は、テストを実行する前にこのフラグを取り除いてください。</li></ul><ol start=2><li>以下のコマンドでノード適合テストを実行します:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># $CONFIG_DIRはKubeletのPodのマニフェストパスです。</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># $LOG_DIRはテスト出力のパスです。</span>
</span></span><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  k8s.gcr.io/node-test:0.2
</span></span></code></pre></div><h2 id=他アーキテクチャ向けのノード適合テストの実行>他アーキテクチャ向けのノード適合テストの実行</h2><p>Kubernetesは他のアーキテクチャ用のノード適合テストのdockerイメージを提供しています:</p><table><thead><tr><th>Arch</th><th style=text-align:center>Image</th></tr></thead><tbody><tr><td>amd64</td><td style=text-align:center>node-test-amd64</td></tr><tr><td>arm</td><td style=text-align:center>node-test-arm</td></tr><tr><td>arm64</td><td style=text-align:center>node-test-arm64</td></tr></tbody></table><h2 id=選択したテストの実行>選択したテストの実行</h2><p>特定のテストを実行するには、環境変数<code>FOCUS</code>を実行したいテストの正規表現で上書きします。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>FOCUS</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># MirrorPodテストのみを実行します</span>
</span></span><span style=display:flex><span>  k8s.gcr.io/node-test:0.2
</span></span></code></pre></div><p>特定のテストをスキップするには、環境変数<code>SKIP</code>をスキップしたいテストの正規表現で上書きします。</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>SKIP</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># MirrorPodテスト以外のすべてのノード適合テストを実行します</span>
</span></span><span style=display:flex><span>  k8s.gcr.io/node-test:0.2
</span></span></code></pre></div><p>ノード適合テストは、<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md>node e2e test</a>のコンテナ化されたバージョンです。
デフォルトでは、すべての適合テストが実行されます。</p><p>理論的には、コンテナを構成し必要なボリュームを適切にマウントすれば、どのノードのe2eテストも実行できます。しかし、不適合テストを実行するためにはより複雑な設定が必要となるため、<strong>適合テストのみを実行することを強く推奨します</strong>。</p><h2 id=注意事項>注意事項</h2><ul><li>このテストでは、ノード適合テストイメージや機能テストで使用されるコンテナのイメージなど、いくつかのdockerイメージがノード上に残ります。</li><li>このテストでは、ノード上にデッドコンテナが残ります。これらのコンテナは機能テスト中に作成されます。</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0394f813094b7a35058dffe5b8bacd20>4 - PKI証明書とその要件</h1><p>Kubernetesでは、TLS認証のためにPKI証明書が必要です。
<a href=/docs/reference/setup-tools/kubeadm/kubeadm/>kubeadm</a>でKubernetesをインストールする場合、必要な証明書は自動で生成されます。
自身で証明書を作成することも可能です。例えば、秘密鍵をAPIサーバーに保持しないことで、管理をよりセキュアにする場合が挙げられます。
本ページでは、クラスターに必要な証明書について説明します。</p><h2 id=クラスタではどのように証明書が使われているのか>クラスタではどのように証明書が使われているのか</h2><p>Kubernetesは下記の用途でPKIを必要とします：</p><ul><li>kubeletがAPIサーバーの認証をするためのクライアント証明書</li><li>APIサーバーのエンドポイント用サーバー証明書</li><li>クラスターの管理者がAPIサーバーの認証を行うためのクライアント証明書</li><li>APIサーバーがkubeletと通信するためのクライアント証明書</li><li>APIサーバーがetcdと通信するためのクライアント証明書</li><li>controller managerがAPIサーバーと通信するためのクライアント証明書およびkubeconfig</li><li>スケジューラーがAPIサーバーと通信するためのクライアント証明書およびkubeconfig</li><li><a href=/docs/tasks/extend-kubernetes/configure-aggregation-layer/>front-proxy</a>用のクライアント証明書およびサーバー証明書</li></ul><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>front-proxy</code>証明書は、<a href=/docs/tasks/extend-kubernetes/setup-extension-api-server/>Kubernetes APIの拡張</a>をサポートするためにkube-proxyを実行する場合のみ必要です。</div><p>さらに、etcdはクライアントおよびピア間の認証に相互TLS通信を実装しています。</p><h2 id=証明書の保存場所>証明書の保存場所</h2><p>kubeadmを使用してKubernetesをインストールする場合、証明書は<code>/etc/kubernetes/pki</code>に保存されます。このドキュメントの全てのパスは、そのディレクトリの相対パスを表します。</p><h2 id=手動で証明書を設定する>手動で証明書を設定する</h2><p>kubeadmで証明書を生成したくない場合は、下記の方法のいずれかで手動で生成可能です。</p><h3 id=単一ルート認証局>単一ルート認証局</h3><p>管理者によりコントロールされた、単一ルート認証局の作成が可能です。このルート認証局は複数の中間認証局を作る事が可能で、作成はKubernetes自身に委ねます。</p><p>必要な認証局:</p><table><thead><tr><th>パス</th><th>デフォルトCN</th><th>説明</th></tr></thead><tbody><tr><td>ca.crt,key</td><td>kubernetes-ca</td><td>Kubernetes全体の認証局　　　</td></tr><tr><td>etcd/ca.crt,key</td><td>etcd-ca</td><td>etcd用　　　　　　　　　　　　　　</td></tr><tr><td>front-proxy-ca.crt,key</td><td>kubernetes-front-proxy-ca</td><td><a href=/docs/tasks/extend-kubernetes/configure-aggregation-layer/>front-end proxy</a>用　　　</td></tr></tbody></table><p>上記の認証局に加えて、サービスアカウント管理用に公開鍵/秘密鍵のペア(<code>sa.key</code>と<code>sa.pub</code>)を取得する事が必要です。</p><h3 id=全ての証明書>全ての証明書</h3><p>CAの秘密鍵をクラスターにコピーしたくない場合、自身で全ての証明書を作成できます。</p><p>必要な証明書:</p><table><thead><tr><th>デフォルトCN</th><th>親認証局</th><th>組織 　　　　　　</th><th>種類</th><th>ホスト名 (SAN)</th></tr></thead><tbody><tr><td>kube-etcd</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-peer</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd-ca</td><td></td><td>client</td><td></td></tr><tr><td>kube-apiserver-etcd-client</td><td>etcd-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>kube-apiserver</td><td>kubernetes-ca</td><td></td><td>server</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>&lt;advertise_IP></code>, <code>[1]</code></td></tr><tr><td>kube-apiserver-kubelet-client</td><td>kubernetes-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>front-proxy-client</td><td>kubernetes-front-proxy-ca</td><td></td><td>client</td><td></td></tr></tbody></table><p>[1]: クラスターに接続するIPおよびDNS名( <a href=/docs/reference/setup-tools/kubeadm/kubeadm/>kubeadm</a>を使用する場合と同様、ロードバランサーのIPおよびDNS名、<code>kubernetes</code>、<code>kubernetes.default</code>、<code>kubernetes.default.svc</code>、<code>kubernetes.default.svc.cluster</code>、<code>kubernetes.default.svc.cluster.local</code>)</p><p><code>kind</code>は下記の<a href=https://pkg.go.dev/k8s.io/api/certificates/v1beta1#KeyUsage>x509の鍵用途</a>のタイプにマッピングされます:</p><table><thead><tr><th>種類</th><th>鍵の用途 　　　</th></tr></thead><tbody><tr><td>server</td><td>digital signature, key encipherment, server auth</td></tr><tr><td>client</td><td>digital signature, key encipherment, client auth</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>備考:</strong> 上記に挙げられたホスト名(SAN)は、クラスターを動作させるために推奨されるものです。
特別なセットアップが求められる場合、全てのサーバー証明書にSANを追加する事ができます。</div><div class="alert alert-info note callout" role=alert><strong>備考:</strong><p>kubeadm利用者のみ：</p><ul><li>秘密鍵なしでCA証明書をクラスターにコピーするシナリオは、kubeadmドキュメントの外部認証局の項目で言及されています。</li><li>kubeadmでPKIを生成すると、<code>kube-etcd</code>、<code>kube-etcd-peer</code>および <code>kube-etcd-healthcheck-client</code>証明書は外部etcdを利用するケースでは生成されない事に留意してください。</li></ul></div><h3 id=証明書のパス>証明書のパス</h3><p>証明書は推奨パスに配置するべきです(<a href=/docs/reference/setup-tools/kubeadm/kubeadm/>kubeadm</a>を使用する場合と同様)。
パスは場所に関係なく与えられた引数で特定されます。</p><table><thead><tr><th>デフォルトCN</th><th>鍵の推奨パス 　　　　　　</th><th>証明書の推奨パス 　　　　　</th><th>コマンド</th><th>鍵を指定する引数</th><th>証明書を指定する引数</th></tr></thead><tbody><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>kube-apiserver</td><td></td><td>--etcd-cafile</td></tr><tr><td>kube-apiserver-etcd-client</td><td>apiserver-etcd-client.key</td><td>apiserver-etcd-client.crt</td><td>kube-apiserver</td><td>--etcd-keyfile</td><td>--etcd-certfile</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-apiserver</td><td></td><td>--client-ca-file</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-controller-manager</td><td>--cluster-signing-key-file</td><td>--client-ca-file, --root-ca-file, --cluster-signing-cert-file</td></tr><tr><td>kube-apiserver</td><td>apiserver.key</td><td>apiserver.crt</td><td>kube-apiserver</td><td>--tls-private-key-file</td><td>--tls-cert-file</td></tr><tr><td>kube-apiserver-kubelet-client</td><td>apiserver-kubelet-client.key</td><td>apiserver-kubelet-client.crt</td><td>kube-apiserver</td><td>--kubelet-client-key</td><td>--kubelet-client-certificate</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-apiserver</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-controller-manager</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-client</td><td>front-proxy-client.key</td><td>front-proxy-client.crt</td><td>kube-apiserver</td><td>--proxy-client-key-file</td><td>--proxy-client-cert-file</td></tr><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>etcd</td><td></td><td>--trusted-ca-file, --peer-trusted-ca-file</td></tr><tr><td>kube-etcd</td><td>etcd/server.key</td><td>etcd/server.crt</td><td>etcd</td><td>--key-file</td><td>--cert-file</td></tr><tr><td>kube-etcd-peer</td><td>etcd/peer.key</td><td>etcd/peer.crt</td><td>etcd</td><td>--peer-key-file</td><td>--peer-cert-file</td></tr><tr><td>etcd-ca</td><td></td><td>etcd/ca.crt</td><td>etcdctl</td><td></td><td>--cacert</td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd/healthcheck-client.key</td><td>etcd/healthcheck-client.crt</td><td>etcdctl</td><td>--key</td><td>--cert</td></tr></tbody></table><p>サービスアカウント用の鍵ペアについても同様です。</p><table><thead><tr><th>秘密鍵のパス 　　　　</th><th>　公開鍵のパス 　　　</th><th>コマンド</th><th>引数</th></tr></thead><tbody><tr><td>sa.key</td><td></td><td>kube-controller-manager</td><td>service-account-private</td></tr><tr><td></td><td>sa.pub</td><td>kube-apiserver</td><td>service-account-key</td></tr></tbody></table><h2 id=ユーザアカウント用に証明書を設定する>ユーザアカウント用に証明書を設定する</h2><p>管理者アカウントおよびサービスアカウントは手動で設定しなければなりません。</p><table><thead><tr><th>ファイル名</th><th>クレデンシャル名</th><th>デフォルトCN</th><th>組織　　　　　　</th></tr></thead><tbody><tr><td>admin.conf</td><td>default-admin</td><td>kubernetes-admin</td><td>system:masters</td></tr><tr><td>kubelet.conf</td><td>default-auth</td><td>system:node:<code>&lt;nodeName></code> (see note)</td><td>system:nodes</td></tr><tr><td>controller-manager.conf</td><td>default-controller-manager</td><td>system:kube-controller-manager</td><td></td></tr><tr><td>scheduler.conf</td><td>default-scheduler</td><td>system:kube-scheduler</td><td></td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>備考:</strong> <code>kubelet.conf</code>における<code>&lt;nodeName></code>の値は<strong>必ず</strong>APIサーバーに登録されたkubeletのノード名と一致しなければなりません。詳細は、<a href=/docs/reference/access-authn-authz/node/>Node Authorization</a>を参照してください。</div><ol><li><p>各コンフィグ毎に、CN名と組織を指定してx509証明書と鍵ペアを生成してください。</p></li><li><p>以下のように、各コンフィグで<code>kubectl</code>を実行してください。</p></li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-cluster default-cluster --server<span style=color:#666>=</span>https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-credentials &lt;credential-name&gt; --client-key &lt;path-to-key&gt;.pem --client-certificate &lt;path-to-cert&gt;.pem --embed-certs
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config set-context default-system --cluster default-cluster --user &lt;credential-name&gt;
</span></span><span style=display:flex><span><span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>&lt;filename&gt; kubectl config use-context default-system
</span></span></code></pre></div><p>これらのファイルは以下のように利用されます:</p><table><thead><tr><th>ファイル名</th><th>コマンド</th><th>コメント</th></tr></thead><tbody><tr><td>admin.conf</td><td>kubectl</td><td>クラスターの管理者設定用</td></tr><tr><td>kubelet.conf</td><td>kubelet</td><td>クラスターの各ノードに1つ必要です。</td></tr><tr><td>controller-manager.conf</td><td>kube-controller-manager</td><td><code>manifests/kube-controller-manager.yaml</code>のマニフェストファイルに追記する必要があります。</td></tr><tr><td>scheduler.conf</td><td>kube-scheduler</td><td><code>manifests/kube-scheduler.yaml</code>のマニフェストファイルに追記する必要があります。</td></tr></tbody></table></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/ja/docs/home/>ホーム</a>
<a class=text-white href=/ja/blog/>Blogs</a>
<a class=text-white href=/ja/training/>トレーニング</a>
<a class=text-white href=/ja/partners/>パートナー</a>
<a class=text-white href=/ja/community/>コミュニティ</a>
<a class=text-white href=/ja/case-studies/>ケーススタディ</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>