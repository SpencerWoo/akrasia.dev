<!doctype html><html lang=id class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/cluster-administration/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/cluster-administration/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/cluster-administration/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/cluster-administration/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/cluster-administration/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/cluster-administration/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/cluster-administration/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/cluster-administration/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/cluster-administration/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/id/docs/concepts/cluster-administration/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Administrasi Klaster | Kubernetes</title><meta property="og:title" content="Administrasi Klaster"><meta property="og:description" content="Orkestrasi Kontainer dengan Skala Produksi"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/id/docs/concepts/cluster-administration/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Administrasi Klaster"><meta itemprop=description content="Orkestrasi Kontainer dengan Skala Produksi"><meta name=twitter:card content="summary"><meta name=twitter:title content="Administrasi Klaster"><meta name=twitter:description content="Orkestrasi Kontainer dengan Skala Produksi"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/id/docs/concepts/cluster-administration/"><meta property="og:title" content="Administrasi Klaster"><meta name=twitter:title content="Administrasi Klaster"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/id/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/id/docs/>Dokumentasi</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/id/community/>Komunitas</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/id/case-studies/>Studi kasus</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versi</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/id/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/id/docs/concepts/cluster-administration/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/id/docs/concepts/cluster-administration/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/id/docs/concepts/cluster-administration/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/id/docs/concepts/cluster-administration/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/id/docs/concepts/cluster-administration/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Bahasa Indonesia</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/cluster-administration/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/cluster-administration/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/cluster-administration/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/cluster-administration/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/cluster-administration/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/cluster-administration/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/cluster-administration/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/cluster-administration/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/cluster-administration/>Português (Portuguese)</a>
<a class=dropdown-item href=/ru/docs/concepts/cluster-administration/>Русский (Russian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/id/docs/concepts/cluster-administration/>Return to the regular view of this page</a>.</p></div><h1 class=title>Administrasi Klaster</h1><ul><li>1: <a href=#pg-fb494ea3b1874bd753dcd11c3f35c2dc>Ikhtisar Administrasi Klaster</a></li><li>2: <a href=#pg-2bf9a93ab5ba014fb6ff70b22c29d432>Sertifikat</a></li><li>3: <a href=#pg-d0e81230313a2684e7b7e40b21834e30>Penyedia Layanan Cloud</a></li><li>4: <a href=#pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>Mengelola Resource</a></li><li>5: <a href=#pg-d649067a69d8d5c7e71564b42b96909e>Jaringan Kluster</a></li><li>6: <a href=#pg-c4b1e87a84441f8a90699a345ce48d68>Arsitektur Logging</a></li><li>7: <a href=#pg-cbfd3654996eae9fcdef009f70fa83f0>Metrik untuk Komponen Sistem Kubernetes</a></li><li>8: <a href=#pg-2e05a56491965ae320c2662590b2ca18>Konfigurasi Garbage Collection pada kubelet</a></li><li>9: <a href=#pg-3003324f360fdacc06ca144e57ff0e97>Federation</a></li><li>10: <a href=#pg-08e94e6a480e0d6b2de72d84a1b97617>Berbagai Proxy di Kubernetes</a></li><li>11: <a href=#pg-d5cc46b61677b53f61a407d20bdd0830>Metrik controller manager</a></li><li>12: <a href=#pg-85d633ae590aa20ec024f1b7af1d74fc>Instalasi Add-ons</a></li><li>13: <a href=#pg-31c9327d2332c585341b64ddafa19cdd>Prioritas dan Kesetaraan API (API Priority and Fairness)</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-fb494ea3b1874bd753dcd11c3f35c2dc>1 - Ikhtisar Administrasi Klaster</h1><p>Ikhtisar administrasi klaster ini ditujukan untuk siapapun yang akan membuat atau mengelola klaster Kubernetes.
Diharapkan untuk memahami beberapa <a href=/docs/concepts/>konsep</a> dasar Kubernetes sebelumnya.</p><h2 id=perencanaan-klaster>Perencanaan Klaster</h2><p>Lihat panduan di <a href=/docs/setup>Persiapan</a> untuk mempelajari beberapa contoh tentang bagaimana merencanakan, mengatur dan mengonfigurasi klaster Kubernetes. Solusi yang akan dipaparkan di bawah ini disebut <em>distro</em>.</p><p>Sebelum memilih panduan, berikut adalah beberapa hal yang perlu dipertimbangkan:</p><ul><li>Apakah kamu hanya ingin mencoba Kubernetes pada komputermu, atau kamu ingin membuat sebuah klaster dengan <em>high-availability</em>, <em>multi-node</em>? Pilihlah distro yang paling sesuai dengan kebutuhanmu.</li><li><strong>Jika kamu merencanakan klaster dengan <em>high-availability</em></strong>, pelajari bagaimana cara mengonfigurasi <a href=/id/docs/concepts/cluster-administration/federation/>klaster pada <em>multiple zone</em></a>.</li><li>Apakah kamu akan menggunakan <strong>Kubernetes klaster di <em>hosting</em></strong>, seperti <a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a>, atau <strong><em>hosting</em> sendiri klastermu</strong>?</li><li>Apakah klastermu berada pada <strong><em>on-premises</em></strong>, atau <strong>di cloud (IaaS)</strong>? Kubernetes belum mendukung secara langsung klaster hibrid. Sebagai gantinya, kamu dapat membuat beberapa klaster.</li><li><strong>Jika kamu ingin mengonfigurasi Kubernetes <em>on-premises</em></strong>, pertimbangkan <a href=/id/docs/concepts/cluster-administration/networking/>model jaringan</a> yang paling sesuai.</li><li>Apakah kamu ingin menjalankan Kubernetes pada <strong>"bare metal" <em>hardware</em></strong> atau pada <strong><em>virtual machines</em> (VM)</strong>?</li><li>Apakah kamu <strong>hanya ingin mencoba klaster Kubernetes</strong>, atau kamu ingin ikut aktif melakukan <strong>pengembangan kode dari proyek Kubernetes</strong>? Jika jawabannya yang terakhir, pilihlah distro yang aktif dikembangkan. Beberapa distro hanya menggunakan rilis <em>binary</em>, namun menawarkan lebih banyak variasi pilihan.</li><li>Pastikan kamu paham dan terbiasa dengan beberapa <a href=/docs/admin/cluster-components/>komponen</a> yang dibutuhkan untuk menjalankan sebuah klaster.</li></ul><p>Catatan: Tidak semua distro aktif dikelola. Pilihlah distro yang telah diuji dengan versi terkini dari Kubernetes.</p><h2 id=mengelola-klaster>Mengelola Klaster</h2><ul><li><p><a href=/docs/tasks/administer-cluster/cluster-management/>Mengelola klaster</a> akan menjabarkan beberapa topik terkait <em>lifecycle</em> dari klaster: membuat klaster baru, melakukan <em>upgrade</em> pada <em>node master</em> dan <em>worker</em>, melakukan pemeliharaan <em>node</em> (contoh: <em>upgrade</em> kernel), dan melakukan <em>upgrade</em> versi Kubernetes API pada klaster yang sedang berjalan.</p></li><li><p>Pelajari bagaimana cara <a href=/docs/concepts/nodes/node/>mengatur <em>node</em></a>.</p></li><li><p>Pelajari bagaimana cara membuat dan mengatur kuota resource <a href=/id/docs/concepts/policy/resource-quotas/>(<em>resource quota</em>)</a> untuk <em>shared</em> klaster.</p></li></ul><h2 id=mengamankan-klaster>Mengamankan Klaster</h2><ul><li><p><a href=/id/docs/concepts/cluster-administration/certificates/>Sertifikat (<em>certificate</em>)</a> akan menjabarkan langkah-langkah untuk membuat sertifikat menggunakan beberapa <em>tool chains</em>.</p></li><li><p><a href=/id/docs/concepts/containers/container-environment-variables/>Kubernetes <em>Container Environment</em></a> akan menjelaskan <em>environment</em> untuk kontainer yang dikelola oleh Kubelet pada Kubernetes <em>node</em>.</p></li><li><p><a href=/docs/reference/access-authn-authz/controlling-access/>Mengontrol Akses ke Kubernetes API</a> akan menjabarkan bagaimana cara mengatur izin (<em>permission</em>) untuk akun pengguna dan <em>service account</em>.</p></li><li><p><a href=/docs/reference/access-authn-authz/authentication/>Autentikasi</a> akan menjelaskan autentikasi di Kubernetes, termasuk ragam pilihan autentikasi.</p></li><li><p><a href=/docs/reference/access-authn-authz/authorization/>Otorisasi</a> dibedakan dari autentikasi, digunakan untuk mengontrol bagaimana <em>HTTP call</em> ditangani.</p></li><li><p><a href=/docs/reference/access-authn-authz/admission-controllers/>Menggunakan <em>Admission Controllers</em></a> akan menjelaskan <em>plug-in</em> yang akan melakukan intersep permintaan sebelum menuju ke server Kubernetes API, setelah autentikasi dan otorisasi dilakukan.</p></li><li><p><a href=/docs/concepts/cluster-administration/sysctl-cluster/>Menggunakan Sysctls pada Klaster Kubernetes</a> akan menjabarkan tentang cara menggunakan perintah <code>sysctl</code> pada <em>command-line</em> untuk mengatur parameter kernel.</p></li><li><p><a href=/docs/tasks/debug-application-cluster/audit/>Audit</a> akan menjelaskan bagaimana cara berinteraksi dengan log audit Kubernetes.</p></li></ul><h3 id=mengamankan-kubelet>Mengamankan Kubelet</h3><ul><li><a href=/docs/concepts/architecture/master-node-communication/>Komunikasi Master-Node</a></li><li><a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>TLS <em>bootstrapping</em></a></li><li><a href=/docs/admin/kubelet-authentication-authorization/>Autentikasi/Otorisasi Kubelet</a></li></ul><h2 id=layanan-tambahan-klaster>Layanan Tambahan Klaster</h2><ul><li><p><a href=/id/docs/concepts/services-networking/dns-pod-service/>Integrasi DNS</a> akan menjelaskan bagaimana cara <em>resolve</em> suatu nama DNS langsung pada <em>service</em> Kubernetes.</p></li><li><p><a href=/id/docs/concepts/cluster-administration/logging/><em>Logging</em> dan <em>Monitoring</em> Aktivitas Klaster</a> akan menjelaskan bagaimana cara <em>logging</em> bekerja di Kubernetes serta bagaimana cara mengimplementasikannya.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2bf9a93ab5ba014fb6ff70b22c29d432>2 - Sertifikat</h1><p>Saat menggunakan autentikasi sertifikat klien, kamu dapat membuat sertifikat
secara manual melalui <code>easyrsa</code>, <code>openssl</code> atau <code>cfssl</code>.</p><h3 id=easyrsa>easyrsa</h3><p><strong>easyrsa</strong> dapat digunakan untuk menghasilkan sertifikat klaster kamu secara manual.</p><ol><li><p>Unduh, buka paket, dan inisialisasi versi tambal easyrsa3.</p><pre><code> curl -LO https://storage.googleapis.com/kubernetes-release/easy-rsa/easy-rsa.tar.gz
 tar xzf easy-rsa.tar.gz
 cd easy-rsa-master/easyrsa3
 ./easyrsa init-pki
</code></pre></li><li><p>Hasilkan CA. (<code>--batch</code> untuk atur mode otomatis. <code>--req-cn</code> untuk menggunakan <em>default</em> CN.)</p><pre><code> ./easyrsa --batch &quot;--req-cn=${MASTER_IP}@`date +%s`&quot; build-ca nopass
</code></pre></li><li><p>Hasilkan sertifikat dan kunci <em>server</em>.
Argumen <code>--subject-alt-name</code> digunakan untuk mengatur alamat IP dan nama DNS yang dapat diakses
oleh <em>server</em> API. <code>MASTER_CLUSTER_IP</code> biasanya merupakan IP pertama dari CIDR <em>service cluster</em>
yang diset dengan argumen <code>--service-cluster-ip-range</code> untuk <em>server</em> API dan
komponen manajer pengontrol. Argumen <code>--days</code> digunakan untuk mengatur jumlah hari
masa berlaku sertifikat.
Sampel di bawah ini juga mengasumsikan bahwa kamu menggunakan <code>cluster.local</code> sebagai nama
<em>domain</em> DNS <em>default</em>.</p><pre><code> ./easyrsa --subject-alt-name=&quot;IP:${MASTER_IP},&quot;\
 &quot;IP:${MASTER_CLUSTER_IP},&quot;\
 &quot;DNS:kubernetes,&quot;\
 &quot;DNS:kubernetes.default,&quot;\
 &quot;DNS:kubernetes.default.svc,&quot;\
 &quot;DNS:kubernetes.default.svc.cluster,&quot;\
 &quot;DNS:kubernetes.default.svc.cluster.local&quot; \
 --days=10000 \
 build-server-full server nopass
</code></pre></li><li><p>Salin <code>pki/ca.crt</code>, <code>pki/issued/server.crt</code>, dan <code>pki/private/server.key</code> ke direktori kamu.</p></li><li><p>Isi dan tambahkan parameter berikut ke dalam parameter mulai <em>server</em> API:</p><pre><code>--client-ca-file=/yourdirectory/ca.crt
--tls-cert-file=/yourdirectory/server.crt
--tls-private-key-file=/yourdirectory/server.key
</code></pre></li></ol><h3 id=openssl>openssl</h3><p><strong>openssl</strong> secara manual dapat menghasilkan sertifikat untuk klaster kamu.</p><ol><li><p>Hasilkan ca.key dengan 2048bit:</p><pre><code>openssl genrsa -out ca.key 2048
</code></pre></li><li><p>Hasilkan ca.crt berdasarkan ca.key (gunakan -days untuk mengatur waktu efektif sertifikat):</p><pre><code>openssl req -x509 -new -nodes -key ca.key -subj &quot;/CN=${MASTER_IP}&quot; -days 10000 -out ca.crt
</code></pre></li><li><p>Hasilkan server.key dengan 2048bit:</p><pre><code> openssl genrsa -out server.key 2048
</code></pre></li><li><p>Buat <em>file</em> konfigurasi untuk menghasilkan <em>Certificate Signing Request</em> (CSR).
Pastikan untuk mengganti nilai yang ditandai dengan kurung sudut (mis. <code>&lt;MASTER_IP></code>)
dengan nilai sebenarnya sebelum menyimpan ke <em>file</em> (mis. <code>csr.conf</code>).
Perhatikan bahwa nilai <code>MASTER_CLUSTER_IP</code> adalah layanan IP klaster untuk
<em>server</em> API seperti yang dijelaskan dalam subbagian sebelumnya.
Sampel di bawah ini juga mengasumsikan bahwa kamu menggunakan <code>cluster.local</code>
sebagai nama <em>domain</em> DNS <em>default</em>.</p><pre><code> [ req ]
 default_bits = 2048
 prompt = no
 default_md = sha256
 req_extensions = req_ext
 distinguished_name = dn

 [ dn ]
 C = &lt;country&gt;
 ST = &lt;state&gt;
 L = &lt;city&gt;
 O = &lt;organization&gt;
 OU = &lt;organization unit&gt;
 CN = &lt;MASTER_IP&gt;

 [ req_ext ]
 subjectAltName = @alt_names

 [ alt_names ]
 DNS.1 = kubernetes
 DNS.2 = kubernetes.default
 DNS.3 = kubernetes.default.svc
 DNS.4 = kubernetes.default.svc.cluster
 DNS.5 = kubernetes.default.svc.cluster.local
 IP.1 = &lt;MASTER_IP&gt;
 IP.2 = &lt;MASTER_CLUSTER_IP&gt;

 [ v3_ext ]
 authorityKeyIdentifier=keyid,issuer:always
 basicConstraints=CA:FALSE
 keyUsage=keyEncipherment,dataEncipherment
 extendedKeyUsage=serverAuth,clientAuth
 subjectAltName=@alt_names
</code></pre></li><li><p>Hasilkan permintaan penandatanganan sertifikat berdasarkan <em>file</em> konfigurasi:</p><pre><code> openssl req -new -key server.key -out server.csr -config csr.conf
</code></pre></li><li><p>Hasilkan sertifikat <em>server</em> menggunakan ca.key, ca.crt dan server.csr:</p><pre><code> openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key \
 -CAcreateserial -out server.crt -days 10000 \
 -extensions v3_ext -extfile csr.conf
</code></pre></li><li><p>Lihat sertifikat:</p><pre><code> openssl x509  -noout -text -in ./server.crt
</code></pre></li></ol><p>Terakhir, tambahkan parameter yang sama ke dalam parameter mulai <em>server</em> API.</p><h3 id=cfssl>cfssl</h3><p><strong>cfssl</strong> adalah alat lain untuk pembuatan sertifikat.</p><ol><li><p>Unduh, buka paket dan siapkan <em>command line tools</em> seperti yang ditunjukkan di bawah ini.
Perhatikan bahwa kamu mungkin perlu menyesuaikan contoh perintah berdasarkan arsitektur
perangkat keras dan versi cfssl yang kamu gunakan.</p><pre><code>curl -L https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -o cfssl
chmod +x cfssl
curl -L https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -o cfssljson
chmod +x cfssljson
curl -L https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -o cfssl-certinfo
chmod +x cfssl-certinfo
</code></pre></li><li><p>Buat direktori untuk menyimpan <em>artifacts</em> dan inisialisasi cfssl:</p><pre><code>mkdir cert
cd cert
../cfssl print-defaults config &gt; config.json
../cfssl print-defaults csr &gt; csr.json
</code></pre></li><li><p>Buat <em>file</em> konfigurasi JSON untuk menghasilkan <em>file</em> CA, misalnya, <code>ca-config.json</code>:</p><pre><code>{
  &quot;signing&quot;: {
    &quot;default&quot;: {
      &quot;expiry&quot;: &quot;8760h&quot;
    },
    &quot;profiles&quot;: {
      &quot;kubernetes&quot;: {
        &quot;usages&quot;: [
          &quot;signing&quot;,
          &quot;key encipherment&quot;,
          &quot;server auth&quot;,
          &quot;client auth&quot;
        ],
        &quot;expiry&quot;: &quot;8760h&quot;
      }
    }
  }
}
</code></pre></li><li><p>Buat <em>file</em> konfigurasi JSON untuk CA <em>certificate signing request</em> (CSR), misalnya,
<code>ca-csr.json</code>. Pastikan untuk mengganti nilai yang ditandai dengan kurung sudut
dengan nilai sebenarnya yang ingin kamu gunakan.</p><pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;:[{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre></li><li><p>Hasilkan kunci CA (<code>ca-key.pem</code>) dan sertifikat (<code>ca.pem</code>):</p><pre><code>../cfssl gencert -initca ca-csr.json | ../cfssljson -bare ca
</code></pre></li><li><p>Buat <em>file</em> konfigurasi JSON untuk menghasilkan kunci dan sertifikat untuk API
<em>server</em>, misalnya, <code>server-csr.json</code>. Pastikan untuk mengganti nilai dalam kurung sudut
dengan nilai sebenarnya yang ingin kamu gunakan. <code>MASTER_CLUSTER_IP</code> adalah layanan
klaster IP untuk <em>server</em> API seperti yang dijelaskan dalam subbagian sebelumnya.
Sampel di bawah ini juga mengasumsikan bahwa kamu menggunakan <code>cluster.local</code> sebagai
nama <em>domain</em> DNS <em>default</em>.</p><pre><code>{
  &quot;CN&quot;: &quot;kubernetes&quot;,
  &quot;hosts&quot;: [
    &quot;127.0.0.1&quot;,
    &quot;&lt;MASTER_IP&gt;&quot;,
    &quot;&lt;MASTER_CLUSTER_IP&gt;&quot;,
    &quot;kubernetes&quot;,
    &quot;kubernetes.default&quot;,
    &quot;kubernetes.default.svc&quot;,
    &quot;kubernetes.default.svc.cluster&quot;,
    &quot;kubernetes.default.svc.cluster.local&quot;
  ],
  &quot;key&quot;: {
    &quot;algo&quot;: &quot;rsa&quot;,
    &quot;size&quot;: 2048
  },
  &quot;names&quot;: [{
    &quot;C&quot;: &quot;&lt;country&gt;&quot;,
    &quot;ST&quot;: &quot;&lt;state&gt;&quot;,
    &quot;L&quot;: &quot;&lt;city&gt;&quot;,
    &quot;O&quot;: &quot;&lt;organization&gt;&quot;,
    &quot;OU&quot;: &quot;&lt;organization unit&gt;&quot;
  }]
}
</code></pre></li><li><p>Buat kunci dan sertifikat untuk server API, yang mana awalnya di
simpan masing-masing ke dalam <em>file</em> <code>server-key.pem</code> dan <code>server.pem</code>:</p><pre><code>../cfssl gencert -ca=ca.pem -ca-key=ca-key.pem \
--config=ca-config.json -profile=kubernetes \
server-csr.json | ../cfssljson -bare server
</code></pre></li></ol><h2 id=distribusi-sertifikat-self-signed-ca>Distribusi Sertifikat <em>Self-Signed</em> CA</h2><p><em>Node</em> klien dapat menolak untuk mengakui sertifikat CA yang ditandatangani sendiri sebagai valid.
Untuk <em>deployment</em> non-produksi, atau untuk <em>deployment</em> yang berjalan di belakang <em>firewall</em> perusahaan,
kamu dapat mendistribusikan sertifikat CA yang ditandatangani sendiri untuk semua klien dan <em>refresh</em>
daftar lokal untuk sertifikat yang valid.</p><p>Pada setiap klien, lakukan operasi berikut:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo cp ca.crt /usr/local/share/ca-certificates/kubernetes.crt
</span></span><span style=display:flex><span>sudo update-ca-certificates
</span></span></code></pre></div><pre tabindex=0><code>Updating certificates in /etc/ssl/certs...
1 added, 0 removed; done.
Running hooks in /etc/ca-certificates/update.d....
done.
</code></pre><h2 id=sertifikat-api>Sertifikat API</h2><p>Kamu dapat menggunakan API <code>Certificate.k8s.io</code> untuk menyediakan
sertifikat x509 yang digunakan untuk autentikasi seperti yang didokumentasikan
<a href=/id/docs/tasks/tls/managing-tls-in-a-cluster>di sini</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d0e81230313a2684e7b7e40b21834e30>3 - Penyedia Layanan Cloud</h1><p>Laman ini akan menjelaskan bagaimana cara mengelola Kubernetes yang berjalan pada penyedia layanan cloud tertentu.</p><h3 id=kubeadm>Kubeadm</h3><p><a href=/docs/reference/setup-tools/kubeadm/>Kubeadm</a> merupakan salah satu cara yang banyak digunakan untuk membuat klaster Kubernetes.
Kubeadm memiliki beragam opsi untuk mengatur konfigurasi spesifik untuk penyedia layanan cloud. Salah satu contoh yang biasa digunakan pada penyedia cloud <em>in-tree</em> yang dapat diatur dengan kubeadm adalah sebagai berikut:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-provider</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;openstack&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-config</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.13.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-provider</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;openstack&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-config</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraVolumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-provider</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;openstack&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cloud-config</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraVolumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/kubernetes/cloud.conf&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Penyedia layanan cloud <em>in-tree</em> biasanya membutuhkan <code>--cloud-provider</code> dan <code>--cloud-config</code> yang ditentukan sebelumnya pada <em>command lines</em> untuk <a href=/docs/admin/kube-apiserver/>kube-apiserver</a>, <a href=/docs/admin/kube-controller-manager/>kube-controller-manager</a> dan
<a href=/docs/admin/kubelet/>kubelet</a>. Konten dari <em>file</em> yang ditentukan pada <code>--cloud-config</code> untuk setiap provider akan dijabarkan di bawah ini.</p><p>Untuk semua penyedia layanan cloud eksternal, silakan ikuti instruksi pada repositori masing-masing penyedia layanan.</p><h2 id=aws>AWS</h2><p>Bagian ini akan menjelaskan semua konfigurasi yang dapat diatur saat menjalankan Kubernetes pada Amazon Web Services.</p><h3 id=nama-node>Nama Node</h3><p>Penyedia layanan cloud AWS menggunakan nama DNS privat dari <em>instance</em> AWS sebagai nama dari objek Kubernetes Node.</p><h3 id=load-balancer><em>Load Balancer</em></h3><p>Kamu dapat mengatur <a href=/id/docs/tasks/access-application-cluster/create-external-load-balancer/>load balancers eksternal</a> sehingga dapat menggunakan fitur khusus AWS dengan mengatur anotasi seperti di bawah ini.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>run</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-ssl-cert</span>:<span style=color:#bbb> </span>arn:aws:acm:xx-xxxx-x:xxxxxxxxx:xxxxxxx/xxxxx-xxxx-xxxx-xxxx-xxxxxxxxx<span style=color:#bbb> </span><span style=color:#080;font-style:italic>#ganti nilai ini</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>5556</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span></span></span></code></pre></div><p>Pengaturan lainnya juga dapat diaplikasikan pada layanan <em>load balancer</em> di AWS dengan menggunakan anotasi-anotasi. Berikut ini akan dijelaskan anotasi yang didukung oleh AWS ELB:</p><ul><li><code>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</code>: Digunakan untuk menentukan interval pengeluaran log akses.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</code>: Digunakan untuk mengaktifkan atau menonaktifkan log akses.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</code>: Digunakan untuk menentukan nama <em>bucket</em> S3 log akses.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</code>: Digunakan untuk menentukan prefix <em>bucket</em> S3 log akses.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags</code>: Digunakan untuk menentukan daftar tag tambahan pada ELB dengan menggunakan parameter <em>key-value</em>. Contoh: <code>"Key1=Val1,Key2=Val2,KeyNoVal1=,KeyNoVal2"</code>.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</code>: Digunakan untuk menentukan protokol yang digunakan oleh <em>backend</em> (pod) di belakang <em>listener</em>. Jika diset ke <code>http</code> (default) atau <code>https</code>, maka akan dibuat HTTPS <em>listener</em> yang akan mengakhiri koneksi dan meneruskan <em>header</em>. Jika diset ke <code>ssl</code> atau <code>tcp</code>, maka akan digunakan "raw" SSL <em>listener</em>. Jika diset ke <code>http</code> dan <code>aws-load-balancer-ssl-cert</code> tidak digunakan, maka akan digunakan HTTP <em>listener</em>.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-ssl-cert</code>: Digunakan untuk meminta <em>secure</em> <em>listener</em>. Nilai yang dimasukkan adalah sertifikat ARN yang valid. Info lebih lanjut lihat <a href=http://docs.aws.amazon.com/ElasticLoadBalancing/latest/DeveloperGuide/elb-listener-config.html>ELB Listener Config</a> CertARN merupakan IAM atau CM certificate ARN, contoh: <code>arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012</code>.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</code>: Digunakan untuk mengaktifkan atau menonaktfkan <em>connection draining</em>.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</code>: Digunakan untuk menentukan <em>connection draining timeout</em>.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout</code>: Digunakan untuk menentukan <em>idle connection timeout</em>.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled</code>: Digunakan untuk mengaktifkan atau menonaktifkan <em>cross-zone load balancing</em>.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-extra-security-groups</code>: Digunakan untuk menentukan grup keamanan yang akan ditambahkan pada ELB yang dibuat.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-internal</code>: Digunakan sebagai indikasi untuk menggunakan internal ELB.</li><li><code>service.beta.kubernetes.io/aws-load-balancer-proxy-protocol</code>: Digunakan untuk mengaktifkan <em>proxy protocol</em> pada ELB. Saat ini hanya dapat menerima nilai <code>*</code> yang berarti mengaktifkan <em>proxy protocol</em> pada semua ELB <em>backends</em>. Di masa mendatang kamu juga dapat mengatur agar <em>proxy protocol</em> hanya aktif pada <em>backends</em> tertentu..</li><li><code>service.beta.kubernetes.io/aws-load-balancer-ssl-ports</code>: Digunakan untuk menentukan daftar port--yang dipisahkan koma-- yang akan menggunakan SSL/HTTPS <em>listeners</em>. Nilai <em>default</em> yaitu <code>*</code> (semua).</li></ul><p>Informasi anotasi untuk AWS di atas diperoleh dari komentar pada <a href=https://github.com/kubernetes/kubernetes/blob/master/pkg/cloudprovider/providers/aws/aws.go>aws.go</a></p><h2 id=azure>Azure</h2><h3 id=nama-node-1>Nama Node</h3><p>Penyedia layanan cloud Azure menggunakan <em>hostname</em> dari <em>node</em> (yang ditentukan oleh kubelet atau menggunakan <code>--hostname-override</code>) sebagai nama dari objek Kubernetes Node.
Perlu diperhatikan bahwa nama Kubernetes Node harus sesuai dengan nama Azure VM.</p><h2 id=cloudstack>CloudStack</h2><h3 id=nama-node-2>Nama Node</h3><p>Penyedia layanan cloud CloudStack menggunakan <em>hostname</em> dari <em>node</em> (yang ditentukan kubelet atau menggunakan <code>--hostname-override</code>) sebagai nama dari objek Kubernetes Node.
Perlu diperhatikan bahwa nama Kubernetes Node harus sesuai dengan nama Cloudstack VM.</p><h2 id=gce>GCE</h2><h3 id=nama-node-3>Nama Node</h3><p>Penyedia layanan cloud GCE menggunakan <em>hostname</em> dari <em>node</em> (yang ditentukan kubelet atau menggunakan <code>--hostname-override</code>) sebagai nama dari objek Kubernetes Node.
Perlu diperhatikan bahwa segmen pertama dari nama Kubernetes Node harus sesuai dengan nama <em>instance</em> GCE (contoh: sebuah <em>node</em> dengan nama <code>kubernetes-node-2.c.my-proj.internal</code> harus sesuai dengan <em>instance</em> yang memiliki nama <code>kubernetes-node-2</code>).</p><h2 id=openstack>OpenStack</h2><p>Bagian ini akan menjelaskan semua konfigurasi yang dapat diatur saat menggunakan OpenStack dengan Kubernetes.</p><h3 id=nama-node-4>Nama Node</h3><p>Penyedia layanan cloud OpenStack menggunakan nama <em>instance</em> (yang diperoleh dari metadata OpenStack) sebagai nama objek Kubernetes Node.
Perlu diperhatikan bahwa nama <em>instance</em> harus berupa nama Kubernetes Node yang valid agar kubelet dapat mendaftarkan objek Node-nya.</p><h3 id=layanan>Layanan</h3><p>Penyedia layanan cloud OpenStack menggunakan beragam layanan OpenStack yang tersedia sebagai <em>underlying cloud</em> agar dapat mendukung Kubernetes:</p><table><thead><tr><th>Layanan</th><th>Versi API</th><th>Wajib</th></tr></thead><tbody><tr><td>Block Storage (Cinder)</td><td>V1†, V2, V3</td><td>Tidak</td></tr><tr><td>Compute (Nova)</td><td>V2</td><td>Tidak</td></tr><tr><td>Identity (Keystone)</td><td>V2‡, V3</td><td>Ya</td></tr><tr><td>Load Balancing (Neutron)</td><td>V1§, V2</td><td>Tidak</td></tr><tr><td>Load Balancing (Octavia)</td><td>V2</td><td>Tidak</td></tr></tbody></table><p>† Block Storage V1 API tidak lagi didukung, dukungan Block Storage V3 API telah
ditambahkan pada Kubernetes 1.9.</p><p>‡ Identity V2 API tidak lagi didukung dan akan dihapus oleh penyedia layanan
pada rilis mendatang. Pada rilis "Queens", OpenStack tidak lagi mengekspos
Identity V2 API.</p><p>§ Dukungan Load Balancing V1 API telah dihapus pada Kubernetes 1.9.</p><p><em>Service discovery</em> dilakukan dengan menggunakan katalog layanan/servis (<em>service catalog</em>) yang diatur oleh
OpenStack Identity (Keystone) menggunakan <code>auth-url</code> yang ditentukan pada konfigurasi
penyedia layanan. Penyedia layanan akan menurunkan fungsionalitas secara perlahan saat layanan OpenStack selain Keystone tidak tersedia dan akan menolak dukungan fitur yang terdampak. Beberapa fitur tertentu dapat diaktifkan atau dinonaktfikan tergantung dari ekstensi yang diekspos oleh Neutron pada <em>underlying cloud</em>.</p><h3 id=cloud-conf>cloud.conf</h3><p>Kubernetes berinteraksi dengan OpenStack melalui <em>file</em> cloud.conf. <em>File</em> ini akan menyuplai Kubernetes dengan kredensial dan lokasi dari Openstack <em>auth endpoint</em>.
Kamu dapat membuat <em>file</em> cloud.conf dengan menambahkan rincian berikut ini di dalam <em>file</em>:</p><h4 id=konfigurasi-pada-umumnya>Konfigurasi pada umumnya</h4><p>Berikut ini merupakan contoh dan konfigurasi yang biasa digunakan dan akan mencakup semua pilihan yang paling sering dibutuhkan. <em>File</em> ini akan merujuk pada <em>endpoint</em> dari Keystone OpenStack, serta menyediakan rincian bagaimana cara mengautentikasi dengannya, termasuk cara mengatur <em>load balancer</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[Global]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>username=user<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>password=pass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>auth-url=https://&lt;keystone_ip&gt;/identity/v3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>tenant-id=c869168a828847f39f7f06edd7305637<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>domain-id=2a73b8f597c04551a0fdc8e95544be8a<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[LoadBalancer]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>subnet-id=6937f8fa-858d-4bc9-a3a5-18d2c957166a<span style=color:#bbb>
</span></span></span></code></pre></div><h5 id=global>Global</h5><p>Konfigurasi untuk penyedia layanan OpenStack berikut ini akan membahas bagian konfigurasi global sehingga harus berada pada bagian <code>[Global]</code> dari <em>file</em> <code>cloud.conf</code>:</p><ul><li><code>auth-url</code> (Wajib): URL dari API keystone digunakan untuk autentikasi. ULR ini dapat ditemukan pada bagian Access dan Security > API Access > Credentials di laman panel kontrol OpenStack.</li><li><code>username</code> (Wajib): Merujuk pada username yang dikelola keystone.</li><li><code>password</code> (Wajib): Merujuk pada kata sandi yang dikelola keystone.</li><li><code>tenant-id</code> (Wajib): Digunakan untuk menentukan id dari <em>project</em> tempat kamu membuat <em>resources</em>.</li><li><code>tenant-name</code> (Opsional): Digunakan untuk menentukan nama dari <em>project</em> tempat kamu ingin membuat <em>resources</em>.</li><li><code>trust-id</code> (Opsional): Digunakan untuk menentukan <em>identifier of the trust</em> untuk digunakan
sebagai otorisasi. Suatu <em>trust</em> merepresentasikan otorisasi dari suatu pengguna (<em>the trustor</em>) untuk didelegasikan
pada pengguna lain (<em>the trustee</em>), dan dapat digunakan oleh <em>trustee</em>
berperan sebagai <em>the trustor</em>. <em>Trust</em> yang tersedia dapat ditemukan pada <em>endpoint</em>
<code>/v3/OS-TRUST/trusts</code> dari Keystone API.</li><li><code>domain-id</code> (Opsional): Digunakan untuk menentukan id dari domain tempat <em>user</em> kamu berada.</li><li><code>domain-name</code> (Opsional): Digunakan untuk menentukan nama dari domain tempat <em>user</em> kamu berada.</li><li><code>region</code> (Opsional): Digunakan untuk menentukan <em>identifier</em> dari region saat digunakan pada
multi-region OpenStack cloud. Sebuah region merupakan pembagian secara umum dari <em>deployment</em> OpenStack. Meskipun region tidak wajib berkorelasi secara geografis, suatu <em>deployment</em> dapat menggunakan nama geografis sebagai region <em>identifier</em> seperti
<code>us-east</code>. Daftar region yang tersedia dapat ditemukan pada <em>endpoint</em> <code>/v3/regions</code>
dari Keystone API.</li><li><code>ca-file</code> (Optional): Digunakan untuk menentukan path dari <em>file</em> <em>custom</em> CA.</li></ul><p>Saat menggunakan Keystone V3 - yang mengganti istilah <em>tenant</em> menjadi <em>project</em> - nilai <code>tenant-id</code>
akan secara otomatis dipetakan pada <em>project</em> yang sesuai di API.</p><h5 id=load-balancer-1><em>Load Balancer</em></h5><p>Konfigurasi berikut ini digunakan untuk mengatur <em>load
balancer</em> dan harus berada pada bagian <code>[LoadBalancer]</code> dari <em>file</em> <code>cloud.conf</code>:</p><ul><li><code>lb-version</code> (Opsional): Digunakan untuk menonaktifkan pendeteksian versi otomatis. Nilai
yang valid yaitu <code>v1</code> atau <code>v2</code>. Jika tidak ditentukan, maka pendeteksian otomatis akan
memilih versi tertinggi yang didukung dari <em>underlying</em> OpenStack
cloud.</li><li><code>use-octavia</code> (Opsional): Digunakan untuk menentukan apakah akan menggunakan <em>endpoint</em> dari layanan Octavia LBaaS. Nilai yang valid yaitu <code>true</code> atau <code>false</code>. Jika diset nilai <code>true</code> namun Octavia LBaaS V2 tidak dapat ditemukan, maka <em>load balancer</em> akan kembali menggunakan <em>endpoint</em> dari Neutron LBaaS V2. Nilai <em>default</em> adalah <code>false</code>.</li><li><code>subnet-id</code> (Opsional): Digunakan untuk menentukan id dari subnet yang ingin kamu
buat <em>load balancer</em> di dalamnya. Nilai id ini dapat dilihat pada Network > Networks. Klik pada
jaringan yang sesuai untuk melihat subnet di dalamnya.</li><li><code>floating-network-id</code> (Opsional): Jika diset, maka akan membuat <em>floating</em> IP
untuk <em>load balancer</em>.</li><li><code>lb-method</code> (Opsional): Digunakan untuk menentukan algoritma pendistribusian
yang akan digunakan. Nilai yang valid yaitu
<code>ROUND_ROBIN</code>, <code>LEAST_CONNECTIONS</code>, atau <code>SOURCE_IP</code>. Jika tidak diset, maka akan
menggunakan algoritma <em>default</em> yaitu <code>ROUND_ROBIN</code>.</li><li><code>lb-provider</code> (Opsional): Digunakan untuk menentukan penyedia dari <em>load balancer</em>.
Jika tidak ditentukan, maka akan menggunakan penyedia <em>default</em> yang ditentukan pada Neutron.</li><li><code>create-monitor</code> (Opsional): Digunakan untuk menentukan apakah akan membuat atau tidak monitor kesehatan
untuk Neutron <em>load balancer</em>. Nilai yang valid yaitu <code>true</code> dan <code>false</code>.
Nilai <em>default</em> adalah <code>false</code>. Jika diset nilai <code>true</code> maka <code>monitor-delay</code>,
<code>monitor-timeout</code>, dan <code>monitor-max-retries</code> juga harus diset.</li><li><code>monitor-delay</code> (Opsional): Waktu antara pengiriman <em>probes</em> ke
anggota dari <em>load balancer</em>. Mohon pastikan kamu memasukkan waktu yang valid. Nilai waktu yang valid yaitu "ns", "us" (atau "µs"), "ms", "s", "m", "h"</li><li><code>monitor-timeout</code> (Opsional): Waktu maksimum dari monitor untuk menunggu
balasan ping sebelum <em>timeout</em>. Nilai ini harus lebih kecil dari nilai <em>delay</em>.
Mohon pastikan kamu memasukkan waktu yang valid. Nilai waktu yang valid yaitu "ns", "us" (atau "µs"), "ms", "s", "m", "h"</li><li><code>monitor-max-retries</code> (Opsional): Jumlah gagal ping yang diizinkan sebelum
mengubah status anggota <em>load balancer</em> menjadi INACTIVE. Harus berupa angka
antara 1 dan 10.</li><li><code>manage-security-groups</code> (Opsional): Digunakan untuk menentukan apakah <em>load balancer</em>
akan mengelola aturan grup keamanan sendiri atau tidak. Nilai yang valid
adalah <code>true</code> dan <code>false</code>. Nilai <em>default</em> adalah <code>false</code>. Saat diset ke <code>true</code> maka
nilai <code>node-security-group</code> juga harus ditentukan.</li><li><code>node-security-group</code> (Opsional): ID dari grup keamanan yang akan dikelola.</li></ul><h5 id=block-storage><em>Block Storage</em></h5><p>Konfigurasi untuk penyedia layanan OpenStack berikut ini digunakan untuk mengatur penyimpanan blok atau <em>block storage</em>
dan harus berada pada bagian <code>[BlockStorage]</code> dari <em>file</em> <code>cloud.conf</code>:</p><ul><li><code>bs-version</code> (Opsional): Digunakan untuk menonaktifkan fitur deteksi versi otomatis. Nilai
yang valid yaitu <code>v1</code>, <code>v2</code>, <code>v3</code> dan <code>auto</code>. Jika diset ke <code>auto</code> maka pendeteksian versi
otomatis akan memilih versi tertinggi yang didukung oleh <em>underlying</em>
OpenStack cloud. Nilai <em>default</em> jika tidak diset adalah <code>auto</code>.</li><li><code>trust-device-path</code> (Opsional): Pada umumnya nama <em>block device</em> yang ditentukan
oleh Cinder (contoh: <code>/dev/vda</code>) tidak dapat diandalkan. Opsi ini dapat mengatur hal
tersebut. Jika diset ke <code>true</code> maka akan menggunakan nama <em>block device</em> yang ditentukan
oleh Cinder. Nilai <em>default</em> adalah <code>false</code> yang berarti <em>path</em> dari <em>device</em> akan ditentukan
oleh nomor serialnya serta pemetaan dari <code>/dev/disk/by-id</code>, dan ini merupakan
cara yang direkomendasikan.</li><li><code>ignore-volume-az</code> (Opsional): Digunakan untuk mengatur penggunaan <em>availability zone</em> saat
menautkan volumes Cinder. Jika Nova dan Cinder memiliki <em>availability
zones</em> yang berbeda, opsi ini harus diset <code>true</code>. Skenario seperti ini yang umumnya terjadi, yaitu
saat terdapat banyak Nova <em>availability zones</em> namun hanya ada satu Cinder <em>availability zone</em>.
Nilai <em>default</em> yaitu <code>false</code> digunakan untuk mendukung penggunaan pada rilis terdahulu,
tetapi nilai ini dapat berubah pada rilis mendatang.</li></ul><p>Jika menjalankan Kubernetes dengan versi &lt;= 1.8 pada OpenStack yang menggunakan <em>paths</em> alih-alih
menggunakan port untuk membedakan antara <em>endpoints</em>, maka mungkin dibutuhkan untuk
secara eksplisit mengatur parameter <code>bs-version</code>. Contoh <em>endpoint</em> yang berdasarkan <em>path</em> yaitu
<code>http://foo.bar/volume</code> sedangkan endpoint yang berdasarkan port memiliki bentuk seperti ini
<code>http://foo.bar:xxx</code>.</p><p>Pada lingkungan yang menggunakan <em>endpoint</em> berdasarkan <em>path</em> dan Kubernetes menggunakan logika deteksi-otomatis yang lama, maka <em>error</em> <code>BS API version autodetection failed.</code> akan muncul saat mencoba
melepaskan volume. Untuk mengatasi isu ini, dimungkinkan
untuk memaksa penggunaan Cinder API versi 2 dengan menambahkan baris berikut ini pada konfigurasi penyedia cloud:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[BlockStorage]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>bs-version=v2<span style=color:#bbb>
</span></span></span></code></pre></div><h5 id=metadata>Metadata</h5><p>Konfigurasi untuk OpenStack berikut ini digunakan untuk mengatur metadata dan
harus berada pada bagian <code>[Metadata]</code> dari <em>file</em> <code>cloud.conf</code>:</p><ul><li><p><code>search-order</code> (Opsional): Konfigurasi berikut ini digunakan untuk mengatur bagaimana
cara provider mengambil metadata terkait dengan <em>instance</em> yang dijalankannya. Nilai
<em>default</em> yaitu <code>configDrive,metadataService</code> yang berarti provider akan mengambil
metadata terkait <em>instance</em> dari <em>config drive</em> terlebih dahulu jika tersedia, namun
jika tidak maka akan menggunakan layanan metadata. Nilai alternatif lainnya yaitu:</p><ul><li><code>configDrive</code> - Hanya mengambil metadata <em>instance</em> dari <em>config
drive</em>.</li><li><code>metadataService</code> - Hanya mengambil data <em>instance</em> dari layanan
metadata.</li><li><code>metadataService,configDrive</code> - Mengambil metadata <em>instance</em> dari layanan metadata terlebih
dahulu jika tersedia, jika tidak maka akan mengambil dari <em>config drive</em>.</li></ul><p>Pengaturan ini memang sebaiknya dilakukan sebab metadata pada <em>config drive</em> bisa saja lambat laun akan kedaluwarsa, sedangkan layanan metadata akan selalu menyediakan metadata yang paling mutakhir. Tidak semua penyedia layanan cloud OpenStack menyediakan kedua layanan <em>config drive</em> dan layanan metadata dan mungkin hanya salah satu saja
yang tersedia. Oleh sebab itu nilai <em>default</em> diatur agar dapat memeriksa keduanya.</p></li></ul><h5 id=router>Router</h5><p>Konfigurasi untuk Openstack berikut ini digunakan untuk mengatur <em>plugin</em> jaringan Kubernetes <a href=/docs/concepts/cluster-administration/network-plugins/#kubenet>kubenet</a>
dan harus berada pada bagian <code>[Router]</code> dari <em>file</em> <code>cloud.conf</code>:</p><ul><li><code>router-id</code> (Opsional): Jika Neutron pada <em>underlying cloud</em> mendukung ekstensi
<code>extraroutes</code> maka gunakan <code>router-id</code> untuk menentukan router mana yang akan ditambahkan rute di dalamnya.
Router yang dipilih harus menjangkau jaringan privat tempat <em>node</em> klaster berada
(biasanya hanya ada satu jaringan <em>node</em>, dan nilai ini harus nilai dari <em>default</em> router
pada jaringan <em>node</em>). Nilai ini dibutuhkan untuk dapat menggunakan <a href=/docs/concepts/cluster-administration/network-plugins/#kubenet>kubenet</a> pada OpenStack.</li></ul><h2 id=ovirt>OVirt</h2><h3 id=nama-node-5>Nama Node</h3><p>Penyedia layanan cloud OVirt menggunakan <em>hostname</em> dari <em>node</em> (yang ditentukan kubelet atau menggunakan <code>--hostname-override</code>) sebagai nama dari objek Kubernetes Node.
Perlu diperhatikan bahwa nama Kubernetes Node harus sesuai dengan VM FQDN (yang ditampilkan oleh OVirt di bawah <code>&lt;vm>&lt;guest_info>&lt;fqdn>...&lt;/fqdn>&lt;/guest_info>&lt;/vm></code>)</p><h2 id=photon>Photon</h2><h3 id=nama-node-6>Nama Node</h3><p>Penyedia layanan cloud Photon menggunakan <em>hostname</em> dari <em>node</em> (yang ditentukan kubelet atau menggunakan <code>--hostname-override</code>) sebagai nama dari objek Kubernetes Node.
Perlu diperhatikan bahwa nama Kubernetes Node name harus sesuai dengan nama Photon VM (atau jika <code>overrideIP</code> diset ke true pada <code>--cloud-config</code>, nama Kubernetes Node harus sesuai dengan alamat IP Photon VM).</p><h2 id=vsphere>VSphere</h2><h3 id=nama-node-7>Nama Node</h3><p>Penyedia layanan cloud VSphere menggunakan <em>hostname</em> yang terdeteksi dari <em>node</em> (yang ditentukan oleh kubelet) sebagai nama dari objek Kubernetes Node.</p><p>Parameter <code>--hostname-override</code> diabaikan oleh penyedia layanan cloud VSphere.</p><h2 id=ibm-cloud-kubernetes-service>IBM Cloud Kubernetes Service</h2><h3 id=node-komputasi>Node Komputasi</h3><p>Saat menggunakan layanan IBM Cloud Kubernetes Service, kamu dapat membuat klaster yang terdiri dari campuran antara mesin virtual dan fisik (<em>bare metal</em>) sebagai <em>node</em> di <em>single zone</em> atau <em>multiple zones</em> pada satu region. Untuk informasi lebih lanjut, lihat <a href="https://cloud.ibm.com/docs/containers?topic=containers-plan_clusters#plan_clusters">Perencanaan klaster dan pengaturan worker node</a>.</p><p>Nama dari objek Kubernetes Node yaitu alamat IP privat dari IBM Cloud Kubernetes Service <em>worker node instance</em>.</p><h3 id=jaringan>Jaringan</h3><p>Penyedia layanan IBM Cloud Kubernetes Service menyediakan VLAN untuk membuat jaringan node yang terisolasi dengan kinerja tinggi. Kamu juga dapat membuat <em>custom firewall</em> dan <em>policy</em> jaringan Calico untuk menambah lapisan perlindungan ekstra bagi klaster kamu, atau hubungkan klaster kamu dengan <em>on-prem</em> data center via VPN. Untuk informasi lebih lanjut, lihat <a href="https://cloud.ibm.com/docs/containers?topic=containers-cs_network_cluster#cs_network_cluster">Perencanaan jaringan privat dan in-cluster</a>.</p><p>Untuk membuka aplikasi ke publik atau di dalam klaster, kamu dapat menggunakan NodePort, LoadBalancer, atau Ingress. Kamu juga dapat menyesuaikan aplikasi <em>load balancer</em> Ingress dengan anotasi. Untuk informasi lebih lanjut, lihat <a href="https://cloud.ibm.com/docs/containers?topic=containers-cs_network_planning#cs_network_planning">Perencanaan untuk membuka aplikasi dengan jaringan eksternal</a>.</p><h3 id=penyimpanan>Penyimpanan</h3><p>Penyedia layanan IBM Cloud Kubernetes Service memanfaatkan Kubernetes-native <em>persistent volumes</em> agar pengguna dapat melakukan <em>mount</em> <em>file</em>, block, dan penyimpanan objek cloud ke aplikasi mereka. Kamu juga dapat menggunakan <em>database-as-a-service</em> dan <em>add-ons</em> pihak ketiga sebagai penyimpanan <em>persistent</em> untuk data kamu. Untuk informasi lebih lanjut, lihat <a href="https://cloud.ibm.com/docs/containers?topic=containers-storage_planning#storage_planning">Perencanaan penyimpanan persistent yang selalu tersedia (<em>highly available</em>)</a>.</p><h2 id=baidu-cloud-container-engine>Baidu Cloud Container Engine</h2><h3 id=nama-node-8>Nama Node</h3><p>Penyedia layanan cloud Baidu menggunakan alamat IP privat dari <em>node</em> (yang ditentukan oleh kubelet atau menggunakan <code>--hostname-override</code>) sebagai nama dari objek Kubernetes Node.
Perlu diperhatikan bahwa nama Kubernetes Node harus sesuai dengan alamat IP privat dari Baidu VM.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>4 - Mengelola Resource</h1><p>Kamu telah melakukan <em>deploy</em> pada aplikasimu dan mengeksposnya melalui sebuah <em>service</em>. Lalu? Kubernetes menyediakan berbagai peralatan untuk membantu mengatur mekanisme <em>deploy</em> aplikasi, termasuk pengaturan kapasitas dan pembaruan. Diantara fitur yang akan didiskusikan lebih mendalam yaitu <a href=/id/docs/concepts/configuration/overview/>berkas konfigurasi</a> dan <a href=/id/docs/concepts/overview/working-with-objects/labels/>label</a>.</p><h2 id=mengelola-konfigurasi-resource>Mengelola konfigurasi <em>resource</em></h2><p>Banyak aplikasi memerlukan beberapa <em>resource</em>, seperti Deployment dan Service. Pengelolaan beberapa <em>resource</em> dapat disederhanakan dengan mengelompokkannya dalam berkas yang sama (dengan pemisah <code>---</code> pada YAML). Contohnya:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/application/nginx-app.yaml download=application/nginx-app.yaml><code>application/nginx-app.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("application-nginx-app-yaml")' title="Copy application/nginx-app.yaml to clipboard"></img></div><div class=includecode id=application-nginx-app-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx-svc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.7.9<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Beberapa <em>resource</em> dapat dibuat seolah-olah satu <em>resource</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>service/my-nginx-svc created
</span></span><span style=display:flex><span>deployment.apps/my-nginx created
</span></span></code></pre></div><p><em>Resource</em> akan dibuat dalam urutan seperti pada berkas. Oleh karena itu, lebih baik menyalakan <em>service</em> lebih dahulu agar menjamin <em>scheduler</em> dapat menyebar <em>pod</em> yang terkait <em>service</em> selagi <em>pod</em> dibangkitkan oleh <em>controller</em>, seperti Deployment.</p><p><code>kubectl apply</code> juga dapat menerima beberapa argumen <code>-f</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><p>Selain berkas, kita dapat juga memasukkan direktori sebagai argumen:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx/
</span></span></code></pre></div><p><code>kubectl</code> akan membaca berkas apapun yang berakhiran <code>.yaml</code>, <code>.yml</code>, or <code>.json</code>.</p><p>Sangat disarankan untuk meletakkan sumber daya yang ada dalam <em>microservice</em> atau <em>tier</em> aplikasi yang sama dalam satu berkas, dan mengelompokkan semua berkas terkait aplikasimu dalam satu direktori. Jika <em>tier</em> masing-masing aplikasi terikat dengan DNS, maka kamu dapat melakukan <em>deploy</em> semua komponen teknologi yang dibutuhkan bersama-sama.</p><p>Lokasi konfigurasi dapat juga diberikan dalam bentuk URL. Ini berguna ketika ingin menjalankan berkas konfigurasi dari Github:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/master/content/en/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx created
</span></span></code></pre></div><h2 id=operasi-majemuk-dalam-kubectl>Operasi majemuk dalam kubectl</h2><p>Pembuatan <em>resource</em> bukanlah satu-satunya operasi yang bisa dijalankan <code>kubectl</code> secara majemuk. Contoh lainnya adalah mengekstrak nama <em>resource</em> dari berkas konfigurasi untuk menjalankan operasi lainnya, seperti untuk menghapus <em>resource</em> yang telah dibuat:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
</span></span><span style=display:flex><span>service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</span></span></code></pre></div><p>Pada kasus dua <em>resource</em>, mudah untuk memasukkan keduanya pada <em>command line</em> menggunakan sintaks <em>resource</em>/nama:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete deployments/my-nginx services/my-nginx-svc
</span></span></code></pre></div><p>Namun, untuk <em>resource</em> yang lebih banyak, memasukkan selektor (<em>label query</em>) menggunakan <code>-l</code> atau <code>--selector</code> untuk memfilter <em>resource</em> berdasarkan label akan lebih mudah:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete deployment,services -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
</span></span><span style=display:flex><span>service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</span></span></code></pre></div><p>Karena <code>kubectl</code> mengembalikan nama resource yang sama dengan sintaks yang diterima, mudah untuk melanjutkan operasi menggunakan <code>$()</code> atau <code>xargs</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get <span style=color:#a2f;font-weight:700>$(</span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service<span style=color:#a2f;font-weight:700>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT<span style=color:#666>(</span>S<span style=color:#666>)</span>      AGE
</span></span><span style=display:flex><span>my-nginx-svc   LoadBalancer   10.0.0.208   &lt;pending&gt;     80/TCP       0s
</span></span></code></pre></div><p>Dengan perintah di atas, pertama kita buat resource di dalam <code>examples/application/nginx/</code>. Lalu tampilkan resources yang terbentuk dengan format keluaran <code>-o name</code> (menampilkan tiap resource dalam format resource/nama). Kemudian lakukan <code>grep</code> hanya pada "service", dan tampilkan dengan <code>kubectl get</code>.</p><p>Untuk dapat menggunakan perintah di atas pada direktori yang bertingkat, kamu dapat memberi argumen <code>--recursive</code> atau <code>-R</code> bersama dengan argumen <code>--filename,-f</code>.</p><p>Misalnya ada sebuah direktori <code>project/k8s/development</code> memuat semua manifests yang berkaitan dengan <em>development environment</em>. Manifest akan tersusun berdasarkan tipe resource:</p><pre tabindex=0><code>project/k8s/development
├── configmap
│   └── my-configmap.yaml
├── deployment
│   └── my-deployment.yaml
└── pvc
    └── my-pvc.yaml
</code></pre><p>Secara <em>default</em>, menjalankan operasi majemuk pada <code>project/k8s/development</code> hanya akan terbatas pada direktori terluar saja. Sehingga ketika kita menjalankan operasi pembuatan dengan perintah berikut, kita akan mendapatkan pesan kesalahan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f project/k8s/development
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>error: you must provide one or more resources by argument or filename <span style=color:#666>(</span>.json|.yaml|.yml|stdin<span style=color:#666>)</span>
</span></span></code></pre></div><p>Solusinya, tambahkan argumen <code>--recursive</code> atau <code>-R</code> bersama dengan <code>--filename,-f</code>, seperti:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f project/k8s/development --recursive
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>configmap/my-config created
</span></span><span style=display:flex><span>deployment.apps/my-deployment created
</span></span><span style=display:flex><span>persistentvolumeclaim/my-pvc created
</span></span></code></pre></div><p>Argumen <code>--recursive</code> berjalan pada operasi apapun yang menerima argumen <code>--filename,-f</code> seperti: <code>kubectl {create,get,delete,describe,rollout} etc.</code></p><p>Argumen <code>--recursive</code> juga berjalan saat beberapa argumen <code>-f</code> diberikan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>namespace/development created
</span></span><span style=display:flex><span>namespace/staging created
</span></span><span style=display:flex><span>configmap/my-config created
</span></span><span style=display:flex><span>deployment.apps/my-deployment created
</span></span><span style=display:flex><span>persistentvolumeclaim/my-pvc created
</span></span></code></pre></div><p>Jika kamu tertarik mempelajari lebih lanjut tentang <code>kubectl</code>, silahkan baca <a href=/id/docs/reference/kubectl/overview/>Ikhtisar kubectl</a>.</p><h2 id=memakai-label-secara-efektif>Memakai label secara efektif</h2><p>Contoh yang kita lihat sejauh ini hanya menggunakan paling banyak satu label pada <em>resource</em>. Ada banyak skenario ketika membutuhkan beberapa label untuk membedakan sebuah kelompok dari yang lainnya.</p><p>Sebagai contoh, aplikasi yang berbeda akan menggunakan label <code>app</code> yang berbeda, tapi pada aplikasi <em>multitier</em>, seperti pada <a href=https://github.com/kubernetes/examples/tree/main/guestbook/>contoh buku tamu</a>, tiap <em>tier</em> perlu dibedakan. Misal untuk menandai <em>tier frontend</em> bisa menggunakan label:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span></code></pre></div><p>sementara itu Redis <em>master</em> dan <em>slave</em> memiliki label <code>tier</code> yang berbeda. Bisa juga menggunakan label tambahan <code>role</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>master<span style=color:#bbb>
</span></span></span></code></pre></div><p>dan</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>slave<span style=color:#bbb>
</span></span></span></code></pre></div><p>Label memungkinkan kita untuk memilah <em>resource</em> dengan pembeda berupa label:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml
</span></span><span style=display:flex><span>kubectl get pods -Lapp -Ltier -Lrole
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                           READY     STATUS    RESTARTS   AGE       APP         TIER       ROLE
</span></span><span style=display:flex><span>guestbook-fe-4nlpb             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
</span></span><span style=display:flex><span>guestbook-fe-ght6d             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
</span></span><span style=display:flex><span>guestbook-fe-jpy62             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
</span></span><span style=display:flex><span>guestbook-redis-master-5pg3b   1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    master
</span></span><span style=display:flex><span>guestbook-redis-slave-2q2yf    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
</span></span><span style=display:flex><span>guestbook-redis-slave-qgazl    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
</span></span><span style=display:flex><span>my-nginx-divi2                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
</span></span><span style=display:flex><span>my-nginx-o0ef1                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -lapp<span style=color:#666>=</span>guestbook,role<span style=color:#666>=</span>slave
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                          READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>guestbook-redis-slave-2q2yf   1/1       Running   <span style=color:#666>0</span>          3m
</span></span><span style=display:flex><span>guestbook-redis-slave-qgazl   1/1       Running   <span style=color:#666>0</span>          3m
</span></span></code></pre></div><h2 id=deploy-dengan-canary>Deploy dengan Canary</h2><p>Skenario lain yang menggunakan beberapa label yaitu saat membedakan deployment komponen yang sama namun dengan rilis atau konfigurasi yang berbeda. Adalah praktik yang umum untuk mendeploy sebuah <em>canary</em> dari rilis aplikasi yang baru (berdasarkan <em>tag image</em> dalam templat <em>pod</em>) bersamaan dengan rilis sebelumnya. Ini memungkinkan rilis yang baru dapat menerima <em>live traffic</em> sebelum benar-benar menggantikan rilis yang lama.</p><p>Salah satu alternatif yaitu kamu dapat memakai label <code>track</code> untuk membedakan antar rilis.</p><p>Rilis primer dan stabil akan memiliki label <code>track</code> yang berisi <code>stable</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>stable<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v3<span style=color:#bbb>
</span></span></span></code></pre></div><p>kemudian kamu buat lagi rilis <em>frontend</em> buku tamu yang membawa label <code>track</code> yang berbeda (misal <code>canary</code>), sehingga <em>pod</em> dalam kedua rilis tidak beririsan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-canary<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>canary<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v4<span style=color:#bbb>
</span></span></span></code></pre></div><p>Servis <em>frontend</em> akan meliputi kedua set replika dengan menentukan subset bersama dari para labelnya (tanpa <code>track</code>). Sehingga <em>traffic</em> akan diarahkan ke kedua aplikasi:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span></code></pre></div><p>Kamu dapat mengatur jumlah replika rilis <em>stable</em> dan <em>canary</em> untuk menentukan rasio dari tiap rilis yang akan menerima <em>traffic production live</em> (dalam kasus ini 3:1).
Ketika telah yakin, kamu dapat memindahkan <em>track stable</em> ke rilis baru dan menghapus <em>canary</em>.</p><p>Untuk contoh yang lebih jelas, silahkan cek <a href=https://github.com/kelseyhightower/talks/tree/master/kubecon-eu-2016/demo#deploy-a-canary>tutorial melakukan deploy Ghost</a>.</p><h2 id=memperbarui-label>Memperbarui label</h2><p>Kadang, <em>pod</em> dan <em>resource</em> lain yang sudah ada harus dilabeli ulang sebelum membuat <em>resource</em> baru. Hal ini dapat dilakukan dengan perintah <code>kubectl label</code>.
Contohnya jika kamu ingin melabeli ulang semua <em>pod</em> nginx sebagai <em>frontend tier</em>, tinggal jalankan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl label pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx <span style=color:#b8860b>tier</span><span style=color:#666>=</span>fe
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pod/my-nginx-2035384211-j5fhi labeled
</span></span><span style=display:flex><span>pod/my-nginx-2035384211-u2c7e labeled
</span></span><span style=display:flex><span>pod/my-nginx-2035384211-u3t6x labeled
</span></span></code></pre></div><p>Perintah ini melakukan filter pada semua <em>pod</em> dengan label "app=nginx", lalu melabelinya dengan "tier=fe".
Untuk melihat <em>pod</em> yang telah dilabeli, jalankan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx -L tier
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                        READY     STATUS    RESTARTS   AGE       TIER
</span></span><span style=display:flex><span>my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          23m       fe
</span></span><span style=display:flex><span>my-nginx-2035384211-u2c7e   1/1       Running   <span style=color:#666>0</span>          23m       fe
</span></span><span style=display:flex><span>my-nginx-2035384211-u3t6x   1/1       Running   <span style=color:#666>0</span>          23m       fe
</span></span></code></pre></div><p>Akan muncul semua <em>pod</em> dengan "app=nginx" dan sebuah kolom label tambahan yaitu tier (ditentukan dengan <code>-L</code> atau <code>--label-columns</code>).</p><p>Untuk informasi lebih lanjut, silahkan baca <a href=/id/docs/concepts/overview/working-with-objects/labels/>label</a> dan <a href=/docs/reference/generated/kubectl/kubectl-commands/#label>kubectl label</a>.</p><h2 id=memperbarui-anotasi>Memperbarui anotasi</h2><p>Kadang resource perlu ditambahkan anotasi. Anotasi adalah metadata sembarang yang tidak unik, seperti <em>tools, libraries</em>, dsb yang digunakan oleh klien API . Ini dapat dilakukan dengan <code>kubectl annotate</code>. Sebagai contoh:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl annotate pods my-nginx-v4-9gw19 <span style=color:#b8860b>description</span><span style=color:#666>=</span><span style=color:#b44>&#39;my frontend running nginx&#39;</span>
</span></span><span style=display:flex><span>kubectl get pods my-nginx-v4-9gw19 -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    description: my frontend running nginx
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>Untuk informasi lebih lanjut, silahkan lihat laman <a href=/id/docs/concepts/overview/working-with-objects/annotations/>annotations</a> dan <a href=/docs/reference/generated/kubectl/kubectl-commands/#annotate>kubectl annotate</a>.</p><h2 id=memperbesar-dan-memperkecil-aplikasi-kamu>Memperbesar dan memperkecil aplikasi kamu</h2><p>Saat beban aplikasi naik maupun turun, mudah untuk mengubah kapasitas dengan <code>kubectl</code>. Contohnya, untuk menurunkan jumlah replika nginx dari 3 ke 1, lakukan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment/my-nginx --replicas<span style=color:#666>=</span><span style=color:#666>1</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.extensions/my-nginx scaled
</span></span></code></pre></div><p>Sekarang kamu hanya memiliki satu <em>pod</em> yang dikelola oleh deployment.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                        READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          30m
</span></span></code></pre></div><p>Agar sistem dapat menyesuaikan jumlah replika nginx yang dibutuhkan secara otomatis dari 1 hingga 3, lakukan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale deployment/my-nginx --min<span style=color:#666>=</span><span style=color:#666>1</span> --max<span style=color:#666>=</span><span style=color:#666>3</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>horizontalpodautoscaler.autoscaling/my-nginx autoscaled
</span></span></code></pre></div><p>Sekarang jumlah replika nginx akan secara otomatis naik dan turun sesuai kebutuhan.</p><p>Informasi tambahan dapat dilihat pada dokumen <a href=/docs/reference/generated/kubectl/kubectl-commands/#scale>kubectl scale</a>, <a href=/docs/reference/generated/kubectl/kubectl-commands/#autoscale>kubectl autoscale</a> dan <a href=/docs/tasks/run-application/horizontal-pod-autoscale/>horizontal <em>pod</em> autoscaler</a>.</p><h2 id=pembaruan-resource-di-tempat>Pembaruan resource di tempat</h2><p>Kadang kita perlu membuat pembaruan kecil, yang tidak mengganggu pada <em>resource</em> yang telah dibuat.</p><h3 id=kubectl-apply>kubectl apply</h3><p>Disarankan untuk menyimpan berkas-berkas konfigurasi dalam <em>source control</em> (lihat <a href=http://martinfowler.com/bliki/InfrastructureAsCode.html>konfigurasi sebagai kode</a>). Sehingga berkas dapat dipelihara dan diatur dalam versi bersama dengan kode milik <em>resource</em> yang diatur oleh konfigurasi tersebut. Berikutnya, kamu dapat menggunakan <a href=/docs/reference/generated/kubectl/kubectl-commands/#apply><code>kubectl apply</code></a> untuk membarui perubahan konfigurasi ke klaster.</p><p>Perintah ini akan membandingkan versi konfigurasi yang disuplai dengan versi sebelumnya yang telah berjalan dan memasang perubahan yang kamu buat tanpa mengganti properti yang tidak berubah sama sekali.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span><span style=display:flex><span>deployment.apps/my-nginx configured
</span></span></code></pre></div><p>Perhatikan bahwa <code>kubectl apply</code> memasang anotasi pada <em>resource</em> untuk menentukan perubahan pada konfigurasi sejak terakhir dipanggil. Ketika dijalankan, <code>kubectl apply</code> melakukan pembandingan <em>three-way</em> antara konfigurasi sebelumnya, masukan yang disuplai, dan konfigurasi <em>resource</em> sekarang, untuk dapat menentukan cara memodifikasi <em>resource</em>.</p><p>Saat ini, <em>resource</em> dibuat tanpa ada anotasi. Jadi pemanggilan pertama pada <code>kubectl apply</code> akan dikembalikan pada perbandingan <em>two-way</em> antara masukan pengguna dan konfigurasi <em>resource</em> sekarang. Saat pemanggilan pertama ini, tidak ada penghapusan set properti yang terdeteksi saat <em>resource</em> dibuat. Sehingga, tidak ada yang dihapus.</p><p>Tiap <code>kubectl apply</code>, atau perintah lain yang memodifikasi konfigurasi seperti <code>kubectl replace</code> dan <code>kubectl edit</code> dijalankan, anotasi akan diperbarui. Sehingga memungkinkan operasi <code>kubectl apply</code> untuk mendeteksi dan melakukan penghapusan secara perbandingan <em>three-way</em>.</p><h3 id=kubectl-edit>kubectl edit</h3><p>Sebagai alternatif, kamu juga dapat membarui resource dengan <code>kubectl edit</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment/my-nginx
</span></span></code></pre></div><p>Ini sama dengan melakukan <code>get</code> pada <em>resource</em>, mengubahnya di text editor, kemudian menjalankan<code>apply</code> pada <em>resource</em> dengan versi terkini:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment my-nginx -o yaml &gt; /tmp/nginx.yaml
</span></span><span style=display:flex><span>vi /tmp/nginx.yaml
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># lakukan pengubahan, lalu simpan berkas</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl apply -f /tmp/nginx.yaml
</span></span><span style=display:flex><span>deployment.apps/my-nginx configured
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rm /tmp/nginx.yaml
</span></span></code></pre></div><p>Cara demikian memungkinkan kamu membuat perubahan signifikan dengan mudah. Lihat bahwa kamu juga dapat menentukan editor dengan variabel environment <code>EDITOR</code> atau <code>KUBE_EDITOR</code>.</p><p>Untuk informasi tambahan, silahkan lihat laman <a href=/docs/reference/generated/kubectl/kubectl-commands/#edit>kubectl edit</a>.</p><h3 id=kubectl-patch>kubectl patch</h3><p>Kamu dapat menggunakan <code>kubectl patch</code> untuk membarui obyek API di tempat. Perintah ini mendukung patch JSON, <em>patch</em> gabungan JSON, dan <em>strategic merge patch</em>. Lihat
<a href=/docs/tasks/run-application/update-api-object-kubectl-patch/>Update API Objects in Place Using kubectl patch</a>
dan
<a href=/docs/reference/generated/kubectl/kubectl-commands/#patch>kubectl patch</a>.</p><h2 id=pembaruan-disruptif>Pembaruan disruptif</h2><p>Pada kasus tertentu, kamu mungkin perlu memperbarui field resource yang tidak dapat diperbarui setelah diinisiasi atau kamu ingin membuat perubahan rekursif segera, seperti memperbaiki <em>pod</em> yang rusak saat menjalankan Deployment. Untuk mengubah field seperti itu, gunakan <code>replace --force</code> yang akan menghapus dan membuat ulang resource. Dalam kasus ini kamu dapat mengubah berkas konfigurasi awalnya:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx deleted
</span></span><span style=display:flex><span>deployment.apps/my-nginx replaced
</span></span></code></pre></div><h2 id=membarui-aplikasi-tanpa-memadamkan-servis>Membarui aplikasi tanpa memadamkan servis</h2><p>Suatu saat, kamu akan perlu untuk membarui aplikasi yang telah terdeploy, biasanya dengan mengganti <em>image</em> atau <em>tag</em> sebagaimana dalam skenario <em>canary deployment</em> di atas. <code>kubectl</code> mendukung beberapa operasi pembaruan, masing-masing dapat digunakan pada skenario berbeda.</p><p>Kami akan memandumu untuk membuat dan membarui aplikasi melalui Deployment.</p><p>Misal kamu telah menjalankan nginx versi 1.7.9:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl run my-nginx --image<span style=color:#666>=</span>nginx:1.7.9 --replicas<span style=color:#666>=</span><span style=color:#666>3</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx created
</span></span></code></pre></div><p>Untuk memperbarui versi ke 1.9.1, ganti <code>.spec.template.spec.containers[0].image</code> dari <code>nginx:1.7.9</code> ke <code>nginx:1.9.1</code>, dengan perintah kubectl yang telah dipelajari di atas.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment/my-nginx
</span></span></code></pre></div><p>Selesai! Deployment akan memperbarui aplikasi nginx yang terdeploy secara berangsur di belakang. Dia akan menjamin hanya ada sekian replika lama yang akan down selagi pembaruan berjalan dan hanya ada sekian replika baru akan dibuat melebihi jumlah pod. Untuk mempelajari lebih lanjut, kunjungi <a href=/id/docs/concepts/workloads/controllers/deployment/>laman Deployment</a>.</p><h2 id=selanjutnya>Selanjutnya</h2><ul><li><a href=/docs/tasks/debug-application-cluster/debug-application-introspection/>Pelajari tentang bagaimana memakai <code>kubectl</code> untuk memeriksa dan <em>debug</em> aplikasi.</a></li><li><a href=/id/docs/concepts/configuration/overview/>Praktik Terbaik dan Tips Konfigurasi</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d649067a69d8d5c7e71564b42b96909e>5 - Jaringan Kluster</h1><p>Jaringan adalah bagian utama dari Kubernetes, tetapi bisa menjadi sulit
untuk memahami persis bagaimana mengharapkannya bisa bekerja.
Ada 4 masalah yang berbeda untuk diatasi:</p><ol><li>Komunikasi antar kontainer yang sangat erat: hal ini diselesaikan oleh
<a href=/id/docs/concepts/workloads/pods/pod/>Pod</a> dan komunikasi <code>localhost</code>.</li><li>Komunikasi antar Pod: ini adalah fokus utama dari dokumen ini.</li><li>Komunikasi Pod dengan Service: ini terdapat di <a href=/id/docs/concepts/services-networking/service/>Service</a>.</li><li>Komunikasi eksternal dengan Service: ini terdapat di <a href=/id/docs/concepts/services-networking/service/>Service</a>.</li></ol><p>Kubernetes adalah tentang berbagi mesin antar aplikasi. Pada dasarnya,
saat berbagi mesin harus memastikan bahwa dua aplikasi tidak mencoba menggunakan
<em>port</em> yang sama. Mengkoordinasikan <em>port</em> di banyak pengembang sangat sulit
dilakukan pada skala yang berbeda dan memaparkan pengguna ke masalah
tingkat kluster yang di luar kendali mereka.</p><p>Alokasi <em>port</em> yang dinamis membawa banyak komplikasi ke sistem - setiap aplikasi
harus menganggap <em>port</em> sebagai <em>flag</em>, <em>server</em> API harus tahu cara memasukkan
nomor <em>port</em> dinamis ke dalam blok konfigurasi, Service-Service harus tahu cara
menemukan satu sama lain, dll. Sebaliknya daripada berurusan dengan ini,
Kubernetes mengambil pendekatan yang berbeda.</p><h2 id=model-jaringan-kubernetes>Model jaringan Kubernetes</h2><p>Setiap Pod mendapatkan alamat IP sendiri. Ini berarti kamu tidak perlu secara langsung membuat tautan antara Pod dan kamu hampir tidak perlu berurusan dengan memetakan <em>port</em> kontainer ke <em>port</em> pada <em>host</em>. Ini menciptakan model yang bersih, kompatibel dengan yang sebelumnya dimana Pod dapat diperlakukan seperti halnya VM atau <em>host</em> fisik dari perspektif alokasi <em>port</em>, penamaan, <em>service discovery</em>, <em>load balancing</em>, konfigurasi aplikasi, dan migrasi.</p><p>Kubernetes memberlakukan persyaratan mendasar berikut pada setiap implementasi jaringan (kecuali kebijakan segmentasi jaringan yang disengaja):</p><ul><li>Pod pada suatu Node dapat berkomunikasi dengan semua Pod pada semua Node tanpa NAT</li><li>agen pada suatu simpul (mis. <em>daemon</em> sistem, kubelet) dapat berkomunikasi dengan semua Pod pada Node itu</li></ul><p>Catatan: Untuk platform yang mendukung Pod yang berjalan di jaringan <em>host</em> (mis. Linux):</p><ul><li>Pod di jaringan <em>host</em> dari sebuah Node dapat berkomunikasi dengan semua Pod pada semua Node tanpa NAT</li></ul><p>Model ini tidak hanya sedikit kompleks secara keseluruhan, tetapi pada prinsipnya kompatibel dengan keinginan Kubernetes untuk memungkinkan <em>low-friction porting</em> dari aplikasi dari VM ke kontainer. Jika pekerjaan kamu sebelumnya dijalankan dalam VM, VM kamu memiliki IP dan dapat berbicara dengan VM lain di proyek yang sama. Ini adalah model dasar yang sama.</p><p>Alamat IP Kubernetes ada di lingkup Pod - kontainer dalam Pod berbagi jaringan <em>namespace</em> mereka - termasuk alamat IP mereka. Ini berarti bahwa kontainer dalam Pod semua dapat mencapai <em>port</em> satu sama lain di <code>_localhost_</code>. Ini juga berarti bahwa kontainer dalam Pod harus mengoordinasikan penggunaan <em>port</em>, tetapi ini tidak berbeda dari proses di VM. Ini disebut model "IP-per-pod".</p><h2 id=bagaimana-menerapkan-model-jaringan-kubernetes>Bagaimana menerapkan model jaringan Kubernetes</h2><p>Ada beberapa cara agar model jaringan ini dapat diimplementasikan. Dokumen ini bukan studi lengkap tentang berbagai metode, tetapi semoga berfungsi sebagai pengantar ke berbagai teknologi dan berfungsi sebagai titik awal.</p><p>Opsi jaringan berikut ini disortir berdasarkan abjad - urutan tidak menyiratkan status istimewa apa pun.</p><h3 id=aci>ACI</h3><p><a href=https://www.cisco.com/c/en/us/solutions/data-center-virtualization/application-centric-infrastructure/index.html>Infrastruktur Sentral Aplikasi Cisco</a> menawarkan solusi SDN overlay dan underlay terintegrasi yang mendukung kontainer, mesin virtual, dan <em>bare metal server</em>. <a href=https://www.github.com/noironetworks/aci-containers>ACI</a> menyediakan integrasi jaringan kontainer untuk ACI. Tinjauan umum integrasi disediakan <a href=https://www.cisco.com/c/dam/en/us/solutions/collateral/data-center-virtualization/application-centric-infrastructure/solution-overview-c22-739493.pdf>di sini</a>.</p><h3 id=aos-dari-apstra>AOS dari Apstra</h3><p><a href=http://www.apstra.com/products/aos/>AOS</a> adalah sistem Jaringan Berbasis Intent yang menciptakan dan mengelola lingkungan pusat data yang kompleks dari platform terintegrasi yang sederhana. AOS memanfaatkan desain terdistribusi sangat <em>scalable</em> untuk menghilangkan pemadaman jaringan sambil meminimalkan biaya.</p><p>Desain Referensi AOS saat ini mendukung <em>host</em> yang terhubung dengan Lapis-3 yang menghilangkan masalah peralihan Lapis-2 yang lama. Host Lapis-3 ini bisa berupa <em>server</em> Linux (Debian, Ubuntu, CentOS) yang membuat hubungan tetangga BGP secara langsung dengan <em>top of rack switches</em> (TORs). AOS mengotomatisasi kedekatan perutean dan kemudian memberikan kontrol yang halus atas <em>route health injections</em> (RHI) yang umum dalam <em>deployment</em> Kubernetes.</p><p>AOS memiliki banyak kumpulan endpoint REST API yang memungkinkan Kubernetes dengan cepat mengubah kebijakan jaringan berdasarkan persyaratan aplikasi. Peningkatan lebih lanjut akan mengintegrasikan model Grafik AOS yang digunakan untuk desain jaringan dengan penyediaan beban kerja, memungkinkan sistem manajemen ujung ke ujung untuk layanan cloud pribadi dan publik.</p><p>AOS mendukung penggunaan peralatan vendor umum dari produsen termasuk Cisco, Arista, Dell, Mellanox, HPE, dan sejumlah besar sistem white-box dan sistem operasi jaringan terbuka seperti Microsoft SONiC, Dell OPX, dan Cumulus Linux.</p><p>Detail tentang cara kerja sistem AOS dapat diakses di sini: <a href=http://www.apstra.com/products/how-it-works/>http://www.apstra.com/products/how-it-works/</a></p><h3 id=aws-vpc-cni-untuk-kubernetes>AWS VPC CNI untuk Kubernetes</h3><p><a href=https://github.com/aws/amazon-vpc-cni-k8s>AWS VPC CNI</a> menawarkan jaringan AWS <em>Virtual Private Cloud</em> (VPC) terintegrasi untuk kluster Kubernetes. Plugin CNI ini menawarkan <em>throughput</em> dan ketersediaan tinggi, latensi rendah, dan <em>jitter</em> jaringan minimal. Selain itu, pengguna dapat menerapkan jaringan AWS VPC dan praktik keamanan terbaik untuk membangun kluster Kubernetes. Ini termasuk kemampuan untuk menggunakan catatan aliran VPC, kebijakan perutean VPC, dan grup keamanan untuk isolasi lalu lintas jaringan.</p><p>Menggunakan <em>plugin</em> CNI ini memungkinkan Pod Kubernetes memiliki alamat IP yang sama di dalam Pod seperti yang mereka lakukan di jaringan VPC. CNI mengalokasikan AWS <em>Elastic Networking Interfaces</em> (ENIs) ke setiap node Kubernetes dan menggunakan rentang IP sekunder dari setiap ENI untuk Pod pada Node. CNI mencakup kontrol untuk pra-alokasi ENI dan alamat IP untuk waktu mulai Pod yang cepat dan memungkinkan kluster besar hingga 2.000 Node.</p><p>Selain itu, CNI dapat dijalankan bersama <a href=https://docs.aws.amazon.com/eks/latest/userguide/calico.html>Calico untuk penegakan kebijakan jaringan</a>. Proyek AWS VPC CNI adalah <em>open source</em> dengan <a href=https://github.com/aws/amazon-vpc-cni-k8s>dokumentasi di GitHub</a>.</p><h3 id=big-cloud-fabric-dari-big-switch-networks>Big Cloud Fabric dari Big Switch Networks</h3><p><a href=https://www.bigswitch.com/container-network-automation>Big Cloud Fabric</a> adalah arsitektur jaringan asli layanan cloud, yang dirancang untuk menjalankan Kubernetes di lingkungan cloud pribadi / lokal. Dengan menggunakan SDN fisik & <em>virtual</em> terpadu, Big Cloud Fabric menangani masalah yang sering melekat pada jaringan kontainer seperti penyeimbangan muatan, visibilitas, pemecahan masalah, kebijakan keamanan & pemantauan lalu lintas kontainer.</p><p>Dengan bantuan arsitektur multi-penyewa Pod virtual pada Big Cloud Fabric, sistem orkestrasi kontainer seperti Kubernetes, RedHat OpenShift, Mesosphere DC/OS & Docker Swarm akan terintegrasi secara alami bersama dengan sistem orkestrasi VM seperti VMware, OpenStack & Nutanix. Pelanggan akan dapat terhubung dengan aman berapa pun jumlah klusternya dan memungkinkan komunikasi antar penyewa di antara mereka jika diperlukan.</p><p>Terbaru ini BCF diakui oleh Gartner sebagai visioner dalam <a href=http://go.bigswitch.com/17GatedDocuments-MagicQuadrantforDataCenterNetworking_Reg.html><em>Magic Quadrant</em></a>. Salah satu penyebaran BCF Kubernetes di tempat (yang mencakup Kubernetes, DC/OS & VMware yang berjalan di beberapa DC di berbagai wilayah geografis) juga dirujuk <a href=https://portworx.com/architects-corner-kubernetes-satya-komala-nio/>di sini</a>.</p><h3 id=cilium>Cilium</h3><p><a href=https://github.com/cilium/cilium>Cilium</a> adalah perangkat lunak <em>open source</em> untuk menyediakan dan secara transparan mengamankan konektivitas jaringan antar kontainer aplikasi. Cilium mengetahui L7/HTTP dan dapat memberlakukan kebijakan jaringan pada L3-L7 menggunakan model keamanan berbasis identitas yang dipisahkan dari pengalamatan jaringan.</p><h3 id=cni-genie-dari-huawei>CNI-Genie dari Huawei</h3><p><a href=https://github.com/Huawei-PaaS/CNI-Genie>CNI-Genie</a> adalah <em>plugin</em> CNI yang memungkinkan Kubernetes [secara bersamaan memiliki akses ke berbagai implementasi](<a href=https://github.com/Huawei-PaaS>https://github.com/Huawei-PaaS</a> /CNI-Genie/blob/master/docs/multiple-cni-plugins/README.md#what-cni-genie-feature-1-multiple-cni-plugins-enables) dari [model jaringan Kubernetes] (<a href=https://git.k8s.io/website/docs/concepts/cluster-administration/networking.md#kubernetes-model>https://git.k8s.io/website/docs/concepts/cluster-administration/networking.md#kubernetes-model</a>) dalam <em>runtime</em>. Ini termasuk setiap implementasi yang berjalan sebagai <a href=https://github.com/containernetworking/cni#3rd-party-plugins><em>plugin</em> CNI</a>, seperti <a href=https://github.com/coreos/flannel#flannel>Flannel</a>, <a href=http://docs.projectcalico.org/>Calico</a>, <a href=http://romana.io>Romana</a>, <a href=https://www.weave.works/products/weave-net/>Weave-net</a>.</p><p>CNI-Genie juga mendukung <a href=https://github.com/Huawei-PaaS/CNI-Genie/blob/master/docs/multiple-ips/README.md#feature-2-extension-cni-genie-multiple-ip-address-per-pod>menetapkan beberapa alamat IP ke sebuah Pod</a>, masing-masing dari <em>plugin</em> CNI yang berbeda.</p><h3 id=cni-ipvlan-vpc-k8s>cni-ipvlan-vpc-k8s</h3><p><a href=https://github.com/lyft/cni-ipvlan-vpc-k8s>cni-ipvlan-vpc-k8s</a> berisi satu set <em>plugin</em> CNI dan IPAM untuk menyediakan kemudahan, host-lokal, latensi rendah, <em>throughput</em> tinggi , dan tumpukan jaringan yang sesuai untuk Kubernetes dalam lingkungan Amazon Virtual Private Cloud (VPC) dengan memanfaatkan Amazon Elastic Network Interfaces (ENI) dan mengikat IP yang dikelola AWS ke Pod-Pod menggunakan <em>driver</em> IPvlan <em>kernel</em> Linux dalam mode L2.</p><p>Plugin ini dirancang untuk secara langsung mengkonfigurasi dan <em>deploy</em> dalam VPC. Kubelet melakukan <em>booting</em> dan kemudian mengkonfigurasi sendiri dan memperbanyak penggunaan IP mereka sesuai kebutuhan tanpa memerlukan kompleksitas yang sering direkomendasikan untuk mengelola jaringan <em>overlay</em>, BGP, menonaktifkan pemeriksaan sumber/tujuan, atau menyesuaikan tabel rute VPC untuk memberikan <em>subnet</em> per <em>instance</em> ke setiap <em>host</em> (yang terbatas hingga 50-100 masukan per VPC). Singkatnya, cni-ipvlan-vpc-k8s secara signifikan mengurangi kompleksitas jaringan yang diperlukan untuk menggunakan Kubernetes yang berskala di dalam AWS.</p><h3 id=contiv>Contiv</h3><p><a href=https://github.com/contiv/netplugin>Contiv</a> menyediakan jaringan yang dapat dikonfigurasi (<em>native</em> l3 menggunakan BGP, <em>overlay</em> menggunakan vxlan, classic l2, atau Cisco-SDN / ACI) untuk berbagai kasus penggunaan. <a href=http://contiv.io>Contiv</a> semuanya open sourced.</p><h3 id=contrail-tungsten-fabric>Contrail / Tungsten Fabric</h3><p><a href=http://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a>, berdasarkan <a href=https://tungsten.io>Tungsten Fabric</a>, adalah platform virtualisasi jaringan dan manajemen kebijakan <em>multi-cloud</em> yang benar-benar terbuka. Contrail dan Tungsten Fabric terintegrasi dengan berbagai sistem orkestrasi seperti Kubernetes, OpenShift, OpenStack dan Mesos, dan menyediakan mode isolasi yang berbeda untuk mesin <em>virtual</em>, banyak kontainer / banyak Pod dan beban kerja <em>bare metal</em>.</p><h3 id=danm>DANM</h3><p>[DANM] (<a href=https://github.com/nokia/danm>https://github.com/nokia/danm</a>) adalah solusi jaringan untuk beban kerja telco yang berjalan di kluster Kubernetes. Dibangun dari komponen-komponen berikut:</p><ul><li>Plugin CNI yang mampu menyediakan antarmuka IPVLAN dengan fitur-fitur canggih</li><li>Modul IPAM built-in dengan kemampuan mengelola dengan jumlah banyak, <em>cluster-wide</em>, <em>discontinous</em> jaringan L3 dan menyediakan skema dinamis, statis, atau tidak ada permintaan skema IP</li><li>Metaplugin CNI yang mampu melampirkan beberapa antarmuka jaringan ke kontainer, baik melalui CNI sendiri, atau mendelegasikan pekerjaan ke salah satu solusi CNI populer seperti SRI-OV, atau Flannel secara paralel</li><li>Pengontrol Kubernetes yang mampu mengatur secara terpusat antarmuka VxLAN dan VLAN dari semua <em>host</em> Kubernetes</li><li>Pengontrol Kubernetes lain yang memperluas konsep <em>service discovery</em> berbasis servis untuk bekerja di semua antarmuka jaringan Pod</li></ul><p>Dengan <em>toolset</em> ini, DANM dapat memberikan beberapa antarmuka jaringan yang terpisah, kemungkinan untuk menggunakan ujung belakang jaringan yang berbeda dan fitur IPAM canggih untuk Pod.</p><h3 id=flannel>Flannel</h3><p>[Flannel] (<a href=https://github.com/coreos/flannel#flannel>https://github.com/coreos/flannel#flannel</a>) adalah jaringan overlay yang sangat sederhana yang memenuhi persyaratan Kubernetes. Banyak orang telah melaporkan kesuksesan dengan Flannel dan Kubernetes.</p><h3 id=google-compute-engine-gce>Google Compute Engine (GCE)</h3><p>Untuk skrip konfigurasi kluster Google Compute Engine, <a href=https://cloud.google.com/vpc/docs/routes>perutean lanjutan</a> digunakan untuk menetapkan setiap VM <em>subnet</em> (standarnya adalah <code>/24</code> - 254 IP). Setiap lalu lintas yang terikat untuk <em>subnet</em> itu akan dialihkan langsung ke VM oleh <em>fabric</em> jaringan GCE. Ini adalah tambahan untuk alamat IP "utama" yang ditugaskan untuk VM, yang NAT'ed untuk akses internet keluar. Sebuah linux <em>bridge</em> (disebut <code>cbr0</code>) dikonfigurasikan untuk ada pada subnet itu, dan diteruskan ke <em>flag</em> <code>-bridge</code> milik docker.</p><p>Docker dimulai dengan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>DOCKER_OPTS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--bridge=cbr0 --iptables=false --ip-masq=false&#34;</span>
</span></span></code></pre></div><p>Jembatan ini dibuat oleh Kubelet (dikontrol oleh <em>flag</em> <code>--network-plugin=kubenet</code>) sesuai dengan <code>.spec.podCIDR</code> yang dimiliki oleh Node.</p><p>Docker sekarang akan mengalokasikan IP dari blok <code>cbr-cidr</code>. Kontainer dapat menjangkau satu sama lain dan Node di atas jembatan <code>cbr0</code>. IP-IP tersebut semuanya dapat dirutekan dalam jaringan proyek GCE.</p><p>GCE sendiri tidak tahu apa-apa tentang IP ini, jadi tidak akan NAT untuk lalu lintas internet keluar. Untuk mencapai itu aturan iptables digunakan untuk menyamar (alias SNAT - untuk membuatnya seolah-olah paket berasal dari lalu lintas <code>Node</code> itu sendiri) yang terikat untuk IP di luar jaringan proyek GCE (10.0.0.0/8).</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>iptables -t nat -A POSTROUTING ! -d 10.0.0.0/8 -o eth0 -j MASQUERADE
</span></span></code></pre></div><p>Terakhir IP forwarding diaktifkan di kernel (sehingga kernel akan memproses paket untuk kontainer yang dijembatani):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sysctl net.ipv4.ip_forward<span style=color:#666>=</span><span style=color:#666>1</span>
</span></span></code></pre></div><p>Hasil dari semua ini adalah bahwa semua Pod dapat saling menjangkau dan dapat keluar lalu lintas ke internet.</p><h3 id=jaguar>Jaguar</h3><p><a href=https://gitlab.com/sdnlab/jaguar>Jaguar</a> adalah solusi open source untuk jaringan Kubernetes berdasarkan OpenDaylight. Jaguar menyediakan jaringan overlay menggunakan vxlan dan Jaguar CNIPlugin menyediakan satu alamat IP per Pod.</p><h3 id=knitter>Knitter</h3><p><a href=https://github.com/ZTE/Knitter/>Knitter</a> adalah solusi jaringan yang mendukung banyak jaringan di Kubernetes. Solusi ini menyediakan kemampuan manajemen penyewa dan manajemen jaringan. Knitter mencakup satu set solusi jaringan kontainer NFV ujung ke ujung selain beberapa pesawat jaringan, seperti menjaga alamat IP untuk aplikasi, migrasi alamat IP, dll.</p><h3 id=kube-ovn>Kube-OVN</h3><p><a href=https://github.com/alauda/kube-ovn>Kube-OVN</a> adalah <em>fabric</em> jaringan kubernetes berbasis OVN untuk <em>enterprises</em>. Dengan bantuan OVN/OVS, solusi ini menyediakan beberapa fitur jaringan <em>overlay</em> canggih seperti <em>subnet</em>, QoS, alokasi IP statis, <em>mirroring traffic</em>, <em>gateway</em>, kebijakan jaringan berbasis <em>openflow</em>, dan proksi layanan.</p><h3 id=kube-router>Kube-router</h3><p><a href=https://github.com/cloudnativelabs/kube-router>Kube-router</a> adalah solusi jaringan yang dibuat khusus untuk Kubernetes yang bertujuan untuk memberikan kinerja tinggi dan kesederhanaan operasional. Kube-router menyediakan Linux <a href=http://www.linuxvirtualserver.org/software/ipvs.html>LVS/IPVS</a> berbasis proksi layanan, solusi jaringan berbasis penerusan <em>pod-to-pod</em> Linux <em>kernel</em> tanpa <em>overlay</em>, dan penegak kebijakan jaringan berbasis <em>iptables/ipset</em>.</p><h3 id=l2-networks-and-linux-bridging>L2 networks and linux bridging</h3><p>Jika Anda memiliki jaringan L2 yang "bodoh", seperti saklar sederhana di <em>environment</em> "bare-metal", kamu harus dapat melakukan sesuatu yang mirip dengan pengaturan GCE di atas. Perhatikan bahwa petunjuk ini hanya dicoba dengan sangat sederhana - sepertinya berhasil, tetapi belum diuji secara menyeluruh. Jika kamu menggunakan teknik ini dan telah menyempurnakan prosesnya, tolong beri tahu kami.</p><p>Ikuti bagian "With Linux Bridge devices" dari <a href=http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/>tutorial yang sangat bagus ini</a> dari Lars Kellogg-Stedman.</p><h3 id=multus-plugin-multi-jaringan>Multus (plugin Multi-Jaringan)</h3><p><a href=https://github.com/Intel-Corp/multus-cni>Multus</a> adalah plugin Multi CNI untuk mendukung fitur Banyak Jaringan di Kubernetes menggunakan objek jaringan berbasis CRD di Kubernetes.</p><p>Multus mendukung semua <a href=https://github.com/containernetworking/plugins>plugin referensi</a> (mis. <a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/flannel>Flannel</a>, <a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/dhcp>DHCP</a>, [Macvlan](<a href=https://github.com/containernetworking/plugins/tree/master/plugins/main>https://github.com/containernetworking/plugins/tree/master/plugins/main</a> / macvlan)) yang mengimplementasikan spesifikasi CNI dan plugin pihak ke-3 (mis. <a href=https://github.com/projectcalico/cni-plugin>Calico</a>, <a href=https://github.com/weaveworks/weave>Weave</a>, <a href=https://github.com/cilium/cilium>Cilium</a>, <a href=https://github.com/contiv/netplugin>Contiv</a>). Selain itu, Multus mendukung <a href=https://github.com/hustcat/sriov-cni>SRIOV</a>, <a href=https://github.com/Intel-Corp/sriov-cni>DPDK</a>, <a href=https://github.com/intel/vhost-user-net-plugin>OVS- DPDK & VPP</a> beban kerja di Kubernetes dengan aplikasi cloud asli dan aplikasi berbasis NFV di Kubernetes.</p><h3 id=nsx-t>NSX-T</h3><p><a href=https://docs.vmware.com/en/VMware-NSX-T/index.html>VMware NSX-T</a> adalah virtualisasi jaringan dan platform keamanan. NSX-T dapat menyediakan virtualisasi jaringan untuk lingkungan multi-cloud dan multi-hypervisor dan berfokus pada kerangka kerja dan arsitektur aplikasi yang muncul yang memiliki titik akhir dan tumpukan teknologi yang heterogen. Selain hypervisor vSphere, lingkungan ini termasuk hypervisor lainnya seperti KVM, wadah, dan bare metal.</p><p><a href=https://docs.vmware.com/en/VMware-NSX-T/2.0/nsxt_20_ncp_kubernetes.pdf>NSX-T Container Plug-in (NCP)</a> menyediakan integrasi antara NSX-T dan pembuat wadah seperti Kubernetes, serta integrasi antara NSX-T dan platform CaaS / PaaS berbasis-kontainer seperti Pivotal Container Service (PKS) dan OpenShift.</p><h3 id=nuage-networks-vcs-layanan-cloud-virtual>Nuage Networks VCS (Layanan Cloud Virtual)</h3><p><a href=http://www.nuagenetworks.net>Nuage</a> menyediakan platform SDN (Software-Defined Networking) berbasis kebijakan yang sangat skalabel. Nuage menggunakan Open vSwitch <em>open source</em> untuk data <em>plane</em> bersama dengan SDN Controller yang kaya fitur yang dibangun pada standar terbuka.</p><p>Platform Nuage menggunakan <em>overlay</em> untuk menyediakan jaringan berbasis kebijakan yang mulus antara Kubernetes Pod-Pod dan lingkungan non-Kubernetes (VM dan server <em>bare metal</em>). Model abstraksi kebijakan Nuage dirancang dengan mempertimbangkan aplikasi dan membuatnya mudah untuk mendeklarasikan kebijakan berbutir halus untuk aplikasi. Mesin analisis <em>real-time</em> platform memungkinkan pemantauan visibilitas dan keamanan untuk aplikasi Kubernetes.</p><h3 id=ovn-open-virtual-networking>OVN (Open Virtual Networking)</h3><p>OVN adalah solusi virtualisasi jaringan opensource yang dikembangkan oleh komunitas Open vSwitch. Ini memungkinkan seseorang membuat switch logis, router logis, ACL stateful, load-balancers dll untuk membangun berbagai topologi jaringan virtual. Proyek ini memiliki plugin dan dokumentasi Kubernetes spesifik di <a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a>.</p><h3 id=project-calico>Project Calico</h3><p><a href=http://docs.projectcalico.org/>Project Calico</a> adalah penyedia jaringan wadah sumber terbuka dan mesin kebijakan jaringan.</p><p>Calico menyediakan solusi jaringan dan kebijakan kebijakan jaringan yang sangat berskala untuk menghubungkan Pod Kubernetes berdasarkan prinsip jaringan IP yang sama dengan internet, untuk Linux (open source) dan Windows (milik - tersedia dari <a href=https://www.tigera.io/essentials/>Tigera</a>). Calico dapat digunakan tanpa enkapsulasi atau <em>overlay</em> untuk menyediakan jaringan pusat data skala tinggi yang berkinerja tinggi. Calico juga menyediakan kebijakan keamanan jaringan berbutir halus, berdasarkan niat untuk Pod Kubernetes melalui <em>firewall</em> terdistribusi.</p><p>Calico juga dapat dijalankan dalam mode penegakan kebijakan bersama dengan solusi jaringan lain seperti Flannel, alias <a href=https://github.com/tigera/canal>kanal</a>, atau jaringan GCE, AWS atau Azure asli.</p><h3 id=romana>Romana</h3><p><a href=http://romana.io>Romana</a> adalah jaringan sumber terbuka dan solusi otomasi keamanan yang memungkinkan kamu menggunakan Kubernetes tanpa jaringan hamparan. Romana mendukung Kubernetes <a href=/id/docs/concepts/services-networking/network-policies/>Kebijakan Jaringan</a> untuk memberikan isolasi di seluruh ruang nama jaringan.</p><h3 id=weave-net-dari-weaveworks>Weave Net dari Weaveworks</h3><p><a href=https://www.weave.works/products/weave-net/>Weave Net</a> adalah jaringan yang tangguh dan mudah digunakan untuk Kubernetes dan aplikasi yang dihostingnya. Weave Net berjalan sebagai <a href=https://www.weave.works/docs/net/latest/cni-plugin/>plug-in CNI</a> atau berdiri sendiri. Di kedua versi, itu tidak memerlukan konfigurasi atau kode tambahan untuk dijalankan, dan dalam kedua kasus, jaringan menyediakan satu alamat IP per Pod - seperti standar untuk Kubernetes.</p><h2 id=selanjutnya>Selanjutnya</h2><p>Desain awal model jaringan dan alasannya, dan beberapa rencana masa depan dijelaskan secara lebih rinci dalam <a href=https://git.k8s.io/community/contributors/design-proposals/network/networking.md>dokumen desain jaringan</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c4b1e87a84441f8a90699a345ce48d68>6 - Arsitektur Logging</h1><p>Log aplikasi dan sistem dapat membantu kamu untuk memahami apa yang terjadi di dalam klaster kamu. Log berguna untuk mengidentifikasi dan menyelesaikan masalah serta memonitor aktivitas klaster. Hampir semua aplikasi modern mempunyai sejenis mekanisme log sehingga hampir semua mesin kontainer didesain untuk mendukung suatu mekanisme <em>logging</em>. Metode <em>logging</em> yang paling mudah untuk aplikasi dalam bentuk kontainer adalah menggunakan <em>standard output</em> dan <em>standard error</em>.</p><p>Namun, fungsionalitas bawaan dari mesin kontainer atau <em>runtime</em> biasanya tidak cukup memadai sebagai solusi log. Contohnya, jika sebuah kontainer gagal, sebuah pod dihapus, atau suatu <em>node</em> mati, kamu biasanya tetap menginginkan untuk mengakses log dari aplikasimu. Oleh sebab itu, log sebaiknya berada pada penyimpanan dan <em>lifecyle</em> yang terpisah dari node, pod, atau kontainer. Konsep ini dinamakan sebagai <em>logging</em> pada level klaster. <em>Logging</em> pada level klaster ini membutuhkan <em>backend</em> yang terpisah untuk menyimpan, menganalisis, dan mengkueri log. Kubernetes tidak menyediakan solusi bawaan untuk penyimpanan data log, namun kamu dapat mengintegrasikan beragam solusi <em>logging</em> yang telah ada ke dalam klaster Kubernetes kamu.</p><p>Arsitektur <em>logging</em> pada level klaster yang akan dijelaskan berikut mengasumsikan bahwa sebuah <em>logging backend</em> telah tersedia baik di dalam maupun di luar klastermu. Meskipun kamu tidak tertarik menggunakan <em>logging</em> pada level klaster, penjelasan tentang bagaimana log disimpan dan ditangani pada node di bawah ini mungkin dapat berguna untukmu.</p><h2 id=hal-dasar-logging-pada-kubernetes>Hal dasar <em>logging</em> pada Kubernetes</h2><p>Pada bagian ini, kamu dapat melihat contoh tentang dasar <em>logging</em> pada Kubernetes yang mengeluarkan data pada <em>standard output</em>. Demonstrasi berikut ini menggunakan sebuah <a href=/examples/debug/counter-pod.yaml>spesifikasi pod</a> dengan kontainer yang akan menuliskan beberapa teks ke <em>standard output</em> tiap detik.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/debug/counter-pod.yaml download=debug/counter-pod.yaml><code>debug/counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("debug-counter-pod-yaml")' title="Copy debug/counter-pod.yaml to clipboard"></img></div><div class=includecode id=debug-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c,<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:#b44>&#39;i=0; while true; do echo &#34;$i: $(date)&#34;; i=$((i+1)); sleep 1; done&#39;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Untuk menjalankan pod ini, gunakan perintah berikut:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</span></span></code></pre></div><p>Keluarannya adalah:</p><pre tabindex=0><code>pod/counter created
</code></pre><p>Untuk mengambil log, gunakan perintah <code>kubectl logs</code> sebagai berikut:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter
</span></span></code></pre></div><p>Keluarannya adalah:</p><pre tabindex=0><code>0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><p>Kamu dapat menambahkan parameter <code>--previous</code> pada perintah <code>kubectl logs</code> untuk mengambil log dari kontainer sebelumnya yang gagal atau <em>crash</em>. Jika pod kamu memiliki banyak kontainer, kamu harus menspesifikasikan kontainer mana yang kamu ingin akses lognya dengan menambahkan nama kontainer pada perintah tersebut. Lihat <a href=/docs/reference/generated/kubectl/kubectl-commands#logs>dokumentasi <code>kubectl logs</code></a> untuk informasi lebih lanjut.</p><h2 id=node-level-logging>Node-level <em>logging</em></h2><p><img src=/images/docs/user-guide/logging/logging-node-level.png alt="Node-level logging"></p><p>Semua hal yang ditulis oleh aplikasi dalam kontainer ke <code>stdout</code> dan <code>stderr</code> akan ditangani dan diarahkan ke suatu tempat oleh mesin atau <em>engine</em> kontainer. Contohnya,mesin kontainer Docker akan mengarahkan kedua aliran tersebut ke <a href=https://docs.docker.com/engine/admin/logging/overview>suatu <em>logging driver</em></a>, yang akan dikonfigurasi pada Kubernetes untuk menuliskan ke dalam berkas dalam format json.</p><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> <em>Logging driver</em> json dari Docker memperlakukan tiap baris sebagai pesan yang terpisah. Saat menggunakan <em>logging driver</em> Docker, tidak ada dukungan untuk menangani pesan <em>multi-line</em>. Kamu harus menangani pesan <em>multi-line</em> pada level agen log atau yang lebih tinggi.</div><p>Secara <em>default</em>, jika suatu kontainer <em>restart</em>, kubelet akan menjaga kontainer yang mati tersebut beserta lognya. Namun jika suatu pod dibuang dari <em>node</em>, maka semua hal dari kontainernya juga akan dibuang, termasuk lognya.</p><p>Hal lain yang perlu diperhatikan dalam <em>logging</em> pada level <em>node</em> adalah implementasi rotasi log, sehingga log tidak menghabiskan semua penyimpanan yang tersedia pada <em>node.</em> Kubernetes saat ini tidak bertanggung jawab dalam melakukan rotasi log, namun <em>deployment tool</em> seharusnya memberikan solusi terhadap masalah tersebut.
Contohnya, pada klaster Kubernetes, yang di <em>deployed</em> menggunakan <code>kube-up.sh</code>, terdapat alat bernama <a href=https://linux.die.net/man/8/logrotate><code>logrotate</code></a> yang dikonfigurasi untuk berjalan tiap jamnya. Kamu juga dapat menggunakan <em>runtime</em> kontainer untuk melakukan rotasi log otomatis, misalnya menggunakan <code>log-opt</code> Docker.
Pada <code>kube-up.sh</code>, metode terakhir digunakan untuk COS <em>image</em> pada GCP, sedangkan metode pertama digunakan untuk lingkungan lainnya. Pada kedua metode, secara <em>default</em> akan dilakukan rotasi pada saat berkas log melewati 10MB.</p><p>Sebagai contoh, kamu dapat melihat informasi lebih rinci tentang bagaimana <code>kube-up.sh</code> mengatur <em>logging</em> untuk COS <em>image</em> pada GCP yang terkait dengan <a href=https://github.com/kubernetes/kubernetes/blob/main/cluster/gce/gci/configure-helper.sh><em>script</em></a>.</p><p>Saat kamu menjalankan perintah <a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code></a> seperti pada contoh tadi, kubelet di <em>node</em> tersebut akan menangani permintaan untuk membaca langsung isi berkas log sebagai respon.</p><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Saat ini, jika suatu sistem eksternal telah melakukan rotasi, hanya konten dari berkas log terbaru yang akan tersedia melalui perintah <code>kubectl logs</code>. Contoh, jika terdapat sebuah berkas 10MB, <code>logrotate</code> akan melakukan rotasi sehingga akan ada dua buah berkas, satu dengan ukuran 10MB, dan satu berkas lainnya yang kosong. Maka <code>kubectl logs</code> akan mengembalikan respon kosong.</div><h3 id=komponen-sistem-log>Komponen sistem log</h3><p>Terdapat dua jenis komponen sistem: yaitu yang berjalan di dalam kontainer dan komponen lain yang tidak berjalan di dalam kontainer. Sebagai contoh:</p><ul><li>Kubernetes <em>scheduler</em> dan kube-proxy berjalan di dalam kontainer.</li><li>Kubelet dan <em>runtime</em> kontainer, contohnya Docker, tidak berjalan di dalam kontainer.</li></ul><p>Pada mesin yang menggunakan systemd, kubelet dan runtime <em>runtime</em> menulis ke journald. Jika systemd tidak tersedia, keduanya akan menulis ke berkas <code>.log</code> pada folder <code>/var/log</code>.
Komponen sistem di dalam kontainer akan selalu menuliskan ke folder <code>/var/log</code>, melewati mekanisme <em>default logging</em>. Mereka akan menggunakan <em>logging library</em> <a href=https://github.com/kubernetes/klog>klog</a>.
Kamu dapat menemukan konvensi tentang tingkat kegawatan <em>logging</em> untuk komponen-komponen tersebut pada <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>dokumentasi <em>development logging</em></a>.</p><p>Seperti halnya pada log kontainer, komponen sistem yang menuliskan log pada folder <code>/var/log</code> juga harus melakukan rotasi log. Pada klaster Kubernetes yang menggunakan <code>kube-up.sh</code>, log tersebut telah dikonfigurasi dan akan dirotasi oleh <code>logrotate</code> secara harian atau saat ukuran log melebihi 100MB.</p><h2 id=arsitektur-klaster-level-logging>Arsitektur klaster-level <em>logging</em></h2><p>Meskipun Kubernetes tidak menyediakan solusi bawaan untuk <em>logging</em> level klaster, ada beberapa pendekatan yang dapat kamu pertimbangkan. Berikut beberapa diantaranya:</p><ul><li>Menggunakan agen <em>logging</em> pada level <em>node</em> yang berjalan pada setiap <em>node</em>.</li><li>Menggunakan kontainer <em>sidecar</em> khusus untuk <em>logging</em> aplikasi di dalam pod.</li><li>Mengeluarkan log langsung ke <em>backend</em> dari dalam aplikasi</li></ul><h3 id=menggunakan-agen-node-level-logging>Menggunakan agen node-level <em>logging</em></h3><p><img src=/images/docs/user-guide/logging/logging-with-node-agent.png alt="Menggunakan agen node-level logging"></p><p>Kamu dapat mengimplementasikan klaster-level <em>logging</em> dengan menggunakan agen yang berjalan pada setiap <em>node</em>. Agen <em>logging</em> merupakan perangkat khusus yang akan mengekspos log atau mengeluarkan log ke <em>backend</em>. Umumnya agen <em>logging</em> merupakan kontainer yang memiliki akses langsung ke direktori tempat berkas log berada dari semua kontainer aplikasi yang berjalan pada <em>node</em> tersebut.</p><p>Karena agen <em>logging</em> harus berjalan pada setiap <em>node</em>, umumnya dilakukan dengan menggunakan replika DaemonSet, <em>manifest</em> pod, atau menjalankan proses khusus pada <em>node</em>. Namun dua cara terakhir sudah dideprekasi dan sangat tidak disarankan.</p><p>Menggunakan agen <em>logging</em> pada level <em>node</em> merupakan cara yang paling umum dan disarankan untuk klaster Kubernetes. Hal ini karena hanya dibutuhkan satu agen tiap node dan tidak membutuhkan perubahan apapun dari sisi aplikasi yang berjalan pada <em>node</em>. Namun, node-level <em>logging</em> hanya dapat dilakukan untuk aplikasi yang menggunakan <em>standard output</em> dan <em>standard error</em>.</p><p>Kubernetes tidak menspesifikasikan khusus suatu agen <em>logging</em>, namun ada dua agen <em>logging</em> yang dimasukkan dalam rilis Kubernetes: <a href=/docs/user-guide/logging/stackdriver>Stackdriver Logging</a> untuk digunakan pada Google Cloud Platform, dan <a href=/docs/user-guide/logging/elasticsearch>Elasticsearch</a>. Kamu dapat melihat informasi dan instruksi pada masing-masing dokumentasi. Keduanya menggunakan <a href=http://www.fluentd.org/>fluentd</a> dengan konfigurasi kustom sebagai agen pada <em>node</em>.</p><h3 id=menggunakan-kontainer-sidecar-dengan-agen-logging>Menggunakan kontainer <em>sidecar</em> dengan agen <em>logging</em></h3><p>Kamu dapat menggunakan kontainer <em>sidecar</em> dengan salah satu cara berikut:</p><ul><li>Kontainer <em>sidecar</em> mengeluarkan log aplikasi ke <code>stdout</code> miliknya sendiri.</li><li>Kontainer <em>sidecar</em> menjalankan agen <em>logging</em> yang dikonfigurasi untuk mengambil log dari aplikasi kontainer.</li></ul><h4 id=kontainer-streaming-sidecar>Kontainer <em>streaming</em> <em>sidecar</em></h4><p><img src=/images/docs/user-guide/logging/logging-with-streaming-sidecar.png alt="Kontainer sidecar dengan kontainer streaming"></p><p>Kamu dapat memanfaatkan kubelet dan agen <em>logging</em> yang telah berjalan pada tiap <em>node</em> dengan menggunakan kontainer <em>sidecar</em>. Kontainer <em>sidecar</em> dapat membaca log dari sebuah berkas, <em>socket</em> atau journald. Tiap kontainer <em>sidecar</em> menuliskan log ke <code>stdout</code> atau <code>stderr</code> mereka sendiri.</p><p>Dengan menggunakan cara ini kamu dapat memisahkan aliran log dari bagian-bagian yang berbeda dari aplikasimu, yang beberapa mungkin tidak mendukung log ke <code>stdout</code> dan <code>stderr</code>. Perubahan logika aplikasimu dengan menggunakan cara ini cukup kecil, sehingga hampir tidak ada <em>overhead</em>. Selain itu, karena <code>stdout</code> dan <code>stderr</code> ditangani oleh kubelet, kamu juga dapat menggunakan alat bawaan seperti <code>kubectl logs</code>.</p><p>Sebagai contoh, sebuah pod berjalan pada satu kontainer tunggal, dan kontainer menuliskan ke dua berkas log yang berbeda, dengan dua format yang berbeda pula. Berikut ini <em>file</em> konfigurasi untuk Pod:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/logging/two-files-counter-pod.yaml download=admin/logging/two-files-counter-pod.yaml><code>admin/logging/two-files-counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-yaml")' title="Copy admin/logging/two-files-counter-pod.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Hal ini akan menyulitkan untuk mengeluarkan log dalam format yang berbeda pada aliran log yang sama, meskipun kamu dapat me-<em>redirect</em> keduanya ke <code>stdout</code> dari kontainer. Sebagai gantinya, kamu dapat menggunakan dua buah kontainer <em>sidecar</em>. Tiap kontainer <em>sidecar</em> dapat membaca suatu berkas log tertentu dari <em>shared volume</em> kemudian mengarahkan log ke <code>stdout</code>-nya sendiri.</p><p>Berikut <em>file</em> konfigurasi untuk pod yang memiliki dua buah kontainer <em>sidecard</em>:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml download=admin/logging/two-files-counter-pod-streaming-sidecar.yaml><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-streaming-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-streaming-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/1.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/2.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Saat kamu menjalankan pod ini, kamu dapat mengakses tiap aliran log secara terpisah dengan menjalankan perintah berikut:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-1
</span></span></code></pre></div><pre tabindex=0><code>0: Mon Jan  1 00:00:00 UTC 2001
1: Mon Jan  1 00:00:01 UTC 2001
2: Mon Jan  1 00:00:02 UTC 2001
...
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-2
</span></span></code></pre></div><pre tabindex=0><code>Mon Jan  1 00:00:00 UTC 2001 INFO 0
Mon Jan  1 00:00:01 UTC 2001 INFO 1
Mon Jan  1 00:00:02 UTC 2001 INFO 2
...
</code></pre><p>Agen node-level yang terpasang di klastermu akan mengambil aliran log tersebut secara otomatis tanpa perlu melakukan konfigurasi tambahan. Bahkan jika kamu mau, kamu dapat mengonfigurasi agen untuk melakukan <em>parse</em> baris log tergantung dari kontainer sumber awalnya.</p><p>Sedikit catatan, meskipun menggunakan memori dan CPU yang cukup rendah (sekitar beberapa milicore untuk CPU dan beberapa megabytes untuk memori), penulisan log ke <em>file</em> kemudian mengalirkannya ke <code>stdout</code> dapat berakibat penggunaan disk yang lebih besar. Jika kamu memiliki aplikasi yang menuliskan ke <em>file</em> tunggal, umumnya lebih baik menggunakan <code>/dev/stdout</code> sebagai tujuan daripada menggunakan pendekatan dengan kontainer <em>sidecar</em>.</p><p>Kontainer <em>sidecar</em> juga dapat digunakan untuk melakukan rotasi berkas log yang tidak dapat dirotasi oleh aplikasi itu sendiri. Contoh dari pendekatan ini adalah sebuah kontainer kecil yang menjalankan rotasi log secara periodik. Namun, direkomendasikan untuk menggunakan <code>stdout</code> dan <code>stderr</code> secara langsung dan menyerahkan kebijakan rotasi dan retensi pada kubelet.</p><h4 id=kontainer-sidecar-dengan-agen-logging>Kontainer <em>sidecar</em> dengan agen <em>logging</em></h4><p><img src=/images/docs/user-guide/logging/logging-with-sidecar-agent.png alt="Kontainer sidecar dengan agen logging"></p><p>Jika agen node-level <em>logging</em> tidak cukup fleksible untuk kebutuhanmu, kamu dapat membuat kontainer <em>sidecar</em> dengan agen <em>logging</em> yang terpisah, yang kamu konfigurasi spesifik untuk berjalan dengan aplikasimu.</p><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Menggunakan agen <em>logging</em> di dalam kontainer <em>sidecar</em> dapat berakibat penggunaan <em>resource</em> yang signifikan. Selain itu, kamu tidak dapat mengakses log itu dengan menggunakan perintah <code>kubectl logs</code>, karena mereka tidak dikontrol oleh kubelet.</div><p>Sebagai contoh, kamu dapat menggunakan <a href=/docs/tasks/debug-application-cluster/logging-stackdriver/>Stackdriver</a>,
yang menggunakan fluentd sebagai agen <em>logging</em>. Berikut ini dua <em>file</em> konfigurasi yang dapat kamu pakai untuk mengimplementasikan cara ini. <em>File</em> yang pertama berisi sebuah <a href=/id/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMap</a> untuk mengonfigurasi fluentd.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/logging/fluentd-sidecar-config.yaml download=admin/logging/fluentd-sidecar-config.yaml><code>admin/logging/fluentd-sidecar-config.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-fluentd-sidecar-config-yaml")' title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard"></img></div><div class=includecode id=admin-logging-fluentd-sidecar-config-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fluentd.conf</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/1.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/1.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format1
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/2.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/2.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format2
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;match **&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type google_cloud
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/match&gt;</span><span style=color:#bbb>    
</span></span></span></code></pre></div></div></div><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Konfigurasi fluentd berada diluar cakupan artikel ini. Untuk informasi lebih lanjut tentang cara mengonfigurasi fluentd, silakan lihat <a href=http://docs.fluentd.org/>dokumentasi resmi fluentd </a>.</div><p><em>File</em> yang kedua mendeskripsikan sebuah pod yang memiliki kontainer <em>sidecar</em> yang menjalankan fluentd. Pod ini melakukan <em>mount</em> sebuah volume yang akan digunakan fluentd untuk mengambil data konfigurasinya.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml download=admin/logging/two-files-counter-pod-agent-sidecar.yaml><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-agent-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-agent-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-agent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/fluentd-gcp:1.30<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>FLUENTD_ARGS<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>-c /etc/fluentd-config/fluentd.conf<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Setelah beberapa saat, kamu akan mendapati pesan log pada <em>interface</em> Stackdriver.</p><p>Ingat, ini hanya contoh saja dan kamu dapat mengganti fluentd dengan agen <em>logging</em> lainnya, yang dapat membaca sumber apa saja dari dalam kontainer aplikasi.</p><h3 id=mengekspos-log-langsung-dari-aplikasi>Mengekspos log langsung dari aplikasi</h3><p><img src=/images/docs/user-guide/logging/logging-from-application.png alt="Mengekspos log langsung dari aplikasi"></p><p>Kamu dapat mengimplementasikan klaster-level <em>logging</em> dengan mengekspos atau mengeluarkan log langsung dari tiap aplikasi; namun cara implementasi mekanisme <em>logging</em> tersebut diluar cakupan dari Kubernetes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-cbfd3654996eae9fcdef009f70fa83f0>7 - Metrik untuk Komponen Sistem Kubernetes</h1><p>Metrik dari komponen sistem dapat memberikan gambaran yang lebih baik tentang apa
yang sedang terjadi di dalam sistem. Metrik sangat berguna untuk membuat dasbor (<em>dashboard</em>)
dan peringatan (<em>alert</em>).</p><p>Komponen Kubernetes mengekspos metrik dalam <a href=https://prometheus.io/docs/instrumenting/exposition_formats/>format Prometheus</a>.
Format ini berupa teks biasa yang terstruktur, dirancang agar orang dan mesin dapat membacanya.</p><h2 id=metrik-metrik-dalam-kubernetes>Metrik-metrik dalam Kubernetes</h2><p>Dalam kebanyakan kasus, metrik tersedia pada <em>endpoint</em> <code>/metrics</code> dari server HTTP.
Untuk komponen yang tidak mengekspos <em>endpoint</em> secara bawaan, <em>endpoint</em> tersebut dapat diaktifkan
dengan menggunakan opsi <code>--bind-address</code>.</p><p>Contoh-contoh untuk komponen tersebut adalah:</p><ul><li><a class=glossary-tooltip title='Komponen control plane yang menjalankan pengontrol.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li><li><a class=glossary-tooltip title='kube-proxy merupakan proksi jaringan yang berjalan pada setiap node di dalam klaster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a></li><li><a class=glossary-tooltip title='Komponen control plane yang mengekspos API Kubernetes. Merupakan front-end dari control plane Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-apiserver/ target=_blank aria-label=kube-apiserver>kube-apiserver</a></li><li><a class=glossary-tooltip title='Komponen control plane yang bertugas mengamati Pod baru yang belum ditempatkan di node manapun dan kemudian memilihkan node di mana Pod baru tersebut akan dijalankan.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li><li><a class=glossary-tooltip title='Agen yang dijalankan pada setiap node di klaster yang bertugas untuk memastikan kontainer dijalankan di dalam Pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a></li></ul><p>Di dalam lingkungan produksi, kamu mungkin ingin mengonfigurasi <a href=https://prometheus.io/>Server Prometheus</a>
atau pengambil metrik (<em>metrics scraper</em>) lainnya untuk mengumpulkan metrik-metrik ini secara berkala
dan membuatnya tersedia dalam semacam pangkalan data deret waktu (<em>time series database</em>).</p><p>Perlu dicatat bahwa <a class=glossary-tooltip title='Agen yang dijalankan pada setiap node di klaster yang bertugas untuk memastikan kontainer dijalankan di dalam Pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
juga mengekspos metrik pada <em>endpoint-endpoint</em> seperti <code>/metrics/cadvisor</code>,
<code>/metrics/resource</code> dan <code>/metrics/probes</code>. Metrik-metrik tersebut tidak memiliki
siklus hidup yang sama.</p><p>Jika klastermu menggunakan <a class=glossary-tooltip title='Mengelola keputusan otorisasi, memungkinkan admin untuk mengonfigurasi kebijakan akses secara dinamis melalui API Kubernetes.' data-toggle=tooltip data-placement=top href=/id/docs/reference/access-authn-authz/rbac/ target=_blank aria-label=RBAC>RBAC</a>,
maka membaca metrik memerlukan otorisasi melalui <em>user</em>, <em>group</em>, atau
ServiceAccount dengan ClusterRole yang memperbolehkan untuk mengakses <code>/metrics</code>.</p><p>Sebagai contoh:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prometheus<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>nonResourceURLs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/metrics&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- get<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=siklus-hidup-metrik>Siklus hidup metrik</h2><p>Metrik alfa (<em>alpha</em>) → Metrik stabil → Metrik usang (<em>deprecated</em>) → Metrik tersembunyi → Metrik terhapus</p><p>Metrik alfa tidak memiliki jaminan stabilitas. Metrik ini
dapat dimodifikasi atau dihapus kapan saja.</p><p>Metrik stabil dijamin tidak akan mengalami perubahan. Hal ini berarti:</p><ul><li>Metrik stabil tanpa penanda usang (<em>deprecated signature</em>) tidak akan dihapus ataupun diganti namanya</li><li>Jenis metrik stabil tidak akan dimodifikasi</li></ul><p>Metrik usang dijadwalkan untuk dihapus, tetapi masih tersedia untuk digunakan.
Metrik ini mencakup anotasi versi di mana metrik ini dianggap menjadi usang.</p><p>Sebagai contoh:</p><ul><li><p>Sebelum menjadi usang</p><pre tabindex=0><code># HELP some_counter this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li><li><p>Setelah menjadi usang</p><pre tabindex=0><code># HELP some_counter (Deprecated since 1.15.0) this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li></ul><p>Metrik tersembunyi tidak lagi dipublikasikan untuk pengambilan metrik (<em>scraping</em>), tetapi masih tersedia untuk digunakan. Untuk menggunakan metrik tersembunyi, lihat bagian <a href=#menampilkan-metrik-tersembunyi>Menampilkan metrik tersembunyi</a>.</p><p>Metrik yang terhapus tidak lagi dipublikasikan dan tidak dapat digunakan lagi.</p><h2 id=menampilkan-metrik-tersembunyi>Menampilkan metrik tersembunyi</h2><p>Seperti yang dijelaskan di atas, admin dapat mengaktifkan metrik tersembunyi melalui opsi baris perintah pada biner (program) tertentu. Ini dimaksudkan untuk digunakan sebagai jalan keluar bagi admin jika mereka melewatkan migrasi metrik usang dalam rilis terakhir.</p><p>Opsi <code>show-hidden-metrics-for-version</code> menerima input versi yang kamu inginkan untuk menampilkan metrik usang dalam rilis tersebut. Versi tersebut dinyatakan sebagai x.y, di mana x adalah versi mayor, y adalah versi minor. Versi <em>patch</em> tidak diperlukan meskipun metrik dapat menjadi usang dalam rilis <em>patch</em>, alasannya adalah kebijakan penandaan metrik usang dijalankan terhadap rilis minor.</p><p>Opsi tersebut hanya dapat menerima input versi minor sebelumnya sebagai nilai. Semua metrik yang disembunyikan di versi sebelumnya akan dikeluarkan jika admin mengatur versi sebelumnya ke <code>show-hidden-metrics-for-version</code>. Versi yang terlalu lama tidak diperbolehkan karena melanggar kebijakan untuk metrik usang.</p><p>Ambil metrik <code>A</code> sebagai contoh, di sini diasumsikan bahwa <code>A</code> sudah menjadi usang di versi 1.n. Berdasarkan kebijakan metrik usang, kita dapat mencapai kesimpulan berikut:</p><ul><li>Pada rilis <code>1.n</code>, metrik menjadi usang, dan dapat dikeluarkan secara bawaan.</li><li>Pada rilis <code>1.n+1</code>, metrik disembunyikan secara bawaan dan dapat dikeluarkan dengan baris perintah <code>show-hidden-metrics-for-version=1.n</code>.</li><li>Pada rilis <code>1.n+2</code>, metrik harus dihapus dari <em>codebase</em>. Tidak ada jalan keluar lagi.</li></ul><p>Jika kamu meningkatkan versi dari rilis <code>1.12</code> ke <code>1.13</code>, tetapi masih bergantung pada metrik <code>A</code> yang usang di <code>1.12</code>, kamu harus mengatur metrik tersembunyi melalui baris perintah: <code>--show-hidden-metrics = 1.12</code> dan ingatlah untuk menghapus ketergantungan terhadap metrik ini sebelum meningkatkan versi rilis ke <code>1.14</code>.</p><h2 id=menonaktifkan-metrik-akselerator>Menonaktifkan metrik akselerator</h2><p>kubelet mengumpulkan metrik akselerator melalui cAdvisor. Untuk mengumpulkan metrik ini, untuk akselerator seperti GPU NVIDIA, kubelet membuka koneksi dengan <em>driver</em> GPU. Ini berarti untuk melakukan perubahan infrastruktur (misalnya, pemutakhiran <em>driver</em>), administrator klaster perlu menghentikan agen kubelet.</p><p>Pengumpulkan metrik akselerator sekarang menjadi tanggung jawab vendor dibandingkan kubelet. Vendor harus menyediakan sebuah kontainer untuk mengumpulkan metrik dan mengeksposnya ke layanan metrik (misalnya, Prometheus).</p><p><a href=/docs/reference/command-line-tools-reference/feature-gates/>Gerbang fitur <code>DisableAcceleratorUsageMetrics</code></a> menonaktifkan metrik yang dikumpulkan oleh kubelet, dengan <a href=https://github.com/kubernetes/enhancements/tree/411e51027db842355bd489691af897afc1a41a5e/keps/sig-node/1867-disable-accelerator-usage-metrics#graduation-criteria>lini masa (<em>timeline</em>) untuk mengaktifkan fitur ini secara bawaan</a>.</p><h2 id=metrik-komponen>Metrik komponen</h2><h3 id=metrik-kube-controller-manager>Metrik kube-controller-manager</h3><p>Metrik <em>controller manager</em> memberikan gambaran penting
tentang kinerja dan kesehatan <em>controller manager</em>. Metrik ini mencakup metrik
<em>runtime</em> bahasa Go yang umum seperti jumlah go_routine dan metrik khusus
pengontrol seperti latensi permintaan etcd atau latensi API Cloudprovider
(AWS, GCE, OpenStack) yang dapat digunakan untuk mengukur kesehatan klaster.</p><p>Mulai dari Kubernetes 1.7, metrik Cloudprovider yang detail tersedia untuk
operasi penyimpanan untuk GCE, AWS, Vsphere, dan OpenStack.
Metrik ini dapat digunakan untuk memantau kesehatan operasi <em>persistent volume</em>.</p><p>Misalnya, untuk GCE metrik-metrik berikut ini dipanggil:</p><pre tabindex=0><code>cloudprovider_gce_api_request_duration_seconds { request = &#34;instance_list&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_insert&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_delete&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;attach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;detach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;list_disk&#34;}
</code></pre><h3 id=metrik-kube-scheduler>Metrik kube-scheduler</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code></div><p>Penjadwal mengekspos metrik opsional yang melaporkan sumber daya yang diminta dan limit yang diinginkan dari semua pod yang berjalan. Metrik ini dapat digunakan untuk membangun dasbor perencanaan kapasitas, mengevaluasi limit penjadwalan yang digunakan saat ini atau secara historis, dengan cepat mengidentifikasi beban kerja yang tidak dapat dijadwalkan karena kurangnya sumber daya, dan membandingkan permintaan sumber daya oleh pod dengan penggunaannya yang aktual.</p><p>kube-scheduler mengidentifikasi <a href=/docs/concepts/configuration/manage-resources-containers/>permintaan dan limit</a> sumber daya yang dikonfigurasi untuk setiap Pod; jika permintaan atau limit bukan nol, kube-scheduler akan melaporkan deret waktu (<em>timeseries</em>) metrik. Deret waktu diberi label dengan:</p><ul><li>namespace</li><li>nama pod</li><li>node di mana pod dijadwalkan atau <em>string</em> kosong jika belum dijadwalkan</li><li>prioritas</li><li>penjadwal yang ditugaskan untuk pod itu</li><li>nama dari sumber daya (misalnya, <code>cpu</code>)</li><li>satuan dari sumber daya jika diketahui (misalnya, <code>cores</code>)</li></ul><p>Setelah pod selesai (memiliki <code>restartPolicy</code> <code>Never</code> atau <code>OnFailure</code> dan berada dalam fase pod <code>Succeeded</code> atau <code>Failed</code>, atau telah dihapus dan semua kontainer dalam keadaan Terminated) deret metrik tidak lagi dilaporkan karena penjadwal sekarang sudah dibebaskan untuk menjadwalkan pod lain untuk dijalankan. Metrik yang dibahas pada bagian ini dikenal sebagai <code>kube_pod_resource_request</code> dan <code>kube_pod_resource_limit</code>.</p><p>Metrik diekspos melalui <em>endpoint</em> HTTP <code>/metrics/resources</code> dan memerlukan otorisasi yang sama seperti endpoint <code>/metrics</code>
pada penjadwal. Kamu harus menggunakan opsi <code>--show-hidden-metrics-for-version=1.20</code> untuk mengekspos metrik-metrik stabilitas alfa ini.</p><h2 id=selanjutnya>Selanjutnya</h2><ul><li>Baca tentang <a href=https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format>format teks Prometheus</a> untuk berbagai metrik</li><li>Baca tentang <a href=/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior>kebijakan <em>deprecation</em> Kubernetes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2e05a56491965ae320c2662590b2ca18>8 - Konfigurasi Garbage Collection pada kubelet</h1><p><em>Garbage collection</em> merupakan fitur kubelet yang sangat bermanfaat, yang akan membersihkan <em>image-image</em> dan juga kontainer-kontainer
yang tidak lagi digunakan. Kubelet akan melakukan <em>garbage collection</em> untuk kontainer setiap satu menit dan <em>garbage collection</em> untuk
<em>image</em> setiap lima menit.</p><p>Perangkat <em>garbage collection</em> eksternal tidak direkomendasikan karena perangkat tersebut berpotensi merusak perilaku kubelet dengan
menghilangkan kontainer-kontainer yang sebenarnya masih diperlukan.</p><h2 id=garbage-collection-untuk-image><em>Garbage Collection</em> untuk <em>Image</em></h2><p>Kubernetes mengelola <em>lifecycle</em> untuk seluruh <em>image</em> melalui <em>imageManager</em>, dengan bantuan cadvisor.</p><p><em>Policy</em> untuk melakukan <em>garbage collection</em> memperhatikan dua hal: <code>HighThresholdPercent</code> dan <code>LowThresholdPercent</code>.
Penggunaan disk yang melewati batas atas (<em>high threshold</em>) akan men-<em>trigger</em> <em>garbage collection</em>.
<em>Garbage collection</em> akan mulai menghapus dari <em>image-image</em> yang paling jarang digunakan (<em>least recently used</em>)
sampai menemui batas bawah (<em>low threshold</em>) kembali.</p><h2 id=garbage-collection-untuk-kontainer><em>Garbage Collection</em> untuk Kontainer</h2><p><em>Policy</em> untuk melakukan <em>garbage collection</em> pada kontainer memperhatikan tiga variabel yang ditentukan oleh pengguna (<em>user-defined</em>).
<code>MinAge</code> merupakan umur minimal dimana suatu kontainer dapat terkena <em>garbage collection</em>.
<code>MaxPerPodContainer</code> merupakan jumlah maksimum yang diperbolehkan untuk setiap pod (UID, container name) <em>pair</em> memiliki
kontainer-kontainer yang sudah mati (<em>dead containers</em>). <code>MaxContainers</code> merupakan jumlah maksimal total dari seluruh kontainer yang sudah mati.
Semua variabel ini dapat dinonaktifkan secara individual, dengan mengatur <code>MinAge</code> ke angka nol serta mengatur <code>MaxPerPodContainer</code> dan <code>MaxContainers</code>
ke angka di bawah nol.</p><p>Kubelet akan mengambil tindakan untuk kontainer-kontainer yang tidak dikenal, sudah dihapus, atau diluar batasan-batasan yang diatur
sebelumnya melalui <em>flag</em>. Kontainer-kontainer yang paling lama (tertua) biasanya akan dihapus terlebih dahulu. <code>MaxPerPodContainer</code> dan <code>MaxContainer</code>
berpotensi mengalami konflik satu sama lain pada situasi saat menjaga jumlah maksimal kontainer per pod (<code>MaxPerPodContainer</code>) akan melebihi
jumlah kontainer mati (<em>dead containers</em>) yang diperbolehkan (<code>MaxContainers</code>).
<code>MaxPerPodContainer</code> dapat diatur sedemikian rupa dalam situasi ini: Seburuk-buruhknya dengan melakukan <em>downgrade</em> <code>MaxPerPodContainer</code> ke angka 1
dan melakukan <em>evict</em> kontainer-kontainer yang paling lama. Selain itu, kontainer-kontainer milik Pod yang telah dihapus akan dihilangkan
saat umur mereka telah melebihi <code>MinAge</code>.</p><p>Kontainer-kontainer yang tidak dikelola oleh kubelet akan terbebas dari <em>garbage collection</em>.</p><h2 id=konfigurasi-pengguna>Konfigurasi Pengguna</h2><p>Para pengguna dapat mengatur <em>threshold-threshold</em> untuk melakukan <em>tuning</em> pada <em>garbage collection image</em>
melalui <em>flag-flag</em> kubelet sebagai berikut:</p><ol><li><code>image-gc-high-threshold</code>, persentase dari penggunaan disk yang men-<em>trigger</em> proses <em>garbage collection</em> untuk <em>image</em>.
<em>Default</em>-nya adalah 85%.</li><li><code>image-gc-low-threshold</code>, persentase dari penggunaan disk dimana <em>garbage collection</em> berusaha menghapus <em>image</em>.
<em>Default</em>-nya adalah 80%.</li></ol><p>Kami juga memperbolehkan para pengguna untuk menyesuaikan <em>policy garbage collection</em> melalui
<em>flag-flag</em> kubelet sebagai berikut:</p><ol><li><code>minimum-container-ttl-duration</code>, umur minimal untuk setiap kontainer yang sudah selesai (<em>finished</em>) sebelum
terkena <em>garbage collection</em>. <em>Default</em>-nya adalah 0 menit, yang berarti setiap kontainer yang telah selesai akan
terkena <em>garbage collection</em>.</li><li><code>maximum-dead-containers-per-container</code>, jumlah maksimal dari kontainer-kontainer lama yang diperbolehkan ada
secara global. <em>Default</em>-nya adalah -1, yang berarti tidak ada batasannya untuk global.</li></ol><p>Kontainer-kontainer dapat berpotensi terkena <em>garbage collection</em> sebelum kegunaannya telah usang. Kontainer-kontainer
ini memliki log dan data lainnya yang bisa saja berguna saat <em>troubleshoot</em>. Sangat direkomendasikan untuk menetapkan
angka yang cukup besar pada <code>maximum-dead-containers-per-container</code>, untuk memperbolehkan paling tidak 1 kontainer mati
untuk dijaga (<em>retained</em>) per jumlah kontainer yang diharapkan. Angka yang lebih besar untuk <code>maximum-dead-containers</code>
juga direkomendasikan untuk alasan serupa.
Lihat <a href=https://github.com/kubernetes/kubernetes/issues/13287>isu ini</a> untuk penjelasan lebih lanjut.</p><h2 id=deprecation><em>Deprecation</em></h2><p>Beberapa fitur <em>Garbage Collection</em> pada kubelet di laman ini akan digantikan oleh fitur <em>eviction</em> nantinya, termasuk:</p><table><thead><tr><th><em>Flag Existing</em></th><th><em>Flag</em> Baru</th><th>Alasan</th></tr></thead><tbody><tr><td><code>--image-gc-high-threshold</code></td><td><code>--eviction-hard</code> atau <code>--eviction-soft</code></td><td>sinyal <em>eviction</em> yang ada (<em>existing</em>) dapat men-<em>trigger</em> <em>garbage collection</em></td></tr><tr><td><code>--image-gc-low-threshold</code></td><td><code>--eviction-minimum-reclaim</code></td><td>hal serupa dapat diperoleh dengan <em>eviction reclaim</em></td></tr><tr><td><code>--maximum-dead-containers</code></td><td></td><td><em>deprecated</em> saat log yang telah usang tersimpan di luar konteks kontainer</td></tr><tr><td><code>--maximum-dead-containers-per-container</code></td><td></td><td><em>deprecated</em> saat log yang telah usang tersimpan di luar konteks kontainer</td></tr><tr><td><code>--minimum-container-ttl-duration</code></td><td></td><td><em>deprecated</em> saat log yang telah usang tersimpan di luar konteks kontainer</td></tr><tr><td><code>--low-diskspace-threshold-mb</code></td><td><code>--eviction-hard</code> atau <code>eviction-soft</code></td><td><em>eviction</em> memberi generalisasi <em>threshold</em> disk untuk <em>resource-resource</em> lainnya</td></tr><tr><td><code>--outofdisk-transition-frequency</code></td><td><code>--eviction-pressure-transition-period</code></td><td><em>eviction</em> memberi generalisasi transisi tekanan <em>disk</em> (<em>disk pressure</em>)untuk <em>resource-resource</em> lainnya</td></tr></tbody></table><h2 id=selanjutnya>Selanjutnya</h2><p>Lihat <a href=/docs/tasks/administer-cluster/out-of-resource/>Konfigurasi untuk Menangani Kehabisan <em>Resource</em></a> untuk penjelasan lebih lanjut.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3003324f360fdacc06ca144e57ff0e97>9 - Federation</h1><blockquote class="deprecation_file_warning callout"><div><h4>Sudah usang</h4><p>Penggunaan <code>Federation V1</code> sangat tidak disarankan. <code>Federation V1</code> tidak pernah masuk dalam GA dan tidak lagi dikembangkan secara aktif. Dokumentasi hanya disediakan sebatas data artefak saja.</p><p>Untuk informasi lebih lanjut mengenai hal ini dan penggantinya kamu dapat membaca <a href=https://github.com/kubernetes-sigs/federation-v2>Kubernetes Federation v2</a>.</p></div></blockquote><p>Laman ini menjelaskan alasan dan cara penggunaan <em>federation</em> untuk melakukan manajemen
klaster Kubernetes.</p><h2 id=kenapa-federation>Kenapa <em>Federation</em> ?</h2><p><em>Federation</em> membuat proses manajemen klaster multipel menjadi lebih mudah.
<em>Federation</em> mencapai hal ini dengan cara menyediakan 2 buah fondasi:</p><ul><li>Melakukan sinkronisasi <em>resource</em> di seluruh klaster: <em>Federation</em>
menyediakan kemampuan untuk melakukan sinkronisasi <em>resources</em> pada <em>multiple</em>
klaster. Sebagai contoh, kamu dapat memastikan <em>Deployment</em> yang sama
tersedia pada klaster multipel.</li><li><em>Cross</em> <em>cluster</em> <em>Discovery</em>: <em>Federation</em> menyediakan kemampuan untuk melakukan
konfigurasi otomatis server DNS dan <em>load balancer</em> dari semua klaster.
Misalnya, kamu dapat memastikan bahwa sebuah VIP atau DNS global dapat digunakan
untuk mengakses <em>backend</em> dari klaster multipel.</li></ul><p>Beberapa penggunaan <em>federation</em> adalah sebagai berikut:</p><ul><li><em>High Availability</em>: Melakukan <em>load balance</em> di seluruh klaster serta
melakukan konfigurasi otomatis server DNS dan <em>load balancer</em>, <em>federation</em>
meminimalisasi dampak yang terjadi apabila terjadi kegagalan klaster.</li><li>Mencegah <em>lock-in</em> yang terjadi akibat penyedia layanan: Dengan cara mempermudah
proses migrasi antar klaster.</li></ul><p>Manfaat <em>federation</em> tidak akan terlalu kelihatan kecuali kamu memiliki beberapa klaster.
Beberapa alasan kenapa kamu butuh beberapa klaster adalah:</p><ul><li><em>Latency</em> yang rendah: Memiliki klaster yang berada di <em>region</em> yang berbeda
meminimalisasi <em>latency</em> dengan cara menyajikan konten ke pengguna
berdasarkan <em>region</em> yang paling dekat dengan pengguna tersebut.</li><li>Isolasi <em>fault</em>: Akan lebih baik apabila kita memiliki beberapa klaster kecil
dibandingkan sebuah klaster besar untuk melakukan isolasi <em>fault</em> (misalnya saja
klaster ini bisa saja berada di <em>availability</em> zona dan penyedia layanan <em>cloud</em>
yang berbeda).</li><li>Skalabilitas: Terdapat batasan skalabilitas untuk sebuah klaster Kubernetes,
hal ini sebenarnya tidak menjadi masalah bagi sebagian besar pengguna. Untuk informasi
lebih lanjut kamu bisa membaca
<a href=https://git.k8s.io/community/sig-scalability/goals.md><em>Kubernetes Scaling</em> dan Perencanaan Performa</a>).</li><li><a href=#hybrid-cloud-capabilities><em>Hybrid cloud</em></a>: Kamu dapat memiliki <em>multiple</em> klsuter
pada penyedia layanan <em>cloud</em> yang berbeda ataupun menggunakan <em>on-premsie</em>.</li></ul><h3 id=kekurangan>Kekurangan</h3><p>Meskipun terdapat banyak kelebihan dari penggunaan <em>federation</em>,
terdapat beberapa kekurangan <em>federation</em> yang dijabarkan sebagai berikut:</p><ul><li>Peningkatan <em>bandwidth</em> dan biaya untuk jaringan: <em>control plane</em> <em>federation</em> bertugas mengawasi semua
kulster yang ada untuk menjamin <em>state</em> yang ada saat ini sesuai dengan <em>state</em> yang diinginkan. Hal ini dapat menyebabkan
peningkatan biaya jaringan apabila klaster yang ada dijalankan pada <em>region</em> yang berbeda baik pada penyedia
layanan <em>cloud</em> yang sama maupun berbeda.</li><li>Berkurangnya isolasi antar klaster: Sebuah <em>bug</em> yang ada pada <em>control plane</em> <em>federation</em> dapat
berdampak pada semua klaster. Hal ini dapat dihindari dengan cara mejaga logika yang ada pada <em>control plane</em> <em>federation</em>
seminimum mungkin.</li><li>Kematangan: Proyek <em>federation</em> ini tergolong baru dan belum cukup matang.
Tidak semua <em>resource</em> yang ada tersedia dan masih banyak feature <em>alpha</em>. <a href=https://github.com/kubernetes/federation/issues/88><em>Issue</em>
88</a> memberikan detail
isu-isu terkait sistem yang masih berusaha dicari solusinya.</li></ul><h3 id=kemampuan-hybrid-penggunaan-layanan-penyedian-cloud>Kemampuan <em>Hybrid</em> Penggunaan Layanan Penyedian <em>Cloud</em></h3><p><em>Federation</em> pada Kubernetes memungkinkan klaster untuk dijalankan
pada penyedia layanan <em>cloud</em> yang berbeda (misalnya Google Cloud, AWS), dan <em>on-premise</em>
(misalnya OpenStack). <a href=/docs/tasks/federation/set-up-cluster-federation-kubefed/>Kubefed</a>
adalah salah satu cara yang direkomendasikan untuk melakukan proses <em>deploy</em>
klaster <em>federation</em>.</p><p>Dengan demikian, <a href=#resources-api><em>resources</em> API</a> yang kamu miliki
dapat berada di klaster atau bahkan penyedia layanan <em>cloud</em> yang berbeda.</p><h2 id=mengaktifkan-federation>Mengaktifkan <em>Federation</em></h2><p>Untuk bisa melakukan <em>federation</em> pada klaster yang berbeda,
pertama kamu harus mengaktifkan <em>control plane</em> <em>federation</em>.
Ikuti <a href=/docs/tutorials/federation/set-up-cluster-federation-kubefed/>petunjuk mengaktifkan <em>control plane</em> <em>federation</em></a>
untuk informasi lebih lanjut.</p><h2 id=resources-api><code>Resources</code> API</h2><p>Setelah kamu mengaktifkan <em>control plane</em>, kamu dapat menggunakan <em>resource</em> API <em>federation</em>.
Berikut merupakan panduan yang akan menjelaskan masing-masing <em>resource</em> secara mendetail:</p><ul><li><a href=/docs/tasks/administer-federation/cluster/>Cluster</a></li><li><a href=/docs/tasks/administer-federation/configmap/>ConfigMap</a></li><li><a href=/docs/tasks/administer-federation/daemonset/>DaemonSets</a></li><li><a href=/docs/tasks/administer-federation/deployment/>Deployment</a></li><li><a href=/docs/tasks/administer-federation/events/>Events</a></li><li><a href=/docs/tasks/administer-federation/hpa/>Hpa</a></li><li><a href=/docs/tasks/administer-federation/ingress/>Ingress</a></li><li><a href=/docs/tasks/administer-federation/job/>Jobs</a></li><li><a href=/docs/tasks/administer-federation/namespaces/>Namespaces</a></li><li><a href=/docs/tasks/administer-federation/replicaset/>ReplicaSets</a></li><li><a href=/docs/tasks/administer-federation/secret/>Secrets</a></li><li><a href=/id/docs/concepts/cluster-administration/federation-service-discovery/>Services</a></li></ul><p><a href=/docs/reference/federation/>Referensi Dokumentasi API</a> memberikan semua daftar
<em>resources</em> yang disediakan <em>apiserver</em> <em>federation</em>.</p><h2 id=penghapusan-berantai>Penghapusan Berantai</h2><p>Kubernetes versi 1.6 menyediakan mekanisme penghapusan berantai
untuk <em>resource</em> yang ada pada <em>federation</em>. Dengan penghapusan berantai,
ketika kamu menghapus sebuah <em>resource</em> dari <em>control plane</em> <em>federation</em>,
kamu juga akan menghapus segala <em>resource</em> tersebut pada semua klaster yang ada.</p><p>Mekanisme penghapusan berantai ini tidak diaktifkan secara <em>default</em>
ketika menggunakan REST API. Untuk mengaktifkannya, ubah nilai dari opsi
<code>DeleteOptions.orphanDependents=false</code> ketika kamu menghapus sebuah <em>resource</em>
dari <em>control plane</em> <em>federation</em> dengan menggunakan REST API.
Penggunaan <code>kubectl delete</code>mengaktifkan penhapusan berantai secara <em>default</em>.
Kamu dapat menonaktifkannya dengan menggunakan <code>kubectl delete --cascade=false</code></p><p>Catatan: Kubernetes versi 1.5 menyediakan penghapusan berantai
untuk sebagian <em>resource</em> <em>federation</em>.</p><h2 id=cakupan-dari-sebuah-klaster>Cakupan dari Sebuah Klaster</h2><p>Pada penyedia IaaS seperti Google Compute Engine atau Amazon Web Services, sebuah VM ada di dalam
<a href=https://cloud.google.com/compute/docs/zones>zona</a> atau <a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html><em>availability
zone</em></a>.
Kami menyarankan agar semua VM pada klaster Kubernetes berada pada <em>availability</em> zona yang sama, karena:</p><ul><li>dibandingkan dengan sebuah klaster global Kubernetes, terdapat lebih sedikit <em>single-points of failure</em>.</li><li>dibandingkan dengan sebuah klaster yang tersebar pada <em>availability zone</em> yang mungkin berbeda, akan lebih mudah untuk merencanakan properti <em>availability</em> dari sebuah
klaster yang berada pada satu zona.</li><li>ketika pengembang Kubernetes mendesain sistem (misalnya, memperkirakan <em>latency</em>, <em>bandwidth</em>, atau
<em>failure</em> yang mungkin terjadi) pengembang tersebut memperkirakan semua mesin akan berada pada sebuah <em>data center</em> yang sama, atau setidaknya masih terdapat pada satu wilayah.</li></ul><p>Sangat direkomendasikan untuk menjalankan sedikit klaster dengan lebih banyak VM pada setiap <em>availability</em> zona;
meskipun begitu hal ini tidak menutup kemungkinan untuk menjalankan klaster multipel
pada setiap <em>availability</em> zona.</p><p>Alasan kenapa menjalankan lebih sedikit klaster pada setiap <em>availability</em> zona lebih dianjurkan:</p><ul><li>meningkatkan <em>bin packing</em> <em>Pod</em> pada beberapa kasus dimana terdapat lebih banyak <em>node</em> dalam sebuah klaster (mengurangi terjadinya <em>fragmentation</em> <em>resource</em>).</li><li>mengurangi <em>overhead</em> operasional (meskipun keuntungan ini akan berkurang seiring bertambah matangnya proses dan <em>tooling</em> operasional).</li><li>mengurangi biaya <em>resource</em> tetap per klaster, misalnya VM <em>apiserver</em>.</li></ul><p>Alasan untuk memiliki klaster multipel:</p><ul><li><em>policy</em> kemananan yang ketat membutuhkan isolasi antar <em>work</em> <em>class</em> (baca Partisi Klaster di bawah).</li><li>melakukan penerapan Kubernetes dan/atau perangkat lunak lain yang versi baru ke salah satu klaster.</li></ul><h2 id=memilih-jumlah-klaster-yang-tepat>Memilih jumlah klaster yang tepat</h2><p>Pemilihan jumlah klaster yang tepat merupakan pilihan yang relatif statis, dan hanya akan ditinjau kembali sewaktu-waktu.
Sebaliknya, jumlah <em>node</em> dan <em>pod</em> dalam suatu <em>service</em> dapat berubah secara cepat seiring bertambahnya <em>workload</em>.</p><p>Untuk memilih jumlah klaster, pertama, pilih <em>region</em> yang memiliki <em>latency</em> yang masih dapat dimaklumi untuk semua pengguna aplikasi kamu
(jika kamu menggunakan <em>Content Distribution Network</em>, kebutuhan informasi nilai <em>latency</em> CDN tidak perlu diperhatikan).
Masalah legal juga perlu diperhitungkan. Misalnya sebuah perusahaan dengan pelanggan global bisa jadi memilih klaster di <em>region</em>
US, EU, AP, dan SA. Jumlah <em>region</em> ini dimisalkan dengan <code>R</code>.</p><p>Kedua, pilih berapa banyak klaster yang bisa jadi <em>unavailable</em> secara bersamaan tanpa membuat <em>service</em> menjadi <em>unavailable</em>.
Misalkan jumlah klaster <em>unavailable</em> ini sebagai <code>U</code>. Jika kamu tidak yakin, maka 1 merupakan pilihan yang tergolong
dapat diterima.</p><p>Jika aplikasimu memungkinkan trafik untuk di-<em>load balance</em> ke <em>region</em> mana saja ketika terjadi <em>failure</em> pada klaster,
maka kamu setidaknya membutuhkan nilai yang lebih banyak dari jumlah <code>R</code> atau <code>U + 1</code> klaster. Jika tidak (misalnya, kamu
ingin menjamin stabilnya <em>latency</em> ketika terjadi <em>failure</em> pada klaster) maka kamu membutuhkan <code>R * (U + 1)</code> klaster
(<code>U + 1</code> di setiap <em>region</em> yang ada pada <code>R</code>). Pada kasus lain, cobalah untuk menerapkan satu klaster
pada zona yang berbeda.</p><p>Terakhir, jika klaster yang kamu miliki membutuhkan jumlah <em>node</em> yang melebihi nilai yang direkomendasikan untuk sebuah klaster Kubernetes,
maka kamu membutuhkan lebih banyak klaster. Kubernetes v1.3 mampu menangani hingga 1000 node untuk setiap klaster. Kubernetes v1.8
mampu menangani hingga 5000 node untuk tiap klaster. Baca <a href=/docs/setup/cluster-large/>Membangun Klaster Besar</a> untuk petunjuk lebih lanjut.</p><h2 id=selanjutnya>Selanjutnya</h2><ul><li>Pelajari lebih lanjut tentang <a href=https://github.com/kubernetes/community/blob/main/contributors/design-proposals/multicluster/federation.md>proposal
<em>Federation</em></a>.</li><li>Baca <a href=/docs/tutorials/federation/set-up-cluster-federation-kubefed/>petunjuk pengaktifan</a> klaster <em>federation</em>.</li><li>Lihat <a href="https://www.youtube.com/watch?v=pq9lbkmxpS8">seminar tentang <em>federation</em> pada Kubecon2016</a></li><li>Lihat <a href="https://www.youtube.com/watch?v=kwOvOLnFYck"><em>update</em> <em>federation</em> pada Kubecon2017 Eropa</a></li><li>Lihat <a href="https://www.youtube.com/watch?v=vGZo5DaThQU"><em>update</em> <em>sig-multicluster</em> pada Kubecon2018 Eropa</a></li><li>Lihat <a href="https://youtu.be/q27rbaX5Jis?t=7m20s">presentasi prototipe <em>Federation-v2</em> pada Kubecon2018 Eropa</a></li><li>Lihat <a href=https://github.com/kubernetes-sigs/federation-v2/blob/master/docs/userguide.md>petunjuk penggunaan <em>Federation-v2</em></a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-08e94e6a480e0d6b2de72d84a1b97617>10 - Berbagai Proxy di Kubernetes</h1><p>Laman ini menjelaskan berbagai <i>proxy</i> yang ada di dalam Kubernetes.</p><h2 id=berbagai-jenis-i-proxy-i>Berbagai Jenis <i>Proxy</i></h2><p>Ada beberapa jenis <i>proxy</i> yang akan kamu temui saat menggunakan Kubernetes:</p><ol><li><p><a href=/id/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api>kubectl proxy</a>:</p><ul><li>dijalankan pada <i>desktop</i> pengguna atau di dalam sebuah Pod</li><li>melakukan <i>proxy</i> dari alamat localhost ke apiserver Kubernetes</li><li>dari klien menuju <i>proxy</i> menggunakan HTTP</li><li>dari <i>proxy</i> menuju apiserver menggunakan HTTPS</li><li>mencari lokasi apiserver</li><li>menambahkan <i>header</i> autentikasi</li></ul></li><li><p><a href=/id/docs/tasks/access-application-cluster/access-cluster/#discovering-builtin-services>apiserver proxy</a>:</p><ul><li>merupakan sebuah <i>bastion</i> yang ada di dalam apiserver</li><li>menghubungkan pengguna di luar klaster ke alamat-alamat IP di dalam klaster yang tidak bisa terjangkau</li><li>dijalankan bersama <i>process-process</i> apiserver</li><li>dari klien menuju <i>proxy</i> menggunakan HTTPS (atau http jika dikonfigurasi pada apiserver)</li><li>dari <i>proxy</i> menuju target menggunakan HTTP atau HTTPS, tergantung pilihan yang diambil oleh <i>proxy</i> melalui informasi yang ada</li><li>dapat digunakan untuk menghubungi Node, Pod, atau Service</li><li>melakukan <i>load balancing</i> saat digunakan untuk menjangkau sebuah Service</li></ul></li><li><p><a href=/id/docs/concepts/services-networking/service/#ips-and-vips>kube proxy</a>:</p><ul><li>dijalankan pada setiap Node</li><li>melakukan <i>proxy</i> untuk UDP, TCP dan SCTP</li><li>tidak mengerti HTTP</li><li>menyediakan <i>load balancing</i></li><li>hanya digunakan untuk menjangkau berbagai Service</li></ul></li><li><p>Sebuah <i>Proxy/Load-balancer</i> di depan satu atau banyak apiserver:</p><ul><li>keberadaan dan implementasinya bervariasi tergantung pada klaster (contohnya nginx)</li><li>ada di antara seluruh klien dan satu/banyak apiserver</li><li>jika ada beberapa apiserver, berfungsi sebagai <i>load balancer</i></li></ul></li><li><p><i>Cloud Load Balancer</i> pada servis eksternal:</p><ul><li>disediakan oleh beberapa penyedia layanan cloud, seperti AWS ELB, Google Cloud Load Balancer</li><li>dibuat secara otomatis ketika Service dari Kubernetes dengan tipe <code>LoadBalancer</code></li><li>biasanya hanya tersedia untuk UDP/TCP</li><li><i>support</i> untuk SCTP tergantung pada <i>load balancer</i> yang diimplementasikan oleh penyedia cloud</li><li>implementasi bervariasi tergantung pada penyedia cloud</li></ul></li></ol><p>Pengguna Kubernetes biasanya hanya cukup perlu tahu tentang kubectl <i>proxy</i> dan apiserver <i>proxy</i>.
Untuk <i>proxy-proxy</i> lain di luar ini, admin klaster biasanya akan memastikan konfigurasinya dengan benar.</p><h2 id=melakukan-i-request-redirect-i>Melakukan <i>request redirect</i></h2><p><i>Proxy</i> telah menggantikan fungsi <i>redirect</i>. <i>Redirect</i> telah terdeprekasi.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d5cc46b61677b53f61a407d20bdd0830>11 - Metrik controller manager</h1><p>Metrik <em>controller manager</em> memberikan informasi penting tentang kinerja dan kesehatan dari <em>controller manager</em>.</p><h2 id=tentang-metrik-controller-manager>Tentang metrik <em>controller manager</em></h2><p>Metrik <em>controller manager</em> ini berfungsi untuk memberikan informasi penting tentang kinerja dan kesehatan dari <em>controller manager</em>.
Metrik ini juga berisi tentang metrik umum dari <em>runtime</em> bahasa pemrograman Go seperti jumlah <em>go_routine</em> dan metrik spesifik dari <em>controller</em> seperti
latensi dari etcd <em>request</em> atau latensi API dari penyedia layanan <em>cloud</em> (AWS, GCE, OpenStack) yang dapat digunakan untuk mengukur kesehatan dari klaster.</p><p>Mulai dari Kubernetes 1.7, metrik yang lebih mendetil tentang operasi penyimpanan dari penyedia layanan <em>cloud</em> juga telah tersedia.
Metrik-metrik ini dapat digunakan untuk memonitor kesehatan dari operasi <em>persistent volume</em>.</p><p>Berikut merupakan contoh nama metrik yang disediakan GCE:</p><pre tabindex=0><code>cloudprovider_gce_api_request_duration_seconds { request = &#34;instance_list&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_insert&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_delete&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;attach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;detach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;list_disk&#34;}
</code></pre><h2 id=konfigurasi>Konfigurasi</h2><p>Pada sebuah klaster, informasi metrik <em>controller manager</em> dapat diakses melalui <code>http://localhost:10252/metrics</code>
dari <em>host</em> tempat <em>controller manager</em> dijalankan.</p><p>Metrik ini dikeluarkan dalam bentuk <a href=https://prometheus.io/docs/instrumenting/exposition_formats/>format prometheus</a> serta mudah untuk dibaca manusia.</p><p>Pada <em>environment</em> produksi, kamu mungkin juga ingin mengonfigurasi prometheus atau pengumpul metrik lainnya untuk mengumpulkan metrik-metrik ini secara berkala dalam bentuk basis data <em>time series</em>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-85d633ae590aa20ec024f1b7af1d74fc>12 - Instalasi Add-ons</h1><p><em>Add-ons</em> berfungsi untuk menambah serta memperluas fungsionalitas dari Kubernetes.</p><p>Laman ini akan menjabarkan beberapa <em>add-ons</em> yang tersedia serta tautan instruksi bagaimana cara instalasi masing-masing <em>add-ons</em>.</p><p><em>Add-ons</em> pada setiap bagian akan diurutkan secara alfabet - pengurutan ini tidak dilakukan berdasarkan status preferensi atau keunggulan.</p><h2 id=jaringan-dan-policy-jaringan>Jaringan dan <em>Policy</em> Jaringan</h2><ul><li><a href=https://www.github.com/noironetworks/aci-containers>ACI</a> menyediakan integrasi jaringan kontainer dan keamanan jaringan dengan Cisco ACI.</li><li><a href=https://docs.projectcalico.org/latest/getting-started/kubernetes/>Calico</a> merupakan penyedia jaringan L3 yang aman dan <em>policy</em> jaringan.</li><li><a href=https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel>Canal</a> menggabungkan Flannel dan Calico, menyediakan jaringan serta <em>policy</em> jaringan.</li><li><a href=https://github.com/cilium/cilium>Cilium</a> merupakan <em>plugin</em> jaringan L3 dan <em>policy</em> jaringan yang dapat menjalankan <em>policy</em> HTTP/API/L7 secara transparan. Mendukung mode <em>routing</em> maupun <em>overlay/encapsulation</em>.</li><li><a href=https://github.com/cni-genie/CNI-Genie>CNI-Genie</a> memungkinkan Kubernetes agar dapat terkoneksi dengan beragam <em>plugin</em> CNI, seperti Calico, Canal, Flannel, Romana, atau Weave dengan mulus.</li><li><a href=http://contiv.github.io>Contiv</a> menyediakan jaringan yang dapat dikonfigurasi (<em>native</em> L3 menggunakan BGP, <em>overlay</em> menggunakan vxlan, klasik L2, dan Cisco-SDN/ACI) untuk berbagai penggunaan serta <em>policy framework</em> yang kaya dan beragam. Proyek Contiv merupakan proyek <a href=http://github.com/contiv>open source</a>. Laman <a href=http://github.com/contiv/install>instalasi</a> ini akan menjabarkan cara instalasi, baik untuk klaster dengan kubeadm maupun non-kubeadm.</li><li><a href=http://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a>, yang berbasis dari <a href=https://tungsten.io>Tungsten Fabric</a>, merupakan sebuah proyek <em>open source</em> yang menyediakan virtualisasi jaringan <em>multi-cloud</em> serta platform manajemen <em>policy</em>. Contrail dan Tungsten Fabric terintegrasi dengan sistem orkestrasi lainnya seperti Kubernetes, OpenShift, OpenStack dan Mesos, serta menyediakan mode isolasi untuk mesin virtual (VM), kontainer/pod dan <em>bare metal</em>.</li><li><a href=https://github.com/flannel-io/flannel#deploying-flannel-manually>Flannel</a> merupakan penyedia jaringan <em>overlay</em> yang dapat digunakan pada Kubernetes.</li><li><a href=https://github.com/ZTE/Knitter/>Knitter</a> merupakan solusi jaringan yang mendukung multipel jaringan pada Kubernetes.</li><li><a href=https://github.com/k8snetworkplumbingwg/multus-cni>Multus</a> merupakan sebuah multi <em>plugin</em> agar Kubernetes mendukung multipel jaringan secara bersamaan sehingga dapat menggunakan semua <em>plugin</em> CNI (contoh: Calico, Cilium, Contiv, Flannel), ditambah pula dengan SRIOV, DPDK, OVS-DPDK dan VPP pada <em>workload</em> Kubernetes.</li><li><a href=https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html>NSX-T</a> Container Plug-in (NCP) menyediakan integrasi antara VMware NSX-T dan orkestrator kontainer seperti Kubernetes, termasuk juga integrasi antara NSX-T dan platform CaaS/PaaS berbasis kontainer seperti <em>Pivotal Container Service</em> (PKS) dan OpenShift.</li><li><a href=https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst>Nuage</a> merupakan platform SDN yang menyediakan <em>policy-based</em> jaringan antara Kubernetes Pods dan non-Kubernetes <em>environment</em> dengan <em>monitoring</em> visibilitas dan keamanan.</li><li><a href=http://romana.io>Romana</a> merupakan solusi jaringan <em>Layer</em> 3 untuk jaringan pod yang juga mendukung <a href=/id/docs/concepts/services-networking/network-policies/><em>NetworkPolicy</em> API</a>. Instalasi Kubeadm <em>add-on</em> ini tersedia <a href=https://github.com/romana/romana/tree/master/containerize>di sini</a>.</li><li><a href=https://www.weave.works/docs/net/latest/kube-addon/>Weave Net</a> menyediakan jaringan serta <em>policy</em> jaringan, yang akan membawa kedua sisi dari partisi jaringan, serta tidak membutuhkan basis data eksternal.</li></ul><h2 id=service-discovery><em>Service Discovery</em></h2><ul><li><a href=https://coredns.io>CoreDNS</a> merupakan server DNS yang fleksibel, mudah diperluas yang dapat <a href=https://github.com/coredns/deployment/tree/master/kubernetes>diinstal</a> sebagai <em>in-cluster</em> DNS untuk pod.</li></ul><h2 id=visualisasi-amp-kontrol>Visualisasi & Kontrol</h2><ul><li><a href=https://github.com/kubernetes/dashboard#kubernetes-dashboard>Dashboard</a> merupakan antarmuka web dasbor untuk Kubernetes.</li><li><a href=https://www.weave.works/documentation/scope-latest-installing/#k8s>Weave Scope</a> merupakan perangkat untuk visualisasi grafis dari kontainer, pod, <em>service</em> dll milikmu. Gunakan bersama dengan <a href=https://cloud.weave.works/>akun Weave Cloud</a> atau <em>host</em> UI-mu sendiri.</li></ul><h2 id=add-ons-terdeprekasi><em>Add-ons</em> Terdeprekasi</h2><p>Ada beberapa <em>add-on</em> lain yang didokumentasikan pada direktori deprekasi <a href=https://git.k8s.io/kubernetes/cluster/addons><em>cluster/addons</em></a>.</p><p><em>Add-on</em> lain yang dipelihara dan dikelola dengan baik dapat ditulis di sini. Ditunggu PR-nya!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-31c9327d2332c585341b64ddafa19cdd>13 - Prioritas dan Kesetaraan API (API Priority and Fairness)</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [alpha]</code></div><p>Mengontrol perilaku server API dari Kubernetes pada situasi beban berlebih
merupakan tugas utama dari administrator klaster. <a class=glossary-tooltip title='Komponen control plane yang mengekspos API Kubernetes. Merupakan front-end dari control plane Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-apiserver/ target=_blank aria-label=kube-apiserver>kube-apiserver</a> memiliki beberapa kontrol yang tersedia
(seperti opsi <code>--max-request-inflight</code> dan <code>--max-mutating-request-inflight</code>
pada baris perintah atau <em>command-line</em>) untuk membatasi jumlah pekerjaan luar biasa yang akan
diterima, untuk mencegah banjirnya permintaan masuk dari beban berlebih
yang berpotensi untuk menghancurkan server API. Namun opsi ini tidak cukup untuk memastikan
bahwa permintaan yang paling penting dapat diteruskan pada saat kondisi lalu lintas (<em>traffic</em>) yang cukup tinggi.</p><p>Fitur Prioritas dan Kesetaraan API atau <em>API Priority and Fairness</em> (APF) adalah alternatif untuk meningkatkan
batasan <em>max-inflight</em> seperti yang disebutkan di atas. APF mengklasifikasi
dan mengisolasi permintaan dengan cara yang lebih halus. Fitur ini juga memperkenalkan
jumlah antrian yang terbatas, sehingga tidak ada permintaan yang ditolak
pada saat terjadi lonjakan permintaan dalam waktu yang sangat singkat. Permintaan dibebaskan dari antrian dengan menggunakan
teknik antrian yang adil (<em>fair queuing</em>) sehingga, sebagai contoh, perilaku buruk dari satu
<a class=glossary-tooltip title='Kontrol tertutup yang mengawasi kondisi bersama dari klaster melalui apiserver dan membuat perubahan yang mencoba untuk membawa kondisi saat ini ke kondisi yang diinginkan.' data-toggle=tooltip data-placement=top href=/id/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> tidak seharusnya
mengakibatkan <em>controller</em> yang lain menderita (meskipun pada tingkat prioritas yang sama).</p><div class="alert alert-warning caution callout" role=alert><strong>Perhatian:</strong> Permintaan yang diklasifikasikan sebagai "long running" - terutama <em>watch</em> - tidak
mengikuti filter prioritas dan kesetaraan API. Dimana ini juga berlaku pada
opsi <code>--max-request-inflight</code> tanpa mengaktifkan APF.</div><h2 id=mengaktifkan-prioritas-dan-kesetaraan-api>Mengaktifkan prioritas dan kesetaraan API</h2><p>Fitur APF dikontrol oleh sebuah gerbang fitur (<em>feature gate</em>)
dan fitur ini tidak diaktifkan secara bawaan. Silahkan lihat
<a href=/docs/reference/command-line-tools-reference/feature-gates/>gerbang fitur</a>
untuk penjelasan umum tentang gerbang fitur dan bagaimana cara mengaktifkan dan menonaktifkannya.
Nama gerbang fitur untuk APF adalah "APIPriorityAndFairness".
Fitur ini melibatkan sebuah <a class=glossary-tooltip title='Sekumpulan path terkait pada API Kubernetes.' data-toggle=tooltip data-placement=top href=/id/docs/concepts/overview/kubernetes-api/#api-groups target=_blank aria-label='Grup API'>Grup API</a> yang harus juga diaktifkan. Kamu bisa melakukan ini dengan
menambahkan opsi pada baris perintah berikut pada permintaan ke <code>kube-apiserver</code> kamu:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kube-apiserver <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--feature-gates<span style=color:#666>=</span><span style=color:#b8860b>APIPriorityAndFairness</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--runtime-config<span style=color:#666>=</span>flowcontrol.apiserver.k8s.io/v1alpha1<span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span> <span style=color:#080;font-style:italic># …dan opsi-opsi lainnya seperti biasa</span>
</span></span></code></pre></div><p>Opsi pada baris perintah <code>--enable-priority-and-fairness=false</code> akan menonaktifkan fitur
APF, bahkan ketika opsi yang lain telah mengaktifkannya.</p><h2 id=konsep>Konsep</h2><p>Ada beberapa fitur lainnya yang terlibat dalam fitur APF. Permintaan yang masuk diklasifikasikan berdasarkan atribut permintaan dengan menggunakan
FlowSchema, dan diserahkan ke tingkat prioritas. Tingkat prioritas menambahkan tingkat
isolasi dengan mempertahankan batas konkurensi yang terpisah, sehingga permintaan yang diserahkan
ke tingkat prioritas yang berbeda tidak dapat membuat satu sama lain menderita. Dalam sebuah tingkat prioritas,
algoritma <em>fair-queuing</em> mencegah permintaan dari <em>flows</em> yang berbeda akan memberikan penderitaan
kepada yang lainnya, dan memperbolehkan permintaan untuk dimasukkan ke dalam antrian untuk mencegah pelonjakan lalu lintas
yang akan menyebabkan gagalnya permintaan, walaupun pada saat beban rata-ratanya cukup rendah.</p><h3 id=tingkat-prioritas-priority-level>Tingkat prioritas (<em>Priority Level</em>)</h3><p>Tanpa pengaktifan APF, keseluruhan konkurensi dalam
server API dibatasi oleh opsi pada <code>kube-apiserver</code>
<code>--max-request-inflight</code> dan <code>--max-mutating-request-inflight</code>. Dengan pengaktifan APF,
batas konkurensi yang ditentukan oleh opsi ini akan dijumlahkan dan kemudian jumlah tersebut dibagikan
untuk sekumpulan tingkat prioritas (<em>priority level</em>) yang dapat dikonfigurasi. Setiap permintaan masuk diserahkan
ke sebuah tingkat prioritas, dan setiap tingkat prioritas hanya akan meneruskan sebanyak mungkin
permintaan secara bersamaan sesuai dengan yang diijinkan dalam konfigurasi.</p><p>Konfigurasi bawaan, misalnya, sudah mencakup tingkat prioritas terpisah untuk
permintaan dalam rangka pemilihan pemimpin (<em>leader-election</em>), permintaan dari <em>controller</em> bawaan, dan permintaan dari
Pod. Hal ini berarti bahwa Pod yang berperilaku buruk, yang bisa membanjiri server API
dengan permintaan, tidak akan mampu mencegah kesuksesan pemilihan pemimpin atau tindakan yang dilakukan oleh <em>controller</em> bawaan.</p><h3 id=antrian-queuing>Antrian (<em>Queuing</em>)</h3><p>Bahkan dalam sebuah tingkat prioritas mungkin akan ada sumber lalu lintas yang berbeda dalam jumlah besar.
Dalam situasi beban berlebih, sangat penting untuk mencegah satu aliran
permintaan dari penderitaan karena aliran yang lainnya (khususnya, dalam kasus yang relatif umum dari sebuah
klien tunggal bermasalah (<em>buggy</em>) yang dapat membanjiri <em>kube-apiserver</em> dengan permintaan, klien bermasalah itu
idealnya tidak memiliki banyak dampak yang bisa diukur terhadap klien yang lainnya). Hal ini
ditangani dengan menggunakan algoritma <em>fair-queuing</em> untuk memproses permintaan yang diserahkan
oleh tingkat prioritas yang sama. Setiap permintaan diserahkan ke sebuah <em>flow</em>, yang diidentifikasi berdasarkan
nama FlowSchema yang sesuai, ditambah dengan <em>flow distinguisher</em> - yang
bisa saja didasarkan pada pengguna yang meminta, sumber daya Namespace dari target, atau tidak sama sekali - dan
sistem mencoba untuk memberikan bobot yang hampir sama untuk permintaan dalam <em>flow</em> yang berbeda dengan tingkat prioritas yang sama.</p><p>Setelah mengklasifikasikan permintaan ke dalam sebuah <em>flow</em>, fitur APF kemudian
dapat menyerahkan permintaan ke dalam sebuah antrian. Penyerahan ini menggunakan
teknik yang dikenal sebagai <a class=glossary-tooltip title='A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.' data-toggle=tooltip data-placement=top href='/id/docs/reference/glossary/?all=true#term-shuffle-sharding' target=_blank aria-label='_shuffle sharding_'>_shuffle sharding_</a>, yang membuat penggunaan antrian yang relatif efisien
untuk mengisolasi <em>flow</em> dengan intensitas rendah dari <em>flow</em> dengan intensitas tinggi.</p><p>Detail dari algoritma antrian dapat disesuaikan untuk setiap tingkat prioritas, dan
memperbolehkan administrator untuk menukar (<em>trade off</em>) dengan penggunaan memori, kesetaraan (properti dimana
<em>flow</em> yang independen akan membuat semua kemajuan ketika total dari lalu lintas sudah melebihi kapasitas),
toleransi untuk lonjakan lalu lintas, dan penambahan latensi yang dihasilkan oleh antrian.</p><h3 id=permintaan-yang-dikecualikan-exempt-request>Permintaan yang dikecualikan (<em>Exempt Request</em>)</h3><p>Beberapa permintaan dianggap cukup penting sehingga mereka tidak akan mengikuti
salah satu batasan yang diberlakukan oleh fitur ini. Pengecualian ini untuk mencegah
konfigurasi <em>flow control</em> yang tidak terkonfigurasi dengan baik sehingga tidak benar-benar menonaktifkan server API.</p><h2 id=bawaan-default>Bawaan (<em>Default</em>)</h2><p>Fitur APF dikirimkan dengan konfigurasi yang disarankan
dimana konfigurasi itu seharusnya cukup untuk bereksperimen; jika klaster kamu cenderung
mengalami beban berat maka kamu harus mempertimbangkan konfigurasi apa yang akan bekerja paling baik.
Kelompok konfigurasi yang disarankan untuk semua permintaan terbagi dalam lima prioritas
kelas:</p><ul><li><p>Tingkat prioritas <code>system</code> diperuntukkan bagi permintaan dari grup <code>system:nodes</code>,
mis. Kubelet, yang harus bisa menghubungi server API agar
mendapatkan <em>workload</em> untuk dijadwalkan.</p></li><li><p>Tingkat prioritas <code>leader-election</code> diperuntukkan bagi permintaan dalam pemilihan pemimpin (<em>leader election</em>)
dari <em>controller</em> bawaan (khususnya, permintaan untuk <code>endpoint</code>, <code>configmaps</code>,
atau <code>leases</code> yang berasal dari <code>system:kube-controller-manager</code> atau pengguna
<code>system:kube-scheduler</code> dan akun Service di Namespace <code>kube-system</code>). Hal ini
penting untuk mengisolasi permintaan ini dari lalu lintas yang lain karena
kegagalan dalam pemilihan pemimpin menyebabkan <em>controller</em> akan gagal dan memulai kembali (<em>restart</em>),
yang pada akhirnya menyebabkan lalu lintas yang lebih mahal karena <em>controller</em>
yang baru perlu menyinkronkan para informannya.</p></li><li><p>Tingkat prioritas <code>workload-high</code> diperuntukkan bagi permintaan yang lain dari <em>controller</em> bawaan.
  </p></li><li><p>Tingkat prioritas <code>workload-low</code> diperuntukkan bagi permintaan dari akun Service yang lain,
yang biasanya mencakup semua permintaan dari <em>controller</em> yang bekerja didalam Pod.
  </p></li><li><p>Tingkat prioritas <code>global-default</code> menangani semua lalu lintas lainnya, mis.
perintah interaktif <code>kubectl</code> yang dijalankan oleh pengguna yang tidak memiliki hak khusus.</p></li></ul><p>Kemudian, ada dua PriorityLevelConfiguration dan dua FlowSchema yang telah
dibangun dan tidak mungkin ditimpa ulang:</p><ul><li><p>Tingkat prioritas khusus <code>exempt</code> diperuntukkan bagi permintaan yang tidak akan dikenakan
<em>flow control</em> sama sekali: permintaan itu akan selalu diteruskan sesegera mungkin.
FlowSchema <code>exempt</code> khusus mengklasifikasikan semua permintaan dari kelompok <code>system:masters</code>
ke dalam tingkat prioritas khusus ini. Kamu juga dapat menentukan FlowSchema lain yang mengarahkan
permintaan lain ke tingkat prioritas ini juga, apabila permintaan tersebut sesuai.</p></li><li><p>Tingkat prioritas khusus <code>catch-all</code> digunakan secara kombinasi dengan spesial
FlowSchema <code>catch-all</code> untuk memastikan bahwa setiap permintaan mendapatkan proses
klasifikasi. Biasanya kamu tidak harus bergantung pada konfigurasi <em>catch-all</em> ini,
dan kamu seharusnya membuat FlowSchema <em>catch-all</em> dan PriorityLevelConfiguration kamu sendiri
(atau gunakan konfigurasi <code>global-default</code> yang sudah diinstal secara bawaan) secara benar.
Untuk membantu menemukan kesalahan konfigurasi yang akan melewatkan beberapa klasifikasi
permintaan, maka tingkat prioritas <code>catch-all</code> hanya wajib mengijinkan satu konkurensi
bersama dan tidak melakukan memasukkan permintaan dalam antrian, sehingga membuat lalu lintas
yang secara relatif hanya sesuai dengan FlowSchema <code>catch-all</code> akan ditolak dengan kode kesalahan HTTP 429.</p></li></ul><h2 id=sumber-daya-resource>Sumber daya (<em>Resource</em>)</h2><p><em>Flow control</em> API melibatkan dua jenis sumber daya.
<a href=/docs/reference/generated/kubernetes-api/v1.25/#prioritylevelconfiguration-v1alpha1-flowcontrol-apiserver-k8s-io>PriorityLevelConfiguration</a>
yang menentukan kelas isolasi yang tersedia, bagian dari konkurensi anggaran yang tersedia
yang masing-masing dapat menangani bagian tersebut, dan memperbolehkan untuk melakukan <em>fine-tuning</em> terhadap perilaku antrian.
<a href=/docs/reference/generated/kubernetes-api/v1.25/#flowschema-v1alpha1-flowcontrol-apiserver-k8s-io>FlowSchema</a>
yang digunakan untuk mengklasifikasikan permintaan individu yang masuk, mencocokkan masing-masing dengan setiap
PriorityLevelConfiguration.</p><h3 id=prioritylevelconfiguration>PriorityLevelConfiguration</h3><p>Sebuah PriorityLevelConfiguration merepresentasikan sebuah kelas isolasi tunggal. Setiap
PriorityLevelConfiguration memiliki batas independensi dalam hal jumlah
permintaan yang belum diselesaikan, dan batasan dalam hal jumlah permintaan yang mengantri.</p><p>Batas konkurensi untuk PriorityLevelConfiguration tidak disebutkan dalam
jumlah permintaan secara mutlak, melainkan dalam "concurrency shares." Total batas konkurensi
untuk server API didistribusikan di antara PriorityLevelConfiguration yang ada
secara proporsional dengan "concurrency shares" tersebut. Ini mengizinkan seorang
administrator klaster untuk meningkatkan atau menurunkan jumlah total lalu lintas ke sebuah
server dengan memulai kembali <code>kube-apiserver</code> dengan nilai opsi
<code>--max-request-inflight</code> (atau <code>--max-mutating-request-inflight</code>) yang berbeda, dan semua
PriorityLevelConfiguration akan melihat konkurensi maksimum yang diizinkan kepadanya untuk menaikkan (atau
menurunkan) dalam fraksi yang sama.</p><div class="alert alert-warning caution callout" role=alert><strong>Perhatian:</strong> Dengan fitur Prioritas dan Kesetaraan yang diaktifkan, batas total konkurensi untuk
server diatur pada nilai penjumlahan dari <code>--max-request-inflight</code> dan
<code>--max-mutating-request-inflight</code>. Tidak akan ada lagi perbedaan
antara permintaan yang bermutasi dan permintaan yang tidak bermutasi; jika kamu ingin melayaninya
secara terpisah untuk suatu sumber daya yang ada, maka perlu membuat FlowSchema terpisah yang sesuai dengan
masing-masing kata kerja dari permintaan yang bermutasi dan yang tidak bermutasi tersebut.</div><p>Ketika jumlah permintaan masuk yang diserahkan kepada sebuah
PriorityLevelConfiguration melebihi dari tingkat konkurensi yang diizinkan,
bagian <code>type</code> dari spesifikasinya menentukan apa yang akan terjadi pada permintaan selanjutnya.
Tipe <code>Reject</code> berarti bahwa kelebihan lalu lintas akan segera ditolak
dengan kode kesalahan HTTP 429 (yang artinya terlalu banyak permintaan). Tipe <code>Queue</code> berarti permintaan
di atas batas tersebut akan mengantri, dengan teknik <em>sharding shuffle</em> dan <em>fair queuing</em> yang digunakan
untuk menyelaraskan kemajuan antara <em>flow</em> permintaan.</p><p>Konfigurasi antrian memungkinkan mengatur algoritma <em>fair queuing</em> untuk sebuah
tingkat prioritas. Detail algoritma dapat dibaca di <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness>proposal pembaharuan</a>, namun secara singkat:</p><ul><li><p>Meningkatkan <code>queue</code> (antrian) berarti mengurangi tingkat tabrakan antara <em>flow</em> yang berbeda,
sehingga berakibat pada biaya untuk meningkatkan penggunaan memori. Nilai 1 di sini secara
efektif menonaktifkan logika <em>fair-queuing</em>, tetapi masih mengizinkan permintaan untuk
dimasukkan kedalam antrian.</p></li><li><p>Meningkatkan <code>queueLengthLimit</code> berarti memperbolehkan lonjakan yang lebih besar dari lalu lintas
untuk berkelanjutan tanpa menggagalkan permintaan apa pun, dengan konsekuensi akan meningkatkan
latensi dan penggunaan memori.</p></li><li><p>Mengubah <code>handSize</code> berarti memperbolehkan kamu untuk menyesuaikan probabilitas tabrakan antara
   <em>flow</em> yang berbeda dan keseluruhan konkurensi yang tersedia untuk satu <em>flow</em> tunggal
dalam situasi beban berlebih.</p><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> <code>HandSize</code> yang lebih besar membuat dua <em>flow</em> individual berpeluang kecil untuk bertabrakan
(dan dimana <em>flow</em> yang satu bisa membuat <em>flow</em> yang lain menderita), tetapi akan lebih memungkinkan
bahwa <em>flow</em> dalam jumlah kecil akan dapat mendominasi apiserver. <code>HandSize</code> yang lebih besar juga
berpotensi meningkatkan jumlah latensi yang diakibatkan oleh satu <em>flow</em> lalu lintas tunggal
yang tinggi. Jumlah maksimum permintaan dalam antrian yang diijinkan dari sebuah <em>flow</em> tunggal
adalah <code>handSize * queueLengthLimit</code>.</div></li></ul><p>Berikut ini adalah tabel yang menunjukkan koleksi konfigurasi <em>shuffle sharding</em>
yang menarik, dimana setiap probabilitas <em>mouse</em> (<em>flow</em> dengan intensitas rendah)
yang diberikan akan dimampatkan oleh <em>elephant</em> (<em>flow</em> dengan intensitas tinggi) dalam sebuah koleksi ilustratif
dari jumlah <em>elephant</em> yang berbeda. Silahkan lihat pada
<a href=https://play.golang.org/p/Gi0PLgVHiUg>https://play.golang.org/p/Gi0PLgVHiUg</a>, yang digunakan untuk menghitung nilai-nilai dalam tabel ini.</p><table><caption style=display:none>Contoh Konfigurasi Shuffle Sharding</caption><thead><tr><th>HandSize</th><th>Queues</th><th>1 elephant</th><th>4 elephants</th><th>16 elephants</th></tr></thead><tbody><tr><td>12</td><td>32</td><td>4.428838398950118e-09</td><td>0.11431348830099144</td><td>0.9935089607656024</td></tr><tr><td>10</td><td>32</td><td>1.550093439632541e-08</td><td>0.0626479840223545</td><td>0.9753101519027554</td></tr><tr><td>10</td><td>64</td><td>6.601827268370426e-12</td><td>0.00045571320990370776</td><td>0.49999929150089345</td></tr><tr><td>9</td><td>64</td><td>3.6310049976037345e-11</td><td>0.00045501212304112273</td><td>0.4282314876454858</td></tr><tr><td>8</td><td>64</td><td>2.25929199850899e-10</td><td>0.0004886697053040446</td><td>0.35935114681123076</td></tr><tr><td>8</td><td>128</td><td>6.994461389026097e-13</td><td>3.4055790161620863e-06</td><td>0.02746173137155063</td></tr><tr><td>7</td><td>128</td><td>1.0579122850901972e-11</td><td>6.960839379258192e-06</td><td>0.02406157386340147</td></tr><tr><td>7</td><td>256</td><td>7.597695465552631e-14</td><td>6.728547142019406e-08</td><td>0.0006709661542533682</td></tr><tr><td>6</td><td>256</td><td>2.7134626662687968e-12</td><td>2.9516464018476436e-07</td><td>0.0008895654642000348</td></tr><tr><td>6</td><td>512</td><td>4.116062922897309e-14</td><td>4.982983350480894e-09</td><td>2.26025764343413e-05</td></tr><tr><td>6</td><td>1024</td><td>6.337324016514285e-16</td><td>8.09060164312957e-11</td><td>4.517408062903668e-07</td></tr></tbody></table><h3 id=flowschema>FlowSchema</h3><p>FlowSchema mencocokkan beberapa permintaan yang masuk dan menetapkan permintaan ke dalam sebuah
tingkat prioritas. Setiap permintaan masuk diuji dengan setiap
FlowSchema secara bergiliran, dimulai dari yang terendah secara numerik ---
yang kita anggap sebagai yang tertinggi secara logis --- <code>matchingPrecedence</code> dan
begitu seterusnya. FlowSchema yang cocok pertama kali akan menang.</p><div class="alert alert-warning caution callout" role=alert><strong>Perhatian:</strong> Hanya FlowSchema yang pertama kali cocok untuk permintaan yang diberikan yang akan dianggap penting. Jika ada banyak
FlowSchema yang cocok dengan sebuah permintaan masuk, maka akan ditetapkan berdasarkan salah satu
yang mempunyai <code>matchingPrecedence</code> tertinggi. Jika ada beberapa FlowSchema dengan nilai
<code>matchingPrecedence</code> yang sama dan cocok dengan permintaan yang sama juga, permintaan dengan leksikografis
<code>name</code> yang lebih kecil akan menang, tetapi akan lebih baik untuk tidak mengandalkan metode ini, dan sebaiknya
perlu memastikan bahwa tidak ada dua FlowSchema yang memiliki <code>matchingPrecedence</code> yang sama.</div><p>Sebuah FlowSchema dianggap cocok dengan sebuah permintaan yang diberikan jika setidaknya salah satu dari <code>rules</code> nya
ada yang cocok. Sebuah aturan (<em>rule</em>) cocok jika setidaknya satu dari <code>subject</code> <em>dan</em>
ada salah satu dari <code>resourceRules</code> atau <code>nonResourceRules</code> (tergantung dari apakah permintaan
yang masuk adalah untuk URL sumber daya atau non-sumber daya) yang cocok dengan permintaan tersebut.</p><p>Untuk bagian <code>name</code> dalam subjek, dan bagian <code>verbs</code>, <code>apiGroups</code>, <code>resources</code>,
<code>namespaces</code>, dan <code>nonResourceURLs</code> dalam aturan sumber daya dan non-sumber daya,
<em>wildcard</em> <code>*</code> mungkin bisa ditetapkan untuk mencocokkan semua nilai pada bagian yang diberikan,
sehingga secara efektif menghapusnya dari pertimbangan.</p><p>Sebuah <code>DistinguisherMethod.type</code> dari FlowSchema menentukan bagaimana permintaan
yang cocok dengan Skema itu akan dipisahkan menjadi <em>flow</em>. Nilai tipe itu bisa jadi <code>ByUser</code>, dalam
hal ini satu pengguna yang meminta tidak akan bisa menghabiskan kapasitas dari pengguna lain,
atau bisa juga <code>ByNamespace</code>, dalam hal ini permintaan sumber daya
di salah satu Namespace tidak akan bisa menyebabkan penderitaan bagi permintaan akan sumber daya
dalam kapasitas Namespace yang lain, atau bisa juga kosong (atau <code>distinguisherMethod</code>
dihilangkan seluruhnya), dalam hal ini semua permintaan yang cocok dengan FlowSchema ini akan
dianggap sebagai bagian dari sebuah <em>flow</em> tunggal. Pilihan yang tepat untuk FlowSchema yang diberikan
akan bergantung pada sumber daya dan lingkungan khusus kamu.</p><h2 id=diagnosis>Diagnosis</h2><p>Setiap respons HTTP dari server API dengan fitur prioritas dan kesetaraan
yang diaktifkan memiliki dua <em>header</em> tambahan: <code>X-Kubernetes-PF-FlowSchema-UID</code> dan
<code>X-Kubernetes-PF-PriorityLevel-UID</code>, yang mencatat skema <em>flow</em> yang cocok dengan permintaan
dan tingkat prioritas masing-masing. Name Objek API tidak termasuk dalam <em>header</em> ini jika pengguna peminta tidak
memiliki izin untuk melihatnya, jadi ketika melakukan <em>debugging</em> kamu dapat menggunakan perintah seperti ini</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get flowschema -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span><span style=display:flex><span>kubectl get prioritylevelconfiguration -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span></code></pre></div><p>untuk mendapatkan pemetaan UID ke names baik untuk FlowSchema maupun PriorityLevelConfiguration.</p><h2 id=observabilitas>Observabilitas</h2><p>Saat kamu mengaktifkan fitur Prioritas dan Kesetaraan API atau APF, kube-apiserver
akan mengeluarkan metrik tambahan. Dengan memantau metrik ini dapat membantu kamu untuk menentukan apakah
konfigurasi kamu tidak tepat dalam membatasi lalu lintas yang penting, atau menemukan
beban kerja yang berperilaku buruk yang dapat membahayakan kesehatan dari sistem.</p><ul><li><p><code>apiserver_flowcontrol_rejected_requests_total</code> menghitung permintaan yang
ditolak, mengelompokkannya berdasarkan nama dari tingkat prioritas yang ditetapkan,
nama FlowSchema yang ditetapkan, dan alasan penolakan tersebut.
Alasan penolakan akan mengambil dari salah satu alasan-alasan berikut:</p><ul><li><code>queue-full</code>, yang mengindikasikan bahwa sudah terlalu banyak permintaan
yang menunggu dalam antrian,</li><li><code>concurrency-limit</code>, yang mengindikasikan bahwa PriorityLevelConfiguration
telah dikonfigurasi untuk menolak, bukan untuk memasukan permintaan berlebih ke
dalam antrian, atau</li><li><code>time-out</code>, yang mengindikasikan bahwa permintaan masih dalam antrian
ketika batas waktu antriannya telah berakhir.</li></ul></li><li><p><code>apiserver_flowcontrol_dispatched_requests_total</code> menghitung permintaan
yang sudah mulai dieksekusi, mengelompokkannya berdasarkan nama dari tingkat
prioritas yang ditetapkan, dan nama dari FlowSchema yang ditetapkan.</p></li><li><p><code>apiserver_flowcontrol_current_inqueue_requests</code> memberikan
jumlah total sesaat secara instan dari permintaan dalam antrian (bukan yang dieksekusi),
dan mengelompokkannya berdasarkan tingkat prioritas dan FlowSchema.</p></li><li><p><code>apiserver_flowcontrol_current_executing_requests</code> memberikan
jumlah total yang instan dari permintaan yang dieksekusi, dan mengelompokkannya
berdasarkan tingkat prioritas dan FlowSchema.</p></li><li><p><code>apiserver_flowcontrol_request_queue_length_after_enqueue</code> memberikan
histogram dari panjang antrian untuk semua antrian yang ada, mengelompokkannya berdasarkan
tingkat prioritas dan FlowSchema, berdasarkan pengambilan sampel oleh permintaan
<em>enqueued</em>. Setiap permintaan yang mendapatkan antrian berkontribusi ke satu sampel
dalam histogramnya, pelaporan panjang antrian dilakukan setelah permintaan yang
mengantri tersebut ditambahkan. Perlu dicatat bahwa ini akan menghasilkan statistik
yang berbeda dengan survei yang tidak bias.</p><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Nilai asing atau tidak biasa dalam histogram akan berarti ada kemungkinan sebuah <em>flow</em></li></ul><p>(misalnya, permintaan oleh satu pengguna atau untuk satu <em>namespace</em>, tergantung pada
konfigurasinya) telah membanjiri server API, dan sedang dicekik. Sebaliknya, jika
histogram dari satu tingkat prioritas menunjukkan bahwa semua antrian dalam prioritas
level itu lebih panjang daripada level prioritas yang lainnya, mungkin akan sesuai
untuk meningkatkan <em>concurrency shares</em> dari PriorityLevelConfiguration itu.</p></div><ul><li><p><code>apiserver_flowcontrol_request_concurrency_limit</code> memberikan hasil perhitungan
batas konkurensi (berdasarkan pada batas konkurensi total dari server API dan
<em>concurrency share</em> dari PriorityLevelConfiguration) untuk setiap
PriorityLevelConfiguration.</p></li><li><p><code>apiserver_flowcontrol_request_wait_duration_seconds</code> memberikan histogram tentang bagaimana
permintaan yang panjang dihabiskan dalam antrian, mengelompokkannya berdasarkan FlowSchema
yang cocok dengan permintaan, tingkat prioritas yang ditetapkan, dan apakah permintaan
tersebut berhasil dieksekusi atau tidak.</p><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Karena setiap FlowSchema selalu memberikan permintaan untuk satu</li></ul><p>PriorityLevelConfiguration, kamu dapat menambahkan histogram untuk semua
FlowSchema dalam satu tingkat prioritas untuk mendapatkan histogram yang efektif
dari permintaan yang ditetapkan ke tingkat prioritas tersebut.</p></div><ul><li><code>apiserver_flowcontrol_request_execution_seconds</code> memberikan histogram tentang bagaimana
caranya permintaan yang panjang diambil untuk benar-benar dieksekusi, mengelompokkannya
berdasarkan FlowSchema yang cocok dengan permintaan dan tingkat prioritas yang ditetapkan pada
permintaan tersebut.</li></ul><h2 id=selanjutnya>Selanjutnya</h2><p>Untuk latar belakang informasi mengenai detail desain dari prioritas dan kesetaraan API, silahkan lihat
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness>proposal pembaharuan</a>.
Kamu juga dapat membuat saran dan permintaan akan fitur melalui <a href=https://github.com/kubernetes/community/tree/master/sig-api-machinery>SIG API
Machinery</a>.</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/id/docs/home/>Home</a>
<a class=text-white href=/id/community/>Komunitas</a>
<a class=text-white href=/id/case-studies/>Studi kasus</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 Para Pencipta Kubernetes | Dokumentasi didistribusikan di bawah <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 Linux Foundation &reg;. Hak cipta dilindungi. Linux Foundation telah mendaftarkan merek dagang dan pengunaannya. Perinciannya bisa dilihat pada <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>halaman penggunaan merek dagang</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>