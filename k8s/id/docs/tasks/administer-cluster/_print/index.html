<!doctype html><html lang=id class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/tasks/administer-cluster/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/tasks/administer-cluster/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/tasks/administer-cluster/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/tasks/administer-cluster/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/tasks/administer-cluster/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/tasks/administer-cluster/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/id/docs/tasks/administer-cluster/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Mengelola Sebuah Klaster | Kubernetes</title><meta property="og:title" content="Mengelola Sebuah Klaster"><meta property="og:description" content="Pelajari tugas-tugas umum dalam pengelolaan sebuah klaster."><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/id/docs/tasks/administer-cluster/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Mengelola Sebuah Klaster"><meta itemprop=description content="Pelajari tugas-tugas umum dalam pengelolaan sebuah klaster."><meta name=twitter:card content="summary"><meta name=twitter:title content="Mengelola Sebuah Klaster"><meta name=twitter:description content="Pelajari tugas-tugas umum dalam pengelolaan sebuah klaster."><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Pelajari tugas-tugas umum dalam pengelolaan sebuah klaster."><meta property="og:description" content="Pelajari tugas-tugas umum dalam pengelolaan sebuah klaster."><meta name=twitter:description content="Pelajari tugas-tugas umum dalam pengelolaan sebuah klaster."><meta property="og:url" content="https://kubernetes.io/id/docs/tasks/administer-cluster/"><meta property="og:title" content="Mengelola Sebuah Klaster"><meta name=twitter:title content="Mengelola Sebuah Klaster"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/id/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/id/docs/>Dokumentasi</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/id/community/>Komunitas</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/id/case-studies/>Studi kasus</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versi</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/id/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/id/docs/tasks/administer-cluster/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/id/docs/tasks/administer-cluster/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/id/docs/tasks/administer-cluster/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/id/docs/tasks/administer-cluster/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/id/docs/tasks/administer-cluster/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Bahasa Indonesia</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/tasks/administer-cluster/>English</a>
<a class=dropdown-item href=/zh-cn/docs/tasks/administer-cluster/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/tasks/administer-cluster/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/tasks/administer-cluster/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/tasks/administer-cluster/>Français (French)</a>
<a class=dropdown-item href=/de/docs/tasks/administer-cluster/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/tasks/administer-cluster/>Español (Spanish)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/id/docs/tasks/administer-cluster/>Return to the regular view of this page</a>.</p></div><h1 class=title>Mengelola Sebuah Klaster</h1><div class=lead>Pelajari tugas-tugas umum dalam pengelolaan sebuah klaster.</div><ul><li>1: <a href=#pg-1239a77618c6278373832a142cd85519>Menggunakan Calico untuk NetworkPolicy</a></li><li>2: <a href=#pg-47be5dd51f686017f1766e6ec7aa6f41>Mengelola Memori, CPU, dan Sumber Daya API</a></li><ul><li>2.1: <a href=#pg-adb489b1ab985c9215657b0d4c6ae92b>Mengatur Batas Minimum dan Maksimum Memori pada sebuah Namespace</a></li></ul><li>3: <a href=#pg-8bcf4aeb5bbb6d6969a146e5ab97557b>Debugging Resolusi DNS</a></li><li>4: <a href=#pg-3d0cd7d2f13d4759094f281504cf57b8>Kustomisasi Service DNS</a></li><li>5: <a href=#pg-b64a1d2bb3f4ed9f7021134e09a75c36>Melakukan Reservasi Sumber Daya Komputasi untuk Daemon Sistem</a></li><li>6: <a href=#pg-1e966f5d0540bbee0876f9d0d08d54dc>Membagi sebuah Klaster dengan Namespace</a></li><li>7: <a href=#pg-0b17e83b6049e53b8ffa864bdfa07c87>Mengatur Control Plane Kubernetes dengan Ketersediaan Tinggi (High-Availability)</a></li><li>8: <a href=#pg-fe5ad73163d38596340536ec03a205f0>Menggunakan sysctl dalam Sebuah Klaster Kubernetes</a></li><li>9: <a href=#pg-c4d0832845adc92b7ccd54aed63fc932>Mengoperasikan klaster etcd untuk Kubernetes</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-1239a77618c6278373832a142cd85519>1 - Menggunakan Calico untuk NetworkPolicy</h1><p>Laman ini menunjukkan beberapa cara cepat untuk membuat klaster Calico pada Kubernetes.</p><h2 id=sebelum-kamu-memulai>Sebelum kamu memulai</h2><p>Putuskan apakah kamu ingin menggelar (<em>deploy</em>) sebuah klaster di <a href=#membuat-klaster-calico-menggunakan-google-kubernetes-engine-gke><em>cloud</em></a> atau di <a href=#membuat-klaster-calico-dengan-kubeadm>lokal</a>.</p><h2 id=membuat-klaster-calico-menggunakan-google-kubernetes-engine-gke>Membuat klaster Calico dengan menggunakan <em>Google Kubernetes Engine</em> (GKE)</h2><p><strong>Prasyarat</strong>: <a href=https://cloud.google.com/sdk/docs/quickstarts>gcloud</a>.</p><ol><li><p>Untuk meluncurkan klaster GKE dengan Calico, cukup sertakan opsi <code>--enable-network-policy</code>.</p><p><strong>Sintaksis</strong></p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>gcloud container clusters create <span style=color:#666>[</span>CLUSTER_NAME<span style=color:#666>]</span> --enable-network-policy
</span></span></code></pre></div><p><strong>Contoh</strong></p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>gcloud container clusters create my-calico-cluster --enable-network-policy
</span></span></code></pre></div></li><li><p>Untuk memverifikasi penggelaran, gunakanlah perintah berikut ini.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods --namespace<span style=color:#666>=</span>kube-system
</span></span></code></pre></div><p>Pod Calico dimulai dengan kata <code>calico</code>. Periksa untuk memastikan bahwa statusnya <code>Running</code>.</p></li></ol><h2 id=membuat-klaster-calico-dengan-kubeadm>Membuat klaster lokal Calico dengan kubeadm</h2><p>Untuk membuat satu klaster Calico dengan hos tunggal dalam waktu lima belas menit dengan menggunakan kubeadm, silakan merujuk pada</p><p><a href=https://docs.projectcalico.org/latest/getting-started/kubernetes/>Memulai cepat Calico</a>.</p><h2 id=selanjutnya>Selanjutnya</h2><p>Setelah klaster kamu berjalan, kamu dapat mengikuti <a href=/id/docs/tasks/administer-cluster/declare-network-policy/>Mendeklarasikan Kebijakan Jaringan</a> untuk mencoba NetworkPolicy Kubernetes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-47be5dd51f686017f1766e6ec7aa6f41>2 - Mengelola Memori, CPU, dan Sumber Daya API</h1></div><div class=td-content><h1 id=pg-adb489b1ab985c9215657b0d4c6ae92b>2.1 - Mengatur Batas Minimum dan Maksimum Memori pada sebuah Namespace</h1><p>Laman ini menunjukkan cara untuk mengatur nilai minimum dan maksimum memori yang digunakan oleh Container
yang berjalan pada sebuah Namespace. Kamu dapat menentukan nilai minimum dan maksimum memori pada objek
<a href=/docs/reference/generated/kubernetes-api/v1.25/#limitrange-v1-core>LimitRange</a>. Jika sebuah Pod tidak memenuhi batasan yang ditentukan oleh LimitRange,
maka Pod tersebut tidak dapat dibuat pada Namespace tersebut.</p><h2 id=sebelum-kamu-memulai>Sebelum kamu memulai</h2><p><p>Kamu harus memiliki klaster Kubernetes, dan perangkat baris perintah kubectl
juga harus dikonfigurasikan untuk berkomunikasi dengan klastermu. Jika kamu
belum memiliki klaster, kamu dapat membuatnya dengan menggunakan
<a href=/id/docs/tasks/tools/#minikube>minikube</a>,
atau kamu juga dapat menggunakan salah satu dari tempat mencoba Kubernetes berikut ini:</p><ul><li><a href=https://killercoda.com/playgrounds/scenario/kubernetes>Killercoda</a></li><li><a href=http://labs.play-with-k8s.com/>Bermain dengan Kubernetes</a></li></ul>Untuk melihat versi, tekan <code>kubectl version</code>.</p><p>Tiap Node dalam klastermu harus memiliki setidaknya 1 GiB memori.</p><h2 id=membuat-sebuah-namespace>Membuat sebuah Namespace</h2><p>Buat sebuah Namespace sehingga sumber daya yang kamu buat pada latihan ini
terisolasi dari komponen lain pada klastermu.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create namespace constraints-mem-example
</span></span></code></pre></div><h2 id=membuat-limitrange-dan-pod>Membuat LimitRange dan Pod</h2><p>Berikut berkas konfigurasi untuk sebuah LimitRange:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/resource/memory-constraints.yaml download=admin/resource/memory-constraints.yaml><code>admin/resource/memory-constraints.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-resource-memory-constraints-yaml")' title="Copy admin/resource/memory-constraints.yaml to clipboard"></img></div><div class=includecode id=admin-resource-memory-constraints-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>LimitRange<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mem-min-max-demo-lr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>max</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>min</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>500Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Container<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Membuat LimitRange:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</span></span></code></pre></div><p>Melihat informasi mendetail mengenai LimitRange:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get limitrange mem-min-max-demo-lr --namespace<span style=color:#666>=</span>constraints-mem-example --output<span style=color:#666>=</span>yaml
</span></span></code></pre></div><p>Keluaran yang dihasilkan menunjukkan batasan minimum dan maksimum dari memori seperti yang diharapkan. Tetapi
perhatikan hal berikut, meskipun kamu tidak menentukan nilai bawaan pada berkas konfigurasi untuk
LimitRange, namun nilai tersebut akan dibuat secara otomatis.</p><pre tabindex=0><code>  limits:
  - default:
      memory: 1Gi
    defaultRequest:
      memory: 1Gi
    max:
      memory: 1Gi
    min:
      memory: 500Mi
    type: Container
</code></pre><p>Mulai sekarang setiap Container yang dibuat pada Namespace constraints-mem-example, Kubernetes
akan menjalankan langkah-langkah berikut:</p><ul><li><p>Jika Container tersebut tidak menentukan permintaan dan limit memori, maka diberikan nilai permintaan
dan limit memori bawaan pada Container.</p></li><li><p>Memastikan Container memiliki permintaan memori yang lebih besar atau sama dengan 500 MiB.</p></li><li><p>Memastikan Container memiliki limit memori yang lebih kecil atau kurang dari 1 GiB.</p></li></ul><p>Berikut berkas konfigurasi Pod yang memiliki satu Container. Manifes Container
menentukan permintaan memori 600 MiB dan limit memori 800 MiB. Nilai tersebut memenuhi
batasan minimum dan maksimum memori yang ditentukan oleh LimitRange.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/resource/memory-constraints-pod.yaml download=admin/resource/memory-constraints-pod.yaml><code>admin/resource/memory-constraints-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-resource-memory-constraints-pod-yaml")' title="Copy admin/resource/memory-constraints-pod.yaml to clipboard"></img></div><div class=includecode id=admin-resource-memory-constraints-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;600Mi&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Membuat Pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</span></span></code></pre></div><p>Memastikan Container pada Pod sudah berjalan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pod constraints-mem-demo --namespace<span style=color:#666>=</span>constraints-mem-example
</span></span></code></pre></div><p>Melihat informasi mendetail tentang Pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pod constraints-mem-demo --output<span style=color:#666>=</span>yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</span></span></code></pre></div><p>Keluaran yang dihasilkan menunjukkan Container memiliki permintaan memori 600 MiB dan limit memori
800 MiB. Nilai tersebut memenuhi batasan yang ditentukan oleh LimitRange.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>800Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>600Mi<span style=color:#bbb>
</span></span></span></code></pre></div><p>Menghapus Podmu:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete pod constraints-mem-demo --namespace<span style=color:#666>=</span>constraints-mem-example
</span></span></code></pre></div><h2 id=mencoba-membuat-pod-yang-melebihi-batasan-maksimum-memori>Mencoba membuat Pod yang melebihi batasan maksimum memori</h2><p>Berikut berkas konfigurasi untuk sebuah Pod yang memiliki satu Container. Container tersebut menentukan
permintaan memori 800 MiB dan batas memori 1.5 GiB.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/resource/memory-constraints-pod-2.yaml download=admin/resource/memory-constraints-pod-2.yaml><code>admin/resource/memory-constraints-pod-2.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-resource-memory-constraints-pod-2-yaml")' title="Copy admin/resource/memory-constraints-pod-2.yaml to clipboard"></img></div><div class=includecode id=admin-resource-memory-constraints-pod-2-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-2-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1.5Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800Mi&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Mencoba membuat Pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-2.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</span></span></code></pre></div><p>Keluaran yang dihasilkan menunjukkan Pod tidak dibuat, karena Container menentukan limit memori yang
terlalu besar:</p><pre tabindex=0><code>Error from server (Forbidden): error when creating &#34;examples/admin/resource/memory-constraints-pod-2.yaml&#34;:
pods &#34;constraints-mem-demo-2&#34; is forbidden: maximum memory usage per Container is 1Gi, but limit is 1536Mi.
</code></pre><h2 id=mencoba-membuat-pod-yang-tidak-memenuhi-permintaan-memori>Mencoba membuat Pod yang tidak memenuhi permintaan memori</h2><p>Berikut berkas konfigurasi untuk sebuah Pod yang memiliki satu Container. Container tersebut menentukan
permintaan memori 100 MiB dan limit memori 800 MiB.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/resource/memory-constraints-pod-3.yaml download=admin/resource/memory-constraints-pod-3.yaml><code>admin/resource/memory-constraints-pod-3.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-resource-memory-constraints-pod-3-yaml")' title="Copy admin/resource/memory-constraints-pod-3.yaml to clipboard"></img></div><div class=includecode id=admin-resource-memory-constraints-pod-3-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-3-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;800Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Mi&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Mencoba membuat Pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-3.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</span></span></code></pre></div><p>Keluaran yang dihasilkan menunjukkan Pod tidak dibuat, karena Container menentukan permintaan memori yang
terlalu kecil:</p><pre tabindex=0><code>Error from server (Forbidden): error when creating &#34;examples/admin/resource/memory-constraints-pod-3.yaml&#34;:
pods &#34;constraints-mem-demo-3&#34; is forbidden: minimum memory usage per Container is 500Mi, but request is 100Mi.
</code></pre><h2 id=membuat-pod-yang-tidak-menentukan-permintaan-ataupun-limit-memori>Membuat Pod yang tidak menentukan permintaan ataupun limit memori</h2><p>Berikut berkas konfigurasi untuk sebuah Pod yang memiliki satu Container. Container tersebut tidak menentukan
permintaan memori dan juga limit memori.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/resource/memory-constraints-pod-4.yaml download=admin/resource/memory-constraints-pod-4.yaml><code>admin/resource/memory-constraints-pod-4.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-resource-memory-constraints-pod-4-yaml")' title="Copy admin/resource/memory-constraints-pod-4.yaml to clipboard"></img></div><div class=includecode id=admin-resource-memory-constraints-pod-4-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>constraints-mem-demo-4-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Mencoba membuat Pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/admin/resource/memory-constraints-pod-4.yaml --namespace<span style=color:#666>=</span>constraints-mem-example
</span></span></code></pre></div><p>Melihat informasi mendetail tentang Pod:</p><pre tabindex=0><code>kubectl get pod constraints-mem-demo-4 --namespace=constraints-mem-example --output=yaml
</code></pre><p>Keluaran yang dihasilkan menunjukkan Container pada Pod memiliki permintaan memori 1 GiB dan limit memori 1 GiB.
Bagaimana Container mendapatkan nilai tersebut?</p><pre tabindex=0><code>resources:
  limits:
    memory: 1Gi
  requests:
    memory: 1Gi
</code></pre><p>Karena Containermu tidak menentukan permintaan dan limit memori, Container tersebut diberikan
<a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>permintaan dan limit memori bawaan</a>
dari LimitRange.</p><p>Pada tahap ini, Containermu mungkin saja berjalan ataupun mungkin juga tidak berjalan. Ingat bahwa prasyarat
untuk tugas ini adalah Node harus memiliki setidaknya 1 GiB memori. Jika tiap Node hanya memiliki
1 GiB memori, maka tidak akan ada cukup memori untuk dialokasikan pada setiap Node untuk memenuhi permintaan 1 Gib memori. Jika ternyata kamu menggunakan Node dengan 2 GiB memori, maka kamu mungkin memiliki cukup ruang untuk memenuhi permintaan 1 GiB tersebut.</p><p>Menghapus Pod:</p><pre tabindex=0><code>kubectl delete pod constraints-mem-demo-4 --namespace=constraints-mem-example
</code></pre><h2 id=pelaksanaan-batasan-minimum-dan-maksimum-memori>Pelaksanaan batasan minimum dan maksimum memori</h2><p>Batasan maksimum dan minimum memori yang yang ditetapkan pada sebuah Namespace oleh LimitRange dilaksanakan
hanya ketika Pod dibuat atau diperbarui. Jika kamu mengubah LimitRange, hal tersebut tidak akan memengaruhi
Pods yang telah dibuat sebelumnya.</p><h2 id=motivasi-untuk-batasan-minimum-dan-maksimum-memori>Motivasi untuk batasan minimum dan maksimum memori</h2><p>Sebagai seorang administrator klaster, kamu mungkin ingin menetapkan pembatasan jumlah memori yang dapat digunakan oleh Pod.
Sebagai contoh:</p><ul><li><p>Tiap Node dalam sebuah klaster memiliki 2 GB memori. Kamu tidak ingin menerima Pod yang meminta
lebih dari 2 GB memori, karena tidak ada Node pada klater yang dapat memenuhi permintaan tersebut.</p></li><li><p>Sebuah klaster digunakan bersama pada departemen produksi dan pengembangan.
Kamu ingin mengizinkan beban kerja (<em>workload</em>) pada produksi untuk menggunakan hingga 8 GB memori, tapi
kamu ingin beban kerja pada pengembangan cukup terbatas sampai dengan 512 MB saja. Kamu membuat Namespace terpisah
untuk produksi dan pengembangan, dan kamu menerapkan batasan memori pada tiap Namespace.</p></li></ul><h2 id=bersih-bersih>Bersih-bersih</h2><p>Menghapus Namespace:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete namespace constraints-mem-example
</span></span></code></pre></div><h2 id=selanjutnya>Selanjutnya</h2><h3 id=untuk-administrator-klaster>Untuk administrator klaster</h3><ul><li><p><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>Mengatur Permintaan dan Limit Memori Bawaan untuk Sebuah Namespace</a></p></li><li><p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>Mengatur Permintaan dan Limit CPU Bawaan untuk Sebuah Namespace</a></p></li><li><p><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>Mengatur Batas Minimum dan Maksimum CPU untuk Sebuah Namespace</a></p></li><li><p><a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>Mengatur Kuota Memori dan CPU untuk Sebuah Namespace</a></p></li><li><p><a href=/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/>Mengatur Kuota Pod untuk Sebuah Namespace</a></p></li><li><p><a href=/docs/tasks/administer-cluster/quota-api-object/>Mengatur Kuota untuk Objek API</a></p></li></ul><h3 id=untuk-pengembang-aplikasi>Untuk pengembang aplikasi</h3><ul><li><p><a href=/docs/tasks/configure-pod-container/assign-memory-resource/>Memberikan Sumber Daya Memori pada Container dan Pod</a></p></li><li><p><a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>Memberikan Sumber Daya CPU pada Container dan Pod</a></p></li><li><p><a href=/docs/tasks/configure-pod-container/quality-service-pod/>Mengatur Kualitas Layanan Pod</a></p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-8bcf4aeb5bbb6d6969a146e5ab97557b>3 - Debugging Resolusi DNS</h1><p>Laman ini menyediakan beberapa petunjuk untuk mendiagnosis masalah DNS.</p><h2 id=sebelum-kamu-memulai>Sebelum kamu memulai</h2><p><p>Kamu harus memiliki klaster Kubernetes, dan perangkat baris perintah kubectl
juga harus dikonfigurasikan untuk berkomunikasi dengan klastermu. Jika kamu
belum memiliki klaster, kamu dapat membuatnya dengan menggunakan
<a href=/id/docs/tasks/tools/#minikube>minikube</a>,
atau kamu juga dapat menggunakan salah satu dari tempat mencoba Kubernetes berikut ini:</p><ul><li><a href=https://killercoda.com/playgrounds/scenario/kubernetes>Killercoda</a></li><li><a href=http://labs.play-with-k8s.com/>Bermain dengan Kubernetes</a></li></ul><br>Klaster kamu harus dikonfigurasi untuk menggunakan
<a class=glossary-tooltip title='Resources that extend the functionality of Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/cluster-administration/addons/ target=_blank aria-label=addon>addon</a> CoreDNS atau pendahulunya,
kube-dns.</p><p>Kubernetes servermu harus dalam versi yang sama atau lebih baru dari v1.6.
Untuk melihat versi, tekan <code>kubectl version</code>.</p><h3 id=membuat-pod-sederhana-yang-digunakan-sebagai-lingkungan-pengujian>Membuat Pod sederhana yang digunakan sebagai lingkungan pengujian</h3><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/id/examples/admin/dns/dnsutils.yaml download=admin/dns/dnsutils.yaml><code>admin/dns/dnsutils.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-dns-dnsutils-yaml")' title="Copy admin/dns/dnsutils.yaml to clipboard"></img></div><div class=includecode id=admin-dns-dnsutils-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dnsutils<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dnsutils<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/e2e-test-images/jessie-dnsutils:1.3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- sleep<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;3600&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Always<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Gunakan manifes berikut untuk membuat sebuah Pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml
</span></span></code></pre></div><pre tabindex=0><code>pod/dnsutils created
</code></pre><p>…dan verifikasi statusnya:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods dnsutils
</span></span></code></pre></div><pre tabindex=0><code>NAME      READY     STATUS    RESTARTS   AGE
dnsutils   1/1       Running   0          &lt;some-time&gt;
</code></pre><p>Setelah Pod tersebut berjalan, kamu dapat menjalankan perintah <code>nslookup</code> di lingkungan tersebut.
Jika kamu melihat hal seperti ini, maka DNS sudah berjalan dengan benar.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>exec</span> -i -t dnsutils -- nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex=0><code>Server:    10.0.0.10
Address 1: 10.0.0.10

Name:      kubernetes.default
Address 1: 10.0.0.1
</code></pre><p>Jika perintah <code>nslookup</code> gagal, periksa hal berikut:</p><h3 id=periksa-konfigurasi-dns-lokal-terlebih-dahulu>Periksa konfigurasi DNS lokal terlebih dahulu</h3><p>Periksa isi dari berkas resolv.conf.
(Lihat <a href=/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node><em>Inheriting</em> DNS dari node</a> dan
<a href=#known-issues>Isu-isu yang dikenal</a> di bawah ini untuk informasi lebih lanjut)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>exec</span> -ti dnsutils -- cat /etc/resolv.conf
</span></span></code></pre></div><p>Verifikasi <em>path</em> pencarian dan nama server telah dibuat agar tampil seperti di bawah ini (perlu diperhatikan bahwa <em>path</em> pencarian dapat berbeda tergantung dari penyedia layanan cloud):</p><pre tabindex=0><code>search default.svc.cluster.local svc.cluster.local cluster.local google.internal c.gce_project_id.internal
nameserver 10.0.0.10
options ndots:5
</code></pre><p>Kesalahan yang muncul berikut ini mengindikasikan terdapat masalah dengan <em>add-on</em> CoreDNS (atau kube-dns) atau Service terkait:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>exec</span> -i -t dnsutils -- nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex=0><code>Server:    10.0.0.10
Address 1: 10.0.0.10

nslookup: can&#39;t resolve &#39;kubernetes.default&#39;
</code></pre><p>atau</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>exec</span> -i -t dnsutils -- nslookup kubernetes.default
</span></span></code></pre></div><pre tabindex=0><code>Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

nslookup: can&#39;t resolve &#39;kubernetes.default&#39;
</code></pre><h3 id=periksa-apakah-pod-dns-sedang-berjalan>Periksa apakah Pod DNS sedang berjalan</h3><p>Gunakan perintah <code>kubectl get pods</code> untuk memverifikasi apakah Pod DNS sedang berjalan.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods --namespace<span style=color:#666>=</span>kube-system -l k8s-app<span style=color:#666>=</span>kube-dns
</span></span></code></pre></div><pre tabindex=0><code>NAME                       READY     STATUS    RESTARTS   AGE
...
coredns-7b96bf9f76-5hsxb   1/1       Running   0           1h
coredns-7b96bf9f76-mvmmt   1/1       Running   0           1h
...
</code></pre><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Nilai dari label <code>k8s-app</code> adalah <code>kube-dns</code> baik untuk CoreDNS maupun kube-dns.</div><p>Jika kamu melihat tidak ada Pod CoreDNS yang sedang berjalan atau Pod tersebut gagal/telah selesai, <em>add-on</em> DNS mungkin tidak dijalankan (<em>deployed</em>) secara bawaan di lingkunganmu saat ini dan kamu harus menjalankannya secara manual.</p><h3 id=periksa-kesalahan-pada-pod-dns>Periksa kesalahan pada Pod DNS</h3><p>Gunakan perintah <code>kubectl logs</code> untuk melihat log dari Container DNS.</p><p>Untuk CoreDNS:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs --namespace<span style=color:#666>=</span>kube-system -l k8s-app<span style=color:#666>=</span>kube-dns
</span></span></code></pre></div><p>Berikut contoh log dari CoreDNS yang sehat (<em>healthy</em>):</p><pre tabindex=0><code>.:53
2018/08/15 14:37:17 [INFO] CoreDNS-1.2.2
2018/08/15 14:37:17 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.2
linux/amd64, go1.10.3, 2e322f6
2018/08/15 14:37:17 [INFO] plugin/reload: Running configuration MD5 = 24e6c59e83ce706f07bcc82c31b1ea1c
</code></pre><p>Periksa jika ada pesan mencurigakan atau tidak terduga dalam log.</p><h3 id=apakah-layanan-dns-berjalan>Apakah layanan DNS berjalan?</h3><p>Verifikasi apakah layanan DNS berjalan dengan menggunakan perintah <code>kubectl get service</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get svc --namespace<span style=color:#666>=</span>kube-system
</span></span></code></pre></div><pre tabindex=0><code>NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
...
kube-dns     ClusterIP   10.0.0.10      &lt;none&gt;        53/UDP,53/TCP        1h
...
</code></pre><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Nama layanan adalah <code>kube-dns</code> baik untuk CoreDNS maupun kube-dns.</div><p>Jika kamu telah membuat Service atau seharusnya Service telah dibuat secara bawaan namun ternyata tidak muncul, lihat
<a href=/docs/tasks/debug/debug-application/debug-service/><em>debugging</em> Service</a> untuk informasi lebih lanjut.</p><h3 id=apakah-endpoint-dns-telah-ekspos>Apakah endpoint DNS telah ekspos?</h3><p>Kamu dapat memverifikasikan apakah <em>endpoint</em> DNS telah diekspos dengan menggunakan perintah <code>kubectl get endpoints</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get endpoints kube-dns --namespace<span style=color:#666>=</span>kube-system
</span></span></code></pre></div><pre tabindex=0><code>NAME       ENDPOINTS                       AGE
kube-dns   10.180.3.17:53,10.180.3.17:53    1h
</code></pre><p>Jika kamu tidak melihat <em>endpoint</em>, lihat bagian <em>endpoint</em> pada dokumentasi
<a href=/docs/tasks/debug/debug-application/debug-service/><em>debugging</em> Service</a>.</p><p>Untuk tambahan contoh Kubernetes DNS, lihat
<a href=https://github.com/kubernetes/examples/tree/master/staging/cluster-dns>contoh cluster-dns</a> pada repositori Kubernetes GitHub.</p><h3 id=apakah-kueri-dns-diterima-diproses>Apakah kueri DNS diterima/diproses?</h3><p>Kamu dapat memverifikasi apakah kueri telah diterima oleh CoreDNS dengan menambahkan plugin <code>log</code> pada konfigurasi CoreDNS (alias Corefile).
CoreDNS Corefile disimpan pada <a class=glossary-tooltip title='Sebuah objek API yang digunakan untuk menyimpan data nonkonfidensial sebagai pasangan kunci-nilai (key-value). Pod dapat menggunakan ConfigMap sebagai variabel lingkungan, argumen baris perintah (command-line), atau berkas konfigurasi dalam sebuah volume.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a> dengan nama <code>coredns</code>. Untuk mengeditnya, gunakan perintah:</p><pre tabindex=0><code>kubectl -n kube-system edit configmap coredns
</code></pre><p>Lalu tambahkan <code>log</code> pada bagian Corefile seperti contoh berikut:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>coredns<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>Corefile</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    .:53 {
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        errors
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        health
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>          pods insecure
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>          upstream
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>          fallthrough in-addr.arpa ip6.arpa
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        }
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        prometheus :9153
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        proxy . /etc/resolv.conf
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        cache 30
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        loop
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        reload
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        loadbalance
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    }</span><span style=color:#bbb>    
</span></span></span></code></pre></div><p>Setelah perubahan disimpan, perubahan dapat memakan waktu satu hingga dua menit untuk Kubernetes menyebarkan perubahan ini pada Pod CoreDNS.</p><p>Berikutnya, coba buat beberapa kueri dan lihat log pada bagian atas dari dokumen ini. Jika pod CoreDNS menerima kueri, kamu seharusnya akan melihatnya pada log.</p><p>Berikut ini contoh kueri yang terdapat di dalam log:</p><pre tabindex=0><code>.:53
2018/08/15 14:37:15 [INFO] CoreDNS-1.2.0
2018/08/15 14:37:15 [INFO] linux/amd64, go1.10.3, 2e322f6
CoreDNS-1.2.0
linux/amd64, go1.10.3, 2e322f6
2018/09/07 15:29:04 [INFO] plugin/reload: Running configuration MD5 = 162475cdf272d8aa601e6fe67a6ad42f
2018/09/07 15:29:04 [INFO] Reloading complete
172.17.0.18:41675 - [07/Sep/2018:15:29:11 +0000] 59925 &#34;A IN kubernetes.default.svc.cluster.local. udp 54 false 512&#34; NOERROR qr,aa,rd,ra 106 0.000066649s
</code></pre><h2 id=isu-isu-yang-dikenal>Isu-isu yang Dikenal</h2><p>Beberapa distribusi Linux (contoh Ubuntu) menggunakan <em>resolver</em> DNS lokal secara bawaan (systemd-resolved).
Systemd-resolved memindahkan dan mengganti <code>/etc/resolv.conf</code> dengan berkas <em>stub</em> yang dapat menyebabkan <em>forwarding loop</em> yang fatal saat meresolusi nama pada server <em>upstream</em>. Ini dapat diatasi secara manual dengan menggunakan <em>flag</em> kubelet <code>--resolv-conf</code>
untuk mengarahkan ke <code>resolv.conf</code> yang benar (Pada <code>systemd-resolved</code>, ini berada di <code>/run/systemd/resolve/resolv.conf</code>).
kubeadm akan otomatis mendeteksi <code>systemd-resolved</code>, dan menyesuaikan <em>flag</em> kubelet sebagai mana mestinya.</p><p>Pemasangan Kubernetes tidak menggunakan berkas <code>resolv.conf</code> pada <em>node</em> untuk digunakan sebagai klaster DNS secara <em>default</em>, karena proses ini umumnya spesifik pada distribusi tertentu. Hal ini bisa jadi akan diimplementasi nantinya.</p><p>Libc Linux (alias glibc) secara bawaan memiliki batasan <code>nameserver</code> DNS sebanyak 3 rekaman (<em>records</em>). Selain itu, pada glibc versi yang lebih lama dari glibc-2.17-222 (<a href=https://access.redhat.com/solutions/58028>versi terbaru lihat isu ini</a>), jumlah rekaman DNS <code>search</code> dibatasi sejumlah 6 (<a href="https://bugzilla.redhat.com/show_bug.cgi?id=168253">lihat masalah sejak 2005 ini</a>). Kubernetes membutuhkan 1 rekaman <code>nameserver</code> dan 3 rekaman <code>search</code>. Ini berarti jika instalasi lokal telah menggunakan 3 <code>nameserver</code> atau menggunakan lebih dari 3 <code>search</code>,sementara versi glibc kamu termasuk yang terkena dampak, beberapa dari pengaturan tersebut akan hilang. Untuk menyiasati batasan rekaman DNS <code>nameserver</code>, <em>node</em> dapat menjalankan <code>dnsmasq</code>,yang akan menyediakan <code>nameserver</code> lebih banyak. Kamu juga dapat menggunakan kubelet <code>--resolv-conf</code> <em>flag</em>. Untuk menyiasati batasan rekaman <code>search</code>, pertimbangkan untuk memperbarui distribusi linux kamu atau memperbarui glibc ke versi yang tidak terdampak.</p><p>Jika kamu menggunakan Alpine versi 3.3 atau lebih lama sebagai dasar <em>image</em> kamu, DNS mungkin tidak dapat bekerja dengan benar disebabkan masalah dengan Alpine.
<a href=https://github.com/kubernetes/kubernetes/issues/30215>Masalah 30215</a> Kubernetes menyediakan informasi lebih detil tentang ini.</p><h2 id=selanjutnya>Selanjutnya</h2><ul><li>Lihat <a href=/docs/tasks/administer-cluster/dns-horizontal-autoscaling/>Penyekalaan otomatis Service DNS dalam klaster</a>.</li><li>Baca <a href=/docs/concepts/services-networking/dns-pod-service/>DNS untuk Service dan Pod</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3d0cd7d2f13d4759094f281504cf57b8>4 - Kustomisasi Service DNS</h1><p>Laman ini menjelaskan cara mengonfigurasi DNS
<a class=glossary-tooltip title='Unit Kubernetes yang paling sederhana dan kecil. Sebuah Pod merepresentasikan sebuah set kontainer yang dijalankan pada kluster kamu.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> kamu dan menyesuaikan
proses resolusi DNS pada klaster kamu.</p><h2 id=sebelum-kamu-memulai>Sebelum kamu memulai</h2><p>Kamu harus memiliki klaster Kubernetes, dan perangkat baris perintah kubectl
juga harus dikonfigurasikan untuk berkomunikasi dengan klastermu. Jika kamu
belum memiliki klaster, kamu dapat membuatnya dengan menggunakan
<a href=/id/docs/tasks/tools/#minikube>minikube</a>,
atau kamu juga dapat menggunakan salah satu dari tempat mencoba Kubernetes berikut ini:</p><ul><li><a href=https://killercoda.com/playgrounds/scenario/kubernetes>Killercoda</a></li><li><a href=http://labs.play-with-k8s.com/>Bermain dengan Kubernetes</a></li></ul><p>Klaster kamu harus menjalankan tambahan (<em>add-on</em>) CoreDNS terlebih dahulu.
<a href=/docs/tasks/administer-cluster/coredns/#migrasi-ke-coredns>Migrasi ke CoreDNS</a>
menjelaskan tentang bagaimana menggunakan <code>kubeadm</code> untuk melakukan migrasi dari <code>kube-dns</code>.</p><p>Kubernetes servermu harus dalam versi yang sama atau lebih baru dari v1.12.
Untuk melihat versi, tekan <code>kubectl version</code>.</p><h2 id=pengenalan>Pengenalan</h2><p>DNS adalah Service bawaan dalam Kubernetes yang diluncurkan secara otomatis
melalui <em>addon manager</em>
<a href=http://releases.k8s.io/main/cluster/addons/README.md>add-on klaster</a>.</p><p>Sejak Kubernetes v1.12, CoreDNS adalah server DNS yang direkomendasikan untuk menggantikan kube-dns. Jika klaster kamu
sebelumnya menggunakan kube-dns, maka kamu mungkin masih menggunakan <code>kube-dns</code> daripada CoreDNS.</p><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Baik Service CoreDNS dan kube-dns diberi nama <code>kube-dns</code> pada <em>field</em> <code>metadata.name</code>.
Hal ini agar ada interoperabilitas yang lebih besar dengan beban kerja yang bergantung pada nama Service <code>kube-dns</code> lama untuk me-<em>resolve</em> alamat internal ke dalam klaster. Dengan menggunakan sebuah Service yang bernama <code>kube-dns</code> mengabstraksi detail implementasi yang dijalankan oleh penyedia DNS di belakang nama umum tersebut.</div><p>Jika kamu menjalankan CoreDNS sebagai sebuah Deployment, maka biasanya akan ditampilkan sebagai sebuah Service Kubernetes dengan alamat IP yang statis.
Kubelet meneruskan informasi DNS <em>resolver</em> ke setiap Container dengan argumen <code>--cluster-dns=&lt;dns-service-ip></code>.</p><p>Nama DNS juga membutuhkan domain. Kamu dapat mengonfigurasi domain lokal di kubelet
dengan argumen <code>--cluster-domain=&lt;default-local-domain></code>.</p><p>Server DNS mendukung <em>forward lookup</em> (<em>record</em> A dan AAAA), <em>port lookup</em> (<em>record</em> SRV), <em>reverse lookup</em> alamat IP (<em>record</em> PTR),
dan lain sebagainya. Untuk informasi lebih lanjut, lihatlah <a href=/id/docs/concepts/services-networking/dns-pod-service/>DNS untuk Service dan Pod</a>.</p><p>Jika <code>dnsPolicy</code> dari Pod diatur menjadi <code>default</code>, itu berarti mewarisi konfigurasi resolusi nama
dari Node yang dijalankan Pod. Resolusi DNS pada Pod
harus berperilaku sama dengan Node tersebut.
Tapi lihat <a href=/docs/tasks/debug-application-cluster/dns-debugging-resolution/#known-issues>Isu-isu yang telah diketahui</a>.</p><p>Jika kamu tidak menginginkan hal ini, atau jika kamu menginginkan konfigurasi DNS untuk Pod berbeda, kamu bisa
menggunakan argumen <code>--resolv-conf</code> pada kubelet. Atur argumen ini menjadi "" untuk mencegah Pod tidak
mewarisi konfigurasi DNS. Atur ke jalur (<em>path</em>) berkas yang tepat untuk berkas yang berbeda dengan
<code>/etc/resolv.conf</code> untuk menghindari mewarisi konfigurasi DNS.</p><h2 id=coredns>CoreDNS</h2><p>CoreDNS adalah server DNS otoritatif untuk kegunaan secara umum yang dapat berfungsi sebagai Service DNS untuk klaster, yang sesuai dengan <a href=https://github.com/kubernetes/dns/blob/master/docs/specification.md>spesifikasi dns</a>.</p><h3 id=opsi-configmap-pada-coredns>Opsi ConfigMap pada CoreDNS</h3><p>CoreDNS adalah server DNS yang modular dan mudah dipasang, dan setiap <em>plugin</em> dapat menambahkan fungsionalitas baru ke CoreDNS.
Fitur ini dapat dikonfigurasikan dengan menjaga berkas <a href=https://coredns.io/2017/07/23/corefile-explained/>Corefile</a>, yang merupakan
berkas konfigurasi dari CoreDNS. Sebagai administrator klaster, kamu dapat memodifikasi
<a class=glossary-tooltip title='Sebuah objek API yang digunakan untuk menyimpan data nonkonfidensial sebagai pasangan kunci-nilai (key-value). Pod dapat menggunakan ConfigMap sebagai variabel lingkungan, argumen baris perintah (command-line), atau berkas konfigurasi dalam sebuah volume.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a> untuk Corefile dari CoreDNS dengan mengubah cara perilaku pencarian Service DNS
pada klaster tersebut.</p><p>Di Kubernetes, CoreDNS diinstal dengan menggunakan konfigurasi Corefile bawaan sebagai berikut:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>coredns<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>Corefile</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    .:53 {
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        errors
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        health {
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>            lameduck 5s
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        }
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        ready
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>            pods insecure
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>            fallthrough in-addr.arpa ip6.arpa
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>            ttl 30
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        }
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        prometheus :9153
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        forward . /etc/resolv.conf
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        cache 30
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        loop
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        reload
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        loadbalance
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    }</span><span style=color:#bbb>    
</span></span></span></code></pre></div><p>Konfigurasi Corefile meliputi <a href=https://coredns.io/plugins/><em>plugin</em></a> berikut ini dari CoreDNS:</p><ul><li><a href=https://coredns.io/plugins/errors/>errors</a>: Kesalahan yang ditampilkan ke output standar (<em>stdout</em>)</li><li><a href=https://coredns.io/plugins/health/>health</a>: Kesehatan dari CoreDNS dilaporkan pada <code>http://localhost:8080/health</code>. Dalam sintaks yang diperluas <code>lameduck</code> akan menangani proses tidak sehat agar menunggu selama 5 detik sebelum proses tersebut dimatikan.</li><li><a href=https://coredns.io/plugins/ready/>ready</a>: <em>Endpoint</em> HTTP pada port 8181 akan mengembalikan OK 200, ketika semua <em>plugin</em> yang dapat memberi sinyal kesiapan, telah memberikan sinyalnya.</li><li><a href=https://coredns.io/plugins/kubernetes/>kubernetes</a>: CoreDNS akan menjawab pertanyaan (<em>query</em>) DNS berdasarkan IP Service dan Pod pada Kubernetes. Kamu dapat menemukan <a href=https://coredns.io/plugins/kubernetes/>lebih detail</a> tentang <em>plugin</em> itu dalam situs web CoreDNS. <code>ttl</code> memungkinkan kamu untuk mengatur TTL khusus untuk respon dari pertanyaan DNS. Standarnya adalah 5 detik. TTL minimum yang diizinkan adalah 0 detik, dan maksimum hanya dibatasi sampai 3600 detik. Mengatur TTL ke 0 akan mencegah <em>record</em> untuk di simpan sementara dalam <em>cache</em>.<br>Opsi <code>pods insecure</code> disediakan untuk kompatibilitas dengan Service <em>kube-dns</em> sebelumnya. Kamu dapat menggunakan opsi <code>pods verified</code>, yang mengembalikan <em>record</em> A hanya jika ada Pod pada Namespace yang sama untuk alamat IP yang sesuai. Opsi <code>pods disabled</code> dapat digunakan jika kamu tidak menggunakan <em>record</em> Pod.</li><li><a href=https://coredns.io/plugins/metrics/>prometheus</a>: Metrik dari CoreDNS tersedia pada <code>http://localhost:9153/metrics</code> dalam format yang sesuai dengan <a href=https://prometheus.io/>Prometheus</a> (dikenal juga sebagai OpenMetrics).</li><li><a href=https://coredns.io/plugins/forward/>forward</a>: Setiap pertanyaan yang tidak berada dalam domain klaster Kubernetes akan diteruskan ke <em>resolver</em> yang telah ditentukan dalam berkas (/etc/resolv.conf).</li><li><a href=https://coredns.io/plugins/cache/>cache</a>: Ini untuk mengaktifkan <em>frontend cache</em>.</li><li><a href=https://coredns.io/plugins/loop/>loop</a>: Mendeteksi <em>forwarding loop</em> sederhana dan menghentikan proses CoreDNS jika <em>loop</em> ditemukan.</li><li><a href=https://coredns.io/plugins/reload>reload</a>: Mengizinkan <em>reload</em> otomatis Corefile yang telah diubah. Setelah kamu mengubah konfigurasi ConfigMap, beri waktu sekitar dua menit agar perubahan yang kamu lakukan berlaku.</li><li><a href=https://coredns.io/plugins/loadbalance>loadbalance</a>: Ini adalah <em>load balancer</em> DNS secara <em>round-robin</em> yang mengacak urutan <em>record</em> A, AAAA, dan MX dalam setiap responnya.</li></ul><p>Kamu dapat memodifikasi perilaku CoreDNS bawaan dengan memodifikasi ConfigMap.</p><h3 id=konfigurasi-stub-domain-dan-nameserver-upstream-dengan-menggunakan-coredns>Konfigurasi <em>Stub-domain</em> dan <em>Nameserver Upstream</em> dengan menggunakan CoreDNS</h3><p>CoreDNS memiliki kemampuan untuk mengonfigurasi <em>stubdomain</em> dan <em>nameserver upstream</em> dengan menggunakan <a href=https://coredns.io/plugins/forward/><em>plugin</em> forward</a>.</p><h4 id=contoh>Contoh</h4><p>Jika operator klaster memiliki sebuah server domain <a href=https://www.consul.io/>Consul</a> yang terletak di 10.150.0.1, dan semua nama Consul memiliki akhiran .consul.local. Untuk mengonfigurasinya di CoreDNS, administrator klaster membuat bait (<em>stanza</em>) berikut dalam ConfigMap CoreDNS.</p><pre tabindex=0><code>consul.local:53 {
        errors
        cache 30
        forward . 10.150.0.1
    }
</code></pre><p>Untuk memaksa secara eksplisit semua pencarian DNS <em>non-cluster</em> melalui <em>nameserver</em> khusus pada 172.16.0.1, arahkan <code>forward</code> ke <em>nameserver</em> bukan ke <code>/etc/resolv.conf</code></p><pre tabindex=0><code>forward .  172.16.0.1
</code></pre><p>ConfigMap terakhir bersama dengan konfigurasi <code>Corefile</code> bawaan terlihat seperti berikut:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>coredns<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>Corefile</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    .:53 {
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        errors
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        health
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        kubernetes cluster.local in-addr.arpa ip6.arpa {
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>           pods insecure
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>           fallthrough in-addr.arpa ip6.arpa
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        }
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        prometheus :9153
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        forward . 172.16.0.1
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        cache 30
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        loop
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        reload
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        loadbalance
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    }
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    consul.local:53 {
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        errors
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        cache 30
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        forward . 10.150.0.1
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    }</span><span style=color:#bbb>    
</span></span></span></code></pre></div><p>Perangkat <code>kubeadm</code> mendukung terjemahan otomatis dari ConfigMap kube-dns
ke ConfigMap CoreDNS yang setara.</p><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Sementara ini kube-dns dapat menerima FQDN untuk <em>stubdomain</em> dan <em>nameserver</em> (mis: ns.foo.com), namun CoreDNS belum mendukung fitur ini.
Selama penerjemahan, semua <em>nameserver</em> FQDN akan dihilangkan dari konfigurasi CoreDNS.</div><h2 id=konfigurasi-coredns-yang-setara-dengan-kube-dns>Konfigurasi CoreDNS yang setara dengan kube-dns</h2><p>CoreDNS mendukung fitur kube-dns dan banyak lagi lainnya.
ConfigMap dibuat agar kube-dns mendukung <code>StubDomains</code> dan <code>upstreamNameservers</code> untuk diterjemahkan ke <em>plugin</em> <code>forward</code> dalam CoreDNS.
Begitu pula dengan <em>plugin</em> <code>Federations</code> dalam kube-dns melakukan translasi untuk <em>plugin</em> <code>federation</code> dalam CoreDNS.</p><h3 id=contoh-1>Contoh</h3><p>Contoh ConfigMap ini untuk kube-dns menentukan federasi, <em>stub domain</em> dan server <em>upstream nameserver</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>federations</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span>{<span style=color:#b44>&#34;foo&#34;</span><span style=color:#bbb> </span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;foo.feddomain.com&#34;</span>}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>stubDomains</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span>{<span style=color:#b44>&#34;abc.com&#34;</span><span style=color:#bbb> </span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;1.2.3.4&#34;</span>],<span style=color:#bbb> </span><span style=color:#b44>&#34;my.cluster.local&#34;</span><span style=color:#bbb> </span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;2.3.4.5&#34;</span>]}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>upstreamNameservers</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span>[<span style=color:#b44>&#34;8.8.8.8&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;8.8.4.4&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span></code></pre></div><p>Untuk konfigurasi yang setara dengan CoreDNS buat Corefile berikut:</p><ul><li>Untuk federasi:</li></ul><pre tabindex=0><code>federation cluster.local {
    foo foo.feddomain.com
}
</code></pre><ul><li>Untuk stubDomain:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>abc.com:53 {<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>errors<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>cache 30<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>forward . 1.2.3.4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>my.cluster.local:53 {<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>errors<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>cache 30<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>forward . 2.3.4.5<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>}<span style=color:#bbb>
</span></span></span></code></pre></div><p>Corefile lengkap dengan <em>plugin</em> bawaan:</p><pre tabindex=0><code>.:53 {
    errors
    health
    kubernetes cluster.local in-addr.arpa ip6.arpa {
        pods insecure
        fallthrough in-addr.arpa ip6.arpa
    }
    federation cluster.local {
        foo foo.feddomain.com
    }
    prometheus :9153
    forward . 8.8.8.8 8.8.4.4
    cache 30
}
abc.com:53 {
    errors
    cache 30
    forward . 1.2.3.4
}
my.cluster.local:53 {
    errors
    cache 30
    forward . 2.3.4.5
}
</code></pre><h2 id=migrasi-ke-coredns>Migrasi ke CoreDNS</h2><p>Untuk bermigrasi dari kube-dns ke CoreDNS,
<a href=https://coredns.io/2018/05/21/migration-from-kube-dns-to-coredns/>artikel blog</a> yang detail
tersedia untuk membantu pengguna mengadaptasi CoreDNS sebagai pengganti dari kube-dns.</p><p>Kamu juga dapat bermigrasi dengan menggunakan
<a href=https://github.com/coredns/deployment/blob/master/kubernetes/deploy.sh>skrip <em>deploy</em></a> CoreDNS yang resmi.</p><h2 id=selanjutnya>Selanjutnya</h2><ul><li>Baca <a href=/docs/tasks/administer-cluster/dns-debugging-resolution/><em>Debugging</em> Resolusi DNS</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b64a1d2bb3f4ed9f7021134e09a75c36>5 - Melakukan Reservasi Sumber Daya Komputasi untuk Daemon Sistem</h1><p>Node Kubernetes dapat dijadwalkan sesuai dengan kapasitas. Secara bawaan, Pod dapat menggunakan
semua kapasitas yang tersedia pada sebuah Node. Ini merupakan masalah karena Node
sebenarnya menjalankan beberapa <em>daemon</em> sistem yang diperlukan oleh OS dan Kubernetes itu sendiri.
Jika sumber daya pada Node tidak direservasi untuk <em>daemon-daemon</em> tersebut, maka
Pod dan <em>daemon</em> akan berlomba-lomba menggunakan sumber daya yang tersedia, sehingga
menyebabkan <em>starvation</em> sumber daya pada Node.</p><p>Fitur bernama <code>Allocatable</code> pada Node diekspos oleh kubelet yang berfungsi untuk melakukan
reservasi sumber daya komputasi untuk <em>daemon</em> sistem. Kubernetes merekomendasikan admin
klaster untuk mengatur <code>Allocatable</code> pada Node berdasarkan tingkat kepadatan (<em>density</em>) beban kerja setiap Node.</p><h2 id=sebelum-kamu-memulai>Sebelum kamu memulai</h2><p><p>Kamu harus memiliki klaster Kubernetes, dan perangkat baris perintah kubectl
juga harus dikonfigurasikan untuk berkomunikasi dengan klastermu. Jika kamu
belum memiliki klaster, kamu dapat membuatnya dengan menggunakan
<a href=/id/docs/tasks/tools/#minikube>minikube</a>,
atau kamu juga dapat menggunakan salah satu dari tempat mencoba Kubernetes berikut ini:</p><ul><li><a href=https://killercoda.com/playgrounds/scenario/kubernetes>Killercoda</a></li><li><a href=http://labs.play-with-k8s.com/>Bermain dengan Kubernetes</a></li></ul>Kubernetes servermu harus dalam versi yang sama atau lebih baru dari 1.8.
Untuk melihat versi, tekan <code>kubectl version</code>.
Kamu harus menjalankan Kubernetes server dengan versi 1.17 atau yang lebih baru
untuk menggunakan perintah baris kubelet dengan opsi <code>--reserved-cpus</code> untuk
menyetel <a href=#melakukan-reservasi-daftar-cpu-secara-eksplisit>daftar reservasi CPU secara eksplisit</a>.</p><h2 id=allocatable-pada-node><em>Allocatable</em> pada Node</h2><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>       Kapasitas Node
</span></span><span style=display:flex><span>------------------------------
</span></span><span style=display:flex><span>|       kube-reserved        |
</span></span><span style=display:flex><span>|----------------------------|
</span></span><span style=display:flex><span>|      system-reserved       |
</span></span><span style=display:flex><span>|----------------------------|
</span></span><span style=display:flex><span>|     eviction-threshold     |
</span></span><span style=display:flex><span>|     (batas pengusiran)     |
</span></span><span style=display:flex><span>|----------------------------|
</span></span><span style=display:flex><span>|                            |
</span></span><span style=display:flex><span>|        allocatable         |
</span></span><span style=display:flex><span>| (dapat digunakan oleh Pod) |
</span></span><span style=display:flex><span>|                            |
</span></span><span style=display:flex><span>|                            |
</span></span><span style=display:flex><span>------------------------------
</span></span></code></pre></div><p><code>Allocatable</code> atau sumber daya yang dialokasikan pada sebuah Node Kubernetes merupakan
jumlah sumber daya komputasi yang dapat digunakan oleh Pod. Penjadwal tidak
dapat melakukan penjadwalan melebihi <code>Allocatable</code>. Saat ini dukungan terhadap
<code>CPU</code>, <code>memory</code> dan <code>ephemeral-storage</code> tersedia.</p><p><code>Allocatable</code> pada Node diekspos oleh objek API <code>v1.Node</code> dan merupakan
bagian dari baris perintah <code>kubectl describe node</code>.</p><p>Sumber daya dapat direservasi untuk dua kategori <em>daemon</em> sistem melalui kubelet.</p><h3 id=mengaktifkan-qos-dan-tingkatan-cgroup-dari-pod>Mengaktifkan QoS dan tingkatan cgroup dari Pod</h3><p>Untuk menerapkan batasan <code>Allocatable</code> pada Node, kamu harus mengaktifkan
hierarki cgroup yang baru melalui <em>flag</em> <code>--cgroups-per-qos</code>. Secara bawaan, <em>flag</em> ini
telah aktif. Saat aktif, kubelet akan memasukkan semua Pod pengguna di bawah
sebuah hierarki cgroup yang dikelola oleh kubelet.</p><h3 id=mengonfigurasi-driver-cgroup>Mengonfigurasi <em>driver</em> cgroup</h3><p>Manipulasi terhadap hierarki cgroup pada hos melalui <em>driver</em> cgroup didukung oleh kubelet.
<em>Driver</em> dikonfigurasi melalui <em>flag</em> <code>--cgroup-driver</code>.</p><p>Nilai yang didukung adalah sebagai berikut:</p><ul><li><code>cgroupfs</code> merupakan <em>driver</em> bawaan yang melakukan manipulasi secara langsung
terhadap <em>filesystem</em> cgroup pada hos untuk mengelola <em>sandbox</em> cgroup.</li><li><code>systemd</code> merupakan <em>driver</em> alternatif yang mengelola <em>sandbox</em> cgroup menggunakan
bagian dari sumber daya yang didukung oleh sistem <em>init</em> yang digunakan.</li></ul><p>Tergantung dari konfigurasi <em>runtime</em> Container yang digunakan,
operator dapat memilih <em>driver</em> cgroup tertentu untuk memastikan perilaku sistem yang tepat.
Misalnya, jika operator menggunakan <em>driver</em> cgroup <code>systemd</code> yang disediakan oleh
<em>runtime</em> docker, maka kubelet harus diatur untuk menggunakan <em>driver</em> cgroup <code>systemd</code>.</p><h3 id=kube-reserved>Kube Reserved</h3><ul><li><strong><em>Flag</em> Kubelet</strong>: <code>--kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]</code></li><li><strong><em>Flag</em> Kubelet</strong>: <code>--kube-reserved-cgroup=</code></li></ul><p><code>kube-reserved</code> berfungsi untuk mengambil informasi sumber daya reservasi
untuk <em>daemon</em> sistem Kubernetes, seperti kubelet, <em>runtime</em> Container, detektor masalah pada Node, dsb.
<code>kube-reserved</code> tidak berfungsi untuk mereservasi sumber daya untuk <em>daemon</em> sistem yang berjalan
sebagai Pod. <code>kube-reserved</code> merupakan fungsi dari kepadatan Pod pada Node.</p><p>Selain dari <code>cpu</code>, <code>memory</code>, dan <code>ephemeral-storage</code>,<code>pid</code> juga dapat
diatur untuk mereservasi jumlah ID proses untuk <em>daemon</em> sistem Kubernetes.</p><p>Secara opsional, kamu dapat memaksa <em>daemon</em> sistem melalui setelan <code>kube-reserved</code>.
Ini dilakukan dengan menspesifikasikan <em>parent</em> cgroup sebagai nilai dari <em>flag</em> <code>--kube-reserved-cgroup</code> pada kubelet.</p><p>Kami merekomendasikan <em>daemon</em> sistem Kubernetes untuk ditempatkan pada
tingkatan cgroup yang tertinggi (contohnya, <code>runtime.slice</code> pada mesin systemd).
Secara ideal, setiap <em>daemon</em> sistem sebaiknya dijalankan pada <em>child</em> cgroup
di bawah <em>parent</em> ini. Lihat <a href=https://git.k8s.io/design-proposals-archive/node/node-allocatable.md#recommended-cgroups-setup>dokumentasi</a>
untuk mengetahui rekomendasi hierarki cgroup secara detail.</p><p>Catatan: kubelet <strong>tidak membuat</strong> <code>--kube-reserved-cgroup</code> jika cgroup
yang diberikan tidak ada pada sistem. Jika cgroup yang tidak valid diberikan,
maka kubelet akan mengalami kegagalan.</p><h3 id=system-reserved>System Reserved</h3><ul><li><strong><em>Flag</em> Kubelet</strong>: <code>--system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi][,][pid=1000]</code></li><li><strong><em>Flag</em> Kubelet</strong>: <code>--system-reserved-cgroup=</code></li></ul><p><code>system-reserved</code> berfungsi untuk mengetahui reservasi sumber daya untuk
<em>daemon</em> sistem pada OS, seperti <code>sshd</code>, <code>udev</code>, dan lainnya. <code>system-reserved</code> sebaiknya
mereservasi memori untuk kernel juga, karena memori kernel tidak termasuk dalam
hitungan kalkulasi Pod pada Kubernetes. Kami juga merekomendasikan reservasi sumber daya
untuk sesi (<em>session</em>) login pengguna (contohnya, <code>user.slice</code> di dalam dunia systemd).</p><h3 id=melakukan-reservasi-daftar-cpu-secara-eksplisit>Melakukan Reservasi Daftar CPU secara Eksplisit</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code></div><ul><li><strong><em>Flag</em> Kubelet</strong>: <code>--reserved-cpus=0-3</code></li></ul><p><code>reserved-cpus</code> berfungsi untuk mendefinisikan <a href=https://www.kernel.org/doc/Documentation/cgroup-v1/cpusets.txt>cpuset</a> secara eksplisit untuk
<em>daemon</em> sistem OS dan <em>daemon</em> sistem Kubernetes. <code>reserved-cpus</code> dimaksudkan untuk
sistem-sistem yang tidak mendefinisikan tingkatan cgroup tertinggi secara terpisah untuk
<em>daemon</em> sistem OS dan <em>daemon</em> sistem Kubernetes yang berkaitan dengan sumber daya cpuset.</p><p>Jika kubelet <strong>tidak memiliki</strong> <code>--system-reserved-cgroup</code> dan <code>--kube-reserved-cgroup</code>,
cpuset akan diberikan secara eksplisit oleh <code>reserved-cpus</code>, yang akan menimpa definisi
yang diberikan oleh opsi <code>--kube-reserved</code> dan <code>--system-reserved</code>.</p><p>Opsi ini dirancang secara spesifik untuk kasus-kasus Telco/NFV, di mana <em>interrupt</em> atau <em>timer</em>
yang tidak terkontrol bisa memengaruhi performa dari beban kerja. Kamu dapat menggunakan
opsi untuk untuk mendefinisikan cpuset secara eksplisit untuk <em>daemon</em> sistem/Kubernetes dan
<em>interrupt</em>/<em>timer</em>, sehingga CPU sisanya dalam sistem akan digunakan untuk beban kerja saja,
dengan dampak yang sedikit terhadap <em>interrupt</em>/<em>timer</em> yang tidak terkontrol. Untuk
memindahkan <em>daemon</em> sistem, <em>daemon</em> Kubernetes serta <em>interrrupt</em>/<em>timer</em> Kubernetes supaya
menggunakan cpuset yang eksplisit didefinisikan oleh opsi ini, sebaiknya digunakan mekanisme lain di luar Kubernetes. Contohnya: pada Centos, kamu dapat melakukan ini dengan menggunakan
toolset yang sudah disetel.</p><h3 id=batas-pengusiran-eviction-threshold>Batas Pengusiran (<em>Eviction Threshold</em>)</h3><ul><li><strong><em>Flag</em> Kubelet</strong>: <code>--eviction-hard=[memory.available&lt;500Mi]</code></li></ul><p>Tekanan memori pada tingkatan Node menyebabkan sistem OOM (<em>Out Of Memory</em>) yang
berdampak pada Node secara keseluruhan dan semua Pod yang dijalankan di dalamnya.
Node dapat berubah menjadi <em>offline</em> sementara sampai memori berhasil diklaim kembali.
Untuk menghindari sistem OOM, atau mengurangi kemungkinan terjadinya OOM, kubelet
menyediakan fungsi untuk pengelolaan saat <a href=/docs/tasks/administer-cluster/out-of-resource/>Kehabisan Sumber Daya (<em>Out of Resource</em>)</a>.
Pengusiran dapat dilakukan hanya untuk kasus kehabisan <code>memory</code> dan <code>ephemeral-storage</code>. Dengan mereservasi
sebagian memori melalui <em>flag</em> <code>--eviction-hard</code>, kubelet akan berusaha untuk "mengusir" (<em>evict</em>)
Pod ketika ketersediaan memori pada Node jatuh di bawah nilai yang telah direservasi.
Dalam bahasa sederhana, jika <em>daemon</em> sistem tidak ada pada Node, maka Pod tidak dapat menggunakan
memori melebihi nilai yang ditentukan oleh <em>flag</em> <code>--eviction-hard</code>. Karena alasan ini,
sumber daya yang direservasi untuk pengusiran tidak tersedia untuk Pod.</p><h3 id=memaksakan-allocatable-pada-node>Memaksakan <em>Allocatable</em> pada Node</h3><ul><li><strong><em>Flag</em> Kubelet</strong>: <code>--enforce-node-allocatable=pods[,][system-reserved][,][kube-reserved]</code></li></ul><p>Penjadwal menganggap <code>Allocatable</code> sebagai kapasitas yang tersedia untuk digunakan oleh Pod.</p><p>Secara bawaan, kubelet memaksakan <code>Allocatable</code> untuk semua Pod. Pemaksaan dilakukan
dengan cara "mengusir" Pod-Pod ketika penggunaan sumber daya Pod secara keseluruhan telah
melewati nilai <code>Allocatable</code>. Lihat <a href=/docs/tasks/administer-cluster/out-of-resource/#eviction-policy>bagian ini</a>
untuk mengetahui kebijakan pengusiran secara detail. Pemaksaan ini dikendalikan dengan
cara memberikan nilai Pod melalui <em>flag</em> <code>--enforce-node-allocatable</code> pada kubelet.</p><p>Secara opsional, kubelet dapat diatur untuk memaksakan <code>kube-reserved</code> dan
<code>system-reserved</code> dengan memberikan nilai melalui <em>flag</em> tersebut. Sebagai catatan,
jika kamu mengatur <code>kube-reserved</code>, maka kamu juga harus mengatur <code>--kube-reserved-cgroup</code>. Begitu pula
jika kamu mengatur <code>system-reserved</code>, maka kamu juga harus mengatur <code>--system-reserved-cgroup</code>.</p><h2 id=panduan-umum>Panduan Umum</h2><p><em>Daemon</em> sistem dilayani mirip seperti Pod <code>Guaranteed</code> yang terjamin sumber dayanya.
<em>Daemon</em> sistem dapat melakukan <em>burst</em> di dalam jangkauan cgroup. Perilaku ini
dikelola sebagai bagian dari penggelaran (<em>deployment</em>) Kubernetes. Sebagai contoh,
kubelet harus memiliki cgroup sendiri dan membagikan sumber daya <code>kube-reserved</code> dengan
<em>runtime</em> Container. Namun begitu, kubelet tidak dapat melakukan <em>burst</em> dan menggunakan
semua sumber daya yang tersedia pada Node jika <code>kube-reserved</code> telah dipaksakan pada sistem.</p><p>Kamu harus berhati-hati ekstra ketika memaksakan reservasi <code>system-reserved</code> karena dapat
menyebabkan layanan sistem yang terpenting mengalami CPU <em>starvation</em>, OOM <em>killed</em>, atau tidak
dapat melakukan <em>fork</em> pada Node. Kami menyarankan untuk memaksakan <code>system-reserved</code> hanya
jika pengguna telah melakukan <em>profiling</em> sebaik mungkin pada Node mereka untuk
mendapatkan estimasi yang akurat dan percaya diri terhadap kemampuan mereka untuk
memulihkan sistem ketika ada grup yang terkena OOM <em>killed</em>.</p><ul><li>Untuk memulai, paksakan <code>Allocatable</code> pada Pod.</li><li>Ketika <em>monitoring</em> dan <em>alerting</em> telah cukup dilakukan untuk memonitor <em>daemon</em>
dari sistem Kubernetes, usahakan untuk memaksakan <code>kube-reserved</code> berdasarkan penggunakan heuristik.</li><li>Jika benar-benar diperlukan, paksakan <code>system-reserved</code> secara bertahap.</li></ul><p>Sumber daya yang diperlukan oleh <em>daemon</em> sistem Kubernetes dapat tumbuh seiring waktu dengan
adanya penambahan fitur-fitur baru. Proyek Kubernetes akan berusaha untuk menurunkan penggunaan sumber daya
dari <em>daemon</em> sistem Node, tetapi belum menjadi prioritas untuk saat ini.
Kamu dapat berekspektasi bahwa fitur kapasitas <code>Allocatable</code> ini akan dihapus pada versi yang akan datang.</p><h2 id=contoh-skenario>Contoh Skenario</h2><p>Berikut ini merupakan contoh yang menggambarkan komputasi <code>Allocatable</code> pada Node:</p><ul><li>Node memiliki 16 CPU, memori sebesar 32Gi, dan penyimpanan sebesar 100Gi.</li><li><code>--kube-reserved</code> diatur menjadi <code>cpu=1,memory=2Gi,ephemeral-storage=1Gi</code></li><li><code>--system-reserved</code> diatur menjadi <code>cpu=500m,memory=1Gi,ephemeral-storage=1Gi</code></li><li><code>--eviction-hard</code> diatur menjadi <code>memory.available&lt;500Mi,nodefs.available&lt;10%</code></li></ul><p>Dalam skenario ini, <code>Allocatable</code> akan menjadi 14.5 CPU, memori 28.5Gi, dan penyimpanan
lokal 88Gi.
Penjadwal memastikan bahwa semua Pod yang berjalan pada Node ini secara total tidak meminta memori melebihi
28.5Gi dan tidak menggunakan penyimpanan lokal melebihi 88Gi.
Pengusiran Pod akan dilakukan kubelet ketika penggunaan memori keseluruhan oleh Pod telah melebihi 28.5Gi,
atau jika penggunaan penyimpanan keseluruhan telah melebihi 88Gi. Jika semua proses pada Node mengonsumsi
CPU sebanyak-banyaknya, Pod-Pod tidak dapat mengonsumsi lebih dari 14.5 CPU.</p><p>Jika <code>kube-reserved</code> dan/atau <code>system-reserved</code> tidak dipaksakan dan <em>daemon</em> sistem
melebihi reservasi mereka, maka kubelet akan mengusir Pod ketika penggunaan memori pada Node
melebihi 31.5Gi atau penggunaan penyimpanan melebihi 90Gi.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1e966f5d0540bbee0876f9d0d08d54dc>6 - Membagi sebuah Klaster dengan Namespace</h1><p>Laman ini menunjukkan bagaimana cara melihat, menggunakan dan menghapus <a class=glossary-tooltip title='Sebuah abstraksi yang digunakan oleh Kubernetes untuk mendukung multipel klaster virtual pada klaster fisik yang sama.' data-toggle=tooltip data-placement=top href=/id/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespaces>namespaces</a>. Laman ini juga menunjukkan bagaimana cara menggunakan Namespace Kubernetes namespaces untuk membagi klaster kamu.</p><h2 id=sebelum-kamu-memulai>Sebelum kamu memulai</h2><ul><li>Memiliki <a href=/id/docs/setup/>Klaster Kubernetes</a>.</li><li>Memiliki pemahaman dasar <a href=/id/docs/concepts/workloads/pods/pod/><em>Pod</em></a>, <a href=/id/docs/concepts/services-networking/service/><em>Service</em></a>, dan <a href=/id/docs/concepts/workloads/controllers/deployment/><em>Deployment</em></a> dalam Kubernetes.</li></ul><h2 id=melihat-namespace>Melihat Namespace</h2><ol><li>Untuk melihat Namespace yang ada saat ini pada sebuah klaster anda bisa menggunakan:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get namespaces
</span></span></code></pre></div><pre tabindex=0><code>NAME          STATUS    AGE
default       Active    11d
kube-system   Active    11d
kube-public   Active    11d
</code></pre><p>Kubernetes mulai dengan tiga Namespace pertama:</p><ul><li><code>default</code> Namespace bawaan untuk objek-objek yang belum terkait dengan Namespace lain</li><li><code>kube-system</code> Namespace untuk objek-objek yang dibuat oleh sistem Kubernetes</li><li><code>kube-public</code> Namespace ini dibuat secara otomatis dan dapat dibaca oleh seluruh pengguna (termasuk yang tidak terotentikasi). Namespace ini sering dicadangkan untuk kepentingan klaster, untuk kasus dimana beberapa sumber daya seharusnya dapat terlihat dan dapat terlihat secara publik di seluruh klaster. Aspek publik pada Namespace ini hanya sebuah konvensi bukan suatu kebutuhan.</li></ul><p>Kamu bisa mendapat ringkasan Namespace tertentu dengan menggunakan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get namespaces &lt;name&gt;
</span></span></code></pre></div><p>Atau kamu bisa mendapatkan informasi detail menggunakan:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe namespaces &lt;name&gt;
</span></span></code></pre></div><pre tabindex=0><code>Name:           default
Labels:         &lt;none&gt;
Annotations:    &lt;none&gt;
Status:         Active

No resource quota.

Resource Limits
 Type       Resource    Min Max Default
 ----               --------    --- --- ---
 Container          cpu         -   -   100m
</code></pre><p>Sebagai catatan, detail diatas menunjukkan baik kuota sumber daya (jika ada) dan juga jangkauan batas sumber daya.</p><p>Kuota sumber daya melacak penggunaan total sumber daya didalam Namespace dan mengijinkan operator-operator klaster mendefinisikan batas atas penggunaan sumber daya yang dapat di gunakan sebuah Namespace.</p><p>Jangkauan batas mendefinisikan pertimbangan min/maks jumlah sumber daya yang dapat di gunakan oleh sebuah entitas dalam sebuah Namespace.</p><p>Lihatlah <a href=https://git.k8s.io/community/contributors/design-proposals/resource-management/admission_control_limit_range.md>Kontrol Admisi: Rentang Batas</a></p><p>Sebuah Namespace dapat berada dalam salah satu dari dua buah fase:</p><ul><li><code>Active</code> Namespace sedang digunakan</li><li><code>Terminating</code> Namespace sedang dihapus dan tidak dapat digunakan untuk objek-objek baru</li></ul><p>Lihat <a href=https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#phases>dokumentasi desain</a> untuk detil lebih lanjut.</p><h2 id=membuat-sebuah-namespace-baru>Membuat sebuah Namespace baru</h2><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Hindari membuat Namespace dengan awalan <code>kube-</code>, karena awalan ini dicadangkan untuk Namespace dari sistem Kubernetes.</div><ol><li><p>Buat berkas YAML baru dengan nama <code>my-namespace.yaml</code> dengan isi berikut ini:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Namespace<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>&lt;masukkan-nama-namespace-disini&gt;<span style=color:#bbb>
</span></span></span></code></pre></div><p>Then run:</p><pre tabindex=0><code>kubectl create -f ./my-namespace.yaml
</code></pre></li><li><p>Sebagai alternatif, kamu bisa membuat Namespace menggunakan perintah dibawah ini:</p><pre tabindex=0><code>kubectl create namespace &lt;masukkan-nama-namespace-disini&gt;
</code></pre></li></ol><p>Nama Namespace kamu harus merupakan
<a href=/docs/concepts/overview/working-with-objects/names#dns-label-names>Label DNS</a> yang valid.</p><p>Ada kolom opsional <code>finalizers</code>, yang memungkinkan <em>observables</em> untuk membersihkan sumber daya ketika Namespace dihapus. Ingat bahwa jika kamu memberikan finalizer yang tidak ada, Namespace akan dibuat tapi akan berhenti pada status <code>Terminating</code> jika pengguna mencoba untuk menghapusnya.</p><p>Informasi lebih lanjut mengenai <code>finalizers</code> bisa dibaca pada <a href=https://git.k8s.io/community/contributors/design-proposals/architecture/namespaces.md#finalizers>dokumentasi desain</a> dari Namespace.</p><h2 id=menghapus-namespace>Menghapus Namespace</h2><p>Hapus Namespace dengan</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete namespaces &lt;insert-some-namespace-name&gt;
</span></span></code></pre></div><div class="alert alert-danger warning callout" role=alert><strong>Peringatan:</strong> Ini akan menghapus semua hal yang ada dalam Namespace!</div><p>Proses penghapusan ini asinkron, jadi untuk beberapa waktu kamu akan melihat Namespace dalam status <code>Terminating</code>.</p><h2 id=membagi-klaster-kamu-menggunakan-namespace-kubernetes>Membagi klaster kamu menggunakan Namespace Kubernetes</h2><ol><li><p>Pahami Namespace bawaan</p><p>Secara bawaan, sebuah klaster Kubernetes akan membuat Namespace bawaan ketika menyediakan klaster untuk menampung Pod, Service, dan Deployment yang digunakan oleh klaster.</p><p>Dengan asumsi kamu memiliki klaster baru, kamu bisa mengecek Namespace yang tersedia dengan melakukan hal berikut:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get namespaces
</span></span></code></pre></div><pre tabindex=0><code>NAME      STATUS    AGE
default   Active    13m
</code></pre></li><li><p>Membuat Namespace baru</p><p>Untuk latihan ini, kita akan membuat dua Namespace Kubernetes tambahan untuk menyimpan konten kita</p><p>Dalam sebuah skenario dimana sebuah organisasi menggunakan klaster Kubernetes yang digunakan bersama untuk penggunaan pengembangan dan produksi:</p><p>Tim pengembang ingin mengelola ruang di dalam klaster dimana mereka bisa melihat daftar Pod, Service, dan Deployment yang digunakan untuk membangun dan menjalankan apliksi mereka. Di ruang ini sumber daya akan datang dan pergi, dan pembatasan yang tidak ketat mengenai siapa yang bisa atau tidak bisa memodifikasi sumber daya untuk mendukung pengembangan secara gesit (<em>agile</em>).</p><p>Tim operasi ingin mengelola ruang didalam klaster dimana mereka bisa memaksakan prosedur ketat mengenai siapa yang bisa atau tidak bisa melakukan manipulasi pada kumpulan Pod, Layanan, dan Deployment yang berjalan pada situs produksi.</p><p>Satu pola yang bisa diikuti organisasi ini adalah dengan membagi klaster Kubernetes menjadi dua Namespace: <code>development</code> dan <code>production</code></p><p>Mari kita buat dua Namespace untuk menyimpan hasil kerja kita.</p><p>Buat Namespace <code>development</code> menggunakan kubectl:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
</span></span></code></pre></div><p>Kemudian mari kita buat Namespace <code>production</code> menggunakan kubectl:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
</span></span></code></pre></div><p>Untuk memastikan apa yang kita lakukan benar, lihat seluruh Namespace dalam klaster.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get namespaces --show-labels
</span></span></code></pre></div><pre tabindex=0><code>NAME          STATUS    AGE       LABELS
default       Active    32m       &lt;none&gt;
development   Active    29s       name=development
production    Active    23s       name=production
</code></pre></li><li><p>Buat pod pada setiap Namespace</p><p>Sebuah Namespace Kubernetes memberikan batasan untuk Pod, Service, dan Deployment dalam klaster.</p><p>Pengguna yang berinteraksi dengan salah satu Namespace tidak melihat konten di dalam Namespace lain</p><p>Untuk menunjukkan hal ini, mari kita jalankan Deployment dan Pod sederhana di dalam Namespace <code>development</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create deployment snowflake --image<span style=color:#666>=</span>k8s.gcr.io/serve_hostname -n<span style=color:#666>=</span>development
</span></span><span style=display:flex><span>kubectl scale deployment snowflake --replicas<span style=color:#666>=</span><span style=color:#666>2</span> -n<span style=color:#666>=</span>development
</span></span></code></pre></div><p>Kita baru aja membuat sebuah Deployment yang memiliki ukuran replika dua yang menjalankan Pod dengan nama <code>snowflake</code> dengan sebuah Container dasar yang hanya melayani <em>hostname</em>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment -n<span style=color:#666>=</span>development
</span></span></code></pre></div><pre tabindex=0><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
snowflake    2/2     2            2           2m
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>snowflake -n<span style=color:#666>=</span>development
</span></span></code></pre></div><pre tabindex=0><code>NAME                         READY     STATUS    RESTARTS   AGE
snowflake-3968820950-9dgr8   1/1       Running   0          2m
snowflake-3968820950-vgc4n   1/1       Running   0          2m
</code></pre><p>Dan ini merupakan sesuatu yang bagus, dimana pengembang bisa melakukan hal yang ingin mereka lakukan tanpa harus khawatir hal itu akan mempengaruhi konten pada namespace <code>production</code>.</p><p>Mari kita pindah ke Namespace <code>production</code> dan menujukkan bagaimana sumber daya di satu Namespace disembunyikan dari yang lain</p><p>Namespace <code>production</code> seharusnya kosong, dan perintah berikut ini seharusnya tidak menghasilkan apapun.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment -n<span style=color:#666>=</span>production
</span></span><span style=display:flex><span>kubectl get pods -n<span style=color:#666>=</span>production
</span></span></code></pre></div><p><code>Production</code> Namespace ingin menjalankan <code>cattle</code>, mari kita buat beberapa Pod <code>cattle</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create deployment cattle --image<span style=color:#666>=</span>k8s.gcr.io/serve_hostname -n<span style=color:#666>=</span>production
</span></span><span style=display:flex><span>kubectl scale deployment cattle --replicas<span style=color:#666>=</span><span style=color:#666>5</span> -n<span style=color:#666>=</span>production
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl get deployment -n<span style=color:#666>=</span>production
</span></span></code></pre></div><pre tabindex=0><code>NAME         READY   UP-TO-DATE   AVAILABLE   AGE
cattle       5/5     5            5           10s
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>cattle -n<span style=color:#666>=</span>production
</span></span></code></pre></div><pre tabindex=0><code>NAME                      READY     STATUS    RESTARTS   AGE
cattle-2263376956-41xy6   1/1       Running   0          34s
cattle-2263376956-kw466   1/1       Running   0          34s
cattle-2263376956-n4v97   1/1       Running   0          34s
cattle-2263376956-p5p3i   1/1       Running   0          34s
cattle-2263376956-sxpth   1/1       Running   0          34s
</code></pre></li></ol><p>Sampai sini, seharusnya sudah jelas bahwa sumber daya yang dibuat pengguna pada sebuah Namespace disembunyikan dari Namespace lainnya.</p><p>Seiring dengan evolusi dukungan kebijakan di Kubernetes, kami akan memperluas skenario ini untuk menunjukkan bagaimana kamu bisa menyediakan aturan otorisasi yang berbeda untuk tiap Namespace.</p><h2 id=memahami-motivasi-penggunaan-namespace>Memahami motivasi penggunaan Namespace</h2><p>Sebuah klaster tunggal umumnya bisa memenuhi kebutuhan pengguna yang berbeda atau kelompok pengguna (itulah sebabnya disebut 'komunitas pengguna').</p><p>Namespace Kubernetes membantu proyek-proyek, tim-tim dan pelanggan yang berbeda untuk berbagi klaster Kubernetes.</p><p>Ini dilakukan dengan menyediakan hal berikut:</p><ol><li>Cakupan untuk <a href=/id/docs/concepts/overview/working-with-objects/names/>Names</a>.</li><li>Sebuah mekanisme untuk memasang otorisasi dan kebijakan untuk bagian dari klaster.</li></ol><p>Penggunaan Namespace berbeda merupakan hal opsional.</p><p>Tiap komunitas pengguna ingin bisa bekerja secara terisolasi dari komunitas lainnya.</p><p>Tiap komunitas pengguna memiliki hal berikut sendiri:</p><ol><li>sumber daya (Pod, Service, ReplicationController, dll.)</li><li>kebijakan (siapa yang bisa atau tidak bisa melakukan hal tertentu dalam komunitasnya)</li><li>batasan (komunitas ini diberi kuota sekian, dll.)</li></ol><p>Seorang operator klaster dapat membuat sebuah Namespace untuk tiap komunitas user yang unik.</p><p>Namespace tersebut memberikan cakupan yang unik untuk:</p><ol><li>penamaan sumber daya (untuk menghindari benturan penamaan dasar)</li><li>pendelegasian otoritas pengelolaan untuk pengguna yang dapat dipercaya</li><li>kemampuan untuk membatasi konsumsi sumber daya komunitas</li></ol><p>Contoh penggunaan mencakup</p><ol><li>Sebagai operator klaster, aku ingin mendukung beberapa komunitas pengguna dalam sebuah klaster.</li><li>Sebagai operator klaster, aku ingin mendelegasikan otoritas untuk mempartisi klaster ke pengguna terpercaya di komunitasnya.</li><li>Sebagai operator klaster, aku ingin membatasi jumlah sumber daya yang bisa dikonsumsi komunitas dalam rangka membatasi dampak ke komunitas lain yang menggunakan klaster yang sama.</li><li>Sebagai pengguna klaster, aku ingin berinteraksi dengan sumber daya yang berkaitan dengan komunitas pengguna saya secara terisolasi dari apa yang dilakukan komunitas lain di klaster yang sama.</li></ol><h2 id=memahami-namespace-dan-dns>Memahami Namespace dan DNS</h2><p>Ketika kamu membuat sebuah <a href=/docs/concepts/services-networking/service/>Service</a>, akan terbentuk <a href=/id/docs/concepts/services-networking/dns-pod-service/>entri DNS</a> untuk Service tersebut.
Entri DNS ini dalam bentuk <code>&lt;service-name>.&lt;namespace-name>.svc.cluster.local</code>, yang berarti jika sebuah Container hanya menggunakan <code>&lt;service-name></code> maka dia akan me-<em>resolve</em> ke layanan yang lokal dalam Namespace yang sama. Ini berguna untuk menggunakan konfigurasi yang sama pada Namespace yang berbeda seperti <em>Development</em>, <em>Staging</em> dan <em>Production</em>. Jika kami ingin menjangkau antar Namespace, kamu harus menggunakan <em>fully qualified domain name</em> (FQDN).</p><h2 id=selanjutnya>Selanjutnya</h2><ul><li>Pelajari lebih lanjut mengenai <a href=/id/docs/concepts/overview/working-with-objects/namespaces/#pengaturan-preferensi-namespace>pengaturan preferensi Namespace</a>.</li><li>Pelajari lebih lanjut mengenai <a href=/id/docs/concepts/overview/working-with-objects/namespaces/#pengaturan-namespace-untuk-sebuah-permintaan>pengaturan namespace untuk sebuah permintaan</a></li><li>Baca <a href=https://github.com/kubernetes/community/blob/main/contributors/design-proposals/architecture/namespaces.md>desain Namespace</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0b17e83b6049e53b8ffa864bdfa07c87>7 - Mengatur Control Plane Kubernetes dengan Ketersediaan Tinggi (High-Availability)</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.5 [alpha]</code></div><p>Kamu dapat mereplikasi <em>control plane</em> Kubernetes dalam skrip <code>kube-up</code> atau <code>kube-down</code> untuk Google Compute Engine (GCE).
Dokumen ini menjelaskan cara menggunakan skrip kube-up/down untuk mengelola <em>control plane</em> dengan ketersedian tinggi atau <em>high_availability</em> (HA) dan bagaimana <em>control plane</em> HA diimplementasikan untuk digunakan dalam GCE.</p><h2 id=sebelum-kamu-memulai>Sebelum kamu memulai</h2><p><p>Kamu harus memiliki klaster Kubernetes, dan perangkat baris perintah kubectl
juga harus dikonfigurasikan untuk berkomunikasi dengan klastermu. Jika kamu
belum memiliki klaster, kamu dapat membuatnya dengan menggunakan
<a href=/id/docs/tasks/tools/#minikube>minikube</a>,
atau kamu juga dapat menggunakan salah satu dari tempat mencoba Kubernetes berikut ini:</p><ul><li><a href=https://killercoda.com/playgrounds/scenario/kubernetes>Killercoda</a></li><li><a href=http://labs.play-with-k8s.com/>Bermain dengan Kubernetes</a></li></ul>Untuk melihat versi, tekan <code>kubectl version</code>.</p><h2 id=memulai-klaster-yang-kompatibel-dengan-ha>Memulai klaster yang kompatibel dengan HA</h2><p>Untuk membuat klaster yang kompatibel dengan HA, kamu harus mengatur tanda ini pada skrip <code>kube-up</code>:</p><ul><li><p><code>MULTIZONE=true</code> - untuk mencegah penghapusan replika <em>control plane</em> kubelet dari zona yang berbeda dengan zona bawaan server.
Ini diperlukan jika kamu ingin menjalankan replika <em>control plane</em> pada zona berbeda, dimana hal ini disarankan.</p></li><li><p><code>ENABLE_ETCD_QUORUM_READ=true</code> - untuk memastikan bahwa pembacaan dari semua server API akan mengembalikan data terbaru.
Jika <code>true</code>, bacaan akan diarahkan ke replika pemimpin dari etcd.
Menetapkan nilai ini menjadi <code>true</code> bersifat opsional: pembacaan akan lebih dapat diandalkan tetapi juga akan menjadi lebih lambat.</p></li></ul><p>Sebagai pilihan, kamu dapat menentukan zona GCE tempat dimana replika <em>control plane</em> pertama akan dibuat.
Atur tanda berikut:</p><ul><li><code>KUBE_GCE_ZONE=zone</code> - zona tempat di mana replika <em>control plane</em> pertama akan berjalan.</li></ul><p>Berikut ini contoh perintah untuk mengatur klaster yang kompatibel dengan HA pada zona GCE europe-west1-b:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>MULTIZONE</span><span style=color:#666>=</span><span style=color:#a2f>true</span> <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>europe-west1-b  <span style=color:#b8860b>ENABLE_ETCD_QUORUM_READS</span><span style=color:#666>=</span><span style=color:#a2f>true</span> ./cluster/kube-up.sh
</span></span></code></pre></div><p>Perhatikan bahwa perintah di atas digunakan untuk membuat klaster dengan sebuah <em>control plane</em>;
Namun, kamu bisa menambahkan replika <em>control plane</em> baru ke klaster dengan perintah berikutnya.</p><h2 id=menambahkan-replika-control-plane-yang-baru>Menambahkan replika <em>control plane</em> yang baru</h2><p>Setelah kamu membuat klaster yang kompatibel dengan HA, kamu bisa menambahkan replika <em>control plane</em> ke sana.
Kamu bisa menambahkan replika <em>control plane</em> dengan menggunakan skrip <code>kube-up</code> dengan tanda berikut ini:</p><ul><li><p><code>KUBE_REPLICATE_EXISTING_MASTER=true</code> - untuk membuat replika dari <em>control plane</em> yang sudah ada.</p></li><li><p><code>KUBE_GCE_ZONE=zone</code> - zona di mana replika <em>control plane</em> itu berjalan.
Region ini harus sama dengan region dari zona replika yang lain.</p></li></ul><p>Kamu tidak perlu mengatur tanda <code>MULTIZONE</code> atau <code>ENABLE_ETCD_QUORUM_READS</code>,
karena tanda itu diturunkan pada saat kamu memulai klaster yang kompatible dengan HA.</p><p>Berikut ini contoh perintah untuk mereplikasi <em>control plane</em> pada klaster sebelumnya yang kompatibel dengan HA:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>europe-west1-c <span style=color:#b8860b>KUBE_REPLICATE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> ./cluster/kube-up.sh
</span></span></code></pre></div><h2 id=menghapus-replika-control-plane>Menghapus replika <em>control plane</em></h2><p>Kamu dapat menghapus replika <em>control plane</em> dari klaster HA dengan menggunakan skrip <code>kube-down</code> dengan tanda berikut:</p><ul><li><p><code>KUBE_DELETE_NODES=false</code> - untuk mencegah penghapusan kubelet.</p></li><li><p><code>KUBE_GCE_ZONE=zone</code> - zona di mana replika <em>control plane</em> akan dihapus.</p></li><li><p><code>KUBE_REPLICA_NAME=replica_name</code> - (opsional) nama replika <em>control plane</em> yang akan dihapus.
Jika kosong: replika mana saja dari zona yang diberikan akan dihapus.</p></li></ul><p>Berikut ini contoh perintah untuk menghapus replika <em>control plane</em> dari klaster HA yang sudah ada sebelumnya:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBE_DELETE_NODES</span><span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>europe-west1-c ./cluster/kube-down.sh
</span></span></code></pre></div><h2 id=mengatasi-replika-control-plane-yang-gagal>Mengatasi replika <em>control plane</em> yang gagal</h2><p>Jika salah satu replika <em>control plane</em> di klaster HA kamu gagal,
praktek terbaik adalah menghapus replika dari klaster kamu dan menambahkan replika baru pada zona yang sama.
Berikut ini contoh perintah yang menunjukkan proses tersebut:</p><ol><li>Menghapus replika yang gagal:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBE_DELETE_NODES</span><span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>replica_zone <span style=color:#b8860b>KUBE_REPLICA_NAME</span><span style=color:#666>=</span>replica_name ./cluster/kube-down.sh
</span></span></code></pre></div><ol start=2><li>Menambahkan replika baru untuk menggantikan replika yang lama</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>KUBE_GCE_ZONE</span><span style=color:#666>=</span>replica-zone <span style=color:#b8860b>KUBE_REPLICATE_EXISTING_MASTER</span><span style=color:#666>=</span><span style=color:#a2f>true</span> ./cluster/kube-up.sh
</span></span></code></pre></div><h2 id=praktek-terbaik-untuk-mereplikasi-control-plane-untuk-klaster-ha>Praktek terbaik untuk mereplikasi <em>control plane</em> untuk klaster HA</h2><ul><li><p>Usahakan untuk menempatkan replika <em>control plane</em> pada zona yang berbeda. Pada saat terjadi kegagalan zona, semua <em>control plane</em> yang ditempatkan dalam zona tersebut akan gagal pula.
Untuk bertahan dari kegagalan pada sebuah zona, tempatkan juga Node pada beberapa zona yang lain
(Lihatlah <a href=/id/docs/setup/best-practices/multiple-zones/>multi-zona</a> untuk lebih detail).</p></li><li><p>Jangan gunakan klaster dengan dua replika <em>control plane</em>. Konsensus pada klaster dengan dua replika membutuhkan kedua replika tersebut berjalan pada saat mengubah keadaan yang persisten.
Akibatnya, kedua replika tersebut diperlukan dan kegagalan salah satu replika mana pun mengubah klaster dalam status kegagalan mayoritas.
Dengan demikian klaster dengan dua replika lebih buruk, dalam hal HA, daripada klaster dengan replika tunggal.</p></li><li><p>Ketika kamu menambahkan sebuah replika <em>control plane</em>, status klaster (etcd) disalin ke sebuah <em>instance</em> baru.
Jika klaster itu besar, mungkin butuh waktu yang lama untuk menduplikasi keadaannya.
Operasi ini dapat dipercepat dengan memigrasi direktori data etcd, seperti yang dijelaskan <a href=https://coreos.com/etcd/docs/latest/admin_guide.html#member-migration>di sini</a>
(Kami sedang mempertimbangkan untuk menambahkan dukungan untuk migrasi direktori data etcd di masa mendatang).</p></li></ul><h2 id=catatan-implementasi>Catatan implementasi</h2><p><img src=/images/docs/ha-master-gce.png alt=ha-master-gce></p><h3 id=ikhtisar>Ikhtisar</h3><p>Setiap replika <em>control plane</em> akan menjalankan komponen berikut dalam mode berikut:</p><ul><li><p><em>instance</em> etcd: semua <em>instance</em> akan dikelompokkan bersama menggunakan konsensus;</p></li><li><p>server API : setiap server akan berbicara dengan lokal etcd - semua server API pada cluster akan tersedia;</p></li><li><p>pengontrol (<em>controller</em>), penjadwal (<em>scheduler</em>), dan <em>scaler</em> klaster automatis: akan menggunakan mekanisme sewa - dimana hanya satu <em>instance</em> dari masing-masing mereka yang akan aktif dalam klaster;</p></li><li><p>manajer tambahan (<em>add-on</em>): setiap manajer akan bekerja secara independen untuk mencoba menjaga tambahan dalam sinkronisasi.</p></li></ul><p>Selain itu, akan ada penyeimbang beban (<em>load balancer</em>) di depan server API yang akan mengarahkan lalu lintas eksternal dan internal menuju mereka.</p><h3 id=penyeimbang-beban>Penyeimbang Beban</h3><p>Saat memulai replika <em>control plane</em> kedua, penyeimbang beban yang berisi dua replika akan dibuat
dan alamat IP dari replika pertama akan dipromosikan ke alamat IP penyeimbang beban.
Demikian pula, setelah penghapusan replika <em>control plane</em> kedua yang dimulai dari paling akhir, penyeimbang beban akan dihapus dan alamat IP-nya akan diberikan ke replika terakhir yang ada.
Mohon perhatikan bahwa pembuatan dan penghapusan penyeimbang beban adalah operasi yang rumit dan mungkin perlu beberapa waktu (~20 menit) untuk dipropagasikan.</p><h3 id=service-control-plane-kubelet>Service <em>control plane</em> & kubelet</h3><p>Daripada sistem mencoba untuk menjaga daftar terbaru dari apiserver Kubernetes yang ada dalam Service Kubernetes,
sistem akan mengarahkan semua lalu lintas ke IP eksternal:</p><ul><li><p>dalam klaster dengan satu <em>control plane</em>, IP diarahkan ke <em>control plane</em> tunggal.</p></li><li><p>dalam klaster dengan multiple <em>control plane</em>, IP diarahkan ke penyeimbang beban yang ada di depan <em>control plane</em>.</p></li></ul><p>Demikian pula, IP eksternal akan digunakan oleh kubelet untuk berkomunikasi dengan <em>control plane</em>.</p><h3 id=sertifikat-control-plane>Sertifikat <em>control plane</em></h3><p>Kubernetes menghasilkan sertifikat TLS <em>control plane</em> untuk IP publik eksternal dan IP lokal untuk setiap replika.
Tidak ada sertifikat untuk IP publik sementara (<em>ephemeral</em>) dari replika;
Untuk mengakses replika melalui IP publik sementara, kamu harus melewatkan verifikasi TLS.</p><h3 id=pengklasteran-etcd>Pengklasteran etcd</h3><p>Untuk mengizinkan pengelompokkan etcd, porta yang diperlukan untuk berkomunikasi antara <em>instance</em> etcd akan dibuka (untuk komunikasi dalam klaster).
Untuk membuat penyebaran itu aman, komunikasi antara <em>instance</em> etcd diotorisasi menggunakan SSL.</p><h2 id=bacaan-tambahan>Bacaan tambahan</h2><p><a href=https://git.k8s.io/community/contributors/design-proposals/cluster-lifecycle/ha_master.md>Dokumen desain - Penyebaran master HA automatis</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-fe5ad73163d38596340536ec03a205f0>8 - Menggunakan sysctl dalam Sebuah Klaster Kubernetes</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.12 [beta]</code></div><p>Dokumen ini menjelaskan tentang cara mengonfigurasi dan menggunakan parameter kernel dalam sebuah
klaster Kubernetes dengan menggunakan antarmuka <a class=glossary-tooltip title='An interface for getting and setting Unix kernel parameters' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/sysctl-cluster/ target=_blank aria-label=sysctl>sysctl</a>.</p><h2 id=sebelum-kamu-memulai>Sebelum kamu memulai</h2><p><p>Kamu harus memiliki klaster Kubernetes, dan perangkat baris perintah kubectl
juga harus dikonfigurasikan untuk berkomunikasi dengan klastermu. Jika kamu
belum memiliki klaster, kamu dapat membuatnya dengan menggunakan
<a href=/id/docs/tasks/tools/#minikube>minikube</a>,
atau kamu juga dapat menggunakan salah satu dari tempat mencoba Kubernetes berikut ini:</p><ul><li><a href=https://killercoda.com/playgrounds/scenario/kubernetes>Killercoda</a></li><li><a href=http://labs.play-with-k8s.com/>Bermain dengan Kubernetes</a></li></ul>Untuk melihat versi, tekan <code>kubectl version</code>.</p><h2 id=melihat-daftar-semua-parameter-sysctl>Melihat Daftar Semua Parameter Sysctl</h2><p>Dalam Linux, antarmuka sysctl memungkinkan administrator untuk memodifikasi kernel
parameter pada <em>runtime</em>. Parameter tersedia melalui sistem file virtual dari proses <code>/proc/sys/</code>.
Parameter mencakup berbagai subsistem seperti:</p><ul><li>kernel (dengan prefiks umum: <code>kernel.</code>)</li><li>networking (dengan prefiks umum: <code>net.</code>)</li><li>virtual memory (dengan prefiks umum: <code>vm.</code>)</li><li>MDADM (dengan prefiks umum: <code>dev.</code>)</li><li>subsistem yang lainnya dideskripsikan pada <a href=https://www.kernel.org/doc/Documentation/sysctl/README>dokumentasi Kernel</a>.</li></ul><p>Untuk mendapatkan daftar semua parameter, kamu bisa menjalankan perintah:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo sysctl -a
</span></span></code></pre></div><h2 id=mengaktifkan-sysctl-unsafe>Mengaktifkan Sysctl <em>Unsafe</em></h2><p>Sysctl dikelompokkan menjadi sysctl <em>safe</em> dan sysctl <em>unsafe</em>. Sebagai tambahan untuk
pengaturan <em>Namespace</em> yang benar, sebuah sysctl <em>safe</em> harus diisolasikan dengan benar diantara Pod dalam Node yang sama.
Hal ini berarti mengatur sysctl <em>safe</em> dalam satu Pod:</p><ul><li>tidak boleh mempengaruhi Pod lain dalam Node</li><li>tidak boleh membahayakan kesehatan dari Node</li><li>tidak mengizinkan untuk mendapatkan sumber daya CPU atau memori di luar batas sumber daya dari sebuah Pod.</li></ul><p>Sejauh ini, sebagian besar sysctl yang diatur sebagai Namespace belum tentu dianggap sysctl <em>safe</em>.
Sysctl berikut ini didukung dalam kelompok <em>safe</em>:</p><ul><li><code>kernel.shm_rmid_forced</code>,</li><li><code>net.ipv4.ip_local_port_range</code>,</li><li><code>net.ipv4.tcp_syncookies</code>,</li><li><code>net.ipv4.ping_group_range</code> (sejak Kubernetes 1.18),</li><li><code>net.ipv4.ip_unprivileged_port_start</code> (sejak Kubernetes 1.22).</li></ul><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong><p>Contoh <code>net.ipv4.tcp_syncookies</code> bukan merupakan Namespace pada kernel Linux versi 4.4 atau lebih rendah.</p><p>Daftar ini akan terus dikembangkan dalam versi Kubernetes berikutnya ketika kubelet
mendukung mekanisme isolasi yang lebih baik.</p><p>Semua sysctl <em>safe</em> diaktifkan secara bawaan.</p><p>Semua sysctl <em>unsafe</em> dinon-aktifkan secara bawaan dan harus diizinkan secara manual oleh
administrator klaster untuk setiap Node. Pod dengan sysctl <em>unsafe</em> yang dinon-aktifkan akan dijadwalkan,
tetapi akan gagal untuk dijalankan.</p><p>Dengan mengingat peringatan di atas, administrator klaster dapat mengizinkan sysctl <em>unsafe</em> tertentu
untuk situasi yang sangat spesial seperti pada saat kinerja tinggi atau
penyetelan aplikasi secara <em>real-time</em>. <em>Unsafe</em> syctl diaktifkan Node demi Node melalui
<em>flag</em> pada kubelet; sebagai contoh:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubelet --allowed-unsafe-sysctls <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  <span style=color:#b44>&#39;kernel.msg*,net.core.somaxconn&#39;</span> ...
</span></span></code></pre></div><p>Untuk <a class=glossary-tooltip title='A tool for running Kubernetes locally.' data-toggle=tooltip data-placement=top href=/docs/setup/learning-environment/minikube/ target=_blank aria-label=Minikube>Minikube</a>, ini dapat dilakukan melalui <em>flag</em> <code>extra-config</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>minikube start --extra-config<span style=color:#666>=</span><span style=color:#b44>&#34;kubelet.allowed-unsafe-sysctls=kernel.msg*,net.core.somaxconn&#34;</span>...
</span></span></code></pre></div><p>Hanya sysctl yang diatur sebagai Namespace dapat diaktifkan dengan cara ini.</p><h2 id=mnegatur-sysctl-untuk-pod>Mnegatur Sysctl untuk Pod</h2><p>Sejumlah sysctl adalah diatur sebagai Namespace dalam Kernel Linux hari ini. Ini berarti
mereka dapat diatur secara independen untuk setiap Pod dalam sebuah Node. Hanya sysctl dengan Namespace
yang dapat dikonfigurasi melalui Pod securityContext dalam Kubernetes.</p><p>Sysctl berikut dikenal sebagai Namespace. Daftar ini dapat berubah
pada versi kernel Linux yang akan datang.</p><ul><li><code>kernel.shm*</code>,</li><li><code>kernel.msg*</code>,</li><li><code>kernel.sem</code>,</li><li><code>fs.mqueue.*</code>,</li><li>Parameter dibawah <code>net.*</code> dapat diatur sebagai Namespace dari <em>container networking</em>
Namun, ada beberapa perkecualian (seperti
<code>net.netfilter.nf_conntrack_max</code> dan <code>net.netfilter.nf_conntrack_expect_max</code>
yang dapat diatur dalam Namespace <em>container networking</em> padahal bukan merupakan Namespace).</li></ul><p>Sysctl tanpa Namespace disebut dengan sysctl <em>node-level</em>. Jika kamu perlu mengatur
mereka, kamu harus secara manual mengonfigurasi mereka pada sistem operasi setiap Node, atau dengan
menggunakan DaemonSet melalui Container yang berwenang.</p><p>Gunakan securityContext dari Pod untuk mengonfigurasi sysctl Namespace. securityContext
berlaku untuk semua Container dalam Pod yang sama.</p><p>Contoh ini menggunakan securityContext dari Pod untuk mengatur sebuah sysctl <em>safe</em>
<code>kernel.shm_rmid_forced</code>, dan dua buah sysctl <em>unsafe</em> <code>net.core.somaxconn</code> dan
<code>kernel.msgmax</code>. Tidak ada perbedaan antara sysctl <em>safe</em> dan sysctl <em>unsafe</em> dalam
spesifikasi tersebut.</p><div class="alert alert-danger warning callout" role=alert><strong>Peringatan:</strong> Hanya modifikasi parameter sysctl setelah kamu memahami efeknya, untuk menghindari
gangguan pada kestabilan sistem operasi kamu.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>sysctl-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>securityContext</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>sysctls</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kernel.shm_rmid_forced<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>net.core.somaxconn<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1024&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kernel.msgmax<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;65536&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-danger warning callout" role=alert><strong>Peringatan:</strong> Karena sifat alami dari sysctl <em>unsafe</em>, penggunaan sysctl <em>unsafe</em>
merupakan resiko kamu sendiri dan dapat menyebabkan masalah parah seperti perilaku yang salah
pada Container, kekurangan sumber daya, atau kerusakan total dari Node.</div><p>Merupakan sebuah praktik yang baik untuk mempertimbangkan Node dengan pengaturan sysctl khusus sebagai
Node yang tercemar (<em>tainted</em>) dalam sebuah cluster, dan hanya menjadwalkan Pod yang membutuhkan pengaturan sysctl.
Sangat disarankan untuk menggunakan Kubernetes <a href=/docs/reference/generated/kubectl/kubectl-commands/#taint>fitur <em>taints and toleration</em></a> untuk mengimplementasikannya.</p><p>Pod dengan sysctl <em>unsafe</em> akan gagal diluncurkan pada sembarang Node yang belum
mengaktifkan kedua sysctl <em>unsafe</em> secara eksplisit. Seperti halnya sysctl <em>node-level</em> sangat
disarankan untuk menggunakan <a href=/docs/reference/generated/kubectl/kubectl-commands/#taint>fitur <em>taints and toleration</em></a> atau
<a href=/id/docs/concepts/scheduling-eviction/taint-and-toleration/>pencemaran dalam Node</a>
untuk Pod dalam Node yang tepat.</p><h2 id=podsecuritypolicy>PodSecurityPolicy</h2><p>Kamu selanjutnya dapat mengontrol sysctl mana saja yang dapat diatur dalam Pod dengan menentukan daftar
sysctl atau pola (<em>pattern</em>) sysctl dalam <code>forbiddenSysctls</code> dan/atau <em>field</em>
<code>allowedUnsafeSysctls</code> dari PodSecurityPolicy. Pola sysctl diakhiri
dengan karakter <code>*</code>, seperti <code>kernel.*</code>. Karakter <code>*</code> saja akan mencakup
semua sysctl.</p><p>Secara bawaan, semua sysctl <em>safe</em> diizinkan.</p><p>Kedua <code>forbiddenSysctls</code> dan <code>allowedUnsafeSysctls</code> merupakan daftar dari nama sysctl
atau pola sysctl yang polos (yang diakhiri dengan karakter <code>*</code>). Karakter <code>*</code> saja berarti sesuai dengan semua sysctl.</p><p><em>Field</em> <code>forbiddenSysctls</code> tidak memasukkan sysctl tertentu. Kamu dapat melarang
kombinasi sysctl <em>safe</em> dan sysctl <em>unsafe</em> dalam daftar tersebut. Untuk melarang pengaturan
sysctl, hanya gunakan <code>*</code> saja.</p><p>Jika kamu menyebutkan sysctl <em>unsafe</em> pada <em>field</em> <code>allowedUnsafeSysctls</code> dan
tidak ada pada <em>field</em> <code>forbiddenSysctls</code>, maka sysctl dapat digunakan pada Pod
dengan menggunakan PodSecurityPolicy ini. Untuk mengizinkan semua sysctl <em>unsafe</em> diatur dalam
PodSecurityPolicy, gunakan karakter <code>*</code> saja.</p><p>Jangan mengonfigurasi kedua <em>field</em> ini sampai tumpang tindih, dimana
sysctl yang diberikan akan diperbolehkan dan dilarang sekaligus.</p><div class="alert alert-danger warning callout" role=alert><strong>Peringatan:</strong> Jika kamu mengizinkan sysctl <em>unsafe</em> melalui <em>field</em> <code>allowUnsafeSysctls</code>
pada PodSecurityPolicy, Pod apa pun yang menggunakan sysctl seperti itu akan gagal dimulai
jika sysctl <em>unsafe</em> tidak diperbolehkan dalam <em>flag</em> kubelet <code>--allowed-unsafe-sysctls</code>
pada Node tersebut.</div><p>Ini merupakan contoh sysctl <em>unsafe</em> yang diawali dengan <code>kernel.msg</code> yang diperbolehkan dan
sysctl <code>kernel.shm_rmid_forced</code> yang dilarang.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>policy/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PodSecurityPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>sysctl-psp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>allowedUnsafeSysctls</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- kernel.msg*<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>forbiddenSysctls</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- kernel.shm_rmid_forced<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span>...<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><div class=td-content style=page-break-before:always><h1 id=pg-c4d0832845adc92b7ccd54aed63fc932>9 - Mengoperasikan klaster etcd untuk Kubernetes</h1><p><p>etcd adalah penyimpanan <i>key value</i> konsisten yang digunakan sebagai penyimpanan data klaster Kubernetes.</p></p><p>Selalu perhatikan mekanisme untuk mem-<i>backup</i> data etcd pada klaster Kubernetes kamu. Untuk informasi lebih lanjut tentang etcd, lihat <a href=https://etcd.io/docs>dokumentasi etcd</a>.</p><h2 id=sebelum-kamu-memulai>Sebelum kamu memulai</h2><p><p>Kamu harus memiliki klaster Kubernetes, dan perangkat baris perintah kubectl
juga harus dikonfigurasikan untuk berkomunikasi dengan klastermu. Jika kamu
belum memiliki klaster, kamu dapat membuatnya dengan menggunakan
<a href=/id/docs/tasks/tools/#minikube>minikube</a>,
atau kamu juga dapat menggunakan salah satu dari tempat mencoba Kubernetes berikut ini:</p><ul><li><a href=https://killercoda.com/playgrounds/scenario/kubernetes>Killercoda</a></li><li><a href=http://labs.play-with-k8s.com/>Bermain dengan Kubernetes</a></li></ul>Untuk melihat versi, tekan <code>kubectl version</code>.</p><h2 id=prerequisites>Prerequisites</h2><ul><li><p>Jalankan etcd sebagai klaster dimana anggotanya berjumlah ganjil.</p></li><li><p>Etcd adalah sistem terdistribusi berbasis <em>leader</em>. Pastikan <em>leader</em> secara berkala mengirimkan <em>heartbeat</em> dengan tepat waktu ke semua pengikutnya untuk menjaga kestabilan klaster.</p></li><li><p>Pastikan tidak terjadi kekurangan sumber daya.</p><p>Kinerja dan stabilitas dari klaster sensitif terhadap jaringan dan <em>IO disk</em>. Kekurangan sumber daya apa pun dapat menyebabkan <em>timeout</em> dari <em>heartbeat</em>, yang menyebabkan ketidakstabilan klaster. Etcd yang tidak stabil mengindikasikan bahwa tidak ada <em>leader</em> yang terpilih. Dalam keadaan seperti itu, sebuah klaster tidak dapat membuat perubahan apa pun ke kondisi saat ini, yang menyebabkan tidak ada Pod baru yang dapat dijadwalkan.</p></li><li><p>Menjaga kestabilan klaster etcd sangat penting untuk stabilitas klaster Kubernetes. Karenanya, jalankan klaster etcd pada mesin khusus atau lingkungan terisolasi untuk <a href=https://github.com/coreos/etcd/blob/master/Documentation/op-guide/hardware.md#hardware-recommendations>persyaratan sumber daya terjamin</a>.</p></li><li><p>Versi minimum yang disarankan untuk etcd yang dijalankan dalam lingkungan produksi adalah <code>3.2.10+</code>.</p></li></ul><h2 id=persyaratan-sumber-daya>Persyaratan sumber daya</h2><p>Mengoperasikan etcd dengan sumber daya terbatas hanya cocok untuk tujuan pengujian. Untuk peluncuran dalam lingkungan produksi, diperlukan konfigurasi perangkat keras lanjutan. Sebelum meluncurkan etcd dalam produksi, lihat <a href=https://github.com/coreos/etcd/blob/master/Documentation/op-guide/hardware.md#example-hardware-configurations>dokumentasi referensi persyaratan sumber daya</a>.</p><h2 id=memulai-klaster-etcd>Memulai Klaster etcd</h2><p>Bagian ini mencakup bagaimana memulai klaster etcd dalam Node tunggal dan Node multipel.</p><h3 id=klaster-etcd-dalam-node-tunggal>Klaster etcd dalam Node tunggal</h3><p>Gunakan Klaster etcd Node tunggal hanya untuk tujuan pengujian</p><ol><li><p>Jalankan perintah berikut ini:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>./etcd --listen-client-urls<span style=color:#666>=</span>http://<span style=color:#b8860b>$PRIVATE_IP</span>:2379 --advertise-client-urls<span style=color:#666>=</span>http://<span style=color:#b8860b>$PRIVATE_IP</span>:2379
</span></span></code></pre></div></li><li><p>Start server API Kubernetes dengan <em>flag</em> <code>--etcd-servers=$PRIVATE_IP:2379</code>.</p><p>Ganti <code>PRIVATE_IP</code> dengan IP klien etcd kamu.</p></li></ol><h3 id=klaster-etcd-dengan-node-multipel>Klaster etcd dengan Node multipel</h3><p>Untuk daya tahan dan ketersediaan tinggi, jalankan etcd sebagai klaster dengan Node multipel dalam lingkungan produksi dan cadangkan secara berkala. Sebuah klaster dengan lima anggota direkomendasikan dalam lingkungan produksi. Untuk informasi lebih lanjut, lihat <a href=https://github.com/coreos/etcd/blob/master/Documentation/faq.md#what-is-failure-tolerance>Dokumentasi FAQ</a>.</p><p>Mengkonfigurasi klaster etcd baik dengan informasi anggota statis atau dengan penemuan dinamis. Untuk informasi lebih lanjut tentang pengklasteran, lihat <a href=https://github.com/coreos/etcd/blob/master/Documentation/op-guide/clustering.md>Dokumentasi pengklasteran etcd</a>.</p><p>Sebagai contoh, tinjau sebuah klaster etcd dengan lima anggota yang berjalan dengan URL klien berikut: <code>http://$IP1:2379</code>, <code>http://$IP2:2379</code>, <code>http://$IP3:2379</code>, <code>http://$IP4:2379</code>, dan <code>http://$IP5:2379</code>. Untuk memulai server API Kubernetes:</p><ol><li><p>Jalankan perintah berikut ini:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>./etcd --listen-client-urls<span style=color:#666>=</span>http://<span style=color:#b8860b>$IP1</span>:2379, http://<span style=color:#b8860b>$IP2</span>:2379, http://<span style=color:#b8860b>$IP3</span>:2379, http://<span style=color:#b8860b>$IP4</span>:2379, http://<span style=color:#b8860b>$IP5</span>:2379 --advertise-client-urls<span style=color:#666>=</span>http://<span style=color:#b8860b>$IP1</span>:2379, http://<span style=color:#b8860b>$IP2</span>:2379, http://<span style=color:#b8860b>$IP3</span>:2379, http://<span style=color:#b8860b>$IP4</span>:2379, http://<span style=color:#b8860b>$IP5</span>:2379
</span></span></code></pre></div></li><li><p>Start server Kubernetes API dengan flag <code>--etcd-servers=$IP1:2379, $IP2:2379, $IP3:2379, $IP4:2379, $IP5:2379</code>.</p><p>Ganti <code>IP</code> dengan alamat IP klien kamu.</p></li></ol><h3 id=klaster-etcd-dengan-node-multipel-dengan-load-balancer>Klaster etcd dengan Node multipel dengan load balancer</h3><p>Untuk menjalankan penyeimbangan beban (<em>load balancing</em>) untuk klaster etcd:</p><ol><li>Siapkan sebuah klaster etcd.</li><li>Konfigurasikan sebuah <em>load balancer</em> di depan klaster etcd.
Sebagai contoh, anggap saja alamat <em>load balancer</em> adalah <code>$LB</code>.</li><li>Mulai Server API Kubernetes dengan <em>flag</em> <code>--etcd-servers=$LB:2379</code>.</li></ol><h2 id=mengamankan-klaster-etcd>Mengamankan klaster etcd</h2><p>Akses ke etcd setara dengan izin root pada klaster sehingga idealnya hanya server API yang memiliki akses ke etcd. Dengan pertimbangan sensitivitas data, disarankan untuk memberikan izin hanya kepada Node-Node yang membutuhkan akses ke klaster etcd.</p><p>Untuk mengamankan etcd, tetapkan aturan <em>firewall</em> atau gunakan fitur keamanan yang disediakan oleh etcd. Fitur keamanan etcd tergantung pada Infrastruktur Kunci Publik / <em>Public Key Infrastructure</em> (PKI) x509. Untuk memulai, buat saluran komunikasi yang aman dengan menghasilkan pasangan kunci dan sertifikat. Sebagai contoh, gunakan pasangan kunci <code>peer.key</code> dan <code>peer.cert</code> untuk mengamankan komunikasi antara anggota etcd, dan <code>client.key</code> dan <code>client.cert</code> untuk mengamankan komunikasi antara etcd dan kliennya. Lihat <a href=https://github.com/coreos/etcd/tree/master/hack/tls-setup>contoh skrip</a> yang disediakan oleh proyek etcd untuk menghasilkan pasangan kunci dan berkas CA untuk otentikasi klien.</p><h3 id=mengamankan-komunikasi>Mengamankan komunikasi</h3><p>Untuk mengonfigurasi etcd dengan <em>secure peer communication</em>, tentukan <em>flag</em> <code>--peer-key-file=peer.key</code> dan <code>--peer-cert-file=peer.cert</code>, dan gunakan https sebagai skema URL.</p><p>Demikian pula, untuk mengonfigurasi etcd dengan <em>secure client communication</em>, tentukan <em>flag</em> <code>--key-file=k8sclient.key</code> dan <code>--cert-file=k8sclient.cert</code>, dan gunakan https sebagai skema URL.</p><h3 id=membatasi-akses-klaster-etcd>Membatasi akses klaster etcd</h3><p>Setelah konfigurasi komunikasi aman, batasi akses klaster etcd hanya ke server API Kubernetes. Gunakan otentikasi TLS untuk melakukannya.</p><p>Sebagai contoh, anggap pasangan kunci <code>k8sclient.key</code> dan <code>k8sclient.cert</code> dipercaya oleh CA <code>etcd.ca</code>. Ketika etcd dikonfigurasi dengan <code>--client-cert-auth</code> bersama dengan TLS, etcd memverifikasi sertifikat dari klien dengan menggunakan CA dari sistem atau CA yang dilewati oleh <em>flag</em> <code>--trusted-ca-file</code>. Menentukan <em>flag</em> <code>--client-cert-auth=true</code> dan <code>--trusted-ca-file=etcd.ca</code> akan membatasi akses kepada klien yang mempunyai sertifikat <code>k8sclient.cert</code>.</p><p>Setelah etcd dikonfigurasi dengan benar, hanya klien dengan sertifikat yang valid dapat mengaksesnya. Untuk memberikan akses kepada server Kubernetes API, konfigurasikan dengan <em>flag</em> <code>--etcd-certfile=k8sclient.cert</code>,<code>--etcd-keyfile=k8sclient.key</code> dan <code>--etcd-cafile=ca.cert</code>.</p><div class="alert alert-info note callout" role=alert><strong>Catatan:</strong> Otentikasi etcd saat ini tidak didukung oleh Kubernetes. Untuk informasi lebih lanjut, lihat masalah terkait <a href=https://github.com/kubernetes/kubernetes/issues/23398>Mendukung Auth Dasar untuk Etcd v2</a>.</div><h2 id=mengganti-anggota-etcd-yang-gagal>Mengganti anggota etcd yang gagal</h2><p>Etcd klaster mencapai ketersediaan tinggi dengan mentolerir kegagalan dari sebagian kecil anggota. Namun, untuk meningkatkan kesehatan keseluruhan dari klaster, segera ganti anggota yang gagal. Ketika banyak anggota yang gagal, gantilah satu per satu. Mengganti anggota yang gagal melibatkan dua langkah: menghapus anggota yang gagal dan menambahkan anggota baru.</p><p>Meskipun etcd menyimpan ID anggota unik secara internal, disarankan untuk menggunakan nama unik untuk setiap anggota untuk menghindari kesalahan manusia. Sebagai contoh, sebuah klaster etcd dengan tiga anggota. Jadikan URL-nya, member1=http://10.0.0.1, member2=http://10.0.0.2, and member3=http://10.0.0.3. Ketika member1 gagal, ganti dengan member4=http://10.0.0.4.</p><ol><li><p>Dapatkan ID anggota yang gagal dari member1:</p><p><code>etcdctl --endpoints=http://10.0.0.2,http://10.0.0.3 member list</code></p><p>Akan tampil pesan berikut:</p><pre><code> 8211f1d0f64f3269, started, member1, http://10.0.0.1:2380, http://10.0.0.1:2379
 91bc3c398fb3c146, started, member2, http://10.0.0.2:2380, http://10.0.0.2:2379
 fd422379fda50e48, started, member3, http://10.0.0.3:2380, http://10.0.0.3:2379
</code></pre></li><li><p>Hapus anggota yang gagal:</p><p><code>etcdctl member remove 8211f1d0f64f3269</code></p><p>Akan tampil pesan berikut:</p><pre><code>Removed member 8211f1d0f64f3269 from cluster
</code></pre></li><li><p>Tambahkan anggota baru:</p><p><code>./etcdctl member add member4 --peer-urls=http://10.0.0.4:2380</code></p><p>Akan tampil pesan berikut:</p><pre><code>Member 2be1eb8f84b7f63e added to cluster ef37ad9dc622a7c4
</code></pre></li><li><p>Jalankan anggota yang baru ditambahkan pada mesin dengan IP <code>10.0.0.4</code>:</p><pre><code> export ETCD_NAME=&quot;member4&quot;
 export ETCD_INITIAL_CLUSTER=&quot;member2=http://10.0.0.2:2380,member3=http://10.0.0.3:2380,member4=http://10.0.0.4:2380&quot;
 export ETCD_INITIAL_CLUSTER_STATE=existing
 etcd [flags]
</code></pre></li><li><p>Lakukan salah satu dari yang berikut:</p><ol><li>Perbarui <em>flag</em> <code>--etcd-server</code> untuk membuat Kubernetes mengetahui perubahan konfigurasi, lalu start ulang server API Kubernetes.</li><li>Perbarui konfigurasi <em>load balancer</em> jika <em>load balancer</em> digunakan dalam Deployment.</li></ol></li></ol><p>Untuk informasi lebih lanjut tentang konfigurasi ulang klaster, lihat <a href=https://github.com/coreos/etcd/blob/master/Documentation/op-guide/runtime-configuration.md#remove-a-member>Dokumentasi Konfigurasi etcd</a>.</p><h2 id=mencadangkan-klaster-etcd>Mencadangkan klaster etcd</h2><p>Semua objek Kubernetes disimpan dalam etcd. Mencadangkan secara berkala data klaster etcd penting untuk memulihkan klaster Kubernetes di bawah skenario bencana, seperti kehilangan semua Node <em>control plane</em>. Berkas <em>snapshot</em> berisi semua status Kubernetes dan informasi penting. Untuk menjaga data Kubernetes yang sensitif aman, enkripsi berkas <em>snapshot</em>.</p><p>Mencadangkan klaster etcd dapat dilakukan dengan dua cara: <em>snapshot</em> etcd bawaan dan <em>snapshot</em> volume.</p><h3 id=snapshot-bawaan>Snapshot bawaan</h3><p>Fitur <em>snapshot</em> didukung oleh etcd secara bawaan, jadi mencadangkan klaster etcd lebih mudah. <em>Snapshot</em> dapat diambil dari anggota langsung dengan command <code>etcdctl snapshot save</code> atau dengan menyalin <code>member/snap/db</code> berkas dari etcd <a href=https://github.com/coreos/etcd/blob/master/Documentation/op-guide/configuration.md#--data-dir>direktori data</a> yang saat ini tidak digunakan oleh proses etcd. Mengambil <em>snapshot</em> biasanya tidak akan mempengaruhi kinerja anggota.</p><p>Di bawah ini adalah contoh untuk mengambil <em>snapshot</em> dari <em>keyspace</em> yang dilayani oleh <code>$ENDPOINT</code> ke berkas <code>snapshotdb</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>ETCDCTL_API</span><span style=color:#666>=</span><span style=color:#666>3</span> etcdctl --endpoints <span style=color:#b8860b>$ENDPOINT</span> snapshot save snapshotdb
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># keluar 0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># memverifikasi hasil snapshot</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ETCDCTL_API</span><span style=color:#666>=</span><span style=color:#666>3</span> etcdctl --write-out<span style=color:#666>=</span>table snapshot status snapshotdb
</span></span><span style=display:flex><span>+----------+----------+------------+------------+
</span></span><span style=display:flex><span>|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |
</span></span><span style=display:flex><span>+----------+----------+------------+------------+
</span></span><span style=display:flex><span>| fe01cf57 |       <span style=color:#666>10</span> |          <span style=color:#666>7</span> | 2.1 MB     |
</span></span><span style=display:flex><span>+----------+----------+------------+------------+
</span></span></code></pre></div><h3 id=snapshot-volume>Snapshot volume</h3><p>Jika etcd berjalan pada volume penyimpanan yang mendukung cadangan, seperti Amazon Elastic Block Store, buat cadangan data etcd dengan mengambil <em>snapshot</em> dari volume penyimpanan.</p><h2 id=memperbesar-skala-dari-klaster-etcd>Memperbesar skala dari klaster etcd</h2><p>Peningkatan skala klaster etcd meningkatkan ketersediaan dengan menukarnya untuk kinerja. Penyekalaan tidak akan meningkatkan kinerja atau kemampuan klaster. Aturan umum adalah untuk tidak melakukan penyekalaan naik atau turun untuk klaster etcd. Jangan mengonfigurasi grup penyekalaan otomatis untuk klaster etcd. Sangat disarankan untuk selalu menjalankan klaster etcd statis dengan lima anggota untuk klaster produksi Kubernetes untuk setiap skala yang didukung secara resmi.</p><p>Penyekalaan yang wajar adalah untuk meningkatkan klaster dengan tiga anggota menjadi dengan lima anggota, ketika dibutuhkan lebih banyak keandalan. Lihat <a href=https://github.com/coreos/etcd/blob/master/Documentation/op-guide/runtime-configuration.md#remove-a-member>Dokumentasi Rekonfigurasi etcd</a> untuk informasi tentang cara menambahkan anggota ke klaster yang ada.</p><h2 id=memulihkan-klaster-etcd>Memulihkan klaster etcd</h2><p>Etcd mendukung pemulihan dari <em>snapshot</em> yang diambil dari proses etcd dari versi <a href=http://semver.org/>major.minor</a>. Memulihkan versi dari versi patch lain dari etcd juga didukung. Operasi pemulihan digunakan untuk memulihkan data klaster yang gagal.</p><p>Sebelum memulai operasi pemulihan, berkas <em>snapshot</em> harus ada. Ini bisa berupa berkas <em>snapshot</em> dari operasi pencadangan sebelumnya, atau dari sisa <a href=https://github.com/coreos/etcd/blob/master/Documentation/op-guide/configuration.md#--data-dir>direktori data</a>. Untuk informasi dan contoh lebih lanjut tentang memulihkan klaster dari berkas <em>snapshot</em>, lihat <a href=https://github.com/coreos/etcd/blob/master/Documentation/op-guide/recovery.md#restoring-a-cluster>dokumentasi pemulihan bencana etcd</a>.</p><p>Jika akses URL dari klaster yang dipulihkan berubah dari klaster sebelumnya, maka server API Kubernetes harus dikonfigurasi ulang sesuai dengan URL tersebut. Pada kasus ini, start kembali server API Kubernetes dengan <em>flag</em> <code>--etcd-servers=$NEW_ETCD_CLUSTER</code> bukan <em>flag</em> <code>--etcd-servers=$OLD_ETCD_CLUSTER</code>. Ganti <code>$NEW_ETCD_CLUSTER</code> dan <code>$OLD_ETCD_CLUSTER</code> dengan alamat IP masing-masing. Jika <em>load balancer</em> digunakan di depan klaster etcd, kamu mungkin hanya perlu memperbarui <em>load balancer</em> sebagai gantinya.</p><p>Jika mayoritas anggota etcd telah gagal secara permanen, klaster etcd dianggap gagal. Dalam skenario ini, Kubernetes tidak dapat membuat perubahan apa pun ke kondisi saat ini. Meskipun Pod terjadwal mungkin terus berjalan, tidak ada Pod baru yang bisa dijadwalkan. Dalam kasus seperti itu, pulihkan klaster etcd dan kemungkinan juga untuk mengonfigurasi ulang server API Kubernetes untuk memperbaiki masalah ini.</p><h2 id=memutakhirkan-dan-memutar-balikan-klaster-etcd>Memutakhirkan dan memutar balikan klaster etcd</h2><p>Pada Kubernetes v1.13.0, etcd2 tidak lagi didukung sebagai <em>backend</em> penyimpanan untuk klaster Kubernetes baru atau yang sudah ada. <em>Timeline</em> untuk dukungan Kubernetes untuk etcd2 dan etcd3 adalah sebagai berikut:</p><ul><li>Kubernetes v1.0: hanya etcd2</li><li>Kubernetes v1.5.1: dukungan etcd3 ditambahkan, standar klaster baru yang dibuat masih ke etcd2</li><li>Kubernetes v1.6.0: standar klaster baru yang dibuat dengan <code>kube-up.sh</code> adalah etcd3,
dan <code>kube-apiserver</code> standarnya ke etcd3</li><li>Kubernetes v1.9.0: pengumuman penghentian <em>backend</em> penyimpanan etcd2 diumumkan</li><li>Kubernetes v1.13.0: <em>backend</em> penyimpanan etcd2 dihapus, <code>kube-apiserver</code> akan
menolak untuk start dengan <code>--storage-backend=etcd2</code>, dengan pesan
<code>etcd2 is no longer a supported storage backend</code></li></ul><p>Sebelum memutakhirkan v1.12.x kube-apiserver menggunakan <code>--storage-backend=etcd2</code> ke
v1.13.x, data etcd v2 harus dimigrasikan ke <em>backend</em> penyimpanan v3 dan
permintaan kube-apiserver harus diubah untuk menggunakan <code>--storage-backend=etcd3</code>.</p><p>Proses untuk bermigrasi dari etcd2 ke etcd3 sangat tergantung pada bagaimana
klaster etcd diluncurkan dan dikonfigurasi, serta bagaimana klaster Kubernetes diluncurkan dan dikonfigurasi. Kami menyarankan kamu berkonsultasi dengan dokumentasi penyedia kluster kamu untuk melihat apakah ada solusi yang telah ditentukan.</p><p>Jika klaster kamu dibuat melalui <code>kube-up.sh</code> dan masih menggunakan etcd2 sebagai penyimpanan <em>backend</em>, silakan baca <a href=https://v1-12.docs.kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/#upgrading-and-rolling-back-etcd-clusters>Kubernetes v1.12 etcd cluster upgrade docs</a></p><h2 id=masalah-umum-penyeimbang-klien-etcd-dengan-secure-endpoint>Masalah umum: penyeimbang klien etcd dengan <em>secure endpoint</em></h2><p>Klien etcd v3, dirilis pada etcd v3.3.13 atau sebelumnya, memiliki <a href=https://github.com/kubernetes/kubernetes/issues/72102><em>critical bug</em></a> yang mempengaruhi kube-apiserver dan penyebaran HA. Pemindahan kegagalan (<em>failover</em>) penyeimbang klien etcd tidak bekerja dengan baik dengan <em>secure endpoint</em>. Sebagai hasilnya, server etcd boleh gagal atau terputus sesaat dari kube-apiserver. Hal ini mempengaruhi peluncuran HA dari kube-apiserver.</p><p>Perbaikan dibuat di <a href=https://github.com/etcd-io/etcd/pull/10911>etcd v3.4</a> (dan di-backport ke v3.3.14 atau yang lebih baru): klien baru sekarang membuat bundel kredensial sendiri untuk menetapkan target otoritas dengan benar dalam fungsi dial.</p><p>Karena perbaikan tersebut memerlukan pemutakhiran dependensi dari gRPC (ke v1.23.0), <em>downstream</em> Kubernetes <a href=https://github.com/kubernetes/kubernetes/issues/72102#issuecomment-526645978>tidak mendukung upgrade etcd</a>, yang berarti <a href=https://github.com/etcd-io/etcd/pull/10911/commits/db61ee106ca9363ba3f188ecf27d1a8843da33ab>perbaikan etcd di kube-apiserver</a> hanya tersedia mulai Kubernetes 1.16.</p><p>Untuk segera memperbaiki celah keamanan (<em>bug</em>) ini untuk Kubernetes 1.15 atau sebelumnya, buat kube-apiserver khusus. kamu dapat membuat perubahan lokal ke <a href=https://github.com/kubernetes/kubernetes/blob/7b85be021cd2943167cd3d6b7020f44735d9d90b/vendor/google.golang.org/grpc/credentials/credentials.go#L135><code>vendor/google.golang.org/grpc/credentials/credentials.go</code></a> dengan <a href=https://github.com/etcd-io/etcd/pull/10911/commits/db61ee106ca9363ba3f188ecf27d1a8843da33ab>etcd@db61ee106</a>.</p><p>Lihat <a href=https://github.com/kubernetes/kubernetes/issues/72102>"kube-apiserver 1.13.x menolak untuk bekerja ketika server etcd pertama tidak tersedia"</a>.</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/id/docs/home/>Home</a>
<a class=text-white href=/id/community/>Komunitas</a>
<a class=text-white href=/id/case-studies/>Studi kasus</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 Para Pencipta Kubernetes | Dokumentasi didistribusikan di bawah <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 Linux Foundation &reg;. Hak cipta dilindungi. Linux Foundation telah mendaftarkan merek dagang dan pengunaannya. Perinciannya bisa dilihat pada <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>halaman penggunaan merek dagang</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>