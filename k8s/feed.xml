<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – Production-Grade Container Orchestration</title><link>https://kubernetes.io/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/</link></image><atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Finding suspicious syscalls with the seccomp notifier</title><link>https://kubernetes.io/blog/2022/12/02/seccomp-notifier/</link><pubDate>Fri, 02 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/02/seccomp-notifier/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Sascha Grunert&lt;/p>
&lt;p>Debugging software in production is one of the biggest challenges we have to
face in our containerized environments. Being able to understand the impact of
the available security options, especially when it comes to configuring our
deployments, is one of the key aspects to make the default security in
Kubernetes stronger. We have all those logging, tracing and metrics data already
at hand, but how do we assemble the information they provide into something
human readable and actionable?&lt;/p>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Seccomp">Seccomp&lt;/a> is one of the standard mechanisms to protect a Linux based
Kubernetes application from malicious actions by interfering with its &lt;a href="https://en.wikipedia.org/wiki/Syscall">system
calls&lt;/a>. This allows us to restrict the application to a defined set of
actionable items, like modifying files or responding to HTTP requests. Linking
the knowledge of which set of syscalls is required to, for example, modify a
local file, to the actual source code is in the same way non-trivial. Seccomp
profiles for Kubernetes have to be written in &lt;a href="https://www.json.org">JSON&lt;/a> and can be understood
as an architecture specific allow-list with superpowers, for example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-json" data-lang="json">&lt;span style="display:flex;">&lt;span>{
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;defaultAction&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;SCMP_ACT_ERRNO&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;defaultErrnoRet&amp;#34;&lt;/span>: &lt;span style="color:#666">38&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;defaultErrno&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ENOSYS&amp;#34;&lt;/span>,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;syscalls&amp;#34;&lt;/span>: [
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;names&amp;#34;&lt;/span>: [&lt;span style="color:#b44">&amp;#34;chmod&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;chown&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;open&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;write&amp;#34;&lt;/span>],
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#008000;font-weight:bold">&amp;#34;action&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;SCMP_ACT_ALLOW&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> }
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The above profile errors by default specifying the &lt;code>defaultAction&lt;/code> of
&lt;code>SCMP_ACT_ERRNO&lt;/code>. This means we have to allow a set of syscalls via
&lt;code>SCMP_ACT_ALLOW&lt;/code>, otherwise the application would not be able to do anything at
all. Okay cool, for being able to allow file operations, all we have to do is
adding a bunch of file specific syscalls like &lt;code>open&lt;/code> or &lt;code>write&lt;/code>, and probably
also being able to change the permissions via &lt;code>chmod&lt;/code> and &lt;code>chown&lt;/code>, right?
Basically yes, but there are issues with the simplicity of that approach:&lt;/p>
&lt;p>Seccomp profiles need to include the minimum set of syscalls required to start
the application. This also includes some syscalls from the lower level
&lt;a href="https://opencontainers.org">Open Container Initiative (OCI)&lt;/a> container runtime, for example
&lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> or &lt;a href="https://github.com/containers/crun">crun&lt;/a>. Beside that, we can only guarantee the required
syscalls for a very specific version of the runtimes and our application,
because the code parts can change between releases. The same applies to the
termination of the application as well as the target architecture we're
deploying on. Features like executing commands within containers also require
another subset of syscalls. Not to mention that there are multiple versions for
syscalls doing slightly different things and the seccomp profiles are able to
modify their arguments. It's also not always clearly visible to the developers
which syscalls are used by their own written code parts, because they rely on
programming language abstractions or frameworks.&lt;/p>
&lt;p>&lt;em>How can we know which syscalls are even required then? Who should create and
maintain those profiles during its development life-cycle?&lt;/em>&lt;/p>
&lt;p>Well, recording and distributing seccomp profiles is one of the problem domains
of the &lt;a href="https://github.com/kubernetes-sigs/security-profiles-operator">Security Profiles Operator&lt;/a>, which is already solving that. The
operator is able to record &lt;a href="https://en.wikipedia.org/wiki/Seccomp">seccomp&lt;/a>, &lt;a href="https://en.wikipedia.org/wiki/Security-Enhanced_Linux">SELinux&lt;/a> and even
&lt;a href="https://en.wikipedia.org/wiki/AppArmor">AppArmor&lt;/a> profiles into a &lt;a href="https://k8s.io/docs/concepts/extend-kubernetes/api-extension/custom-resources">Custom Resource Definition (CRD)&lt;/a>,
reconciles them to each node and makes them available for usage.&lt;/p>
&lt;p>The biggest challenge about creating security profiles is to catch all code
paths which execute syscalls. We could achieve that by having &lt;strong>100%&lt;/strong> logical
coverage of the application when running an end-to-end test suite. You get the
problem with the previous statement: It's too idealistic to be ever fulfilled,
even without taking all the moving parts during application development and
deployment into account.&lt;/p>
&lt;p>Missing a syscall in the seccomp profiles' allow list can have tremendously
negative impact on the application. It's not only that we can encounter crashes,
which are trivially detectable. It can also happen that they slightly change
logical paths, change the business logic, make parts of the application
unusable, slow down performance or even expose security vulnerabilities. We're
simply not able to see the whole impact of that, especially because blocked
syscalls via &lt;code>SCMP_ACT_ERRNO&lt;/code> do not provide any additional &lt;a href="https://linux.die.net/man/8/auditd">audit&lt;/a>
logging on the system.&lt;/p>
&lt;p>Does that mean we're lost? Is it just not realistic to dream about a Kubernetes
where &lt;a href="https://github.com/kubernetes/enhancements/issues/2413">everyone uses the default seccomp profile&lt;/a>? Should we
stop striving towards maximum security in Kubernetes and accept that it's not
meant to be secure by default?&lt;/p>
&lt;p>&lt;strong>Definitely not.&lt;/strong> Technology evolves over time and there are many folks
working behind the scenes of Kubernetes to indirectly deliver features to
address such problems. One of the mentioned features is the &lt;em>seccomp notifier&lt;/em>,
which can be used to find suspicious syscalls in Kubernetes.&lt;/p>
&lt;p>The seccomp notify feature consists of a set of changes introduced in Linux 5.9.
It makes the kernel capable of communicating seccomp related events to the user
space. That allows applications to act based on the syscalls and opens for a
wide range of possible use cases. We not only need the right kernel version,
but also at least runc v1.1.0 (or crun v0.19) to be able to make the notifier
work at all. The Kubernetes container runtime &lt;a href="https://cri-o.io">CRI-O&lt;/a> gets &lt;a href="https://github.com/cri-o/cri-o/pull/6120">support for
the seccomp notifier in v1.26.0&lt;/a>. The new feature allows us to
identify possibly malicious syscalls in our application, and therefore makes it
possible to verify profiles for consistency and completeness. Let's give that a
try.&lt;/p>
&lt;p>First of all we need to run the latest &lt;code>main&lt;/code> version of CRI-O, because v1.26.0
has not been released yet at time of writing. You can do that by either
compiling it from the &lt;a href="https://github.com/cri-o/cri-o/blob/main/install.md#build-and-install-cri-o-from-source">source code&lt;/a> or by using the pre-built binary
bundle via &lt;a href="https://github.com/cri-o/cri-o#installing-cri-o">the get-script&lt;/a>. The seccomp notifier feature of CRI-O is
guarded by an annotation, which has to be explicitly allowed, for example by
using a configuration drop-in like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> cat /etc/crio/crio.conf.d/02-runtimes.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[crio.runtime]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default_runtime = &lt;span style="color:#b44">&amp;#34;runc&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[crio.runtime.runtimes.runc]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>allowed_annotations = [ &lt;span style="color:#b44">&amp;#34;io.kubernetes.cri-o.seccompNotifierAction&amp;#34;&lt;/span> ]
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If CRI-O is up and running, then it should indicate that the seccomp notifier is
available as well:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> sudo ./bin/crio --enable-metrics
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">INFO[…] Starting seccomp notifier watcher
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">INFO[…] Serving metrics on :9090 via HTTP
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We also enable the metrics, because they provide additional telemetry data about
the notifier. Now we need a running Kubernetes cluster for demonstration
purposes. For this demo, we mainly stick to the
&lt;a href="https://github.com/cri-o/cri-o#running-kubernetes-with-cri-o">&lt;code>hack/local-up-cluster.sh&lt;/code>&lt;/a> approach to locally spawn a single node
Kubernetes cluster.&lt;/p>
&lt;p>If everything is up and running, then we would have to define a seccomp profile
for testing purposes. But we do not have to create our own, we can just use the
&lt;code>RuntimeDefault&lt;/code> profile which gets shipped with each container runtime. For
example the &lt;code>RuntimeDefault&lt;/code> profile for CRI-O can be found in the
&lt;a href="https://github.com/containers/common/blob/afff1d6/pkg/seccomp/seccomp.json">containers/common&lt;/a> library.&lt;/p>
&lt;p>Now we need a test container, which can be a simple &lt;a href="https://www.nginx.com">nginx&lt;/a> pod like
this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">io.kubernetes.cri-o.seccompNotifierAction&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;stop&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx:1.23.2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">seccompProfile&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>RuntimeDefault&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Please note the annotation &lt;code>io.kubernetes.cri-o.seccompNotifierAction&lt;/code>, which
enables the seccomp notifier for this workload. The value of the annotation can
be either &lt;code>stop&lt;/code> for stopping the workload or anything else for doing nothing
else than logging and throwing metrics. Because of the termination we also use
the &lt;code>restartPolicy: Never&lt;/code> to not automatically recreate the container on
failure.&lt;/p>
&lt;p>Let's run the pod and check if it works:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl apply -f nginx.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl get pods -o wide
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">nginx 1/1 Running 0 3m39s 10.85.0.3 127.0.0.1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can also test if the web server itself works as intended:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> curl 10.85.0.3
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&amp;lt;!DOCTYPE html&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&amp;lt;html&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&amp;lt;head&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>While everything is now up and running, CRI-O also indicates that it has started
the seccomp notifier:&lt;/p>
&lt;pre tabindex="0">&lt;code>…
INFO[…] Injecting seccomp notifier into seccomp profile of container 662a3bb0fdc7dd1bf5a88a8aa8ef9eba6296b593146d988b4a9b85822422febb
…
&lt;/code>&lt;/pre>&lt;p>If we would now run a forbidden syscall inside of the container, then we can
expect that the workload gets terminated. Let's give that a try by running
&lt;code>chroot&lt;/code> in the containers namespaces:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- bash
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">root@nginx:/# chroot /tmp
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">chroot: cannot change root directory to &amp;#39;/tmp&amp;#39;: Function not implemented
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">root@nginx:/# command terminated with exit code 137
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The exec session got terminated, so it looks like the container is not running
any more:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl get pods
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">NAME READY STATUS RESTARTS AGE
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">nginx 0/1 seccomp killed 0 96s
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Alright, the container got killed by seccomp, do we get any more information
about what was going on?&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl describe pod nginx
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Name: nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">Containers:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> nginx:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> …
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> State: Terminated
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Reason: seccomp killed
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Message: Used forbidden syscalls: chroot (1x)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Exit Code: 137
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Started: Mon, 14 Nov 2022 12:19:46 +0100
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Finished: Mon, 14 Nov 2022 12:20:26 +0100
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">…
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The seccomp notifier feature of CRI-O correctly set the termination reason and
message, including which forbidden syscall has been used how often (&lt;code>1x&lt;/code>). How
often? Yes, the notifier gives the application up to 5 seconds after the last
seen syscall until it starts the termination. This means that it's possible to
catch multiple forbidden syscalls within one test by avoiding time-consuming
trial and errors.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- chroot /tmp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">chroot: cannot change root directory to &amp;#39;/tmp&amp;#39;: Function not implemented
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">command terminated with exit code 125
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- chroot /tmp
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">chroot: cannot change root directory to &amp;#39;/tmp&amp;#39;: Function not implemented
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">command terminated with exit code 125
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- swapoff -a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">command terminated with exit code 32
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl &lt;span style="color:#a2f">exec&lt;/span> -it nginx -- swapoff -a
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">command terminated with exit code 32
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> kubectl describe pod nginx | grep Message
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> Message: Used forbidden syscalls: chroot (2x), swapoff (2x)
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The CRI-O metrics will also reflect that:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">&amp;gt;&lt;/span> curl -sf localhost:9090/metrics | grep seccomp_notifier
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">#&lt;/span> HELP container_runtime_crio_containers_seccomp_notifier_count_total Amount of containers stopped because they used a forbidden syscalls by their name
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">#&lt;/span> TYPE container_runtime_crio_containers_seccomp_notifier_count_total counter
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">container_runtime_crio_containers_seccomp_notifier_count_total{name=&amp;#34;…&amp;#34;,syscalls=&amp;#34;chroot (1x)&amp;#34;} 1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">container_runtime_crio_containers_seccomp_notifier_count_total{name=&amp;#34;…&amp;#34;,syscalls=&amp;#34;chroot (2x), swapoff (2x)&amp;#34;} 1
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>How does it work in detail? CRI-O uses the chosen seccomp profile and injects
the action &lt;code>SCMP_ACT_NOTIFY&lt;/code> instead of &lt;code>SCMP_ACT_ERRNO&lt;/code>, &lt;code>SCMP_ACT_KILL&lt;/code>,
&lt;code>SCMP_ACT_KILL_PROCESS&lt;/code> or &lt;code>SCMP_ACT_KILL_THREAD&lt;/code>. It also sets a local listener
path which will be used by the lower level OCI runtime (runc or crun) to create
the seccomp notifier socket. If the connection between the socket and CRI-O has
been established, then CRI-O will receive notifications for each syscall being
interfered by seccomp. CRI-O stores the syscalls, allows a bit of timeout for
them to arrive and then terminates the container if the chosen
&lt;code>seccompNotifierAction=stop&lt;/code>. Unfortunately, the seccomp notifier is not able to
notify on the &lt;code>defaultAction&lt;/code>, which means that it's required to have
a list of syscalls to test for custom profiles. CRI-O does also state that
limitation in the logs:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-log" data-lang="log">INFO[…] The seccomp profile default action SCMP_ACT_ERRNO cannot be overridden to SCMP_ACT_NOTIFY,
which means that syscalls using that default action can&amp;#39;t be traced by the notifier
&lt;/code>&lt;/pre>&lt;p>As a conclusion, the seccomp notifier implementation in CRI-O can be used to
verify if your applications behave correctly when using &lt;code>RuntimeDefault&lt;/code> or any
other custom profile. Alerts can be created based on the metrics to create long
running test scenarios around that feature. Making seccomp understandable and
easier to use will increase adoption as well as help us to move towards a more
secure Kubernetes by default!&lt;/p>
&lt;p>Thank you for reading this blog post. If you'd like to read more about the
seccomp notifier, checkout the following resources:&lt;/p>
&lt;ul>
&lt;li>The Seccomp Notifier - New Frontiers in Unprivileged Container Development: &lt;a href="https://brauner.io/2020/07/23/seccomp-notify.html">https://brauner.io/2020/07/23/seccomp-notify.html&lt;/a>&lt;/li>
&lt;li>Bringing Seccomp Notify to Runc and Kubernetes: &lt;a href="https://kinvolk.io/blog/2022/03/bringing-seccomp-notify-to-runc-and-kubernetes">https://kinvolk.io/blog/2022/03/bringing-seccomp-notify-to-runc-and-kubernetes&lt;/a>&lt;/li>
&lt;li>Seccomp Agent reference implementation: &lt;a href="https://github.com/opencontainers/runc/tree/6b16d00/contrib/cmd/seccompagent">https://github.com/opencontainers/runc/tree/6b16d00/contrib/cmd/seccompagent&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Boosting Kubernetes container runtime observability with OpenTelemetry</title><link>https://kubernetes.io/blog/2022/12/01/runtime-observability-opentelemetry/</link><pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/12/01/runtime-observability-opentelemetry/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Sascha Grunert&lt;/p>
&lt;p>When speaking about observability in the cloud native space, then probably
everyone will mention &lt;a href="https://opentelemetry.io">OpenTelemetry (OTEL)&lt;/a> at some point in the
conversation. That's great, because the community needs standards to rely on
for developing all cluster components into the same direction. OpenTelemetry
enables us to combine logs, metrics, traces and other contextual information
(called baggage) into a single resource. Cluster administrators or software
engineers can use this resource to get a viewport about what is going on in the
cluster over a defined period of time. But how can Kubernetes itself make use of
this technology stack?&lt;/p>
&lt;p>Kubernetes consists of multiple components where some are independent and others
are stacked together. Looking at the architecture from a container runtime
perspective, then there are from the top to the bottom:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>kube-apiserver&lt;/strong>: Validates and configures data for the API objects&lt;/li>
&lt;li>&lt;strong>kubelet&lt;/strong>: Agent running on each node&lt;/li>
&lt;li>&lt;strong>CRI runtime&lt;/strong>: Container Runtime Interface (CRI) compatible container runtime
like &lt;a href="https://cri-o.io">CRI-O&lt;/a> or &lt;a href="https://containerd.io">containerd&lt;/a>&lt;/li>
&lt;li>&lt;strong>OCI runtime&lt;/strong>: Lower level &lt;a href="https://opencontainers.org">Open Container Initiative (OCI)&lt;/a> runtime
like &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> or &lt;a href="https://github.com/containers/crun">crun&lt;/a>&lt;/li>
&lt;li>&lt;strong>Linux kernel&lt;/strong> or &lt;strong>Microsoft Windows&lt;/strong>: Underlying operating system&lt;/li>
&lt;/ul>
&lt;p>That means if we encounter a problem with running containers in Kubernetes, then
we start looking at one of those components. Finding the root cause for problems
is one of the most time consuming actions we face with the increased
architectural complexity from today's cluster setups. Even if we know the
component which seems to cause the issue, we still have to take the others into
account to maintain a mental timeline of events which are going on. How do we
achieve that? Well, most folks will probably stick to scraping logs, filtering
them and assembling them together over the components borders. We also have
metrics, right? Correct, but bringing metrics values in correlation with plain
logs makes it even harder to track what is going on. Some metrics are also not
made for debugging purposes. They have been defined based on the end user
perspective of the cluster for linking usable alerts and not for developers
debugging a cluster setup.&lt;/p>
&lt;p>OpenTelemetry to the rescue: the project aims to combine signals such as
&lt;a href="https://opentelemetry.io/docs/concepts/signals/traces">traces&lt;/a>, &lt;a href="https://opentelemetry.io/docs/concepts/signals/metrics">metrics&lt;/a> and &lt;a href="https://opentelemetry.io/docs/concepts/signals/logs">logs&lt;/a> together to maintain the
right viewport on the cluster state.&lt;/p>
&lt;p>What is the current state of OpenTelemetry tracing in Kubernetes? From an API
server perspective, we have alpha support for tracing since Kubernetes v1.22,
which will graduate to beta in one of the upcoming releases. Unfortunately the
beta graduation has missed the v1.26 Kubernetes release. The design proposal can
be found in the &lt;a href="https://github.com/kubernetes/enhancements/issues/647">&lt;em>API Server Tracing&lt;/em> Kubernetes Enhancement Proposal
(KEP)&lt;/a> which provides more information about it.&lt;/p>
&lt;p>The kubelet tracing part is tracked &lt;a href="https://github.com/kubernetes/enhancements/issues/2831">in another KEP&lt;/a>, which was
implemented in an alpha state in Kubernetes v1.25. A beta graduation is not
planned as time of writing, but more may come in the v1.27 release cycle.
There are other side-efforts going on beside both KEPs, for example &lt;a href="https://github.com/kubernetes/klog/issues/356">klog is
considering OTEL support&lt;/a>, which would boost the observability by
linking log messages to existing traces. Within SIG Instrumentation and SIG Node,
we're also discussing &lt;a href="https://github.com/kubernetes/kubernetes/issues/113414">how to link the
kubelet traces together&lt;/a>, because right now they're focused on the
&lt;a href="https://grpc.io">gRPC&lt;/a> calls between the kubelet and the CRI container runtime.&lt;/p>
&lt;p>CRI-O features OpenTelemetry tracing support &lt;a href="https://github.com/cri-o/cri-o/pull/4883">since v1.23.0&lt;/a> and is
working on continuously improving them, for example by &lt;a href="https://github.com/cri-o/cri-o/pull/6294">attaching the logs to the
traces&lt;/a> or extending the &lt;a href="https://github.com/cri-o/cri-o/pull/6343">spans to logical parts of the
application&lt;/a>. This helps users of the traces to gain the same
information like parsing the logs, but with enhanced capabilities of scoping and
filtering to other OTEL signals. The CRI-O maintainers are also working on a
container monitoring replacement for &lt;a href="https://github.com/containers/conmon">conmon&lt;/a>, which is called
&lt;a href="https://github.com/containers/conmon-rs">conmon-rs&lt;/a> and is purely written in &lt;a href="https://www.rust-lang.org">Rust&lt;/a>. One benefit of
having a Rust implementation is to be able to add features like OpenTelemetry
support, because the crates (libraries) for those already exist. This allows a
tight integration with CRI-O and lets consumers see the most low level tracing
data from their containers.&lt;/p>
&lt;p>The &lt;a href="https://containerd.io">containerd&lt;/a> folks added tracing support since v1.6.0, which is
available &lt;a href="https://github.com/containerd/containerd/blob/7def13d/docs/tracing.md">by using a plugin&lt;/a>. Lower level OCI runtimes like
&lt;a href="https://github.com/opencontainers/runc">runc&lt;/a> or &lt;a href="https://github.com/containers/crun">crun&lt;/a> feature no support for OTEL at all and it does not
seem to exist a plan for that. We always have to consider that there is a
performance overhead when collecting the traces as well as exporting them to a
data sink. I still think it would be worth an evaluation on how extended
telemetry collection could look like in OCI runtimes. Let's see if the Rust OCI
runtime &lt;a href="https://github.com/containers/youki/issues/1348">youki&lt;/a> is considering something like that in the future.&lt;/p>
&lt;p>I'll show you how to give it a try. For my demo I'll stick to a stack with a single local node
that has runc, conmon-rs, CRI-O, and a kubelet. To enable tracing in the kubelet, I need to
apply the following &lt;code>KubeletConfiguration&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubelet.config.k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeletConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">featureGates&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">KubeletTracing&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">tracing&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">samplingRatePerMillion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>A &lt;code>samplingRatePerMillion&lt;/code> equally to one million will internally translate to
sampling everything. A similar configuration has to be applied to CRI-O; I can
either start the &lt;code>crio&lt;/code> binary with &lt;code>--enable-tracing&lt;/code> and
&lt;code>--tracing-sampling-rate-per-million 1000000&lt;/code> or we use a drop-in configuration
like this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat /etc/crio/crio.conf.d/99-tracing.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[crio.tracing]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>enable_tracing = &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>tracing_sampling_rate_per_million = &lt;span style="color:#666">1000000&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>To configure CRI-O to use conmon-rs, you require at least the latest CRI-O
v1.25.x and conmon-rs v0.4.0. Then a configuration drop-in like this can be used
to make CRI-O use conmon-rs:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>cat /etc/crio/crio.conf.d/99-runtimes.conf
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-toml" data-lang="toml">&lt;span style="display:flex;">&lt;span>[crio.runtime]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>default_runtime = &lt;span style="color:#b44">&amp;#34;runc&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>[crio.runtime.runtimes.runc]
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>runtime_type = &lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>monitor_path = &lt;span style="color:#b44">&amp;#34;/path/to/conmonrs&amp;#34;&lt;/span> &lt;span style="color:#080;font-style:italic"># or will be looked up in $PATH&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>That's it, the default configuration will point to an &lt;a href="https://opentelemetry.io/docs/collector/getting-started">OpenTelemetry
collector&lt;/a> &lt;a href="https://grpc.io">gRPC&lt;/a> endpoint of &lt;code>localhost:4317&lt;/code>, which has to be up and
running as well. There are multiple ways to run OTLP as &lt;a href="https://opentelemetry.io/docs/collector/getting-started">described in the
docs&lt;/a>, but it's also possible to &lt;code>kubectl proxy&lt;/code> into an existing
instance running within Kubernetes.&lt;/p>
&lt;p>If everything is set up, then the collector should log that there are incoming
traces:&lt;/p>
&lt;pre tabindex="0">&lt;code>ScopeSpans #0
ScopeSpans SchemaURL:
InstrumentationScope go.opentelemetry.io/otel/sdk/tracer
Span #0
Trace ID : 71896e69f7d337730dfedb6356e74f01
Parent ID : a2a7714534c017e6
ID : 1d27dbaf38b9da8b
Name : github.com/cri-o/cri-o/server.(*Server).filterSandboxList
Kind : SPAN_KIND_INTERNAL
Start time : 2022-11-15 09:50:20.060325562 +0000 UTC
End time : 2022-11-15 09:50:20.060326291 +0000 UTC
Status code : STATUS_CODE_UNSET
Status message :
Span #1
Trace ID : 71896e69f7d337730dfedb6356e74f01
Parent ID : a837a005d4389579
ID : a2a7714534c017e6
Name : github.com/cri-o/cri-o/server.(*Server).ListPodSandbox
Kind : SPAN_KIND_INTERNAL
Start time : 2022-11-15 09:50:20.060321973 +0000 UTC
End time : 2022-11-15 09:50:20.060330602 +0000 UTC
Status code : STATUS_CODE_UNSET
Status message :
Span #2
Trace ID : fae6742709d51a9b6606b6cb9f381b96
Parent ID : 3755d12b32610516
ID : 0492afd26519b4b0
Name : github.com/cri-o/cri-o/server.(*Server).filterContainerList
Kind : SPAN_KIND_INTERNAL
Start time : 2022-11-15 09:50:20.0607746 +0000 UTC
End time : 2022-11-15 09:50:20.060795505 +0000 UTC
Status code : STATUS_CODE_UNSET
Status message :
Events:
SpanEvent #0
-&amp;gt; Name: log
-&amp;gt; Timestamp: 2022-11-15 09:50:20.060778668 +0000 UTC
-&amp;gt; DroppedAttributesCount: 0
-&amp;gt; Attributes::
-&amp;gt; id: Str(adf791e5-2eb8-4425-b092-f217923fef93)
-&amp;gt; log.message: Str(No filters were applied, returning full container list)
-&amp;gt; log.severity: Str(DEBUG)
-&amp;gt; name: Str(/runtime.v1.RuntimeService/ListContainers)
&lt;/code>&lt;/pre>&lt;p>I can see that the spans have a trace ID and typically have a parent attached.
Events such as logs are part of the output as well. In the above case, the kubelet is
periodically triggering a &lt;code>ListPodSandbox&lt;/code> RPC to CRI-O caused by the Pod
Lifecycle Event Generator (PLEG). Displaying those traces can be done via,
for example, &lt;a href="https://www.jaegertracing.io/">Jaeger&lt;/a>. When running the tracing stack locally, then a Jaeger
instance should be exposed on &lt;code>http://localhost:16686&lt;/code> per default.&lt;/p>
&lt;p>The &lt;code>ListPodSandbox&lt;/code> requests are directly visible within the Jaeger UI:&lt;/p>
&lt;p>&lt;img src="list_pod_sandbox.png" alt="ListPodSandbox RPC in the Jaeger UI">&lt;/p>
&lt;p>That's not too exciting, so I'll run a workload directly via &lt;code>kubectl&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl run -it --rm --restart&lt;span style="color:#666">=&lt;/span>Never --image&lt;span style="color:#666">=&lt;/span>alpine alpine -- &lt;span style="color:#a2f">echo&lt;/span> hi
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;pre tabindex="0">&lt;code>hi
pod &amp;#34;alpine&amp;#34; deleted
&lt;/code>&lt;/pre>&lt;p>Looking now at Jaeger, we can see that we have traces for &lt;code>conmonrs&lt;/code>, &lt;code>crio&lt;/code> as
well as the &lt;code>kubelet&lt;/code> for the &lt;code>RunPodSandbox&lt;/code> and &lt;code>CreateContainer&lt;/code> CRI RPCs:&lt;/p>
&lt;p>&lt;img src="create_container.png" alt="Container creation in the Jaeger UI">&lt;/p>
&lt;p>The kubelet and CRI-O spans are connected to each other to make investigation
easier. If we now take a closer look at the spans, then we can see that CRI-O's
logs are correctly accosted with the corresponding functionality. For example we
can extract the container user from the traces like this:&lt;/p>
&lt;p>&lt;img src="crio_spans.png" alt="CRI-O in the Jaeger UI">&lt;/p>
&lt;p>The lower level spans of conmon-rs are also part of this trace. For example
conmon-rs maintains an internal &lt;code>read_loop&lt;/code> for handling IO between the
container and the end user. The logs for reading and writing bytes are part of
the span. The same applies to the &lt;code>wait_for_exit_code&lt;/code> span, which tells us that
the container exited successfully with code &lt;code>0&lt;/code>:&lt;/p>
&lt;p>&lt;img src="conmonrs_spans.png" alt="conmon-rs in the Jaeger UI">&lt;/p>
&lt;p>Having all that information at hand side by side to the filtering capabilities
of Jaeger makes the whole stack a great solution for debugging container issues!
Mentioning the &amp;quot;whole stack&amp;quot; also shows the biggest downside of the overall
approach: Compared to parsing logs it adds a noticeable overhead on top of the
cluster setup. Users have to maintain a sink like &lt;a href="https://www.elastic.co">Elasticsearch&lt;/a> to
persist the data, expose the Jaeger UI and possibly take the performance
drawback into account. Anyways, it's still one of the best ways to increase the
observability aspect of Kubernetes.&lt;/p>
&lt;p>Thank you for reading this blog post, I'm pretty sure we're looking into a
bright future for OpenTelemetry support in Kubernetes to make troubleshooting
simpler.&lt;/p></description></item><item><title>Blog: registry.k8s.io: faster, cheaper and Generally Available (GA)</title><link>https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/</link><pubDate>Mon, 28 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/11/28/registry-k8s-io-faster-cheaper-ga/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Adolfo García Veytia (Chainguard), Bob Killen (Google)&lt;/p>
&lt;p>Starting with Kubernetes 1.25, our container image registry has changed from k8s.gcr.io to &lt;a href="https://registry.k8s.io">registry.k8s.io&lt;/a>. This new registry spreads the load across multiple Cloud Providers &amp;amp; Regions, functioning as a sort of content delivery network (CDN) for Kubernetes container images. This change reduces the project’s reliance on a single entity and provides a faster download experience for a large number of users.&lt;/p>
&lt;h2 id="tl-dr-what-you-need-to-know-about-this-change">TL;DR: What you need to know about this change&lt;/h2>
&lt;ul>
&lt;li>Container images for Kubernetes releases from 1.25 onward are no longer published to k8s.gcr.io, only to registry.k8s.io.&lt;/li>
&lt;li>In the upcoming December patch releases, the new registry domain default will be backported to all branches still in support (1.22, 1.23, 1.24).&lt;/li>
&lt;li>If you run in a restricted environment and apply strict domain/IP address access policies limited to k8s.gcr.io, the &lt;strong>image pulls will not function&lt;/strong> after the migration to this new registry. For these users, the recommended method is to mirror the release images to a private registry.&lt;/li>
&lt;/ul>
&lt;p>If you’d like to know more about why we made this change, or some potential issues you might run into, keep reading.&lt;/p>
&lt;h2 id="why-has-kubernetes-changed-to-a-different-image-registry">Why has Kubernetes changed to a different image registry?&lt;/h2>
&lt;p>k8s.gcr.io is hosted on a custom &lt;a href="https://cloud.google.com/container-registry">Google Container Registry&lt;/a> (GCR) domain that was setup solely for the Kubernetes project. This has worked well since the inception of the project, and we thank Google for providing these resources, but today there are other cloud providers and vendors that would like to host images to provide a better experience for the people on their platforms. In addition to Google’s &lt;a href="https://www.cncf.io/google-cloud-recommits-3m-to-kubernetes/">renewed commitment to donate $3 million&lt;/a> to support the project's infrastructure, Amazon announced a matching donation during their Kubecon NA 2022 keynote in Detroit. This will provide a better experience for users (closer servers = faster downloads) and will reduce the egress bandwidth and costs from GCR at the same time. registry.k8s.io will spread the load between Google and Amazon, with other providers to follow in the future.&lt;/p>
&lt;h2 id="why-isn-t-there-a-stable-list-of-domains-ips-why-can-t-i-restrict-image-pulls">Why isn’t there a stable list of domains/IPs? Why can’t I restrict image pulls?&lt;/h2>
&lt;p>registry.k8s.io is a &lt;a href="https://github.com/kubernetes/registry.k8s.io/blob/main/cmd/archeio/docs/request-handling.md">secure blob redirector&lt;/a> that connects clients to the closest cloud provider. The nature of this change means that a client pulling an image could be redirected to any one of a large number of backends. We expect the set of backends to keep changing and will only increase as more and more cloud providers and vendors come on board to help mirror the release images.&lt;/p>
&lt;p>Restrictive control mechanisms like man-in-the-middle proxies or network policies that restrict access to a specific list of IPs/domains will break with this change. For these scenarios, we encourage you to mirror the release images to a local registry that you have strict control over.&lt;/p>
&lt;p>For more information on this policy, please see the &lt;a href="https://github.com/kubernetes/registry.k8s.io#stability">stability section of the registry.k8s.io documentation&lt;/a>.&lt;/p>
&lt;h2 id="what-kind-of-errors-will-i-see-how-will-i-know-if-i-m-still-using-the-old-address">What kind of errors will I see? How will I know if I’m still using the old address?&lt;/h2>
&lt;p>Errors may depend on what kind of container runtime you are using, and what endpoint you are routed to, but it should present as a container failing to be created with the warning &lt;code>FailedCreatePodSandBox&lt;/code>.&lt;/p>
&lt;p>Below is an example error message showing a proxied deployment failing to pull due to an unknown certificate:&lt;/p>
&lt;pre tabindex="0">&lt;code>FailedCreatePodSandBox: Failed to create pod sandbox: rpc error: code = Unknown desc = Error response from daemon: Head “https://us-west1-docker.pkg.dev/v2/k8s-artifacts-prod/images/pause/manifests/3.8”: x509: certificate signed by unknown authority
&lt;/code>&lt;/pre>&lt;h2 id="i-m-impacted-by-this-change-how-do-i-revert-to-the-old-registry-address">I’m impacted by this change, how do I revert to the old registry address?&lt;/h2>
&lt;p>If using the new registry domain name is not an option, you can revert to the old domain name for cluster versions less than 1.25. Keep in mind that, eventually, you will have to switch to the new registry, as new image tags will no longer be pushed to GCR.&lt;/p>
&lt;h3 id="reverting-the-registry-name-in-kubeadm">Reverting the registry name in kubeadm&lt;/h3>
&lt;p>The registry used by kubeadm to pull its images can be controlled by two methods:&lt;/p>
&lt;p>Setting the &lt;code>--image-repository&lt;/code> flag.&lt;/p>
&lt;pre tabindex="0">&lt;code>kubeadm init --image-repository=k8s.gcr.io
&lt;/code>&lt;/pre>&lt;p>Or in &lt;a href="https://kubernetes.io/docs/reference/config-api/kubeadm-config.v1beta3/">kubeadm config&lt;/a> &lt;code>ClusterConfiguration&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubeadm.k8s.io/v1beta3&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">imageRepository&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;k8s.gcr.io&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="reverting-the-registry-name-in-kubelet">Reverting the Registry Name in kubelet&lt;/h3>
&lt;p>The image used by kubelet for the pod sandbox (&lt;code>pause&lt;/code>) can be overridden by setting the &lt;code>--pod-infra-container-image&lt;/code> flag. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubelet --pod-infra-container-image=k8s.gcr.io/pause:3.5
&lt;/code>&lt;/pre>&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>&lt;strong>Change is hard&lt;/strong>, and evolving our image-serving platform is needed to ensure a sustainable future for the project. We strive to make things better for everyone using Kubernetes. Many contributors from all corners of our community have been working long and hard to ensure we are making the best decisions possible, executing plans, and doing our best to communicate those plans.&lt;/p>
&lt;p>Thanks to Aaron Crickenberger, Arnaud Meukam, Benjamin Elder, Caleb Woodbine, Davanum Srinivas, Mahamed Ali, and Tim Hockin from SIG K8s Infra, Brian McQueen, and Sergey Kanzhelev from SIG Node, Lubomir Ivanov from SIG Cluster Lifecycle, Adolfo García Veytia, Jeremy Rickard, Sascha Grunert, and Stephen Augustus from SIG Release, Bob Killen and Kaslin Fields from SIG Contribex, Tim Allclair from the Security Response Committee. Also a big thank you to our friends acting as liaisons with our cloud provider partners: Jay Pipes from Amazon and Jon Johnson Jr. from Google.&lt;/p></description></item><item><title>Blog: Kubernetes Removals, Deprecations, and Major Changes in 1.26</title><link>https://kubernetes.io/blog/2022/11/18/upcoming-changes-in-kubernetes-1-26/</link><pubDate>Fri, 18 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/11/18/upcoming-changes-in-kubernetes-1-26/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Frederico Muñoz (SAS)&lt;/p>
&lt;p>Change is an integral part of the Kubernetes life-cycle: as Kubernetes grows and matures, features may be deprecated, removed, or replaced with improvements for the health of the project. For Kubernetes v1.26 there are several planned: this article identifies and describes some of them, based on the information available at this mid-cycle point in the v1.26 release process, which is still ongoing and can introduce additional changes.&lt;/p>
&lt;h2 id="k8s-api-deprecation-process">The Kubernetes API Removal and Deprecation process&lt;/h2>
&lt;p>The Kubernetes project has a &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">well-documented deprecation policy&lt;/a> for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API is one that has been marked for removal in a future Kubernetes release; it will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.&lt;/p>
&lt;ul>
&lt;li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.&lt;/li>
&lt;li>Beta or pre-release API versions must be supported for 3 releases after deprecation.&lt;/li>
&lt;li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.&lt;/li>
&lt;/ul>
&lt;p>Whether an API is removed as a result of a feature graduating from beta to stable or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the documentation.&lt;/p>
&lt;h2 id="cri-api-removal">A note about the removal of the CRI &lt;code>v1alpha2&lt;/code> API and containerd 1.5 support&lt;/h2>
&lt;p>Following the adoption of the &lt;a href="https://kubernetes.io/docs/concepts/architecture/cri/">Container Runtime Interface&lt;/a> (CRI) and the [removal of dockershim] in v1.24 , the CRI is the supported and documented way through which Kubernetes interacts with different container runtimes. Each kubelet negotiates which version of CRI to use with the container runtime on that node.&lt;/p>
&lt;p>The Kubernetes project recommends using CRI version &lt;code>v1&lt;/code>; in Kubernetes v1.25 the kubelet can also negotiate the use of CRI &lt;code>v1alpha2&lt;/code> (which was deprecated along at the same time as adding support for the stable &lt;code>v1&lt;/code> interface).&lt;/p>
&lt;p>Kubernetes v1.26 will not support CRI &lt;code>v1alpha2&lt;/code>. That &lt;a href="https://github.com/kubernetes/kubernetes/pull/110618">removal&lt;/a> will result in the kubelet not registering the node if the container runtime doesn't support CRI &lt;code>v1&lt;/code>. This means that containerd minor version 1.5 and older will not be supported in Kubernetes 1.26; if you use containerd, you will need to upgrade to containerd version 1.6.0 or later &lt;strong>before&lt;/strong> you upgrade that node to Kubernetes v1.26. Other container runtimes that only support the &lt;code>v1alpha2&lt;/code> are equally affected: if that affects you, you should contact the container runtime vendor for advice or check their website for additional instructions in how to move forward.&lt;/p>
&lt;p>If you want to benefit from v1.26 features and still use an older container runtime, you can run an older kubelet. The &lt;a href="https://kubernetes.io/releases/version-skew-policy/#kubelet">supported skew&lt;/a> for the kubelet allows you to run a v1.25 kubelet, which still is still compatible with &lt;code>v1alpha2&lt;/code> CRI support, even if you upgrade the control plane to the 1.26 minor release of Kubernetes.&lt;/p>
&lt;p>As well as container runtimes themselves, that there are tools like &lt;a href="https://github.com/containerd/stargz-snapshotter">stargz-snapshotter&lt;/a> that act as a proxy between kubelet and container runtime and those also might be affected.&lt;/p>
&lt;h2 id="deprecations-removals">Deprecations and removals in Kubernetes v1.26&lt;/h2>
&lt;p>In addition to the above, Kubernetes v1.26 is targeted to include several additional removals and deprecations.&lt;/p>
&lt;h3 id="removal-of-the-v1beta1-flow-control-api-group">Removal of the &lt;code>v1beta1&lt;/code> flow control API group&lt;/h3>
&lt;p>The &lt;code>flowcontrol.apiserver.k8s.io/v1beta1&lt;/code> API version of FlowSchema and PriorityLevelConfiguration &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#flowcontrol-resources-v126">will no longer be served in v1.26&lt;/a>. Users should migrate manifests and API clients to use the &lt;code>flowcontrol.apiserver.k8s.io/v1beta2&lt;/code> API version, available since v1.23.&lt;/p>
&lt;h3 id="removal-of-the-v2beta2-horizontalpodautoscaler-api">Removal of the &lt;code>v2beta2&lt;/code> HorizontalPodAutoscaler API&lt;/h3>
&lt;p>The &lt;code>autoscaling/v2beta2&lt;/code> API version of HorizontalPodAutoscaler &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#horizontalpodautoscaler-v126">will no longer be served in v1.26&lt;/a>. Users should migrate manifests and API clients to use the &lt;code>autoscaling/v2&lt;/code> API version, available since v1.23.&lt;/p>
&lt;h3 id="removal-of-in-tree-credential-management-code">Removal of in-tree credential management code&lt;/h3>
&lt;p>In this upcoming release, legacy vendor-specific authentication code that is part of Kubernetes
will be &lt;a href="https://github.com/kubernetes/kubernetes/pull/112341">removed&lt;/a> from both
&lt;code>client-go&lt;/code> and &lt;code>kubectl&lt;/code>.
The existing mechanism supports authentication for two specific cloud providers:
Azure and Google Cloud.
In its place, Kubernetes already offers a vendor-neutral
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins">authentication plugin mechanism&lt;/a> -
you can switch over right now, before the v1.26 release happens.
If you're affected, you can find additional guidance on how to proceed for
&lt;a href="https://github.com/Azure/kubelogin#readme">Azure&lt;/a> and for
&lt;a href="https://cloud.google.com/blog/products/containers-kubernetes/kubectl-auth-changes-in-gke">Google Cloud&lt;/a>.&lt;/p>
&lt;h3 id="removal-of-kube-proxy-userspace-modes">Removal of &lt;code>kube-proxy&lt;/code> userspace modes&lt;/h3>
&lt;p>The &lt;code>userspace&lt;/code> proxy mode, deprecated for over a year, is &lt;a href="https://github.com/kubernetes/kubernetes/pull/112133">no longer supported on either Linux or Windows&lt;/a> and will be removed in this release. Users should use &lt;code>iptables&lt;/code> or &lt;code>ipvs&lt;/code> on Linux, or &lt;code>kernelspace&lt;/code> on Windows: using &lt;code>--mode userspace&lt;/code> will now fail.&lt;/p>
&lt;h3 id="removal-of-in-tree-openstack-cloud-provider">Removal of in-tree OpenStack cloud provider&lt;/h3>
&lt;p>Kubernetes is switching from in-tree code for storage integrations, in favor of the Container Storage Interface (CSI).
As part of this, Kubernetes v1.26 will remove the deprecated in-tree storage integration for OpenStack
(the &lt;code>cinder&lt;/code> volume type). You should migrate to external cloud provider and CSI driver from
&lt;a href="https://github.com/kubernetes/cloud-provider-openstack">https://github.com/kubernetes/cloud-provider-openstack&lt;/a> instead.
For more information, visit &lt;a href="https://github.com/kubernetes/enhancements/issues/1489">Cinder in-tree to CSI driver migration&lt;/a>.&lt;/p>
&lt;h3 id="removal-of-the-glusterfs-in-tree-driver">Removal of the GlusterFS in-tree driver&lt;/h3>
&lt;p>The in-tree GlusterFS driver was &lt;a href="https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/#deprecations-and-removals">deprecated in v1.25&lt;/a>, and will be removed from Kubernetes v1.26.&lt;/p>
&lt;h3 id="deprecation-of-non-inclusive-kubectl-flag">Deprecation of non-inclusive &lt;code>kubectl&lt;/code> flag&lt;/h3>
&lt;p>As part of the implementation effort of the &lt;a href="https://www.cncf.io/announcements/2021/10/13/inclusive-naming-initiative-announces-new-community-resources-for-a-more-inclusive-future/">Inclusive Naming Initiative&lt;/a>,
the &lt;code>--prune-whitelist&lt;/code> flag will be &lt;a href="https://github.com/kubernetes/kubernetes/pull/113116">deprecated&lt;/a>, and replaced with &lt;code>--prune-allowlist&lt;/code>.
Users that use this flag are strongly advised to make the necessary changes prior to the final removal of the flag, in a future release.&lt;/p>
&lt;h3 id="removal-of-dynamic-kubelet-configuration">Removal of dynamic kubelet configuration&lt;/h3>
&lt;p>&lt;em>Dynamic kubelet configuration&lt;/em> allowed &lt;a href="https://github.com/kubernetes/enhancements/tree/2cd758cc6ab617a93f578b40e97728261ab886ed/keps/sig-node/281-dynamic-kubelet-configuration">new kubelet configurations to be rolled out via the Kubernetes API&lt;/a>, even in a live cluster.
A cluster operator could reconfigure the kubelet on a Node by specifying a ConfigMap
that contained the configuration data that the kubelet should use.
Dynamic kubelet configuration was removed from the kubelet in v1.24, and will be
&lt;a href="https://github.com/kubernetes/kubernetes/pull/112643">removed from the API server&lt;/a> in the v1.26 release.&lt;/p>
&lt;h3 id="deprecations-for-kube-apiserver-command-line-arguments">Deprecations for &lt;code>kube-apiserver&lt;/code> command line arguments&lt;/h3>
&lt;p>The &lt;code>--master-service-namespace&lt;/code> command line argument to the kube-apiserver doesn't have
any effect, and was already informally &lt;a href="https://github.com/kubernetes/kubernetes/pull/38186">deprecated&lt;/a>.
That command line argument will be formally marked as deprecated in v1.26, preparing for its
removal in a future release.
The Kubernetes project does not expect any impact from this deprecation and removal.&lt;/p>
&lt;h3 id="deprecations-for-kubectl-run-command-line-arguments">Deprecations for &lt;code>kubectl run&lt;/code> command line arguments&lt;/h3>
&lt;p>Several unused option arguments for the &lt;code>kubectl run&lt;/code> subcommand will be &lt;a href="https://github.com/kubernetes/kubernetes/pull/112261">marked as deprecated&lt;/a>, including:&lt;/p>
&lt;ul>
&lt;li>&lt;code>--cascade&lt;/code>&lt;/li>
&lt;li>&lt;code>--filename&lt;/code>&lt;/li>
&lt;li>&lt;code>--force&lt;/code>&lt;/li>
&lt;li>&lt;code>--grace-period&lt;/code>&lt;/li>
&lt;li>&lt;code>--kustomize&lt;/code>&lt;/li>
&lt;li>&lt;code>--recursive&lt;/code>&lt;/li>
&lt;li>&lt;code>--timeout&lt;/code>&lt;/li>
&lt;li>&lt;code>--wait&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>These arguments are already ignored so no impact is expected: the explicit deprecation sets a warning message and prepares the removal of the arguments in a future release.&lt;/p>
&lt;h3 id="removal-of-legacy-command-line-arguments-relating-to-logging">Removal of legacy command line arguments relating to logging&lt;/h3>
&lt;p>Kubernetes v1.26 will &lt;a href="https://github.com/kubernetes/kubernetes/pull/112120">remove&lt;/a> some
command line arguments relating to logging. These command line arguments were
already deprecated.
For more information, see &lt;a href="https://github.com/kubernetes/enhancements/tree/3cb66bd0a1ef973ebcc974f935f0ac5cba9db4b2/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components">Deprecate klog specific flags in Kubernetes Components&lt;/a>.&lt;/p>
&lt;h2 id="looking-ahead">Looking ahead&lt;/h2>
&lt;p>The official list of &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-27">API removals&lt;/a> planned for Kubernetes 1.27 includes:&lt;/p>
&lt;ul>
&lt;li>All beta versions of the CSIStorageCapacity API; specifically: &lt;code>storage.k8s.io/v1beta1&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="want-to-know-more">Want to know more?&lt;/h3>
&lt;p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation">Kubernetes 1.25&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>We will formally announce the deprecations that come with &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.26.md#deprecation">Kubernetes 1.26&lt;/a> as part of the CHANGELOG for that release.&lt;/p></description></item><item><title>Blog: Live and let live with Kluctl and Server Side Apply</title><link>https://kubernetes.io/blog/2022/11/04/live-and-let-live-with-kluctl-and-ssa/</link><pubDate>Fri, 04 Nov 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/11/04/live-and-let-live-with-kluctl-and-ssa/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Alexander Block&lt;/p>
&lt;p>This blog post was inspired by a previous Kubernetes blog post about
&lt;a href="https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/">Advanced Server Side Apply&lt;/a>.
The author of said blog post listed multiple benefits for applications and
controllers when switching to server-side apply (from now on abbreviated with
SSA). Especially the chapter about
&lt;a href="https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/#ci-cd-systems">CI/CD systems&lt;/a>
motivated me to respond and write down my thoughts and experiences.&lt;/p>
&lt;p>These thoughts and experiences are the results of me working on &lt;a href="https://kluctl.io">Kluctl&lt;/a>
for the past 2 years. I describe Kluctl as &amp;quot;The missing glue to put together
large Kubernetes deployments, composed of multiple smaller parts
(Helm/Kustomize/...) in a manageable and unified way.&amp;quot;&lt;/p>
&lt;p>To get a basic understanding of Kluctl, I suggest to visit the &lt;a href="https://kluctl.io">kluctl.io&lt;/a>
website and read through the documentation and tutorials, for example the
&lt;a href="https://kluctl.io/docs/guides/tutorials/microservices-demo/">microservices demo tutorial&lt;/a>.
As an alternative, you can watch &lt;a href="https://www.youtube.com/watch?v=9LoYLjDjOdg">Hands-on Introduction to kluctl&lt;/a>
from the Rawkode Academy YouTube channel which shows a hands-on demo session.&lt;/p>
&lt;p>There is also a &lt;a href="https://github.com/codablock/podtato-head/tree/kluctl/delivery/kluctl">Kluctl delivery scenario&lt;/a>
available in my fork of the &lt;a href="https://github.com/codablock/podtato-head">podtato-head&lt;/a> demo project.&lt;/p>
&lt;h2 id="live-and-let-live">Live and let live&lt;/h2>
&lt;p>One of the main philosophies that Kluctl follows is &lt;a href="https://kluctl.io/docs/philosophy/#live-and-let-live">&amp;quot;live and let live&amp;quot;&lt;/a>,
meaning that it will try its best to work in conjunction with any other tool or
controller running outside or inside your clusters. Kluctl will not overwrite
any fields that it lost ownership of, unless you explicitly tell it to do so.&lt;/p>
&lt;p>Achieving this would not have been possible (or at least several magnitudes
harder) without the use of SSA. Server-side apply allows Kluctl
to detect when ownership for a field got lost, for example when another controller
or operator updates that field to another value. Kluctl can then decide on a
field-by-field basis if force-applying is required before retrying based on these
decisions.&lt;/p>
&lt;h2 id="the-days-before-ssa">The days before SSA&lt;/h2>
&lt;p>The first versions of Kluctl were based on shelling out to &lt;code>kubectl&lt;/code> and thus
implicitly relied on client-side apply. At that time, SSA was
still alpha and quite buggy. And to be honest, I didn't even know it was a
thing at that time.&lt;/p>
&lt;p>The way client-side apply worked had some serious drawbacks. The most obvious one
(it was guaranteed that you'd stumble on this by yourself if enough time passed)
is that it relied on an annotation (&lt;code>kubectl.kubernetes.io/last-applied-configuration&lt;/code>)
being added to the object, bringing in all the limitations and issues with huge
annotation values. A good example of such issues are
&lt;a href="https://github.com/prometheus-operator/prometheus-operator/issues/4439">CRDs being so large&lt;/a>,
that they don't fit into the annotation's value anymore.&lt;/p>
&lt;p>Another drawback can be seen just by looking at the name (&lt;strong>client&lt;/strong>-side apply).
Being &lt;strong>client&lt;/strong> side means that each client has to provide the apply-logic on
its own, which at that time was only properly implemented inside &lt;code>kubectl&lt;/code>,
making it hard to be replicated inside controllers.&lt;/p>
&lt;p>This added &lt;code>kubectl&lt;/code> as a dependency (either as an executable or in the form of
Go packages) to all controllers that wanted to leverage the apply-logic.&lt;/p>
&lt;p>However, even if one managed to get client-side apply running from inside a
controller, you ended up with a solution that gave no control over how it
worked internally. As an example, there was no way to individually decide which
fields to overwrite in case of external changes and which ones to let go.&lt;/p>
&lt;h2 id="discovering-ssa-apply">Discovering SSA apply&lt;/h2>
&lt;p>I was never happy with the solution described above and then somehow stumbled
across &lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">server-side apply&lt;/a>,
which was still in beta at that time. Experimenting with it via
&lt;code>kubectl apply --server-side&lt;/code> revealed immediately that the true power of
SSA can not be easily leveraged by shelling out to &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;p>The way SSA is implemented in &lt;code>kubectl&lt;/code> does not allow enough
control over conflict resolution as it can only switch between
&amp;quot;not force-applying anything and erroring out&amp;quot; and &amp;quot;force-applying everything
without showing any mercy!&amp;quot;.&lt;/p>
&lt;p>The API documentation however made it clear that SSA is able to
control conflict resolution on field level, simply by choosing which fields
to include and which fields to omit from the supplied object.&lt;/p>
&lt;h2 id="moving-away-from-kubectl">Moving away from kubectl&lt;/h2>
&lt;p>This meant that Kluctl had to move away from shelling out to &lt;code>kubectl&lt;/code> first. Only
after that was done, I would have been able to properly implement SSA
with its powerful conflict resolution.&lt;/p>
&lt;p>To achieve this, I first implemented access to the target clusters via a
Kubernetes client library. This had the nice side effect of dramatically
speeding up Kluctl as well. It also improved the security and usability of
Kluctl by ensuring that a running Kluctl command could not be messed around
with by externally modifying the kubeconfig while it was running.&lt;/p>
&lt;h2 id="implementing-ssa">Implementing SSA&lt;/h2>
&lt;p>After switching to a Kubernetes client library, leveraging SSA
felt easy. Kluctl now has to send each manifest to the API server as part of a
&lt;code>PATCH&lt;/code> request, which signals
that Kluctl wants to perform a SSA operation. The API server then
responds with an OK response (HTTP status code 200), or with a Conflict response
(HTTP status 409).&lt;/p>
&lt;p>In case of a Conflict response, the body of that response includes machine-readable
details about the conflicts. Kluctl can then use these details to figure out
which fields are in conflict and which actors (field managers) have taken
ownership of the conflicted fields.&lt;/p>
&lt;p>Then, for each field, Kluctl will decide if the conflict should be ignored or
if it should be force-applied. If any field needs to be force-applied, Kluctl
will retry the apply operation with the ignored fields omitted and the &lt;code>force&lt;/code>
flag being set on the API call.&lt;/p>
&lt;p>In case a conflict is ignored, Kluctl will issue a warning to the user so that
the user can react properly (or ignore it forever...).&lt;/p>
&lt;p>That's basically it. That is all that is required to leverage SSA.
Big thanks and thumbs-up to the Kubernetes developers who made this possible!&lt;/p>
&lt;h2 id="conflict-resolution">Conflict Resolution&lt;/h2>
&lt;p>Kluctl has a few simple rules to figure out if a conflict should be ignored
or force-applied.&lt;/p>
&lt;p>It first checks the field's actor (the field manager) against a list of known
field manager strings from tools that are frequently used to perform manual modifications. These
are for example &lt;code>kubectl&lt;/code> and &lt;code>k9s&lt;/code>. Any modifications performed with these tools
are considered &amp;quot;temporary&amp;quot; and will be overwritten by Kluctl.&lt;/p>
&lt;p>If you're using Kluctl along with &lt;code>kubectl&lt;/code> where you don't want the changes from
&lt;code>kubectl&lt;/code> to be overwritten (for example, using in a script) then you can specify
&lt;code>--field-manager=&amp;lt;manager-name&amp;gt;&lt;/code> on the command line to &lt;code>kubectl&lt;/code>, and Kluctl
doesn't apply its special heuristic.&lt;/p>
&lt;p>If the field manager is not known by Kluctl, it will check if force-applying is
requested for that field. Force-applying can be requested in different ways:&lt;/p>
&lt;ol>
&lt;li>By passing &lt;code>--force-apply&lt;/code> to Kluctl. This will cause ALL fields to be force-applied on conflicts.&lt;/li>
&lt;li>By adding the &lt;a href="https://kluctl.io/docs/reference/deployments/annotations/all-resources/#kluctlioforce-apply">&lt;code>kluctl.io/force-apply=true&lt;/code>&lt;/a> annotation to the object in question. This will cause all fields of that object to be force-applied on conflicts.&lt;/li>
&lt;li>By adding the &lt;a href="https://kluctl.io/docs/reference/deployments/annotations/all-resources/#kluctlioforce-apply-field">&lt;code>kluctl.io/force-apply-field=my.json.path&lt;/code>&lt;/a> annotation to the object in question. This causes only fields matching the JSON path to be force-applied on conflicts.&lt;/li>
&lt;/ol>
&lt;p>Marking a field to be force-applied is required whenever some other actor is
known to erroneously claim fields (the ECK operator does this to the nodeSets
field for example), you can ensure that Kluctl always overwrites these fields
to the original or a new value.&lt;/p>
&lt;p>In the future, Kluctl will allow even more control about conflict resolution.
For example, the CLI will allow to control force-applying on field level.&lt;/p>
&lt;h2 id="devops-vs-controllers">DevOps vs Controllers&lt;/h2>
&lt;p>So how does SSA in Kluctl lead to &amp;quot;live and let live&amp;quot;?&lt;/p>
&lt;p>It allows the co-existence of classical pipelines (e.g. Github Actions or
Gitlab CI), controllers (e.g. the HPA controller or GitOps style controllers)
and even admins running deployments from their local machines.&lt;/p>
&lt;p>Wherever you are on your infrastructure automation journey, Kluctl has a place
for you. From running deployments using a script on your PC, all the way to
fully automated CI/CD with the pipelines themselves defined in code, Kluctl
aims to complement the workflow that's right for you.&lt;/p>
&lt;p>And even after fully automating everything, you can intervene with your admin
permissions if required and run a &lt;code>kubectl&lt;/code> command that will modify a field
and prevent Kluctl from overwriting it. You'd just have to switch to a
field-manager (e.g. &amp;quot;admin-override&amp;quot;) that is not overwritten by Kluctl.&lt;/p>
&lt;h2 id="a-few-takeaways">A few takeaways&lt;/h2>
&lt;p>Server-side apply is a great feature and essential for the future of
controllers and tools in Kubernetes. The amount of controllers involved
will only get more and proper modes of working together are a must.&lt;/p>
&lt;p>I believe that CI/CD-related controllers and tools should leverage
SSA to perform proper conflict resolution. I also believe that
other controllers (e.g. Flux and ArgoCD) would benefit from the same kind
of conflict resolution control on field-level.&lt;/p>
&lt;p>It might even be a good idea to come together and work on a standardized
set of annotations to control conflict resolution for CI/CD-related tooling.&lt;/p>
&lt;p>On the other side, non CI/CD-related controllers should ensure that they don't
cause unnecessary conflicts when modifying objects. As of
&lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/#using-server-side-apply-in-a-controller">the server-side apply documentation&lt;/a>,
it is strongly recommended for controllers to always perform force-applying. When
following this recommendation, controllers should really make sure that only
fields related to the controller are included in the applied object.
Otherwise, unnecessary conflicts are guaranteed.&lt;/p>
&lt;p>In many cases, controllers are meant to only modify the status subresource
of the objects they manage. In this case, controllers should only patch the
status subresource and not touch the actual object. If this is followed,
conflicts become impossible to occur.&lt;/p>
&lt;p>If you are a developer of such a controller and unsure about your controller
adhering to the above, simply try to retrieve an object managed by your
controller and look at the &lt;code>managedFields&lt;/code> (you'll need to pass
&lt;code>--show-managed-fields -oyaml&lt;/code> to &lt;code>kubectl get&lt;/code>) to see if some field got
claimed unexpectedly.&lt;/p></description></item><item><title>Blog: Server Side Apply Is Great And You Should Be Using It</title><link>https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/</link><pubDate>Thu, 20 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/20/advanced-server-side-apply/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Daniel Smith (Google)&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Server-side apply&lt;/a> (SSA) has now
been &lt;a href="https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/">GA for a few releases&lt;/a>, and I
have found myself in a number of conversations, recommending that people / teams
in various situations use it. So I’d like to write down some of those reasons.&lt;/p>
&lt;h2 id="benefits">Obvious (and not-so-obvious) benefits of SSA&lt;/h2>
&lt;p>A list of improvements / niceties you get from switching from various things to
Server-side apply!&lt;/p>
&lt;ul>
&lt;li>Versus client-side-apply (that is, plain &lt;code>kubectl apply&lt;/code>):
&lt;ul>
&lt;li>The system gives you conflicts when you accidentally fight with another
actor over the value of a field!&lt;/li>
&lt;li>When combined with &lt;code>--dry-run&lt;/code>, there’s no chance of accidentally running a
client-side dry run instead of a server side dry run.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Versus hand-rolling patches:
&lt;ul>
&lt;li>The SSA patch format is extremely natural to write, with no weird syntax.
It’s just a regular object, but you can (and should) omit any field you
don’t care about.&lt;/li>
&lt;li>The old patch format (“strategic merge patch”) was ad-hoc and still has some
bugs; JSON-patch and JSON merge-patch fail to handle some cases that are
common in the Kubernetes API, namely lists with items that should be
recursively merged based on a “name” or other identifying field.&lt;/li>
&lt;li>There’s also now great &lt;a href="https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/#using-server-side-apply-in-a-controller">go-language library support&lt;/a>
for building apply calls programmatically!&lt;/li>
&lt;li>You can use SSA to explicitly delete fields you don’t “own” by setting them
to &lt;code>null&lt;/code>, which makes it a feature-complete replacement for all of the old
patch formats.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Versus shelling out to kubectl:
&lt;ul>
&lt;li>You can use the &lt;strong>apply&lt;/strong> API call from any language without shelling out to
kubectl!&lt;/li>
&lt;li>As stated above, the &lt;a href="https://kubernetes.io/blog/2021/08/06/server-side-apply-ga/#server-side-apply-support-in-client-go">Go library has dedicated mechanisms&lt;/a>
to make this easy now.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Versus GET-modify-PUT:
&lt;ul>
&lt;li>(This one is more complicated and you can skip it if you've never written a
controller!)&lt;/li>
&lt;li>To use GET-modify-PUT correctly, you have to handle and retry a write
failure in the case that someone else has modified the object in any way
between your GET and PUT. This is an “optimistic concurrency failure” when
it happens.&lt;/li>
&lt;li>SSA offloads this task to the server– you only have to retry if there’s a
conflict, and the conflicts you can get are all meaningful, like when you’re
actually trying to take a field away from another actor in the system.&lt;/li>
&lt;li>To put it another way, if 10 actors do a GET-modify-PUT cycle at the same
time, 9 will get an optimistic concurrency failure and have to retry, then
8, etc, for up to 50 total GET-PUT attempts in the worst case (that’s .5N^2
GET and PUT calls for N actors making simultaneous changes). If the actors
are using SSA instead, and the changes don’t actually conflict over specific
fields, then all the changes can go in in any order. Additionally, SSA
changes can often be done without a GET call at all. That’s only N &lt;strong>apply&lt;/strong>
requests for N actors, which is a drastic improvement!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-can-i-use-ssa">How can I use SSA?&lt;/h2>
&lt;h3 id="users">Users&lt;/h3>
&lt;p>Use &lt;code>kubectl apply --server-side&lt;/code>! Soon we (SIG API Machinery) hope to make this
the default and remove the “client side” apply completely!&lt;/p>
&lt;h3 id="controller-authors">Controller authors&lt;/h3>
&lt;p>There’s two main categories here, but for both of them, &lt;strong>you should probably
&lt;em>force conflicts&lt;/em> when using SSA&lt;/strong>. This is because your controller probably
doesn’t know what to do when some other entity in the system has a different
desire than your controller about a particular field. (See the &lt;a href="#ci-cd-systems">CI/CD
section&lt;/a>, though!)&lt;/p>
&lt;h4 id="get-modify-put-patch-controllers">Controllers that use either a GET-modify-PUT sequence or a PATCH&lt;/h4>
&lt;p>This kind of controller GETs an object (possibly from a
&lt;a href="https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes">&lt;strong>watch&lt;/strong>&lt;/a>),
modifies it, and then PUTs it back to write its changes. Sometimes it constructs
a custom PATCH, but the semantics are the same. Most existing controllers
(especially those in-tree) work like this.&lt;/p>
&lt;p>If your controller is perfect, great! You don’t need to change it. But if you do
want to change it, you can take advantage of the new client library’s &lt;em>extract&lt;/em>
workflow– that is, &lt;strong>get&lt;/strong> the existing object, extract your existing desires,
make modifications, and re-&lt;strong>apply&lt;/strong>. For many controllers that were computing
the smallest API changes possible, this will be a minor update to the existing
implementation.&lt;/p>
&lt;p>This workflow avoids the failure mode of accidentally trying to own every field
in the object, which is what happens if you just GET the object, make changes,
and then &lt;strong>apply&lt;/strong>. (Note that the server will notice you did this and reject
your change!)&lt;/p>
&lt;h4 id="reconstructive-controllers">Reconstructive controllers&lt;/h4>
&lt;p>This kind of controller wasn't really possible prior to SSA. The idea here is to
(whenever something changes etc) reconstruct from scratch the fields of the
object as the controller wishes them to be, and then &lt;strong>apply&lt;/strong> the change to the
server, letting it figure out the result. I now recommend that new controllers
start out this way–it's less fiddly to say what you want an object to look like
than it is to say how you want it to change.&lt;/p>
&lt;p>The client library supports this method of operation by default.&lt;/p>
&lt;p>The only downside is that you may end up sending unneeded &lt;strong>apply&lt;/strong> requests to
the API server, even if actually the object already matches your controller’s
desires. This doesn't matter if it happens once in a while, but for extremely
high-throughput controllers, it might cause a performance problem for the
cluster–specifically, the API server. No-op writes are not written to storage
(etcd) or broadcast to any watchers, so it’s not really that big of a deal. If
you’re worried about this anyway, today you could use the method explained in
the previous section, or you could still do it this way for now, and wait for an
additional client-side mechanism to suppress zero-change applies.&lt;/p>
&lt;p>To get around this downside, why not GET the object and only send your &lt;strong>apply&lt;/strong>
if the object needs it? Surprisingly, it doesn't help much – a no-op &lt;strong>apply&lt;/strong> is
not very much more work for the API server than an extra GET; and an &lt;strong>apply&lt;/strong>
that changes things is cheaper than that same &lt;strong>apply&lt;/strong> with a preceding GET.
Worse, since it is a distributed system, something could change between your GET
and &lt;strong>apply&lt;/strong>, invalidating your computation. Instead, you can use this
optimization on an object retrieved from a cache–then it legitimately will
reduce load on the system (at the cost of a delay when a change is needed and
the cache is a bit behind).&lt;/p>
&lt;h4 id="ci-cd-systems">CI/CD systems&lt;/h4>
&lt;p>Continuous integration (CI) and/or continuous deployment (CD) systems are a
special kind of controller which is doing something like reading manifests from
source control (such as a Git repo) and automatically pushing them into the
cluster. Perhaps the CI / CD process first generates manifests from a template,
then runs some tests, and then deploys a change. Typically, users are the
entities pushing changes into source control, although that’s not necessarily
always the case.&lt;/p>
&lt;p>Some systems like this continuously reconcile with the cluster, others may only
operate when a change is pushed to the source control system. The following
considerations are important for both, but more so for the continuously
reconciling kind.&lt;/p>
&lt;p>CI/CD systems are literally controllers, but for the purpose of &lt;strong>apply&lt;/strong>, they
are more like users, and unlike other controllers, they need to pay attention to
conflicts. Reasoning:&lt;/p>
&lt;ul>
&lt;li>Abstractly, CI/CD systems can change anything, which means they could conflict
with &lt;strong>any&lt;/strong> controller out there. The recommendation that controllers force
conflicts is assuming that controllers change a limited number of things and
you can be reasonably sure that they won’t fight with other controllers about
those things; that’s clearly not the case for CI/CD controllers.&lt;/li>
&lt;li>Concrete example: imagine the CI/CD system wants &lt;code>.spec.replicas&lt;/code> for some
Deployment to be 3, because that is the value that is checked into source
code; however there is also a HorizontalPodAutoscaler (HPA) that targets the
same deployment. The HPA computes a target scale and decides that there should
be 10 replicas. Which should win? I just said that most controllers–including
the HPA–should ignore conflicts. The HPA has no idea if it has been enabled
incorrectly, and the HPA has no convenient way of informing users of errors.&lt;/li>
&lt;li>The other common cause of a CI/CD system getting a conflict is probably when
it is trying to overwrite a hot-fix (hand-rolled patch) placed there by a
system admin / SRE / dev-on-call. You almost certainly don’t want to override
that automatically.&lt;/li>
&lt;li>Of course, sometimes SRE makes an accidental change, or a dev makes an
unauthorized change – those you do want to notice and overwrite; however, the
CI/CD system can’t tell the difference between these last two cases.&lt;/li>
&lt;/ul>
&lt;p>Hopefully this convinces you that CI/CD systems need error paths–a way to
back-propagate these conflict errors to humans; in fact, they should have this
already, certainly continuous integration systems need some way to report that
tests are failing. But maybe I can also say something about how &lt;em>humans&lt;/em> can
deal with errors:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Reject the hotfix: the (human) administrator of the CI/CD system observes the
error, and manually force-applies the manifest in question. Then the CI/CD
system will be able to apply the manifest successfully and become a co-owner.&lt;/p>
&lt;p>Optional: then the administrator applies a blank manifest (just the object
type / namespace / name) to relinquish any fields they became a manager for.
if this step is omitted, there's some chance the administrator will end up
owning fields and causing an unwanted future conflict.&lt;/p>
&lt;p>&lt;strong>Note&lt;/strong>: why an administrator? I'm assuming that developers which ordinarily
push to the CI/CD system and / or its source control system may not have
permissions to push directly to the cluster.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Accept the hotfix: the author of the change in question sees the conflict, and
edits their change to accept the value running in production.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Accept then reject: as in the accept option, but after that manifest is
applied, and the CI/CD queue owns everything again (so no conflicts), re-apply
the original manifest.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>I can also imagine the CI/CD system permitting you to mark a manifest as
“force conflicts” somehow– if there’s demand for this we could consider making
a more standardized way to do this. A rigorous version of this which lets you
declare exactly which conflicts you intend to force would require support from
the API server; in lieu of that, you can make a second manifest with only that
subset of fields.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Future work: we could imagine an especially advanced CI/CD system that could
parse &lt;code>metadata.managedFields&lt;/code> data to see who or what they are conflicting
with, over what fields, and decide whether or not to ignore the conflict. In
fact, this information is also presented in any conflict errors, though
perhaps not in an easily machine-parseable format. We (SIG API Machinery)
mostly didn't expect that people would want to take this approach — so we
would love to know if in fact people want/need the features implied by this
approach, such as the ability, when &lt;strong>apply&lt;/strong>ing to request to override
certain conflicts but not others.&lt;/p>
&lt;p>If this sounds like an approach you'd want to take for your own controller,
come talk to SIG API Machinery!&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Happy &lt;strong>apply&lt;/strong>ing!&lt;/p></description></item><item><title>Blog: Current State: 2019 Third Party Security Audit of Kubernetes</title><link>https://kubernetes.io/blog/2022/10/05/current-state-2019-third-party-audit/</link><pubDate>Wed, 05 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/05/current-state-2019-third-party-audit/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong> (in alphabetical order): Cailyn Edwards (Shopify), Pushkar Joglekar (VMware), Rey Lejano (SUSE) and Rory McCune (DataDog)&lt;/p>
&lt;p>We expect the brand new Third Party Security Audit of Kubernetes will be
published later this month (Oct 2022).&lt;/p>
&lt;p>In preparation for that, let's look at the state of findings that were made
public as part of the last &lt;a href="https://github.com/kubernetes/sig-security/tree/main/sig-security-external-audit/security-audit-2019">third party security audit of
2019&lt;/a>
that was based on &lt;a href="https://github.com/kubernetes/kubernetes/tree/release-1.13">Kubernetes v1.13.4&lt;/a>.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>&lt;a href="https://github.com/cji">Craig Ingram&lt;/a> has graciously attempted over the years to keep track of the
status of the findings reported in the last audit in this issue:
&lt;a href="https://github.com/kubernetes/kubernetes/issues/81146">kubernetes/kubernetes#81146&lt;/a>.
This blog post will attempt to dive deeper into this, address any gaps
in tracking and become a point in time summary of the state of the
findings reported from 2019.&lt;/p>
&lt;p>This article should also help readers gain confidence through transparent
communication, of work done by the community to address these findings and
bubble up any findings that need help from community contributors.&lt;/p>
&lt;h2 id="current-state">Current State&lt;/h2>
&lt;p>The status of each issue / finding here is represented in a best effort manner.
Authors do not claim to be 100% accurate on the status and welcome any
corrections or feedback if the current state is not reflected accurately by
commenting directly on the relevant issue.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>&lt;strong>#&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Title&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Issue&lt;/strong>&lt;/th>
&lt;th>&lt;strong>Status&lt;/strong>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>1&lt;/td>
&lt;td>hostPath PersistentVolumes enable PodSecurityPolicy bypass&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81110">#81110&lt;/a>&lt;/td>
&lt;td>closed, addressed by &lt;a href="https://github.com/kubernetes/website/pull/15756">kubernetes/website#15756&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/pull/109798">kubernetes/kubernetes#109798&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>2&lt;/td>
&lt;td>Kubernetes does not facilitate certificate revocation&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81111">#81111&lt;/a>&lt;/td>
&lt;td>duplicate of &lt;a href="https://github.com/kubernetes/kubernetes/issues/18982">#18982&lt;/a> and &lt;strong>needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>3&lt;/td>
&lt;td>HTTPS connections are not authenticated&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81112">#81112&lt;/a>&lt;/td>
&lt;td>Largely left as an end user exercise in setting up the right configuration&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4&lt;/td>
&lt;td>&lt;abbr title="Time-of-check to time-of-use bug">TOCTOU&lt;/abbr> when moving PID to manager's cgroup via kubelet&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81113">#81113&lt;/a>&lt;/td>
&lt;td>Requires Node access for successful exploitation. Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>5&lt;/td>
&lt;td>Improperly patched directory traversal in &lt;code>kubectl cp&lt;/code>&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/pull/76788">#76788&lt;/a>&lt;/td>
&lt;td>closed, assigned &lt;a href="https://github.com/advisories/GHSA-v8c4-hw4j-x4pr">CVE-2019-11249&lt;/a>, fixed in &lt;a href="https://github.com/kubernetes/kubernetes/pull/80436">#80436&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>6&lt;/td>
&lt;td>Bearer tokens are revealed in logs&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81114">#81114&lt;/a>&lt;/td>
&lt;td>closed, assigned &lt;a href="https://github.com/advisories/GHSA-jmrx-5g74-6v2f">CVE-2019-11250&lt;/a>, fixed in &lt;a href="https://github.com/kubernetes/kubernetes/pull/81330">#81330&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>7&lt;/td>
&lt;td>Seccomp is disabled by default&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81115">#81115&lt;/a>&lt;/td>
&lt;td>closed, addressed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/101943">#101943&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>8&lt;/td>
&lt;td>Pervasive world-accessible file permissions&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81116">#81116&lt;/a>&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/pull/112384">#112384&lt;/a> ( in progress)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>9&lt;/td>
&lt;td>Environment variables expose sensitive data&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81117">#81117&lt;/a>&lt;/td>
&lt;td>closed, addressed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/84992">#84992&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/pull/84677">#84677&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>10&lt;/td>
&lt;td>Use of InsecureIgnoreHostKey in SSH connections&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81118">#81118&lt;/a>&lt;/td>
&lt;td>This feature was removed in v1.22: &lt;a href="https://github.com/kubernetes/kubernetes/pull/102297">#102297&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>11&lt;/td>
&lt;td>Use of InsecureSkipVerify and other TLS weaknesses&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81119">#81119&lt;/a>&lt;/td>
&lt;td>&lt;strong>Needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12&lt;/td>
&lt;td>&lt;code>kubeadm&lt;/code> performs potentially-dangerous reset operations&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81120">#81120&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81495">#81495&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/pull/81494">#81494&lt;/a>, and &lt;a href="https://github.com/kubernetes/website/pull/15881">kubernetes/website#15881&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>13&lt;/td>
&lt;td>Overflows when using strconv.Atoi and downcasting the result&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81121">#81121&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/89120">#89120&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>14&lt;/td>
&lt;td>kubelet can cause an Out of Memory error with a malicious manifest&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81122">#81122&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/76518">#76518&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>15&lt;/td>
&lt;td>&lt;code>kubectl&lt;/code> can cause an Out Of Memory error with a malicious Pod specification&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81123">#81123&lt;/a>&lt;/td>
&lt;td>Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>16&lt;/td>
&lt;td>Improper fetching of PIDs allows incorrect cgroup movement&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81124">#81124&lt;/a>&lt;/td>
&lt;td>Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>17&lt;/td>
&lt;td>Directory traversal of host logs running kube-apiserver and kubelet&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81125">#81125&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/87273">#87273&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>18&lt;/td>
&lt;td>Non-constant time password comparison&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81126">#81126&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81152">#81152&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>19&lt;/td>
&lt;td>Encryption recommendations not in accordance with best practices&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81127">#81127&lt;/a>&lt;/td>
&lt;td>Work in Progress&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>20&lt;/td>
&lt;td>Adding credentials to containers by default is unsafe&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81128">#81128&lt;/a>&lt;/td>
&lt;td>Closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/89193">#89193&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>21&lt;/td>
&lt;td>kubelet liveness probes can be used to enumerate host network&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81129">#81129&lt;/a>&lt;/td>
&lt;td>&lt;strong>Needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>22&lt;/td>
&lt;td>iSCSI volume storage cleartext secrets in logs&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81130">#81130&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81215">#81215&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>23&lt;/td>
&lt;td>Hard coded credential paths&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81131">#81131&lt;/a>&lt;/td>
&lt;td>closed, awaiting more evidence&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>24&lt;/td>
&lt;td>Log rotation is not atomic&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81132">#81132&lt;/a>&lt;/td>
&lt;td>Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>25&lt;/td>
&lt;td>Arbitrary file paths without bounding&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81133">#81133&lt;/a>&lt;/td>
&lt;td>Fix needed.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>26&lt;/td>
&lt;td>Unsafe JSON construction&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81134">#81134&lt;/a>&lt;/td>
&lt;td>Partially fixed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>27&lt;/td>
&lt;td>kubelet crash due to improperly handled errors&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81135">#81135&lt;/a>&lt;/td>
&lt;td>Closed. Fixed by &lt;a href="https://github.com/kubernetes/kubernetes/issues/81135">#81135&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>28&lt;/td>
&lt;td>Legacy tokens do not expire&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81136">#81136&lt;/a>&lt;/td>
&lt;td>closed, fixed as part of &lt;a href="https://github.com/kubernetes/kubernetes/issues/70679">#70679&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>29&lt;/td>
&lt;td>CoreDNS leaks internal cluster information across namespaces&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81137">#81137&lt;/a>&lt;/td>
&lt;td>Closed, resolved with CoreDNS v1.6.2. &lt;a href="https://github.com/kubernetes/kubernetes/issues/81137">#81137&lt;/a> (comment)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>30&lt;/td>
&lt;td>Services use questionable default functions&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81138">#81138&lt;/a>&lt;/td>
&lt;td>Fix needed&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>31&lt;/td>
&lt;td>Incorrect docker daemon process name in container manager&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81139">#81139&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81083">#81083&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>32&lt;/td>
&lt;td>Use standard formats everywhere&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81140">#81140&lt;/a>&lt;/td>
&lt;td>&lt;strong>Needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>33&lt;/td>
&lt;td>Superficial health check provides false sense of safety&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81141">#81141&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81319">#81319&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>34&lt;/td>
&lt;td>Hardcoded use of insecure gRPC transport&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81142">#81142&lt;/a>&lt;/td>
&lt;td>&lt;strong>Needs a KEP&lt;/strong>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>35&lt;/td>
&lt;td>Incorrect handling of &lt;code>Retry-After&lt;/code>&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81143">#81143&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/91048">#91048&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>36&lt;/td>
&lt;td>Incorrect isKernelPid check&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81144">#81144&lt;/a>&lt;/td>
&lt;td>closed, fixed by &lt;a href="https://github.com/kubernetes/kubernetes/pull/81086">#81086&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>37&lt;/td>
&lt;td>Kubelet supports insecure TLS ciphersuites&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81145">#81145&lt;/a>&lt;/td>
&lt;td>closed but fix needed for &lt;a href="https://github.com/kubernetes/kubernetes/issues/91444">#91444&lt;/a> (see &lt;a href="https://github.com/kubernetes/kubernetes/issues/81145#issuecomment-630291221">this comment&lt;/a>)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="inspired-outcomes">Inspired outcomes&lt;/h3>
&lt;p>Apart from fixes to the specific issues, the 2019 third party security audit
also motivated security focussed enhancements in the next few releases of
Kubernetes. One such example is
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-security/1933-secret-logging-static-analysis">Kubernetes Enhancement Proposal (KEP) 1933 Defend Against Logging Secrets via Static Analysis&lt;/a> to prevent exposing
secrets to logs with &lt;a href="@PurelyApplied">Patrick Rhomberg&lt;/a> driving the
implementation. As a result of this KEP,
&lt;a href="https://github.com/google/go-flow-levee">&lt;code>go-flow-levee&lt;/code>&lt;/a>, a taint propagation
analysis tool configured to detect logging of secrets, is executed in a
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/hack/verify-govet-levee.sh">script&lt;/a>
as a Prow presubmit job. This KEP was introduced in v1.20.0 as an alpha
feature, then graduated to beta in v1.21.0, and graduated to stable in
v1.23.0. As stable, the analysis runs as a blocking presubmit test. This
KEP also helped resolve the following issues from the 2019 third party security audit:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81114">#81114 Bearer tokens are revealed in logs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81117">#81117 Environment variables expose sensitive data&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81130">#81130 iSCSI volume storage cleartext secrets in logs&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="remaining-work">Remaining Work&lt;/h2>
&lt;p>Many of the 37 findings identified were fixed by work from
our community members over the last 3 years. However, we still have some work
left to do. Here's a breakdown of remaining work with rough estimates on
time commitment, complexity and benefits to the ecosystem on fixing
these pending issues.&lt;/p>
&lt;div class="alert alert-info note callout" role="alert">
&lt;strong>Note:&lt;/strong> Anything requiring a KEP (Kubernetes Enhancement Proposal) is considered
&lt;em>high&lt;/em> time commitment and &lt;em>high&lt;/em> complexity. Benefits to Ecosystem are
roughly equivalent to risk of keeping the finding unfixed which is
determined by Severity Level + Likelihood of a successful vulnerability
exploit. These estimates and values in the table below are the authors'
personal opinion. An individual or end users' threat model may rate the
benefits to fix a particular issue higher or lower.
&lt;/div>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Title&lt;/th>
&lt;th>Issue&lt;/th>
&lt;th>Time Commitment&lt;/th>
&lt;th>Complexity&lt;/th>
&lt;th>Benefit to Ecosystem&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Kubernetes does not facilitate certificate revocation&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81111">#81111&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Use of InsecureSkipVerify and other TLS weaknesses&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81119">#81119&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>kubectl&lt;/code> can cause a local Out Of Memory error with a malicious Pod specification&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81123">#81123&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Improper fetching of PIDs allows incorrect cgroup movement&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81124">#81124&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>kubelet liveness probes can be used to enumerate host network&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81129">#81129&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>API Server supports insecure TLS ciphersuites&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81145">#81145&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;abbr title="Time-of-check to time-of-use bug">TOCTOU&lt;/abbr> when moving PID to manager's cgroup via kubelet&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81113">#81113&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Log rotation is not atomic&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81132">#81132&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Arbitrary file paths without bounding&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81133">#81133&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Services use questionable default functions&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81138">#81138&lt;/a>&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Medium&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Use standard formats everywhere&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81140">#81140&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Very Low&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Hardcoded use of insecure gRPC transport&lt;/td>
&lt;td>&lt;a href="https://github.com/kubernetes/kubernetes/issues/81142">#81142&lt;/a>&lt;/td>
&lt;td>High&lt;/td>
&lt;td>High&lt;/td>
&lt;td>Very Low&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>To get started on fixing any of these findings that need help, please
consider getting involved in &lt;a href="https://github.com/kubernetes/community/tree/master/sig-security#contact">Kubernetes SIG
Security&lt;/a>
by joining our bi-weekly meetings or hanging out with us on our Slack
Channel.&lt;/p></description></item><item><title>Blog: Introducing Kueue</title><link>https://kubernetes.io/blog/2022/10/04/introducing-kueue/</link><pubDate>Tue, 04 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/04/introducing-kueue/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Abdullah Gharaibeh (Google), Aldo Culquicondor (Google)&lt;/p>
&lt;p>Whether on-premises or in the cloud, clusters face real constraints for resource usage, quota, and cost management reasons. Regardless of the autoscalling capabilities, clusters have finite capacity. As a result, users want an easy way to fairly and
efficiently share resources.&lt;/p>
&lt;p>In this article, we introduce &lt;a href="https://github.com/kubernetes-sigs/kueue/tree/main/docs#readme">Kueue&lt;/a>,
an open source job queueing controller designed to manage batch jobs as a single unit.
Kueue leaves pod-level orchestration to existing stable components of Kubernetes.
Kueue natively supports the Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/">Job&lt;/a>
API and offers hooks for integrating other custom-built APIs for batch jobs.&lt;/p>
&lt;h2 id="why-kueue">Why Kueue?&lt;/h2>
&lt;p>Job queueing is a key feature to run batch workloads at scale in both on-premises and cloud environments. The main goal
of job queueing is to manage access to a limited pool of resources shared by multiple tenants. Job queueing decides which
jobs should wait, which can start immediately, and what resources they can use.&lt;/p>
&lt;p>Some of the most desired job queueing requirements include:&lt;/p>
&lt;ul>
&lt;li>Quota and budgeting to control who can use what and up to what limit. This is not only needed in clusters with static resources like on-premises,
but it is also needed in cloud environments to control spend or usage of scarce resources.&lt;/li>
&lt;li>Fair sharing of resources between tenants. To maximize the usage of available resources, any unused quota assigned to inactive tenants should be
allowed to be shared fairly between active tenants.&lt;/li>
&lt;li>Flexible placement of jobs across different resource types based on availability. This is important in cloud environments which have heterogeneous
resources such as different architectures (GPU or CPU models) and different provisioning modes (spot vs on-demand).&lt;/li>
&lt;li>Support for autoscaled environments where resources can be provisioned on demand.&lt;/li>
&lt;/ul>
&lt;p>Plain Kubernetes doesn't address the above requirements. In normal circumstances, once a Job is created, the job-controller instantly creates the
pods and kube-scheduler continuously attempts to assign the pods to nodes. At scale, this situation can work the control plane to death. There is
also currently no good way to control at the job level which jobs should get which resources first, and no way to express order or fair sharing. The
current ResourceQuota model is not a good fit for these needs because quotas are enforced on resource creation, and there is no queueing of requests. The
intent of ResourceQuotas is to provide a builtin reliability mechanism with policies needed by admins to protect clusters from failing over.&lt;/p>
&lt;p>In the Kubernetes ecosystem, there are several solutions for job scheduling. However, we found that these alternatives have one or more of the following problems:&lt;/p>
&lt;ul>
&lt;li>They replace existing stable components of Kubernetes, like kube-scheduler or the job-controller. This is problematic not only from an operational point of view, but
also the duplication in the job APIs causes fragmentation of the ecosystem and reduces portability.&lt;/li>
&lt;li>They don't integrate with autoscaling, or&lt;/li>
&lt;li>They lack support for resource flexibility.&lt;/li>
&lt;/ul>
&lt;h2 id="overview">How Kueue works&lt;/h2>
&lt;p>With Kueue we decided to take a different approach to job queueing on Kubernetes that is anchored around the following aspects:&lt;/p>
&lt;ul>
&lt;li>Not duplicating existing functionalities already offered by established Kubernetes components for pod scheduling, autoscaling and job
lifecycle management.&lt;/li>
&lt;li>Adding key features that are missing to existing components. For example, we invested in the Job API to cover more use cases like
&lt;a href="https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs">IndexedJob&lt;/a> and &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-tracking-with-finalizers">fixed long standing issues related to pod
tracking&lt;/a>. While this path takes longer to
land features, we believe it is the more sustainable long term solution.&lt;/li>
&lt;li>Ensuring compatibility with cloud environments where compute resources are elastic and heterogeneous.&lt;/li>
&lt;/ul>
&lt;p>For this approach to be feasible, Kueue needs knobs to influence the behavior of those established components so it can effectively manage
when and where to start a job. We added those knobs to the Job API in the form of two features:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#suspending-a-job">Suspend field&lt;/a>, which allows Kueue to signal to the job-controller
when to start or stop a Job.&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/#mutable-scheduling-directives">Mutable scheduling directives&lt;/a>, which allows Kueue to
update a Job's &lt;code>.spec.template.spec.nodeSelector&lt;/code> before starting the Job. This way, Kueue can control Pod placement while still
delegating to kube-scheduler the actual pod-to-node scheduling.&lt;/li>
&lt;/ul>
&lt;p>Note that any custom job API can be managed by Kueue if that API offers the above two capabilities.&lt;/p>
&lt;h3 id="resource-model">Resource model&lt;/h3>
&lt;p>Kueue defines new APIs to address the requirements mentioned at the beginning of this post. The three main APIs are:&lt;/p>
&lt;ul>
&lt;li>ResourceFlavor: a cluster-scoped API to define resource flavor available for consumption, like a GPU model. At its core, a ResourceFlavor is
a set of labels that mirrors the labels on the nodes that offer those resources.&lt;/li>
&lt;li>ClusterQueue: a cluster-scoped API to define resource pools by setting quotas for one or more ResourceFlavor.&lt;/li>
&lt;li>LocalQueue: a namespaced API for grouping and managing single tenant jobs. In its simplest form, a LocalQueue is a pointer to the ClusterQueue
that the tenant (modeled as a namespace) can use to start their jobs.&lt;/li>
&lt;/ul>
&lt;p>For more details, take a look at the &lt;a href="https://sigs.k8s.io/kueue/docs/concepts">API concepts documentation&lt;/a>. While the three APIs may look overwhelming,
most of Kueue’s operations are centered around ClusterQueue; the ResourceFlavor and LocalQueue APIs are mainly organizational wrappers.&lt;/p>
&lt;h3 id="example-use-case">Example use case&lt;/h3>
&lt;p>Imagine the following setup for running batch workloads on a Kubernetes cluster on the cloud:&lt;/p>
&lt;ul>
&lt;li>You have &lt;a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">cluster-autoscaler&lt;/a> installed in the cluster to automatically
adjust the size of your cluster.&lt;/li>
&lt;li>There are two types of autoscaled node groups that differ on their provisioning policies: spot and on-demand. The nodes of each group are
differentiated by the label &lt;code>instance-type=spot&lt;/code> or &lt;code>instance-type=ondemand&lt;/code>.
Moreover, since not all Jobs can tolerate running on spot nodes, the nodes are tainted with &lt;code>spot=true:NoSchedule&lt;/code>.&lt;/li>
&lt;li>To strike a balance between cost and resource availability, imagine you want Jobs to use up to 1000 cores of on-demand nodes, then use up to
2000 cores of spot nodes.&lt;/li>
&lt;/ul>
&lt;p>As an admin for the batch system, you define two ResourceFlavors that represent the two types of nodes:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kueue.x-k8s.io/v1alpha2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ResourceFlavor&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ondemand&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">instance-type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ondemand &lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kueue.x-k8s.io/v1alpha2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ResourceFlavor&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">instance-type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">taints&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>- &lt;span style="color:#008000;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NoSchedule&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then you define the quotas by creating a ClusterQueue as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kueue.x-k8s.io/v1alpha2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterQueue&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>research-pool&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{}&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;cpu&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">flavors&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ondemand&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">quota&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">min&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">quota&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">min&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">2000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Note that the order of flavors in the ClusterQueue resources matters: Kueue will attempt to fit jobs in the available quotas according to
the order unless the job has an explicit affinity to specific flavors.&lt;/p>
&lt;p>For each namespace, you define a LocalQueue that points to the ClusterQueue above:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kueue.x-k8s.io/v1alpha2&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>LocalQueue&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>training&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>team-ml&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">clusterQueue&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>research-pool&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Admins create the above setup once. Batch users are able to find the queues they are allowed to
submit to by listing the LocalQueues in their namespace(s). The command is similar to the following: &lt;code>kubectl get -n my-namespace localqueues&lt;/code>&lt;/p>
&lt;p>To submit work, create a Job and set the &lt;code>kueue.x-k8s.io/queue-name&lt;/code> annotation as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">generateName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>sample-job-&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kueue.x-k8s.io/queue-name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>training&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">parallelism&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">completions&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">tolerations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spot&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Exists&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;NoSchedule&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-batch-workload&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>registry.example/batch/calculate-pi:3.14&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;30s&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Kueue intervenes to suspend the Job as soon as it is created. Once the Job is at the head of the ClusterQueue, Kueue evaluates if it can start
by checking if the resources requested by the job fit the available quota.&lt;/p>
&lt;p>In the above example, the Job tolerates spot resources. If there are previously admitted Jobs consuming all existing on-demand quota but
not all of spot’s, Kueue admits the Job using the spot quota. Kueue does this by issuing a single update to the Job object that:&lt;/p>
&lt;ul>
&lt;li>Changes the &lt;code>.spec.suspend&lt;/code> flag to false&lt;/li>
&lt;li>Adds the term &lt;code>instance-type: spot&lt;/code> to the job's &lt;code>.spec.template.spec.nodeSelector&lt;/code> so that when the pods are created by the job controller, those pods can only schedule
onto spot nodes.&lt;/li>
&lt;/ul>
&lt;p>Finally, if there are available empty nodes with matching node selector terms, then kube-scheduler will directly schedule the pods. If not, then
kube-scheduler will initially mark the pods as unschedulable, which will trigger the cluster-autoscaler to provision new nodes.&lt;/p>
&lt;h2 id="future-work-and-getting-involved">Future work and getting involved&lt;/h2>
&lt;p>The example above offers a glimpse of some of Kueue's features including support for quota, resource flexibility, and integration with cluster
autoscaler. Kueue also supports fair-sharing, job priorities, and different queueing strategies. Take a look at the
&lt;a href="https://github.com/kubernetes-sigs/kueue/tree/main/docs">Kueue documentation&lt;/a> to learn more about those features and how to use Kueue.&lt;/p>
&lt;p>We have a number of features that we plan to add to Kueue, such as hierarchical quota, budgets, and support for dynamically sized jobs. In
the more immediate future, we are focused on adding support for job preemption.&lt;/p>
&lt;p>The latest &lt;a href="https://github.com/kubernetes-sigs/kueue/releases">Kueue release&lt;/a> is available on Github;
try it out if you run batch workloads on Kubernetes (requires v1.22 or newer).
We are in the early stages of this project and we are seeking feedback of all levels, major or minor, so please don’t hesitate to reach out. We’re
also open to additional contributors, whether it is to fix or report bugs, or help add new features or write documentation. You can get in touch with
us via our &lt;a href="http://sigs.k8s.io/kueue">repo&lt;/a>, &lt;a href="https://groups.google.com/a/kubernetes.io/g/wg-batch">mailing list&lt;/a> or on
&lt;a href="https://kubernetes.slack.com/messages/wg-batch">Slack&lt;/a>.&lt;/p>
&lt;p>Last but not least, thanks to all &lt;a href="https://github.com/kubernetes-sigs/kueue/graphs/contributors">our contributors&lt;/a> who made this project possible!&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: alpha support for running Pods with user namespaces</title><link>https://kubernetes.io/blog/2022/10/03/userns-alpha/</link><pubDate>Mon, 03 Oct 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/10/03/userns-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Rodrigo Campos (Microsoft), Giuseppe Scrivano (Red Hat)&lt;/p>
&lt;p>Kubernetes v1.25 introduces the support for user namespaces.&lt;/p>
&lt;p>This is a major improvement for running secure workloads in
Kubernetes. Each pod will have access only to a limited subset of the
available UIDs and GIDs on the system, thus adding a new security
layer to protect from other pods running on the same system.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>A process running on Linux can use up to 4294967296 different UIDs and
GIDs.&lt;/p>
&lt;p>User namespaces is a Linux feature that allows mapping a set of users
in the container to different users in the host, thus restricting what
IDs a process can effectively use.
Furthermore, the capabilities granted in a new user namespace do not
apply in the host initial namespaces.&lt;/p>
&lt;h2 id="why-is-it-important">Why is it important?&lt;/h2>
&lt;p>There are mainly two reasons why user namespaces are important:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>improve security since they restrict the IDs a pod can use, so each
pod can run in its own separate environment with unique IDs.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>enable running workloads as root in a safer manner.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In a user namespace we can map the root user inside the pod to a
non-zero ID outside the container, containers believe in running as
root while they are a regular unprivileged ID from the host point of
view.&lt;/p>
&lt;p>The process can keep capabilities that are usually restricted to
privileged pods and do it in a safe way since the capabilities granted
in a new user namespace do not apply in the host initial namespaces.&lt;/p>
&lt;h2 id="how-do-i-enable-user-namespaces">How do I enable user namespaces?&lt;/h2>
&lt;p>At the moment, user namespaces support is opt-in, so you must enable
it for a pod setting &lt;code>hostUsers&lt;/code> to &lt;code>false&lt;/code> under the pod spec stanza:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Pod
spec:
hostUsers: false
containers:
- name: nginx
image: docker.io/nginx
&lt;/code>&lt;/pre>&lt;p>The feature is behind a feature gate, so make sure to enable
the &lt;code>UserNamespacesStatelessPodsSupport&lt;/code> gate before you can use
the new feature.&lt;/p>
&lt;p>The runtime must also support user namespaces:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>containerd: support is planned for the 1.7 release. See containerd
issue &lt;a href="https://github.com/containerd/containerd/issues/7063">#7063&lt;/a> for more details.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CRI-O: v1.25 has support for user namespaces.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Support for this in &lt;code>cri-dockerd&lt;/code> is &lt;a href="https://github.com/Mirantis/cri-dockerd/issues/74">not planned&lt;/a> yet.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>You can reach SIG Node by several means:&lt;/p>
&lt;ul>
&lt;li>Slack: &lt;a href="https://kubernetes.slack.com/messages/sig-node">#sig-node&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-node">Mailing list&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/community/labels/sig%2Fnode">Open Community Issues/PRs&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>You can also contact us directly:&lt;/p>
&lt;ul>
&lt;li>GitHub / Slack: @rata @giuseppe&lt;/li>
&lt;/ul></description></item><item><title>Blog: Enforce CRD Immutability with CEL Transition Rules</title><link>https://kubernetes.io/blog/2022/09/29/enforce-immutability-using-cel/</link><pubDate>Thu, 29 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/29/enforce-immutability-using-cel/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://github.com/alexzielenski">Alexander Zielenski&lt;/a> (Google)&lt;/p>
&lt;p>Immutable fields can be found in a few places in the built-in Kubernetes types.
For example, you can't change the &lt;code>.metadata.name&lt;/code> of an object. Specific objects
have fields where changes to existing objects are constrained; for example, the
&lt;code>.spec.selector&lt;/code> of a Deployment.&lt;/p>
&lt;p>Aside from simple immutability, there are other common design patterns such as
lists which are append-only, or a map with mutable values and immutable keys.&lt;/p>
&lt;p>Until recently the best way to restrict field mutability for CustomResourceDefinitions
has been to create a validating
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks">admission webhook&lt;/a>:
this means a lot of complexity for the common case of making a field immutable.&lt;/p>
&lt;p>Beta since Kubernetes 1.25, CEL Validation Rules allow CRD authors to express
validation constraints on their fields using a rich expression language,
&lt;a href="https://github.com/google/cel-spec">CEL&lt;/a>. This article explores how you can
use validation rules to implement a few common immutability patterns directly in
the manifest for a CRD.&lt;/p>
&lt;h2 id="basics-of-validation-rules">Basics of validation rules&lt;/h2>
&lt;p>The new support for CEL validation rules in Kubernetes allows CRD authors to add
complicated admission logic for their resources without writing any code!&lt;/p>
&lt;p>For example, A CEL rule to constrain a field &lt;code>maximumSize&lt;/code> to be greater than a
&lt;code>minimumSize&lt;/code> for a CRD might look like the following:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>|&lt;span style="color:#b44;font-style:italic">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44;font-style:italic"> &lt;/span>&lt;span style="color:#bbb"> &lt;/span>self.maximumSize &amp;gt; self.minimumSize&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;Maximum size must be greater than minimum size.&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The rule field contains an expression written in CEL. &lt;code>self&lt;/code> is a special keyword
in CEL which refers to the object whose type contains the rule.&lt;/p>
&lt;p>The message field is an error message which will be sent to Kubernetes clients
whenever this particular rule is not satisfied.&lt;/p>
&lt;p>For more details about the capabilities and limitations of Validation Rules using
CEL, please refer to
&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">validation rules&lt;/a>.
The &lt;a href="https://github.com/google/cel-spec">CEL specification&lt;/a> is also a good
reference for information specifically related to the language.&lt;/p>
&lt;h2 id="immutability-patterns-with-cel-validation-rules">Immutability patterns with CEL validation rules&lt;/h2>
&lt;p>This section implements several common use cases for immutability in Kubernetes
CustomResourceDefinitions, using validation rules expressed as
&lt;a href="https://book.kubebuilder.io/reference/markers/crd.html">kubebuilder marker comments&lt;/a>.
Resultant OpenAPI generated by the kubebuilder marker comments will also be
included so that if you are writing your CRD manifests by hand you can still
follow along.&lt;/p>
&lt;h2 id="project-setup">Project setup&lt;/h2>
&lt;p>To use CEL rules with kubebuilder comments, you first need to set up a Golang
project structure with the CRD defined in Go.&lt;/p>
&lt;p>You may skip this step if you are not using kubebuilder or are only interested
in the resultant OpenAPI extensions.&lt;/p>
&lt;p>Begin with a folder structure of a Go module set up like the following. If
you have your own project already set up feel free to adapt this tutorial to your liking:&lt;/p>
&lt;figure>
&lt;div class="mermaid">
graph LR
. --> generate.go
. --> pkg --> apis --> stable.example.com --> v1
v1 --> doc.go
v1 --> types.go
. --> tools.go
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">JavaScript must be &lt;a href="https://www.enable-javascript.com/">enabled&lt;/a> to view this content&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;p>This is the typical folder structure used by Kubernetes projects for defining new API resources.&lt;/p>
&lt;p>&lt;code>doc.go&lt;/code> contains package-level metadata such as the group and the version:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +groupName=stable.example.com
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +versionName=v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> v1
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>types.go&lt;/code> contains all type definitions in stable.example.com/v1&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> v1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1 &lt;span style="color:#b44">&amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// An empty CRD as an example of defining a type using controller tools
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:storageversion
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:subresource:status
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> TestCRD &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Spec TestCRDSpec &lt;span style="color:#b44">`json:&amp;#34;spec,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Status TestCRDStatus &lt;span style="color:#b44">`json:&amp;#34;status,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> TestCRDStatus &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {}
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> TestCRDSpec &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// You will fill this in as you go along
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>tools.go&lt;/code> contains a dependency on &lt;a href="https://book.kubebuilder.io/reference/generating-crd.html#generating-crds">controller-gen&lt;/a> which will be used to generate the CRD definition:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">//go:build tools
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> celimmutabilitytutorial
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// Force direct dependency on code-generator so that it may be executed with go run
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> _ &lt;span style="color:#b44">&amp;#34;sigs.k8s.io/controller-tools/cmd/controller-gen&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, &lt;code>generate.go&lt;/code>contains a &lt;code>go:generate&lt;/code> directive to make use of
&lt;code>controller-gen&lt;/code>. &lt;code>controller-gen&lt;/code> parses our &lt;code>types.go&lt;/code> and creates generates
CRD yaml files into a &lt;code>crd&lt;/code> folder:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">package&lt;/span> celimmutabilitytutorial
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">//go:generate go run sigs.k8s.io/controller-tools/cmd/controller-gen crd paths=./pkg/apis/... output:dir=./crds
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You may now want to add dependencies for our definitions and test the code generation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f">cd&lt;/span> cel-immutability-tutorial
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go mod init &amp;lt;your-org&amp;gt;/&amp;lt;your-module-name&amp;gt;
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go mod tidy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After running these commands you now have completed the basic project structure.
Your folder tree should look like the following:&lt;/p>
&lt;figure>
&lt;div class="mermaid">
graph LR
. --> crds --> stable.example.com_testcrds.yaml
. --> generate.go
. --> go.mod
. --> go.sum
. --> pkg --> apis --> stable.example.com --> v1
v1 --> doc.go
v1 --> types.go
. --> tools.go
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">JavaScript must be &lt;a href="https://www.enable-javascript.com/">enabled&lt;/a> to view this content&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;p>The manifest for the example CRD is now available in &lt;code>crds/stable.example.com_testcrds.yaml&lt;/code>.&lt;/p>
&lt;h2 id="immutablility-after-first-modification">Immutablility after first modification&lt;/h2>
&lt;p>A common immutability design pattern is to make the field immutable once it has
been first set. This example will throw a validation error if the field after
changes after being first initialized.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;!has(oldSelf.value) || has(self.value)&amp;#34;, message=&amp;#34;Value is required once set&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> ImmutableSinceFirstWrite &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:Optional
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;self == oldSelf&amp;#34;,message=&amp;#34;Value is immutable&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxLength=512
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> Value &lt;span style="color:#0b0;font-weight:bold">string&lt;/span> &lt;span style="color:#b44">`json:&amp;#34;value&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>+kubebuilder&lt;/code> directives in the comments inform controller-gen how to
annotate the generated OpenAPI. The &lt;code>XValidation&lt;/code> rule causes the rule to appear
among the &lt;code>x-kubernetes-validations&lt;/code> OpenAPI extension. Kubernetes then
respects the OpenAPI spec to enforce our constraints.&lt;/p>
&lt;p>To enforce a field's immutability after its first write, you need to apply the following constraints:&lt;/p>
&lt;ol>
&lt;li>Field must be allowed to be initially unset &lt;code>+kubebuilder:validation:Optional&lt;/code>&lt;/li>
&lt;li>Once set, field must not be allowed to be removed: &lt;code>!has(oldSelf.value) | has(self.value)&lt;/code> (type-scoped rule)&lt;/li>
&lt;li>Once set, field must not be allowed to change value &lt;code>self == oldSelf&lt;/code> (field-scoped rule)&lt;/li>
&lt;/ol>
&lt;p>Also note the additional directive &lt;code>+kubebuilder:validation:MaxLength&lt;/code>. CEL
requires that all strings have attached max length so that it may estimate the
computation cost of the rule. Rules that are too expensive will be rejected.
For more information on CEL cost budgeting, check out the other tutorial.&lt;/p>
&lt;h3 id="example-usage">Example usage&lt;/h3>
&lt;p>Generating and installing the CRD should succeed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f crds/stable.example.com_immutablesincefirstwrites.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">customresourcedefinition.apiextensions.k8s.io/immutablesincefirstwrites.stable.example.com created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Creating initial empty object with no &lt;code>value&lt;/code> is permitted since &lt;code>value&lt;/code> is &lt;code>optional&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceFirstWrite
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">immutablesincefirstwrite.stable.example.com/test1 created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The initial modification of &lt;code>value&lt;/code> succeeds:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceFirstWrite
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value: Hello, world!
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">immutablesincefirstwrite.stable.example.com/test1 configured
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>An attempt to change &lt;code>value&lt;/code> is blocked by the field-level validation rule. Note
the error message shown to the user comes from the validation rule.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceFirstWrite
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value: Hello, new world!
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceFirstWrite &amp;#34;test1&amp;#34; is invalid: value: Invalid value: &amp;#34;string&amp;#34;: Value is immutable
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>An attempt to remove the &lt;code>value&lt;/code> field altogether is blocked by the other validation rule
on the type. The error message also comes from the rule.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceFirstWrite
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceFirstWrite &amp;#34;test1&amp;#34; is invalid: &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;object&amp;#34;: Value is required once set
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generated-schema">Generated schema&lt;/h3>
&lt;p>Note that in the generated schema there are two separate rule locations.
One is directly attached to the property &lt;code>immutable_since_first_write&lt;/code>.
The other rule is associated with the crd type itself.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxLength&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">512&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is immutable&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>self == oldSelf&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is required once set&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;!has(oldSelf.value) || has(self.value)&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="immutability-upon-object-creation">Immutability upon object creation&lt;/h2>
&lt;p>A field which is immutable upon creation time is implemented similarly to the
earlier example. The difference is that that field is marked required, and the
type-scoped rule is no longer necessary.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> ImmutableSinceCreation &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:Required
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;self == oldSelf&amp;#34;,message=&amp;#34;Value is immutable&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxLength=512
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> Value &lt;span style="color:#0b0;font-weight:bold">string&lt;/span> &lt;span style="color:#b44">`json:&amp;#34;value&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This field will be required when the object is created, and after that point will
not be allowed to be modified. Our CEL Validation Rule &lt;code>self == oldSelf&lt;/code>&lt;/p>
&lt;h3 id="usage-example">Usage example&lt;/h3>
&lt;p>Generating and installing the CRD should succeed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f crds/stable.example.com_immutablesincecreations.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">customresourcedefinition.apiextensions.k8s.io/immutablesincecreations.stable.example.com created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Applying an object without the required field should fail:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceCreation
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceCreation &amp;#34;test1&amp;#34; is invalid:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">* value: Required value
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">* &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;null&amp;#34;: some validation rules were not checked because the object was invalid; correct the existing errors to complete validation
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now that the field has been added, the operation is permitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceCreation
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value: Hello, world!
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">immutablesincecreation.stable.example.com/test1 created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you attempt to change the &lt;code>value&lt;/code>, the operation is blocked due to the
validation rules in the CRD. Note that the error message is as it was defined
in the validation rule.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceCreation
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value: Hello, new world!
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceCreation &amp;#34;test1&amp;#34; is invalid: value: Invalid value: &amp;#34;string&amp;#34;: Value is immutable
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Also if you attempted to remove &lt;code>value&lt;/code> altogether after adding it, you will
see an error as expected:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: ImmutableSinceCreation
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: test1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The ImmutableSinceCreation &amp;#34;test1&amp;#34; is invalid:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">* value: Required value
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">* &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;null&amp;#34;: some validation rules were not checked because the object was invalid; correct the existing errors to complete validation
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generated-schema-1">Generated schema&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxLength&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">512&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is immutable&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>self == oldSelf&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">required&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- value&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="append-only-list-of-containers">Append-only list of containers&lt;/h2>
&lt;p>In the case of ephemeral containers on Pods, Kubernetes enforces that the
elements in the list are immutable, and can’t be removed. The following example
shows how you could use CEL to achieve the same behavior.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;!has(oldSelf.value) || has(self.value)&amp;#34;, message=&amp;#34;Value is required once set&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> AppendOnlyList &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:Optional
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxItems=100
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;oldSelf.all(x, x in self)&amp;#34;,message=&amp;#34;Values may only be added&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> Values []v1.EphemeralContainer &lt;span style="color:#b44">`json:&amp;#34;value&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>Once set, field must not be deleted: &lt;code>!has(oldSelf.value) || has(self.value)&lt;/code> (type-scoped)&lt;/li>
&lt;li>Once a value is added it is not removed: &lt;code>oldSelf.all(x, x in self)&lt;/code> (field-scoped)&lt;/li>
&lt;li>Value may be initially unset: &lt;code>+kubebuilder:validation:Optional&lt;/code>&lt;/li>
&lt;/ol>
&lt;p>Note that for cost-budgeting purposes, &lt;code>MaxItems&lt;/code> is also required to be specified.&lt;/p>
&lt;h3 id="example-usage-1">Example usage&lt;/h3>
&lt;p>Generating and installing the CRD should succeed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f crds/stable.example.com_appendonlylists.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">customresourcedefinition.apiextensions.k8s.io/appendonlylists.stable.example.com created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Creating an inital list with one element inside should succeed without problem:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: AppendOnlyList
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testlist
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> - name: container1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> image: nginx/nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">appendonlylist.stable.example.com/testlist created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Adding an element to the list should also proceed without issue as expected:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: AppendOnlyList
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testlist
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> - name: container1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> image: nginx/nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> - name: container2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> image: mongodb/mongodb
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">appendonlylist.stable.example.com/testlist configured
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But if you now attempt to remove an element, the error from the validation rule
is triggered:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: AppendOnlyList
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testlist
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">value:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> - name: container1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> image: nginx/nginx
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The AppendOnlyList &amp;#34;testlist&amp;#34; is invalid: value: Invalid value: &amp;#34;array&amp;#34;: Values may only be added
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Additionally, to attempt to remove the field once it has been set is also disallowed
by the type-scoped validation rule.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: AppendOnlyList
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testlist
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The AppendOnlyList &amp;#34;testlist&amp;#34; is invalid: &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;object&amp;#34;: Value is required once set
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generated-schema-2">Generated schema&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">items&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxItems&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">100&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>array&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Values may only be added&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>oldSelf.all(x, x in self)&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is required once set&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;!has(oldSelf.value) || has(self.value)&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="map-with-append-only-keys-immutable-values">Map with append-only keys, immutable values&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-go" data-lang="go">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// A map which does not allow keys to be removed or their values changed once set. New keys may be added, however.
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;!has(oldSelf.values) || has(self.values)&amp;#34;, message=&amp;#34;Value is required once set&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> MapAppendOnlyKeys &lt;span style="color:#a2f;font-weight:bold">struct&lt;/span> {
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.TypeMeta &lt;span style="color:#b44">`json:&amp;#34;,inline&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> metav1.ObjectMeta &lt;span style="color:#b44">`json:&amp;#34;metadata,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:Optional
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:MaxProperties=10
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#080;font-style:italic">// +kubebuilder:validation:XValidation:rule=&amp;#34;oldSelf.all(key, key in self &amp;amp;&amp;amp; self[key] == oldSelf[key])&amp;#34;,message=&amp;#34;Keys may not be removed and their values must stay the same&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic">&lt;/span> Values &lt;span style="color:#a2f;font-weight:bold">map&lt;/span>[&lt;span style="color:#0b0;font-weight:bold">string&lt;/span>]&lt;span style="color:#0b0;font-weight:bold">string&lt;/span> &lt;span style="color:#b44">`json:&amp;#34;values,omitempty&amp;#34;`&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>}
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol>
&lt;li>Once set, field must not be deleted: &lt;code>!has(oldSelf.values) || has(self.values)&lt;/code> (type-scoped)&lt;/li>
&lt;li>Once a key is added it is not removed nor is its value modified: &lt;code>oldSelf.all(key, key in self &amp;amp;&amp;amp; self[key] == oldSelf[key])&lt;/code> (field-scoped)&lt;/li>
&lt;li>Value may be initially unset: &lt;code>+kubebuilder:validation:Optional&lt;/code>&lt;/li>
&lt;/ol>
&lt;h3 id="example-usage-2">Example usage&lt;/h3>
&lt;p>Generating and installing the CRD should succeed:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#080;font-style:italic"># Ensure the CRD yaml is generated by controller-gen&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>go generate ./...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f crds/stable.example.com_mapappendonlykeys.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">customresourcedefinition.apiextensions.k8s.io/mapappendonlykeys.stable.example.com created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Creating an initial object with one key within &lt;code>values&lt;/code> should be permitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: MapAppendOnlyKeys
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testmap
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">values:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> key1: value1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">mapappendonlykeys.stable.example.com/testmap created
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Adding new keys to the map should also be permitted:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: MapAppendOnlyKeys
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testmap
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">values:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> key1: value1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> key2: value2
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">mapappendonlykeys.stable.example.com/testmap configured
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>But if a key is removed, the error messagr from the validation rule should be
returned:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: MapAppendOnlyKeys
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testmap
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">values:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> key1: value1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The MapAppendOnlyKeys &amp;#34;testmap&amp;#34; is invalid: values: Invalid value: &amp;#34;object&amp;#34;: Keys may not be removed and their values must stay the same
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the entire field is removed, the other validation rule is triggered and the
operation is prevented. Note that the error message for the validation rule is
shown to the user.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">---
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">apiVersion: stable.example.com/v1
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">kind: MapAppendOnlyKeys
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44"> name: testmap
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">The MapAppendOnlyKeys &amp;#34;testmap&amp;#34; is invalid: &amp;lt;nil&amp;gt;: Invalid value: &amp;#34;object&amp;#34;: Value is required once set
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="generated-schema-3">Generated schema&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">description&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>A map which does not allow keys to be removed or their values&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>changed once set. New keys may be added, however.&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">additionalProperties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">maxProperties&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Keys may not be removed and their values must stay the same&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>oldSelf.all(key, key in self &amp;amp;&amp;amp; self[key] == oldSelf[key])&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Value is required once set&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;!has(oldSelf.values) || has(self.values)&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="going-further">Going further&lt;/h1>
&lt;p>The above examples showed how CEL rules can be added to kubebuilder types.
The same rules can be added directly to OpenAPI if writing a manifest for a CRD by hand.&lt;/p>
&lt;p>For native types, the same behavior can be achieved using kube-openapi’s marker
&lt;a href="https://github.com/kubernetes/kube-openapi/blob/923526ac052c59656d41710b45bbcb03748aa9d6/pkg/generators/extension.go#L69">&lt;code>+validations&lt;/code>&lt;/a>.&lt;/p>
&lt;p>Usage of CEL within Kubernetes Validation Rules is so much more powerful than
what has been shown in this article. For more information please check out
&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">validation rules&lt;/a>
in the Kubernetes documentation and &lt;a href="https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/">CRD Validation Rules Beta&lt;/a> blog post.&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: Kubernetes In-Tree to CSI Volume Migration Status Update</title><link>https://kubernetes.io/blog/2022/09/26/storage-in-tree-to-csi-migration-status-update-1.25/</link><pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/26/storage-in-tree-to-csi-migration-status-update-1.25/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Jiawei Wang (Google)&lt;/p>
&lt;p>The Kubernetes in-tree storage plugin to &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface (CSI)&lt;/a> migration infrastructure has already been &lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">beta&lt;/a> since v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
Since then, SIG Storage and other Kubernetes special interest groups are working to ensure feature stability and compatibility in preparation for CSI Migration feature to go GA.&lt;/p>
&lt;p>SIG Storage is excited to announce that the core CSI Migration feature is &lt;strong>generally available&lt;/strong> in Kubernetes v1.25 release!&lt;/p>
&lt;p>SIG Storage wrote a blog post in v1.23 for &lt;a href="https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/">CSI Migration status update&lt;/a> which discussed the CSI migration status for each storage driver. It has been a while and this article is intended to give a latest status update on each storage driver for their CSI Migration status in Kubernetes v1.25.&lt;/p>
&lt;h2 id="quick-recap-what-is-csi-migration-and-why-migrate">Quick recap: What is CSI Migration, and why migrate?&lt;/h2>
&lt;p>The Container Storage Interface (CSI) was designed to help Kubernetes replace its existing, in-tree storage driver mechanisms - especially vendor specific plugins.
Kubernetes support for the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md#README">Container Storage Interface&lt;/a> has been
&lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">generally available&lt;/a> since Kubernetes v1.13.
Support for using CSI drivers was introduced to make it easier to add and maintain new integrations between Kubernetes and storage backend technologies. Using CSI drivers allows for better maintainability (driver authors can define their own release cycle and support lifecycle) and reduce the opportunity for vulnerabilities (with less in-tree code, the risks of a mistake are reduced, and cluster operators can select only the storage drivers that their cluster requires).&lt;/p>
&lt;p>As more CSI Drivers were created and became production ready, SIG Storage wanted all Kubernetes users to benefit from the CSI model. However, we could not break API compatibility with the existing storage API types due to k8s architecture conventions. The solution we came up with was CSI migration: a feature that translates in-tree APIs to equivalent CSI APIs and delegates operations to a replacement CSI driver.&lt;/p>
&lt;p>The CSI migration effort enables the replacement of existing in-tree storage plugins such as &lt;code>kubernetes.io/gce-pd&lt;/code> or &lt;code>kubernetes.io/aws-ebs&lt;/code> with a corresponding &lt;a href="https://kubernetes-csi.github.io/docs/introduction.html">CSI driver&lt;/a> from the storage backend.
If CSI Migration is working properly, Kubernetes end users shouldn’t notice a difference. Existing &lt;code>StorageClass&lt;/code>, &lt;code>PersistentVolume&lt;/code> and &lt;code>PersistentVolumeClaim&lt;/code> objects should continue to work.
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing workloads that utilize PVCs which are backed by in-tree storage plugins will continue to function as they always have.
However, behind the scenes, Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.&lt;/p>
&lt;p>For example, suppose you are a &lt;code>kubernetes.io/gce-pd&lt;/code> user; after CSI migration, you can still use &lt;code>kubernetes.io/gce-pd&lt;/code> to provision new volumes, mount existing GCE-PD volumes or delete existing volumes. All existing APIs and Interface will still function correctly. However, the underlying function calls are all going through the &lt;a href="https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver">GCE PD CSI driver&lt;/a> instead of the in-tree Kubernetes function.&lt;/p>
&lt;p>This enables a smooth transition for end users. Additionally as storage plugin developers, we can reduce the burden of maintaining the in-tree storage plugins and eventually remove them from the core Kubernetes binary.&lt;/p>
&lt;h2 id="timeline-and-status">What is the timeline / status?&lt;/h2>
&lt;p>The current and targeted releases for each individual driver is shown in the table below:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Driver&lt;/th>
&lt;th>Alpha&lt;/th>
&lt;th>Beta (in-tree deprecated)&lt;/th>
&lt;th>Beta (on-by-default)&lt;/th>
&lt;th>GA&lt;/th>
&lt;th>Target &amp;quot;in-tree plugin&amp;quot; removal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>AWS EBS&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure Disk&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Azure File&lt;/td>
&lt;td>1.15&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>1.28 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ceph FS&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Ceph RBD&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;td>1.28 (Target)&lt;/td>
&lt;td>1.30 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GCE PD&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.17&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>OpenStack Cinder&lt;/td>
&lt;td>1.14&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.21&lt;/td>
&lt;td>1.24&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Portworx&lt;/td>
&lt;td>1.23&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>1.27 (Target)&lt;/td>
&lt;td>1.29 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vSphere&lt;/td>
&lt;td>1.18&lt;/td>
&lt;td>1.19&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;td>1.28 (Target)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The following storage drivers will not have CSI migration support.
The &lt;code>scaleio&lt;/code>, &lt;code>flocker&lt;/code>, &lt;code>quobyte&lt;/code> and &lt;code>storageos&lt;/code> drivers were removed; the others are deprecated and will be removed from core Kubernetes in the coming releases.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Driver&lt;/th>
&lt;th>Deprecated&lt;/th>
&lt;th>Code Removal&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Flocker&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GlusterFS&lt;/td>
&lt;td>1.25&lt;/td>
&lt;td>1.26 (Target)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Quobyte&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ScaleIO&lt;/td>
&lt;td>1.16&lt;/td>
&lt;td>1.22&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>StorageOS&lt;/td>
&lt;td>1.22&lt;/td>
&lt;td>1.25&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="what-does-it-mean-for-the-core-csi-migration-feature-to-go-ga">What does it mean for the core CSI Migration feature to go GA?&lt;/h2>
&lt;p>Core CSI Migration goes to GA means that the general framework, core library and API for CSI migration is
stable for Kubernetes v1.25 and will be part of future Kubernetes releases as well.&lt;/p>
&lt;ul>
&lt;li>If you are a Kubernetes distribution maintainer, this means if you disabled &lt;code>CSIMigration&lt;/code> feature gate previously, you are no longer allowed to do so because the feature gate has been locked.&lt;/li>
&lt;li>If you are a Kubernetes storage driver developer, this means you can expect no backwards incompatibility changes in the CSI migration library.&lt;/li>
&lt;li>If you are a Kubernetes maintainer, expect nothing changes from your day to day development flows.&lt;/li>
&lt;li>If you are a Kubernetes user, expect nothing to change from your day-to-day usage flows. If you encounter any storage related issues, contact the people who operate your cluster (if that's you, contact the provider of your Kubernetes distribution, or get help from the &lt;a href="https://kubernetes.io/community/#discuss">community&lt;/a>).&lt;/li>
&lt;/ul>
&lt;h2 id="what-does-it-mean-for-the-storage-driver-csi-migration-to-go-ga">What does it mean for the storage driver CSI migration to go GA?&lt;/h2>
&lt;p>Storage Driver CSI Migration goes to GA means that the specific storage driver supports CSI Migration. Expect feature parity between the in-tree plugin with the CSI driver.&lt;/p>
&lt;ul>
&lt;li>If you are a Kubernetes distribution maintainer, make sure you install the corresponding
CSI driver on the distribution. And make sure you are not disabling the specific &lt;code>CSIMigration{provider}&lt;/code> flag, as they are locked.&lt;/li>
&lt;li>If you are a Kubernetes storage driver maintainer, make sure the CSI driver can ensure feature parity if it supports CSI migration.&lt;/li>
&lt;li>If you are a Kubernetes maintainer/developer, expect nothing to change from your day-to-day development flows.&lt;/li>
&lt;li>If you are a Kubernetes user, the CSI Migration feature should be completely transparent
to you, the only requirement is to install the corresponding CSI driver.&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-next">What's next?&lt;/h2>
&lt;p>We are expecting cloud provider in-tree storage plugins code removal to start to happen as part of the v1.26 and v1.27 releases of Kubernetes. More and more drivers that support CSI migration will go GA in the upcoming releases.&lt;/p>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>The Kubernetes Slack channel &lt;a href="https://kubernetes.slack.com/messages/csi-migration">#csi-migration&lt;/a> along with any of the standard &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md#contact">SIG Storage communication channels&lt;/a> are great ways to reach out to the SIG Storage and migration working group teams.&lt;/p>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. We offer a huge thank you to the contributors who stepped up these last quarters to help move the project forward:&lt;/p>
&lt;ul>
&lt;li>Xing Yang (xing-yang)&lt;/li>
&lt;li>Hemant Kumar (gnufied)&lt;/li>
&lt;/ul>
&lt;p>Special thanks to the following people for the insightful reviews, thorough consideration and valuable contribution to the CSI migration feature:&lt;/p>
&lt;ul>
&lt;li>Andy Zhang (andyzhangz)&lt;/li>
&lt;li>Divyen Patel (divyenpatel)&lt;/li>
&lt;li>Deep Debroy (ddebroy)&lt;/li>
&lt;li>Humble Devassy Chirammal (humblec)&lt;/li>
&lt;li>Ismail Alidzhikov (ialidzhikov)&lt;/li>
&lt;li>Jordan Liggitt (liggitt)&lt;/li>
&lt;li>Matthew Cary (mattcary)&lt;/li>
&lt;li>Matthew Wong (wongma7)&lt;/li>
&lt;li>Neha Arora (nearora-msft)&lt;/li>
&lt;li>Oksana Naumov (trierra)&lt;/li>
&lt;li>Saad Ali (saad-ali)&lt;/li>
&lt;li>Michelle Au (msau42)&lt;/li>
&lt;/ul>
&lt;p>Those interested in getting involved with the design and development of CSI or any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group (SIG)&lt;/a>. We’re rapidly growing and always welcome new contributors.&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: CustomResourceDefinition Validation Rules Graduate to Beta</title><link>https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/</link><pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/23/crd-validation-rules-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Joe Betz (Google), Cici Huang (Google), Kermit Alexander (Google)&lt;/p>
&lt;p>In Kubernetes 1.25, &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">Validation rules for CustomResourceDefinitions&lt;/a> (CRDs) have graduated to Beta!&lt;/p>
&lt;p>Validation rules make it possible to declare how custom resources are validated using the &lt;a href="https://github.com/google/cel-spec">Common Expression Language&lt;/a> (CEL). For example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apiextensions.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CustomResourceDefinition&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>object&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">x-kubernetes-validations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">rule&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;self.minReplicas &amp;lt;= self.replicas &amp;amp;&amp;amp; self.replicas &amp;lt;= self.maxReplicas&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">message&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;replicas should be in the range minReplicas..maxReplicas.&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>integer&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Validation rules support a wide range of use cases. To get a sense of some of the capabilities, let's look at a few examples:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Validation Rule&lt;/th>
&lt;th>Purpose&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>self.minReplicas &amp;lt;= self.replicas&lt;/code>&lt;/td>
&lt;td>Validate an integer field is less than or equal to another integer field&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>'Available' in self.stateCounts&lt;/code>&lt;/td>
&lt;td>Validate an entry with the 'Available' key exists in a map&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.set1.all(e, !(e in self.set2))&lt;/code>&lt;/td>
&lt;td>Validate that the elements of two sets are disjoint&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self == oldSelf&lt;/code>&lt;/td>
&lt;td>Validate that a required field is immutable once it is set&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.created + self.ttl &amp;lt; self.expired&lt;/code>&lt;/td>
&lt;td>Validate that 'expired' date is after a 'create' date plus a 'ttl' duration&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Validation rules are expressive and flexible. See the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">Validation Rules documentation&lt;/a> to learn more about what validation rules are capable of.&lt;/p>
&lt;h2 id="why-cel">Why CEL?&lt;/h2>
&lt;p>CEL was chosen as the language for validation rules for a couple reasons:&lt;/p>
&lt;ul>
&lt;li>CEL expressions can easily be inlined into CRD schemas. They are sufficiently expressive to replace the vast majority of CRD validation checks currently implemented in admission webhooks. This results in CRDs that are self-contained and are easier to understand.&lt;/li>
&lt;li>CEL expressions are compiled and type checked against a CRD's schema &amp;quot;ahead-of-time&amp;quot; (when CRDs are created and updated) allowing them to be evaluated efficiently and safely &amp;quot;runtime&amp;quot; (when custom resources are validated). Even regex string literals in CEL are validated and pre-compiled when CRDs are created or updated.&lt;/li>
&lt;/ul>
&lt;h2 id="why-not-use-validation-webhooks">Why not use validation webhooks?&lt;/h2>
&lt;p>Benefits of using validation rules when compared with validation webhooks:&lt;/p>
&lt;ul>
&lt;li>CRD authors benefit from a simpler workflow since validation rules eliminate the need to develop and maintain a webhook.&lt;/li>
&lt;li>Cluster administrators benefit by no longer having to install, upgrade and operate webhooks for the purposes of CRD validation.&lt;/li>
&lt;li>Cluster operability improves because CRD validation no longer requires a remote call to a webhook endpoint, eliminating a potential point of failure in the request-serving-path of the Kubernetes API server. This allows clusters to retain high availability while scaling to larger amounts of installed CRD extensions, since expected control plane availability would otherwise decrease with each additional webhook installed.&lt;/li>
&lt;/ul>
&lt;h2 id="getting-started-with-validation-rules">Getting started with validation rules&lt;/h2>
&lt;h3 id="writing-validation-rules-in-openapiv3-schemas">Writing validation rules in OpenAPIv3 schemas&lt;/h3>
&lt;p>You can define validation rules for any level of a CRD's OpenAPIv3 schema. Validation rules are automatically scoped to their location in the schema where they are declared.&lt;/p>
&lt;p>Good practices for CRD validation rules:&lt;/p>
&lt;ul>
&lt;li>Scope validation rules as close as possible to the fields(s) they validate.&lt;/li>
&lt;li>Use multiple rules when validating independent constraints.&lt;/li>
&lt;li>Do not use validation rules for validations already&lt;/li>
&lt;li>Use OpenAPIv3 &lt;a href="https://swagger.io/specification/#properties">value validations&lt;/a> (&lt;code>maxLength&lt;/code>, &lt;code>maxItems&lt;/code>, &lt;code>maxProperties&lt;/code>, &lt;code>required&lt;/code>, &lt;code>enum&lt;/code>, &lt;code>minimum&lt;/code>, &lt;code>maximum&lt;/code>, ..) and &lt;a href="https://swagger.io/docs/specification/data-models/data-types/#format">string formats&lt;/a> where available.&lt;/li>
&lt;li>Use &lt;code>x-kubernetes-int-or-string&lt;/code>, &lt;code>x-kubernetes-embedded-type&lt;/code> and &lt;code>x-kubernetes-list-type=(set|map)&lt;/code> were appropriate.&lt;/li>
&lt;/ul>
&lt;p>Examples of good practice:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Validation&lt;/th>
&lt;th>Best Practice&lt;/th>
&lt;th>Example(s)&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Validate an integer is between 0 and 100.&lt;/td>
&lt;td>Use OpenAPIv3 value validations.&lt;/td>
&lt;td>&lt;pre>type: integer&lt;br>minimum: 0&lt;br>maximum: 100&lt;/pre>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Constraint the max size limits on maps (objects with additionalProperties), arrays and string.&lt;/td>
&lt;td>Use OpenAPIv3 value validations. Recommended for all maps, arrays and strings. This best practice is essential for rule cost estimation (explained below).&lt;/td>
&lt;td>&lt;pre>type:&lt;br>maxItems: 100&lt;/pre>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Require a date-time be more recent than a particular timestamp.&lt;/td>
&lt;td>Use OpenAPIv3 string formats to declare that the field is a date-time. Use validation rules to compare it to a particular timestamp.&lt;/td>
&lt;td>&lt;pre>type: string&lt;br>format: date-time&lt;br>x-kubernetes-validations:&lt;br> - rule: &amp;quot;self &amp;gt;= timestamp('2000-01-01T00:00:00.000Z')&amp;quot;&lt;/pre>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Require two sets to be disjoint.&lt;/td>
&lt;td>Use x-kubernetes-list-type to validate that the arrays are sets. &lt;br>Use validation rules to validate the sets are disjoint.&lt;/td>
&lt;td>&lt;pre>type: object&lt;br>properties:&lt;br> set1:&lt;br> type: array&lt;br> x-kubernetes-list-type: set&lt;br> set2: ...&lt;br> x-kubernetes-validations:&lt;br> - rule: &amp;quot;!self.set1.all(e, !(e in self.set2))&amp;quot;&lt;/pre>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="crd-transition-rules">CRD transition rules&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#transition-rules">Transition Rules&lt;/a> make it possible to compare the new state against the old state of a resource in validation rules. You use transition rules to make sure that the cluster's API server does not accept invalid state transitions. A transition rule is a validation rule that references 'oldSelf'. The API server only evaluates transition rules when both an old value and new value exist.&lt;/p>
&lt;p>Transition rule examples:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Transition Rule&lt;/th>
&lt;th>Purpose&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>self == oldSelf&lt;/code>&lt;/td>
&lt;td>For a required field, make that field immutable once it is set. For an optional field, only allow transitioning from unset to set, or from set to unset.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>(on parent of field) &lt;code>has(self.field) == has(oldSelf.field)&lt;/code>&lt;br>on field: &lt;code>self == oldSelf&lt;/code>&lt;/td>
&lt;td>Make a field immutable: validate that a field, even if optional, never changes after the resource is created (for a required field, the previous rule is simpler).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.all(x, x in oldSelf)&lt;/code>&lt;/td>
&lt;td>Only allow adding items to a field that represents a set (prevent removals).&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self &amp;gt;= oldSelf&lt;/code>&lt;/td>
&lt;td>Validate that a number is monotonically increasing.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="using-the-functions-libraries">Using the Functions Libraries&lt;/h2>
&lt;p>Validation rules have access to a couple different function libraries:&lt;/p>
&lt;ul>
&lt;li>CEL standard functions, defined in the &lt;a href="https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#list-of-standard-definitions">list of standard definitions&lt;/a>&lt;/li>
&lt;li>CEL standard &lt;a href="https://github.com/google/cel-spec/blob/v0.7.0/doc/langdef.md#macros">macros&lt;/a>&lt;/li>
&lt;li>CEL &lt;a href="https://pkg.go.dev/github.com/google/cel-go/ext#Strings">extended string function library&lt;/a>&lt;/li>
&lt;li>Kubernetes &lt;a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#pkg-functions">CEL extension library&lt;/a> which includes supplemental functions for &lt;a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#pkg-functions">lists&lt;/a>, &lt;a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#Regex">regex&lt;/a>, and &lt;a href="https://pkg.go.dev/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/library#Regex">URLs&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Examples of function libraries in use:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Validation Rule&lt;/th>
&lt;th>Purpose&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>!(self.getDayOfWeek() in [0, 6])&lt;/code>&lt;/td>
&lt;td>Validate that a date is not a Sunday or Saturday.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>isUrl(self) &amp;amp;&amp;amp; url(self).getHostname() in [a.example.com', 'b.example.com']&lt;/code>&lt;/td>
&lt;td>Validate that a URL has an allowed hostname.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.map(x, x.weight).sum() == 1&lt;/code>&lt;/td>
&lt;td>Validate that the weights of a list of objects sum to 1.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>int(self.find('^[0-9]*')) &amp;lt; 100&lt;/code>&lt;/td>
&lt;td>Validate that a string starts with a number less than 100.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>self.isSorted()&lt;/code>&lt;/td>
&lt;td>Validates that a list is sorted.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="resource-use-and-limits">Resource use and limits&lt;/h2>
&lt;p>To prevent CEL evaluation from consuming excessive compute resources, validation rules impose some limits. These limits are based on CEL &lt;em>cost units&lt;/em>, a platform and machine independent measure of execution cost. As a result, the limits are the same regardless of where they are enforced.&lt;/p>
&lt;h3 id="estimated-cost-limit">Estimated cost limit&lt;/h3>
&lt;p>CEL is, by design, non-Turing-complete in such a way that the halting problem isn’t a concern. CEL takes advantage of this design choice to include an &amp;quot;estimated cost&amp;quot; subsystem that can statically compute the worst case run time cost of any CEL expression. Validation rules are &lt;a href="o/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#resource-use-by-validation-functions">integrated with the estimated cost system&lt;/a> and disallow CEL expressions from being included in CRDs if they have a sufficiently poor (high) estimated cost. The estimated cost limit is set quite high and typically requires an O(n^2) or worse operation, across something of unbounded size, to be exceeded. Fortunately the fix is usually quite simple: because the cost system is aware of size limits declared in the CRD's schema, CRD authors can add size limits to the CRD's schema (&lt;code>maxItems&lt;/code> for arrays, &lt;code>maxProperties&lt;/code> for maps, &lt;code>maxLength&lt;/code> for strings) to reduce the estimated cost.&lt;/p>
&lt;p>Good practice:&lt;/p>
&lt;p>Set &lt;code>maxItems&lt;/code>, &lt;code>maxProperties&lt;/code> and &lt;code>maxLength&lt;/code> on all array, map (&lt;code>object&lt;/code> with &lt;code>additionalProperties&lt;/code>) and string types in CRD schemas! This results in lower and more accurate estimated costs and generally makes a CRD safer to use.&lt;/p>
&lt;h3 id="runtime-cost-limits-for-crd-validation-rules">Runtime cost limits for CRD validation rules&lt;/h3>
&lt;p>In addition to the estimated cost limit, CEL keeps track of actual cost while evaluating a CEL expression and will halt execution of the expression if a limit is exceeded.&lt;/p>
&lt;p>With the estimated cost limit already in place, the runtime cost limit is rarely encountered. But it is possible. For example, it might be encountered for a large resource composed entirely of a single large list and a validation rule that is either evaluated on each element in the list, or traverses the entire list.&lt;/p>
&lt;p>CRD authors can ensure the runtime cost limit will not be exceeded in much the same way the estimated cost limit is avoided: by setting &lt;code>maxItems&lt;/code>, &lt;code>maxProperties&lt;/code> and &lt;code>maxLength&lt;/code> on array, map and string types.&lt;/p>
&lt;h2 id="future-work">Future work&lt;/h2>
&lt;p>We look forward to working with the community on the adoption of CRD Validation Rules, and hope to see this feature promoted to general availability in an upcoming Kubernetes release!&lt;/p>
&lt;p>There is a growing community of Kubernetes contributors thinking about how to make it possible to write extensible admission controllers using CEL as a substitute for admission webhooks for policy enforcement use cases. Anyone interested should reach out to us on the usual &lt;a href="https://github.com/kubernetes/community/tree/master/sig-api-machinery">SIG API Machinery&lt;/a> channels or via slack at &lt;a href="https://kubernetes.slack.com/archives/C02TTBG6LF4">#sig-api-machinery-cel-dev&lt;/a>.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>Special thanks to Cici Huang, Ben Luddy, Jordan Liggitt, David Eads, Daniel Smith, Dr. Stefan Schimanski, Leila Jalali and everyone who contributed to Validation Rules!&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: Use Secrets for Node-Driven Expansion of CSI Volumes</title><link>https://kubernetes.io/blog/2022/09/21/kubernetes-1-25-use-secrets-while-expanding-csi-volumes-on-node-alpha/</link><pubDate>Wed, 21 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/21/kubernetes-1-25-use-secrets-while-expanding-csi-volumes-on-node-alpha/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Humble Chirammal (Red Hat), Louis Koo (deeproute.ai)&lt;/p>
&lt;p>Kubernetes v1.25, released earlier this month, introduced a new feature
that lets your cluster expand storage volumes, even when access to those
volumes requires a secret (for example: a credential for accessing a SAN fabric)
to perform node expand operation. This new behavior is in alpha and you
must enable a feature gate (&lt;code>CSINodeExpandSecret&lt;/code>) to make use of it.
You must also be using &lt;a href="https://kubernetes-csi.github.io/docs/">CSI&lt;/a>
storage; this change isn't relevant to storage drivers that are built in to Kubernetes.&lt;/p>
&lt;p>To turn on this new, alpha feature, you enable the &lt;code>CSINodeExpandSecret&lt;/code> feature
gate for the kube-apiserver and kubelet, which turns on a mechanism to send &lt;code>secretRef&lt;/code>
configuration as part of NodeExpansion by the CSI drivers thus make use of
the same to perform node side expansion operation with the underlying
storage system.&lt;/p>
&lt;h2 id="what-is-this-all-about">What is this all about?&lt;/h2>
&lt;p>Before Kubernetes v1.24, you were able to define a cluster-level StorageClass
that made use of &lt;a href="https://kubernetes-csi.github.io/docs/secrets-and-credentials-storage-class.html">StorageClass Secrets&lt;/a>,
but you didn't have any mechanism to specify the credentials that would be used for
operations that take place when the storage was mounted onto a node and when
the volume has to be expanded at node side.&lt;/p>
&lt;p>The Kubernetes CSI already implemented a similar mechanism specific kinds of
volume resizes; namely, resizes of PersistentVolumes where the resizes take place
independently from any node referred as Controller Expansion. In that case, you
associate a PersistentVolume with a Secret that contains credentials for volume resize
actions, so that controller expansion can take place. CSI also supports a &lt;code>nodeExpandVolume&lt;/code>
operation which CSI drivers can make use independent of Controller Expansion or along with
Controller Expansion on which, where the resize is driven from a node in your cluster where
the volume is attached. Please read &lt;a href="https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/">Kubernetes 1.24: Volume Expansion Now A Stable Feature&lt;/a>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>At times, the CSI driver needs to check the actual size of the backend block storage (or image)
before proceeding with a node-level filesystem expand operation. This avoids false positive returns
from the backend storage cluster during filesystem expands.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When a PersistentVolume represents encrypted block storage (for example using LUKS)
you need to provide a passphrase in order to expand the device, and also to make it possible
to grow the filesystem on that device.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>For various validations at time of node expansion, the CSI driver has to be connected
to the backend storage cluster. If the &lt;code>nodeExpandVolume&lt;/code> request includes a &lt;code>secretRef&lt;/code>
then the CSI driver can make use of the same and connect to the storage cluster to
perform the cluster operations.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;p>To enable this functionality from this version of Kubernetes, SIG Storage have introduced
a new feature gate called &lt;code>CSINodeExpandSecret&lt;/code>. Once the feature gate is enabled
in the cluster, NodeExpandVolume requests can include a &lt;code>secretRef&lt;/code> field. The NodeExpandVolume request
is part of CSI; for example, in a request which has been sent from the Kubernetes
control plane to the CSI driver.&lt;/p>
&lt;p>As a cluster operator, you admin can specify these secrets as an opaque parameter in a StorageClass,
the same way that you can already specify other CSI secret data. The StorageClass needs to have some
CSI-specific parameters set. Here's an example of those parameters:&lt;/p>
&lt;pre tabindex="0">&lt;code>csi.storage.k8s.io/node-expand-secret-name: test-secret
csi.storage.k8s.io/node-expand-secret-namespace: default
&lt;/code>&lt;/pre>&lt;p>If feature gates are enabled and storage class carries the above secret configuration,
the CSI provisioner receives the credentials from the Secret as part of the NodeExpansion request.&lt;/p>
&lt;p>CSI volumes that require secrets for online expansion will have NodeExpandSecretRef
field set. If not set, the NodeExpandVolume CSI RPC call will be made without a secret.&lt;/p>
&lt;h2 id="trying-it-out">Trying it out&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>Enable the &lt;code>CSINodeExpandSecret&lt;/code> feature gate (please refer to
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">Feature Gates&lt;/a>).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Create a Secret, and then a StorageClass that uses that Secret.&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>Here's an example manifest for a Secret that holds credentials:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Secret&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-secret&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">data&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">stringData&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">username&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admin&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">password&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>t0p-Secret&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here's an example manifest for a StorageClass that refers to those credentials:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StorageClass&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-blockstorage-sc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/node-expand-secret-name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-secret &lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># the name of the Secret&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi.storage.k8s.io/node-expand-secret-namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default &lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># the namespace that the Secret is in&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">provisioner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>blockstorage.cloudprovider.example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">reclaimPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeBindingMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Immediate&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">allowVolumeExpansion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="example-output">Example output&lt;/h2>
&lt;p>If the PersistentVolumeClaim (PVC) was created successfully, you can see that
configuration within the &lt;code>spec.csi&lt;/code> field of the PersistentVolume (look for
&lt;code>spec.csi.nodeExpandSecretRef&lt;/code>).
Check that it worked by running &lt;code>kubectl get persistentvolume &amp;lt;pv_name&amp;gt; -o yaml&lt;/code>.
You should see something like.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolume&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">pv.kubernetes.io/provisioned-by&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>blockstorage.cloudprovider.example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">creationTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2022-08-26T15:14:07Z&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">finalizers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- kubernetes.io/pv-protection&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;420263&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">uid&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>6fa824d7-8a06-4e0c-b722-d3f897dcbd65&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">capacity&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>6Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claimRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resourceVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;419862&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">uid&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>95eb531a-d675-49f6-940b-9bc3fde83eb0&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">csi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>blockstorage.cloudprovider.example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">nodeExpandSecretRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-secret&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeAttributes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage.kubernetes.io/csiProvisionerIdentity&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1648042783218-8081&lt;/span>-blockstorage.cloudprovider.example&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>e21c7809-aabb-11ec-917a-2e2e254eb4cf&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">nodeAffinity&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">required&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">nodeSelectorTerms&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">matchExpressions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>topology.hostpath.csi/node&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>In&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- racknode01&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeReclaimPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-blockstorage-sc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">phase&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Bound&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you then trigger online storage expansion, the kubelet passes the appropriate credentials
to the CSI driver, by loading that Secret and passing the data to the storage driver.&lt;/p>
&lt;p>Here's an example debug log:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0330 03:29:51.966241 1 server.go:101] GRPC call: /csi.v1.Node/NodeExpandVolume
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0330 03:29:51.966261 1 server.go:105] GRPC request: {&amp;#34;capacity_range&amp;#34;:{&amp;#34;required_bytes&amp;#34;:7516192768},&amp;#34;secrets&amp;#34;:&amp;#34;***stripped***&amp;#34;,&amp;#34;staging_target_path&amp;#34;:&amp;#34;/var/lib/kubelet/plugins/kubernetes.io/csi/blockstorage.cloudprovider.example/f7c62e6e08ce21e9b2a95c841df315ed4c25a15e91d8fcaf20e1c2305e5300ab/globalmount&amp;#34;,&amp;#34;volume_capability&amp;#34;:{&amp;#34;AccessType&amp;#34;:{&amp;#34;Mount&amp;#34;:{}},&amp;#34;access_mode&amp;#34;:{&amp;#34;mode&amp;#34;:7}},&amp;#34;volume_id&amp;#34;:&amp;#34;e21c7809-aabb-11ec-917a-2e2e254eb4cf&amp;#34;,&amp;#34;volume_path&amp;#34;:&amp;#34;/var/lib/kubelet/pods/bcb1b2c4-5793-425c-acf1-47163a81b4d7/volumes/kubernetes.io~csi/pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0/mount&amp;#34;}
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0330 03:29:51.966360 1 nodeserver.go:459] req:volume_id:&amp;#34;e21c7809-aabb-11ec-917a-2e2e254eb4cf&amp;#34; volume_path:&amp;#34;/var/lib/kubelet/pods/bcb1b2c4-5793-425c-acf1-47163a81b4d7/volumes/kubernetes.io~csi/pvc-95eb531a-d675-49f6-940b-9bc3fde83eb0/mount&amp;#34; capacity_range:&amp;lt;required_bytes:7516192768 &amp;gt; staging_target_path:&amp;#34;/var/lib/kubelet/plugins/kubernetes.io/csi/blockstorage.cloudprovider.example/f7c62e6e08ce21e9b2a95c841df315ed4c25a15e91d8fcaf20e1c2305e5300ab/globalmount&amp;#34; volume_capability:&amp;lt;mount:&amp;lt;&amp;gt; access_mode:&amp;lt;mode:SINGLE_NODE_MULTI_WRITER &amp;gt; &amp;gt; secrets:&amp;lt;key:&amp;#34;XXXXXX&amp;#34; value:&amp;#34;XXXXX&amp;#34; &amp;gt; secrets:&amp;lt;key:&amp;#34;XXXXX&amp;#34; value:&amp;#34;XXXXXX&amp;#34; &amp;gt;
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="the-future">The future&lt;/h2>
&lt;p>As this feature is still in alpha, Kubernetes Storage SIG expect to update or get feedback from CSI driver
authors with more tests and implementation. The community plans to eventually
promote the feature to Beta in upcoming releases.&lt;/p>
&lt;h2 id="get-involved-or-learn-more">Get involved or learn more?&lt;/h2>
&lt;p>The enhancement proposal includes lots of detail about the history and technical
implementation of this feature.&lt;/p>
&lt;p>To learn more about StorageClass based dynamic provisioning in Kubernetes, please refer to
&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Classes&lt;/a> and
&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volumes&lt;/a>.&lt;/p>
&lt;p>Please get involved by joining the Kubernetes
&lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/README.md">Storage SIG&lt;/a>
(Special Interest Group) to help us enhance this feature.
There are a lot of good ideas already and we'd be thrilled to have more!&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: Local Storage Capacity Isolation Reaches GA</title><link>https://kubernetes.io/blog/2022/09/19/local-storage-capacity-isolation-ga/</link><pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/19/local-storage-capacity-isolation-ga/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Jing Xu (Google)&lt;/p>
&lt;p>Local ephemeral storage capacity isolation was introduced as a alpha feature in Kubernetes 1.7 and it went beta in 1.9. With Kubernetes 1.25 we are excited to announce general availability(GA) of this feature.&lt;/p>
&lt;p>Pods use ephemeral local storage for scratch space, caching, and logs. The lifetime of local ephemeral storage does not extend beyond the life of the individual pod. It is exposed to pods using the container’s writable layer, logs directory, and &lt;code>EmptyDir&lt;/code> volumes. Before this feature was introduced, there were issues related to the lack of local storage accounting and isolation, such as Pods not knowing how much local storage is available and being unable to request guaranteed local storage. Local storage is a best-effort resource and pods can be evicted due to other pods filling the local storage.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage">local storage capacity isolation feature&lt;/a> allows users to manage local ephemeral storage in the same way as managing CPU and memory. It provides support for capacity isolation of shared storage between pods, such that a pod can be hard limited in its consumption of shared resources by evicting Pods if its consumption of shared storage exceeds that limit. It also allows setting ephemeral storage requests for resource reservation. The limits and requests for shared &lt;code>ephemeral-storage&lt;/code> are similar to those for memory and CPU consumption.&lt;/p>
&lt;h3 id="how-to-use-local-storage-capacity-isolation">How to use local storage capacity isolation&lt;/h3>
&lt;p>A typical configuration for local ephemeral storage is to place all different kinds of ephemeral local data (emptyDir volumes, writeable layers, container images, logs) into one filesystem. Typically, both /var/lib/kubelet and /var/log are on the system's root filesystem. If users configure the local storage in different ways, kubelet might not be able to correctly measure disk usage and use this feature.&lt;/p>
&lt;h4 id="setting-requests-and-limits-for-local-ephemeral-storage">Setting requests and limits for local ephemeral storage&lt;/h4>
&lt;p>You can specify &lt;code>ephemeral-storage&lt;/code> for managing local ephemeral storage. Each container of a Pod can specify either or both of the following:&lt;/p>
&lt;ul>
&lt;li>&lt;code>spec.containers[].resources.limits.ephemeral-storage&lt;/code>&lt;/li>
&lt;li>&lt;code>spec.containers[].resources.requests.ephemeral-storage&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>In the following example, the Pod has two containers. The first container has a request of 8GiB of local ephemeral storage and a limit of 12GiB. The second container requests 2GiB of local storage, but no limit setting. Therefore, the Pod requests a total of 10GiB (8GiB+2GiB) of local ephemeral storage and enforces a limit of 12GiB of local ephemeral storage. It also sets emptyDir sizeLimit to 5GiB. With this setting in pod spec, it will affect how the scheduler makes a decision on scheduling pods and also how kubelet evict pods.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>frontend&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>app&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>images.my-company.example/app:v4&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;8Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;12Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ephemeral&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/tmp&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>log-aggregator&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>images.my-company.example/log-aggregator:v6&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ephemeral&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/tmp&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ephemeral&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">emptyDir&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{}&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">sizeLimit&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>5Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>First of all, the scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node. In this case, the pod can be assigned to a node only if its available ephemeral storage (allocatable resource) has more than 10GiB.&lt;/p>
&lt;p>Secondly, at container level, since one of the container sets resource limit, kubelet eviction manager will measure the disk usage of this container and evict the pod if the storage usage of the first container exceeds its limit (12GiB). At pod level, kubelet works out an overall Pod storage limit by
adding up the limits of all the containers in that Pod. In this case, the total storage usage at pod level is the sum of the disk usage from all containers plus the Pod's &lt;code>emptyDir&lt;/code> volumes. If this total usage exceeds the overall Pod storage limit (12GiB), then the kubelet also marks the Pod for eviction.&lt;/p>
&lt;p>Last, in this example, emptyDir volume sets its sizeLimit to 5Gi. It means that if this pod's emptyDir used up more local storage than 5GiB, the pod will be evicted from the node.&lt;/p>
&lt;h4 id="setting-resource-quota-and-limitrange-for-local-ephemeral-storage">Setting resource quota and limitRange for local ephemeral storage&lt;/h4>
&lt;p>This feature adds two more resource quotas for storage. The request and limit set constraints on the total requests/limits of all containers’ in a namespace.&lt;/p>
&lt;ul>
&lt;li>&lt;code>requests.ephemeral-storage&lt;/code>&lt;/li>
&lt;li>&lt;code>limits.ephemeral-storage&lt;/code>&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ResourceQuota&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage-resources&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">hard&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests.ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;10Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits.ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;20Gi&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Similar to CPU and memory, admin could use LimitRange to set default container’s local storage request/limit, and/or minimum/maximum resource constraints for a namespace.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>LimitRange&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage-limit-range&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">limits&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">default&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">defaultRequest&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ephemeral-storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>5Gi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Container&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Also, ephemeral-storage may be specified to reserve for kubelet or system. example, &lt;code>--system-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=10Gi][,][pid=1000] --kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=5Gi][,][pid=1000]&lt;/code>. If your cluster node root disk capacity is 100Gi, after setting system-reserved and kube-reserved value, the available allocatable ephemeral storage would become 85Gi. The schedule will use this information to assign pods based on request and allocatable resources from each node. The eviction manager will also use allocatable resource to determine pod eviction. See more details from &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">Reserve Compute Resources for System Daemons&lt;/a>.&lt;/p>
&lt;h3 id="how-do-i-get-involved">How do I get involved?&lt;/h3>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together.&lt;/p>
&lt;p>We offer a huge thank you to all the contributors in &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage SIG&lt;/a> and CSI community who helped review the design and implementation of the project, including but not limited to the following:&lt;/p>&lt;ul>&lt;li>Benjamin Elder (&lt;a href=https://github.com/BenTheElder>BenTheElder&lt;/a>)&lt;/li>&lt;li>Michelle Au (&lt;a href=https://github.com/msau42>msau42&lt;/a>)&lt;/li>&lt;li>Tim Hockin (&lt;a href=https://github.com/thockin>thockin&lt;/a>)&lt;/li>&lt;li>Jordan Liggitt (&lt;a href=https://github.com/liggitt>liggitt&lt;/a>)&lt;/li>&lt;li>Xing Yang (&lt;a href=https://github.com/xing-yang>xing-yang&lt;/a>)&lt;/li>&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: Two Features for Apps Rollouts Graduate to Stable</title><link>https://kubernetes.io/blog/2022/09/15/app-rollout-features-reach-stable/</link><pubDate>Thu, 15 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/15/app-rollout-features-reach-stable/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Ravi Gudimetla (Apple), Filip Křepinský (Red Hat), Maciej Szulik (Red Hat)&lt;/p>
&lt;p>This blog describes the two features namely &lt;code>minReadySeconds&lt;/code> for StatefulSets and &lt;code>maxSurge&lt;/code> for DaemonSets that SIG Apps is happy to graduate to stable in Kubernetes 1.25.&lt;/p>
&lt;p>Specifying &lt;code>minReadySeconds&lt;/code> slows down a rollout of a StatefulSet, when using a &lt;code>RollingUpdate&lt;/code> value in &lt;code>.spec.updateStrategy&lt;/code> field, by waiting for each pod for a desired time.
This time can be used for initializing the pod (e.g. warming up the cache) or as a delay before acknowledging the pod.&lt;/p>
&lt;p>&lt;code>maxSurge&lt;/code> allows a DaemonSet workload to run multiple instances of the same pod on a node during a rollout when using a &lt;code>RollingUpdate&lt;/code> value in &lt;code>.spec.updateStrategy&lt;/code> field.
This helps to minimize the downtime of the DaemonSet for consumers.&lt;/p>
&lt;p>These features were already available in a Deployment and other workloads. This graduation helps to align this functionality across the workloads.&lt;/p>
&lt;h2 id="what-problems-do-these-features-solve">What problems do these features solve?&lt;/h2>
&lt;h3 id="solved-problem-statefulset-minreadyseconds">minReadySeconds for StatefulSets&lt;/h3>
&lt;p>&lt;code>minReadySeconds&lt;/code> ensures that the StatefulSet workload is &lt;code>Ready&lt;/code> for the given number of seconds before reporting the
pod as &lt;code>Available&lt;/code>. The notion of being &lt;code>Ready&lt;/code> and &lt;code>Available&lt;/code> is quite important for workloads. For example, some workloads, like Prometheus with multiple instances of Alertmanager, should be considered &lt;code>Available&lt;/code> only when the Alertmanager's state transfer is complete. &lt;code>minReadySeconds&lt;/code> also helps when using loadbalancers with cloud providers. Since the pod should be &lt;code>Ready&lt;/code> for the given number of seconds, it provides buffer time to prevent killing pods in rotation before new pods show up.&lt;/p>
&lt;h3 id="how-use-daemonset-maxsurge">maxSurge for DaemonSets&lt;/h3>
&lt;p>Kubernetes system-level components like CNI, CSI are typically run as DaemonSets. These components can have impact on the availability of the workloads if those DaemonSets go down momentarily during the upgrades. The feature allows DaemonSet pods to temporarily increase their number, thereby ensuring zero-downtime for the DaemonSets.&lt;/p>
&lt;p>Please note that the usage of &lt;code>hostPort&lt;/code> in conjunction with &lt;code>maxSurge&lt;/code> in DaemonSets is not allowed as DaemonSet pods are tied to a single node and two active pods cannot share the same port on the same node.&lt;/p>
&lt;h2 id="how-does-it-work">How does it work?&lt;/h2>
&lt;h3 id="how-does-statefulset-minreadyseconds-work">minReadySeconds for StatefulSets&lt;/h3>
&lt;p>The StatefulSet controller watches for the StatefulSet pods and counts how long a particular pod has been in the &lt;code>Running&lt;/code> state, if this value is greater than or equal to the time specified in &lt;code>.spec.minReadySeconds&lt;/code> field of the StatefulSet, the StatefulSet controller updates the &lt;code>AvailableReplicas&lt;/code> field in the StatefulSet's status.&lt;/p>
&lt;h3 id="how-does-daemonset-maxsurge-work">maxSurge for DaemonSets&lt;/h3>
&lt;p>The DaemonSet controller creates the additional pods (above the desired number resulting from DaemonSet spec) based on the value given in &lt;code>.spec.strategy.rollingUpdate.maxSurge&lt;/code>. The additional pods would run on the same node where the old DaemonSet pod is running till the old pod gets killed.&lt;/p>
&lt;ul>
&lt;li>The default value is 0.&lt;/li>
&lt;li>The value cannot be &lt;code>0&lt;/code> when &lt;code>MaxUnavailable&lt;/code> is 0.&lt;/li>
&lt;li>The value can be specified either as an absolute number of pods, or a percentage (rounded up) of desired pods.&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;h3 id="how-use-statefulset-minreadyseconds">minReadySeconds for StatefulSets&lt;/h3>
&lt;p>Specify a value for &lt;code>minReadySeconds&lt;/code> for any StatefulSet and check if pods are available or not by inspecting
&lt;code>AvailableReplicas&lt;/code> field using:&lt;/p>
&lt;p>&lt;code>kubectl get statefulset/&amp;lt;name_of_the_statefulset&amp;gt; -o yaml&lt;/code>&lt;/p>
&lt;p>Please note that the default value of &lt;code>minReadySeconds&lt;/code> is 0.&lt;/p>
&lt;h3 id="how-use-daemonset-maxsurge">maxSurge for DaemonSets&lt;/h3>
&lt;p>Specify a value for &lt;code>.spec.updateStrategy.rollingUpdate.maxSurge&lt;/code> and set &lt;code>.spec.updateStrategy.rollingUpdate.maxUnavailable&lt;/code> to &lt;code>0&lt;/code>.&lt;/p>
&lt;p>Then observe a faster rollout and higher number of pods running at the same time in the next rollout.&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl rollout restart daemonset &amp;lt;name_of_the_daemonset&amp;gt;
kubectl get pods -w
&lt;/code>&lt;/pre>&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;h3 id="learn-more-statefulset-minreadyseconds">minReadySeconds for StatefulSets&lt;/h3>
&lt;ul>
&lt;li>Documentation: &lt;a href="https://k8s.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds">https://k8s.io/docs/concepts/workloads/controllers/statefulset/#minimum-ready-seconds&lt;/a>&lt;/li>
&lt;li>KEP: &lt;a href="https://github.com/kubernetes/enhancements/issues/2599">https://github.com/kubernetes/enhancements/issues/2599&lt;/a>&lt;/li>
&lt;li>API Changes: &lt;a href="https://github.com/kubernetes/kubernetes/pull/100842">https://github.com/kubernetes/kubernetes/pull/100842&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="learn-more-daemonset-maxsurge">maxSurge for DaemonSets&lt;/h3>
&lt;ul>
&lt;li>Documentation: &lt;a href="https://k8s.io/docs/tasks/manage-daemon/update-daemon-set/">https://k8s.io/docs/tasks/manage-daemon/update-daemon-set/&lt;/a>&lt;/li>
&lt;li>KEP: &lt;a href="https://github.com/kubernetes/enhancements/issues/1591">https://github.com/kubernetes/enhancements/issues/1591&lt;/a>&lt;/li>
&lt;li>API Changes: &lt;a href="https://github.com/kubernetes/kubernetes/pull/96375">https://github.com/kubernetes/kubernetes/pull/96375&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="how-do-i-get-involved">How do I get involved?&lt;/h2>
&lt;p>Please reach out to us on &lt;a href="https://kubernetes.slack.com/archives/C18NZM5K9">#sig-apps&lt;/a> channel on Slack, or through the SIG Apps mailing list &lt;a href="https://groups.google.com/g/kubernetes-sig-apps">kubernetes-sig-apps@googlegroups.com&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: PodHasNetwork Condition for Pods</title><link>https://kubernetes.io/blog/2022/09/14/pod-has-network-condition/</link><pubDate>Wed, 14 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/14/pod-has-network-condition/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong>
Deep Debroy (Apple)&lt;/p>
&lt;p>Kubernetes 1.25 introduces Alpha support for a new kubelet-managed pod condition
in the status field of a pod: &lt;code>PodHasNetwork&lt;/code>. The kubelet, for a worker node,
will use the &lt;code>PodHasNetwork&lt;/code> condition to accurately surface the initialization
state of a pod from the perspective of pod sandbox creation and network
configuration by a container runtime (typically in coordination with CNI
plugins). The kubelet starts to pull container images and start individual
containers (including init containers) after the status of the &lt;code>PodHasNetwork&lt;/code>
condition is set to &lt;code>&amp;quot;True&amp;quot;&lt;/code>. Metrics collection services that report latency of
pod initialization from a cluster infrastructural perspective (i.e. agnostic of
per container characteristics like image size or payload) can utilize the
&lt;code>PodHasNetwork&lt;/code> condition to accurately generate Service Level Indicators
(SLIs). Certain operators or controllers that manage underlying pods may utilize
the &lt;code>PodHasNetwork&lt;/code> condition to optimize the set of actions performed when pods
repeatedly fail to come up.&lt;/p>
&lt;h3 id="how-is-this-different-from-the-existing-initialized-condition-reported-for-pods">How is this different from the existing Initialized condition reported for pods?&lt;/h3>
&lt;p>The kubelet sets the status of the existing &lt;code>Initialized&lt;/code> condition reported in
the status field of a pod depending on the presence of init containers in a pod.&lt;/p>
&lt;p>If a pod specifies init containers, the status of the &lt;code>Initialized&lt;/code> condition in
the pod status will not be set to &lt;code>&amp;quot;True&amp;quot;&lt;/code> until all init containers for the pod
have succeeded. However, init containers, configured by users, may have errors
(payload crashing, invalid image, etc) and the number of init containers
configured in a pod may vary across different workloads. Therefore,
cluster-wide, infrastructural SLIs around pod initialization cannot depend on
the &lt;code>Initialized&lt;/code> condition of pods.&lt;/p>
&lt;p>If a pod does not specify init containers, the status of the &lt;code>Initialized&lt;/code>
condition in the pod status is set to &lt;code>&amp;quot;True&amp;quot;&lt;/code> very early in the lifecycle of
the pod. This occurs before the kubelet initiates any pod runtime sandbox
creation and network configuration steps. As a result, a pod without init
containers will report the status of the &lt;code>Initialized&lt;/code> condition as &lt;code>&amp;quot;True&amp;quot;&lt;/code>
even if the container runtime is not able to successfully initialize the pod
sandbox environment.&lt;/p>
&lt;p>Relative to either situation above, the &lt;code>PodHasNetwork&lt;/code> condition surfaces more
accurate data around when the pod runtime sandbox was initialized with
networking configured so that the kubelet can proceed to launch user-configured
containers (including init containers) in the pod.&lt;/p>
&lt;h3 id="special-cases">Special Cases&lt;/h3>
&lt;p>If a pod specifies &lt;code>hostNetwork&lt;/code> as &lt;code>&amp;quot;True&amp;quot;&lt;/code>, the &lt;code>PodHasNetwork&lt;/code> condition is
set to &lt;code>&amp;quot;True&amp;quot;&lt;/code> based on successful creation of the pod sandbox while the
network configuration state of the pod sandbox is ignored. This is because the
CRI implementation typically skips any pod sandbox network configuration when
&lt;code>hostNetwork&lt;/code> is set to &lt;code>&amp;quot;True&amp;quot;&lt;/code> for a pod.&lt;/p>
&lt;p>A node agent may dynamically re-configure network interface(s) for a pod by
watching changes in pod annotations that specify additional networking
configuration (e.g. &lt;code>k8s.v1.cni.cncf.io/networks&lt;/code>). Dynamic updates of pod
networking configuration after the pod sandbox is initialized by Kubelet (in
coordination with a container runtime) are not reflected by the &lt;code>PodHasNetwork&lt;/code>
condition.&lt;/p>
&lt;h3 id="try-out-the-podhasnetwork-condition-for-pods">Try out the PodHasNetwork condition for pods&lt;/h3>
&lt;p>In order to have the kubelet report the &lt;code>PodHasNetwork&lt;/code> condition in the status
field of a pod, please enable the &lt;code>PodHasNetworkCondition&lt;/code> feature gate on the
kubelet.&lt;/p>
&lt;p>For a pod whose runtime sandbox has been successfully created and has networking
configured, the kubelet will report the &lt;code>PodHasNetwork&lt;/code> condition with status set to &lt;code>&amp;quot;True&amp;quot;&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl describe pod nginx1
Name: nginx1
Namespace: default
...
Conditions:
Type Status
PodHasNetwork True
Initialized True
Ready True
ContainersReady True
PodScheduled True
&lt;/code>&lt;/pre>&lt;p>For a pod whose runtime sandbox has not been created yet (and networking not
configured either), the kubelet will report the &lt;code>PodHasNetwork&lt;/code> condition with
status set to &lt;code>&amp;quot;False&amp;quot;&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl describe pod nginx2
Name: nginx2
Namespace: default
...
Conditions:
Type Status
PodHasNetwork False
Initialized True
Ready False
ContainersReady False
PodScheduled True
&lt;/code>&lt;/pre>&lt;h3 id="what-s-next">What’s next?&lt;/h3>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push the
reporting of the &lt;code>PodHasNetwork&lt;/code> condition to Beta in 1.26 or 1.27.&lt;/p>
&lt;h3 id="how-can-i-learn-more">How can I learn more?&lt;/h3>
&lt;p>Please check out the
&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/">documentation&lt;/a> for the
&lt;code>PodHasNetwork&lt;/code> condition to learn more about it and how it fits in relation to
other pod conditions.&lt;/p>
&lt;h3 id="how-to-get-involved">How to get involved?&lt;/h3>
&lt;p>This feature is driven by the SIG Node community. Please join us to connect with
the community and share your ideas and feedback around the above feature and
beyond. We look forward to hearing from you!&lt;/p>
&lt;h3 id="acknowledgements">Acknowledgements&lt;/h3>
&lt;p>We want to thank the following people for their insightful and helpful reviews
of the KEP and PRs around this feature: Derek Carr (@derekwaynecarr), Mrunal
Patel (@mrunalp), Dawn Chen (@dchen1107), Qiutong Song (@qiutongs), Ruiwen Zhao
(@ruiwen-zhao), Tim Bannister (@sftim), Danielle Lancashire (@endocrimes) and
Agam Dua (@agamdua).&lt;/p></description></item><item><title>Blog: Announcing the Auto-refreshing Official Kubernetes CVE Feed</title><link>https://kubernetes.io/blog/2022/09/12/k8s-cve-feed-alpha/</link><pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/12/k8s-cve-feed-alpha/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Pushkar Joglekar (VMware)&lt;/p>
&lt;p>A long-standing request from the Kubernetes community has been to have a
programmatic way for end users to keep track of Kubernetes security issues
(also called &amp;quot;CVEs&amp;quot;, after the database that tracks public security issues across
different products and vendors). Accompanying the release of Kubernetes v1.25,
we are excited to announce availability of such
a &lt;a href="https://kubernetes.io/docs/reference/issues-security/official-cve-feed/">feed&lt;/a> as an &lt;code>alpha&lt;/code>
feature. This blog will cover the background and scope of this new service.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>With the growing number of eyes on Kubernetes, the number of CVEs related to
Kubernetes have increased. Although most CVEs that directly, indirectly, or
transitively impact Kubernetes are regularly fixed, there is no single place for
the end users of Kubernetes to programmatically subscribe or pull the data of
fixed CVEs. Current options are either broken or incomplete.&lt;/p>
&lt;h2 id="scope">Scope&lt;/h2>
&lt;h3 id="what-this-does">What This Does&lt;/h3>
&lt;p>Create a periodically auto-refreshing, human and machine-readable list of
official Kubernetes CVEs&lt;/p>
&lt;h3 id="what-this-doesn-t-do">What This Doesn't Do&lt;/h3>
&lt;ul>
&lt;li>Triage and vulnerability disclosure will continue to be done by SRC (Security
Response Committee).&lt;/li>
&lt;li>Listing CVEs that are identified in build time dependencies and container
images are out of scope.&lt;/li>
&lt;li>Only official CVEs announced by the Kubernetes SRC will be published in the
feed.&lt;/li>
&lt;/ul>
&lt;h3 id="who-it-s-for">Who It's For&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>End Users&lt;/strong>: Persons or teams who &lt;em>use&lt;/em> Kubernetes to deploy applications
they own&lt;/li>
&lt;li>&lt;strong>Platform Providers&lt;/strong>: Persons or teams who &lt;em>manage&lt;/em> Kubernetes clusters&lt;/li>
&lt;li>&lt;strong>Maintainers&lt;/strong>: Persons or teams who &lt;em>create&lt;/em> and &lt;em>support&lt;/em> Kubernetes
releases through their work in Kubernetes Community - via various Special
Interest Groups and Committees.&lt;/li>
&lt;/ul>
&lt;h2 id="implementation-details">Implementation Details&lt;/h2>
&lt;p>A supporting
&lt;a href="https://kubernetes.dev/blog/2022/09/12/k8s-cve-feed-alpha/">contributor blog&lt;/a>
was published that describes in depth on how this CVE feed was implemented to
ensure the feed was reasonably protected against tampering and was automatically
updated after a new CVE was announced.&lt;/p>
&lt;h2 id="what-s-next">What's Next?&lt;/h2>
&lt;p>In order to graduate this feature, SIG Security
is gathering feedback from end users who are using this alpha feed.&lt;/p>
&lt;p>So in order to improve the feed in future Kubernetes Releases, if you have any
feedback, please let us know by adding a comment to
this &lt;a href="https://github.com/kubernetes/sig-security/issues/1">tracking issue&lt;/a> or
let us know on
&lt;a href="https://kubernetes.slack.com/archives/C01CUSVMHPY">#sig-security-tooling&lt;/a>
Kubernetes Slack channel.
(Join &lt;a href="https://slack.k8s.io">Kubernetes Slack here&lt;/a>)&lt;/p>
&lt;p>&lt;em>A special shout out and massive thanks to Neha Lohia
&lt;a href="https://github.com/nehalohia27">(@nehalohia27)&lt;/a> and Tim
Bannister &lt;a href="https://github.com/sftim">(@sftim)&lt;/a> for their stellar collaboration
for many months from &amp;quot;ideation to implementation&amp;quot; of this feature.&lt;/em>&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: KMS V2 Improvements</title><link>https://kubernetes.io/blog/2022/09/09/kms-v2-improvements/</link><pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/09/kms-v2-improvements/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Anish Ramasekar, Rita Zhang, Mo Khan, and Xander Grzywinski (Microsoft)&lt;/p>
&lt;p>With Kubernetes v1.25, SIG Auth is introducing a new &lt;code>v2alpha1&lt;/code> version of the Key Management Service (KMS) API. There are a lot of improvements in the works, and we're excited to be able to start down the path of a new and improved KMS!&lt;/p>
&lt;h2 id="what-is-kms">What is KMS?&lt;/h2>
&lt;p>One of the first things to consider when securing a Kubernetes cluster is encrypting persisted API data at rest. KMS provides an interface for a provider to utilize a key stored in an external key service to perform this encryption.&lt;/p>
&lt;p>Encryption at rest using KMS v1 has been a feature of Kubernetes since version v1.10, and is currently in beta as of version v1.12.&lt;/p>
&lt;h2 id="what-s-new-in-v2alpha1">What’s new in &lt;code>v2alpha1&lt;/code>?&lt;/h2>
&lt;p>While the original v1 implementation has been successful in helping Kubernetes users encrypt etcd data, it did fall short in a few key ways:&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Performance:&lt;/strong> When starting a cluster, all resources are serially fetched and decrypted to fill the &lt;code>kube-apiserver&lt;/code> cache. When using a KMS plugin, this can cause slow startup times due to the large number of requests made to the remote vault. In addition, there is the potential to hit API rate limits on external key services depending on how many encrypted resources exist in the cluster.&lt;/li>
&lt;li>&lt;strong>Key Rotation:&lt;/strong> With KMS v1, rotation of a key-encrypting key is a manual and error-prone process. It can be difficult to determine what encryption keys are in-use on a cluster.&lt;/li>
&lt;li>&lt;strong>Health Check &amp;amp; Status:&lt;/strong> Before the KMS v2 API, the &lt;code>kube-apiserver&lt;/code> was forced to make encrypt and decrypt calls as a proxy to determine if the KMS plugin is healthy. With cloud services these operations usually cost actual money with cloud service. Whatever the cost, those operations on their own do not provide a holistic view of the service's health.&lt;/li>
&lt;li>&lt;strong>Observability:&lt;/strong> Without some kind of trace ID, it's has been difficult to correlate events found in the various logs across &lt;code>kube-apiserver&lt;/code>, KMS, and KMS plugins.&lt;/li>
&lt;/ol>
&lt;p>The KMS v2 enhancement attempts to address all of these shortcomings, though not all planned features are implemented in the initial alpha release. Here are the improvements that arrived in Kubernetes v1.25:&lt;/p>
&lt;ol>
&lt;li>Support for KMS plugins that use a key hierarchy to reduce network requests made to the remote vault. To learn more, check out the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#key-hierachy">design details for how a KMS plugin can leverage key hierarchy&lt;/a>.&lt;/li>
&lt;li>Extra metadata is now tracked to allow a KMS plugin to communicate what key it is currently using with the &lt;code>kube-apiserver&lt;/code>, allowing for rotation without API server restart. Data stored in etcd follows a more standard proto format to allow external tools to observe its state. To learn more, check out the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#metadata">details for metadata&lt;/a>.&lt;/li>
&lt;li>A dedicated status API is used to communicate the health of the KMS plugin with the API server. To learn more, check out the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#status-api">details for status API&lt;/a>.&lt;/li>
&lt;li>To improve observability, a new &lt;code>UID&lt;/code> field is included in &lt;code>EncryptRequest&lt;/code> and &lt;code>DecryptRequest&lt;/code> of the v2 API. The UID is generated for each envelope operation. To learn more, check out the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-auth/3299-kms-v2-improvements#Observability">details for observability&lt;/a>.&lt;/li>
&lt;/ol>
&lt;h3 id="sequence-diagram">Sequence Diagram&lt;/h3>
&lt;h4 id="encrypt-request">Encrypt Request&lt;/h4>
&lt;!-- source - https://mermaid.ink/img/pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O)](https://mermaid.live/edit#pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O -->
&lt;figure class="diagram-large">
&lt;img src="https://kubernetes.io/images/blog/2022-09-09-kubernetes-1.25-kms-v2-improvements/kubernetes-1.25-encryption.svg"
alt="Sequence diagram for KMSv2 Encrypt"/>
&lt;/figure>
&lt;h4 id="decrypt-request">Decrypt Request&lt;/h4>
&lt;!-- source - https://mermaid.ink/img/pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O)](https://mermaid.live/edit#pako:eNrNVD1v2zAQ_SsEC0GLkxgt2kEIvEQeCo8tOgkoTuTJIiyRypFMIwj67yUlxx-w0CLoUg0a7t29e3eP5MCFkcgzniSD0splbEhdjS2mGUtLsJiu2Bz4AaSgbNAGZGBpR6oF6p9MYyjmfvj08YvAzzH9CH3HV3eGq6qaqK6C6_U6HccxSQpt8dmjFpgr2BO0hWbh64CcEqoD7Rg6IW-jB18idMoivSAtwK3tGr9XeoHv1SFpaELKDF5R3W02p9qMBWHUd45RFGndnA-NY94qvWcH7FmtkIBE3c_gRPhGsEyWb3fsl3I1a4yAhu22u-XSC6Hn4lPNTEHYGofXHBd1iwJQ_q3zRY0AUeM7Ki93mQV5zpO-WKPtTHCcPZb0sGFDwYMnNVI8HAXPWMEfz53CmjYFX8Ul_1RyAs_Tsq_5BM5EBQetjQOnAnskCsxB1X1UQxod2ntlHibpdwc83LQ6DRU4x3GeDJugM5D-2eokYcITYThXJdbwogy9w8z8H23M_xcbbg04rVHL5VsWr3XGrDOEt8JAy6Ux45-veIvUgpLh8RpipODTOzUrl1iBb8IYhR5Dqu8kONxKFfrwrIJg6oqDd-ZbrwXPHHl8Szo-QMes8Tffb72O ](https://mermaid.ink/img/pako:eNrVVU2P0zAQ_SsjoyggdXcrEHuIVr3QHlBvgDhFQtN40lhN7GA7C1GU_47jdOuUhi4HkKCn1DPzPkbPcscyxYklLIo6IYVNoIttQRXFCcQ7NBQvYDz4jFrgriTjKh3EtRYV6vadKpUeel-8eX2f0duh_Vj6RN9tKOd57qHODpfLZdz3fRSl0tDXhmRGa4F7jVUqwf1q1FZkokZp4dDsCGthSD-SnilXpi6bvZCXJUdJWmLpWsZiFIHIoVQZlrDdbEFIQCmVRSuUNAtwfiU0Rsg9FII06qxox0ksHZzMdFtb4lME8xPI2A7nqm9Wq5PMBDh5HNCDc2PhYafvVtClzMkuSA-bSlkCKXsIjOvNdpWyBRyo_SK4L46fsFfWOtVovHVQOWzGqQ9kaieI_NzIkbKpUsfhSJ2w20GslmTJ3Ap1593dHOhwoeLk22H2_ZPVK9uRkGFWUOj0q3laxfxanFX4JmwRcMI4lYZmmZyr32CbBCLwBRDPqqlSls5pPXWYndU9lfPH_F4Z91avk5Pk4c8ZzDScibNsGy0nuRyDE4JZlyjkJJeBdSaXYYHwfv2Xw_fLPLh7eYzEzN38b27n9I49m-P1ZYLhpcGKYEcFPgqlBxlWcWxfTTLyfKzX00z9gzE6hUFytmAV6QoFdy9bNxynzD9iIyOnHJvS0aeyd61NzdHShgurNEtydGFaMGys-tjKjCVWN_TUdHydjl39D0CLbdk)](https://mermaid.live/edit#pako:eNrVVU2P0zAQ_SsjoyggdXcrEHuIVr3QHlBvgDhFQtN40lhN7GA7C1GU_47jdOuUhi4HkKCn1DPzPkbPcscyxYklLIo6IYVNoIttQRXFCcQ7NBQvYDz4jFrgriTjKh3EtRYV6vadKpUeel-8eX2f0duh_Vj6RN9tKOd57qHODpfLZdz3fRSl0tDXhmRGa4F7jVUqwf1q1FZkokZp4dDsCGthSD-SnilXpi6bvZCXJUdJWmLpWsZiFIHIoVQZlrDdbEFIQCmVRSuUNAtwfiU0Rsg9FII06qxox0ksHZzMdFtb4lME8xPI2A7nqm9Wq5PMBDh5HNCDc2PhYafvVtClzMkuSA-bSlkCKXsIjOvNdpWyBRyo_SK4L46fsFfWOtVovHVQOWzGqQ9kaieI_NzIkbKpUsfhSJ2w20GslmTJ3Ap1593dHOhwoeLk22H2_ZPVK9uRkGFWUOj0q3laxfxanFX4JmwRcMI4lYZmmZyr32CbBCLwBRDPqqlSls5pPXWYndU9lfPH_F4Z91avk5Pk4c8ZzDScibNsGy0nuRyDE4JZlyjkJJeBdSaXYYHwfv2Xw_fLPLh7eYzEzN38b27n9I49m-P1ZYLhpcGKYEcFPgqlBxlWcWxfTTLyfKzX00z9gzE6hUFytmAV6QoFdy9bNxynzD9iIyOnHJvS0aeyd61NzdHShgurNEtydGFaMGys-tjKjCVWN_TUdHydjl39D0CLbdk -->
&lt;figure class="diagram-large">
&lt;img src="https://kubernetes.io/images/blog/2022-09-09-kubernetes-1.25-kms-v2-improvements/kubernetes-1.25-decryption.svg"
alt="Sequence diagram for KMSv2 Decrypt"/>
&lt;/figure>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>For Kubernetes v1.26, we expect to ship another alpha version. As of right now, the alpha API will be ready to be used by KMS plugin authors. We hope to include a reference plugin implementation with the next release, and you'll be able to try out the feature at that time.&lt;/p>
&lt;p>You can learn more about KMS v2 by reading &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption&lt;/a>. You can also follow along on the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-auth/3299-kms-v2-improvements/#readme">KEP&lt;/a> to track progress across the coming Kubernetes releases.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved&lt;/h2>
&lt;p>If you are interested in getting involved in the development of this feature or would like to share feedback, please reach out on the &lt;a href="https://kubernetes.slack.com/archives/C03035EH4VB">#sig-auth-kms-dev&lt;/a> channel on Kubernetes Slack.&lt;/p>
&lt;p>You are also welcome to join the bi-weekly &lt;a href="https://github.com/kubernetes/community/blob/master/sig-auth/README.md#meetings">SIG Auth meetings&lt;/a>, held every-other Wednesday.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>This feature has been an effort driven by contributors from several different companies. We would like to extend a huge thank you to everyone that contributed their time and effort to help make this possible.&lt;/p></description></item><item><title>Blog: Kubernetes’s IPTables Chains Are Not API</title><link>https://kubernetes.io/blog/2022/09/07/iptables-chains-not-api/</link><pubDate>Wed, 07 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/07/iptables-chains-not-api/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Dan Winship (Red Hat)&lt;/p>
&lt;p>Some Kubernetes components (such as kubelet and kube-proxy) create
iptables chains and rules as part of their operation. These chains
were never intended to be part of any Kubernetes API/ABI guarantees,
but some external components nonetheless make use of some of them (in
particular, using &lt;code>KUBE-MARK-MASQ&lt;/code> to mark packets as needing to be
masqueraded).&lt;/p>
&lt;p>As a part of the v1.25 release, SIG Network made this declaration
explicit: that (with one exception), the iptables chains that
Kubernetes creates are intended only for Kubernetes’s own internal
use, and third-party components should not assume that Kubernetes will
create any specific iptables chains, or that those chains will contain
any specific rules if they do exist.&lt;/p>
&lt;p>Then, in future releases, as part of &lt;a href="https://github.com/kubernetes/enhancements/issues/3178">KEP-3178&lt;/a>, we will begin phasing
out certain chains that Kubernetes itself no longer needs. Components
outside of Kubernetes itself that make use of &lt;code>KUBE-MARK-MASQ&lt;/code>,
&lt;code>KUBE-MARK-DROP&lt;/code>, or other Kubernetes-generated iptables chains should
start migrating away from them now.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>In addition to various service-specific iptables chains, kube-proxy
creates certain general-purpose iptables chains that it uses as part
of service proxying. In the past, kubelet also used iptables for a few
features (such as setting up &lt;code>hostPort&lt;/code> mapping for pods) and so it
also redundantly created some of the same chains.&lt;/p>
&lt;p>However, with &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">the removal of dockershim&lt;/a> in Kubernetes in 1.24,
kubelet now no longer ever uses any iptables rules for its own
purposes; the things that it used to use iptables for are now always
the responsibility of the container runtime or the network plugin, and
there is no reason for kubelet to be creating any iptables rules.&lt;/p>
&lt;p>Meanwhile, although &lt;code>iptables&lt;/code> is still the default kube-proxy backend
on Linux, it is unlikely to remain the default forever, since the
associated command-line tools and kernel APIs are essentially
deprecated, and no longer receiving improvements. (RHEL 9
&lt;a href="https://access.redhat.com/solutions/6739041">logs a warning&lt;/a> if you use the iptables API, even via
&lt;code>iptables-nft&lt;/code>.)&lt;/p>
&lt;p>Although as of Kubernetes 1.25 iptables kube-proxy remains popular,
and kubelet continues to create the iptables rules that it
historically created (despite no longer &lt;em>using&lt;/em> them), third party
software cannot assume that core Kubernetes components will keep
creating these rules in the future.&lt;/p>
&lt;h2 id="upcoming-changes">Upcoming changes&lt;/h2>
&lt;p>Starting a few releases from now, kubelet will no longer create the
following iptables chains in the &lt;code>nat&lt;/code> table:&lt;/p>
&lt;ul>
&lt;li>&lt;code>KUBE-MARK-DROP&lt;/code>&lt;/li>
&lt;li>&lt;code>KUBE-MARK-MASQ&lt;/code>&lt;/li>
&lt;li>&lt;code>KUBE-POSTROUTING&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Additionally, the &lt;code>KUBE-FIREWALL&lt;/code> chain in the &lt;code>filter&lt;/code> table will no
longer have the functionality currently associated with
&lt;code>KUBE-MARK-DROP&lt;/code> (and it may eventually go away entirely).&lt;/p>
&lt;p>This change will be phased in via the &lt;code>IPTablesOwnershipCleanup&lt;/code>
feature gate. That feature gate is available and can be manually
enabled for testing in Kubernetes 1.25. The current plan is that it
will become enabled-by-default in Kubernetes 1.27, though this may be
delayed to a later release. (It will not happen sooner than Kubernetes
1.27.)&lt;/p>
&lt;h2 id="what-to-do-if-you-use-kubernetes-s-iptables-chains">What to do if you use Kubernetes’s iptables chains&lt;/h2>
&lt;p>(Although the discussion below focuses on short-term fixes that are
still based on iptables, you should probably also start thinking about
eventually migrating to nftables or another API).&lt;/p>
&lt;h3 id="use-case-kube-mark-masq">If you use &lt;code>KUBE-MARK-MASQ&lt;/code>...&lt;/h3>
&lt;p>If you are making use of the &lt;code>KUBE-MARK-MASQ&lt;/code> chain to cause packets
to be masqueraded, you have two options: (1) rewrite your rules to use
&lt;code>-j MASQUERADE&lt;/code> directly, (2) create your own alternative “mark for
masquerade” chain.&lt;/p>
&lt;p>The reason kube-proxy uses &lt;code>KUBE-MARK-MASQ&lt;/code> is because there are lots
of cases where it needs to call both &lt;code>-j DNAT&lt;/code> and &lt;code>-j MASQUERADE&lt;/code> on
a packet, but it’s not possible to do both of those at the same time
in iptables; &lt;code>DNAT&lt;/code> must be called from the &lt;code>PREROUTING&lt;/code> (or &lt;code>OUTPUT&lt;/code>)
chain (because it potentially changes where the packet will be routed
to) while &lt;code>MASQUERADE&lt;/code> must be called from &lt;code>POSTROUTING&lt;/code> (because the
masqueraded source IP that it picks depends on what the final routing
decision was).&lt;/p>
&lt;p>In theory, kube-proxy could have one set of rules to match packets in
&lt;code>PREROUTING&lt;/code>/&lt;code>OUTPUT&lt;/code> and call &lt;code>-j DNAT&lt;/code>, and then have a second set
of rules to match the same packets in &lt;code>POSTROUTING&lt;/code> and call &lt;code>-j MASQUERADE&lt;/code>. But instead, for efficiency, it only matches them once,
during &lt;code>PREROUTING&lt;/code>/&lt;code>OUTPUT&lt;/code>, at which point it calls &lt;code>-j DNAT&lt;/code> and
then calls &lt;code>-j KUBE-MARK-MASQ&lt;/code> to set a bit on the kernel packet mark
as a reminder to itself. Then later, during &lt;code>POSTROUTING&lt;/code>, it has a
single rule that matches all previously-marked packets, and calls &lt;code>-j MASQUERADE&lt;/code> on them.&lt;/p>
&lt;p>If you have &lt;em>a lot&lt;/em> of rules where you need to apply both DNAT and
masquerading to the same packets like kube-proxy does, then you may
want a similar arrangement. But in many cases, components that use
&lt;code>KUBE-MARK-MASQ&lt;/code> are only doing it because they copied kube-proxy’s
behavior without understanding why kube-proxy was doing it that way.
Many of these components could easily be rewritten to just use
separate DNAT and masquerade rules. (In cases where no DNAT is
occurring then there is even less point to using &lt;code>KUBE-MARK-MASQ&lt;/code>;
just move your rules from &lt;code>PREROUTING&lt;/code> to &lt;code>POSTROUTING&lt;/code> and call &lt;code>-j MASQUERADE&lt;/code> directly.)&lt;/p>
&lt;h3 id="use-case-kube-mark-drop">If you use &lt;code>KUBE-MARK-DROP&lt;/code>...&lt;/h3>
&lt;p>The rationale for &lt;code>KUBE-MARK-DROP&lt;/code> is similar to the rationale for
&lt;code>KUBE-MARK-MASQ&lt;/code>: kube-proxy wanted to make packet-dropping decisions
alongside other decisions in the &lt;code>nat&lt;/code> &lt;code>KUBE-SERVICES&lt;/code> chain, but you
can only call &lt;code>-j DROP&lt;/code> from the &lt;code>filter&lt;/code> table. So instead, it uses
&lt;code>KUBE-MARK-DROP&lt;/code> to mark packets to be dropped later on.&lt;/p>
&lt;p>In general, the approach for removing a dependency on &lt;code>KUBE-MARK-DROP&lt;/code>
is the same as for removing a dependency on &lt;code>KUBE-MARK-MASQ&lt;/code>. In
kube-proxy’s case, it is actually quite easy to replace the usage of
&lt;code>KUBE-MARK-DROP&lt;/code> in the &lt;code>nat&lt;/code> table with direct calls to &lt;code>DROP&lt;/code> in the
&lt;code>filter&lt;/code> table, because there are no complicated interactions between
DNAT rules and drop rules, and so the drop rules can simply be moved
from &lt;code>nat&lt;/code> to &lt;code>filter&lt;/code>.&lt;/p>
&lt;p>In more complicated cases, it might be necessary to “re-match” the
same packets in both &lt;code>nat&lt;/code> and &lt;code>filter&lt;/code>.&lt;/p>
&lt;h3 id="use-case-iptables-mode">If you use Kubelet’s iptables rules to figure out &lt;code>iptables-legacy&lt;/code> vs &lt;code>iptables-nft&lt;/code>...&lt;/h3>
&lt;p>Components that manipulate host-network-namespace iptables rules from
inside a container need some way to figure out whether the host is
using the old &lt;code>iptables-legacy&lt;/code> binaries or the newer &lt;code>iptables-nft&lt;/code>
binaries (which talk to a different kernel API underneath).&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/iptables-wrappers/">&lt;code>iptables-wrappers&lt;/code>&lt;/a> module provides a way for such components to
autodetect the system iptables mode, but in the past it did this by
assuming that Kubelet will have created “a bunch” of iptables rules
before any containers start, and so it can guess which mode the
iptables binaries in the host filesystem are using by seeing which
mode has more rules defined.&lt;/p>
&lt;p>In future releases, Kubelet will no longer create many iptables rules,
so heuristics based on counting the number of rules present may fail.&lt;/p>
&lt;p>However, as of 1.24, Kubelet always creates a chain named
&lt;code>KUBE-IPTABLES-HINT&lt;/code> in the &lt;code>mangle&lt;/code> table of whichever iptables
subsystem it is using. Components can now look for this specific chain
to know which iptables subsystem Kubelet (and thus, presumably, the
rest of the system) is using.&lt;/p>
&lt;p>(Additionally, since Kubernetes 1.17, kubelet has created a chain
called &lt;code>KUBE-KUBELET-CANARY&lt;/code> in the &lt;code>mangle&lt;/code> table. While this chain
may go away in the future, it will of course still be there in older
releases, so in any recent version of Kubernetes, at least one of
&lt;code>KUBE-IPTABLES-HINT&lt;/code> or &lt;code>KUBE-KUBELET-CANARY&lt;/code> will be present.)&lt;/p>
&lt;p>The &lt;code>iptables-wrappers&lt;/code> package has &lt;a href="https://github.com/kubernetes-sigs/iptables-wrappers/pull/3">already been updated&lt;/a> with this new
heuristic, so if you were previously using that, you can rebuild your
container images with an updated version of that.&lt;/p>
&lt;h2 id="further-reading">Further reading&lt;/h2>
&lt;p>The project to clean up iptables chain ownership and deprecate the old
chains is tracked by &lt;a href="https://github.com/kubernetes/enhancements/issues/3178">KEP-3178&lt;/a>.&lt;/p></description></item><item><title>Blog: Introducing COSI: Object Storage Management using Kubernetes APIs</title><link>https://kubernetes.io/blog/2022/09/02/cosi-kubernetes-object-storage-management/</link><pubDate>Fri, 02 Sep 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/09/02/cosi-kubernetes-object-storage-management/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Sidhartha Mani (&lt;a href="https://min.io">Minio, Inc&lt;/a>)&lt;/p>
&lt;p>This article introduces the Container Object Storage Interface (COSI), a standard for provisioning and consuming object storage in Kubernetes. It is an alpha feature in Kubernetes v1.25.&lt;/p>
&lt;p>File and block storage are treated as first class citizens in the Kubernetes ecosystem via &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">Container Storage Interface&lt;/a> (CSI). Workloads using CSI volumes enjoy the benefits of portability across vendors and across Kubernetes clusters without the need to change application manifests. An equivalent standard does not exist for Object storage.&lt;/p>
&lt;p>Object storage has been rising in popularity in recent years as an alternative form of storage to filesystems and block devices. Object storage paradigm promotes disaggregation of compute and storage. This is done by making data available over the network, rather than locally. Disaggregated architectures allow compute workloads to be stateless, which consequently makes them easier to manage, scale and automate.&lt;/p>
&lt;h2 id="cosi">COSI&lt;/h2>
&lt;p>COSI aims to standardize consumption of object storage to provide the following benefits:&lt;/p>
&lt;ul>
&lt;li>Kubernetes Native - Use the Kubernetes API to provision, configure and manage buckets&lt;/li>
&lt;li>Self Service - A clear delineation between administration and operations (DevOps) to enable self-service capability for DevOps personnel&lt;/li>
&lt;li>Portability - Vendor neutrality enabled through portability across Kubernetes Clusters and across Object Storage vendors&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Portability across vendors is only possible when both vendors support a common datapath-API. Eg. it is possible to port from AWS S3 to Ceph, or AWS S3 to MinIO and back as they all use S3 API. In contrast, it is not possible to port from AWS S3 and Google Cloud’s GCS or vice versa.&lt;/em>&lt;/p>
&lt;h2 id="architecture">Architecture&lt;/h2>
&lt;p>COSI is made up of three components:&lt;/p>
&lt;ul>
&lt;li>COSI Controller Manager&lt;/li>
&lt;li>COSI Sidecar&lt;/li>
&lt;li>COSI Driver&lt;/li>
&lt;/ul>
&lt;p>The COSI Controller Manager acts as the main controller that processes changes to COSI API objects. It is responsible for fielding requests for bucket creation, updates, deletion and access management. One instance of the controller manager is required per kubernetes cluster. Only one is needed even if multiple object storage providers are used in the cluster.&lt;/p>
&lt;p>The COSI Sidecar acts as a translator between COSI API requests and vendor-specific COSI Drivers. This component uses a standardized gRPC protocol that vendor drivers are expected to satisfy.&lt;/p>
&lt;p>The COSI Driver is the vendor specific component that receives requests from the sidecar and calls the appropriate vendor APIs to create buckets, manage their lifecycle and manage access to them.&lt;/p>
&lt;h2 id="api">API&lt;/h2>
&lt;p>The COSI API is centered around buckets, since bucket is the unit abstraction for object storage. COSI defines three Kubernetes APIs aimed at managing them&lt;/p>
&lt;ul>
&lt;li>Bucket&lt;/li>
&lt;li>BucketClass&lt;/li>
&lt;li>BucketClaim&lt;/li>
&lt;/ul>
&lt;p>In addition, two more APIs for managing access to buckets are also defined:&lt;/p>
&lt;ul>
&lt;li>BucketAccess&lt;/li>
&lt;li>BucketAccessClass&lt;/li>
&lt;/ul>
&lt;p>In a nutshell, Bucket and BucketClaim can be considered to be similar to PersistentVolume and PersistentVolumeClaim respectively. The BucketClass’ counterpart in the file/block device world is StorageClass.&lt;/p>
&lt;p>Since Object Storage is always authenticated, and over the network, access credentials are required to access buckets. The two APIs, namely, BucketAccess and BucketAccessClass are used to denote access credentials and policies for authentication. More info about these APIs can be found in the official COSI proposal - &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support">https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1979-object-storage-support&lt;/a>&lt;/p>
&lt;h2 id="self-service">Self-Service&lt;/h2>
&lt;p>Other than providing kubernetes-API driven bucket management, COSI also aims to empower DevOps personnel to provision and manage buckets on their own, without admin intervention. This, further enabling dev teams to realize faster turn-around times and faster time-to-market.&lt;/p>
&lt;p>COSI achieves this by dividing bucket provisioning steps among two different stakeholders, namely the administrator (admin), and the cluster operator. The administrator will be responsible for setting broad policies and limits on how buckets are provisioned, and how access is obtained for them. The cluster operator will be free to create and utilize buckets within the limits set by the admin.&lt;/p>
&lt;p>For example, a cluster operator could use an admin policy could be used to restrict maximum provisioned capacity to 100GB, and developers would be allowed to create buckets and store data upto that limit. Similarly for access credentials, admins would be able to restrict who can access which buckets, and developers would be able to access all the buckets available to them.&lt;/p>
&lt;h2 id="portability">Portability&lt;/h2>
&lt;p>The third goal of COSI is to achieve vendor neutrality for bucket management. COSI enables two kinds of portability:&lt;/p>
&lt;ul>
&lt;li>Cross Cluster&lt;/li>
&lt;li>Cross Provider&lt;/li>
&lt;/ul>
&lt;p>Cross Cluster portability is allowing buckets provisioned in one cluster to be available in another cluster. This is only valid when the object storage backend itself is accessible from both clusters.&lt;/p>
&lt;p>Cross-provider portability is about allowing organizations or teams to move from one object storage provider to another seamlessly, and without requiring changes to application definitions (PodTemplates, StatefulSets, Deployment and so on). This is only possible if the source and destination providers use the same data.&lt;/p>
&lt;p>&lt;em>COSI does not handle data migration as it is outside of its scope. In case porting between providers requires data to be migrated as well, then other measures need to be taken to ensure data availability.&lt;/em>&lt;/p>
&lt;h2 id="what-s-next">What’s next&lt;/h2>
&lt;p>The amazing sig-storage-cosi community has worked hard to bring the COSI standard to alpha status. We are looking forward to onboarding a lot of vendors to write COSI drivers and become COSI compatible!&lt;/p>
&lt;p>We want to add more authentication mechanisms for COSI buckets, we are designing advanced bucket sharing primitives, multi-cluster bucket management and much more. Lots of great ideas and opportunities ahead!&lt;/p>
&lt;p>Stay tuned for what comes next, and if you have any questions, comments or suggestions&lt;/p>
&lt;ul>
&lt;li>Chat with us on the Kubernetes &lt;a href="https://kubernetes.slack.com/archives/C017EGC1C6N">Slack:#sig-storage-cosi&lt;/a>&lt;/li>
&lt;li>Join our &lt;a href="https://zoom.us/j/614261834?pwd=Sk1USmtjR2t0MUdjTGVZeVVEV1BPQT09">Zoom meeting&lt;/a>, every Thursday at 10:00 Pacific Time&lt;/li>
&lt;li>Participate in the &lt;a href="https://github.com/kubernetes/enhancements/pull/2813">bucket API proposal PR&lt;/a> to add your ideas, suggestions and more.&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes 1.25: cgroup v2 graduates to GA</title><link>https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/</link><pubDate>Wed, 31 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/31/cgroupv2-ga-1-25/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong>: David Porter (Google), Mrunal Patel (Red Hat)&lt;/p>
&lt;p>Kubernetes 1.25 brings cgroup v2 to GA (general availability), letting the
&lt;a href="https://kubernetes.io/docs/concepts/overview/components/#kubelet">kubelet&lt;/a> use the latest container resource
management capabilities.&lt;/p>
&lt;h2 id="what-are-cgroups">What are cgroups?&lt;/h2>
&lt;p>Effective &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/">resource management&lt;/a> is a
critical aspect of Kubernetes. This involves managing the finite resources in
your nodes, such as CPU, memory, and storage.&lt;/p>
&lt;p>&lt;em>cgroups&lt;/em> are a Linux kernel capability that establish resource management
functionality like limiting CPU usage or setting memory limits for running
processes.&lt;/p>
&lt;p>When you use the resource management capabilities in Kubernetes, such as configuring
&lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">requests and limits for Pods and containers&lt;/a>,
Kubernetes uses cgroups to enforce your resource requests and limits.&lt;/p>
&lt;p>The Linux kernel offers two versions of cgroups: cgroup v1 and cgroup v2.&lt;/p>
&lt;h2 id="what-is-cgroup-v2">What is cgroup v2?&lt;/h2>
&lt;p>cgroup v2 is the latest version of the Linux cgroup API. cgroup v2 provides a
unified control system with enhanced resource management capabilities.&lt;/p>
&lt;p>cgroup v2 has been in development in the Linux Kernel since 2016 and in recent
years has matured across the container ecosystem. With Kubernetes 1.25, cgroup
v2 support has graduated to general availability.&lt;/p>
&lt;p>Many recent releases of Linux distributions have switched over to cgroup v2 by
default so it's important that Kubernetes continues to work well on these new
updated distros.&lt;/p>
&lt;p>cgroup v2 offers several improvements over cgroup v1, such as the following:&lt;/p>
&lt;ul>
&lt;li>Single unified hierarchy design in API&lt;/li>
&lt;li>Safer sub-tree delegation to containers&lt;/li>
&lt;li>Newer features like &lt;a href="https://www.kernel.org/doc/html/latest/accounting/psi.html">Pressure Stall Information&lt;/a>&lt;/li>
&lt;li>Enhanced resource allocation management and isolation across multiple resources
&lt;ul>
&lt;li>Unified accounting for different types of memory allocations (network and kernel memory, etc)&lt;/li>
&lt;li>Accounting for non-immediate resource changes such as page cache write backs&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>Some Kubernetes features exclusively use cgroup v2 for enhanced resource
management and isolation. For example,
the &lt;a href="https://kubernetes.io/blog/2021/11/26/qos-memory-resources/">MemoryQoS feature&lt;/a> improves
memory utilization and relies on cgroup v2 functionality to enable it. New
resource management features in the kubelet will also take advantage of the new
cgroup v2 features moving forward.&lt;/p>
&lt;h2 id="how-do-you-use-cgroup-v2">How do you use cgroup v2?&lt;/h2>
&lt;p>Many Linux distributions are switching to cgroup v2 by default; you might start
using it the next time you update the Linux version of your control plane and
nodes!&lt;/p>
&lt;p>Using a Linux distribution that uses cgroup v2 by default is the recommended
method. Some of the popular Linux distributions that use cgroup v2 include the
following:&lt;/p>
&lt;ul>
&lt;li>Container Optimized OS (since M97)&lt;/li>
&lt;li>Ubuntu (since 21.10)&lt;/li>
&lt;li>Debian GNU/Linux (since Debian 11 Bullseye)&lt;/li>
&lt;li>Fedora (since 31)&lt;/li>
&lt;li>Arch Linux (since April 2021)&lt;/li>
&lt;li>RHEL and RHEL-like distributions (since 9)&lt;/li>
&lt;/ul>
&lt;p>To check if your distribution uses cgroup v2 by default,
refer to &lt;a href="https://kubernetes.io/docs/concepts/architecture/cgroups/#check-cgroup-version">Check your cgroup version&lt;/a> or
consult your distribution's documentation.&lt;/p>
&lt;p>If you're using a managed Kubernetes offering, consult your provider to
determine how they're adopting cgroup v2, and whether you need to take action.&lt;/p>
&lt;p>To use cgroup v2 with Kubernetes, you must meet the following requirements:&lt;/p>
&lt;ul>
&lt;li>Your Linux distribution enables cgroup v2 on kernel version 5.8 or later&lt;/li>
&lt;li>Your container runtime supports cgroup v2. For example:
&lt;ul>
&lt;li>&lt;a href="https://containerd.io/">containerd&lt;/a> v1.4 or later&lt;/li>
&lt;li>&lt;a href="https://cri-o.io/">cri-o&lt;/a> v1.20 or later&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>The kubelet and the container runtime are configured to use the &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup driver&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>The kubelet and container runtime use a &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#cgroup-drivers">cgroup driver&lt;/a>
to set cgroup paramaters. When using cgroup v2, it's strongly recommended that both
the kubelet and your container runtime use the
&lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup driver&lt;/a>,
so that there's a single cgroup manager on the system. To configure the kubelet
and the container runtime to use the driver, refer to the
&lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver">systemd cgroup driver documentation&lt;/a>.&lt;/p>
&lt;h2 id="migrate-to-cgroup-v2">Migrate to cgroup v2&lt;/h2>
&lt;p>When you run Kubernetes with a Linux distribution that enables cgroup v2, the
kubelet should automatically adapt without any additional configuration
required, as long as you meet the requirements.&lt;/p>
&lt;p>In most cases, you won't see a difference in the user experience when you
switch to using cgroup v2 unless your users access the cgroup file system
directly.&lt;/p>
&lt;p>If you have applications that access the cgroup file system directly, either on
the node or from inside a container, you must update the applications to use
the cgroup v2 API instead of the cgroup v1 API.&lt;/p>
&lt;p>Scenarios in which you might need to update to cgroup v2 include the following:&lt;/p>
&lt;ul>
&lt;li>If you run third-party monitoring and security agents that depend on the cgroup file system, update the
agents to versions that support cgroup v2.&lt;/li>
&lt;li>If you run &lt;a href="https://github.com/google/cadvisor">cAdvisor&lt;/a> as a stand-alone
DaemonSet for monitoring pods and containers, update it to v0.43.0 or later.&lt;/li>
&lt;li>If you deploy Java applications with the JDK, prefer to use JDK 11.0.16 and
later or JDK 15 and later, which &lt;a href="https://bugs.openjdk.org/browse/JDK-8230305">fully support cgroup v2&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h2 id="learn-more">Learn more&lt;/h2>
&lt;ul>
&lt;li>Read the &lt;a href="https://kubernetes.io/docs/concepts/architecture/cgroups/">Kubernetes cgroup v2 documentation&lt;/a>&lt;/li>
&lt;li>Read the enhancement proposal, &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2254-cgroup-v2/README.md">KEP 2254&lt;/a>&lt;/li>
&lt;li>Learn more about
&lt;a href="https://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups&lt;/a> on Linux Manual Pages
and &lt;a href="https://docs.kernel.org/admin-guide/cgroup-v2.html">cgroup v2&lt;/a> on the Linux Kernel documentation&lt;/li>
&lt;/ul>
&lt;h2 id="get-involved">Get involved&lt;/h2>
&lt;p>Your feedback is always welcome! SIG Node meets regularly and are available in
the &lt;code>#sig-node&lt;/code> channel in the Kubernetes &lt;a href="https://slack.k8s.io/">Slack&lt;/a>, or
using the SIG &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node#contact">mailing list&lt;/a>.&lt;/p>
&lt;p>cgroup v2 has had a long journey and is a great example of open source
community collaboration across the industry because it required work across the
stack, from the Linux Kernel to systemd to various container runtimes, and (of
course) Kubernetes.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>We would like to thank &lt;a href="https://github.com/giuseppe">Giuseppe Scrivano&lt;/a> who
initiated cgroup v2 support in Kubernetes, and reviews and leadership from the
SIG Node community including chairs &lt;a href="https://github.com/dchen1107">Dawn Chen&lt;/a>
and &lt;a href="https://github.com/derekwaynecarr">Derek Carr&lt;/a>.&lt;/p>
&lt;p>We'd also like to thank the maintainers of container runtimes like Docker,
containerd and CRI-O, and the maintainers of components like
&lt;a href="https://github.com/google/cadvisor">cAdvisor&lt;/a>
and &lt;a href="https://github.com/opencontainers/runc">runc, libcontainer&lt;/a>,
which underpin many container runtimes. Finally, this wouldn't have been
possible without support from systemd and upstream Linux Kernel maintainers.&lt;/p>
&lt;p>It's a team effort!&lt;/p></description></item><item><title>Blog: Kubernetes 1.25: CSI Inline Volumes have graduated to GA</title><link>https://kubernetes.io/blog/2022/08/29/csi-inline-volumes-ga/</link><pubDate>Mon, 29 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/29/csi-inline-volumes-ga/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Jonathan Dobson (Red Hat)&lt;/p>
&lt;p>CSI Inline Volumes were introduced as an alpha feature in Kubernetes 1.15 and have been beta since 1.16. We are happy to announce that this feature has graduated to General Availability (GA) status in Kubernetes 1.25.&lt;/p>
&lt;p>CSI Inline Volumes are similar to other ephemeral volume types, such as &lt;code>configMap&lt;/code>, &lt;code>downwardAPI&lt;/code> and &lt;code>secret&lt;/code>. The important difference is that the storage is provided by a CSI driver, which allows the use of ephemeral storage provided by third-party vendors. The volume is defined as part of the pod spec and follows the lifecycle of the pod, meaning the volume is created once the pod is scheduled and destroyed when the pod is destroyed.&lt;/p>
&lt;h2 id="what-s-new-in-1-25">What's new in 1.25?&lt;/h2>
&lt;p>There are a couple of new bug fixes related to this feature in 1.25, and the &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">CSIInlineVolume feature gate&lt;/a> has been locked to &lt;code>True&lt;/code> with the graduation to GA. There are no new API changes, so users of this feature during beta should not notice any significant changes aside from these bug fixes.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/89290">#89290 - CSI inline volumes should support fsGroup&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/79980">#79980 - CSI volume reconstruction does not work for ephemeral volumes&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="when-to-use-this-feature">When to use this feature&lt;/h2>
&lt;p>CSI inline volumes are meant for simple local volumes that should follow the lifecycle of the pod. They may be useful for providing secrets, configuration data, or other special-purpose storage to the pod from a CSI driver.&lt;/p>
&lt;p>A CSI driver is not suitable for inline use when:&lt;/p>
&lt;ul>
&lt;li>The volume needs to persist longer than the lifecycle of a pod&lt;/li>
&lt;li>Volume snapshots, cloning, or volume expansion are required&lt;/li>
&lt;li>The CSI driver requires &lt;code>volumeAttributes&lt;/code> that should be restricted to an administrator&lt;/li>
&lt;/ul>
&lt;h2 id="how-to-use-this-feature">How to use this feature&lt;/h2>
&lt;p>In order to use this feature, the &lt;code>CSIDriver&lt;/code> spec must explicitly list &lt;code>Ephemeral&lt;/code> as one of the supported &lt;code>volumeLifecycleModes&lt;/code>. Here is a simple example from the &lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secrets Store CSI Driver&lt;/a>.&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
name: secrets-store.csi.k8s.io
spec:
podInfoOnMount: true
attachRequired: false
volumeLifecycleModes:
- Ephemeral
&lt;/code>&lt;/pre>&lt;p>Any pod spec may then reference that CSI driver to create an inline volume, as in this example.&lt;/p>
&lt;pre tabindex="0">&lt;code>kind: Pod
apiVersion: v1
metadata:
name: my-csi-app-inline
spec:
containers:
- name: my-frontend
image: busybox
volumeMounts:
- name: secrets-store-inline
mountPath: &amp;#34;/mnt/secrets-store&amp;#34;
readOnly: true
command: [ &amp;#34;sleep&amp;#34;, &amp;#34;1000000&amp;#34; ]
volumes:
- name: secrets-store-inline
csi:
driver: secrets-store.csi.k8s.io
readOnly: true
volumeAttributes:
secretProviderClass: &amp;#34;my-provider&amp;#34;
&lt;/code>&lt;/pre>&lt;p>If the driver supports any volume attributes, you can provide these as part of the &lt;code>spec&lt;/code> for the Pod as well:&lt;/p>
&lt;pre tabindex="0">&lt;code> csi:
driver: block.csi.vendor.example
volumeAttributes:
foo: bar
&lt;/code>&lt;/pre>&lt;h2 id="example-use-cases">Example Use Cases&lt;/h2>
&lt;p>Two existing CSI drivers that support the &lt;code>Ephemeral&lt;/code> volume lifecycle mode are the Secrets Store CSI Driver and the Cert-Manager CSI Driver.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secrets Store CSI Driver&lt;/a> allows users to mount secrets from external secret stores into a pod as an inline volume. This can be useful when the secrets are stored in an external managed service or Vault instance.&lt;/p>
&lt;p>The &lt;a href="https://github.com/cert-manager/csi-driver">Cert-Manager CSI Driver&lt;/a> works along with &lt;a href="https://cert-manager.io/">cert-manager&lt;/a> to seamlessly request and mount certificate key pairs into a pod. This allows the certificates to be renewed and updated in the application pod automatically.&lt;/p>
&lt;h2 id="security-considerations">Security Considerations&lt;/h2>
&lt;p>Special consideration should be given to which CSI drivers may be used as inline volumes. &lt;code>volumeAttributes&lt;/code> are typically controlled through the &lt;code>StorageClass&lt;/code>, and may contain attributes that should remain restricted to the cluster administrator. Allowing a CSI driver to be used for inline ephmeral volumes means that any user with permission to create pods may also provide &lt;code>volumeAttributes&lt;/code> to the driver through a pod spec.&lt;/p>
&lt;p>Cluster administrators may choose to omit (or remove) &lt;code>Ephemeral&lt;/code> from &lt;code>volumeLifecycleModes&lt;/code> in the CSIDriver spec to prevent the driver from being used as an inline ephemeral volume, or use an &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission webhook&lt;/a> to restrict how the driver is used.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;p>For more information on this feature, see:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes">Kubernetes documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html">CSI documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/596-csi-inline-volumes/README.md">KEP-596&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/">Beta blog post for CSI Inline Volumes&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes v1.25: Pod Security Admission Controller in Stable</title><link>https://kubernetes.io/blog/2022/08/25/pod-security-admission-stable/</link><pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/25/pod-security-admission-stable/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Tim Allclair (Google), Sam Stoelinga (Google)&lt;/p>
&lt;p>The release of Kubernetes v1.25 marks a major milestone for Kubernetes out-of-the-box pod security
controls: Pod Security admission (PSA) graduated to stable, and Pod Security Policy (PSP) has been
removed.
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PSP was deprecated in Kubernetes v1.21&lt;/a>,
and no longer functions in Kubernetes v1.25 and later.&lt;/p>
&lt;p>The Pod Security admission controller replaces PodSecurityPolicy, making it easier to enforce predefined
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a> by
simply adding a label to a namespace. The Pod Security Standards are maintained by the K8s
community, which means you automatically get updated security policies whenever new
security-impacting Kubernetes features are introduced.&lt;/p>
&lt;h2 id="what-s-new-since-beta">What’s new since Beta?&lt;/h2>
&lt;p>Pod Security Admission hasn’t changed much since the Beta in Kubernetes v1.23. The focus has been on
improving the user experience, while continuing to maintain a high quality bar.&lt;/p>
&lt;h3 id="improved-violation-messages">Improved violation messages&lt;/h3>
&lt;p>We improved violation messages so that you get
&lt;a href="https://github.com/kubernetes/kubernetes/pull/107698">fewer duplicate messages&lt;/a>. For example,
instead of the following message when the Baseline and Restricted policies check the same
capability:&lt;/p>
&lt;pre tabindex="0">&lt;code>pods &amp;#34;admin-pod&amp;#34; is forbidden: violates PodSecurity &amp;#34;restricted:latest&amp;#34;: non-default capabilities (container &amp;#34;admin&amp;#34; must not include &amp;#34;SYS_ADMIN&amp;#34; in securityContext.capabilities.add), unrestricted capabilities (container &amp;#34;admin&amp;#34; must not include &amp;#34;SYS_ADMIN&amp;#34; in securityContext.capabilities.add)
&lt;/code>&lt;/pre>&lt;p>You get this message:&lt;/p>
&lt;pre tabindex="0">&lt;code>pods &amp;#34;admin-pod&amp;#34; is forbidden: violates PodSecurity &amp;#34;restricted:latest&amp;#34;: unrestricted capabilities (container &amp;#34;admin&amp;#34; must not include &amp;#34;SYS_ADMIN&amp;#34; in securityContext.capabilities.add)
&lt;/code>&lt;/pre>&lt;h3 id="improved-namespace-warnings">Improved namespace warnings&lt;/h3>
&lt;p>When you modify the &lt;code>enforce&lt;/code> Pod Security labels on a namespace, the Pod Security
admission controller checks all existing pods for
violations and surfaces a &lt;a href="https://kubernetes.io/blog/2020/09/03/warnings/">warning&lt;/a> if any are out of compliance. These
&lt;a href="https://github.com/kubernetes/kubernetes/pull/105889">warnings are now aggregated&lt;/a> for pods with
identical violations, making large namespaces with many replicas much more manageable. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>Warning: frontend-h23gf2: allowPrivilegeEscalation != false
Warning: myjob-g342hj (and 6 other pods): host namespaces, allowPrivilegeEscalation != false Warning: backend-j23h42 (and 1 other pod): non-default capabilities, unrestricted capabilities
&lt;/code>&lt;/pre>&lt;p>Additionally, when you apply a non-privileged label to a namespace that has been
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/#exemptions">configured to be exempt&lt;/a>,
you will now get a warning alerting you to this fact:&lt;/p>
&lt;pre tabindex="0">&lt;code>Warning: namespace &amp;#39;kube-system&amp;#39; is exempt from Pod Security, and the policy (enforce=baseline:latest) will be ignored
&lt;/code>&lt;/pre>&lt;h3 id="changes-to-the-pod-security-standards">Changes to the Pod Security Standards&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a>,
which Pod Security admission enforces, have been updated with support for the new Pod OS
field. In v1.25 and later, if you use the Restricted policy, the following Linux-specific restrictions will no
longer be required if you explicitly set the pod's &lt;code>.spec.os.name&lt;/code> field to &lt;code>windows&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>Seccomp - The &lt;code>seccompProfile.type&lt;/code> field for Pod and container security contexts&lt;/li>
&lt;li>Privilege escalation - The &lt;code>allowPrivilegeEscalation&lt;/code> field on container security contexts&lt;/li>
&lt;li>Capabilities - The requirement to drop &lt;code>ALL&lt;/code> capabilities in the &lt;code>capabilities&lt;/code> field on containers&lt;/li>
&lt;/ul>
&lt;p>In Kubernetes v1.23 and earlier, the kubelet didn't enforce the Pod OS field.
If your cluster includes nodes running a v1.23 or older kubelet, you should explicitly
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces">pin Restricted policies&lt;/a>
to a version prior to v1.25.&lt;/p>
&lt;h2 id="migrating-from-podsecuritypolicy-to-the-pod-security-admission-controller">Migrating from PodSecurityPolicy to the Pod Security admission controller&lt;/h2>
&lt;p>For instructions to migrate from PodSecurityPolicy to the Pod Security admission controller, and
for help choosing a migration strategy, refer to the
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">migration guide&lt;/a>.
We're also developing a tool called
&lt;a href="https://github.com/kubernetes-sigs/pspmigrator">pspmigrator&lt;/a> to automate parts
of the migration process.&lt;/p>
&lt;p>We'll be talking about PSP migration in more detail at our upcoming KubeCon 2022 NA talk,
&lt;a href="https://sched.co/182Jx">&lt;em>Migrating from Pod Security Policy&lt;/em>&lt;/a>. Use the
&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/program/schedule/">KubeCon NA schedule&lt;/a>
to learn more.&lt;/p></description></item><item><title>Blog: PodSecurityPolicy: The Historical Context</title><link>https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/</link><pubDate>Tue, 23 Aug 2022 15:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2022/08/23/podsecuritypolicy-the-historical-context/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Mahé Tardy (Quarkslab)&lt;/p>
&lt;p>The PodSecurityPolicy (PSP) admission controller has been removed, as of
Kubernetes v1.25. Its deprecation was announced and detailed in the blog post
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>,
published for the Kubernetes v1.21 release.&lt;/p>
&lt;p>This article aims to provide historical context on the birth and evolution of
PSP, explain why the feature never made it to stable, and show why it was
removed and replaced by Pod Security admission control.&lt;/p>
&lt;p>PodSecurityPolicy, like other specialized admission control plugins, provided
fine-grained permissions on specific fields concerning the pod security settings
as a built-in policy API. It acknowledged that cluster administrators and
cluster users are usually not the same people, and that creating workloads in
the form of a Pod or any resource that will create a Pod should not equal being
&amp;quot;root on the cluster&amp;quot;. It could also encourage best practices by configuring
more secure defaults through mutation and decoupling low-level Linux security
decisions from the deployment process.&lt;/p>
&lt;h2 id="the-birth-of-podsecuritypolicy">The birth of PodSecurityPolicy&lt;/h2>
&lt;p>PodSecurityPolicy originated from OpenShift's SecurityContextConstraints
(SCC) that were in the very first release of the Red Hat OpenShift Container Platform,
even before Kubernetes 1.0. PSP was a stripped-down version of the SCC.&lt;/p>
&lt;p>The origin of the creation of PodSecurityPolicy is difficult to track, notably
because it was mainly added before Kubernetes Enhancements Proposal (KEP)
process, when design proposals were still a thing. Indeed, the archive of the final
&lt;a href="https://github.com/kubernetes/design-proposals-archive/blob/main/auth/pod-security-policy.md">design proposal&lt;/a>
is still available. Nevertheless, a &lt;a href="https://github.com/kubernetes/enhancements/issues/5">KEP issue number five&lt;/a>
was created after the first pull requests were merged.&lt;/p>
&lt;p>Before adding the first piece of code that created PSP, two main pull
requests were merged into Kubernetes, a &lt;a href="https://github.com/kubernetes/kubernetes/pull/7343">&lt;code>SecurityContext&lt;/code> subresource&lt;/a>
that defined new fields on pods' containers, and the first iteration of the &lt;a href="https://github.com/kubernetes/kubernetes/pull/7101">ServiceAccount&lt;/a>
API.&lt;/p>
&lt;p>Kubernetes 1.0 was released on 10 July 2015 without any mechanism to restrict the
security context and sensitive options of workloads, other than an alpha-quality
SecurityContextDeny admission plugin (then known as &lt;code>scdeny&lt;/code>).
The &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#securitycontextdeny">SecurityContextDeny plugin&lt;/a>
is still in Kubernetes today (as an alpha feature) and creates an admission controller that
prevents the usage of some fields in the security context.&lt;/p>
&lt;p>The roots of the PodSecurityPolicy were added with
&lt;a href="https://github.com/kubernetes/kubernetes/pull/7893">the very first pull request on security policy&lt;/a>,
which added the design proposal with the new PSP object, based on the SCC (Security Context Constraints). It
was a long discussion of nine months, with back and forth from OpenShift's SCC,
many rebases, and the rename to PodSecurityPolicy that finally made it to
upstream Kubernetes in February 2016. Now that the PSP object
had been created, the next step was to add an admission controller that could enforce
these policies. The first step was to add the admission
&lt;a href="https://github.com/kubernetes/kubernetes/pull/7893#issuecomment-180410539">without taking into account the users or groups&lt;/a>.
A specific &lt;a href="https://github.com/kubernetes/kubernetes/issues/23217">issue to bring PodSecurityPolicy to a usable state&lt;/a>
was added to keep track of the progress and a first version of the admission
controller was merged in &lt;a href="https://github.com/kubernetes/kubernetes/pull/24600">pull request named PSP admission&lt;/a>
in May 2016. Then around two months later, Kubernetes 1.3 was released.&lt;/p>
&lt;p>Here is a timeline that recaps the main pull requests of the birth of the
PodSecurityPolicy and its admission controller with 1.0 and 1.3 releases as
reference points.&lt;/p>
&lt;figure>
&lt;img src="./timeline.svg"
alt="Timeline of the PodSecurityPolicy creation pull requests"/>
&lt;/figure>
&lt;p>After that, the PSP admission controller was enhanced by adding what was initially
left aside. &lt;a href="https://github.com/kubernetes/kubernetes/pull/33080">The authorization mechanism&lt;/a>,
merged in early November 2016 allowed administrators to use multiple policies
in a cluster to grant different levels of access for different types of users.
Later, a &lt;a href="https://github.com/kubernetes/kubernetes/pull/52849">pull request&lt;/a>
merged in October 2017 fixed &lt;a href="https://github.com/kubernetes/kubernetes/issues/36184">a design issue&lt;/a>
on ordering PodSecurityPolicies between mutating and alphabetical order, and continued to
build the PSP admission as we know it. After that, many improvements and fixes
followed to build the PodSecurityPolicy feature of recent Kubernetes releases.&lt;/p>
&lt;h2 id="the-rise-of-pod-security-admission">The rise of Pod Security Admission&lt;/h2>
&lt;p>Despite the crucial issue it was trying to solve, PodSecurityPolicy presented
some major flaws:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Flawed authorization model&lt;/strong> - users can create a pod if they have the
&lt;strong>use&lt;/strong> verb on the PSP that allows that pod or the pod's service account has
the &lt;strong>use&lt;/strong> permission on the allowing PSP.&lt;/li>
&lt;li>&lt;strong>Difficult to roll out&lt;/strong> - PSP fail-closed. That is, in the absence of a policy,
all pods are denied. It mostly means that it cannot be enabled by default and
that users have to add PSPs for all workloads before enabling the feature,
thus providing no audit mode to discover which pods would not be allowed by
the new policy. The opt-in model also leads to insufficient test coverage and
frequent breakage due to cross-feature incompatibility. And unlike RBAC,
there was no strong culture of shipping PSP manifests with projects.&lt;/li>
&lt;li>&lt;strong>Inconsistent unbounded API&lt;/strong> - the API has grown with lots of
inconsistencies notably because of many requests for niche use cases: e.g.
labels, scheduling, fine-grained volume controls, etc. It has poor
composability with a weak prioritization model, leading to unexpected
mutation priority. It made it really difficult to combine PSP with other
third-party admission controllers.&lt;/li>
&lt;li>&lt;strong>Require security knowledge&lt;/strong> - effective usage still requires an
understanding of Linux security primitives. e.g. MustRunAsNonRoot +
AllowPrivilegeEscalation.&lt;/li>
&lt;/ul>
&lt;p>The experience with PodSecurityPolicy concluded that most users care for two or three
policies, which led to the creation of the &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-standards/">Pod Security Standards&lt;/a>,
that define three policies:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Privileged&lt;/strong> - unrestricted policy.&lt;/li>
&lt;li>&lt;strong>Baseline&lt;/strong> - minimally restrictive policy, allowing the default pod
configuration.&lt;/li>
&lt;li>&lt;strong>Restricted&lt;/strong> - security best practice policy.&lt;/li>
&lt;/ul>
&lt;p>The replacement for PSP, the new &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a>
is an in-tree, stable for Kubernetes v1.25, admission plugin to enforce these
standards at the namespace level. It makes it easier to enforce basic pod
security without deep security knowledge. For more sophisticated use cases, you
might need a third-party solution that can be easily combined with Pod Security
Admission.&lt;/p>
&lt;h2 id="what-s-next">What's next&lt;/h2>
&lt;p>For further details on the SIG Auth processes, covering PodSecurityPolicy removal and
creation of Pod Security admission, the
&lt;a href="https://www.youtube.com/watch?v=SFtHRmPuhEw">SIG auth update at KubeCon NA 2019&lt;/a>
and the &lt;a href="https://www.youtube.com/watch?v=HsRRmlTJpls">PodSecurityPolicy Replacement: Past, Present, and Future&lt;/a>
presentation at KubeCon NA 2021 records are available.&lt;/p>
&lt;p>Particularly on the PSP removal, the
&lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy Deprecation: Past, Present, and Future&lt;/a>
blog post is still accurate.&lt;/p>
&lt;p>And for the new Pod Security admission,
&lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">documentation is available&lt;/a>.
In addition, the blog post
&lt;a href="https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/">Kubernetes 1.23: Pod Security Graduates to Beta&lt;/a>
along with the KubeCon EU 2022 presentation
&lt;a href="https://www.youtube.com/watch?v=gcz5VsvOYmI">The Hitchhiker's Guide to Pod Security&lt;/a>
give great hands-on tutorials to learn.&lt;/p></description></item><item><title>Blog: Kubernetes v1.25: Combiner</title><link>https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/</link><pubDate>Tue, 23 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/23/kubernetes-v1-25-release/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md">Kubernetes 1.25 Release Team&lt;/a>&lt;/p>
&lt;p>Announcing the release of Kubernetes v1.25!&lt;/p>
&lt;p>This release includes a total of 40 enhancements. Fifteen of those enhancements are entering Alpha, ten are graduating to Beta, and thirteen are graduating to Stable. We also have two features being deprecated or removed.&lt;/p>
&lt;h2 id="release-theme-and-logo">Release theme and logo&lt;/h2>
&lt;p>&lt;strong>Kubernetes 1.25: Combiner&lt;/strong>&lt;/p>
&lt;figure class="release-logo">
&lt;img src="https://kubernetes.io/images/blog/2022-08-23-kubernetes-1.25-release/kubernetes-1.25.png"
alt="Combiner logo"/>
&lt;/figure>
&lt;p>The theme for Kubernetes v1.25 is &lt;em>Combiner&lt;/em>.&lt;/p>
&lt;p>The Kubernetes project itself is made up of many, many individual components that, when combined, take the form of the project you see today. It is also built and maintained by many individuals, all of them with different skills, experiences, histories, and interests, who join forces not just as the release team but as the many SIGs that support the project and the community year-round.&lt;/p>
&lt;p>With this release, we wish to honor the collaborative, open spirit that takes us from isolated developers, writers, and users spread around the globe to a combined force capable of changing the world. Kubernetes v1.25 includes a staggering 40 enhancements, none of which would exist without the incredible power we have when we work together.&lt;/p>
&lt;p>Inspired by our release lead's son, Albert Song, Kubernetes v1.25 is named for each and every one of you, no matter how you choose to contribute your unique power to the combined force that becomes Kubernetes.&lt;/p>
&lt;h2 id="what-s-new-major-themes">What's New (Major Themes)&lt;/h2>
&lt;h3 id="pod-security-changes">PodSecurityPolicy is removed; Pod Security Admission graduates to Stable&lt;/h3>
&lt;p>PodSecurityPolicy was initially &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">deprecated in v1.21&lt;/a>, and with the release of v1.25, it has been removed. The updates required to improve its usability would have introduced breaking changes, so it became necessary to remove it in favor of a more friendly replacement. That replacement is &lt;a href="https://kubernetes.io/docs/concepts/security/pod-security-admission/">Pod Security Admission&lt;/a>, which graduates to Stable with this release. If you are currently relying on PodSecurityPolicy, please follow the instructions for &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">migration to Pod Security Admission&lt;/a>.&lt;/p>
&lt;h3 id="ephemeral-containers-graduate-to-stable">Ephemeral Containers Graduate to Stable&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">Ephemeral Containers&lt;/a> are containers that exist for only a limited time within an existing pod. This is particularly useful for troubleshooting when you need to examine another container but cannot use &lt;code>kubectl exec&lt;/code> because that container has crashed or its image lacks debugging utilities. Ephemeral containers graduated to Beta in Kubernetes v1.23, and with this release, the feature graduates to Stable.&lt;/p>
&lt;h3 id="support-for-cgroups-v2-graduates-to-stable">Support for cgroups v2 Graduates to Stable&lt;/h3>
&lt;p>It has been more than two years since the Linux kernel cgroups v2 API was declared stable. With some distributions now defaulting to this API, Kubernetes must support it to continue operating on those distributions. cgroups v2 offers several improvements over cgroups v1, for more information see the &lt;a href="https://kubernetes.io/docs/concepts/architecture/cgroups/">cgroups v2&lt;/a> documentation. While cgroups v1 will continue to be supported, this enhancement puts us in a position to be ready for its eventual deprecation and replacement.&lt;/p>
&lt;h3 id="improved-windows-support">Improved Windows support&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="http://perf-dash.k8s.io/#/?jobname=soak-tests-capz-windows-2019">Performance dashboards&lt;/a> added support for Windows&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues/51540">Unit tests&lt;/a> added support for Windows&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/108592">Conformance tests&lt;/a> added support for Windows&lt;/li>
&lt;li>New GitHub repository created for &lt;a href="https://github.com/kubernetes-sigs/windows-operational-readiness">Windows Operational Readiness&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="moved-container-registry-service-from-k8s-gcr-io-to-registry-k8s-io">Moved container registry service from k8s.gcr.io to registry.k8s.io&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/kubernetes/pull/109938">Moving container registry from k8s.gcr.io to registry.k8s.io&lt;/a> got merged. For more details, see the &lt;a href="https://github.com/kubernetes/k8s.io/wiki/New-Registry-url-for-Kubernetes-(registry.k8s.io)">wiki page&lt;/a>, &lt;a href="https://groups.google.com/a/kubernetes.io/g/dev/c/DYZYNQ_A6_c/m/oD9_Q8Q9AAAJ">announcement&lt;/a> was sent to the kubernetes development mailing list.&lt;/p>
&lt;h3 id="promoted-seccompdefault-to-beta">Promoted SeccompDefault to Beta&lt;/h3>
&lt;p>SeccompDefault promoted to beta, see the tutorial &lt;a href="https://kubernetes.io/docs/tutorials/security/seccomp/#enable-the-use-of-runtimedefault-as-the-default-seccomp-profile-for-all-workloads">Restrict a Container's Syscalls with seccomp&lt;/a> for more details.&lt;/p>
&lt;h3 id="promoted-endport-in-network-policy-to-stable">Promoted endPort in Network Policy to Stable&lt;/h3>
&lt;p>Promoted &lt;code>endPort&lt;/code> in &lt;a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/#targeting-a-range-of-ports">Network Policy&lt;/a> to GA. Network Policy providers that support &lt;code>endPort&lt;/code> field now can use it to specify a range of ports to apply a Network Policy. Previously, each Network Policy could only target a single port.&lt;/p>
&lt;p>Please be aware that &lt;code>endPort&lt;/code> field &lt;strong>must be supported&lt;/strong> by the Network Policy provider. If your provider does not support &lt;code>endPort&lt;/code>, and this field is specified in a Network Policy, the Network Policy will be created covering only the port field (single port).&lt;/p>
&lt;h3 id="promoted-local-ephemeral-storage-capacity-isolation-to-stable">Promoted Local Ephemeral Storage Capacity Isolation to Stable&lt;/h3>
&lt;p>The &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/361-local-ephemeral-storage-isolation">Local Ephemeral Storage Capacity Isolation&lt;/a> feature moved to GA. This was introduced as alpha in 1.8, moved to beta in 1.10, and it is now a stable feature. It provides support for capacity isolation of local ephemeral storage between pods, such as &lt;code>EmptyDir&lt;/code>, so that a pod can be hard limited in its consumption of shared resources by evicting Pods if its consumption of local ephemeral storage exceeds that limit.&lt;/p>
&lt;h3 id="promoted-core-csi-migration-to-stable">Promoted core CSI Migration to Stable&lt;/h3>
&lt;p>&lt;a href="https://kubernetes.io/blog/2021/12/10/storage-in-tree-to-csi-migration-status-update/#quick-recap-what-is-csi-migration-and-why-migrate">CSI Migration&lt;/a> is an ongoing effort that SIG Storage has been working on for a few releases. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. The &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">core CSI Migration&lt;/a> feature moved to GA. CSI Migration for GCE PD and AWS EBS also moved to GA. CSI Migration for vSphere remains in beta (but is on by default). CSI Migration for Portworx moved to Beta (but is off-by-default).&lt;/p>
&lt;h3 id="promoted-csi-ephemeral-volume-to-stable">Promoted CSI Ephemeral Volume to Stable&lt;/h3>
&lt;p>The &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/596-csi-inline-volumes">CSI Ephemeral Volume&lt;/a> feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it moved to GA. This feature is used by some CSI drivers such as the &lt;a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">secret-store CSI driver&lt;/a>.&lt;/p>
&lt;h3 id="promoted-crd-validation-expression-language-to-beta">Promoted CRD Validation Expression Language to Beta&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-api-machinery/2876-crd-validation-expression-language/README.md">CRD Validation Expression Language&lt;/a> is promoted to beta, which makes it possible to declare how custom resources are validated using the &lt;a href="https://github.com/google/cel-spec">Common Expression Language (CEL)&lt;/a>. Please see the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules">validation rules&lt;/a> guide.&lt;/p>
&lt;h3 id="promoted-server-side-unknown-field-validation-to-beta">Promoted Server Side Unknown Field Validation to Beta&lt;/h3>
&lt;p>Promoted the &lt;code>ServerSideFieldValidation&lt;/code> feature gate to beta (on by default). This allows optionally triggering schema validation on the API server that errors when unknown fields are detected. This allows the removal of client-side validation from kubectl while maintaining the same core functionality of erroring out on requests that contain unknown or invalid fields.&lt;/p>
&lt;h3 id="introduced-kms-v2-api">Introduced KMS v2 API&lt;/h3>
&lt;p>Introduce KMS v2alpha1 API to add performance, rotation, and observability improvements. Encrypt data at rest (ie Kubernetes &lt;code>Secrets&lt;/code>) with DEK using AES-GCM instead of AES-CBC for kms data encryption. No user action is required. Reads with AES-GCM and AES-CBC will continue to be allowed. See the guide &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/">Using a KMS provider for data encryption&lt;/a> for more information.&lt;/p>
&lt;h3 id="kube-proxy-images-are-now-based-on-distroless-images">Kube-proxy images are now based on distroless images&lt;/h3>
&lt;p>In previous releases, kube-proxy container images were built using Debian as the base image. Starting with this release, the images are now built using &lt;a href="https://github.com/GoogleContainerTools/distroless">distroless&lt;/a>. This change reduced image size by almost 50% and decreased the number of installed packages and files to only those strictly required for kube-proxy to do its job.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduations-to-stable">Graduations to Stable&lt;/h3>
&lt;p>This release includes a total of thirteen enhancements promoted to stable:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/277">Ephemeral Containers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/361">Local Ephemeral Storage Resource Management&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/596">CSI Ephemeral Volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/625">CSI Migration - Core&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/785">Graduate the kube-scheduler ComponentConfig to GA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1487">CSI Migration - AWS&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1488">CSI Migration - GCE&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1591">DaemonSets Support MaxSurge&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2079">NetworkPolicy Port Range&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2254">cgroups v2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2579">Pod Security Admission&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2599">Add &lt;code>minReadySeconds&lt;/code> to Statefulsets&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2802">Identify Windows pods at API admission level authoritatively&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="deprecations-and-removals">Deprecations and Removals&lt;/h3>
&lt;p>Two features were &lt;a href="https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/">deprecated or removed&lt;/a> from Kubernetes with this release.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/5">PodSecurityPolicy is removed&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3446">GlusterFS plugin deprecated from available in-tree drivers&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="release-notes">Release Notes&lt;/h3>
&lt;p>The complete details of the Kubernetes v1.25 release are available in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md">release notes&lt;/a>.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Kubernetes v1.25 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.25.0">GitHub&lt;/a>.
To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local
Kubernetes clusters using containers as “nodes”, with &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a>.
You can also easily install 1.25 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h3 id="release-team">Release Team&lt;/h3>
&lt;p>Kubernetes is only possible with the support, commitment, and hard work of its community. Each release team is made up of dedicated community volunteers who work together to build the many pieces that, when combined, make up the Kubernetes releases you rely on. This requires the specialized skills of people from all corners of our community, from the code itself to its documentation and project management.&lt;/p>
&lt;p>We would like to thank the entire release team for the hours spent hard at work to ensure we deliver a solid Kubernetes v1.25 release for our community. Every one of you had a part to play in building this, and you all executed beautifully. We would like to extend special thanks to our fearless release lead, Cici Huang, for all she did to guarantee we had what we needed to succeed.&lt;/p>
&lt;h3 id="user-highlights">User Highlights&lt;/h3>
&lt;ul>
&lt;li>Finleap Connect operates in a highly regulated environment. &lt;a href="https://www.cncf.io/case-studies/finleap-connect/">In 2019, they had five months to implement mutual TLS (mTLS) across all services in their clusters for their business code to comply with the new European PSD2 payment directive&lt;/a>.&lt;/li>
&lt;li>PNC sought to develop a way to ensure new code would meet security standards and audit compliance requirements automatically—replacing the cumbersome 30-day manual process they had in place. Using Knative, &lt;a href="https://www.cncf.io/case-studies/pnc-bank/">PNC developed internal tools to automatically check new code and changes to existing code&lt;/a>.&lt;/li>
&lt;li>Nexxiot needed highly-reliable, secure, performant, and cost efficient Kubernetes clusters. &lt;a href="https://www.cncf.io/case-studies/nexxiot/">They turned to Cilium as the CNI to lock down their clusters and enable resilient networking with reliable day two operations&lt;/a>.&lt;/li>
&lt;li>Because the process of creating cyber insurance policies is a complicated multi-step process, At-Bay sought to improve operations by using asynchronous message-based communication patterns/facilities. &lt;a href="https://www.cncf.io/case-studies/at-bay/">They determined that Dapr fulfilled its desired list of requirements and much more&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="ecosystem-updates">Ecosystem Updates&lt;/h3>
&lt;ul>
&lt;li>KubeCon + CloudNativeCon North America 2022 will take place in Detroit, Michigan from 24 – 28 October 2022! You can find more information about the conference and registration on the &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-north-america/">event site&lt;/a>.&lt;/li>
&lt;li>KubeDay event series kicks off with KubeDay Japan on December 7! Register or submit a proposal on the &lt;a href="https://events.linuxfoundation.org/kubeday-japan/">event site&lt;/a>&lt;/li>
&lt;li>In the &lt;a href="https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/">2021 Cloud Native Survey&lt;/a>, the CNCF saw record Kubernetes and container adoption. Take a look at the &lt;a href="https://www.cncf.io/reports/cncf-annual-survey-2021/">results of the survey&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="project-velocity">Project Velocity&lt;/h3>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;amp;refresh=15m">CNCF K8s DevStats&lt;/a> project
aggregates a number of interesting data points related to the velocity of Kubernetes and various
sub-projects. This includes everything from individual contributions to the number of companies that
are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.25 release cycle, which &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25">ran for 14 weeks&lt;/a> (May 23 to August 23), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;amp;var-metric=contributions">1065 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.24.0%20-%20v1.25.0&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&amp;amp;var-repo_name=kubernetes%2Fkubernetes">1620 individuals&lt;/a>.&lt;/p>
&lt;h2 id="upcoming-release-webinar">Upcoming Release Webinar&lt;/h2>
&lt;p>Join members of the Kubernetes v1.25 release team on Thursday September 22, 2022 10am – 11am PT to learn about
the major features of this release, as well as deprecations and removals to help plan for upgrades.
For more information and registration, visit the &lt;a href="https://community.cncf.io/events/details/cncf-cncf-online-programs-presents-cncf-live-webinar-kubernetes-v125-release/">event page&lt;/a>.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests.
Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributors&lt;/a> website&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for the latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="https://serverfault.com/questions/tagged/kubernetes">Server Fault&lt;/a>.&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Spotlight on SIG Storage</title><link>https://kubernetes.io/blog/2022/08/22/sig-storage-spotlight/</link><pubDate>Mon, 22 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/22/sig-storage-spotlight/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Frederico Muñoz (SAS)&lt;/p>
&lt;p>Since the very beginning of Kubernetes, the topic of persistent data and how to address the requirement of stateful applications has been an important topic. Support for stateless deployments was natural, present from the start, and garnered attention, becoming very well-known. Work on better support for stateful applications was also present from early on, with each release increasing the scope of what could be run on Kubernetes.&lt;/p>
&lt;p>Message queues, databases, clustered filesystems: these are some examples of the solutions that have different storage requirements and that are, today, increasingly deployed in Kubernetes. Dealing with ephemeral and persistent storage, local or remote, file or block, from many different vendors, while considering how to provide the needed resiliency and data consistency that users expect, all of this is under SIG Storage's umbrella.&lt;/p>
&lt;p>In this SIG Storage spotlight, &lt;a href="https://twitter.com/fredericomunoz">Frederico Muñoz&lt;/a> (Cloud &amp;amp; Architecture Lead at SAS) talked with &lt;a href="https://twitter.com/2000xyang">Xing Yang&lt;/a>, Tech Lead at VMware and co-chair of SIG Storage, on how the SIG is organized, what are the current challenges and how anyone can get involved and contribute.&lt;/p>
&lt;h2 id="about-sig-storage">About SIG Storage&lt;/h2>
&lt;p>&lt;strong>Frederico (FSM)&lt;/strong>: Hello, thank you for the opportunity of learning more about SIG Storage. Could you tell us a bit about yourself, your role, and how you got involved in SIG Storage.&lt;/p>
&lt;p>&lt;strong>Xing Yang (XY)&lt;/strong>: I am a Tech Lead at VMware, working on Cloud Native Storage. I am also a Co-Chair of SIG Storage. I started to get involved in K8s SIG Storage at the end of 2017, starting with contributing to the &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">VolumeSnapshot&lt;/a> project. At that time, the VolumeSnapshot project was still in an experimental, pre-alpha stage. It needed contributors. So I volunteered to help. Then I worked with other community members to bring VolumeSnapshot to Alpha in K8s 1.12 release in 2018, Beta in K8s 1.17 in 2019, and eventually GA in 1.20 in 2020.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Reading the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/charter.md">SIG Storage charter&lt;/a> alone it’s clear that SIG Storage covers a lot of ground, could you describe how the SIG is organised?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: In SIG Storage, there are two Co-Chairs and two Tech Leads. Saad Ali from Google and myself are Co-Chairs. Michelle Au from Google and Jan Šafránek from Red Hat are Tech Leads.&lt;/p>
&lt;p>We have bi-weekly meetings where we go through features we are working on for each particular release, getting the statuses, making sure each feature has dev owners and reviewers working on it, and reminding people about the release deadlines, etc. More information on the SIG is on the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">community page&lt;/a>. People can also add PRs that need attention, design proposals that need discussion, and other topics to the meeting agenda doc. We will go over them after project tracking is done.&lt;/p>
&lt;p>We also have other regular meetings, i.e., CSI Implementation meeting, Object Bucket API design meeting, and one-off meetings for specific topics if needed. There is also a &lt;a href="https://github.com/kubernetes/community/blob/master/wg-data-protection/README.md">K8s Data Protection Workgroup&lt;/a> that is sponsored by SIG Storage and SIG Apps. SIG Storage owns or co-owns features that are being discussed at the Data Protection WG.&lt;/p>
&lt;h2 id="storage-and-kubernetes">Storage and Kubernetes&lt;/h2>
&lt;p>&lt;strong>FSM&lt;/strong>: Storage is such a foundational component in so many things, not least in Kubernetes: what do you think are the Kubernetes-specific challenges in terms of storage management?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: In Kubernetes, there are multiple components involved for a volume operation. For example, creating a Pod to use a PVC has multiple components involved. There are the Attach Detach Controller and the external-attacher working on attaching the PVC to the pod. There’s the Kubelet that works on mounting the PVC to the pod. Of course the CSI driver is involved as well. There could be race conditions sometimes when coordinating between multiple components.&lt;/p>
&lt;p>Another challenge is regarding core vs &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resource Definitions&lt;/a> (CRD), not really storage specific. CRD is a great way to extend Kubernetes capabilities while not adding too much code to the Kubernetes core itself. However, this also means there are many external components that are needed when running a Kubernetes cluster.&lt;/p>
&lt;p>From the SIG Storage side, one most notable example is Volume Snapshot. Volume Snapshot APIs are defined as CRDs. API definitions and controllers are out-of-tree. There is a common snapshot controller and a snapshot validation webhook that should be deployed on the control plane, similar to how kube-controller-manager is deployed. Although Volume Snapshot is a CRD, it is a core feature of SIG Storage. It is recommended for the K8s cluster distros to deploy Volume Snapshot CRDs, the snapshot controller, and the snapshot validation webhook, however, most of the time we don’t see distros deploy them. So this becomes a problem for the storage vendors: now it becomes their responsibility to deploy these non-driver specific common components. This could cause conflicts if a customer wants to use more than one storage system and deploy more than one CSI driver.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Not only the complexity of a single storage system, you have to consider how they will be used together in Kubernetes?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Yes, there are many different storage systems that can provide storage to containers in Kubernetes. They don’t work the same way. It is challenging to find a solution that works for everyone.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Storage in Kubernetes also involves interacting with external solutions, perhaps more so than other parts of Kubernetes. Is this interaction with vendors and external providers challenging? Has it evolved with time in any way?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Yes, it is definitely challenging. Initially Kubernetes storage had in-tree volume plugin interfaces. Multiple storage vendors implemented in-tree interfaces and have volume plugins in the Kubernetes core code base. This caused lots of problems. If there is a bug in a volume plugin, it affects the entire Kubernetes code base. All volume plugins must be released together with Kubernetes. There was no flexibility if storage vendors need to fix a bug in their plugin or want to align with their own product release.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: That’s where CSI enters the game?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Exactly, then there comes &lt;a href="https://kubernetes-csi.github.io/docs/">Container Storage Interface&lt;/a> (CSI). This is an industry standard trying to design common storage interfaces so that a storage vendor can write one plugin and have it work across a range of container orchestration systems (CO). Now Kubernetes is the main CO, but back when CSI just started, there were Docker, Mesos, Cloud Foundry, in addition to Kubernetes. CSI drivers are out-of-tree so bug fixes and releases can happen at their own pace.&lt;/p>
&lt;p>CSI is definitely a big improvement compared to in-tree volume plugins. Kubernetes implementation of CSI has been GA &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">since the 1.13 release&lt;/a>. It has come a long way. SIG Storage has been working on moving in-tree volume plugins to out-of-tree CSI drivers for several releases now.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Moving drivers away from the Kubernetes main tree and into CSI was an important improvement.&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: CSI interface is an improvement over the in-tree volume plugin interface, however, there are still challenges. There are lots of storage systems. Currently &lt;a href="https://kubernetes-csi.github.io/docs/drivers.html">there are more than 100 CSI drivers listed in CSI driver docs&lt;/a>. These storage systems are also very diverse. So it is difficult to design a common API that works for all. We introduced capabilities at CSI driver level, but we also have challenges when volumes provisioned by the same driver have different behaviors. The other day we just had a meeting discussing Per Volume CSI Driver Capabilities. We have a problem differentiating some CSI driver capabilities when the same driver supports both block and file volumes. We are going to have follow up meetings to discuss this problem.&lt;/p>
&lt;h2 id="ongoing-challenges">Ongoing challenges&lt;/h2>
&lt;p>&lt;strong>FSM&lt;/strong>: Specifically for the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.25">1.25 release&lt;/a> we can see that there are a relevant number of storage-related &lt;a href="https://bit.ly/k8s125-enhancements">KEPs&lt;/a> in the pipeline, would you say that this release is particularly important for the SIG?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: I wouldn’t say one release is more important than other releases. In any given release, we are working on a few very important things.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Indeed, but are there any 1.25 specific specificities and highlights you would like to point out though?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Yes. For the 1.25 release, I want to highlight the following:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">CSI Migration&lt;/a> is an on-going effort that SIG Storage has been working on for a few releases now. The goal is to move in-tree volume plugins to out-of-tree CSI drivers and eventually remove the in-tree volume plugins. There are 7 KEPs that we are targeting in 1.25 are related to CSI migration. There is one core KEP for the general CSI Migration feature. That is targeting GA in 1.25. CSI Migration for GCE PD and AWS EBS are targeting GA. CSI Migration for vSphere is targeting to have the feature gate on by default while staying in 1.25 that are in Beta. Ceph RBD and PortWorx are targeting Beta, with feature gate off by default. Ceph FS is targeting Alpha.&lt;/li>
&lt;li>The second one I want to highlight is &lt;a href="https://github.com/kubernetes-sigs/container-object-storage-interface-spec">COSI, the Container Object Storage Interface&lt;/a>. This is a sub-project under SIG Storage. COSI proposes object storage Kubernetes APIs to support orchestration of object store operations for Kubernetes workloads. It also introduces gRPC interfaces for object storage providers to write drivers to provision buckets. The COSI team has been working on this project for more than two years now. The COSI feature is targeting Alpha in 1.25. The KEP just got merged. The COSI team is working on updating the implementation based on the updated KEP.&lt;/li>
&lt;li>Another feature I want to mention is &lt;a href="https://github.com/kubernetes/enhancements/issues/596">CSI Ephemeral Volume&lt;/a> support. This feature allows CSI volumes to be specified directly in the pod specification for ephemeral use cases. They can be used to inject arbitrary states, such as configuration, secrets, identity, variables or similar information, directly inside pods using a mounted volume. This was initially introduced in 1.15 as an alpha feature, and it is now targeting GA in 1.25.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>FSM&lt;/strong>: If you had to single something out, what would be the most pressing areas the SIG is working on?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: CSI migration is definitely one area that the SIG has put in lots of effort and it has been on-going for multiple releases now. It involves work from multiple cloud providers and storage vendors as well.&lt;/p>
&lt;h2 id="community-involvement">Community involvement&lt;/h2>
&lt;p>&lt;strong>FSM&lt;/strong>: Kubernetes is a community-driven project. Any recommendation for anyone looking into getting involved in SIG Storage work? Where should they start?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: Take a look at the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG Storage community page&lt;/a>, it has lots of information on how to get started. There are &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/annual-report-2021.md">SIG annual reports&lt;/a> that tell you what we did each year. Take a look at the Contributing guide. It has links to presentations that can help you get familiar with Kubernetes storage concepts.&lt;/p>
&lt;p>Join our &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">bi-weekly meetings on Thursdays&lt;/a>. Learn how the SIG operates and what we are working on for each release. Find a project that you are interested in and help out. As I mentioned earlier, I got started in SIG Storage by contributing to the Volume Snapshot project.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Any closing thoughts you would like to add?&lt;/p>
&lt;p>&lt;strong>XY&lt;/strong>: SIG Storage always welcomes new contributors. We need contributors to help with building new features, fixing bugs, doing code reviews, writing tests, monitoring test grid health, and improving documentation, etc.&lt;/p>
&lt;p>&lt;strong>FSM&lt;/strong>: Thank you so much for your time and insights into the workings of SIG Storage!&lt;/p></description></item><item><title>Blog: Stargazing, solutions and staycations: the Kubernetes 1.24 release interview</title><link>https://kubernetes.io/blog/2022/08/18/stargazing-solutions-and-staycations-the-kubernetes-1.24-release-interview/</link><pubDate>Thu, 18 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/18/stargazing-solutions-and-staycations-the-kubernetes-1.24-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>The Kubernetes project has participants from all around the globe. Some are friends, some are colleagues, and some are strangers. The one thing that unifies them, no matter their differences, are that they all have an interesting story. It is my pleasure to be the documentarian for the stories of the Kubernetes community in the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a>. With every new Kubernetes release comes an interview with the release team lead, telling the story of that release, but also their own personal story.&lt;/p>
&lt;p>With 1.25 around the corner, &lt;a href="https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog">the tradition continues&lt;/a> with a look back at the story of 1.24. That release was led by &lt;a href="https://twitter.com/jameslaverack">James Laverack&lt;/a> of Jetstack. &lt;a href="https://kubernetespodcast.com/episode/178-kubernetes-1.24/">James was on the podcast&lt;/a> in May, and while you can read his story below, if you can, please do listen to it in his own voice.&lt;/p>
&lt;p>Make sure you &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe, wherever you get your podcasts&lt;/a>, so you hear all our stories from the cloud native community, including the story of 1.25 next week.&lt;/p>
&lt;p>&lt;em>This transcript has been lightly edited and condensed for clarity.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>CRAIG BOX: Your journey to Kubernetes went through the financial technology (fintech) industry. Tell me a little bit about how you came to software?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I took a pretty traditional path to software engineering. I went through school and then I did a computer science degree at the University of Bristol, and then I just ended up taking a software engineer job from there. Somewhat rather by accident, I ended up doing fintech work, which is pretty interesting, pretty engaging.&lt;/p>
&lt;p>But in my most recent fintech job before I joined &lt;a href="https://www.jetstack.io/">Jetstack&lt;/a>, I ended up working on a software project. We needed Kubernetes to solve a technical problem. So we implemented Kubernetes, and as often happens, I ended up as the one person of a team that understood the infrastructure, while everyone else was doing all of the application development.&lt;/p>
&lt;p>I ended up enjoying the infrastructure side so much that I decided to move and do that full time. So I looked around and I found Jetstack, whose offices were literally across the road. I could see them out of our office window. And so I decided to just hop across the road and join them, and do all of this Kubernetes stuff more.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What's the tech scene like in Bristol? You went there for school and never left?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Pretty much. It's happened to a lot of people I know and a lot of my friends, is that you go to University somewhere and you're just kind of stuck there forever, so to speak. It's been known for being quite hot in the area in terms of that part of the UK. It has a lot of tech companies, obviously, it was a fintech company I worked at before. I think some larger companies have offices there. For &amp;quot;not London&amp;quot;, it's not doing too bad, I don't think.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: When you say hot, though, that's tech industry, not weather, I'm assuming.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, weather is the usual UK. It's kind of a nice overcast and rainy, which I quite like. I'm quite fond of it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Public transport good?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Buses are all right. We've got a new bus installed recently, which everyone hated while it was being built. And now it's complete, everyone loves. So, standard I think.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That is the way. As someone who lived in London for a long time, it's very easy for me to say &amp;quot;well, London's kind of like Singapore. It's its own little city-state.&amp;quot; But whenever we did go out to that part of the world, Bath especially, a very lovely town&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Oh, Bath's lovely. I've been a couple of times.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Have you been to Box?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: To where, sorry?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's &lt;a href="https://en.wikipedia.org/wiki/Box,_Wiltshire">a town called Box&lt;/a> just outside Bath. I had my picture taken outside all the buildings. Proclaimed myself the mayor.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Oh, no, I don't think I have.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Well, look it up if you're ever in the region, everybody. Let's get back to Jetstack, though. They were across the road. Great company, the &lt;a href="https://www.jetstack.io/about/mattbarker/">two&lt;/a> &lt;a href="https://www.jetstack.io/about/mattbates/">Matts&lt;/a>, the co-founders there. What was the interview process like for you?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It was pretty relaxed. One lunchtime, I just walked down the road and went to a coffee shop with Matt and we had this lovely conversation talking about my background and Jetstack and what I was looking to achieve in a new role and all this. And I'd applied to be a software engineer. And then they kind of at the end of it, he looked over at me and was like, &amp;quot;well, how about being a solutions engineer instead?&amp;quot; And I was like, what's that?&lt;/p>
&lt;p>And he's like, &amp;quot;well, you know, it's just effectively being a software consultant. You go, you help companies implement Kubernetes, users, saying all that stuff you enjoy. But you do it full time.&amp;quot; I was like, &amp;quot;well, maybe.&amp;quot; And in the end he convinced me. I ended up joining as a solutions engineer with the idea of if I didn't like it, I could transfer to be a software engineer again.&lt;/p>
&lt;p>Nearly three years later, I've never taken them up on the offer. I've just &lt;a href="https://www.jetstack.io/blog/life-as-a-solutions-engineer/">stayed as a solutions engineer&lt;/a> the entire time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: At the company you were working at, I guess you were effectively the consultant between the people writing the software and the deployment in Kubernetes. Did it make sense then for you to carry on in that role, as you moved to Jetstack?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think so. I think it's something that I enjoyed. Not that I didn't enjoy writing software applications. I always enjoyed it, and we had a really interesting product and a really fun team. But I just found that more interesting. And it was becoming increasingly difficult to justify spending time on it when we had an application to write.&lt;/p>
&lt;p>Which was just completely fine, and that made sense for the needs of the team at the time. But it's not what I wanted to do.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you think that talks to the split between Kubernetes being for developers or for operators? Do you think there's always going to be the need to have a different set of people who are maintaining the running infrastructure versus the people who are writing the code that run on it?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think to some extent, yes, whether or not that's a separate platform team or whether or not that is because the people running it are consultants of some kind. Or whether or not this has been abstracted away from you in some of the more batteries-included versions of Kubernetes — some of the cloud-hosted ones, especially, somewhat remove that need. So I don't think it's absolutely necessary to employ a platform team. But I think someone needs to do it or you need to implicitly or explicitly pay for someone to do it in some way.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: In the three years you have been at Jetstack now, how different are the jobs that you do for the customers? Is this just a case of learning one thing and rolling it out to multiple people, or is there always a different challenge with everyone you come across?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think there's always a different challenge. My role has varied drastically. For example, a long time ago, I did an Istio install. But it was a relatively complicated, single mesh, multi-cluster install. And that was before multi-cluster support was really as readily available as it is now. Conversely, I've worked building custom orchestration platforms on top of Kubernetes for specific customer use cases.&lt;/p>
&lt;p>It's all varied and every single customer engagement is different. That is an element I really like about the job, that variability in how things are and how things go.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: When the platform catches up and does things like makes it easier to manage multi-cluster environments, do you go back to the customers and bring them up to date with the newest methods?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It depends. Most of our engagements are to solve a specific problem. And once we've solved that problem, they may have us back. But typically speaking, in my line of work, it's not an ongoing engagement. There are some within Jetstack that do that, but not so much in my team.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Your bio suggests that you were once called &amp;quot;the reason any corporate policy evolves.&amp;quot; What's the story there?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [CHUCKLES] I think I just couldn't leave things well enough alone. I was talking to our operations director inside of Jetstack, and he once said to me that whenever he's thinking of a new corporate policy, he asks will it pass the James Laverack test. That is, will I look at it and find some horrendous loophole?&lt;/p>
&lt;p>For example when I first joined, I took a look at our acceptable use policy for company equipment. And it stated that you're not allowed to have copyrighted material on your laptop. And of course, this makes sense, as you know, you don't want people doing software piracy or anything. But as written, that would imply you're not allowed to have anything that is copyrighted by anyone on your machine.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Such as perhaps the operating system that comes installed on it?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Such as perhaps the operating system, or anything. And you know, this clearly didn't make any sense. So he adjusted that, and I've kind of been fiddling with that sort of policy ever since.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The release team is often seen as an administrative role versus a pure coding role. Does that speak to the kind of change you've had in career in previously being a software developer and now being more of a consultant, or was there something else that attracted you to get involved in that particular part of the community?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I wouldn't really consider it less technical. I mean, yes, you do much less coding. This is something that constantly surprises my friends and some of my colleagues, when I tell them more detail about my role. There's not really any coding involved.&lt;/p>
&lt;p>I don't think my role has really changed to have less coding. In fact, one of my more recent projects at Jetstack, a client project, involved a lot of coding. But I think that what attracted me to this role within Kubernetes is really the community. I found it really rewarding to engage with SIG Release and to engage with the release team. So I've always just enjoyed doing it, even though there is, as you say, not all that much coding involved.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Indeed; your wife said to you, &lt;a href="https://twitter.com/JamesLaverack/status/1483201645286678529">&amp;quot;I don't think your job is to code anymore. You just talk to people all day.&amp;quot;&lt;/a> How did that make you feel?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Ahh, annoyed, because she was right. This was kind of a couple of months ago when I was in the middle of it with all of the Kubernetes meetings. Also, my client project at the time involved a lot of technical discussion. I was in three or four hours of calls every day. And I don't mind that. But I would come out, in part because of course you're working from home, so she sees me all the time. So I'd come out, I'd grab a coffee and be like, &amp;quot;oh, I've got a meeting, I've got to go.&amp;quot; And she'd be like, &amp;quot;do you ever code anymore?&amp;quot;
I think it was in fact just after Christmas when she asked me, &amp;quot;when was the last time you programmed anything?&amp;quot; And I had to think about it. Then I realized that perhaps there was a problem there. Well, not a problem, but I realized that perhaps I don't code as much as I used to.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Are you the kind of person who will pick up a hobby project to try and fix that?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Absolutely. I've recently started writing &lt;a href="https://github.com/JamesLaverack/kubernetes-minecraft-operator">a Kubernetes operator for my Minecraft server&lt;/a>. That probably tells you about the state I'm in.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: If it's got Kubernetes in it, it doesn't sound that much of a hobby.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHING] Do you not consider Kubernetes to be a hobby?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It depends.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think I do.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think by now.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: In some extents.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned observing the release team in process before you decided to get involved. Was that as part of working with customers and looking to see whether a particular feature would make it into a release, or was there some other reason that that was how you saw the Kubernetes community?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Just after I joined Jetstack, I got the opportunity to go to KubeCon San Diego. I think we actually met there.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We did.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: We had dinner, didn't we? So when I went, I'd only been at Jetstack for a few months. I really wasn't involved in the community in any serious way at all. As a result, I just ended up following around my then colleague, James Munnelly. James is lovely. And, you know, I just kind of went around with him, because he knew everyone.&lt;/p>
&lt;p>I ended up in this hotel bar with a bunch of Kubernetes people, including Stephen Augustus, the co-chair of SIG Release and holder of a bunch of other roles within the community. I happened to ask him, I want to get involved. What is a good way to get involved with the Kubernetes community, if I've never been involved before? And he said, oh, you should join the release team.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: So it's all down to where you end up in the bar with someone.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, pretty much.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: If I'd got to you sooner, you could have been working on Istio.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, I could've been working on Istio, I could have ended up in some other SIG doing something. I just happened to be talking to Stephen. And Stephen suggested it, and I gave it a go. And here I am three years later.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think I remember at the time you were working on an etcd operator?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, that's correct. That was part of a client project, which they, thankfully &lt;a href="https://github.com/improbable-eng/etcd-cluster-operator">let us open source&lt;/a>. This was an operator for etcd, where they had a requirement to run it in Kubernetes, which of course is the opposite way around to how you'd normally want to run it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And I remember having you up at the time, like I'm pretty sure those things exist already, and asking what the need was for there to be something different.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It was that they needed something very specific. The ones that existed already were all designed to run clusters that couldn't be shut down. As long as one replica stayed up, you could keep running etcd. But they needed to be able to suspend and restart the entire cluster, which means it needs disk-persistence support, which it turns out is quite complicated.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's easier if you just throw all the data away.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It's much easier to throw all the data away. We needed to be a little bit careful about how we managed it. We thought about forking and changing an existing one. But we realized it would probably just be as easy to start from scratch, so we did that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You've been a member of every release team since that point, since Kubernetes 1.18 in 2020, in a wide range of roles. Which set of roles have you been through?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I started out as a release notes shadow, and did that for a couple of releases, in 1.18 and 1.19. In 1.20, I was the release notes lead. And then in 1.21, I moved into being a shadow again as an enhancement shadow, before in 1.22 becoming an enhancements lead, but in 1.23 a release lead shadow, and finally in 1.24, release lead as a whole.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That's quite a long time to be with the release team. You're obviously going to move into an emeritus role after this release. Do you see yourself still remaining involved? Is it something that you're clearly very passionate about?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think I'm going to be around in SIG Release for as long as people want me there. I find it a really interesting part of the community. And I find the people super-interesting and super-inviting.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's talk then about &lt;a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/">Kubernetes 1.24&lt;/a>. First, as always, congratulations on the release.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Thank you.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This release consists of 46 enhancements. 14 have graduated to stable, 15 have moved to beta, and 13 are in alpha. 2 are deprecated and 2 have been removed. How is that versus other releases recently? Is that an average number? That seems like a lot of stable enhancements, especially.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think it's pretty similar. Most of the recent releases have been quite similar in the number of enhancements they have and in what categories. For example, in 1.23, the previous release, there were 47. I think 1.22, before that, had 53, so slightly more. But it's around about that number.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You didn't want to sneak in two extra so you could say you were one more than the last one?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: No, I don't think so. I think we had enough going on.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The release team is obviously beholden to what features the SIGs are developing and what their plans are. Is there ever any coordination between the release process and the SIGs in terms of things like saying, this release is going to be a catch-up release, like the old Snow Leopard releases for macOS, for example, where we say we don't want as many new features, but we really want more stabilization, and could you please work on those kind of things?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Not really. The cornerstone of a Kubernetes organization is the SIGs themselves, so the special interest groups that make up the organization. It's really up to them what they want to do. We don't do any particular coordination on the style of thing that should be implemented. A lot of SIGs have roadmaps that are looking over multiple releases to try to get features that they think are important in.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Let's talk about some of the new features in 1.24. We have been hearing for many releases now about the impending doom which is the removal of Dockershim. &lt;a href="https://github.com/kubernetes/enhancements/issues/2221">It is gone in 1.24&lt;/a>. Do we worry?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I don't think we worry. This is something that the community has been preparing for for a long time. &lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">We've&lt;/a> &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">published&lt;/a> a &lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">lot&lt;/a> of &lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">documentation&lt;/a> &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">about&lt;/a> &lt;a href="https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/">how&lt;/a> you need to approach this. The honest truth is that most users, most application developers in Kubernetes, will simply not notice a difference or have to worry about it.&lt;/p>
&lt;p>It's only really platform teams that administer Kubernetes clusters and people in very specific circumstances that are using Docker directly, not through the Kubernetes API, that are going to experience any issue at all.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And I see that Mirantis and Docker have developed a CRI plugin for Docker anyway, so you can just switch over to that and everything continues.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, absolutely, or you can use one of the many other CRI implementations. There are two in the CNCF, &lt;a href="https://containerd.io/">containerd&lt;/a>, and &lt;a href="https://cri-o.io/">CRI-O&lt;/a>.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Having gone through the process of communicating this change over several releases, what has the team learnt in terms of how we will communicate a message like this in future?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think that this has been really interesting from the perspective that this is the biggest removal that the Kubernetes project has had to date. We've removed features before. In fact, we're removing another one in this release as well. But this is one of the most user-visible changes we've made.&lt;/p>
&lt;p>I think there are very good reasons for doing it. But I think we've learned a lot about how and when to communicate, and the importance of having migration guides, the importance of having official documentation that really clarifies the thing. I think that's the real, it's an area in which the Kubernetes project has matured a lot since I've been on the team.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What is the other feature that's being removed?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: The other feature that we're removing is dynamic Kubelet configuration. This is a feature that was in beta for a while. But I believe we decided that it just wasn't being used enough to justify keeping it. So we're removing it. We deprecated it back in 1.22 and we're removing it this release.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There was a change in policy a few releases ago that talked about features not being allowed to stay in beta forever. Have there been any features that were at risk of being removed due to lack of maintenance, or are all the SIGs pretty good now at keeping their features on track?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think the SIGs are getting pretty good at it. We had a spate of a long time when a lot of features were kind of perpetually in beta. As you remember, Ingress was in beta for a long, long time.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I choose to believe it still is.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHTER] I think it's really good that we're moving towards that stability approach with things like Kubernetes. I think it's a very positive change.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The fact that Ingress was in beta for so long, along with things like the main workload controllers, for example, did lead people to believing that beta APIs were stable and production ready, and could and should be used. Something that's changing in this release is that &lt;a href="https://github.com/kubernetes/enhancements/issues/3136">beta APIs are going to be off by default&lt;/a>. Why that change?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: This is really about encouraging the use of stable APIs. There was a perception, like you say, that beta APIs were actually stable. Because they can be removed very quickly, we often ended up in the state where we wanted to follow the policy and remove a beta API, but were unable to, because it was de facto stable, according to the community. This meant that cluster operators and users had a lot of breaking changes when doing upgrades that could have been avoided. This is really just to help stability as we go through more upgrades in the future.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I understand that only applies now to new APIs. Things that are in beta at the moment will continue to be available. So there'll be no breaking changes again?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: That's correct. There's no breaking changes in beta APIs other than the ones we've documented this release. It's only new things.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now in this release, &lt;a href="https://github.com/kubernetes/enhancements/issues/3031">the artifacts are signed&lt;/a> using Cosign signatures, and there is &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-images/">experimental support for verification of those signatures&lt;/a>. What needed to happen to make that process possible?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: This was a huge process from the other half of SIG Release. SIG Release has the release team, but it also has the release engineering team that handles the mechanics of actually pushing releases out. They have spent, and one of my friends over there, Adolfo, has spent a lot of time trying to bring us in line with &lt;a href="https://slsa.dev/">SLSA&lt;/a> compliance. I believe we're &lt;a href="https://github.com/kubernetes/enhancements/issues/3027">looking now at Level 3 compliance&lt;/a>.&lt;/p>
&lt;p>SLSA is a framework that describes software supply chain security. That is, of course, a really big issue in our industry at the moment. And it's really good to see the project adopting the best practices for this.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I was looking back at &lt;a href="https://kubernetespodcast.com/episode/167-kubernetes-1.23/">the conversation I had with Rey Lejano about the 1.23 release&lt;/a>, and we were basically approaching Level 2. We're now obviously stepping up to Level 3. I think I asked Rey at the time was, is it fair to say that SLSA is inspired by large projects like Kubernetes, and in theory, it should be really easy for these projects to tick the boxes to get to that level, because the SLSA framework is written with a project like Kubernetes in mind?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think so. I think it's been somewhat difficult, just because it's one thing to do it, but it's another thing to prove that you're doing it, which is the whole point around these frameworks — the assertation, that proof.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: As an end user of Kubernetes, whether I install it myself or I take it from a service like GKE, what will this provenance then let me prove? If we think back to &lt;a href="https://kubernetespodcast.com/episode/174-in-toto/">the orange juice example we talked to Santiago about recently&lt;/a>, how do I tell that my software is safe to run?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: If you're downloading and running Kubernetes yourself, you can use the verifying image signatures feature to verify the thing you've downloaded, and the thing you are running, is actually the thing that the Kubernetes project has released, and that it has been built from the actual source code in the Kubernetes GitHub repository. This can give you a lot of confidence in what you're running, especially if you're running in a highly secure or regulated environment of some kind.&lt;/p>
&lt;p>As an end user, this isn't something that will necessarily directly impact you. But it means that service providers that provide managed Kubernetes options, such as Google and GKE, can provide even greater levels of security and safety themselves about the services that they run.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A lot of people get access to their Kubernetes server just by being granted an API endpoint, and they start running kubectl against it. They're not actually installing their own Kubernetes. They have a provider or a platform team do it for them. Do you think it's feasible to get to a world where there's something that you can run when you're deploying your workloads which queries the API server, for example, and gets access to that same provenance data?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I think it's going to be very difficult to do it that way, simply because this provenance and assertation data implies that you actually have access to the underlying executables, which typically, when you're running in a managed platform, you don't. If you're having Kubernetes provided to you, I think you're still going to have to trust the platform team or the organization that's providing it to you.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Just like when you go to the hotel breakfast bar, you have to trust that they've been good with their orange juice.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, I think the orange juice example is great. If you're making it yourself, then you can use assertation. If you're not, if you've just been given a glass, then you're going to have to trust who's pouring it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Continuing with our exploration of new stable features, &lt;a href="https://github.com/kubernetes/enhancements/issues/1472">storage capacity tracking&lt;/a> and &lt;a href="https://github.com/kubernetes/enhancements/issues/284">volume expansion&lt;/a> are generally available. What do those features enable me to do?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: This is a really great set of stable features coming out of SIG Storage. Storage capacity tracking allows applications on Kubernetes to use the Kubernetes API to understand how much storage is available, which can drive application decisions. With volume expansion, that again allows an application to use the Kubernetes API to request additional storage, which can enable applications to make all kinds of operational decisions.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: SIG Storage are also working through &lt;a href="https://github.com/kubernetes/enhancements/issues/625">a project to migrate all of their in-tree storage plugins out to CSI plugins&lt;/a>. How are they going with that process?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: In 1.24 we have a couple of them that have been migrated out. The &lt;a href="https://github.com/kubernetes/enhancements/issues/1490">Azure Disk&lt;/a> and &lt;a href="https://github.com/kubernetes/enhancements/issues/1489">OpenStack Cinder&lt;/a> plugins have both been migrated. They're maintaining the original API, but the actual implementation now happens in those CSI plugins.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do they have a long way to go, or are they just cutting off a couple every release?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: They're just doing a couple every release from what I see. There are a couple of others to go. This is really part of a larger theme within Kubernetes, which is pushing application-specific things out behind interfaces, such as the container storage interface and the container runtime interface.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That obviously sets up a situation where you have a stable interface and you can have beta implementations of that that are outside of Kubernetes and get around the problem we talked about before with not being able to run beta things.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, exactly. It also makes it easy to expand Kubernetes. You don't have to try to get code in-tree in order to implement a new storage engine, for example.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: &lt;a href="https://github.com/kubernetes/enhancements/issues/2727">gRPC probes have graduated to beta in 1.24&lt;/a>. What does that functionality provide?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: This is one of the changes that's going to be most visible to application developers in Kubernetes, I think. Until now, Kubernetes has had the ability to do readiness and liveness checks on containers and be able to make intelligent routing and pod restart decisions based on those. But those checks had to be HTTP REST endpoints.&lt;/p>
&lt;p>With Kubernetes 1.24, we're enabling a beta feature that allows them to use gRPC. This means that if you're building an application that is primarily gRPC-based, as many microservices applications are, you can now use that same technology in order to implement your probes without having to bundle an HTTP server as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Are there any other enhancements that are particularly notable or relevant perhaps to the work you've been doing?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: There's a really interesting one from SIG Network which is about &lt;a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/#avoiding-collisions-in-ip-allocation-to-services">avoiding collisions in IP allocations to services&lt;/a>. In existing versions of Kubernetes, you can allocate a service to have a particular internal cluster IP, or you can leave it blank and it will generate its own IP.&lt;/p>
&lt;p>In Kubernetes 1.24, there's an opt-in feature, which allows you to specify a pool for dynamic IPs to be generated from. This means that you can statically allocate an IP to a service and know that IP can not be accidentally dynamically allocated. This is a problem I've actually had in my local Kubernetes cluster, where I use static IP addresses for a bunch of port forwarding rules. I've always worried that during server start-up, they're going to get dynamically allocated to one of the other services. Now, with 1.24, and this feature, I won't have to worry about it more.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This is like the analog of allocating an IP in your DHCP server rather than just claiming it statically on your local machine?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Pretty much. It means that you can't accidentally double allocate something.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Why don't we all just use IPv6?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: That is a very deep question I don't think we have time for.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The margins of this podcast would be unable to contain it even if we did.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHING]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: &lt;a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/#release-theme-and-logo">The theme for Kubernetes 1.24 is Stargazer&lt;/a>. How did you pick that as the theme?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Every release lead gets to pick their theme, pretty much by themselves. When I started, I asked Rey, the previous release lead, how he picked his theme, because he picked the Next Frontier for Kubernetes 1.23. And he told me that he'd actually picked it before the release even started, which meant for the first couple of weeks and months of the release, I was really worried about it, because I hadn't picked one yet, and I wasn't sure what to pick.&lt;/p>
&lt;p>Then again, I was speaking to another former release lead, and they told me that they picked theirs like two weeks out. It seems to really vary. About halfway through the release, I had some ideas down. I thought maybe we could talk about — I live in a city called Bristol in the UK, which has a very famous bridge — and I thought, oh, we could talk about bridges and architectural and a metaphor for community bridging gaps and things like this. I kind of liked the idea, but it didn't really grab me.&lt;/p>
&lt;p>One thing about me is that I am a serious night owl. I cannot work effectively in the mornings. I've always enjoyed the night. And that got me thinking about astronomy and the stars. I think one night I was trying to get to sleep, because I couldn't sleep, and I was watching &lt;a href="https://www.youtube.com/channel/UC7_gcs09iThXybpVgjHZ_7g">PBS Space Time&lt;/a>, which is this fantastic YouTube channel talking about physics. And I'm not a physicist. I don't understand any of the maths. But I find it really interesting as a topic.&lt;/p>
&lt;p>I just thought, well, why don't I make a theme about stars. Kubernetes has often had a space theme in many releases. As I'm sure you're aware, its original name was based off of Star Trek. The previous release had a Star Trek-based theme. I thought, well, let's do that. So I came up with the idea of Stargazer.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Once you have a theme, you then need a release logo. I understand you have a household artist?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHS] I don't think she'd appreciate being called that, but, yes. My wife is an artist, and in particular, a digital artist. I had a bit of a conversation with the SIG Release folks to see if they'd be comfortable with my wife doing it, and they said they'd be completely fine with that.&lt;/p>
&lt;p>I asked if she would be willing to spend some time creating a logo for us. And thankfully for me, she was. She has produced this — well, I'm somewhat obliged to say — she produced us a beautiful logo, which you can see in our release blog and probably around social media. It is a telescope set over starry skies, and I absolutely love it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It is objectively very nice. It obviously has the seven stars or the Seven Sisters of the Pleiades. Do the colors have any particular meaning?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: The colors are based on the Kubernetes blue. If you look in the background, that haze is actually in the shape of a Kubernetes wheel from the original Kubernetes logo.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You must have to squint at it the right way. Very abstract. As is the wont of art.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: As is the wont.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned before Rey Lejano, the 1.23 release lead. We ask every interview what the person learned from the last lead and what they're going to put in the proverbial envelope for the next. At the time, Rey said that he would encourage you to use teachable moments in the release team meetings. Was that something you were able to do?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Not as much as I would have liked. I think the thing that I really took from Rey was communicate more. I've made a big effort this time to put as much communication in the open as possible. I was actually worried that I was going to be spamming the SIG Release Slack channel too much. I asked our SIG Release chairs Stephen and Sasha about it. And they said, just don't worry about it. Just spam as much as you want.&lt;/p>
&lt;p>And so I think the majority of the conversation in SIG Release Slack over the past few months has just been me. [LAUGHING] That seemed to work out pretty well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: That's what it's for.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It is what it's for. But SIG Release does more than just the individual release process, of course. It's release engineering, too.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I'm sure they'd be interested in what's going on anyway?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It's true. It's true. It's been really nice to be able to talk to everyone that way, I think.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We talked before about your introduction to Kubernetes being at a KubeCon, and meeting people in person. How has it been running the release almost entirely virtually?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: It's not been so bad. The release team has always been geographically distributed, somewhat by design. It's always been a very virtual engagement, so I don't think it's been impacted too, too much by the pandemic and travel restrictions. Of course, I'm looking forward to KubeCon Valencia and being able to see everyone again. But I think the release team has handled excellently in the current situation.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What is the advice that you will pass on to &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.25/release-team.md">the next release lead&lt;/a>, which has been announced to be Cici Huang from Google?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: I would say to Cici that open communication is really important. I made a habit of posting every single week in SIG Release a summary of what's happened. I'm super-glad that I did that, and I'm going to encourage her to do the same if she wants to.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This release was originally due out two weeks earlier, but &lt;a href="https://groups.google.com/a/kubernetes.io/g/dev/c/9IZaUGVMnmo">it was delayed&lt;/a>. What happened?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: That delay was the result of a release-blocking bug — an absolute showstopper. This was in the underlying Go implementation of TLS certificate verification. It meant that a lot of clients simply would not be able to connect to clusters or anything else. So we took the decision that we can't release with a bug this big. Thus the term release-blocking.&lt;/p>
&lt;p>The fix had to be merged upstream in Go 1.18.1, and then we had to, of course, rebuild and release release candidates. Given the time we like to have things to sit and stabilize after we make a lot of changes like that, we felt it was more prudent to push out the release by a couple of weeks than risk shipping a broken point-zero.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Go 1.18 is itself quite new. How does the project decide how quickly to upgrade its underlying programming language?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: A lot of it is driven by support requirements. We support each release for three releases. So Kubernetes 1.24 will be most likely in support until this time next year, in 2023, as we do three releases per year. That means that right up until May, 2023, we're probably going to be shipping updates for Kubernetes 1.24, which means that the version of Go we're using, and other dependencies, have to be supported as well. My understanding is that the older version of Go, Go 1.17, just wouldn't be supported long enough.&lt;/p>
&lt;p>Any underlying critical bug fixes that were coming in, they wouldn't have been back ported to Go 1.17, and therefore we might not be able to adequately support Kubernetes 1.24.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: A side effect of the unfortunate delay was an unfortunate holiday situation, where you were booked to take the week after the release off and instead you ended up taking the week before the release off. Were you able to actually have any holiday and relax in that situation?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Well, I didn't go anywhere, if that's what you're asking.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: No one ever does. This is what the pandemic's been, staycations.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yeah, staycations. It's been interesting. On the one hand, I've done a lot of Kubernetes work in that time. So you could argue it's not really been a holiday. On the other hand, my highly annoying friends have gotten me into playing an MMO, so I've been spending a lot of time playing that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I hear also you have a new vacuum cleaner?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: [LAUGHS] You've been following my Twitter. Yes, I couldn't find the charging cord for my old vacuum cleaner. And so I decided just to buy a new one. I decided, at long last, just to buy one of the nice brand-name ones. And it is just better.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This isn't the BBC. You're allowed to name it if you want.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Yes, we went and bought one of these nice Dyson vacuum cleaners, and the first time I've gotten one so expensive. On the one hand, I feel a little bit bad spending a lot of money on a vacuum cleaner. On the other hand, it's so much easier.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Is it one of those handheld ones, like a giant Dust-Buster with a long leg?&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: No, I got one of the corded floor ones, because the problem was, of course, I lost the charger for the last one, so I didn't want that to happen again. So I got a wall plug-in one.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I must say, going from a standard &lt;a href="https://www.myhenry.com/">Henry Hoover&lt;/a> to — the place we're staying at the moment has what I'll call a knock-off Dyson portable vacuum cleaner — having something that you can just pick up and carry around with you, and not have to worry about the cord, actually does encourage me to keep the place tidier.&lt;/strong>&lt;/p>
&lt;p>JAMES LAVERACK: Really? I think our last one was corded, but it didn't encourage us to use it anymore, just because it was so useless.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/jameslaverack">James Laverack&lt;/a> is a Staff Solutions Engineer at Jetstack, and was the release team lead for Kubernetes 1.24.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Meet Our Contributors - APAC (China region)</title><link>https://kubernetes.io/blog/2022/08/15/meet-our-contributors-china-ep-03/</link><pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/15/meet-our-contributors-china-ep-03/</guid><description>
&lt;p>&lt;strong>Authors &amp;amp; Interviewers:&lt;/strong> &lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>, &lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>, &lt;a href="https://github.com/jayesh-srivastava">Jayesh Srivastava&lt;/a>, &lt;a href="https://github.com/Priyankasaggu11929/">Priyanka Saggu&lt;/a>, &lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>, &lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Hello, everyone 👋&lt;/p>
&lt;p>Welcome back to the third edition of the &amp;quot;Meet Our Contributors&amp;quot; blog post series for APAC.&lt;/p>
&lt;p>This post features four outstanding contributors from China, who have played diverse leadership and community roles in the upstream Kubernetes project.&lt;/p>
&lt;p>So, without further ado, let's get straight to the article.&lt;/p>
&lt;h2 id="andy-zhang-https-github-com-andyzhangx">&lt;a href="https://github.com/andyzhangx">Andy Zhang&lt;/a>&lt;/h2>
&lt;p>Andy Zhang currently works for Microsoft China at the Shanghai site. His main focus is on Kubernetes storage drivers. Andy started contributing to Kubernetes about 5 years ago.&lt;/p>
&lt;p>He states that as he is working in Azure Kubernetes Service team and spends most of his time contributing to the Kubernetes community project. Now he is the main contributor of quite a lot Kubernetes subprojects such as Kubernetes cloud provider code.&lt;/p>
&lt;p>His open source contributions are mainly self-motivated. In the last two years he has mentored a few students contributing to Kubernetes through the LFX Mentorship program, some of whom got jobs due to their expertise and contributions on Kubernetes projects.&lt;/p>
&lt;p>Andy is an active member of the China Kubernetes community. He adds that the Kubernetes community has a good guide about how to become members, code reviewers, approvers and finally when he found out that some open source projects are in the very early stage, he actively contributed to those projects and became the project maintainer.&lt;/p>
&lt;h2 id="shiming-zhang-https-github-com-wzshiming">&lt;a href="https://github.com/wzshiming">Shiming Zhang&lt;/a>&lt;/h2>
&lt;p>Shiming Zhang is a Software Engineer working on Kubernetes for DaoCloud in Shanghai, China.&lt;/p>
&lt;p>He has mostly been involved with SIG Node as a reviewer. His major contributions have mainly been bug fixes and feature improvements in an ongoing &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2712-pod-priority-based-graceful-node-shutdown">KEP&lt;/a>, all revolving around SIG Node.&lt;/p>
&lt;p>Some of his major PRs are &lt;a href="https://github.com/kubernetes/kubernetes/pull/100326">fixing watchForLockfileContention memory leak&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/pull/101093">fixing startupProbe behaviour&lt;/a>, &lt;a href="https://github.com/kubernetes/enhancements/pull/2661">adding Field status.hostIPs for Pod&lt;/a>.&lt;/p>
&lt;h2 id="paco-xu-https-github-com-pacoxu">&lt;a href="https://github.com/pacoxu">Paco Xu&lt;/a>&lt;/h2>
&lt;p>Paco Xu works at DaoCloud, a Shanghai-based cloud-native firm. He works with the infra and the open source team, focusing on enterprise cloud native platforms based on Kubernetes.&lt;/p>
&lt;p>He started with Kubernetes in early 2017 and his first contribution was in March 2018. He started with a bug that he found, but his solution was not that graceful, hence wasn't accepted. He then started with some good first issues, which helped him to a great extent. In addition to this, from 2016 to 2017, he made some minor contributions to Docker.&lt;/p>
&lt;p>Currently, Paco is a reviewer for &lt;code>kubeadm&lt;/code> (a SIG Cluster Lifecycle product), and for SIG Node.&lt;/p>
&lt;p>Paco says that you should contribute to open source projects you use. For him, an open source project is like a book to learn, getting inspired through discussions with the project maintainers.&lt;/p>
&lt;blockquote>
&lt;p>In my opinion, the best way for me is learning how owners work on the project.&lt;/p>
&lt;/blockquote>
&lt;h2 id="jintao-zhang-https-github-com-tao12345666333">&lt;a href="https://github.com/tao12345666333">Jintao Zhang&lt;/a>&lt;/h2>
&lt;p>Jintao Zhang is presently employed at API7, where he focuses on ingress and service mesh.&lt;/p>
&lt;p>In 2017, he encountered an issue which led to a community discussion and his contributions to Kubernetes started. Before contributing to Kubernetes, Jintao was a long-time contributor to Docker-related open source projects.&lt;/p>
&lt;p>Currently Jintao is a maintainer for the &lt;a href="https://kubernetes.github.io/ingress-nginx/">ingress-nginx&lt;/a> project.&lt;/p>
&lt;p>He suggests keeping track of job opportunities at open source companies so that you can find one that allows you to contribute full time. For new contributors Jintao says that if anyone wants to make a significant contribution to an open source project, then they should choose the project based on their interests and should generously invest time.&lt;/p>
&lt;hr>
&lt;p>If you have any recommendations/suggestions for who we should interview next, please let us know in the &lt;a href="https://kubernetes.slack.com/archives/C1TU9EB9S">#sig-contribex channel&lt;/a> channel on the Kubernetes Slack. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.&lt;/p>
&lt;p>We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋&lt;/p></description></item><item><title>Blog: Enhancing Kubernetes one KEP at a Time</title><link>https://kubernetes.io/blog/2022/08/11/enhancing-kubernetes-one-kep-at-a-time/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/11/enhancing-kubernetes-one-kep-at-a-time/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Ryler Hockenbury (Mastercard)&lt;/p>
&lt;p>Did you know that Kubernetes v1.24 has &lt;a href="https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/">46 enhancements&lt;/a>? That's a lot of new functionality packed into a 4-month release cycle. The Kubernetes release team coordinates the logistics of the release, from remediating test flakes to publishing updated docs. It's a ton of work, but they always deliver.&lt;/p>
&lt;p>The release team comprises around 30 people across six subteams - Bug Triage, CI Signal, Enhancements, Release Notes, Communications, and Docs.  Each of these subteams manages a component of the release. This post will focus on the role of the enhancements subteam and how you can get involved.&lt;/p>
&lt;h2 id="what-s-the-enhancements-subteam">What's the enhancements subteam?&lt;/h2>
&lt;p>Great question. We'll get to that in a second but first, let's talk about how features are managed in Kubernetes.&lt;/p>
&lt;p>Each new feature requires a &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/README.md">Kubernetes Enhancement Proposal&lt;/a> - KEP for short. KEPs are small structured design documents that provide a way to propose and coordinate new features. The KEP author describes the motivation, design (and alternatives), risks, and tests - then community members provide feedback to build consensus.&lt;/p>
&lt;p>KEPs are submitted and updated through a pull request (PR) workflow on the &lt;a href="https://github.com/kubernetes/enhancements">k/enhancements repo&lt;/a>. Features start in alpha and move through a graduation process to beta and stable as they mature. For example, here's a cool KEP about &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/1981-windows-privileged-container-support/kep.yaml">privileged container support on Windows Server&lt;/a>.  It was introduced as alpha in Kubernetes v1.22 and graduated to beta in v1.23.&lt;/p>
&lt;p>Now getting back to the question - the enhancements subteam coordinates the lifecycle tracking of the KEPs for each release. Each KEP is required to meet a set of requirements to be cleared for inclusion in a release. The enhancements subteam verifies each requirement for each KEP and tracks the status.&lt;/p>
&lt;p>At the start of a release, &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Kubernetes Special Interest Groups&lt;/a> (SIGs) submit their enhancements to opt into a release. A typical release might have from 60 to 90 enhancements at the beginning.  During the release, many enhancements will drop out. Some do not quite meet the KEP requirements, and others do not complete their implementation in code. About 60%-70% of the opted-in KEPs will make it into the final release.&lt;/p>
&lt;h2 id="what-does-the-enhancements-subteam-do">What does the enhancements subteam do?&lt;/h2>
&lt;p>Another great question, keep them coming! The enhancements team is involved in two crucial milestones during each release: enhancements freeze and code freeze.&lt;/p>
&lt;h4 id="enhancements-freeze">Enhancements Freeze&lt;/h4>
&lt;p>Enhancements freeze is the deadline for a KEP to be complete in order for the enhancement to be included in a release. It's a quality gate to enforce alignment around maintaining and updating KEPs. The most notable requirements are a (1) &lt;a href="https://github.com/kubernetes/community/blob/master/sig-architecture/production-readiness.md">production readiness review &lt;/a>(PRR) and a (2) &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/NNNN-kep-template">KEP file&lt;/a> with a complete test plan and graduation criteria.&lt;/p>
&lt;p>The enhancements subteam communicates to each KEP author through comments on the KEP issue on Github. As a first step, they'll verify the status and check if it meets the requirements.  The KEP gets marked as tracked after satisfying the requirements; otherwise, it's considered at risk. If a KEP is still at risk when enhancement freeze is in effect, the KEP is removed from the release.&lt;/p>
&lt;p>This part of the cycle is typically the busiest for the enhancements subteam because of the large number of KEPs to groom, and each KEP might need to be visited multiple times to verify whether it meets requirements.&lt;/p>
&lt;h4 id="code-freeze">Code Freeze&lt;/h4>
&lt;p>Code freeze is the implementation deadline for all enhancements. The code must be implemented, reviewed, and merged by this point if a code change or update is needed for the enhancement. The latter third of the release is focused on stabilizing the codebase - fixing flaky tests, resolving various regressions, and preparing docs - and all the code needs to be in place before those steps can happen.&lt;/p>
&lt;p>The enhancements subteam verifies that all PRs for an enhancement are merged into the &lt;a href="https://github.com/kubernetes/kubernetes">Kubernetes codebase&lt;/a> (k/k). During this period, the subteam reaches out to KEP authors to understand what PRs are part of the KEP, verifies that those PRs get merged, and then updates the status of the KEP. The enhancement is removed from the release if the code isn't all merged before the code freeze deadline.&lt;/p>
&lt;h2 id="how-can-i-get-involved-with-the-release-team">How can I get involved with the release team?&lt;/h2>
&lt;p>I'm glad you asked. The most direct way is to apply to be a &lt;a href="https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md">release team shadow&lt;/a>. The shadow role is a hands-on apprenticeship intended to prepare individuals for leadership positions on the release team. Many shadow roles are non-technical and do not require prior contributions to the Kubernetes codebase.&lt;/p>
&lt;p>With 3 Kubernetes releases every year and roughly 25 shadows per release, the release team is always in need of individuals wanting to contribute. Before each release cycle, the release team opens the application for the shadow program. When the application goes live, it's posted in the &lt;a href="https://groups.google.com/a/kubernetes.io/g/dev">Kubernetes Dev Mailing List&lt;/a>.  You can subscribe to notifications from that list (or check it regularly!) to watch when the application opens. The announcement will typically go out in mid-April, mid-July, and mid-December - or roughly a month before the start of each release.&lt;/p>
&lt;h2 id="how-can-i-find-out-more">How can I find out more?&lt;/h2>
&lt;p>Check out the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks">role handbooks&lt;/a> if you're curious about the specifics of all the Kubernetes release subteams. The handbooks capture the logistics of each subteam, including a week-by-week breakdown of the subteam activities.  It's an excellent reference for getting to know each team better.&lt;/p>
&lt;p>You can also check out the release-related Kubernetes slack channels - particularly #release, #sig-release, and #sig-arch. These channels have discussions and updates surrounding many aspects of the release.&lt;/p></description></item><item><title>Blog: Kubernetes Removals and Major Changes In 1.25</title><link>https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/</link><pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/04/upcoming-changes-in-kubernetes-1-25/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: Kat Cosgrove, Frederico Muñoz, Debabrata Panigrahi&lt;/p>
&lt;p>As Kubernetes grows and matures, features may be deprecated, removed, or replaced with improvements for the health of the project. Kubernetes v1.25 includes several major changes and one major removal.&lt;/p>
&lt;h2 id="the-kubernetes-api-removal-and-deprecation-process">The Kubernetes API Removal and Deprecation process&lt;/h2>
&lt;p>The Kubernetes project has a well-documented &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a> for features. This policy states that stable APIs may only be deprecated when a newer, stable version of that same API is available and that APIs have a minimum lifetime for each stability level. A deprecated API is one that has been marked for removal in a future Kubernetes release; it will continue to function until removal (at least one year from the deprecation), but usage will result in a warning being displayed. Removed APIs are no longer available in the current version, at which point you must migrate to using the replacement.&lt;/p>
&lt;ul>
&lt;li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.&lt;/li>
&lt;li>Beta or pre-release API versions must be supported for 3 releases after deprecation.&lt;/li>
&lt;li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.&lt;/li>
&lt;/ul>
&lt;p>Whether an API is removed as a result of a feature graduating from beta to stable or because that API simply did not succeed, all removals comply with this deprecation policy. Whenever an API is removed, migration options are communicated in the documentation.&lt;/p>
&lt;h2 id="podsecuritypolicy-removal">A note about PodSecurityPolicy&lt;/h2>
&lt;p>In Kubernetes v1.25, we will be removing PodSecurityPolicy &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">after its deprecation in v1.21&lt;/a>. PodSecurityPolicy has served us honorably, but its complex and often confusing usage necessitated changes, which unfortunately would have been breaking changes. To address this, it is being removed in favor of a replacement, Pod Security Admission, which is graduating to stable in this release as well. If you are currently relying on PodSecurityPolicy, follow the instructions for &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/migrate-from-psp/">migration to Pod Security Admission&lt;/a>.&lt;/p>
&lt;h2 id="major-changes-for-kubernetes-v1-25">Major Changes for Kubernetes v1.25&lt;/h2>
&lt;p>Kubernetes v1.25 will include several major changes, in addition to the removal of PodSecurityPolicy.&lt;/p>
&lt;h3 id="csi-migration-https-github-com-kubernetes-enhancements-issues-625">&lt;a href="https://github.com/kubernetes/enhancements/issues/625">CSI Migration&lt;/a>&lt;/h3>
&lt;p>The effort to move the in-tree volume plugins to out-of-tree CSI drivers continues, with the core CSI Migration feature going GA in v1.25. This is an important step towards removing the in-tree volume plugins entirely.&lt;/p>
&lt;h3 id="deprecations-and-removals-for-storage-drivers">Deprecations and removals for storage drivers&lt;/h3>
&lt;p>Several volume plugins are being deprecated or removed.&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/3446">GlusterFS will be deprecated in v1.25&lt;/a>. While a CSI driver was built for it, it has not been maintained. The possibility of migration to a compatible CSI driver &lt;a href="https://github.com/kubernetes/kubernetes/issues/100897">was discussed&lt;/a>, but a decision was ultimately made to begin the deprecation of the GlusterFS plugin from in-tree drivers. The &lt;a href="https://github.com/kubernetes/enhancements/issues/2589">Portworx in-tree volume plugin&lt;/a> is also being deprecated with this release. The Flocker, Quobyte, and StorageOS in-tree volume plugins are being removed.&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/kubernetes/pull/111618">Flocker&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/pull/111619">Quobyte&lt;/a>, and &lt;a href="https://github.com/kubernetes/kubernetes/pull/111620">StorageOS&lt;/a> in-tree volume plugins will be removed in v1.25 as part of the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/625-csi-migration">CSI Migration&lt;/a>.&lt;/p>
&lt;h3 id="change-to-vsphere-version-support-https-github-com-kubernetes-kubernetes-pull-111255">&lt;a href="https://github.com/kubernetes/kubernetes/pull/111255">Change to vSphere version support&lt;/a>&lt;/h3>
&lt;p>From Kubernetes v1.25, the in-tree vSphere volume driver will not support any vSphere release before 7.0u2. Once Kubernetes v1.25 is released, check the v1.25 detailed release notes for more advice on how to handle this.&lt;/p>
&lt;h3 id="cleaning-up-iptables-chain-ownership-https-github-com-kubernetes-enhancements-issues-3178">&lt;a href="https://github.com/kubernetes/enhancements/issues/3178">Cleaning up IPTables Chain Ownership&lt;/a>&lt;/h3>
&lt;p>On Linux, Kubernetes (usually) creates iptables chains to ensure that network packets reach
Although these chains and their names have been an internal implementation detail, some tooling
has relied upon that behavior.
will only support for internal Kubernetes use cases. Starting with v1.25, the Kubelet will gradually move towards not creating the following iptables chains in the &lt;code>nat&lt;/code> table:&lt;/p>
&lt;ul>
&lt;li>&lt;code>KUBE-MARK-DROP&lt;/code>&lt;/li>
&lt;li>&lt;code>KUBE-MARK-MASQ&lt;/code>&lt;/li>
&lt;li>&lt;code>KUBE-POSTROUTING&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>This change will be phased in via the &lt;code>IPTablesCleanup&lt;/code> feature gate. Although this is not formally a deprecation, some end users have come to rely on specific internal behavior of &lt;code>kube-proxy&lt;/code>. The Kubernetes project overall wants to make it clear that depending on these internal details is not supported, and that future implementations will change their behavior here.&lt;/p>
&lt;h2 id="looking-ahead">Looking ahead&lt;/h2>
&lt;p>The official &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26">list of API removals planned for Kubernetes 1.26&lt;/a> is:&lt;/p>
&lt;ul>
&lt;li>The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)&lt;/li>
&lt;/ul>
&lt;h3 id="want-to-know-more">Want to know more?&lt;/h3>
&lt;p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24&lt;/a>&lt;/li>
&lt;li>We will formally announce the deprecations that come with &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.25.md#deprecation">Kubernetes 1.25&lt;/a> as part of the CHANGELOG for that release.&lt;/li>
&lt;/ul>
&lt;p>For information on the process of deprecation and removal, check out the official Kubernetes &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">deprecation policy&lt;/a> document.&lt;/p></description></item><item><title>Blog: Spotlight on SIG Docs</title><link>https://kubernetes.io/blog/2022/08/02/sig-docs-spotlight-2022/</link><pubDate>Tue, 02 Aug 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/08/02/sig-docs-spotlight-2022/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Purneswar Prasad&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The official documentation is the go-to source for any open source project. For Kubernetes,
it's an ever-evolving Special Interest Group (SIG) with people constantly putting in their efforts
to make details about the project easier to consume for new contributors and users. SIG Docs publishes
the official documentation on &lt;a href="https://kubernetes.io">kubernetes.io&lt;/a> which includes,
but is not limited to, documentation of the core APIs, core architectural details, and CLI tools
shipped with the Kubernetes release.&lt;/p>
&lt;p>To learn more about the work of SIG Docs and its future ahead in shaping the community, I have summarised
my conversation with the co-chairs, &lt;a href="https://twitter.com/Divya_Mohan02">Divya Mohan&lt;/a> (DM),
&lt;a href="https://twitter.com/reylejano">Rey Lejano&lt;/a> (RL) and Natali Vlatko (NV), who ran through the
SIG's goals and how fellow contributors can help.&lt;/p>
&lt;h2 id="a-summary-of-the-conversation">A summary of the conversation&lt;/h2>
&lt;h3 id="could-you-tell-us-a-little-bit-about-what-sig-docs-does">Could you tell us a little bit about what SIG Docs does?&lt;/h3>
&lt;p>SIG Docs is the special interest group for documentation for the Kubernetes project on kubernetes.io,
generating reference guides for the Kubernetes API, kubeadm and kubectl as well as maintaining the official
website’s infrastructure and analytics. The remit of their work also extends to docs releases, translation of docs,
improvement and adding new features to existing documentation, pushing and reviewing content for the official
Kubernetes blog and engaging with the Release Team for each cycle to get docs and blogs reviewed.&lt;/p>
&lt;h3 id="there-are-2-subprojects-under-docs-blogs-and-localization-how-has-the-community-benefited-from-it-and-are-there-some-interesting-contributions-by-those-teams-you-want-to-highlight">There are 2 subprojects under Docs: blogs and localization. How has the community benefited from it and are there some interesting contributions by those teams you want to highlight?&lt;/h3>
&lt;p>&lt;strong>Blogs&lt;/strong>: This subproject highlights new or graduated Kubernetes enhancements, community reports, SIG updates
or any relevant news to the Kubernetes community such as thought leadership, tutorials and project updates,
such as the Dockershim removal and removal of PodSecurityPolicy, which is upcoming in the 1.25 release.
Tim Bannister, one of the SIG Docs tech leads, does awesome work and is a major force when pushing contributions
through to the docs and blogs.&lt;/p>
&lt;p>&lt;strong>Localization&lt;/strong>: With this subproject, the Kubernetes community has been able to achieve greater inclusivity
and diversity among both users and contributors. This has also helped the project gain more contributors,
especially students, since a couple of years ago.
One of the major highlights and up-and-coming localizations are Hindi and Bengali. The efforts for Hindi
localization are currently being spearheaded by students in India.&lt;/p>
&lt;p>In addition to that, there are two other subprojects: &lt;a href="https://github.com/kubernetes-sigs/reference-docs">reference-docs&lt;/a> and the &lt;a href="https://github.com/kubernetes/website">website&lt;/a>, which is built with Hugo and is an important ownership area.&lt;/p>
&lt;h3 id="dockershim-removal">Recently there has been a lot of buzz around the Kubernetes ecosystem as well as the industry regarding the removal of dockershim in the latest 1.24 release. How has SIG Docs helped the project to ensure a smooth change among the end-users?&lt;/h3>
&lt;p>Documenting the removal of Dockershim was a mammoth task, requiring the revamping of existing documentation
and communicating to the various stakeholders regarding the deprecation efforts. It needed a community effort,
so ahead of the 1.24 release, SIG Docs partnered with Docs and Comms verticals, the Release Lead from the
Release Team, and also the CNCF to help put the word out. Weekly meetings and a GitHub project board were
set up to track progress, review issues and approve PRs and keep the Kubernetes website updated. This has
also helped new contributors know about the depreciation, so that if any good-first-issue pops up, they could chip in.
A dedicated Slack channel was used to communicate meeting updates, invite feedback or to solicit help on
outstanding issues and PRs. The weekly meeting also continued for a month after the 1.24 release to review related issues and fix them.
A huge shoutout to &lt;a href="https://twitter.com/celeste_horgan">Celeste Horgan&lt;/a>, who kept the ball rolling on this
conversation throughout the deprecation process.&lt;/p>
&lt;h3 id="why-should-new-and-existing-contributors-consider-joining-this-sig">Why should new and existing contributors consider joining this SIG?&lt;/h3>
&lt;p>Kubernetes is a vast project and can be intimidating at first for a lot of folks to find a place to start.
Any open source project is defined by its quality of documentation and SIG Docs aims to be a welcoming,
helpful place for new contributors to get onboard. One gets the perks of working with the project docs
as well as learning by reading it. They can also bring their own, new perspective to create and improve
the documentation. In the long run if they stick to SIG Docs, they can rise up the ladder to be maintainers.
This will help make a big project like Kubernetes easier to parse and navigate.&lt;/p>
&lt;h3 id="how-do-you-help-new-contributors-get-started-are-there-any-prerequisites-to-join">How do you help new contributors get started? Are there any prerequisites to join?&lt;/h3>
&lt;p>There are no such prerequisites to get started with contributing to Docs. But there is certainly a fantastic
Contribution to Docs guide which is always kept as updated and relevant as possible and new contributors
are urged to read it and keep it handy. Also, there are a lot of useful pins and bookmarks in the
community Slack channel &lt;a href="https://kubernetes.slack.com/archives/C1J0BPD2M">#sig-docs&lt;/a>. GitHub issues with
the good-first-issue labels in the kubernetes/website repo is a great place to create your first PR.
Now, SIG Docs has a monthly New Contributor Meet and Greet on the first Tuesday of the month with the
first occupant of the New Contributor Ambassador role, &lt;a href="https://twitter.com/RinkiyaKeDad">Arsh Sharma&lt;/a>.
This has helped in making a more accessible point of contact within the SIG for new contributors.&lt;/p>
&lt;h3 id="any-sig-related-accomplishment-that-you-re-really-proud-of">Any SIG related accomplishment that you’re really proud of?&lt;/h3>
&lt;p>&lt;strong>DM &amp;amp; RL&lt;/strong> : The formalization of the localization subproject in the last few months has been a big win
for SIG Docs, given all the great work put in by contributors from different countries. Earlier the
localization efforts didn’t have any streamlined process and focus was given to provide a structure by
drafting a KEP over the past couple of months for localization to be formalized as a subproject, which
is planned to be pushed through by the end of third quarter.&lt;/p>
&lt;p>&lt;strong>DM&lt;/strong> : Another area where there has been a lot of success is the New Contributor Ambassador role,
which has helped in making a more accessible point of contact for the onboarding of new contributors into the project.&lt;/p>
&lt;p>&lt;strong>NV&lt;/strong> : For each release cycle, SIG Docs have to review release docs and feature blogs highlighting
release updates within a short window. This is always a big effort for the docs and blogs reviewers.&lt;/p>
&lt;h3 id="is-there-something-exciting-coming-up-for-the-future-of-sig-docs-that-you-want-the-community-to-know">Is there something exciting coming up for the future of SIG Docs that you want the community to know?&lt;/h3>
&lt;p>SIG Docs is now looking forward to establishing a roadmap, having a steady pipeline of folks being able
to push improvements to the documentation and streamlining community involvement in triaging issues and
reviewing PRs being filed. To build one such contributor and reviewership base, a mentorship program is
being set up to help current contributors become reviewers. This definitely is a space to watch out for more!&lt;/p>
&lt;h2 id="wrap-up">Wrap Up&lt;/h2>
&lt;p>SIG Docs hosted a &lt;a href="https://www.youtube.com/watch?v=GDfcBF5et3Q">deep dive talk&lt;/a>
during on KubeCon + CloudNativeCon North America 2021, covering their awesome SIG.
They are very welcoming and have been the starting ground into Kubernetes
for a lot of new folks who want to contribute to the project.
Join the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-docs/README.md">SIG's meetings&lt;/a> to find out
about the most recent research results, their plans for the forthcoming year, and how to get involved in the upstream Docs team as a contributor!&lt;/p></description></item><item><title>Blog: Kubernetes Gateway API Graduates to Beta</title><link>https://kubernetes.io/blog/2022/07/13/gateway-api-graduates-to-beta/</link><pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/07/13/gateway-api-graduates-to-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Shane Utt (Kong), Rob Scott (Google), Nick Young (VMware), Jeff Apple (HashiCorp)&lt;/p>
&lt;p>We are excited to announce the v0.5.0 release of Gateway API. For the first
time, several of our most important Gateway API resources are graduating to
beta. Additionally, we are starting a new initiative to explore how Gateway API
can be used for mesh and introducing new experimental concepts such as URL
rewrites. We'll cover all of this and more below.&lt;/p>
&lt;h2 id="what-is-gateway-api">What is Gateway API?&lt;/h2>
&lt;p>Gateway API is a collection of resources centered around &lt;a href="https://gateway-api.sigs.k8s.io/api-types/gateway/">Gateway&lt;/a> resources
(which represent the underlying network gateways / proxy servers) to enable
robust Kubernetes service networking through expressive, extensible and
role-oriented interfaces that are implemented by many vendors and have broad
industry support.&lt;/p>
&lt;p>Originally conceived as a successor to the well known &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/">Ingress&lt;/a> API, the
benefits of Gateway API include (but are not limited to) explicit support for
many commonly used networking protocols (e.g. &lt;code>HTTP&lt;/code>, &lt;code>TLS&lt;/code>, &lt;code>TCP&lt;/code>, &lt;code>UDP&lt;/code>) as
well as tightly integrated support for Transport Layer Security (TLS). The
&lt;code>Gateway&lt;/code> resource in particular enables implementations to manage the lifecycle
of network gateways as a Kubernetes API.&lt;/p>
&lt;p>If you're an end-user interested in some of the benefits of Gateway API we
invite you to jump in and find an implementation that suits you. At the time of
this release there are over a dozen &lt;a href="https://gateway-api.sigs.k8s.io/implementations/">implementations&lt;/a> for popular API
gateways and service meshes and guides are available to start exploring quickly.&lt;/p>
&lt;h3 id="getting-started">Getting started&lt;/h3>
&lt;p>Gateway API is an official Kubernetes API like
&lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>.
Gateway API represents a superset of Ingress functionality, enabling more
advanced concepts. Similar to Ingress, there is no default implementation of
Gateway API built into Kubernetes. Instead, there are many different
&lt;a href="https://gateway-api.sigs.k8s.io/implementations/">implementations&lt;/a> available, providing significant choice in terms of underlying
technologies while providing a consistent and portable experience.&lt;/p>
&lt;p>Take a look at the &lt;a href="https://gateway-api.sigs.k8s.io/concepts/api-overview/">API concepts documentation&lt;/a> and check out some of
the &lt;a href="https://gateway-api.sigs.k8s.io/guides/getting-started/">Guides&lt;/a> to start familiarizing yourself with the APIs and how they
work. When you're ready for a practical application open the &lt;a href="https://gateway-api.sigs.k8s.io/implementations/">implementations
page&lt;/a> and select an implementation that belongs to an existing technology
you may already be familiar with or the one your cluster provider uses as a
default (if applicable). Gateway API is a &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/">Custom Resource Definition
(CRD)&lt;/a> based API so you'll need to &lt;a href="https://gateway-api.sigs.k8s.io/guides/getting-started/#install-the-crds">install the CRDs&lt;/a> onto a
cluster to use the API.&lt;/p>
&lt;p>If you're specifically interested in helping to contribute to Gateway API, we
would love to have you! Please feel free to &lt;a href="https://github.com/kubernetes-sigs/gateway-api/issues/new/choose">open a new issue&lt;/a> on the
repository, or join in the &lt;a href="https://github.com/kubernetes-sigs/gateway-api/discussions">discussions&lt;/a>. Also check out the &lt;a href="https://gateway-api.sigs.k8s.io/contributing/community/">community
page&lt;/a> which includes links to the Slack channel and community meetings.&lt;/p>
&lt;h2 id="release-highlights">Release highlights&lt;/h2>
&lt;h3 id="graduation-to-beta">Graduation to beta&lt;/h3>
&lt;p>The &lt;code>v0.5.0&lt;/code> release is particularly historic because it marks the growth in
maturity to a beta API version (&lt;code>v1beta1&lt;/code>) release for some of the key APIs:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/api-types/gatewayclass/">GatewayClass&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/api-types/gateway/">Gateway&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/api-types/httproute/">HTTPRoute&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>This achievement was marked by the completion of several graduation criteria:&lt;/p>
&lt;ul>
&lt;li>API has been &lt;a href="https://gateway-api.sigs.k8s.io/implementations/">widely implemented&lt;/a>.&lt;/li>
&lt;li>Conformance tests provide basic coverage for all resources and have multiple implementations passing tests.&lt;/li>
&lt;li>Most of the API surface is actively being used.&lt;/li>
&lt;li>Kubernetes SIG Network API reviewers have approved graduation to beta.&lt;/li>
&lt;/ul>
&lt;p>For more information on Gateway API versioning, refer to the &lt;a href="https://gateway-api.sigs.k8s.io/concepts/versioning/">official
documentation&lt;/a>. To see
what's in store for future releases check out the &lt;a href="#next-steps">next steps&lt;/a>
section.&lt;/p>
&lt;h3 id="release-channels">Release channels&lt;/h3>
&lt;p>This release introduces the &lt;code>experimental&lt;/code> and &lt;code>standard&lt;/code> &lt;a href="https://gateway-api.sigs.k8s.io/concepts/versioning/#release-channels-eg-experimental-standard">release channels&lt;/a>
which enable a better balance of maintaining stability while still enabling
experimentation and iterative development.&lt;/p>
&lt;p>The &lt;code>standard&lt;/code> release channel includes:&lt;/p>
&lt;ul>
&lt;li>resources that have graduated to beta&lt;/li>
&lt;li>fields that have graduated to standard (no longer considered experimental)&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>experimental&lt;/code> release channel includes everything in the &lt;code>standard&lt;/code> release
channel, plus:&lt;/p>
&lt;ul>
&lt;li>&lt;code>alpha&lt;/code> API resources&lt;/li>
&lt;li>fields that are considered experimental and have not graduated to &lt;code>standard&lt;/code> channel&lt;/li>
&lt;/ul>
&lt;p>Release channels are used internally to enable iterative development with
quick turnaround, and externally to indicate feature stability to implementors
and end-users.&lt;/p>
&lt;p>For this release we've added the following experimental features:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/geps/gep-957/">Routes can attach to Gateways by specifying port numbers&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/geps/gep-726/">URL rewrites and path redirects&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="other-improvements">Other improvements&lt;/h3>
&lt;p>For an exhaustive list of changes included in the &lt;code>v0.5.0&lt;/code> release, please see
the &lt;a href="https://github.com/kubernetes-sigs/gateway-api/releases/tag/v0.5.0">v0.5.0 release notes&lt;/a>.&lt;/p>
&lt;h2 id="gateway-api-for-service-mesh-the-gamma-initiative">Gateway API for service mesh: the GAMMA Initiative&lt;/h2>
&lt;p>Some service mesh projects have &lt;a href="https://gateway-api.sigs.k8s.io/implementations/">already implemented support for the Gateway
API&lt;/a>. Significant overlap
between the Service Mesh Interface (SMI) APIs and the Gateway API has &lt;a href="https://github.com/servicemeshinterface/smi-spec/issues/249">inspired
discussion in the SMI
community&lt;/a> about
possible integration.&lt;/p>
&lt;p>We are pleased to announce that the service mesh community, including
representatives from Cilium Service Mesh, Consul, Istio, Kuma, Linkerd, NGINX
Service Mesh and Open Service Mesh, is coming together to form the &lt;a href="https://gateway-api.sigs.k8s.io/contributing/gamma/">GAMMA
Initiative&lt;/a>, a dedicated
workstream within the Gateway API subproject focused on Gateway API for Mesh
Management and Administration.&lt;/p>
&lt;p>This group will deliver &lt;a href="https://gateway-api.sigs.k8s.io/v1beta1/contributing/gep/">enhancement
proposals&lt;/a> consisting
of resources, additions, and modifications to the Gateway API specification for
mesh and mesh-adjacent use-cases.&lt;/p>
&lt;p>This work has begun with &lt;a href="https://docs.google.com/document/d/1T_DtMQoq2tccLAtJTpo3c0ohjm25vRS35MsestSL9QU/edit#heading=h.jt37re3yi6k5">an exploration of using Gateway API for
service-to-service
traffic&lt;/a>
and will continue with enhancement in areas such as authentication and
authorization policy.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>As we continue to mature the API for production use cases, here are some of the highlights of what we'll be working on for the next Gateway API releases:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/gateway-api/blob/master/site-src/geps/gep-1016.md">GRPCRoute&lt;/a> for &lt;a href="https://grpc.io/">gRPC&lt;/a> traffic routing&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/gateway-api/pull/1085">Route delegation&lt;/a>&lt;/li>
&lt;li>Layer 4 API maturity: Graduating &lt;a href="https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/tcproute_types.go">TCPRoute&lt;/a>, &lt;a href="https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/udproute_types.go">UDPRoute&lt;/a> and
&lt;a href="https://github.com/kubernetes-sigs/gateway-api/blob/main/apis/v1alpha2/tlsroute_types.go">TLSRoute&lt;/a> to beta&lt;/li>
&lt;li>&lt;a href="https://gateway-api.sigs.k8s.io/contributing/gamma/">GAMMA Initiative&lt;/a> - Gateway API for Service Mesh&lt;/li>
&lt;/ul>
&lt;p>If there's something on this list you want to get involved in, or there's
something not on this list that you want to advocate for to get on the roadmap
please join us in the #sig-network-gateway-api channel on Kubernetes Slack or our weekly &lt;a href="https://gateway-api.sigs.k8s.io/contributing/community/#meetings">community calls&lt;/a>.&lt;/p></description></item><item><title>Blog: Annual Report Summary 2021</title><link>https://kubernetes.io/blog/2022/06/01/annual-report-summary-2021/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/06/01/annual-report-summary-2021/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Paris Pittman (Steering Committee)&lt;/p>
&lt;p>Last year, we published our first &lt;a href="https://kubernetes.io/blog/2021/06/28/announcing-kubernetes-community-group-annual-reports/">Annual Report Summary&lt;/a> for 2020 and it's already time for our second edition!&lt;/p>
&lt;p>&lt;a href="https://www.cncf.io/reports/kubernetes-annual-report-2021/">2021 Annual Report Summary&lt;/a>&lt;/p>
&lt;p>This summary reflects the work that has been done in 2021 and the initiatives on deck for the rest of 2022. Please forward to organizations and indidviduals participating in upstream activities, planning cloud native strategies, and/or those looking to help out. To find a specific community group's complete report, go to the &lt;a href="https://github.com/kubernetes/community">kubernetes/community repo&lt;/a> under the groups folder. Example: &lt;a href="https://github.com/kubernetes/community/blob/master/sig-api-machinery/annual-report-2021.md">sig-api-machinery/annual-report-2021.md&lt;/a>&lt;/p>
&lt;p>You’ll see that this report summary is a growth area in itself. It takes us roughly 6 months to prepare and execute, which isn’t helpful or valuable to anyone as a fast moving project with short and long term needs. How can we make this better? Provide your feedback here: &lt;a href="https://github.com/kubernetes/steering/issues/242">https://github.com/kubernetes/steering/issues/242&lt;/a>&lt;/p>
&lt;p>Reference:
&lt;a href="https://github.com/kubernetes/community/blob/master/committee-steering/governance/annual-reports.md">Annual Report Documentation&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: Maximum Unavailable Replicas for StatefulSet</title><link>https://kubernetes.io/blog/2022/05/27/maxunavailable-for-statefulset/</link><pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/27/maxunavailable-for-statefulset/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Mayank Kumar (Salesforce)&lt;/p>
&lt;p>Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">StatefulSets&lt;/a>, since their introduction in
1.5 and becoming stable in 1.9, have been widely used to run stateful applications. They provide stable pod identity, persistent
per pod storage and ordered graceful deployment, scaling and rolling updates. You can think of StatefulSet as the atomic building
block for running complex stateful applications. As the use of Kubernetes has grown, so has the number of scenarios requiring
StatefulSets. Many of these scenarios, require faster rolling updates than the currently supported one-pod-at-a-time updates, in the
case where you're using the &lt;code>OrderedReady&lt;/code> Pod management policy for a StatefulSet.&lt;/p>
&lt;p>Here are some examples:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>I am using a StatefulSet to orchestrate a multi-instance, cache based application where the size of the cache is large. The cache
starts cold and requires some siginificant amount of time before the container can start. There could be more initial startup tasks
that are required. A RollingUpdate on this StatefulSet would take a lot of time before the application is fully updated. If the
StatefulSet supported updating more than one pod at a time, it would result in a much faster update.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>My stateful application is composed of leaders and followers or one writer and multiple readers. I have multiple readers or
followers and my application can tolerate multiple pods going down at the same time. I want to update this application more than
one pod at a time so that i get the new updates rolled out quickly, especially if the number of instances of my application are
large. Note that my application still requires unique identity per pod.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In order to support such scenarios, Kubernetes 1.24 includes a new alpha feature to help. Before you can use the new feature you must
enable the &lt;code>MaxUnavailableStatefulSet&lt;/code> feature flag. Once you enable that, you can specify a new field called &lt;code>maxUnavailable&lt;/code>, part
of the &lt;code>spec&lt;/code> for a StatefulSet. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: apps/v1
kind: StatefulSet
metadata:
name: web
namespace: default
spec:
podManagementPolicy: OrderedReady # you must set OrderedReady
replicas: 5
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
containers:
- image: k8s.gcr.io/nginx-slim:0.8
imagePullPolicy: IfNotPresent
name: nginx
updateStrategy:
rollingUpdate:
maxUnavailable: 2 # this is the new alpha field, whose default value is 1
partition: 0
type: RollingUpdate
&lt;/code>&lt;/pre>&lt;p>If you enable the new feature and you don't specify a value for &lt;code>maxUnavailable&lt;/code> in a StatefulSet, Kubernetes applies a default
&lt;code>maxUnavailable: 1&lt;/code>. This matches the behavior you would see if you don't enable the new feature.&lt;/p>
&lt;p>I'll run through a scenario based on that example manifest to demonstrate how this feature works. I will deploy a StatefulSet that
has 5 replicas, with &lt;code>maxUnavailable&lt;/code> set to 2 and &lt;code>partition&lt;/code> set to 0.&lt;/p>
&lt;p>I can trigger a rolling update by changing the image to &lt;code>k8s.gcr.io/nginx-slim:0.9&lt;/code>. Once I initiate the rolling update, I can
watch the pods update 2 at a time as the current value of maxUnavailable is 2. The below output shows a span of time and is not
complete. The maxUnavailable can be an absolute number (for example, 2) or a percentage of desired Pods (for example, 10%). The
absolute number is calculated from percentage by rounding up to the nearest integer.&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl get pods --watch
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code>NAME READY STATUS RESTARTS AGE
web-0 1/1 Running 0 85s
web-1 1/1 Running 0 2m6s
web-2 1/1 Running 0 106s
web-3 1/1 Running 0 2m47s
web-4 1/1 Running 0 2m27s
web-4 1/1 Terminating 0 5m43s ----&amp;gt; start terminating 4
web-3 1/1 Terminating 0 6m3s ----&amp;gt; start terminating 3
web-3 0/1 Terminating 0 6m7s
web-3 0/1 Pending 0 0s
web-3 0/1 Pending 0 0s
web-4 0/1 Terminating 0 5m48s
web-4 0/1 Terminating 0 5m48s
web-3 0/1 ContainerCreating 0 2s
web-3 1/1 Running 0 2s
web-4 0/1 Pending 0 0s
web-4 0/1 Pending 0 0s
web-4 0/1 ContainerCreating 0 0s
web-4 1/1 Running 0 1s
web-2 1/1 Terminating 0 5m46s ----&amp;gt; start terminating 2 (only after both 4 and 3 are running)
web-1 1/1 Terminating 0 6m6s ----&amp;gt; start terminating 1
web-2 0/1 Terminating 0 5m47s
web-1 0/1 Terminating 0 6m7s
web-1 0/1 Pending 0 0s
web-1 0/1 Pending 0 0s
web-1 0/1 ContainerCreating 0 1s
web-1 1/1 Running 0 2s
web-2 0/1 Pending 0 0s
web-2 0/1 Pending 0 0s
web-2 0/1 ContainerCreating 0 0s
web-2 1/1 Running 0 1s
web-0 1/1 Terminating 0 6m6s ----&amp;gt; start terminating 0 (only after 2 and 1 are running)
web-0 0/1 Terminating 0 6m7s
web-0 0/1 Pending 0 0s
web-0 0/1 Pending 0 0s
web-0 0/1 ContainerCreating 0 0s
web-0 1/1 Running 0 1s
&lt;/code>&lt;/pre>&lt;p>Note that as soon as the rolling update starts, both 4 and 3 (the two highest ordinal pods) start terminating at the same time. Pods
with ordinal 4 and 3 may become ready at their own pace. As soon as both pods 4 and 3 are ready, pods 2 and 1 start terminating at the
same time. When pods 2 and 1 are both running and ready, pod 0 starts terminating.&lt;/p>
&lt;p>In Kubernetes, updates to StatefulSets follow a strict ordering when updating Pods. In this example, the update starts at replica 4, then
replica 3, then replica 2, and so on, one pod at a time. When going one pod at a time, its not possible for 3 to be running and ready
before 4. When &lt;code>maxUnavailable&lt;/code> is more than 1 (in the example scenario I set &lt;code>maxUnavailable&lt;/code> to 2), it is possible that replica 3 becomes
ready and running before replica 4 is ready—and that is ok. If you're a developer and you set &lt;code>maxUnavailable&lt;/code> to more than 1, you should
know that this outcome is possible and you must ensure that your application is able to handle such ordering issues that occur
if any. When you set &lt;code>maxUnavailable&lt;/code> greater than 1, the ordering is guaranteed in between each batch of pods being updated. That guarantee
means that pods in update batch 2 (replicas 2 and 1) cannot start updating until the pods from batch 0 (replicas 4 and 3) are ready.&lt;/p>
&lt;p>Although Kubernetes refers to these as &lt;em>replicas&lt;/em>, your stateful application may have a different view and each pod of the StatefulSet may
be holding completely different data than other pods. The important thing here is that updates to StatefulSets happen in batches, and you can
now have a batch size larger than 1 (as an alpha feature).&lt;/p>
&lt;p>Also note, that the above behavior is with &lt;code>podManagementPolicy: OrderedReady&lt;/code>. If you defined a StatefulSet as &lt;code>podManagementPolicy: Parallel&lt;/code>,
not only &lt;code>maxUnavailable&lt;/code> number of replicas are terminated at the same time; &lt;code>maxUnavailable&lt;/code> number of replicas start in &lt;code>ContainerCreating&lt;/code>
phase at the same time as well. This is called bursting.&lt;/p>
&lt;p>So, now you may have a lot of questions about:-&lt;/p>
&lt;ul>
&lt;li>What is the behavior when you set &lt;code>podManagementPolicy: Parallel&lt;/code>?&lt;/li>
&lt;li>What is the behavior when &lt;code>partition&lt;/code> to a value other than &lt;code>0&lt;/code>?&lt;/li>
&lt;/ul>
&lt;p>It might be better to try and see it for yourself. This is an alpha feature, and the Kubernetes contributors are looking for feedback on this feature. Did
this help you achieve your stateful scenarios Did you find a bug or do you think the behavior as implemented is not intuitive or can
break applications or catch them by surprise? Please &lt;a href="https://github.com/kubernetes/kubernetes/issues">open an issue&lt;/a> to let us know.&lt;/p>
&lt;h2 id="next-steps">Further reading and next steps&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#maximum-unavailable-pods">Maximum unavailable Pods&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-apps/961-maxunavailable-for-statefulset">KEP for MaxUnavailable for StatefulSet&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/82162/files">Implementation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/961">Enhancement Tracking Issue&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Contextual Logging in Kubernetes 1.24</title><link>https://kubernetes.io/blog/2022/05/25/contextual-logging/</link><pubDate>Wed, 25 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/25/contextual-logging/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes/community/blob/master/wg-structured-logging/README.md">Structured Logging Working
Group&lt;/a>
has added new capabilities to the logging infrastructure in Kubernetes
1.24. This blog post explains how developers can take advantage of those to
make log output more useful and how they can get involved with improving Kubernetes.&lt;/p>
&lt;h2 id="structured-logging">Structured logging&lt;/h2>
&lt;p>The goal of &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/1602-structured-logging/README.md">structured
logging&lt;/a>
is to replace C-style formatting and the resulting opaque log strings with log
entries that have a well-defined syntax for storing message and parameters
separately, for example as a JSON struct.&lt;/p>
&lt;p>When using the traditional klog text output format for structured log calls,
strings were originally printed with &lt;code>\n&lt;/code> escape sequences, except when
embedded inside a struct. For structs, log entries could still span multiple
lines, with no clean way to split the log stream into individual entries:&lt;/p>
&lt;pre tabindex="0">&lt;code>I1112 14:06:35.783529 328441 structured_logging.go:51] &amp;#34;using InfoS&amp;#34; longData={Name:long Data:Multiple
lines
with quite a bit
of text. internal:0}
I1112 14:06:35.783549 328441 structured_logging.go:52] &amp;#34;using InfoS with\nthe message across multiple lines&amp;#34; int=1 stringData=&amp;#34;long: Multiple\nlines\nwith quite a bit\nof text.&amp;#34; str=&amp;#34;another value&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Now, the &lt;code>&amp;lt;&lt;/code> and &lt;code>&amp;gt;&lt;/code> markers along with indentation are used to ensure that splitting at a
klog header at the start of a line is reliable and the resulting output is human-readable:&lt;/p>
&lt;pre tabindex="0">&lt;code>I1126 10:31:50.378204 121736 structured_logging.go:59] &amp;#34;using InfoS&amp;#34; longData=&amp;lt;
{Name:long Data:Multiple
lines
with quite a bit
of text. internal:0}
&amp;gt;
I1126 10:31:50.378228 121736 structured_logging.go:60] &amp;#34;using InfoS with\nthe message across multiple lines&amp;#34; int=1 stringData=&amp;lt;
long: Multiple
lines
with quite a bit
of text.
&amp;gt; str=&amp;#34;another value&amp;#34;
&lt;/code>&lt;/pre>&lt;p>Note that the log message itself is printed with quoting. It is meant to be a
fixed string that identifies a log entry, so newlines should be avoided there.&lt;/p>
&lt;p>Before Kubernetes 1.24, some log calls in kube-scheduler still used &lt;code>klog.Info&lt;/code>
for multi-line strings to avoid the unreadable output. Now all log calls have
been updated to support structured logging.&lt;/p>
&lt;h2 id="contextual-logging">Contextual logging&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-instrumentation/3077-contextual-logging/README.md">Contextual logging&lt;/a>
is based on the &lt;a href="https://github.com/go-logr/logr#a-minimal-logging-api-for-go">go-logr API&lt;/a>. The key
idea is that libraries are passed a logger instance by their caller and use
that for logging instead of accessing a global logger. The binary decides about
the logging implementation, not the libraries. The go-logr API is designed
around structured logging and supports attaching additional information to a
logger.&lt;/p>
&lt;p>This enables additional use cases:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>The caller can attach additional information to a logger:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://pkg.go.dev/github.com/go-logr/logr#Logger.WithName">&lt;code>WithName&lt;/code>&lt;/a> adds a prefix&lt;/li>
&lt;li>&lt;a href="https://pkg.go.dev/github.com/go-logr/logr#Logger.WithValues">&lt;code>WithValues&lt;/code>&lt;/a> adds key/value pairs&lt;/li>
&lt;/ul>
&lt;p>When passing this extended logger into a function and a function uses it
instead of the global logger, the additional information is
then included in all log entries, without having to modify the code that
generates the log entries. This is useful in highly parallel applications
where it can become hard to identify all log entries for a certain operation
because the output from different operations gets interleaved.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When running unit tests, log output can be associated with the current test.
Then when a test fails, only the log output of the failed test gets shown
by &lt;code>go test&lt;/code>. That output can also be more verbose by default because it
will not get shown for successful tests. Tests can be run in parallel
without interleaving their output.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>One of the design decisions for contextual logging was to allow attaching a
logger as value to a &lt;code>context.Context&lt;/code>. Since the logger encapsulates all
aspects of the intended logging for the call, it is &lt;em>part&lt;/em> of the context and
not just &lt;em>using&lt;/em> it. A practical advantage is that many APIs already have a
&lt;code>ctx&lt;/code> parameter or adding one has additional advantages, like being able to get
rid of &lt;code>context.TODO()&lt;/code> calls inside the functions.&lt;/p>
&lt;p>Another decision was to not break compatibility with klog v2:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Libraries that use the traditional klog logging calls in a binary that has
set up contextual logging will work and log through the logging backend
chosen by the binary. However, such log output will not include the
additional information and will not work well in unit tests, so libraries
should be modified to support contextual logging. The &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md">migration guide&lt;/a>
for structured logging has been extended to also cover contextual logging.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>When a library supports contextual logging and retrieves a logger from its
context, it will still work in a binary that does not initialize contextual
logging because it will get a logger that logs through klog.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>In Kubernetes 1.24, contextual logging is a new alpha feature with
&lt;code>ContextualLogging&lt;/code> as feature gate. When disabled (the default), the new klog
API calls for contextual logging (see below) become no-ops to avoid performance
or functional regressions.&lt;/p>
&lt;p>No Kubernetes component has been converted yet. An &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go">example program&lt;/a>
in the Kubernetes repository demonstrates how to enable contextual logging in a
binary and how the output depends on the binary's parameters:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-console" data-lang="console">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> &lt;span style="color:#a2f">cd&lt;/span> &lt;span style="color:#b8860b">$GOPATH&lt;/span>/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> go run . --help
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> --feature-gates mapStringBool A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> AllAlpha=true|false (ALPHA - default=false)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> AllBeta=true|false (BETA - default=false)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888"> ContextualLogging=true|false (ALPHA - default=false)
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">&lt;/span>&lt;span style="color:#000080;font-weight:bold">$&lt;/span> go run . --feature-gates &lt;span style="color:#b8860b">ContextualLogging&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f">true&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">...
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0404 18:00:02.916429 451895 logger.go:94] &amp;#34;example/myname: runtime&amp;#34; foo=&amp;#34;bar&amp;#34; duration=&amp;#34;1m0s&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#888">I0404 18:00:02.916447 451895 logger.go:95] &amp;#34;example: another runtime&amp;#34; foo=&amp;#34;bar&amp;#34; duration=&amp;#34;1m0s&amp;#34;
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>example&lt;/code> prefix and &lt;code>foo=&amp;quot;bar&amp;quot;&lt;/code> were added by the caller of the function
which logs the &lt;code>runtime&lt;/code> message and &lt;code>duration=&amp;quot;1m0s&amp;quot;&lt;/code> value.&lt;/p>
&lt;p>The sample code for klog includes an
&lt;a href="https://github.com/kubernetes/klog/blob/v2.60.1/ktesting/example/example_test.go">example&lt;/a>
for a unit test with per-test output.&lt;/p>
&lt;h2 id="klog-enhancements">klog enhancements&lt;/h2>
&lt;h3 id="contextual-logging-api">Contextual logging API&lt;/h3>
&lt;p>The following calls manage the lookup of a logger:&lt;/p>
&lt;dl>
&lt;dt>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#FromContext">&lt;code>FromContext&lt;/code>&lt;/a>&lt;/dt>
&lt;dd>from a &lt;code>context&lt;/code> parameter, with fallback to the global logger&lt;/dd>
&lt;dt>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#Background">&lt;code>Background&lt;/code>&lt;/a>&lt;/dt>
&lt;dd>the global fallback, with no intention to support contextual logging&lt;/dd>
&lt;dt>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#TODO">&lt;code>TODO&lt;/code>&lt;/a>&lt;/dt>
&lt;dd>the global fallback, but only as a temporary solution until the function gets extended to accept
a logger through its parameters&lt;/dd>
&lt;dt>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#SetLoggerWithOptions">&lt;code>SetLoggerWithOptions&lt;/code>&lt;/a>&lt;/dt>
&lt;dd>changes the fallback logger; when called with &lt;a href="https://pkg.go.dev/k8s.io/klog/v2#ContextualLogger">&lt;code>ContextualLogger(true)&lt;/code>&lt;/a>,
the logger is ready to be called directly, in which case logging will be done
without going through klog&lt;/dd>
&lt;/dl>
&lt;p>To support the feature gate mechanism in Kubernetes, klog has wrapper calls for
the corresponding go-logr calls and a global boolean controlling their behavior:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#LoggerWithName">&lt;code>LoggerWithName&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#LoggerWithValues">&lt;code>LoggerWithValues&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#NewContext">&lt;code>NewContext&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#EnableContextualLogging">&lt;code>EnableContextualLogging&lt;/code>&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Usage of those functions in Kubernetes code is enforced with a linter
check. The klog default for contextual logging is to enable the functionality
because it is considered stable in klog. It is only in Kubernetes binaries
where that default gets overridden and (in some binaries) controlled via the
&lt;code>--feature-gate&lt;/code> parameter.&lt;/p>
&lt;h3 id="ktesting-logger">ktesting logger&lt;/h3>
&lt;p>The new &lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting">ktesting&lt;/a> package
implements logging through &lt;code>testing.T&lt;/code> using klog's text output format. It has
a &lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting#NewTestContext">single API call&lt;/a> for
instrumenting a test case and &lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/ktesting/init">support for command line flags&lt;/a>.&lt;/p>
&lt;h3 id="klogr">klogr&lt;/h3>
&lt;p>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr">&lt;code>klog/klogr&lt;/code>&lt;/a> continues to be
supported and it's default behavior is unchanged: it formats structured log
entries using its own, custom format and prints the result via klog.&lt;/p>
&lt;p>However, this usage is discouraged because that format is neither
machine-readable (in contrast to real JSON output as produced by zapr, the
go-logr implementation used by Kubernetes) nor human-friendly (in contrast to
the klog text format).&lt;/p>
&lt;p>Instead, a klogr instance should be created with
&lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/klogr#WithFormat">&lt;code>WithFormat(FormatKlog)&lt;/code>&lt;/a>
which chooses the klog text format. A simpler construction method with the same
result is the new
&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#NewKlogr">&lt;code>klog.NewKlogr&lt;/code>&lt;/a>. That is the
logger that klog returns as fallback when nothing else is configured.&lt;/p>
&lt;h3 id="reusable-output-test">Reusable output test&lt;/h3>
&lt;p>A lot of go-logr implementations have very similar unit tests where they check
the result of certain log calls. If a developer didn't know about certain
caveats like for example a &lt;code>String&lt;/code> function that panics when called, then it
is likely that both the handling of such caveats and the unit test are missing.&lt;/p>
&lt;p>&lt;a href="https://pkg.go.dev/k8s.io/klog/v2@v2.60.1/test">&lt;code>klog.test&lt;/code>&lt;/a> is a reusable set
of test cases that can be applied to a go-logr implementation.&lt;/p>
&lt;h3 id="output-flushing">Output flushing&lt;/h3>
&lt;p>klog used to start a goroutine unconditionally during &lt;code>init&lt;/code> which flushed
buffered data at a hard-coded interval. Now that goroutine is only started on
demand (i.e. when writing to files with buffering) and can be controlled with
&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#StopFlushDaemon">&lt;code>StopFlushDaemon&lt;/code>&lt;/a> and
&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#StartFlushDaemon">&lt;code>StartFlushDaemon&lt;/code>&lt;/a>.&lt;/p>
&lt;p>When a go-logr implementation buffers data, flushing that data can be
integrated into &lt;a href="https://pkg.go.dev/k8s.io/klog/v2#Flush">&lt;code>klog.Flush&lt;/code>&lt;/a> by
registering the logger with the
&lt;a href="https://pkg.go.dev/k8s.io/klog/v2#FlushLogger">&lt;code>FlushLogger&lt;/code>&lt;/a> option.&lt;/p>
&lt;h3 id="various-other-changes">Various other changes&lt;/h3>
&lt;p>For a description of all other enhancements see in the &lt;a href="https://github.com/kubernetes/klog/releases">release notes&lt;/a>.&lt;/p>
&lt;h2 id="logcheck">logcheck&lt;/h2>
&lt;p>Originally designed as a linter for structured log calls, the
&lt;a href="https://github.com/kubernetes/klog/tree/788efcdee1e9be0bfbe5b076343d447314f2377e/hack/tools/logcheck">&lt;code>logcheck&lt;/code>&lt;/a>
tool has been enhanced to support also contextual logging and traditional klog
log calls. These enhanced checks already found bugs in Kubernetes, like calling
&lt;code>klog.Info&lt;/code> instead of &lt;code>klog.Infof&lt;/code> with a format string and parameters.&lt;/p>
&lt;p>It can be included as a plugin in a &lt;code>golangci-lint&lt;/code> invocation, which is how
&lt;a href="https://github.com/kubernetes/kubernetes/commit/17e3c555c5115f8c9176bae10ba45baa04d23a7b">Kubernetes uses it now&lt;/a>,
or get invoked stand-alone.&lt;/p>
&lt;p>We are in the process of &lt;a href="https://github.com/kubernetes/klog/issues/312">moving the tool&lt;/a> into a new repository because it isn't
really related to klog and its releases should be tracked and tagged properly.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>The &lt;a href="https://github.com/kubernetes/community/tree/master/wg-structured-logging">Structured Logging WG&lt;/a>
is always looking for new contributors. The migration
away from C-style logging is now going to target structured, contextual logging
in one step to reduce the overall code churn and number of PRs. Changing log
calls is good first contribution to Kubernetes and an opportunity to get to
know code in various different areas.&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: Avoid Collisions Assigning IP Addresses to Services</title><link>https://kubernetes.io/blog/2022/05/23/service-ip-dynamic-and-static-allocation/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/23/service-ip-dynamic-and-static-allocation/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Antonio Ojea (Red Hat)&lt;/p>
&lt;p>In Kubernetes, &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services&lt;/a> are an abstract way to expose
an application running on a set of Pods. Services
can have a cluster-scoped virtual IP address (using a Service of &lt;code>type: ClusterIP&lt;/code>).
Clients can connect using that virtual IP address, and Kubernetes then load-balances traffic to that
Service across the different backing Pods.&lt;/p>
&lt;h2 id="how-service-clusterips-are-allocated">How Service ClusterIPs are allocated?&lt;/h2>
&lt;p>A Service &lt;code>ClusterIP&lt;/code> can be assigned:&lt;/p>
&lt;dl>
&lt;dt>&lt;em>dynamically&lt;/em>&lt;/dt>
&lt;dd>the cluster's control plane automatically picks a free IP address from within the configured IP range for &lt;code>type: ClusterIP&lt;/code> Services.&lt;/dd>
&lt;dt>&lt;em>statically&lt;/em>&lt;/dt>
&lt;dd>you specify an IP address of your choice, from within the configured IP range for Services.&lt;/dd>
&lt;/dl>
&lt;p>Across your whole cluster, every Service &lt;code>ClusterIP&lt;/code> must be unique.
Trying to create a Service with a specific &lt;code>ClusterIP&lt;/code> that has already
been allocated will return an error.&lt;/p>
&lt;h2 id="why-do-you-need-to-reserve-service-cluster-ips">Why do you need to reserve Service Cluster IPs?&lt;/h2>
&lt;p>Sometimes you may want to have Services running in well-known IP addresses, so other components and
users in the cluster can use them.&lt;/p>
&lt;p>The best example is the DNS Service for the cluster. Some Kubernetes installers assign the 10th address from
the Service IP range to the DNS service. Assuming you configured your cluster with Service IP range
10.96.0.0/16 and you want your DNS Service IP to be 10.96.0.10, you'd have to create a Service like
this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-dns&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kubernetes.io/cluster-service&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kubernetes.io/name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CoreDNS&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-dns&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">clusterIP&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">10.96.0.10&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dns&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">53&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>UDP&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">53&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dns-tcp&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">53&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TCP&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">53&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-dns&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterIP&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>but as I explained before, the IP address 10.96.0.10 has not been reserved; if other Services are created
before or in parallel with dynamic allocation, there is a chance they can allocate this IP, hence,
you will not be able to create the DNS Service because it will fail with a conflict error.&lt;/p>
&lt;h2 id="avoid-ClusterIP-conflict">How can you avoid Service ClusterIP conflicts?&lt;/h2>
&lt;p>In Kubernetes 1.24, you can enable a new feature gate &lt;code>ServiceIPStaticSubrange&lt;/code>.
Turning this on allows you to use a different IP
allocation strategy for Services, reducing the risk of collision.&lt;/p>
&lt;p>The &lt;code>ClusterIP&lt;/code> range will be divided, based on the formula &lt;code>min(max(16, cidrSize / 16), 256)&lt;/code>,
described as &lt;em>never less than 16 or more than 256 with a graduated step between them&lt;/em>.&lt;/p>
&lt;p>Dynamic IP assignment will use the upper band by default, once this has been exhausted it will
use the lower range. This will allow users to use static allocations on the lower band with a low
risk of collision.&lt;/p>
&lt;p>Examples:&lt;/p>
&lt;h4 id="service-ip-cidr-block-10-96-0-0-24">Service IP CIDR block: 10.96.0.0/24&lt;/h4>
&lt;p>Range Size: 2&lt;sup>8&lt;/sup> - 2 = 254&lt;br>
Band Offset: &lt;code>min(max(16, 256/16), 256)&lt;/code> = &lt;code>min(16, 256)&lt;/code> = 16&lt;br>
Static band start: 10.96.0.1&lt;br>
Static band end: 10.96.0.16&lt;br>
Range end: 10.96.0.254&lt;/p>
&lt;figure>
&lt;div class="mermaid">
pie showData
title 10.96.0.0/24
"Static" : 16
"Dynamic" : 238
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">JavaScript must be &lt;a href="https://www.enable-javascript.com/">enabled&lt;/a> to view this content&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;h4 id="service-ip-cidr-block-10-96-0-0-20">Service IP CIDR block: 10.96.0.0/20&lt;/h4>
&lt;p>Range Size: 2&lt;sup>12&lt;/sup> - 2 = 4094&lt;br>
Band Offset: &lt;code>min(max(16, 4096/16), 256)&lt;/code> = &lt;code>min(256, 256)&lt;/code> = 256&lt;br>
Static band start: 10.96.0.1&lt;br>
Static band end: 10.96.1.0&lt;br>
Range end: 10.96.15.254&lt;/p>
&lt;figure>
&lt;div class="mermaid">
pie showData
title 10.96.0.0/20
"Static" : 256
"Dynamic" : 3838
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">JavaScript must be &lt;a href="https://www.enable-javascript.com/">enabled&lt;/a> to view this content&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;h4 id="service-ip-cidr-block-10-96-0-0-16">Service IP CIDR block: 10.96.0.0/16&lt;/h4>
&lt;p>Range Size: 2&lt;sup>16&lt;/sup> - 2 = 65534&lt;br>
Band Offset: &lt;code>min(max(16, 65536/16), 256)&lt;/code> = &lt;code>min(4096, 256)&lt;/code> = 256&lt;br>
Static band start: 10.96.0.1&lt;br>
Static band ends: 10.96.1.0&lt;br>
Range end: 10.96.255.254&lt;/p>
&lt;figure>
&lt;div class="mermaid">
pie showData
title 10.96.0.0/16
"Static" : 256
"Dynamic" : 65278
&lt;/div>
&lt;/figure>
&lt;noscript>
&lt;div class="alert alert-secondary callout" role="alert">
&lt;em class="javascript-required">JavaScript must be &lt;a href="https://www.enable-javascript.com/">enabled&lt;/a> to view this content&lt;/em>
&lt;/div>
&lt;/noscript>
&lt;h2 id="get-involved-with-sig-network">Get involved with SIG Network&lt;/h2>
&lt;p>The current SIG-Network &lt;a href="https://github.com/orgs/kubernetes/projects/10">KEPs&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/issues?q=is%3Aopen+is%3Aissue+label%3Asig%2Fnetwork">issues&lt;/a> on GitHub illustrate the SIG’s areas of emphasis.&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-network">SIG Network meetings&lt;/a> are a friendly, welcoming venue for you to connect with the community and share your ideas.
Looking forward to hearing from you!&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: Introducing Non-Graceful Node Shutdown Alpha</title><link>https://kubernetes.io/blog/2022/05/20/kubernetes-1-24-non-graceful-node-shutdown-alpha/</link><pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/20/kubernetes-1-24-non-graceful-node-shutdown-alpha/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong> Xing Yang and Yassine Tijani (VMware)&lt;/p>
&lt;p>Kubernetes v1.24 introduces alpha support for &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/2268-non-graceful-shutdown">Non-Graceful Node Shutdown&lt;/a>. This feature allows stateful workloads to failover to a different node after the original node is shutdown or in a non-recoverable state such as hardware failure or broken OS.&lt;/p>
&lt;h2 id="how-is-this-different-from-graceful-node-shutdown">How is this different from Graceful Node Shutdown&lt;/h2>
&lt;p>You might have heard about the &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#graceful-node-shutdown">Graceful Node Shutdown&lt;/a> capability of Kubernetes,
and are wondering how the Non-Graceful Node Shutdown feature is different from that. Graceful Node Shutdown
allows Kubernetes to detect when a node is shutting down cleanly, and handles that situation appropriately.
A Node Shutdown can be &amp;quot;graceful&amp;quot; only if the node shutdown action can be detected by the kubelet ahead
of the actual shutdown. However, there are cases where a node shutdown action may not be detected by
the kubelet. This could happen either because the shutdown command does not trigger the systemd inhibitor
locks mechanism that kubelet relies upon, or because of a configuration error
(the &lt;code>ShutdownGracePeriod&lt;/code> and &lt;code>ShutdownGracePeriodCriticalPods&lt;/code> are not configured properly).&lt;/p>
&lt;p>Graceful node shutdown relies on Linux-specific support. The kubelet does not watch for upcoming
shutdowns on Windows nodes (this may change in a future Kubernetes release).&lt;/p>
&lt;p>When a node is shutdown but without the kubelet detecting it, pods on that node
also shut down ungracefully. For stateless apps, that's often not a problem (a ReplicaSet adds a new pod once
the cluster detects that the affected node or pod has failed). For stateful apps, the story is more complicated.
If you use a StatefulSet and have a pod from that StatefulSet on a node that fails uncleanly, that affected pod
will be marked as terminating; the StatefulSet cannot create a replacement pod because the pod
still exists in the cluster.
As a result, the application running on the StatefulSet may be degraded or even offline. If the original, shut
down node comes up again, the kubelet on that original node reports in, deletes the existing pods, and
the control plane makes a replacement pod for that StatefulSet on a different running node.
If the original node has failed and does not come up, those stateful pods would be stuck in a
terminating status on that failed node indefinitely.&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl get pod -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
web-0 1/1 Running 0 100m 10.244.2.4 k8s-node-876-1639279816 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
web-1 1/1 Terminating 0 100m 10.244.1.3 k8s-node-433-1639279804 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;h2 id="try-out-the-new-non-graceful-shutdown-handling">Try out the new non-graceful shutdown handling&lt;/h2>
&lt;p>To use the non-graceful node shutdown handling, you must enable the &lt;code>NodeOutOfServiceVolumeDetach&lt;/code>
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">feature gate&lt;/a> for the &lt;code>kube-controller-manager&lt;/code>
component.&lt;/p>
&lt;p>In the case of a node shutdown, you can manually taint that node as out of service. You should make certain that
the node is truly shutdown (not in the middle of restarting) before you add that taint. You could add that
taint following a shutdown that the kubelet did not detect and handle in advance; another case where you
can use that taint is when the node is in a non-recoverable state due to a hardware failure or a broken OS.
The values you set for that taint can be &lt;code>node.kubernetes.io/out-of-service=nodeshutdown: &amp;quot;NoExecute&amp;quot;&lt;/code>
or &lt;code>node.kubernetes.io/out-of-service=nodeshutdown:&amp;quot; NoSchedule&amp;quot;&lt;/code>.
Provided you have enabled the feature gate mentioned earlier, setting the out-of-service taint on a Node
means that pods on the node will be deleted unless if there are matching tolerations on the pods.
Persistent volumes attached to the shutdown node will be detached, and for StatefulSets, replacement pods will
be created successfully on a different running node.&lt;/p>
&lt;pre tabindex="0">&lt;code>$ kubectl taint nodes &amp;lt;node-name&amp;gt; node.kubernetes.io/out-of-service=nodeshutdown:NoExecute
$ kubectl get pod -o wide
NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
web-0 1/1 Running 0 150m 10.244.2.4 k8s-node-876-1639279816 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
web-1 1/1 Running 0 10m 10.244.1.7 k8s-node-433-1639279804 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;p>Note: Before applying the out-of-service taint, you &lt;strong>must&lt;/strong> verify that a node is already in shutdown or power off state (not in the middle of restarting), either because the user intentionally shut it down or the node is down due to hardware failures, OS issues, etc.&lt;/p>
&lt;p>Once all the workload pods that are linked to the out-of-service node are moved to a new running node, and the shutdown node has been recovered, you should remove
that taint on the affected node after the node is recovered.
If you know that the node will not return to service, you could instead delete the node from the cluster.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>Depending on feedback and adoption, the Kubernetes team plans to push the Non-Graceful Node Shutdown implementation to Beta in either 1.25 or 1.26.&lt;/p>
&lt;p>This feature requires a user to manually add a taint to the node to trigger workloads failover and remove the taint after the node is recovered. In the future, we plan to find ways to automatically detect and fence nodes that are shutdown/failed and automatically failover workloads to another node.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>Check out the &lt;a href="https://kubernetes.io/docs/concepts/architecture/nodes/#non-graceful-node-shutdown">documentation&lt;/a>
for non-graceful node shutdown.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>This feature has a long story. Yassine Tijani (&lt;a href="https://github.com/yastij">yastij&lt;/a>) started the KEP more than two years ago. Xing Yang (&lt;a href="https://github.com/xing-yang">xing-yang&lt;/a>) continued to drive the effort. There were many discussions among SIG Storage, SIG Node, and API reviewers to nail down the design details. Ashutosh Kumar (&lt;a href="https://github.com/sonasingh46">sonasingh46&lt;/a>) did most of the implementation and brought it to Alpha in Kubernetes 1.24.&lt;/p>
&lt;p>We want to thank the following people for their insightful reviews: Tim Hockin (&lt;a href="https://github.com/thockin">thockin&lt;/a>) for his guidance on the design, Jing Xu (&lt;a href="https://github.com/jingxu97">jingxu97&lt;/a>), Hemant Kumar (&lt;a href="https://github.com/gnufied">gnufied&lt;/a>), and Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>) for reviews from SIG Storage side, and Mrunal Patel (&lt;a href="https://github.com/mrunalp">mrunalp&lt;/a>), David Porter (&lt;a href="https://github.com/bobbypage">bobbypage&lt;/a>), Derek Carr (&lt;a href="https://github.com/derekwaynecarr">derekwaynecarr&lt;/a>), and Danielle Endocrimes (&lt;a href="https://github.com/endocrimes">endocrimes&lt;/a>) for reviews from SIG Node side.&lt;/p>
&lt;p>There are many people who have helped review the design and implementation along the way. We want to thank everyone who has contributed to this effort including the about 30 people who have reviewed the &lt;a href="https://github.com/kubernetes/enhancements/pull/1116">KEP&lt;/a> and implementation over the last couple of years.&lt;/p>
&lt;p>This feature is a collaboration between SIG Storage and SIG Node. For those interested in getting involved with the design and development of any part of the Kubernetes Storage system, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG). For those interested in getting involved with the design and development of the components that support the controlled interactions between pods and host resources, join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">Kubernetes Node SIG&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: Prevent unauthorised volume mode conversion</title><link>https://kubernetes.io/blog/2022/05/18/prevent-unauthorised-volume-mode-conversion-alpha/</link><pubDate>Wed, 18 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/18/prevent-unauthorised-volume-mode-conversion-alpha/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Raunak Pradip Shah (Mirantis)&lt;/p>
&lt;p>Kubernetes v1.24 introduces a new alpha-level feature that prevents unauthorised users
from modifying the volume mode of a &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">&lt;code>PersistentVolumeClaim&lt;/code>&lt;/a> created from an
existing &lt;a href="https://kubernetes.io/docs/concepts/storage/volume-snapshots/">&lt;code>VolumeSnapshot&lt;/code>&lt;/a> in the Kubernetes cluster.&lt;/p>
&lt;h3 id="the-problem">The problem&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-mode">Volume Mode&lt;/a> determines whether a volume
is formatted into a filesystem or presented as a raw block device.&lt;/p>
&lt;p>Users can leverage the &lt;code>VolumeSnapshot&lt;/code> feature, which has been stable since Kubernetes v1.20,
to create a &lt;code>PersistentVolumeClaim&lt;/code> (shortened as PVC) from an existing &lt;code>VolumeSnapshot&lt;/code> in
the Kubernetes cluster. The PVC spec includes a &lt;code>dataSource&lt;/code> field, which can point to an
existing &lt;code>VolumeSnapshot&lt;/code> instance.
Visit &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#create-persistent-volume-claim-from-volume-snapshot">Create a PersistentVolumeClaim from a Volume Snapshot&lt;/a> for more details.&lt;/p>
&lt;p>When leveraging the above capability, there is no logic that validates whether the mode of the
original volume, whose snapshot was taken, matches the mode of the newly created volume.&lt;/p>
&lt;p>This presents a security gap that allows malicious users to potentially exploit an
as-yet-unknown vulnerability in the host operating system.&lt;/p>
&lt;p>Many popular storage backup vendors convert the volume mode during the course of a
backup operation, for efficiency purposes, which prevents Kubernetes from blocking
the operation completely and presents a challenge in distinguishing trusted
users from malicious ones.&lt;/p>
&lt;h3 id="preventing-unauthorised-users-from-converting-the-volume-mode">Preventing unauthorised users from converting the volume mode&lt;/h3>
&lt;p>In this context, an authorised user is one who has access rights to perform &lt;code>Update&lt;/code>
or &lt;code>Patch&lt;/code> operations on &lt;code>VolumeSnapshotContents&lt;/code>, which is a cluster-level resource.&lt;br>
It is upto the cluster administrator to provide these rights only to trusted users
or applications, like backup vendors.&lt;/p>
&lt;p>If the alpha feature is &lt;a href="https://kubernetes-csi.github.io/docs/">enabled&lt;/a> in
&lt;code>snapshot-controller&lt;/code>, &lt;code>snapshot-validation-webhook&lt;/code> and &lt;code>external-provisioner&lt;/code>,
then unauthorised users will not be allowed to modify the volume mode of a PVC
when it is being created from a &lt;code>VolumeSnapshot&lt;/code>.&lt;/p>
&lt;p>To convert the volume mode, an authorised user must do the following:&lt;/p>
&lt;ol>
&lt;li>Identify the &lt;code>VolumeSnapshot&lt;/code> that is to be used as the data source for a newly
created PVC in the given namespace.&lt;/li>
&lt;li>Identify the &lt;code>VolumeSnapshotContent&lt;/code> bound to the above &lt;code>VolumeSnapshot&lt;/code>.&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl get volumesnapshot -n &amp;lt;namespace&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ol start="3">
&lt;li>
&lt;p>Add the annotation &lt;a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#snapshot-storage-kubernetes-io-allowvolumemodechange">&lt;code>snapshot.storage.kubernetes.io/allowVolumeModeChange&lt;/code>&lt;/a>
to the &lt;code>VolumeSnapshotContent&lt;/code>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>This annotation can be added either via software or manually by the authorised
user. The &lt;code>VolumeSnapshotContent&lt;/code> annotation must look like following manifest fragment:&lt;/p>
&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">snapshot.storage.kubernetes.io/allowVolumeModeChange&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#00f;font-weight:bold">...&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Note&lt;/strong>: For pre-provisioned &lt;code>VolumeSnapshotContents&lt;/code>, you must take an extra
step of setting &lt;code>spec.sourceVolumeMode&lt;/code> field to either &lt;code>Filesystem&lt;/code> or &lt;code>Block&lt;/code>,
depending on the mode of the volume from which this snapshot was taken.&lt;/p>
&lt;p>An example is shown below:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>snapshot.storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VolumeSnapshotContent&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">annotations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">snapshot.storage.kubernetes.io/allowVolumeModeChange&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>new-snapshot-content-test&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">deletionPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">driver&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hostpath.csi.k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">source&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">snapshotHandle&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>7bdd0de3-aaeb-11e8-9aae-0242ac110002&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">sourceVolumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeSnapshotRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>new-snapshot-test&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Repeat steps 1 to 3 for all &lt;code>VolumeSnapshotContents&lt;/code> whose volume mode needs to be
converted during a backup or restore operation.&lt;/p>
&lt;p>If the annotation shown in step 4 above is present on a &lt;code>VolumeSnapshotContent&lt;/code>
object, Kubernetes will not prevent the volume mode from being converted.
Users should keep this in mind before they attempt to add the annotation
to any &lt;code>VolumeSnapshotContent&lt;/code>.&lt;/p>
&lt;h3 id="what-s-next">What's next&lt;/h3>
&lt;p>&lt;a href="https://kubernetes-csi.github.io/docs/">Enable this feature&lt;/a> and let us know
what you think!&lt;/p>
&lt;p>We hope this feature causes no disruption to existing workflows while preventing
malicious users from exploiting security vulnerabilities in their clusters.&lt;/p>
&lt;p>For any queries or issues, join &lt;a href="https://slack.k8s.io/">Kubernetes on Slack&lt;/a> and
create a thread in the #sig-storage channel. Alternately, create an issue in the
CSI external-snapshotter &lt;a href="https://github.com/kubernetes-csi/external-snapshotter">repository&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: Volume Populators Graduate to Beta</title><link>https://kubernetes.io/blog/2022/05/16/volume-populators-beta/</link><pubDate>Mon, 16 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/16/volume-populators-beta/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong>
Ben Swartzlander (NetApp)&lt;/p>
&lt;p>The volume populators feature is now two releases old and entering beta! The &lt;code>AnyVolumeDataSource&lt;/code> feature
gate defaults to enabled in Kubernetes v1.24, which means that users can specify any custom resource
as the data source of a PVC.&lt;/p>
&lt;p>An &lt;a href="https://kubernetes.io/blog/2021/08/30/volume-populators-redesigned/">earlier blog article&lt;/a> detailed how the
volume populators feature works. In short, a cluster administrator can install a CRD and
associated populator controller in the cluster, and any user who can create instances of
the CR can create pre-populated volumes by taking advantage of the populator.&lt;/p>
&lt;p>Multiple populators can be installed side by side for different purposes. The SIG storage
community is already seeing some implementations in public, and more prototypes should
appear soon.&lt;/p>
&lt;p>Cluster administrations are &lt;strong>strongly encouraged&lt;/strong> to install the
volume-data-source-validator controller and associated &lt;code>VolumePopulator&lt;/code> CRD before installing
any populators so that users can get feedback about invalid PVC data sources.&lt;/p>
&lt;h2 id="new-features">New Features&lt;/h2>
&lt;p>The &lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a> library
on which populators are built now includes metrics to help operators monitor and detect
problems. This library is now beta and latest release is v1.0.1.&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes-csi/volume-data-source-validator">volume data source validator&lt;/a>
controller also has metrics support added, and is in beta. The &lt;code>VolumePopulator&lt;/code> CRD is
beta and the latest release is v1.0.1.&lt;/p>
&lt;h2 id="trying-it-out">Trying it out&lt;/h2>
&lt;p>To see how this works, you can install the sample &amp;quot;hello&amp;quot; populator and try it
out.&lt;/p>
&lt;p>First install the volume-data-source-validator controller.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/client/config/crd/populator.storage.k8s.io_volumepopulators.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/rbac-data-source-validator.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/volume-data-source-validator/v1.0.1/deploy/kubernetes/setup-data-source-validator.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next install the example populator.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/v1.0.1/example/hello-populator/crd.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/lib-volume-populator/87a47467b86052819e9ad13d15036d65b9a32fbb/example/hello-populator/deploy.yaml
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Your cluster now has a new CustomResourceDefinition that provides a test API named Hello.
Create an instance of the &lt;code>Hello&lt;/code> custom resource, with some text:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.example.com/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">fileContents&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello, world!&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create a PVC that refers to that CR as its data source.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10Mi&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">dataSourceRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hello.example.com&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Hello&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-hello&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Filesystem&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next, run a Job that reads the file in the PVC.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>batch/v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Job&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-job&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-container&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>busybox:latest&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- cat&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- /mnt/example.txt&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/mnt&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">restartPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Never&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>vol&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">persistentVolumeClaim&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">claimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Wait for the job to complete (including all of its dependencies).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl &lt;span style="color:#a2f">wait&lt;/span> --for&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b8860b">condition&lt;/span>&lt;span style="color:#666">=&lt;/span>Complete job/example-job
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And last examine the log from the job.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl logs job/example-job
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output should be:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-terminal" data-lang="terminal">Hello, world!
&lt;/code>&lt;/pre>&lt;p>Note that the volume already contained a text file with the string contents from
the CR. This is only the simplest example. Actual populators can set up the volume
to contain arbitrary contents.&lt;/p>
&lt;h2 id="how-to-write-your-own-volume-populator">How to write your own volume populator&lt;/h2>
&lt;p>Developers interested in writing new poplators are encouraged to use the
&lt;a href="https://github.com/kubernetes-csi/lib-volume-populator">lib-volume-populator&lt;/a> library
and to only supply a small controller wrapper around the library, and a pod image
capable of attaching to volumes and writing the appropriate data to the volume.&lt;/p>
&lt;p>Individual populators can be extremely generic such that they work with every type
of PVC, or they can do vendor specific things to rapidly fill a volume with data
if the volume was provisioned by a specific CSI driver from the same vendor, for
example, by communicating directly with the storage for that volume.&lt;/p>
&lt;h2 id="how-can-i-learn-more">How can I learn more?&lt;/h2>
&lt;p>The enhancement proposal,
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1495-volume-populators">Volume Populators&lt;/a>, includes lots of detail about the history and technical implementation
of this feature.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-populators-and-data-sources">Volume populators and data sources&lt;/a>, within the documentation topic about persistent volumes,
explains how to use this feature in your cluster.&lt;/p>
&lt;p>Please get involved by joining the Kubernetes storage SIG to help us enhance this
feature. There are a lot of good ideas already and we'd be thrilled to have more!&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: gRPC container probes in beta</title><link>https://kubernetes.io/blog/2022/05/13/grpc-probes-now-in-beta/</link><pubDate>Fri, 13 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/13/grpc-probes-now-in-beta/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Sergey Kanzhelev (Google)&lt;/p>
&lt;p>With Kubernetes 1.24 the gRPC probes functionality entered beta and is available by default.
Now you can configure startup, liveness, and readiness probes for your gRPC app
without exposing any HTTP endpoint, nor do you need an executable. Kubernetes can natively connect to your your workload via gRPC and query its status.&lt;/p>
&lt;h2 id="some-history">Some history&lt;/h2>
&lt;p>It's useful to let the system managing your workload check that the app is
healthy, has started OK, and whether the app considers itself good to accept
traffic. Before the gRPC support was added, Kubernetes already allowed you to
check for health based on running an executable from inside the container image,
by making an HTTP request, or by checking whether a TCP connection succeeded.&lt;/p>
&lt;p>For most apps, those checks are enough. If your app provides a gRPC endpoint
for a health (or readiness) check, it is easy
to repurpose the &lt;code>exec&lt;/code> probe to use it for gRPC health checking.
In the blog article &lt;a href="https://kubernetes.io/blog/2018/10/01/health-checking-grpc-servers-on-kubernetes/">Health checking gRPC servers on Kubernetes&lt;/a>,
Ahmet Alp Balkan described how you can do that — a mechanism that still works today.&lt;/p>
&lt;p>There is a commonly used tool to enable this that was &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/commit/2df4478982e95c9a57d5fe3f555667f4365c025d">created&lt;/a>
on August 21, 2018, and with
the first release at &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/releases/tag/v0.1.0-alpha.1">Sep 19, 2018&lt;/a>.&lt;/p>
&lt;p>This approach for gRPC apps health checking is very popular. There are &lt;a href="https://github.com/search?l=Dockerfile&amp;amp;q=grpc_health_probe&amp;amp;type=code">3,626 Dockerfiles&lt;/a>
with the &lt;code>grpc_health_probe&lt;/code> and &lt;a href="https://github.com/search?l=YAML&amp;amp;q=grpc_health_probe&amp;amp;type=Code">6,621 yaml&lt;/a> files that are discovered with the
basic search on GitHub (at the moment of writing). This is good indication of the tool popularity
and the need to support this natively.&lt;/p>
&lt;p>Kubernetes v1.23 introduced an alpha-quality implementation of native support for
querying a workload status using gRPC. Because it was an alpha feature,
this was disabled by default for the v1.23 release.&lt;/p>
&lt;h2 id="using-the-feature">Using the feature&lt;/h2>
&lt;p>We built gRPC health checking in similar way with other probes and believe
it will be &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe">easy to use&lt;/a>
if you are familiar with other probe types in Kubernetes.
The natively supported health probe has many benefits over the workaround involving &lt;code>grpc_health_probe&lt;/code> executable.&lt;/p>
&lt;p>With the native gRPC support you don't need to download and carry &lt;code>10MB&lt;/code> of an additional executable with your image.
Exec probes are generally slower than a gRPC call as they require instantiating a new process to run an executable.
It also makes the checks less sensible for edge cases when the pod is running at maximum resources and has troubles
instantiating new processes.&lt;/p>
&lt;p>There are a few limitations though. Since configuring a client certificate for probes is hard,
services that require client authentication are not supported. The built-in probes are also
not checking the server certificates and ignore related problems.&lt;/p>
&lt;p>Built-in checks also cannot be configured to ignore certain types of errors
(&lt;code>grpc_health_probe&lt;/code> returns different exit codes for different errors),
and cannot be &amp;quot;chained&amp;quot; to run the health check on multiple services in a single probe.&lt;/p>
&lt;p>But all these limitations are quite standard for gRPC and there are easy workarounds
for those.&lt;/p>
&lt;h2 id="try-it-for-yourself">Try it for yourself&lt;/h2>
&lt;h3 id="cluster-level-setup">Cluster-level setup&lt;/h3>
&lt;p>You can try this feature today. To try native gRPC probes, you can spin up a Kubernetes cluster
yourself with the &lt;code>GRPCContainerProbe&lt;/code> feature gate enabled, there are many &lt;a href="https://kubernetes.io/docs/tasks/tools/">tools available&lt;/a>.&lt;/p>
&lt;p>Since the feature gate &lt;code>GRPCContainerProbe&lt;/code> is enabled by default in 1.24,
many vendors will have this functionality working out of the box.
So you may just create an 1.24 cluster on platform of your choice. Some vendors
allow to enable alpha features on 1.23 clusters.&lt;/p>
&lt;p>For example, at the moment of writing, you can spin up the test cluster on GKE for a quick test.
Other vendors may also have similar capabilities, especially if you
are reading this blog post long after the Kubernetes 1.24 release.&lt;/p>
&lt;p>On GKE use the following command (note, version is &lt;code>1.23&lt;/code> and &lt;code>enable-kubernetes-alpha&lt;/code> are specified).&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>gcloud container clusters create test-grpc &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --enable-kubernetes-alpha &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --no-enable-autorepair &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --no-enable-autoupgrade &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --release-channel&lt;span style="color:#666">=&lt;/span>rapid &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --cluster-version&lt;span style="color:#666">=&lt;/span>1.23
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You will also need to configure &lt;code>kubectl&lt;/code> to access the cluster:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>gcloud container clusters get-credentials test-grpc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="trying-the-feature-out">Trying the feature out&lt;/h3>
&lt;p>Let's create the pod to test how gRPC probes work. For this test we will use the &lt;code>agnhost&lt;/code> image.
This is a k8s maintained image with that can be used for all sorts of workload testing.
For example, it has a useful &lt;a href="https://github.com/kubernetes/kubernetes/blob/b2c5bd2a278288b5ef19e25bf7413ecb872577a4/test/images/agnhost/README.md#grpc-health-checking">grpc-health-checking&lt;/a> module
that exposes two ports - one is serving health checking service,
another - http port to react on commands &lt;code>make-serving&lt;/code> and &lt;code>make-not-serving&lt;/code>.&lt;/p>
&lt;p>Here is an example pod definition. It starts the &lt;code>grpc-health-checking&lt;/code> module,
exposes ports &lt;code>5000&lt;/code> and &lt;code>8080&lt;/code>, and configures gRPC readiness probe:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#00f;font-weight:bold">---&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>test-grpc&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#008000;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>agnhost&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8s.gcr.io/e2e-test-images/agnhost:2.35&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;/agnhost&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;grpc-health-checking&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">5000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#008000;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">readinessProbe&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">grpc&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">5000&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If the file called &lt;code>test.yaml&lt;/code>, you can create the pod and check it's status.
The pod will be in ready state as indicated by the snippet of the output.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl apply -f test.yaml
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>kubectl describe test-grpc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output will contain something like this:&lt;/p>
&lt;pre tabindex="0">&lt;code>Conditions:
Type Status
Initialized True
Ready True
ContainersReady True
PodScheduled True
&lt;/code>&lt;/pre>&lt;p>Now let's change the health checking endpoint status to NOT_SERVING.
In order to call the http port of the Pod, let's create a port forward:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl port-forward test-grpc 8080:8080
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can &lt;code>curl&lt;/code> to call the command...&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>curl http://localhost:8080/make-not-serving
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>... and in a few seconds the port status will switch to not ready.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-shell" data-lang="shell">&lt;span style="display:flex;">&lt;span>kubectl describe pod test-grpc
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output now will have:&lt;/p>
&lt;pre tabindex="0">&lt;code>Conditions:
Type Status
Initialized True
Ready False
ContainersReady False
PodScheduled True
...
Warning Unhealthy 2s (x6 over 42s) kubelet Readiness probe failed: service unhealthy (responded with &amp;#34;NOT_SERVING&amp;#34;)
&lt;/code>&lt;/pre>&lt;p>Once it is switched back, in about one second the Pod will get back to ready status:&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-bsh" data-lang="bsh">curl http://localhost:8080/make-serving
kubectl describe test-grpc
&lt;/code>&lt;/pre>&lt;p>The output indicates that the Pod went back to being &lt;code>Ready&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>Conditions:
Type Status
Initialized True
Ready True
ContainersReady True
PodScheduled True
&lt;/code>&lt;/pre>&lt;p>This new built-in gRPC health probing on Kubernetes makes implementing a health-check via gRPC
much easier than the older approach that relied on using a separate &lt;code>exec&lt;/code> probe. Read through
the official
&lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-a-grpc-liveness-probe">documentation&lt;/a>
to learn more and provide feedback before the feature will be promoted to GA.&lt;/p>
&lt;h2 id="summary">Summary&lt;/h2>
&lt;p>Kubernetes is a popular workload orchestration platform and we add features based on feedback and demand.
Features like gRPC probes support is a minor improvement that will make life of many app developers
easier and apps more resilient. Try it today and give feedback, before the feature went into GA.&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: Storage Capacity Tracking Now Generally Available</title><link>https://kubernetes.io/blog/2022/05/06/storage-capacity-ga/</link><pubDate>Fri, 06 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/06/storage-capacity-ga/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>The v1.24 release of Kubernetes brings &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">storage capacity&lt;/a>
tracking as a generally available feature.&lt;/p>
&lt;h2 id="problems-we-have-solved">Problems we have solved&lt;/h2>
&lt;p>As explained in more detail in the &lt;a href="https://kubernetes.io/blog/2021/04/14/local-storage-features-go-beta/">previous blog post about this
feature&lt;/a>, storage capacity
tracking allows a CSI driver to publish information about remaining
capacity. The kube-scheduler then uses that information to pick suitable nodes
for a Pod when that Pod has volumes that still need to be provisioned.&lt;/p>
&lt;p>Without this information, a Pod may get stuck without ever being scheduled onto
a suitable node because kube-scheduler has to choose blindly and always ends up
picking a node for which the volume cannot be provisioned because the
underlying storage system managed by the CSI driver does not have sufficient
capacity left.&lt;/p>
&lt;p>Because CSI drivers publish storage capacity information that gets used at a
later time when it might not be up-to-date anymore, it can still happen that a
node is picked that doesn't work out after all. Volume provisioning recovers
from that by informing the scheduler that it needs to try again with a
different node.&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/master/docs/storage-capacity-tracking.md">Load
tests&lt;/a>
that were done again for promotion to GA confirmed that all storage in a
cluster can be consumed by Pods with storage capacity tracking whereas Pods got
stuck without it.&lt;/p>
&lt;h2 id="problems-we-have-not-solved">Problems we have &lt;em>not&lt;/em> solved&lt;/h2>
&lt;p>Recovery from a failed volume provisioning attempt has one known limitation: if a Pod
uses two volumes and only one of them could be provisioned, then all future
scheduling decisions are limited by the already provisioned volume. If that
volume is local to a node and the other volume cannot be provisioned there, the
Pod is stuck. This problem pre-dates storage capacity tracking and while the
additional information makes it less likely to occur, it cannot be avoided in
all cases, except of course by only using one volume per Pod.&lt;/p>
&lt;p>An idea for solving this was proposed in a &lt;a href="https://github.com/kubernetes/enhancements/pull/1703">KEP
draft&lt;/a>: volumes that were
provisioned and haven't been used yet cannot have any valuable data and
therefore could be freed and provisioned again elsewhere. SIG Storage is
looking for interested developers who want to continue working on this.&lt;/p>
&lt;p>Also not solved is support in Cluster Autoscaler for Pods with volumes. For CSI
drivers with storage capacity tracking, a prototype was developed and discussed
in &lt;a href="https://github.com/kubernetes/autoscaler/pull/3887">a PR&lt;/a>. It was meant to
work with arbitrary CSI drivers, but that flexibility made it hard to configure
and slowed down scale up operations: because autoscaler was unable to simulate
volume provisioning, it only scaled the cluster by one node at a time, which
was seen as insufficient.&lt;/p>
&lt;p>Therefore that PR was not merged and a different approach with tighter coupling
between autoscaler and CSI driver will be needed. For this a better
understanding is needed about which local storage CSI drivers are used in
combination with cluster autoscaling. Should this lead to a new KEP, then users
will have to try out an implementation in practice before it can move to beta
or GA. So please reach out to SIG Storage if you have an interest in this
topic.&lt;/p>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>Thanks a lot to the members of the community who have contributed to this
feature or given feedback including members of &lt;a href="https://github.com/kubernetes/community/tree/master/sig-scheduling">SIG
Scheduling&lt;/a>,
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-autoscaling">SIG
Autoscaling&lt;/a>,
and of course &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">SIG
Storage&lt;/a>!&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: Volume Expansion Now A Stable Feature</title><link>https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/</link><pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/05/volume-expansion-ga/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Hemant Kumar (Red Hat)&lt;/p>
&lt;p>Volume expansion was introduced as a alpha feature in Kubernetes 1.8 and it went beta in 1.11 and with Kubernetes 1.24 we are excited to announce general availability(GA)
of volume expansion.&lt;/p>
&lt;p>This feature allows Kubernetes users to simply edit their &lt;code>PersistentVolumeClaim&lt;/code> objects and specify new size in PVC Spec and Kubernetes will automatically expand the volume
using storage backend and also expand the underlying file system in-use by the Pod without requiring any downtime at all if possible.&lt;/p>
&lt;h3 id="how-to-use-volume-expansion">How to use volume expansion&lt;/h3>
&lt;p>You can trigger expansion for a PersistentVolume by editing the &lt;code>spec&lt;/code> field of a PVC, specifying a different
(and larger) storage request. For example, given following PVC:&lt;/p>
&lt;pre tabindex="0">&lt;code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
name: myclaim
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1Gi # specify new size here
&lt;/code>&lt;/pre>&lt;p>You can request expansion of the underlying PersistentVolume by specifying a new value instead of old &lt;code>1Gi&lt;/code> size.
Once you've changed the requested size, watch the &lt;code>status.conditions&lt;/code> field of the PVC to see if the
resize has completed.&lt;/p>
&lt;p>When Kubernetes starts expanding the volume - it will add &lt;code>Resizing&lt;/code> condition to the PVC, which will be removed once expansion completes. More information about progress of
expansion operation can also be obtained by monitoring events associated with the PVC:&lt;/p>
&lt;pre tabindex="0">&lt;code>kubectl describe pvc &amp;lt;pvc&amp;gt;
&lt;/code>&lt;/pre>&lt;h3 id="storage-driver-support">Storage driver support&lt;/h3>
&lt;p>Not every volume type however is expandable by default. Some volume types such as - intree hostpath volumes are not expandable at all. For CSI volumes - the CSI driver
must have capability &lt;code>EXPAND_VOLUME&lt;/code> in controller or node service (or both if appropriate). Please refer to documentation of your CSI driver, to find out
if it supports volume expansion.&lt;/p>
&lt;p>Please refer to volume expansion documentation for intree volume types which support volume expansion - &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims">Expanding Persistent Volumes&lt;/a>.&lt;/p>
&lt;p>In general to provide some degree of control over volumes that can be expanded, only dynamically provisioned PVCs whose storage class has &lt;code>allowVolumeExpansion&lt;/code> parameter set to &lt;code>true&lt;/code> are expandable.&lt;/p>
&lt;p>A Kubernetes cluster administrator must edit the appropriate StorageClass object and set
the &lt;code>allowVolumeExpansion&lt;/code> field to &lt;code>true&lt;/code>. For example:&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: gp2-default
provisioner: kubernetes.io/aws-ebs
parameters:
secretNamespace: &amp;#34;&amp;#34;
secretName: &amp;#34;&amp;#34;
allowVolumeExpansion: true
&lt;/code>&lt;/pre>&lt;h3 id="online-expansion-compared-to-offline-expansion">Online expansion compared to offline expansion&lt;/h3>
&lt;p>By default, Kubernetes attempts to expand volumes immediately after user requests a resize.
If one or more Pods are using the volume, Kubernetes tries to expands the volume using an online resize;
as a result volume expansion usually requires no application downtime.
Filesystem expansion on the node is also performed online and hence does not require shutting
down any Pod that was using the PVC.&lt;/p>
&lt;p>If you expand a PersistentVolume that is not in use, Kubernetes does an offline resize (and,
because the volume isn't in use, there is again no workload disruption).&lt;/p>
&lt;p>In some cases though - if underlying Storage Driver can only support offline expansion, users of the PVC must take down their Pod before expansion can succeed. Please refer to documentation of your storage
provider to find out - what mode of volume expansion it supports.&lt;/p>
&lt;p>When volume expansion was introduced as an alpha feature, Kubernetes only supported offline filesystem
expansion on the node and hence required users to restart their pods for file system resizing to finish.
His behaviour has been changed and Kubernetes tries its best to fulfil any resize request regardless
of whether the underlying PersistentVolume volume is online or offline. If your storage provider supports
online expansion then no Pod restart should be necessary for volume expansion to finish.&lt;/p>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>Although volume expansion is now stable as part of the recent v1.24 release,
SIG Storage are working to make it even simpler for users of Kubernetes to expand their persistent storage.
Kubernetes 1.23 introduced features for triggering recovery from failed volume expansion, allowing users
to attempt self-service healing after a failed resize.
See &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#recovering-from-failure-when-expanding-volumes">Recovering from volume expansion failure&lt;/a> for more details.&lt;/p>
&lt;p>The Kubernetes contributor community is also discussing the potential for StatefulSet-driven storage expansion. This proposed
feature would let you trigger expansion for all underlying PVs that are providing storage to a StatefulSet,
by directly editing the StatefulSet object.
See the &lt;a href="https://github.com/kubernetes/enhancements/issues/661">Support Volume Expansion Through StatefulSets&lt;/a> enhancement proposal for more details.&lt;/p></description></item><item><title>Blog: Dockershim: The Historical Context</title><link>https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/03/dockershim-historical-context/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kat Cosgrove&lt;/p>
&lt;p>Dockershim has been removed as of Kubernetes v1.24, and this is a positive move for the project. However, context is important for fully understanding something, be it socially or in software development, and this deserves a more in-depth review. Alongside the dockershim removal in Kubernetes v1.24, we’ve seen some confusion (sometimes at a panic level) and dissatisfaction with this decision in the community, largely due to a lack of context around this removal. The decision to deprecate and eventually remove dockershim from Kubernetes was not made quickly or lightly. Still, it’s been in the works for so long that many of today’s users are newer than that decision, and certainly newer than the choices that led to the dockershim being necessary in the first place.&lt;/p>
&lt;p>So what is the dockershim, and why is it going away?&lt;/p>
&lt;p>In the early days of Kubernetes, we only supported one container runtime. That runtime was Docker Engine. Back then, there weren’t really a lot of other options out there and Docker was the dominant tool for working with containers, so this was not a controversial choice. Eventually, we started adding more container runtimes, like rkt and hypernetes, and it became clear that Kubernetes users want a choice of runtimes working best for them. So Kubernetes needed a way to allow cluster operators the flexibility to use whatever runtime they choose.&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface&lt;/a> (CRI) was released to allow that flexibility. The introduction of CRI was great for the project and users alike, but it did introduce a problem: Docker Engine’s use as a container runtime predates CRI, and Docker Engine is not CRI-compatible. To solve this issue, a small software shim (dockershim) was introduced as part of the kubelet component specifically to fill in the gaps between Docker Engine and CRI, allowing cluster operators to continue using Docker Engine as their container runtime largely uninterrupted.&lt;/p>
&lt;p>However, this little software shim was never intended to be a permanent solution. Over the course of years, its existence has introduced a lot of unnecessary complexity to the kubelet itself. Some integrations are inconsistently implemented for Docker because of this shim, resulting in an increased burden on maintainers, and maintaining vendor-specific code is not in line with our open source philosophy. To reduce this maintenance burden and move towards a more collaborative community in support of open standards, &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">KEP-2221 was introduced&lt;/a>, proposing the removal of the dockershim. With the release of Kubernetes v1.20, the deprecation was official.&lt;/p>
&lt;p>We didn’t do a great job communicating this, and unfortunately, the deprecation announcement led to some panic within the community. Confusion around what this meant for Docker as a company, if container images built by Docker would still run, and what Docker Engine actually is led to a conflagration on social media. This was our fault; we should have more clearly communicated what was happening and why at the time. To combat this, we released &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">a blog&lt;/a> and &lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">accompanying FAQ&lt;/a> to allay the community’s fears and correct some misconceptions about what Docker is and how containers work within Kubernetes. As a result of the community’s concerns, Docker and Mirantis jointly agreed to continue supporting the dockershim code in the form of &lt;a href="https://www.mirantis.com/blog/the-future-of-dockershim-is-cri-dockerd/">cri-dockerd&lt;/a>, allowing you to continue using Docker Engine as your container runtime if need be. For the interest of users who want to try other runtimes, like containerd or cri-o, &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">migration documentation was written&lt;/a>.&lt;/p>
&lt;p>We later &lt;a href="https://kubernetes.io/blog/2021/11/12/are-you-ready-for-dockershim-removal/">surveyed the community&lt;/a> and &lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim">discovered that there are still many users with questions and concerns&lt;/a>. In response, Kubernetes maintainers and the CNCF committed to addressing these concerns by extending documentation and other programs. In fact, this blog post is a part of this program. With so many end users successfully migrated to other runtimes, and improved documentation, we believe that everyone has a paved way to migration now.&lt;/p>
&lt;p>Docker is not going away, either as a tool or as a company. It’s an important part of the cloud native community and the history of the Kubernetes project. We wouldn’t be where we are without them. That said, removing dockershim from kubelet is ultimately good for the community, the ecosystem, the project, and open source at large. This is an opportunity for all of us to come together to support open standards, and we’re glad to be doing so with the help of Docker and the community.&lt;/p></description></item><item><title>Blog: Kubernetes 1.24: Stargazer</title><link>https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/</link><pubDate>Tue, 03 May 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/05/03/kubernetes-1-24-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.24/release-team.md">Kubernetes 1.24 Release Team&lt;/a>&lt;/p>
&lt;p>We are excited to announce the release of Kubernetes 1.24, the first release of 2022!&lt;/p>
&lt;p>This release consists of 46 enhancements: fourteen enhancements have graduated to stable,
fifteen enhancements are moving to beta, and thirteen enhancements are entering alpha.
Also, two features have been deprecated, and two features have been removed.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="dockershim-removed-from-kubelet">Dockershim Removed from kubelet&lt;/h3>
&lt;p>After its deprecation in v1.20, the dockershim component has been removed from the kubelet in Kubernetes v1.24.
From v1.24 onwards, you will need to either use one of the other &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">supported runtimes&lt;/a> (such as containerd or CRI-O)
or use cri-dockerd if you are relying on Docker Engine as your container runtime.
For more information about ensuring your cluster is ready for this removal, please
see &lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">this guide&lt;/a>.&lt;/p>
&lt;h3 id="beta-apis-off-by-default">Beta APIs Off by Default&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/3136">New beta APIs will not be enabled in clusters by default&lt;/a>.
Existing beta APIs and new versions of existing beta APIs will continue to be enabled by default.&lt;/p>
&lt;h3 id="signing-release-artifacts">Signing Release Artifacts&lt;/h3>
&lt;p>Release artifacts are &lt;a href="https://github.com/kubernetes/enhancements/issues/3031">signed&lt;/a> using &lt;a href="https://github.com/sigstore/cosign">cosign&lt;/a>
signatures,
and there is experimental support for &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-images/">verifying image signatures&lt;/a>.
Signing and verification of release artifacts is part of &lt;a href="https://github.com/kubernetes/enhancements/issues/3027">increasing software supply chain security for the Kubernetes release process&lt;/a>.&lt;/p>
&lt;h3 id="openapi-v3">OpenAPI v3&lt;/h3>
&lt;p>Kubernetes 1.24 offers beta support for publishing its APIs in the &lt;a href="https://github.com/kubernetes/enhancements/issues/2896">OpenAPI v3 format&lt;/a>.&lt;/p>
&lt;h3 id="storage-capacity-and-volume-expansion-are-generally-available">Storage Capacity and Volume Expansion Are Generally Available&lt;/h3>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/1472">Storage capacity tracking&lt;/a>
supports exposing currently available storage capacity via &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/#api">CSIStorageCapacity objects&lt;/a>
and enhances scheduling of pods that use CSI volumes with late binding.&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/284">Volume expansion&lt;/a> adds support
for resizing existing persistent volumes.&lt;/p>
&lt;h3 id="nonpreemptingpriority-to-stable">NonPreemptingPriority to Stable&lt;/h3>
&lt;p>This feature adds &lt;a href="https://github.com/kubernetes/enhancements/issues/902">a new option to PriorityClasses&lt;/a>,
which can enable or disable pod preemption.&lt;/p>
&lt;h3 id="storage-plugin-migration">Storage Plugin Migration&lt;/h3>
&lt;p>Work is underway to &lt;a href="https://github.com/kubernetes/enhancements/issues/625">migrate the internals of in-tree storage plugins&lt;/a> to call out to CSI Plugins
while maintaining the original API.
The &lt;a href="https://github.com/kubernetes/enhancements/issues/1490">Azure Disk&lt;/a>
and &lt;a href="https://github.com/kubernetes/enhancements/issues/1489">OpenStack Cinder&lt;/a> plugins
have both been migrated.&lt;/p>
&lt;h3 id="grpc-probes-graduate-to-beta">gRPC Probes Graduate to Beta&lt;/h3>
&lt;p>With Kubernetes 1.24, the &lt;a href="https://github.com/kubernetes/enhancements/issues/2727">gRPC probes functionality&lt;/a>
has entered beta and is available by default. You can now &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes">configure startup, liveness, and readiness probes&lt;/a> for your gRPC app
natively within Kubernetes without exposing an HTTP endpoint or
using an extra executable.&lt;/p>
&lt;h3 id="kubelet-credential-provider-graduates-to-beta">Kubelet Credential Provider Graduates to Beta&lt;/h3>
&lt;p>Originally released as Alpha in Kubernetes 1.20, the kubelet's support for
&lt;a href="https://kubernetes.io/docs/tasks/kubelet-credential-provider/kubelet-credential-provider/">image credential providers&lt;/a>
has now graduated to Beta.
This allows the kubelet to dynamically retrieve credentials for a container image registry
using exec plugins rather than storing credentials on the node's filesystem.&lt;/p>
&lt;h3 id="contextual-logging-in-alpha">Contextual Logging in Alpha&lt;/h3>
&lt;p>Kubernetes 1.24 has introduced &lt;a href="https://github.com/kubernetes/enhancements/issues/3077">contextual logging&lt;/a>
that enables the caller of a function to control all aspects of logging (output formatting, verbosity, additional values, and names).&lt;/p>
&lt;h3 id="avoiding-collisions-in-ip-allocation-to-services">Avoiding Collisions in IP allocation to Services&lt;/h3>
&lt;p>Kubernetes 1.24 introduces a new opt-in feature that allows you to
&lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/#service-ip-static-sub-range">soft-reserve a range for static IP address assignments&lt;/a>
to Services.
With the manual enablement of this feature, the cluster will prefer automatic assignment from
the pool of Service IP addresses, thereby reducing the risk of collision.&lt;/p>
&lt;p>A Service &lt;code>ClusterIP&lt;/code> can be assigned:&lt;/p>
&lt;ul>
&lt;li>dynamically, which means the cluster will automatically pick a free IP within the configured Service IP range.&lt;/li>
&lt;li>statically, which means the user will set one IP within the configured Service IP range.&lt;/li>
&lt;/ul>
&lt;p>Service &lt;code>ClusterIP&lt;/code> are unique; hence, trying to create a Service with a &lt;code>ClusterIP&lt;/code> that has already been allocated will return an error.&lt;/p>
&lt;h3 id="dynamic-kubelet-configuration-is-removed-from-the-kubelet">Dynamic Kubelet Configuration is Removed from the Kubelet&lt;/h3>
&lt;p>After being deprecated in Kubernetes 1.22, Dynamic Kubelet Configuration has been removed from the kubelet. The feature will be removed from the API server in Kubernetes 1.26.&lt;/p>
&lt;h2 id="cni-version-related-breaking-change">CNI Version-Related Breaking Change&lt;/h2>
&lt;p>Before you upgrade to Kubernetes 1.24, please verify that you are using/upgrading to a container
runtime that has been tested to work correctly with this release.&lt;/p>
&lt;p>For example, the following container runtimes are being prepared, or have already been prepared, for Kubernetes:&lt;/p>
&lt;ul>
&lt;li>containerd v1.6.4 and later, v1.5.11 and later&lt;/li>
&lt;li>CRI-O 1.24 and later&lt;/li>
&lt;/ul>
&lt;p>Service issues exist for pod CNI network setup and tear down in containerd
v1.6.0–v1.6.3 when the CNI plugins have not been upgraded and/or the CNI config
version is not declared in the CNI config files. The containerd team reports, &amp;quot;these issues are resolved in containerd v1.6.4.&amp;quot;&lt;/p>
&lt;p>With containerd v1.6.0–v1.6.3, if you do not upgrade the CNI plugins and/or
declare the CNI config version, you might encounter the following &amp;quot;Incompatible
CNI versions&amp;quot; or &amp;quot;Failed to destroy network for sandbox&amp;quot; error conditions.&lt;/p>
&lt;h2 id="csi-snapshot">CSI Snapshot&lt;/h2>
&lt;p>&lt;em>This information was added after initial publication.&lt;/em>&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/enhancements/issues/177">VolumeSnapshot v1beta1 CRD has been removed&lt;/a>.
Volume snapshot and restore functionality for Kubernetes and the Container Storage Interface (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, moved to GA in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.20 and is now unsupported. Refer to &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot">KEP-177: CSI Snapshot&lt;/a> and &lt;a href="https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/">Volume Snapshot GA blog&lt;/a> for more information.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduations-to-stable">Graduations to Stable&lt;/h3>
&lt;p>This release saw fourteen enhancements promoted to stable:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/284">Container Storage Interface (CSI) Volume Expansion&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/688">Pod Overhead&lt;/a>: Account for resources tied to the pod sandbox but not specific containers.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/902">Add non-preempting option to PriorityClasses&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1472">Storage Capacity Tracking&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1489">OpenStack Cinder In-Tree to CSI Driver Migration&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1490">Azure Disk In-Tree to CSI Driver Migration&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1904">Efficient Watch Resumption&lt;/a>: Watch can be efficiently resumed after kube-apiserver reboot.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1959">Service Type=LoadBalancer Class Field&lt;/a>: Introduce a new Service annotation &lt;code>service.kubernetes.io/load-balancer-class&lt;/code> that allows multiple implementations of &lt;code>type: LoadBalancer&lt;/code> Services in the same cluster.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2214">Indexed Job&lt;/a>: Add a completion index to Pods of Jobs with a fixed completion count.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2232">Add Suspend Field to Jobs API&lt;/a>: Add a suspend field to the Jobs API to allow orchestrators to create jobs with more control over when pods are created.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2249">Pod Affinity NamespaceSelector&lt;/a>: Add a &lt;code>namespaceSelector&lt;/code> field for to pod affinity/anti-affinity spec.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2436">Leader Migration for Controller Managers&lt;/a>: kube-controller-manager and cloud-controller-manager can apply new controller-to-controller-manager assignment in HA control plane without downtime.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2784">CSR Duration&lt;/a>: Extend the CertificateSigningRequest API with a mechanism to allow clients to request a specific duration for the issued certificate.&lt;/li>
&lt;/ul>
&lt;h3 id="major-changes">Major Changes&lt;/h3>
&lt;p>This release saw two major changes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2221">Dockershim Removal&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/3136">Beta APIs are off by Default&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="release-notes">Release Notes&lt;/h3>
&lt;p>Check out the full details of the Kubernetes 1.24 release in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">release notes&lt;/a>.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Kubernetes 1.24 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.24.0">GitHub&lt;/a>.
To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local
Kubernetes clusters using containers as “nodes”, with &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a>.
You can also easily install 1.24 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h3 id="release-team">Release Team&lt;/h3>
&lt;p>This release would not have been possible without the combined efforts of committed individuals
comprising the Kubernetes 1.24 release team. This team came together to deliver all of the components
that go into each Kubernetes release, including code, documentation, release notes, and more.&lt;/p>
&lt;p>Special thanks to James Laverack, our release lead, for guiding us through a successful release cycle,
and to all of the release team members for the time and effort they put in to deliver the v1.24
release for the Kubernetes community.&lt;/p>
&lt;h3 id="release-theme-and-logo">Release Theme and Logo&lt;/h3>
&lt;p>&lt;strong>Kubernetes 1.24: Stargazer&lt;/strong>&lt;/p>
&lt;figure class="release-logo">
&lt;img src="https://kubernetes.io/images/blog/2022-05-03-kubernetes-release-1.24/kubernetes-1.24.png"/>
&lt;/figure>
&lt;p>The theme for Kubernetes 1.24 is &lt;em>Stargazer&lt;/em>.&lt;/p>
&lt;p>Generations of people have looked to the stars in awe and wonder, from ancient astronomers to the
scientists who built the James Webb Space Telescope. The stars have inspired us, set our imagination
alight, and guided us through long nights on difficult seas.&lt;/p>
&lt;p>With this release we gaze upwards, to what is possible when our community comes together. Kubernetes
is the work of hundreds of contributors across the globe and thousands of end-users supporting
applications that serve millions. Every one is a star in our sky, helping us chart our course.&lt;/p>
&lt;p>The release logo is made by &lt;a href="https://www.instagram.com/artsyfie/">Britnee Laverack&lt;/a>, and depicts a telescope set upon starry skies and the
&lt;a href="https://en.wikipedia.org/wiki/Pleiades">Pleiades&lt;/a>, often known in mythology as the “Seven Sisters”. The number seven is especially auspicious
for the Kubernetes project, and is a reference back to our original “Project Seven” name.&lt;/p>
&lt;p>This release of Kubernetes is named for those that would look towards the night sky and wonder — for
all the stargazers out there. ✨&lt;/p>
&lt;h3 id="user-highlights">User Highlights&lt;/h3>
&lt;ul>
&lt;li>Check out how leading retail e-commerce company &lt;a href="https://www.cncf.io/case-studies/la-redoute/">La Redoute used Kubernetes, alongside other CNCF projects, to transform and streamline its software delivery lifecycle&lt;/a> - from development to operations.&lt;/li>
&lt;li>Trying to ensure no change to an API call would cause any breaks, &lt;a href="https://www.cncf.io/case-studies/salt-security/">Salt Security built its microservices entirely on Kubernetes, and it communicates via gRPC while Linkerd ensures messages are encrypted&lt;/a>.&lt;/li>
&lt;li>In their effort to migrate from private to public cloud, &lt;a href="https://www.cncf.io/case-studies/allianz/">Allainz Direct engineers redesigned its CI/CD pipeline in just three months while managing to condense 200 workflows down to 10-15&lt;/a>.&lt;/li>
&lt;li>Check out how &lt;a href="https://www.cncf.io/case-studies/bink/">Bink, a UK based fintech company, updated its in-house Kubernetes distribution with Linkerd to build a cloud-agnostic platform that scales as needed whilst allowing them to keep a close eye on performance and stability&lt;/a>.&lt;/li>
&lt;li>Using Kubernetes, the Dutch organization &lt;a href="http://www.stichtingopennederland.nl/">Stichting Open Nederland&lt;/a> created a testing portal in just one-and-a-half months to help safely reopen events in the Netherlands. The &lt;a href="https://www.testenvoortoegang.org/">Testing for Entry (Testen voor Toegang)&lt;/a> platform &lt;a href="https://www.cncf.io/case-studies/true/">leveraged the performance and scalability of Kubernetes to help individuals book over 400,000 COVID-19 testing appointments per day. &lt;/a>&lt;/li>
&lt;li>Working alongside SparkFabrik and utilizing Backstage, &lt;a href="https://www.cncf.io/case-studies/santagostino/">Santagostino created the developer platform Samaritan to centralize services and documentation, manage the entire lifecycle of services, and simplify the work of Santagostino developers&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="ecosystem-updates">Ecosystem Updates&lt;/h3>
&lt;ul>
&lt;li>KubeCon + CloudNativeCon Europe 2022 will take place in Valencia, Spain, from 16 – 20 May 2022! You can find more information about the conference and registration on the &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">event site&lt;/a>.&lt;/li>
&lt;li>In the &lt;a href="https://www.cncf.io/announcements/2022/02/10/cncf-sees-record-kubernetes-and-container-adoption-in-2021-cloud-native-survey/">2021 Cloud Native Survey&lt;/a>, the CNCF saw record Kubernetes and container adoption. Take a look at the &lt;a href="https://www.cncf.io/reports/cncf-annual-survey-2021/">results of the survey&lt;/a>.&lt;/li>
&lt;li>The &lt;a href="https://www.linuxfoundation.org/">Linux Foundation&lt;/a> and &lt;a href="https://www.cncf.io/">The Cloud Native Computing Foundation&lt;/a> (CNCF) announced the availability of a new &lt;a href="https://training.linuxfoundation.org/training/cloudnativedev-bootcamp/?utm_source=lftraining&amp;amp;utm_medium=pr&amp;amp;utm_campaign=clouddevbc0322">Cloud Native Developer Bootcamp&lt;/a> to provide participants with the knowledge and skills to design, build, and deploy cloud native applications. Check out the &lt;a href="https://www.cncf.io/announcements/2022/03/15/new-cloud-native-developer-bootcamp-provides-a-clear-path-to-cloud-native-careers/">announcement&lt;/a> to learn more.&lt;/li>
&lt;/ul>
&lt;h3 id="project-velocity">Project Velocity&lt;/h3>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1&amp;amp;refresh=15m">CNCF K8s DevStats&lt;/a> project
aggregates a number of interesting data points related to the velocity of Kubernetes and various
sub-projects. This includes everything from individual contributions to the number of companies that
are contributing, and is an illustration of the depth and breadth of effort that goes into evolving this ecosystem.&lt;/p>
&lt;p>In the v1.24 release cycle, which &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.24">ran for 17 weeks&lt;/a> (January 10 to May 3), we saw contributions from &lt;a href="https://k8s.devstats.cncf.io/d/9/companies-table?orgId=1&amp;amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;amp;var-metric=contributions">1029 companies&lt;/a> and &lt;a href="https://k8s.devstats.cncf.io/d/66/developer-activity-counts-by-companies?orgId=1&amp;amp;var-period_name=v1.23.0%20-%20v1.24.0&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=Kubernetes&amp;amp;var-country_name=All&amp;amp;var-companies=All&amp;amp;var-repo_name=kubernetes%2Fkubernetes">1179 individuals&lt;/a>.&lt;/p>
&lt;h2 id="upcoming-release-webinar">Upcoming Release Webinar&lt;/h2>
&lt;p>Join members of the Kubernetes 1.24 release team on Tue May 24, 2022 9:45am – 11am PT to learn about
the major features of this release, as well as deprecations and removals to help plan for upgrades.
For more information and registration, visit the &lt;a href="https://community.cncf.io/e/mck3kd/">event page&lt;/a>
on the CNCF Online Programs site.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests.
Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below:&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributors&lt;/a> website&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for the latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="https://serverfault.com/questions/tagged/kubernetes">Server Fault&lt;/a>.&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Frontiers, fsGroups and frogs: the Kubernetes 1.23 release interview</title><link>https://kubernetes.io/blog/2022/04/29/frontiers-fsgroups-and-frogs-the-kubernetes-1.23-release-interview/</link><pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/04/29/frontiers-fsgroups-and-frogs-the-kubernetes-1.23-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>One of the highlights of hosting the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> is talking to the release managers for each new Kubernetes version. The release team is constantly refreshing. Many working their way from small documentation fixes, step up to shadow roles, and then eventually lead a release.&lt;/p>
&lt;p>As we prepare for the 1.24 release next week, &lt;a href="https://www.google.com/search?q=%22release+interview%22+site%3Akubernetes.io%2Fblog">in accordance with long-standing tradition&lt;/a>, I'm pleased to bring you a look back at the story of 1.23. The release was led by &lt;a href="https://twitter.com/reylejano">Rey Lejano&lt;/a>, a Field Engineer at SUSE. &lt;a href="https://kubernetespodcast.com/episode/167-kubernetes-1.23/">I spoke to Rey&lt;/a> in December, as he was awaiting the birth of his first child.&lt;/p>
&lt;p>Make sure you &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe, wherever you get your podcasts&lt;/a>, so you hear all our stories from the Cloud Native community, including the story of 1.24 next week.&lt;/p>
&lt;p>&lt;em>This transcript has been lightly edited and condensed for clarity.&lt;/em>&lt;/p>
&lt;hr>
&lt;p>&lt;strong>CRAIG BOX: I'd like to start with what is, of course, on top of everyone's mind at the moment. Let's talk African clawed frogs!&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] Oh, you mean &lt;a href="https://en.wikipedia.org/wiki/African_clawed_frog">Xenopus lavis&lt;/a>, the scientific name for the African clawed frog?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Of course.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Not many people know, but my background and my degree is actually in microbiology, from the University of California Davis. I did some research for about four years in biochemistry, in a biochemistry lab, and I &lt;a href="https://www.sciencedirect.com/science/article/pii/">do have a research paper published&lt;/a>. It's actually on glycoproteins, particularly something called &amp;quot;cortical granule lectin&amp;quot;. We used frogs, because they generate lots and lots of eggs, from which we can extract the protein. That protein prevents polyspermy. When the sperm goes into the egg, the egg releases a glycoprotein, cortical granule lectin, to the membrane, and prevents any other sperm from going inside the egg.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Were you able to take anything from the testing that we did on frogs and generalize that to higher-order mammals, perhaps?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes. Since mammals also have cortical granule lectin, we were able to analyze both the convergence and the evolutionary pattern, not just from multiple species of frogs, but also into mammals as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, there's a couple of different threads to unravel here. When you were young, what led you into the fields of biology, and perhaps more the technical side of it?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I think it was mostly from family, since I do have a family history in the medical field that goes back generations. So I kind of felt like that was the natural path going into college.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, of course, you're working in a more abstract tech field. What led you out of microbiology?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] Well, I've always been interested in tech. Taught myself a little programming when I was younger, before high school, did some web dev stuff. Just kind of got burnt out being in a lab. I was literally in the basement. I had a great opportunity to join a consultancy that specialized in &lt;a href="https://www.axelos.com/certifications/itil-service-management/what-is-itil">ITIL&lt;/a>. I actually started off with application performance management, went into monitoring, went into operation management and also ITIL, which is aligning your IT asset management and service managements with business services. Did that for a good number of years, actually.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's very interesting, as people describe the things that they went through and perhaps the technologies that they worked on, you can pretty much pinpoint how old they might be. There's a lot of people who come into tech these days that have never heard of ITIL. They have no idea what it is. It's basically just SRE with more process.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes, absolutely. It's not very cloud native. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Not at all.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: You don't really hear about it in the cloud native landscape. Definitely, you can tell someone's been in the field for a little bit, if they specialize or have worked with ITIL before.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned that you wanted to get out of the basement. That is quite often where people put the programmers. Did they just give you a bit of light in the new basement?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [LAUGHS] They did give us much better lighting. Able to get some vitamin D sometimes, as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: To wrap up the discussion about your previous career — over the course of the last year, with all of the things that have happened in the world, I could imagine that microbiology skills may be more in demand than perhaps they were when you studied them?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Oh, absolutely. I could definitely see a big increase of numbers of people going into the field. Also, reading what's going on with the world currently kind of brings back all the education I've learned in the past, as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you keep in touch with people you went through school with?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Just some close friends, but not in the microbiology field.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One thing that I think will probably happen as a result of the pandemic is a renewed interest in some of these STEM fields. It will be interesting to see what impact that has on society at large.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. I think that'll be great.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned working at a consultancy doing IT management, application performance monitoring, and so on. When did Kubernetes come into your professional life?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: One of my good friends at the company I worked at, left in mid-2015. He went on to a company that was pretty heavily into Docker. He taught me a little bit. I did my first &amp;quot;docker run&amp;quot; around 2015, maybe 2016. Then, one of the applications we were using for the ITIL framework was containerized around 2018 or so, also in Kubernetes. At that time, it was pretty buggy. That was my initial introduction to Kubernetes and containerised applications.&lt;/p>
&lt;p>Then I left that company, and I actually joined my friend over at &lt;a href="https://rx-m.com/">RX-M&lt;/a>, which is a cloud native consultancy and training firm. They specialize in Docker and Kubernetes. I was able to get my feet wet. I got my CKD, got my CKA as well. And they were really, really great at encouraging us to learn more about Kubernetes and also to be involved in the community.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You will have seen, then, the life cycle of people adopting Kubernetes and containerization at large, through your own initial journey and then through helping customers. How would you characterize how that journey has changed from the early days to perhaps today?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I think the early days, there was a lot of questions of, why do I have to containerize? Why can't I just stay with virtual machines?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's a line item on your CV.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] It is. And nowadays, I think people know the value of using containers, of orchestrating containers with Kubernetes. I don't want to say &amp;quot;jumping on the bandwagon&amp;quot;, but it's become the de-facto standard to orchestrate containers.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's not something that a consultancy needs to go out and pitch to customers that they should be doing. They're just taking it as, that will happen, and starting a bit further down the path, perhaps.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Absolutely.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Working at a consultancy like that, how much time do you get to work on improving process, perhaps for multiple customers, and then looking at how you can upstream that work, versus paid work that you do for just an individual customer at a time?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Back then, it would vary. They helped me introduce myself, and I learned a lot about the cloud native landscape and Kubernetes itself. They helped educate me as to how the cloud native landscape, and the tools around it, can be used together. My boss at that company, Randy, he actually encouraged us to start contributing upstream, and encouraged me to join the release team. He just said, this is a great opportunity. Definitely helped me with starting with the contributions early on.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Was the release team the way that you got involved with upstream Kubernetes contribution?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Actually, no. My first contribution was with SIG Docs. I met Taylor Dolezal — he was the release team lead for 1.19, but he is involved with SIG Docs as well. I met him at KubeCon 2019, I sat at his table during a luncheon. I remember Paris Pittman was hosting this luncheon at the Marriott. Taylor says he was involved with SIG Docs. He encouraged me to join. I started joining into meetings, started doing a few drive-by PRs. That's what we call them — drive-by — little typo fixes. Then did a little bit more, started to send better or higher quality pull requests, and also reviewing PRs.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: When did you first formally take your release team role?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: That was in &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">1.18&lt;/a>, in December. My boss at the time encouraged me to apply. I did, was lucky enough to get accepted for the release notes shadow. Then from there, stayed in with release notes for a few cycles, then went into Docs, naturally then led Docs, then went to Enhancements, and now I'm the release lead for 1.23.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I don't know that a lot of people think about what goes into a good release note. What would you say does?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] You have to tell the end user what has changed or what effect that they might see in the release notes. It doesn't have to be highly technical. It could just be a few lines, and just saying what has changed, what they have to do if they have to do anything as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: As you moved through the process of shadowing, how did you learn from the people who were leading those roles?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I said this a few times when I was the release lead for this cycle. You get out of the release team as much as you put in, or it directly aligns to how much you put in. I learned a lot. I went into the release team having that mindset of learning from the role leads, learning from the other shadows, as well. That's actually a saying that my first role lead told me. I still carry it to heart, and that was back in 1.18. That was Eddie, in the very first meeting we had, and I still carry it to heart.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You, of course, were &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.23">the release lead for 1.23&lt;/a>. First of all, congratulations on the release.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Thank you very much.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The theme for this release is &lt;a href="https://kubernetes.io/blog/2021/12/07/kubernetes-1-23-release-announcement/">The Next Frontier&lt;/a>. Tell me the story of how we came to the theme and then the logo.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: The Next Frontier represents a few things. It not only represents the next enhancements in this release, but Kubernetes itself also has a history of Star Trek references. The original codename for Kubernetes was Project Seven, a reference to Seven of Nine, originally from Star Trek Voyager. Also the seven spokes in the helm in the logo of Kubernetes as well. And, of course, Borg, the predecessor to Kubernetes.&lt;/p>
&lt;p>The Next Frontier continues that Star Trek reference. It's a fusion of two titles in the Star Trek universe. One is &lt;a href="https://en.wikipedia.org/wiki/Star_Trek_V:_The_Final_Frontier">Star Trek V, the Final Frontier&lt;/a>, and the Star Trek: The Next Generation.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you have any opinion on the fact that Star Trek V was an odd-numbered movie, and they are &lt;a href="https://screenrant.com/star-trek-movies-odd-number-curse-explained/">canonically referred to as being lesser than the even-numbered ones&lt;/a>?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I can't say, because I am such a sci-fi nerd that I love all of them even though they're bad. Even the post-Next Generation movies, after the series, I still liked all of them, even though I know some weren't that great.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Am I right in remembering that Star Trek V was the one directed by William Shatner?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes, that is correct.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think that says it all.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [CHUCKLES] Yes.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, I understand that the theme comes from a part of the &lt;a href="https://github.com/kubernetes/community/blob/master/sig-release/charter.md">SIG Release charter&lt;/a>?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes. There's a line in the SIG Release charter, &amp;quot;ensure there is a consistent group of community members in place to support the release process across time.&amp;quot; With the release team, we have new shadows that join every single release cycle. With this, we're growing with this community. We're growing the release team members. We're growing SIG Release. We're growing the Kubernetes community itself. For a lot of people, this is their first time contributing to open source, so that's why I say it's their new open source frontier.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And the logo is obviously very Star Trek-inspired. It sort of surprised me that it took that long for someone to go this route.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I was very surprised as well. I had to relearn Adobe Illustrator to create the logo.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This your own work, is it?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: This is my own work.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's very nice.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Thank you very much. Funny, the galaxy actually took me the longest time versus the ship. Took me a few days to get that correct. I'm always fine-tuning it, so there might be a final change when this is actually released.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: No frontier is ever truly final.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: True, very true.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Moving now from the theme of the release to the substance, perhaps, what is new in 1.23?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: We have 47 enhancements. I'm going to run through most of the stable ones, if not all of them, some of the key Beta ones, and a few of the Alpha enhancements for 1.23.&lt;/p>
&lt;p>One of the key enhancements is &lt;a href="https://github.com/kubernetes/enhancements/issues/563">dual-stack IPv4/IPv6&lt;/a>, which went GA in 1.23.&lt;/p>
&lt;p>Some background info: dual-stack was introduced as Alpha in 1.15. You probably saw a keynote at KubeCon 2019. Back then, the way dual-stack worked was that you needed two services — you needed a service per IP family. You would need a service for IPv4 and a service for IPv6. It was refactored in 1.20. In 1.21, it was in Beta; clusters were enabled to be dual-stack by default.&lt;/p>
&lt;p>And then in 1.23 we did remove the IPv6 dual-stack feature flag. It's not mandatory to use dual-stack. It's actually not &amp;quot;default&amp;quot; still. The pods, the services still default to single-stack. There are some requirements to be able to use dual-stack. The nodes have to be routable on IPv4 and IPv6 network interfaces. You need a CNI plugin that supports dual-stack. The pods themselves have to be configured to be dual-stack. And the services need the ipFamilyPolicy field to specify prefer dual-stack, or require dual-stack.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: This sounds like there's an implication in this that v4 is still required. Do you see a world where we can actually move to v6-only clusters?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I think we'll be talking about IPv4 and IPv6 for many, many years to come. I remember a long time ago, they kept saying &amp;quot;it's going to be all IPv6&amp;quot;, and that was decades ago.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I think I may have mentioned on the show before, but there was &lt;a href="https://www.youtube.com/watch?v=AEaJtZVimqs">a meeting in London that Vint Cerf attended&lt;/a>, and he gave a public presentation at the time to say, now is the time of v6. And that was 10 years ago at least. It's still not the time of v6, and my desktop still doesn't have Linux on it. One day.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [LAUGHS] In my opinion, that's one of the big key features that went stable for 1.23.&lt;/p>
&lt;p>One of the other highlights of 1.23 is &lt;a href="https://kubernetes.io/blog/2021/12/09/pod-security-admission-beta/">pod security admission going to Beta&lt;/a>. I know this feature is going to Beta, but I highlight this because as some people might know, PodSecurityPolicy, which was deprecated in 1.21, is targeted to be removed in 1.25. Pod security admission replaces pod security policy. It's an admission controller. It evaluates the pods against a predefined set of pod security standards to either admit or deny the pod for running.&lt;/p>
&lt;p>There's three levels of pod security standards. Privileged, that's totally open. Baseline, known privileges escalations are minimized. Or Restricted, which is hardened. And you could set pod security standards either to run in three modes, which is enforce: reject any pods that are in violation; to audit: pods are allowed to be created, but the violations are recorded; or warn: it will send a warning message to the user, and the pod is allowed.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned there that PodSecurityPolicy is due to be deprecated in two releases' time. Are we lining up these features so that pod security admission will be GA at that time?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes. Absolutely. I'll talk about that for another feature in a little bit as well. There's also another feature that went to GA. It was an API that went to GA, and therefore the Beta API is now deprecated. I'll talk about that a little bit.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: All right. Let's talk about what's next on the list.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Let's move on to more stable enhancements. One is the &lt;a href="https://github.com/kubernetes/enhancements/issues/592">TTL controller&lt;/a>. This cleans up jobs and pods after the jobs are finished. There is a TTL timer that starts when the job or pod is finished. This TTL controller watches all the jobs, and ttlSecondsAfterFinished needs to be set. The controller will see if the ttlSecondsAfterFinished, combined with the last transition time, if it's greater than now. If it is, then it will delete the job and the pods of that job.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Loosely, it could be called a garbage collector?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes. Garbage collector for pods and jobs, or jobs and pods.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: If Kubernetes is truly becoming a programming language, it of course has to have a garbage collector implemented.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. There's another one, too, coming in Alpha. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Tell me about that.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: That one is coming in in Alpha. It's actually one of my favorite features, because there's only a few that I'm going to highlight today. &lt;a href="https://github.com/kubernetes/enhancements/issues/1847">PVCs for StafeulSet will be cleaned up&lt;/a>. It will auto-delete PVCs created by StatefulSets, when you delete that StatefulSet.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What's next on our tour of stable features?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Next one is, &lt;a href="https://github.com/kubernetes/enhancements/issues/695">skip volume ownership change goes to stable&lt;/a>. This is from SIG Storage. There are times when you're running a stateful application, like many databases, they're sensitive to permission bits changing underneath. Currently, when a volume is bind mounted inside the container, the permissions of that volume will change recursively. It might take a really long time.&lt;/p>
&lt;p>Now, there's a field, the fsGroupChangePolicy, which allows you, as a user, to tell Kubernetes how you want the permission and ownership change for that volume to happen. You can set it to always, to always change permissions, or just on mismatch, to only do it when the permission ownership changes at the top level is different from what is expected.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It does feel like a lot of these enhancements came from a very particular use case where someone said, &amp;quot;hey, this didn't work for me and I've plumbed in a feature that works with exactly the thing I need to have&amp;quot;.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Absolutely. People create issues for these, then create Kubernetes enhancement proposals, and then get targeted for releases.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Another GA feature in this release — ephemeral volumes.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: We've always been able to use empty dir for ephemeral volumes, but now we could actually have &lt;a href="https://github.com/kubernetes/enhancements/issues/1698">ephemeral inline volumes&lt;/a>, meaning that you could take your standard CSI driver and be able to use ephemeral volumes with it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: And, a long time coming, &lt;a href="https://github.com/kubernetes/enhancements/issues/19">CronJobs&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: CronJobs is a funny one, because it was stable before 1.23. For 1.23, it was still tracked,but it was just cleaning up some of the old controller. With CronJobs, there's a v2 controller. What was cleaned up in 1.23 is just the old v1 controller.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Were there any other duplications or major cleanups of note in this release?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. There were a few you might see in the major themes. One's a little tricky, around FlexVolumes. This is one of the efforts from SIG Storage. They have an effort to migrate in-tree plugins to CSI drivers. This is a little tricky, because FlexVolumes were actually deprecated in November 2020. We're &lt;a href="https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors">formally announcing it in 1.23&lt;/a>.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: FlexVolumes, in my mind, predate CSI as a concept. So it's about time to get rid of them.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yes, it is. There's another deprecation, just some &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#klog">klog specific flags&lt;/a>, but other than that, there are no other big deprecations in 1.23.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The buzzword of the last KubeCon, and in some ways the theme of the last 12 months, has been secure software supply chain. What work is Kubernetes doing to improve in this area?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: For 1.23, Kubernetes is now SLSA compliant at Level 1, which means that provenance attestation files that describe the staging and release phases of the release process are satisfactory for the SLSA framework.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What needs to happen to step up to further levels?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Level 1 means a few things — that the build is scripted; that the provenance is available, meaning that the artifacts are verified and they're handed over from one phase to the next; and describes how the artifact is produced. Level 2 means that the source is version-controlled, which it is, provenance is authenticated, provenance is service-generated, and there is a build service. There are four levels of SLSA compliance.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It does seem like the levels were largely influenced by what it takes to build a big, secure project like this. It doesn't seem like it will take a lot of extra work to move up to verifiable provenance, for example. There's probably just a few lines of script required to meet many of those requirements.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Absolutely. I feel like we're almost there; we'll see what will come out of 1.24. And I do want to give a big shout-out to SIG Release and Release Engineering, primarily to Adolfo García Veytia, who is aka Puerco on GitHub and on Slack. He's been driving this forward.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You've mentioned some APIs that are being graduated in time to replace their deprecated version. Tell me about the new HPA API.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: The &lt;a href="https://github.com/kubernetes/enhancements/issues/2702">horizontal pod autoscaler v2 API&lt;/a>, is now stable, which means that the v2beta2 API is deprecated. Just for everyone's knowledge, the v1 API is not being deprecated. The difference is that v2 adds support for multiple and custom metrics to be used for HPA.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's also now a facility to validate my CRDs with an expression language.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. You can use the &lt;a href="https://github.com/google/cel-spec">Common Expression Language, or CEL&lt;/a>, to validate your CRDs, so you no longer need to use webhooks. This also makes the CRDs more self-contained and declarative, because the rules are now kept within the CRD object definition.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What new features, perhaps coming in Alpha or Beta, have taken your interest?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Aside from pod security policies, I really love &lt;a href="https://github.com/kubernetes/enhancements/issues/277">ephemeral containers&lt;/a> supporting kubectl debug. It launches an ephemeral container and a running pod, shares those pod namespaces, and you can do all your troubleshooting with just running kubectl debug.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's also been some interesting changes in the way that events are handled with kubectl.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. kubectl events has always had some issues, like how things weren't sorted. &lt;a href="https://github.com/kubernetes/enhancements/issues/1440">kubectl events improved&lt;/a> that so now you can do &lt;code>--watch&lt;/code>, and it will also sort with the &lt;code>--watch&lt;/code> option as well. That is something new. You can actually combine fields and custom columns. And also, you can list events in the timeline with doing the last N number of minutes. And you can also sort events using other criteria as well.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You are a field engineer at SUSE. Are there any things that are coming in that your individual customers that you deal with are looking out for?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: More of what I look out for to help the customers.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I really love kubectl events. Really love the PVCs being cleaned up with StatefulSets. Most of it's for selfish reasons that it will improve troubleshooting efforts. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I have always hoped that a release team lead would say to me, &amp;quot;yes, I have selfish reasons. And I finally got something I wanted in.&amp;quot;&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: [LAUGHS]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Perhaps I should run to be release team lead, just so I can finally get init containers fixed once and for all.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Oh, init containers, I've been looking for that for a while. I've actually created animated GIFs on how init containers will be run with that Kubernetes enhancement proposal, but it's halted currently.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: One day.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: One day. Maybe I shouldn't stay halted.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You mentioned there are obviously the things you look out for. Are there any things that are coming down the line, perhaps Alpha features or maybe even just proposals you've seen lately, that you're personally really looking forward to seeing which way they go?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. Oone is a very interesting one, it affects the whole community, so it's not just for personal reasons. As you may have known, Dockershim is deprecated. And we did release a blog that it will be removed in 1.24.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Scared a bunch of people.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Scared a bunch of people. From a survey, we saw that a lot of people are still using Docker and Dockershim. One of the enhancements for 1.23 is, &lt;a href="https://github.com/kubernetes/enhancements/issues/2040">kubelet CRI goes to Beta&lt;/a>. This promotes the CRI API, which is required. This had to be in Beta for Dockershim to be removed in 1.24.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Now, in the last release team lead interview, &lt;a href="https://kubernetespodcast.com/episode/157-kubernetes-1.22/">we spoke with Savitha Raghunathan&lt;/a>, and she talked about what she would advise you as her successor. It was to look out for the mental health of the team members. How were you able to take that advice on board?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: That was great advice from Savitha. A few things I've made note of with each release team meeting. After each release team meeting, I stop the recording, because we do record all the meetings and post them on YouTube. And I open up the floor to anyone who wants to say anything that's not recorded, that's not going to be on the agenda. Also, I tell people not to work on weekends. I broke this rule once, but other than that, I told people it could wait. Just be mindful of your mental health.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: It's just been announced that &lt;a href="https://twitter.com/JamesLaverack/status/1466834312993644551">James Laverack from Jetstack&lt;/a> will be the release team lead for 1.24. James and I shared an interesting Mexican dinner at the last KubeCon in San Diego.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Oh, nice. I didn't know you knew James.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The British tech scene. We're a very small world. What will your advice to James be?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: What I would tell James for 1.24 is use teachable moments in the release team meetings. When you're a shadow for the first time, it's very daunting. It's very difficult, because you don't know the repos. You don't know the release process. Everyone around you seems like they know the release process, and very familiar with what the release process is. But as a first-time shadow, you don't know all the vernacular for the community. I just advise to use teachable moments. Take a few minutes in the release team meetings to make it a little easier for new shadows to ramp up and to be familiar with the release process.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Has there been major evolution in the process in the time that you've been involved? Or do you think that it's effectively doing what it needs to do?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: It's always evolving. I remember my first time in release notes, 1.18, we said that our goal was to automate and program our way out so that we don't have a release notes team anymore. That's changed [CHUCKLES] quite a bit. Although there's been significant advancements in the release notes process by Adolfo and also James, they've created a subcommand in krel to generate release notes.&lt;/p>
&lt;p>But nowadays, all their release notes are richer. Still not there at the automation process yet. Every release cycle, there is something a little bit different. For this release cycle, we had a production readiness review deadline. It was a soft deadline. A production readiness review is a review by several people in the community. It's actually been required since 1.21, and it ensures that the enhancements are observable, scalable, supportable, and it's safe to operate in production, and could also be disabled or rolled back. In 1.23, we had a deadline to have the production readiness review completed by a specific date.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: How have you found the change of schedule to three releases per year rather than four?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Moving to three releases a year from four, in my opinion, has been an improvement, because we support the last three releases, and now we can actually support the last releases in a calendar year instead of having 9 months out of 12 months of the year.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The next event on the calendar is a &lt;a href="https://www.kubernetes.dev/events/kcc2021/">Kubernetes contributor celebration&lt;/a> starting next Monday. What can we expect from that event?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: This is our second time running this virtual event. It's a virtual celebration to recognize the whole community and all of our accomplishments of the year, and also contributors. There's a number of events during this week of celebration. It starts the week of December 13.&lt;/p>
&lt;p>There's events like the Kubernetes Contributor Awards, where SIGs honor and recognize the hard work of the community and contributors. There's also a DevOps party game as well. There is a cloud native bake-off. I do highly suggest people to go to &lt;a href="https://www.kubernetes.dev/events/past-events/2021/kcc2021/">kubernetes.dev/celebration&lt;/a> to learn more.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: How exactly does one judge a virtual bake-off?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: That I don't know. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I tasted my scones. I think they're the best. I rate them 10 out of 10.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: Yeah. That is very difficult to do virtually. I would have to say, probably what the dish is, how closely it is tied with Kubernetes or open source or to CNCF. There's a few judges. I know Josh Berkus and Rin Oliver are a few of the judges running the bake-off.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Yes. We spoke with Josh about his love of the kitchen, and so he seems like a perfect fit for that role.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: He is.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Finally, your wife and yourself are expecting your first child in January. Have you had a production readiness review for that?&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: I think we failed that review. [CHUCKLES]&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: There's still time.&lt;/strong>&lt;/p>
&lt;p>REY LEJANO: We are working on refactoring. We're going to refactor a little bit in December, and &lt;code>--apply&lt;/code> again.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/reylejano">Rey Lejano&lt;/a> is a field engineer at SUSE, by way of Rancher Labs, and was the release team lead for Kubernetes 1.23. He is now also a co-chair for SIG Docs. His son Liam is now 3 and a half months old.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Increasing the security bar in Ingress-NGINX v1.2.0</title><link>https://kubernetes.io/blog/2022/04/28/ingress-nginx-1-2-0/</link><pubDate>Thu, 28 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/04/28/ingress-nginx-1-2-0/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Ricardo Katz (VMware), James Strong (Chainguard)&lt;/p>
&lt;p>The &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a> may be one of the most targeted components
of Kubernetes. An Ingress typically defines an HTTP reverse proxy, exposed to the Internet, containing
multiple websites, and with some privileged access to Kubernetes API (such as to read Secrets relating to
TLS certificates and their private keys).&lt;/p>
&lt;p>While it is a risky component in your architecture, it is still the most popular way to properly expose your services.&lt;/p>
&lt;p>Ingress-NGINX has been part of security assessments that figured out we have a big problem: we don't
do all proper sanitization before turning the configuration into an &lt;code>nginx.conf&lt;/code> file, which may lead to information
disclosure risks.&lt;/p>
&lt;p>While we understand this risk and the real need to fix this, it's not an easy process to do, so we took another approach to reduce (but not remove!) this risk in the current (v1.2.0) release.&lt;/p>
&lt;h2 id="meet-ingress-nginx-v1-2-0-and-the-chrooted-nginx-process">Meet Ingress NGINX v1.2.0 and the chrooted NGINX process&lt;/h2>
&lt;p>One of the main challenges is that Ingress-NGINX runs the web proxy server (NGINX) alongside the Ingress
controller (the component that has access to Kubernetes API that and that creates the &lt;code>nginx.conf&lt;/code> file).&lt;/p>
&lt;p>So, NGINX does have the same access to the filesystem of the controller (and Kubernetes service account token, and other configurations from the container). While splitting those components is our end goal, the project needed a fast response; that lead us to the idea of using &lt;code>chroot()&lt;/code>.&lt;/p>
&lt;p>Let's take a look into what an Ingress-NGINX container looked like before this change:&lt;/p>
&lt;p>&lt;img src="ingress-pre-chroot.png" alt="Ingress NGINX pre chroot">&lt;/p>
&lt;p>As we can see, the same container (not the Pod, the container!) that provides HTTP Proxy is the one that watches Ingress objects and writes the Container Volume&lt;/p>
&lt;p>Now, meet the new architecture:&lt;/p>
&lt;p>&lt;img src="ingress-post-chroot.png" alt="Ingress NGINX post chroot">&lt;/p>
&lt;p>What does all of this mean? A basic summary is: that we are isolating the NGINX service as a container inside the
controller container.&lt;/p>
&lt;p>While this is not strictly true, to understand what was done here, it's good to understand how
Linux containers (and underlying mechanisms such as kernel namespaces) work.
You can read about cgroups in the Kubernetes glossary: &lt;a href="https://kubernetes.io/docs/reference/glossary/?fundamental=true#term-cgroup">&lt;code>cgroup&lt;/code>&lt;/a> and learn more about cgroups interact with namespaces in the NGINX project article
&lt;a href="https://www.nginx.com/blog/what-are-namespaces-cgroups-how-do-they-work/">What Are Namespaces and cgroups, and How Do They Work?&lt;/a>.
(As you read that, bear in mind that Linux kernel namespaces are a different thing from
&lt;a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">Kubernetes namespaces&lt;/a>).&lt;/p>
&lt;h2 id="skip-the-talk-what-do-i-need-to-use-this-new-approach">Skip the talk, what do I need to use this new approach?&lt;/h2>
&lt;p>While this increases the security, we made this feature an opt-in in this release so you can have
time to make the right adjustments in your environment(s). This new feature is only available from
release v1.2.0 of the Ingress-NGINX controller.&lt;/p>
&lt;p>There are two required changes in your deployments to use this feature:&lt;/p>
&lt;ul>
&lt;li>Append the suffix &amp;quot;-chroot&amp;quot; to the container image name. For example: &lt;code>gcr.io/k8s-staging-ingress-nginx/controller-chroot:v1.2.0&lt;/code>&lt;/li>
&lt;li>In your Pod template for the Ingress controller, find where you add the capability &lt;code>NET_BIND_SERVICE&lt;/code> and add the capability &lt;code>SYS_CHROOT&lt;/code>. After you edit the manifest, you'll see a snippet like:&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">capabilities&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">drop&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- ALL&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">add&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- NET_BIND_SERVICE&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>- SYS_CHROOT&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you deploy the controller using the official Helm chart then change the following setting in
&lt;code>values.yaml&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#008000;font-weight:bold">controller&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#008000;font-weight:bold">chroot&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Ingress controllers are normally set up cluster-wide (the IngressClass API is cluster scoped). If you manage the
Ingress-NGINX controller but you're not the overall cluster operator, then check with your cluster admin about
whether you can use the &lt;code>SYS_CHROOT&lt;/code> capability, &lt;strong>before&lt;/strong> you enable it in your deployment.&lt;/p>
&lt;h2 id="ok-but-how-does-this-increase-the-security-of-my-ingress-controller">OK, but how does this increase the security of my Ingress controller?&lt;/h2>
&lt;p>Take the following configuration snippet and imagine, for some reason it was added to your &lt;code>nginx.conf&lt;/code>:&lt;/p>
&lt;pre tabindex="0">&lt;code>location /randomthing/ {
alias /;
autoindex on;
}
&lt;/code>&lt;/pre>&lt;p>If you deploy this configuration, someone can call &lt;code>http://website.example/randomthing&lt;/code> and get some listing (and access) to the whole filesystem of the Ingress controller.&lt;/p>
&lt;p>Now, can you spot the difference between chrooted and non chrooted Nginx on the listings below?&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Without extra &lt;code>chroot()&lt;/code>&lt;/th>
&lt;th>With extra &lt;code>chroot()&lt;/code>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>bin&lt;/code>&lt;/td>
&lt;td>&lt;code>bin&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>dev&lt;/code>&lt;/td>
&lt;td>&lt;code>dev&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>etc&lt;/code>&lt;/td>
&lt;td>&lt;code>etc&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>home&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>lib&lt;/code>&lt;/td>
&lt;td>&lt;code>lib&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>media&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>mnt&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>opt&lt;/code>&lt;/td>
&lt;td>&lt;code>opt&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>proc&lt;/code>&lt;/td>
&lt;td>&lt;code>proc&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>root&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>run&lt;/code>&lt;/td>
&lt;td>&lt;code>run&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>sbin&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>srv&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>sys&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>tmp&lt;/code>&lt;/td>
&lt;td>&lt;code>tmp&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>usr&lt;/code>&lt;/td>
&lt;td>&lt;code>usr&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>var&lt;/code>&lt;/td>
&lt;td>&lt;code>var&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>dbg&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>nginx-ingress-controller&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>wait-shutdown&lt;/code>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>The one in left side is not chrooted. So NGINX has full access to the filesystem. The one in right side is chrooted, so a new filesystem with only the required files to make NGINX work is created.&lt;/p>
&lt;h2 id="what-about-other-security-improvements-in-this-release">What about other security improvements in this release?&lt;/h2>
&lt;p>We know that the new &lt;code>chroot()&lt;/code> mechanism helps address some portion of the risk, but still, someone
can try to inject commands to read, for example, the &lt;code>nginx.conf&lt;/code> file and extract sensitive information.&lt;/p>
&lt;p>So, another change in this release (this is opt-out!) is the &lt;em>deep inspector&lt;/em>.
We know that some directives or regular expressions may be dangerous to NGINX, so the deep inspector
checks all fields from an Ingress object (during its reconciliation, and also with a
&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook">validating admission webhook&lt;/a>)
to verify if any fields contains these dangerous directives.&lt;/p>
&lt;p>The ingress controller already does this for annotations, and our goal is to move this existing validation to happen inside
deep inspection as part of a future release.&lt;/p>
&lt;p>You can take a look into the existing rules in &lt;a href="https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go">https://github.com/kubernetes/ingress-nginx/blob/main/internal/ingress/inspector/rules.go&lt;/a>.&lt;/p>
&lt;p>Due to the nature of inspecting and matching all strings within relevant Ingress objects, this new feature may consume a bit more CPU. You can disable it by running the ingress controller with the command line argument &lt;code>--deep-inspect=false&lt;/code>.&lt;/p>
&lt;h2 id="what-s-next">What's next?&lt;/h2>
&lt;p>This is not our final goal. Our final goal is to split the control plane and the data plane processes.
In fact, doing so will help us also achieve a &lt;a href="https://gateway-api.sigs.k8s.io/">Gateway&lt;/a> API implementation,
as we may have a different controller as soon as it &amp;quot;knows&amp;quot; what to provide to the data plane
(we need some help here!!)&lt;/p>
&lt;p>Some other projects in Kubernetes already take this approach
(like &lt;a href="https://github.com/kubernetes-sigs/kpng">KPNG&lt;/a>, the proposed replacement for &lt;code>kube-proxy&lt;/code>),
and we plan to align with them and get the same experience for Ingress-NGINX.&lt;/p>
&lt;h2 id="further-reading">Further reading&lt;/h2>
&lt;p>If you want to take a look into how chrooting was done in Ingress NGINX, take a look
into &lt;a href="https://github.com/kubernetes/ingress-nginx/pull/8337">https://github.com/kubernetes/ingress-nginx/pull/8337&lt;/a>
The release v1.2.0 containing all the changes can be found at
&lt;a href="https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0">https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.2.0&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes Removals and Deprecations In 1.24</title><link>https://kubernetes.io/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/04/07/upcoming-changes-in-kubernetes-1-24/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Mickey Boxell (Oracle)&lt;/p>
&lt;p>As Kubernetes evolves, features and APIs are regularly revisited and removed. New features may offer
an alternative or improved approach to solving existing problems, motivating the team to remove the
old approach.&lt;/p>
&lt;p>We want to make sure you are aware of the changes coming in the Kubernetes 1.24 release. The release will
&lt;strong>deprecate&lt;/strong> several (beta) APIs in favor of stable versions of the same APIs. The major change coming
in the Kubernetes 1.24 release is the
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">removal of Dockershim&lt;/a>.
This is discussed below and will be explored in more depth at release time. For an early look at the
changes coming in Kubernetes 1.24, take a look at the in-progress
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md">CHANGELOG&lt;/a>.&lt;/p>
&lt;h2 id="a-note-about-dockershim">A note about Dockershim&lt;/h2>
&lt;p>It's safe to say that the removal receiving the most attention with the release of Kubernetes 1.24
is Dockershim. Dockershim was deprecated in v1.20. As noted in the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">Kubernetes 1.20 changelog&lt;/a>:
&amp;quot;Docker support in the kubelet is now deprecated and will be removed in a future release. The kubelet
uses a module called &amp;quot;dockershim&amp;quot; which implements CRI support for Docker and it has seen maintenance
issues in the Kubernetes community.&amp;quot; With the upcoming release of Kubernetes 1.24, the Dockershim will
finally be removed.&lt;/p>
&lt;p>In the article &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Don't Panic: Kubernetes and Docker&lt;/a>,
the authors succinctly captured the change's impact and encouraged users to remain calm:&lt;/p>
&lt;blockquote>
&lt;p>Docker as an underlying runtime is being deprecated in favor of runtimes that use the
Container Runtime Interface (CRI) created for Kubernetes. Docker-produced images
will continue to work in your cluster with all runtimes, as they always have.&lt;/p>
&lt;/blockquote>
&lt;p>Several guides have been created with helpful information about migrating from dockershim
to container runtimes that are directly compatible with Kubernetes. You can find them on the
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim&lt;/a>
page in the Kubernetes documentation.&lt;/p>
&lt;p>For more information about why Kubernetes is moving away from dockershim, check out the aptly
named: &lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">Kubernetes is Moving on From Dockershim&lt;/a>
and the &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">updated dockershim removal FAQ&lt;/a>.&lt;/p>
&lt;p>Take a look at the &lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">Is Your Cluster Ready for v1.24?&lt;/a> post to learn about how to ensure your cluster continues to work after upgrading from v1.23 to v1.24.&lt;/p>
&lt;h2 id="the-kubernetes-api-removal-and-deprecation-process">The Kubernetes API removal and deprecation process&lt;/h2>
&lt;p>Kubernetes contains a large number of components that evolve over time. In some cases, this
evolution results in APIs, flags, or entire features, being removed. To prevent users from facing
breaking changes, Kubernetes contributors adopted a feature &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">deprecation policy&lt;/a>.
This policy ensures that stable APIs may only be deprecated when a newer stable version of that
same API is available and that APIs have a minimum lifetime as indicated by the following stability levels:&lt;/p>
&lt;ul>
&lt;li>Generally available (GA) or stable API versions may be marked as deprecated but must not be removed within a major version of Kubernetes.&lt;/li>
&lt;li>Beta or pre-release API versions must be supported for 3 releases after deprecation.&lt;/li>
&lt;li>Alpha or experimental API versions may be removed in any release without prior deprecation notice.&lt;/li>
&lt;/ul>
&lt;p>Removals follow the same deprecation policy regardless of whether an API is removed due to a beta feature
graduating to stable or because that API was not proven to be successful. Kubernetes will continue to make
sure migration options are documented whenever APIs are removed.&lt;/p>
&lt;p>&lt;strong>Deprecated&lt;/strong> APIs are those that have been marked for removal in a future Kubernetes release. &lt;strong>Removed&lt;/strong>
APIs are those that are no longer available for use in current, supported Kubernetes versions after having
been deprecated. These removals have been superseded by newer, stable/generally available (GA) APIs.&lt;/p>
&lt;h2 id="api-removals-deprecations-and-other-changes-for-kubernetes-1-24">API removals, deprecations, and other changes for Kubernetes 1.24&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/281">Dynamic kubelet configuration&lt;/a>: &lt;code>DynamicKubeletConfig&lt;/code> is used to enable the dynamic configuration of the kubelet. The &lt;code>DynamicKubeletConfig&lt;/code> flag was deprecated in Kubernetes 1.22. In v1.24, this feature gate will be removed from the kubelet. See &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">Reconfigure kubelet&lt;/a>. Refer to the &lt;a href="https://github.com/kubernetes/enhancements/issues/281">&amp;quot;Dynamic kubelet config is removed&amp;quot; KEP&lt;/a> for more information.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107207">Dynamic log sanitization&lt;/a>: The experimental dynamic log sanitization feature is deprecated and will be removed in v1.24. This feature introduced a logging filter that could be applied to all Kubernetes system components logs to prevent various types of sensitive information from leaking via logs. Refer to &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#deprecation">KEP-1753: Kubernetes system components logs sanitization&lt;/a> for more information and an &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1753-logs-sanitization#alternatives=">alternative approach&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/2221">Removing Dockershim from kubelet&lt;/a>: the Container Runtime Interface (CRI) for Docker (i.e. Dockershim) is currently a built-in container runtime in the kubelet code base. It was deprecated in v1.20. As of v1.24, the kubelet will no longer have dockershim. Check out this blog on &lt;a href="https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/">what you need to do be ready for v1.24&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1472">Storage capacity tracking for pod scheduling&lt;/a>: The CSIStorageCapacity API supports exposing currently available storage capacity via CSIStorageCapacity objects and enhances scheduling of pods that use CSI volumes with late binding. In v1.24, the CSIStorageCapacity API will be stable. The API graduating to stable initates the deprecation of the v1beta1 CSIStorageCapacity API. Refer to the &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1472-storage-capacity-tracking">Storage Capacity Constraints for Pod Scheduling KEP&lt;/a> for more information.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/107533">The &lt;code>master&lt;/code> label is no longer present on kubeadm control plane nodes&lt;/a>. For new clusters, the label 'node-role.kubernetes.io/master' will no longer be added to control plane nodes, only the label 'node-role.kubernetes.io/control-plane' will be added. For more information, refer to &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-cluster-lifecycle/kubeadm/2067-rename-master-label-taint">KEP-2067: Rename the kubeadm &amp;quot;master&amp;quot; label and taint&lt;/a>.&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/177">VolumeSnapshot v1beta1 CRD will be removed&lt;/a>. Volume snapshot and restore functionality for Kubernetes and the &lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">Container Storage Interface&lt;/a> (CSI), which provides standardized APIs design (CRDs) and adds PV snapshot/restore support for CSI volume drivers, moved to GA in v1.20. VolumeSnapshot v1beta1 was deprecated in v1.20 and will become unsupported with the v1.24 release. Refer to &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/177-volume-snapshot#kep-177-csi-snapshot">KEP-177: CSI Snapshot&lt;/a> and the &lt;a href="https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/">Volume Snapshot GA blog&lt;/a> blog article for more information.&lt;/li>
&lt;/ul>
&lt;h2 id="what-to-do">What to do&lt;/h2>
&lt;h3 id="dockershim-removal">Dockershim removal&lt;/h3>
&lt;p>As stated earlier, there are several guides about
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">Migrating from dockershim&lt;/a>.
You can start with &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">Finding what container runtime are on your nodes&lt;/a>.
If your nodes are using dockershim, there are other possible Docker Engine dependencies such as
Pods or third-party tools executing Docker commands or private registries in the Docker configuration file. You can follow the
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">Check whether Dockershim removal affects you&lt;/a> guide to review possible
Docker Engine dependencies. Before upgrading to v1.24, you decide to either remain using Docker Engine and
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/">Migrate Docker Engine nodes from dockershim to cri-dockerd&lt;/a> or migrate to a CRI-compatible runtime. Here's a guide to
&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">change the container runtime on a node from Docker Engine to containerd&lt;/a>.&lt;/p>
&lt;h3 id="kubectl-convert">&lt;code>kubectl convert&lt;/code>&lt;/h3>
&lt;p>The &lt;a href="https://kubernetes.io/docs/tasks/tools/included/kubectl-convert-overview/">&lt;code>kubectl convert&lt;/code>&lt;/a> plugin for &lt;code>kubectl&lt;/code>
can be helpful to address migrating off deprecated APIs. The plugin facilitates the conversion of
manifests between different API versions, for example, from a deprecated to a non-deprecated API
version. More general information about the API migration process can be found in the &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/">Deprecated API Migration Guide&lt;/a>.
Follow the &lt;a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-kubectl-convert-plugin">install &lt;code>kubectl convert&lt;/code> plugin&lt;/a>
documentation to download and install the &lt;code>kubectl-convert&lt;/code> binary.&lt;/p>
&lt;h3 id="looking-ahead">Looking ahead&lt;/h3>
&lt;p>The Kubernetes 1.25 and 1.26 releases planned for later this year will stop serving beta versions
of several currently stable Kubernetes APIs. The v1.25 release will also remove PodSecurityPolicy,
which was deprecated with Kubernetes 1.21 and will not graduate to stable. See &lt;a href="https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/">PodSecurityPolicy
Deprecation: Past, Present, and Future&lt;/a> for more information.&lt;/p>
&lt;p>The official &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25">list of API removals planned for Kubernetes 1.25&lt;/a> is:&lt;/p>
&lt;ul>
&lt;li>The beta CronJob API (batch/v1beta1)&lt;/li>
&lt;li>The beta EndpointSlice API (discovery.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta Event API (events.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta1)&lt;/li>
&lt;li>The beta PodDisruptionBudget API (policy/v1beta1)&lt;/li>
&lt;li>The beta PodSecurityPolicy API (policy/v1beta1)&lt;/li>
&lt;li>The beta RuntimeClass API (node.k8s.io/v1beta1)&lt;/li>
&lt;/ul>
&lt;p>The official &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-26">list of API removals planned for Kubernetes 1.26&lt;/a> is:&lt;/p>
&lt;ul>
&lt;li>The beta FlowSchema and PriorityLevelConfiguration APIs (flowcontrol.apiserver.k8s.io/v1beta1)&lt;/li>
&lt;li>The beta HorizontalPodAutoscaler API (autoscaling/v2beta2)&lt;/li>
&lt;/ul>
&lt;h3 id="want-to-know-more">Want to know more?&lt;/h3>
&lt;p>Deprecations are announced in the Kubernetes release notes. You can see the announcements of pending deprecations in the release notes for:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.21.md#deprecation">Kubernetes 1.21&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md#deprecation">Kubernetes 1.22&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.23.md#deprecation">Kubernetes 1.23&lt;/a>&lt;/li>
&lt;li>We will formally announce the deprecations that come with &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.24.md#deprecation">Kubernetes 1.24&lt;/a> as part of the CHANGELOG for that release.&lt;/li>
&lt;/ul>
&lt;p>For information on the process of deprecation and removal, check out the official Kubernetes &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/#deprecating-parts-of-the-api">deprecation policy&lt;/a> document.&lt;/p></description></item><item><title>Blog: Is Your Cluster Ready for v1.24?</title><link>https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/</link><pubDate>Thu, 31 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/03/31/ready-for-dockershim-removal/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kat Cosgrove&lt;/p>
&lt;p>Way back in December of 2020, Kubernetes announced the &lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">deprecation of Dockershim&lt;/a>. In Kubernetes, dockershim is a software shim that allows you to use the entire Docker engine as your container runtime within Kubernetes. In the upcoming v1.24 release, we are removing Dockershim - the delay between deprecation and removal in line with the &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">project’s policy&lt;/a> of supporting features for at least one year after deprecation. If you are a cluster operator, this guide includes the practical realities of what you need to know going into this release. Also, what do you need to do to ensure your cluster doesn’t fall over!&lt;/p>
&lt;h2 id="first-does-this-even-affect-you">First, does this even affect you?&lt;/h2>
&lt;p>If you are rolling your own cluster or are otherwise unsure whether or not this removal affects you, stay on the safe side and &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">check to see if you have any dependencies on Docker Engine&lt;/a>. Please note that using Docker Desktop to build your application containers is not a Docker dependency for your cluster. Container images created by Docker are compliant with the &lt;a href="https://opencontainers.org/">Open Container Initiative (OCI)&lt;/a>, a Linux Foundation governance structure that defines industry standards around container formats and runtimes. They will work just fine on any container runtime supported by Kubernetes.&lt;/p>
&lt;p>If you are using a managed Kubernetes service from a cloud provider, and you haven’t explicitly changed the container runtime, there may be nothing else for you to do. Amazon EKS, Azure AKS, and Google GKE all default to containerd now, though you should make sure they do not need updating if you have any node customizations. To check the runtime of your nodes, follow &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/find-out-runtime-you-use/">Find Out What Container Runtime is Used on a Node&lt;/a>.&lt;/p>
&lt;p>Regardless of whether you are rolling your own cluster or using a managed Kubernetes service from a cloud provider, you may need to &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrating-telemetry-and-security-agents/">migrate telemetry or security agents that rely on Docker Engine&lt;/a>.&lt;/p>
&lt;h2 id="i-have-a-docker-dependency-what-now">I have a Docker dependency. What now?&lt;/h2>
&lt;p>If your Kubernetes cluster depends on Docker Engine and you intend to upgrade to Kubernetes v1.24 (which you should eventually do for security and similar reasons), you will need to change your container runtime from Docker Engine to something else or use &lt;a href="https://github.com/Mirantis/cri-dockerd">cri-dockerd&lt;/a>. Since &lt;a href="https://containerd.io/">containerd&lt;/a> is a graduated CNCF project and the runtime within Docker itself, it’s a safe bet as an alternative container runtime. Fortunately, the Kubernetes project has already documented the process of &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/change-runtime-containerd/">changing a node’s container runtime&lt;/a>, using containerd as an example. Instructions are similar for switching to one of the other supported runtimes.&lt;/p>
&lt;h2 id="i-want-to-upgrade-kubernetes-and-i-need-to-maintain-compatibility-with-docker-as-a-runtime-what-are-my-options">I want to upgrade Kubernetes, and I need to maintain compatibility with Docker as a runtime. What are my options?&lt;/h2>
&lt;p>Fear not, you aren’t being left out in the cold and you don’t have to take the security risk of staying on an old version of Kubernetes. Mirantis and Docker have jointly released, and are maintaining, a replacement for dockershim. That replacement is called &lt;a href="https://github.com/Mirantis/cri-dockerd">cri-dockerd&lt;/a>. If you do need to maintain compatibility with Docker as a runtime, install cri-dockerd following the instructions in the project’s documentation.&lt;/p>
&lt;h2 id="is-that-it">Is that it?&lt;/h2>
&lt;p>Yes. As long as you go into this release aware of the changes being made and the details of your own clusters, and you make sure to communicate clearly with your development teams, it will be minimally dramatic. You may have some changes to make to your cluster, application code, or scripts, but all of these requirements are documented. Switching from using Docker Engine as your runtime to using &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">one of the other supported container runtimes&lt;/a> effectively means removing the middleman, since the purpose of dockershim is to access the container runtime used by Docker itself. From a practical perspective, this removal is better both for you and for Kubernetes maintainers in the long-run.&lt;/p>
&lt;p>If you still have questions, please first check the &lt;a href="https://kubernetes.io/blog/2022/02/17/dockershim-faq/">Dockershim Removal FAQ&lt;/a>.&lt;/p></description></item><item><title>Blog: Meet Our Contributors - APAC (Aus-NZ region)</title><link>https://kubernetes.io/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</link><pubDate>Wed, 16 Mar 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/03/16/meet-our-contributors-au-nz-ep-02/</guid><description>
&lt;p>&lt;strong>Authors &amp;amp; Interviewers:&lt;/strong> &lt;a href="https://github.com/anubha-v-ardhan">Anubhav Vardhan&lt;/a>, &lt;a href="https://github.com/Atharva-Shinde">Atharva Shinde&lt;/a>, &lt;a href="https://github.com/AvineshTripathi">Avinesh Tripathi&lt;/a>, &lt;a href="https://github.com/bradmccoydev">Brad McCoy&lt;/a>, &lt;a href="https://github.com/Debanitrkl">Debabrata Panigrahi&lt;/a>, &lt;a href="https://github.com/jayesh-srivastava">Jayesh Srivastava&lt;/a>, &lt;a href="https://github.com/verma-kunal">Kunal Verma&lt;/a>, &lt;a href="https://github.com/PranshuSrivastava">Pranshu Srivastava&lt;/a>, &lt;a href="github.com/Priyankasaggu11929/">Priyanka Saggu&lt;/a>, &lt;a href="https://github.com/PurneswarPrasad">Purneswar Prasad&lt;/a>, &lt;a href="https://github.com/vedant-kakde">Vedant Kakde&lt;/a>&lt;/p>
&lt;hr>
&lt;p>Good day, everyone 👋&lt;/p>
&lt;p>Welcome back to the second episode of the &amp;quot;Meet Our Contributors&amp;quot; blog post series for APAC.&lt;/p>
&lt;p>This post will feature four outstanding contributors from the Australia and New Zealand regions, who have played diverse leadership and community roles in the Upstream Kubernetes project.&lt;/p>
&lt;p>So, without further ado, let's get straight to the blog.&lt;/p>
&lt;h2 id="caleb-woodbine-https-github-com-bobymcbobs">&lt;a href="https://github.com/BobyMCbobs">Caleb Woodbine&lt;/a>&lt;/h2>
&lt;p>Caleb Woodbine is currently a member of the ii.nz organisation.&lt;/p>
&lt;p>He began contributing to the Kubernetes project in 2018 as a member of the Kubernetes Conformance working group. His experience was positive, and he benefited from early guidance from &lt;a href="https://github.com/hh">Hippie Hacker&lt;/a>, a fellow contributor from New Zealand.&lt;/p>
&lt;p>He has made major contributions to Kubernetes project since then through &lt;code>SIG k8s-infra&lt;/code> and &lt;code>k8s-conformance&lt;/code> working group.&lt;/p>
&lt;p>Caleb is also a co-organizer of the &lt;a href="https://www.meetup.com/cloudnative-nz/">CloudNative NZ&lt;/a> community events, which aim to expand the reach of Kubernetes project throughout New Zealand in order to encourage technical education and improved employment opportunities.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>There need to be more outreach in APAC and the educators and universities must pick up Kubernetes, as they are very slow and about 8+ years out of date. NZ tends to rather pay overseas than educate locals on the latest cloud tech Locally.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="dylan-graham-https-github-com-dylangraham">&lt;a href="https://github.com/DylanGraham">Dylan Graham&lt;/a>&lt;/h2>
&lt;p>Dylan Graham is a cloud engineer from Adeliade, Australia. He has been contributing to the upstream Kubernetes project since 2018.&lt;/p>
&lt;p>He stated that being a part of such a large-scale project was initially overwhelming, but that the community's friendliness and openness assisted him in getting through it.&lt;/p>
&lt;p>He began by contributing to the project documentation and is now mostly focused on the community support for the APAC region.&lt;/p>
&lt;p>He believes that consistent attendance at community/project meetings, taking on project tasks, and seeking community guidance as needed can help new aspiring developers become effective contributors.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>The feeling of being a part of a large community is really special. I've met some amazing people, even some before the pandemic in real life :)&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="hippie-hacker-https-github-com-hh">&lt;a href="https://github.com/hh">Hippie Hacker&lt;/a>&lt;/h2>
&lt;p>Hippie has worked for the CNCF.io as a Strategic Initiatives contractor from New Zealand for almost 5+ years. He is an active contributor to k8s-infra, API conformance testing, Cloud provider conformance submissions, and apisnoop.cncf.io domains of the upstream Kubernetes &amp;amp; CNCF projects.&lt;/p>
&lt;p>He recounts their early involvement with the Kubernetes project, which began roughly 5 years ago when their firm, ii.nz, demonstrated &lt;a href="https://ii.nz/post/bringing-the-cloud-to-your-community/">network booting from a Raspberry Pi using PXE and running Gitlab in-cluster to install Kubernetes on servers&lt;/a>.&lt;/p>
&lt;p>He describes their own contributing experience as someone who, at first, tried to do all of the hard lifting on their own, but eventually saw the benefit of group contributions which reduced burnout and task division which allowed folks to keep moving forward on their own momentum.&lt;/p>
&lt;p>He recommends that new contributors use pair programming.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>The cross pollination of approaches and two pairs of eyes on the same work can often yield a much more amplified effect than a PR comment / approval alone can afford.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;h2 id="nick-young-https-github-com-youngnick">&lt;a href="https://github.com/youngnick">Nick Young&lt;/a>&lt;/h2>
&lt;p>Nick Young works at VMware as a technical lead for Contour, a CNCF ingress controller. He was active with the upstream Kubernetes project from the beginning, and eventually became the chair of the LTS working group, where he advocated user concerns. He is currently the SIG Network Gateway API subproject's maintainer.&lt;/p>
&lt;p>His contribution path was notable in that he began working on major areas of the Kubernetes project early on, skewing his trajectory.&lt;/p>
&lt;p>He asserts the best thing a new contributor can do is to &amp;quot;start contributing&amp;quot;. Naturally, if it is relevant to their employment, that is excellent; however, investing non-work time in contributing can pay off in the long run in terms of work. He believes that new contributors, particularly those who are currently Kubernetes users, should be encouraged to participate in higher-level project discussions.&lt;/p>
&lt;blockquote>
&lt;p>&lt;em>Just being active and contributing will get you a long way. Once you've been active for a while, you'll find that you're able to answer questions, which will mean you're asked questions, and before you know it you are an expert.&lt;/em>&lt;/p>
&lt;/blockquote>
&lt;hr>
&lt;p>If you have any recommendations/suggestions for who we should interview next, please let us know in #sig-contribex. Your suggestions would be much appreciated. We're thrilled to have additional folks assisting us in reaching out to even more wonderful individuals of the community.&lt;/p>
&lt;p>We'll see you all in the next one. Everyone, till then, have a happy contributing! 👋&lt;/p></description></item><item><title>Blog: Updated: Dockershim Removal FAQ</title><link>https://kubernetes.io/blog/2022/02/17/dockershim-faq/</link><pubDate>Thu, 17 Feb 2022 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2022/02/17/dockershim-faq/</guid><description>
&lt;p>&lt;strong>This supersedes the original
&lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Dockershim Deprecation FAQ&lt;/a> article,
published in late 2020. The article includes updates from the v1.24
release of Kubernetes.&lt;/strong>&lt;/p>
&lt;hr>
&lt;p>This document goes over some frequently asked questions regarding the
removal of &lt;em>dockershim&lt;/em> from Kubernetes. The removal was originally
&lt;a href="https://kubernetes.io/blog/2020/12/08/kubernetes-1-20-release-announcement/">announced&lt;/a>
as a part of the Kubernetes v1.20 release. The Kubernetes
&lt;a href="https://kubernetes.io/releases/#release-v1-24">v1.24 release&lt;/a> actually removed the dockershim
from Kubernetes.&lt;/p>
&lt;p>For more on what that means, check out the blog post
&lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Don't Panic: Kubernetes and Docker&lt;/a>.&lt;/p>
&lt;p>To determine the impact that the removal of dockershim would have for you or your organization,
you can read &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/">Check whether dockershim removal affects you&lt;/a>.&lt;/p>
&lt;p>In the months and days leading up to the Kubernetes 1.24 release, Kubernetes contributors worked hard to try to make this a smooth transition.&lt;/p>
&lt;ul>
&lt;li>A blog post detailing our &lt;a href="https://kubernetes.io/blog/2022/01/07/kubernetes-is-moving-on-from-dockershim/">commitment and next steps&lt;/a>.&lt;/li>
&lt;li>Checking if there were major blockers to migration to &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#container-runtimes">other container runtimes&lt;/a>.&lt;/li>
&lt;li>Adding a &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/">migrating from dockershim&lt;/a> guide.&lt;/li>
&lt;li>Creating a list of
&lt;a href="https://kubernetes.io/docs/reference/node/topics-on-dockershim-and-cri-compatible-runtimes/">articles on dockershim removal and on using CRI-compatible runtimes&lt;/a>.
That list includes some of the already mentioned docs, and also covers selected external sources
(including vendor guides).&lt;/li>
&lt;/ul>
&lt;h3 id="why-was-the-dockershim-removed-from-kubernetes">Why was the dockershim removed from Kubernetes?&lt;/h3>
&lt;p>Early versions of Kubernetes only worked with a specific container runtime:
Docker Engine. Later, Kubernetes added support for working with other container runtimes.
The CRI standard was &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">created&lt;/a> to
enable interoperability between orchestrators (like Kubernetes) and many different container
runtimes.
Docker Engine doesn't implement that interface (CRI), so the Kubernetes project created
special code to help with the transition, and made that &lt;em>dockershim&lt;/em> code part of Kubernetes
itself.&lt;/p>
&lt;p>The dockershim code was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2221-remove-dockershim">Dockershim Removal Kubernetes Enhancement Proposal&lt;/a>.
In fact, maintaining dockershim had become a heavy burden on the Kubernetes maintainers.&lt;/p>
&lt;p>Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing the dockershim from Kubernetes allows further development in those areas.&lt;/p>
&lt;h3 id="are-docker-and-containers-the-same-thing">Are Docker and containers the same thing?&lt;/h3>
&lt;p>Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.&lt;/p>
&lt;h3 id="will-my-existing-container-images-still-work">Will my existing container images still work?&lt;/h3>
&lt;p>Yes, the images produced from &lt;code>docker build&lt;/code> will work with all CRI implementations.
All your existing images will still work exactly the same.&lt;/p>
&lt;h4 id="what-about-private-images">What about private images?&lt;/h4>
&lt;p>Yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.&lt;/p>
&lt;h3 id="can-i-still-use-docker-engine-in-kubernetes-1-23">Can I still use Docker Engine in Kubernetes 1.23?&lt;/h3>
&lt;p>Yes, the only thing changed in 1.20 is a single warning log printed at &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
startup if using Docker Engine as the runtime. You'll see this warning in all versions up to 1.23. The dockershim removal occurred
in Kubernetes 1.24.&lt;/p>
&lt;p>If you're running Kubernetes v1.24 or later, see &lt;a href="#can-i-still-use-docker-engine-as-my-container-runtime">Can I still use Docker Engine as my container runtime?&lt;/a>.
(Remember, you can switch away from the dockershim if you're using any supported Kubernetes release; from release v1.24, you
&lt;strong>must&lt;/strong> switch as Kubernetes no longer includes the dockershim).&lt;/p>
&lt;h3 id="which-cri-implementation-should-i-use">Which CRI implementation should I use?&lt;/h3>
&lt;p>That’s a complex question and it depends on a lot of factors. If Docker Engine is
working for you, moving to containerd should be a relatively easy swap and
will have strictly better performance and less overhead. However, we encourage you
to explore all the options from the &lt;a href="https://landscape.cncf.io/card-mode?category=container-runtime&amp;amp;grouping=category">CNCF landscape&lt;/a> in case another would be an
even better fit for your environment.&lt;/p>
&lt;h4 id="can-i-still-use-docker-engine-as-my-container-runtime">Can I still use Docker Engine as my container runtime?&lt;/h4>
&lt;p>First off, if you use Docker on your own PC to develop or test containers: nothing changes.
You can still use Docker locally no matter what container runtime(s) you use for your
Kubernetes clusters. Containers make this kind of interoperability possible.&lt;/p>
&lt;p>Mirantis and Docker have &lt;a href="https://www.mirantis.com/blog/mirantis-to-take-over-support-of-kubernetes-dockershim-2/">committed&lt;/a> to maintaining a replacement adapter for
Docker Engine, and to maintain that adapter even after the in-tree dockershim is removed
from Kubernetes. The replacement adapter is named &lt;a href="https://github.com/Mirantis/cri-dockerd">&lt;code>cri-dockerd&lt;/code>&lt;/a>.&lt;/p>
&lt;p>You can install &lt;code>cri-dockerd&lt;/code> and use it to connect the kubelet to Docker Engine. Read &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/migrating-from-dockershim/migrate-dockershim-dockerd/">Migrate Docker Engine nodes from dockershim to cri-dockerd&lt;/a> to learn more.&lt;/p>
&lt;h3 id="are-there-examples-of-folks-using-other-runtimes-in-production-today">Are there examples of folks using other runtimes in production today?&lt;/h3>
&lt;p>All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.&lt;/p>
&lt;p>Additionally, the &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the &lt;a href="https://cri-o.io/">CRI-O&lt;/a> runtime in production since June 2019.&lt;/p>
&lt;p>For other examples and references you can look at the adopters of containerd and
CRI-O, two container runtimes under the Cloud Native Computing Foundation (&lt;a href="https://cncf.io">CNCF&lt;/a>).&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="people-keep-referencing-oci-what-is-that">People keep referencing OCI, what is that?&lt;/h3>
&lt;p>OCI stands for the &lt;a href="https://opencontainers.org/about/overview/">Open Container Initiative&lt;/a>, which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>, which is the underlying default runtime for both
&lt;a href="https://containerd.io/">containerd&lt;/a> and &lt;a href="https://cri-o.io/">CRI-O&lt;/a>. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.&lt;/p>
&lt;h3 id="what-should-i-look-out-for-when-changing-cri-implementations">What should I look out for when changing CRI implementations?&lt;/h3>
&lt;p>While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:&lt;/p>
&lt;ul>
&lt;li>Logging configuration&lt;/li>
&lt;li>Runtime resource limitations&lt;/li>
&lt;li>Node provisioning scripts that call docker or use Docker Engine via its control socket&lt;/li>
&lt;li>Plugins for &lt;code>kubectl&lt;/code> that require the &lt;code>docker&lt;/code> CLI or the Docker Engine control socket&lt;/li>
&lt;li>Tools from the Kubernetes project that require direct access to Docker Engine
(for example: the deprecated &lt;code>kube-imagepuller&lt;/code> tool)&lt;/li>
&lt;li>Configuration of functionality like &lt;code>registry-mirrors&lt;/code> and insecure registries&lt;/li>
&lt;li>Other support scripts or daemons that expect Docker Engine to be available and are run
outside of Kubernetes (for example, monitoring or security agents)&lt;/li>
&lt;li>GPUs or special hardware and how they integrate with your runtime and Kubernetes&lt;/li>
&lt;/ul>
&lt;p>If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you've customized
your &lt;code>dockerd&lt;/code> configuration, you’ll need to adapt that for your new container
runtime where possible.&lt;/p>
&lt;p>Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> tool as a drop-in replacement (see
&lt;a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/#mapping-from-docker-cli-to-crictl">mapping from docker cli to crictl&lt;/a>)
and for the latter you can use newer container build options like &lt;a href="https://github.com/genuinetools/img">img&lt;/a>, &lt;a href="https://github.com/containers/buildah">buildah&lt;/a>,
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>, or &lt;a href="https://github.com/vmware-tanzu/buildkit-cli-for-kubectl">buildkit-cli-for-kubectl&lt;/a> that don’t require Docker.&lt;/p>
&lt;p>For containerd, you can start with their &lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">documentation&lt;/a> to see what configuration
options are available as you migrate things over.&lt;/p>
&lt;p>For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">Container Runtimes&lt;/a>.&lt;/p>
&lt;h3 id="what-if-i-have-more-questions">What if I have more questions?&lt;/h3>
&lt;p>If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: &lt;a href="https://discuss.kubernetes.io/">https://discuss.kubernetes.io/&lt;/a>.&lt;/p>
&lt;p>You can discuss the decision to remove dockershim via a dedicated
&lt;a href="https://github.com/kubernetes/kubernetes/issues/106917">GitHub issue&lt;/a>.&lt;/p>
&lt;p>You can also check out the excellent blog post
&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">Wait, Docker is deprecated in Kubernetes now?&lt;/a> a more in-depth technical
discussion of the changes.&lt;/p>
&lt;h3 id="is-there-any-tooling-that-can-help-me-find-dockershim-in-use">Is there any tooling that can help me find dockershim in use?&lt;/h3>
&lt;p>Yes! The &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">Detector for Docker Socket (DDS)&lt;/a> is a kubectl plugin that you can
install and then use to check your cluster. DDS can detect if active Kubernetes workloads
are mounting the Docker Engine socket (&lt;code>docker.sock&lt;/code>) as a volume.
Find more details and usage patterns in the DDS project's &lt;a href="https://github.com/aws-containers/kubectl-detector-for-docker-socket">README&lt;/a>.&lt;/p>
&lt;h3 id="can-i-have-a-hug">Can I have a hug?&lt;/h3>
&lt;p>Yes, we're still giving hugs as requested. 🤗🤗🤗&lt;/p></description></item></channel></rss>