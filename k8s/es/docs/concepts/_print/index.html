<!doctype html><html lang=es class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/><link rel=alternate hreflang=pl href=https://kubernetes.io/pl/docs/concepts/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/concepts/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/es/docs/concepts/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Conceptos | Kubernetes</title><meta property="og:title" content="Conceptos"><meta property="og:description" content="Orquestación de contenedores para producción"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/es/docs/concepts/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Conceptos"><meta itemprop=description content="Orquestación de contenedores para producción"><meta name=twitter:card content="summary"><meta name=twitter:title content="Conceptos"><meta name=twitter:description content="Orquestación de contenedores para producción"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="La sección de conceptos te ayudará a conocer los componentes de Kubernetes así como las abstracciones que utiliza para representar tu cluster. Además, te ayudará a obtener un conocimiento más profundo sobre cómo funciona Kubernetes.
Introducción En Kubernetes se utilizan los objetos de la API de Kubernetes para describir el estado deseado del clúster: qué aplicaciones u otras cargas de trabajo se quieren ejecutar, qué imagenes de contenedores usan, el número de replicas, qué red y qué recursos de almacenamiento quieres que tengan disponibles, etc."><meta property="og:description" content="La sección de conceptos te ayudará a conocer los componentes de Kubernetes así como las abstracciones que utiliza para representar tu cluster. Además, te ayudará a obtener un conocimiento más profundo sobre cómo funciona Kubernetes.
Introducción En Kubernetes se utilizan los objetos de la API de Kubernetes para describir el estado deseado del clúster: qué aplicaciones u otras cargas de trabajo se quieren ejecutar, qué imagenes de contenedores usan, el número de replicas, qué red y qué recursos de almacenamiento quieres que tengan disponibles, etc."><meta name=twitter:description content="La sección de conceptos te ayudará a conocer los componentes de Kubernetes así como las abstracciones que utiliza para representar tu cluster. Además, te ayudará a obtener un conocimiento más profundo sobre cómo funciona Kubernetes.
Introducción En Kubernetes se utilizan los objetos de la API de Kubernetes para describir el estado deseado del clúster: qué aplicaciones u otras cargas de trabajo se quieren ejecutar, qué imagenes de contenedores usan, el número de replicas, qué red y qué recursos de almacenamiento quieres que tengan disponibles, etc."><meta property="og:url" content="https://kubernetes.io/es/docs/concepts/"><meta property="og:title" content="Conceptos"><meta name=twitter:title content="Conceptos"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/es/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/es/docs/>Documentación</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/es/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/es/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/es/community/>Comunidad</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/es/case-studies/>Casos de éxito</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/es/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/es/docs/concepts/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/es/docs/concepts/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/es/docs/concepts/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/es/docs/concepts/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/es/docs/concepts/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Español (Spanish)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/>Deutsch (German)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/>Bahasa Indonesia</a>
<a class=dropdown-item href=/ru/docs/concepts/>Русский (Russian)</a>
<a class=dropdown-item href=/pl/docs/concepts/>Polski (Polish)</a>
<a class=dropdown-item href=/uk/docs/concepts/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Versión imprimible multipagina.
<a href=# onclick="return print(),!1">Haga click aquí para imprimir</a>.</p><p><a href=/es/docs/concepts/>Volver a la vista normal de esta página</a>.</p></div><h1 class=title>Conceptos</h1><ul><li>1: <a href=#pg-0554ac387412eaf4e6e89b2f847dacde>Introducción</a></li><ul><li>1.1: <a href=#pg-45bdca6129cf540121623e903c18ba46>¿Qué es Kubernetes?</a></li><li>1.2: <a href=#pg-13b0f1dbe89228e3d76d2ac231e245f1>Componentes de Kubernetes</a></li><li>1.3: <a href=#pg-0c745f42e623d2b70a53bc0e6db73d95>API de Kubernetes</a></li><li>1.4: <a href=#pg-110f33530cf761140cb1dab536baef04>Objetos de Kubernetes</a></li><ul><li>1.4.1: <a href=#pg-9f5adfa77f48c50d5cc81155a3cecb98>Entender los Objetos de Kubernetes</a></li><li>1.4.2: <a href=#pg-f37749a83c2916b63279ea60f3cfe53e>Nombres</a></li><li>1.4.3: <a href=#pg-1127165f472b7181b9c1d5a0b187d620>Espacios de nombres</a></li><li>1.4.4: <a href=#pg-f1dec4557fb8ffbac9f11390aaaf9fa4>Etiquetas y Selectores</a></li><li>1.4.5: <a href=#pg-93cd7a1d4e1623e2bf01afc49a5af69c>Anotaciones</a></li><li>1.4.6: <a href=#pg-046c03090d47bc4b89b818dc645c3865>Selectores de Campo</a></li><li>1.4.7: <a href=#pg-5dd62c6a4a481b4cf1ac50f6799eb581>Etiquetas recomendadas</a></li></ul><li>1.5: <a href=#pg-796ad76ff5c9b7f23bc99ef9a33fba72>Gestión de objetos usando kubectl</a></li><ul></ul></ul><li>2: <a href=#pg-2bf36ccd6b3dbeafecf87c39761b07c7>Arquitectura de Kubernetes</a></li><ul><li>2.1: <a href=#pg-9ef2890698e773b6c0d24fd2c20146f5>Nodos</a></li><li>2.2: <a href=#pg-63e7fdf87ba61eb2586bb8c625c23506>Comunicación Nodo-Maestro</a></li><li>2.3: <a href=#pg-bc804b02614d67025b4c788f1ca87fbc>Conceptos subyacentes del Cloud Controller Manager</a></li></ul><li>3: <a href=#pg-a5f7383c83ab9eb9cd0e3c4c020b3ae6>Contenedores</a></li><ul><li>3.1: <a href=#pg-a858027489648786a3b16264e451272b>RuntimeClass</a></li><li>3.2: <a href=#pg-e92055f79467a0422ecbc6d5169fcd38>Variables de entorno de un Container</a></li><li>3.3: <a href=#pg-e6941d969d81540208a3e78bc56f43bc>Container Lifecycle Hooks</a></li></ul><li>4: <a href=#pg-d52aadda80edd9f8c514cfe2321363c2>Cargas de trabajo</a></li><ul><li>4.1: <a href=#pg-4d68b0ccf9c683e6368ffdcc40c838d4>Pods</a></li><ul><li>4.1.1: <a href=#pg-99cce294fe789317ee684a6e1f07f20f>Pods</a></li><li>4.1.2: <a href=#pg-1ccbd4eeded6ab138d98b59175bd557e>Contenedores de Inicialización</a></li><li>4.1.3: <a href=#pg-4e9b9cbc9776b12e7335c53da377c9c8>Pod Preset</a></li><li>4.1.4: <a href=#pg-4aaf43c715cd764bc8ed4436f3537e68>Interrupciones</a></li><li>4.1.5: <a href=#pg-53a1005011e1bda2ce81819aad7c8b32>Containers Efímeros</a></li></ul><li>4.2: <a href=#pg-89637410cacae45a36ab1cc278c482eb>Controladores</a></li><ul><li>4.2.1: <a href=#pg-d459b930218774655fa7fd1620625539>ReplicaSet</a></li><li>4.2.2: <a href=#pg-27f1331d515d95f76aa1156088b4ad91>ReplicationController</a></li><li>4.2.3: <a href=#pg-a2dc0393e0c4079e1c504b6429844e86>Deployment</a></li><li>4.2.4: <a href=#pg-6d72299952c37ca8cc61b416e5bdbcd4>StatefulSets</a></li><li>4.2.5: <a href=#pg-41600eb8b6631c88848156f381e9d588>DaemonSet</a></li><li>4.2.6: <a href=#pg-9add0d2120634b63073ad08dc8683bd6>Recolección de Basura</a></li><li>4.2.7: <a href=#pg-4de50a37ebb6f2340484192126cb7a04>Controlador TTL para Recursos Finalizados</a></li><li>4.2.8: <a href=#pg-230b370ed7a1dedf04163f02fa701802>Jobs - Ejecución hasta el final</a></li><li>4.2.9: <a href=#pg-2e4cec01c525b45eccd6010e21cc76d9>CronJob</a></li></ul></ul><li>5: <a href=#pg-0a0a7eca3e302a3c08f8c85e15d337fd>Servicios, balanceo de carga y redes</a></li><ul><li>5.1: <a href=#pg-5701136fd2ce258047b6ddc389112352>Service</a></li><li>5.2: <a href=#pg-ded1daafdcd293023ee333728007ca61>Políticas de red (Network Policies)</a></li></ul><li>6: <a href=#pg-f018f568c6723865753f150c3c59bdda>Almacenamiento</a></li><ul><li>6.1: <a href=#pg-27795584640a03bd2024f1fe3b3ab754>Volumes</a></li><li>6.2: <a href=#pg-c262af210c6828dec445d2f55a1d877a>Snapshots de Volúmenes</a></li><li>6.3: <a href=#pg-707ca81a34eb1ca202f34692e9917d1e>Clonación de volumen CSI</a></li><li>6.4: <a href=#pg-4d00116c86dade62bdd5be7dc2afa1ca>Volume Snapshot Classes</a></li><li>6.5: <a href=#pg-018f0a7fc6e2f6d16da37702fc39b4f3>Aprovisionamiento Dinámico de volumen</a></li><li>6.6: <a href=#pg-00cd24f4570b7acaac75c2551c948bc7>Capacidad de Almacenamiento</a></li><li>6.7: <a href=#pg-b2e4b16ac37988c678a3312a4a6639f8>Límites de Volumen específicos del Nodo</a></li><li>6.8: <a href=#pg-4f40cb95a671e51b4f0156a409d95c6d>Supervisión del Estado del Volumen</a></li></ul><li>7: <a href=#pg-275bea454e1cf4c5adeca4058b5af988>Configuración</a></li><ul><li>7.1: <a href=#pg-6b5ccadd699df0904e8e9917c5450c4b>ConfigMaps</a></li><li>7.2: <a href=#pg-6bc4711b1e133ac278036e963f4ce696>Sobrecarga de Pod</a></li><li>7.3: <a href=#pg-436057b96151ecb8a4a9a9f456b5d0fc>Administrando los recursos de los contenedores</a></li><li>7.4: <a href=#pg-e511ed821ada65d0053341dbd8ad2bb5>Secrets</a></li><li>7.5: <a href=#pg-ab6d20f33ad930a67ee7ef57bff6c75e>Organizar el acceso a los clústeres utilizando archivos kubeconfig</a></li></ul><li>8: <a href=#pg-712cb3c03ff14a39e5a83a6d9b71d203>Seguridad</a></li><ul><li>8.1: <a href=#pg-04eeb110d75afc8acb2cf7a3db743985>Vista General de Seguridad Cloud Native</a></li><li>8.2: <a href=#pg-4d77d1ae4c06aa14f54b385191627881>Controlando el Acceso a la API de Kubernetes</a></li></ul><li>9: <a href=#pg-ac9161c6d952925b083ad9602b4e8e7f>Políticas</a></li><ul><li>9.1: <a href=#pg-a935ff8c59eb116b43494255cc67f69a>Rangos de límites (Limit Ranges)</a></li></ul><li>10: <a href=#pg-285a3785fd3d20f437c28d87ca4dadca>Administración del Clúster</a></li><ul></ul><li>11: <a href=#pg-7e0d97616b15e2c383c6a0a96ec442cb>Extendiendo Kubernetes</a></li><ul><li>11.1: <a href=#pg-0af41d3bd7c785621b58b7564793396a>Extendiendo la API de Kubernetes</a></li><ul></ul><li>11.2: <a href=#pg-c8937cdc9df96f3328becf04f8211292>Extensiones de computación, almacenamiento y redes</a></li><ul></ul></ul></ul><div class=content><p>La sección de conceptos te ayudará a conocer los componentes de Kubernetes así como las abstracciones que utiliza para representar tu cluster. Además, te ayudará a obtener un conocimiento más profundo sobre cómo funciona Kubernetes.</p><h2 id=introducción>Introducción</h2><p>En Kubernetes se utilizan los <em>objetos de la API de Kubernetes</em> para describir el <em>estado deseado</em> del clúster: qué aplicaciones u otras cargas de trabajo se quieren ejecutar, qué imagenes de contenedores usan, el número de replicas, qué red y qué recursos de almacenamiento quieres que tengan disponibles, etc. Se especifica el estado deseado del clúster mediante la creación de objetos usando la API de Kubernetes, típicamente mediante la interfaz de línea de comandos, <code>kubectl</code>. También se puede usar la API de Kubernetes directamente para interactuar con el clúster y especificar o modificar tu estado deseado.</p><p>Una vez que se especifica el estado deseado, el <em>Plano de Control de Kubernetes</em> realizará las acciones necesarias para que el estado actual del clúster coincida con el estado deseado. Para ello, Kubernetes realiza diferentes tareas de forma automática, como pueden ser: parar o arrancar contenedores, escalar el número de réplicas de una aplicación dada, etc. El Plano de Control de Kubernetes consiste en un grupo de daemons que corren en tu clúster:</p><ul><li><p>El <strong>Master de Kubernetes</strong> es un conjunto de tres daemons que se ejecutan en un único nodo del clúster, que se denomina nodo master. Estos daemons son: <a href=/docs/admin/kube-apiserver/>kube-apiserver</a>, <a href=/docs/admin/kube-controller-manager/>kube-controller-manager</a> y <a href=/docs/admin/kube-scheduler/>kube-scheduler</a>.</p></li><li><p>Los restantes nodos no master contenidos en tu clúster, ejecutan los siguientes dos daemons:</p><ul><li><strong><a href=/docs/admin/kubelet/>kubelet</a></strong>, el cual se comunica con el Master de Kubernetes.</li><li><strong><a href=/docs/admin/kube-proxy/>kube-proxy</a></strong>, un proxy de red que implementa los servicios de red de Kubernetes en cada nodo.</li></ul></li></ul><h2 id=objetos-de-kubernetes>Objetos de Kubernetes</h2><p>Kubernetes tiene diferentes abstracciones que representan el estado de tu sistema: aplicaciones contenerizadas desplegadas y cargas de trabajo, sus recursos de red y almacenamiento asociados e información adicional acerca de lo que el clúster está haciendo en un momento dado. Estas abstracciones están representadas por objetos de la API de Kubernetes. Puedes revisar [Entendiendo los Objetos de Kubernetes] (/docs/concepts/overview/working-with-objects/kubernetes-objects/) para obtener más detalles.</p><p>Los objetos básicos de Kubernetes incluyen:</p><ul><li><a href=/es/docs/concepts/workloads/pods/pod/>Pod</a></li><li><a href=/docs/concepts/services-networking/service/>Service</a></li><li><a href=/docs/concepts/storage/volumes/>Volume</a></li><li><a href=/es/docs/concepts/overview/working-with-objects/namespaces/>Namespace</a></li></ul><p>Además, Kubernetes contiene abstracciónes de nivel superior llamadas Controladores. Los Controladores se basan en los objetos básicos y proporcionan funcionalidades adicionales sobre ellos. Incluyen:</p><ul><li><a href=/es/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a></li><li><a href=/es/docs/concepts/workloads/controllers/deployment/>Deployment</a></li><li><a href=/es/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a></li><li><a href=/es/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a></li><li><a href=/es/docs/concepts/workloads/controllers/jobs-run-to-completion/>Job</a></li></ul><h2 id=plano-de-control-de-kubernetes>Plano de Control de Kubernetes</h2><p>Los distintos componentes del Plano de Control de Kubernetes, tales como el Master de Kubernetes y el proceso kubelet, gobiernan cómo Kubernetes se comunica con el clúster. El Plano de Control mantiene un registro de todos los Objetos de Kubernetes presentes en el sistema y ejecuta continuos bucles de control para gestionar el estado de los mismos. En un momento dado, los bucles del Plano de Control responderán a los cambios que se realicen en el clúster y ejecutarán las acciones necesarias para hacer que el estado actual de todos los objetos del sistema converjan hacia el estado deseado que has proporcionado.</p><p>Por ejemplo, cuando usas la API de Kubernetes para crear un Deployment, estás proporcionando un nuevo estado deseado para el sistema. El Plano de Control de Kubernetes registra la creación del objeto y lleva a cabo tus instrucciones ejecutando las aplicaciones requeridas en los nodos del clúster, haciendo de esta manera que el estado actual coincida con el estado deseado.</p><h3 id=el-master-de-kubernetes>El Master de Kubernetes</h3><p>El Master de Kubernetes es el responsable de mantener el estado deseado de tu clúster. Cuando interactuas con Kubernetes, como por ejemplo cuando utilizas la interfaz de línea de comandos <code>kubectl</code>, te estás comunicando con el master de tu clúster de Kubernetes.</p><blockquote><p>Por "master" entendemos la colección de daemons que gestionan el estado del clúster. Típicamente, estos daemons se ejecutan todos en un único nodo del clúster, y este nodo recibe por tanto la denominación de master. El master puede estar replicado por motivos de disponibilidad y redundancia.</p></blockquote><h3 id=kubernetes-nodes>Kubernetes Nodes</h3><p>En un clúster de Kubernetes, los nodos son las máquinas (máquinas virtuales, servidores físicos, etc) que ejecutan tus aplicaciones y flujos de trabajo en la nube. El master de Kubernetes controla cada nodo, por lo que en raras ocasiones interactuarás con los nodos directamente.</p><h4 id=metadatos-de-los-objectos>Metadatos de los Objectos</h4><ul><li><a href=/es/docs/concepts/overview/working-with-objects/annotations/>Annotations</a></li></ul><h2 id=siguientes-pasos>Siguientes pasos</h2><p>Si quieres empezar a contribuir a la documentación de Kubernetes accede a la página <a href=/es/docs/contribute/start/>Empieza a contribuir</a>.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-0554ac387412eaf4e6e89b2f847dacde>1 - Introducción</h1></div><div class=td-content><h1 id=pg-45bdca6129cf540121623e903c18ba46>1.1 - ¿Qué es Kubernetes?</h1><p>Esta página ofrece una visión general sobre Kubernetes.</p><p>Kubernetes es una plataforma portable y extensible de código abierto para
administrar cargas de trabajo y servicios. Kubernetes facilita la automatización
y la configuración declarativa. Tiene un ecosistema grande y en rápido crecimiento.
El soporte, las herramientas y los servicios para Kubernetes están ampliamente disponibles.</p><p>Google liberó el proyecto Kubernetes en el año 2014. Kubernetes se basa en <a href=https://research.google.com/pubs/pub43438.html>la experiencia de
Google corriendo aplicaciones en producción a gran escala por década y media</a>, junto a las mejores ideas y prácticas de la comunidad.</p><h2 id=por-qué-necesito-kubernetes-y-qué-puede-hacer-por-mi>¿Por qué necesito Kubernetes y qué puede hacer por mi?</h2><p>Kubernetes tiene varias características. Puedes pensar en Kubernetes como:</p><ul><li>una plataforma de contenedores</li><li>una plataforma de microservicios</li><li>una plataforma portable de nube</li></ul><p>y mucho más.</p><p>Kubernetes ofrece un entorno de administración <strong>centrado en contenedores</strong>. Kubernetes
orquesta la infraestructura de cómputo, redes y almacenamiento para que las cargas de
trabajo de los usuarios no tengan que hacerlo. Esto ofrece la simplicidad de las Plataformas
como Servicio (PaaS) con la flexibilidad de la Infraestructura como Servicio (IaaS) y permite
la portabilidad entre proveedores de infraestructura.</p><h2 id=qué-hace-de-kubernetes-una-plataforma>¿Qué hace de Kubernetes una plataforma?</h2><p>A pesar de que Kubernetes ya ofrece muchas funcionalidades, siempre hay nuevos
escenarios que se benefician de nuevas características. Los flujos de trabajo
de las aplicaciones pueden optimizarse para acelerar el tiempo de desarrollo.
Una solución de orquestación propia puede ser suficiente al principio, pero suele requerir
una automatización robusta cuando necesita escalar. Es por ello que Kubernetes fue diseñada como
una plataforma: para poder construir un ecosistema de componentes y herramientas que hacen
más fácil el desplegar, escalar y administrar aplicaciones.</p><p>Las etiquetas, o <a href=/es/docs/concepts/overview/working-with-objects/labels/>Labels</a>, le
permiten a los usuarios organizar sus recursos como deseen. Las anotaciones, o <a href=/es/docs/concepts/overview/working-with-objects/annotations/>Annotations</a>, les permiten asignar información arbitraria a un recurso para
facilitar sus flujos de trabajo y hacer más fácil a las herramientas administrativas inspeccionar el estado.</p><p>Además, el <a href=/docs/concepts/overview/components/>Plano de Control</a> de Kubernetes usa las mismas
<a href=/docs/reference/using-api/api-overview/>APIs</a> que usan los desarrolladores y usuarios finales.
Los usuarios pueden escribir sus propios controladores, como por ejemplo un planificador o <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/scheduler.md>scheduler</a>,
usando <a href=/docs/concepts/api-extension/custom-resources/>sus propias
APIs</a>
desde una <a href=/docs/user-guide/kubectl-overview/>herramienta de línea de comandos</a>.</p><p>Este
<a href=https://git.k8s.io/design-proposals-archive/architecture/architecture.md>diseño</a>
ha permitido que otros sistemas sean construidos sobre Kubernetes.</p><h2 id=lo-que-kubernetes-no-es>Lo que Kubernetes no es</h2><p>Kubernetes no es una Plataforma como Servicio (PaaS) convencional. Ya que
Kubernetes opera a nivel del contenedor y no a nivel del hardware, ofrece
algunas características que las PaaS también ofrecen, como deployments,
escalado, balanceo de carga, registros y monitoreo. Dicho esto, Kubernetes
no es monolítico y las soluciones que se ofrecen de forma predeterminada
son opcionales e intercambiables.</p><p>Kubernetes ofrece los elementos esenciales para construir una plataforma
para desarrolladores, preservando la elección del usuario y la flexibilidad
en las partes más importantes.</p><p>Entonces, podemos decir que Kubernetes:</p><ul><li>No limita el tipo de aplicaciones que soporta. Kubernetes busca dar soporte a un número diverso de cargas de trabajo, que incluyen aplicaciones con y sin estado así como aplicaciones que procesan datos. Si la aplicación puede correr en un contenedor, debería correr bien en Kubernetes.</li><li>No hace deployment de código fuente ni compila tu aplicación. Los flujos de integración, entrega y deployment continuo (CI/CD) vienen determinados por la cultura y preferencia organizacional y sus requerimientos técnicos.</li><li>No provee servicios en capa de aplicación como middleware (por ejemplo, buses de mensaje), frameworks de procesamiento de datos (como Spark), bases de datos (como MySQL), caches o sistemas de almacenamiento (como Ceph). Es posible correr estas aplicaciones en Kubernetes, o acceder a ellos desde una aplicación usando un mecanismo portable como el Open Service Broker.</li><li>No dictamina las soluciones de registros, monitoreo o alerta que se deben usar. Hay algunas integraciones que se ofrecen como prueba de concepto, y existen mecanismos para recolectar y exportar métricas.</li><li>No provee ni obliga a usar un sistema o lenguaje de configuración (como <a href=https://github.com/google/jsonnet>jsonnet</a>) sino que ofrece una API declarativa que puede ser usada con cualquier forma de especificación declarativa</li><li>No provee ni adopta un sistema exhaustivo de mantenimiento, administración o corrección automática de errores</li></ul><p>Además, Kubernetes no es un mero <em>sistema de orquestación</em>. De hecho, Kubernetes elimina la necesidad de orquestar. <em>Orquestación</em> se define como la ejecución de un flujo de trabajo definido: haz A, luego B y entonces C. Kubernetes está compuesto de un conjunto de procesos de control independientes y combinables entre si que llevan el estado actual hacia el estado deseado. No debería importar demasiado como llegar de A a C. No se requiere control centralizado y, como resultado, el sistema es más fácil de usar, más poderoso, robusto, resiliente y extensible.</p><h2 id=por-qué-usar-contenedores>¿Por qué usar contenedores?</h2><p>¿Te preguntas las razones para usar contenedores?</p><p><img src=/images/docs/why_containers.svg alt="Why Containers?"></p><p>La <em>Manera Antigua</em> de desplegar aplicaciones era instalarlas en un
servidor usando el administrador de paquetes del sistema operativo.
La desventaja era que los ejecutables, la configuración, las librerías
y el ciclo de vida de todos estos componentes se entretejían unos a
otros. Podíamos construir imágenes de máquina virtual inmutables para
tener rollouts y rollbacks predecibles, pero las máquinas virtuales
son pesadas y poco portables.</p><p>La <em>Manera Nueva</em> es desplegar contenedores basados en virtualización
a nivel del sistema operativo, en vez del hardware. Estos contenedores
están aislados entre ellos y con el servidor anfitrión: tienen sus propios
sistemas de archivos, no ven los procesos de los demás y el uso de recursos
puede ser limitado. Son más fáciles de construir que una máquina virtual, y
porque no están acoplados a la infraestructura y sistema de archivos del
anfitrión, pueden llevarse entre nubes y distribuciones de sistema operativo.</p><p>Ya que los contenedores son pequeños y rápidos, una aplicación puede ser
empaquetada en una imagen de contenedor. Esta relación uno a uno entre
aplicación e imagen nos abre un abanico de beneficios para usar contenedores.
Con contenedores, podemos crear imágenes inmutables al momento de la compilación
en vez del despliegue ya que las aplicaciones no necesitan componerse junto al
resto del <em>stack</em> ni atarse al entorno de infraestructura de producción. Generar
una imagen de contenedor al momento de la compilación permite tener un entorno
consistente que va desde desarrollo hasta producción. De igual forma, los contenedores
son más transparentes que las máquinas virtuales y eso hace que el monitoreo y la
administración sean más fáciles. Esto se aprecia más cuando los ciclos de vida de
los contenedores son administrados por la infraestructura en vez de un proceso supervisor
escondido en el contenedor. Por último, ya que solo hay una aplicación por contenedor,
administrar el despliegue de la aplicación se reduce a administrar el contenedor.</p><p>En resumen, los beneficios de usar contenedores incluyen:</p><ul><li><strong>Ágil creación y despliegue de aplicaciones</strong>:
Mayor facilidad y eficiencia al crear imágenes de contenedor en vez de máquinas virtuales</li><li><strong>Desarrollo, integración y despliegue continuo</strong>:
Permite que la imagen de contenedor se construya y despliegue de forma frecuente y confiable,
facilitando los rollbacks pues la imagen es inmutable</li><li><strong>Separación de tareas entre Dev y Ops</strong>:
Puedes crear imágenes de contenedor al momento de compilar y no al desplegar, desacoplando la
aplicación de la infraestructura</li><li><strong>Observabilidad</strong>
No solamente se presenta la información y métricas del sistema operativo, sino la salud de la
aplicación y otras señales</li><li><strong>Consistencia entre los entornos de desarrollo, pruebas y producción</strong>:
La aplicación funciona igual en un laptop y en la nube</li><li><strong>Portabilidad entre nubes y distribuciones</strong>:
Funciona en Ubuntu, RHEL, CoreOS, tu datacenter físico, Google Kubernetes Engine y todo lo demás</li><li><strong>Administración centrada en la aplicación</strong>:
Eleva el nivel de abstracción del sistema operativo y el hardware virtualizado a la aplicación que funciona en un sistema con recursos lógicos</li><li><strong><a href=https://martinfowler.com/articles/microservices.html>Microservicios</a></strong> distribuidos, elásticos, liberados y débilmente acoplados:
Las aplicaciones se separan en piezas pequeñas e independientes que pueden ser desplegadas y administradas de forma dinámica, y no como una aplicación monolítica que opera en una sola máquina de gran capacidad</li><li><strong>Aislamiento de recursos</strong>:
Hace el rendimiento de la aplicación más predecible</li><li><strong>Utilización de recursos</strong>:
Permite mayor eficiencia y densidad</li></ul><h2 id=qué-significa-kubernetes-qué-significa-k8s>¿Qué significa Kubernetes? ¿Qué significa K8S?</h2><p>El nombre <strong>Kubernetes</strong> proviene del griego y significa <em>timonel</em> o <em>piloto</em>. Es la raíz de <em>gobernador</em> y de <a href="http://www.etymonline.com/index.php?term=cybernetics">cibernética</a>. <em>K8s</em>
es una abrevación que se obtiene al reemplazar las ocho letras "ubernete" con el número 8.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>¿Estás listo para <a href=/docs/setup/>empezar</a>?</li><li>Para saber más, visita el resto de la <a href=/docs/home/>documentación de Kubernetes</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-13b0f1dbe89228e3d76d2ac231e245f1>1.2 - Componentes de Kubernetes</h1><p>Este documento describe los distintos componentes que
son necesarios para operar un clúster de Kubernetes.</p><h2 id=componentes-del-plano-de-control>Componentes del plano de control</h2><p>Los componentes que forman el plano de control toman decisiones globales sobre
el clúster (por ejemplo, la planificación) y detectan y responden a eventos del clúster, como la creación
de un nuevo pod cuando la propiedad <code>replicas</code> de un controlador de replicación no se cumple.</p><p>Estos componentes pueden ejecutarse en cualquier nodo del clúster. Sin embargo para simplificar, los
scripts de instalación típicamente se inician en el mismo nodo de forma exclusiva,
sin que se ejecuten contenedores de los usuarios en esos nodos. El plano de control se ejecuta en varios nodos
para garantizar la <a href=/docs/admin/high-availability/>alta disponibilidad</a>.</p><h3 id=kube-apiserver>kube-apiserver</h3><p>El servidor de la API es el componente del <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/es/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='plano de control'>plano de control</a>
de Kubernetes que expone la API de Kubernetes. Se trata del frontend de Kubernetes,
recibe las peticiones y actualiza acordemente el estado en <a class=glossary-tooltip title='Almacén de datos persistente, consistente y distribuido de clave-valor utilizado para almacenar toda a la información del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>.</p><p>La principal implementación de un servidor de la API de Kubernetes es
<a href=/docs/reference/generated/kube-apiserver/>kube-apiserver</a>.
Es una implementación preparada para ejecutarse en alta disponiblidad y que
puede escalar horizontalmente para balancear la carga entre varias instancias.</p><h3 id=etcd>etcd</h3><p>Almacén de datos persistente, consistente y distribuido de clave-valor utilizado
para almacenar toda a la información del clúster de Kubernetes.</p><p>Si tu clúster utiliza etcd como sistema de almacenamiento, échale un vistazo a la
documentación sobre <a href=/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster>estrategias de backup</a>.</p><p>Puedes encontrar información detallada sobre etcd en su <a href=https://etcd.io/docs/>documentación oficial</a>.</p><h3 id=kube-scheduler>kube-scheduler</h3><p>Componente del plano de control que está pendiente de los
<a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> que no tienen ningún
<a class=glossary-tooltip title='Un Node, nodo en castellano, es una de las máquinas del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodo>nodo</a> asignado
y seleciona uno donde ejecutarlo.</p><p>Para decidir en qué <a class=glossary-tooltip title='Un Node, nodo en castellano, es una de las máquinas del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodo>nodo</a>
se ejecutará el <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=pod>pod</a>, se tienen
en cuenta diversos factores: requisitos de recursos, restricciones de hardware/software/políticas,
afinidad y anti-afinidad, localización de datos dependientes, entre otros.</p><h3 id=kube-controller-manager>kube-controller-manager</h3><p>Componente del plano de control que ejecuta los <a class=glossary-tooltip title='Los controladores son bucles de control que observan el estado del clúster, y ejecutan o solicitan los cambios que sean necesarios para alcanzar el estado deseado.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controladores>controladores</a> de Kubernetes.</p><p>Lógicamente cada <a class=glossary-tooltip title='Los controladores son bucles de control que observan el estado del clúster, y ejecutan o solicitan los cambios que sean necesarios para alcanzar el estado deseado.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controlador>controlador</a>
es un proceso independiente, pero para reducir la complejidad, todos se compilan
en un único binario y se ejecuta en un mismo proceso.</p><p>Estos controladores incluyen:</p><ul><li>Controlador de nodos: es el responsable de detectar y responder cuándo un nodo deja de funcionar</li><li>Controlador de replicación: es el responsable de mantener el número correcto de pods para cada controlador
de replicación del sistema</li><li>Controlador de endpoints: construye el objeto <code>Endpoints</code>, es decir, hace una unión entre los <code>Services</code> y los <code>Pods</code></li><li>Controladores de tokens y cuentas de servicio: crean cuentas y tokens de acceso a la API por defecto para los nuevos <a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=Namespaces>Namespaces</a>.</li></ul><h3 id=cloud-controller-manager>cloud-controller-manager</h3><p><a href=/docs/tasks/administer-cluster/running-cloud-controller/>cloud-controller-manager</a> ejecuta controladores que
interactúan con proveedores de la nube. El binario <code>cloud-controller-manager</code> es una característica alpha que se introdujo en la versión 1.6 de Kubernetes.</p><p><code>cloud-controller-manager</code> sólo ejecuta ciclos de control específicos para cada proveedor de la nube. Es posible
desactivar estos ciclos en <code>kube-controller-manager</code> pasando la opción <code>--cloud-provider= external</code> cuando se arranque el <code>kube-controller-manager</code>.</p><p><code>cloud-controller-manager</code> permite que el código de Kubernetes y el del proveedor de la nube evolucionen de manera independiente. Anteriormente, el código de Kubernetes dependía de la funcionalidad específica de cada proveedor de la nube. En el futuro, el código que sea específico a una plataforma debería ser mantenido por el proveedor de la nube y enlazado a <code>cloud-controller-manager</code> al correr Kubernetes.</p><p>Los siguientes controladores dependen de alguna forma de un proveedor de la nube:</p><ul><li>Controlador de nodos: es el responsable de detectar y actuar cuándo un nodo deja de responder</li><li>Controlador de rutas: para configurar rutas en la infraestructura de nube subyacente</li><li>Controlador de servicios: para crear, actualizar y eliminar balanceadores de carga en la nube</li><li>Controlador de volúmenes: para crear, conectar y montar volúmenes e interactuar con el proveedor de la nube para orquestarlos</li></ul><h2 id=componentes-de-nodo>Componentes de nodo</h2><p>Los componentes de nodo corren en cada nodo, manteniendo a los pods en funcionamiento y proporcionando el entorno de ejecución de Kubernetes.</p><h3 id=kubelet>kubelet</h3><p>Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.</p><p>El agente kubelet toma un conjunto de especificaciones de <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, llamados
PodSpecs, que han sido creados por Kubernetes y garantiza que los contenedores descritos en ellos estén funcionando y
en buen estado.</p><h3 id=kube-proxy>kube-proxy</h3><p><a href=/docs/admin/kube-proxy/>kube-proxy</a> permite abstraer un servicio en Kubernetes manteniendo las
reglas de red en el anfitrión y haciendo reenvío de conexiones.</p><h3 id=runtime-de-contenedores>Runtime de contenedores</h3><p>El <a class=glossary-tooltip title='El Container Runtime, entorno de ejecución de un contenedor, es el software responsable de ejecutar contenedores.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='runtime de los contenedores'>runtime de los contenedores</a> es el software responsable de ejecutar los contenedores. Kubernetes soporta varios de
ellos: <a href=http://www.docker.com>Docker</a>, <a href=https://containerd.io>containerd</a>, <a href=https://cri-o.io/>cri-o</a>, <a href=https://github.com/kubernetes-incubator/rktlet>rktlet</a> y cualquier implementación de la interfaz de runtime de contenedores de Kubernetes, o <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Kubernetes CRI</a>.</p><h2 id=addons>Addons</h2><p>Los <em>addons</em> son pods y servicios que implementan funcionalidades del clúster. Estos pueden ser administrados
por <code>Deployments</code>, <code>ReplicationControllers</code> y otros. Los <em>addons</em> asignados a un espacio de nombres se crean en el espacio <code>kube-system</code>.</p><p>Más abajo se describen algunos <em>addons</em>. Para una lista más completa de los <em>addons</em> disponibles, por favor visite <a href=/docs/concepts/cluster-administration/addons/>Addons</a>.</p><h3 id=dns>DNS</h3><p>Si bien los otros <em>addons</em> no son estrictamente necesarios, todos los clústers de Kubernetes deberían tener un <a href=/docs/concepts/services-networking/dns-pod-service/>DNS interno del clúster</a> ya que la mayoría de los ejemplos lo requieren.</p><p>El DNS interno del clúster es un servidor DNS, adicional a los que ya podrías tener en tu red, que sirve registros DNS a los servicios de Kubernetes.</p><p>Los contenedores que son iniciados por Kubernetes incluyen automáticamente este servidor en sus búsquedas DNS.</p><h3 id=dashboard>Interfaz Web (Dashboard)</h3><p>El <a href=/docs/tasks/access-application-cluster/web-ui-dashboard/>Dashboard</a> es una interfaz Web de propósito general para clústeres de Kubernetes. Le permite a los usuarios administrar y resolver problemas que puedan presentar tanto las aplicaciones como el clúster.</p><h3 id=monitor-de-recursos-de-contenedores>Monitor de recursos de contenedores</h3><p>El <a href=/docs/tasks/debug-application-cluster/resource-usage-monitoring/>Monitor de recursos de contenedores</a> almacena
de forma centralizada series de tiempo con métricas sobre los contenedores, y provee una interfaz para navegar estos
datos.</p><h3 id=registros-del-clúster>Registros del clúster</h3><p>El mecanismo de <a href=/docs/concepts/cluster-administration/logging/>registros del clúster</a> está a cargo de almacenar
los registros de los contenedores de forma centralizada, proporcionando una interfaz de búsqueda y navegación.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0c745f42e623d2b70a53bc0e6db73d95>1.3 - API de Kubernetes</h1><p>Las convenciones globales de la API se describen en el <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md>documento de convenciones de la API</a>.</p><p>Los endpoints, tipos de recursos y ejemplos se describen en la <a href=/es/docs/reference>Referencia</a> de la API.</p><p>El acceso remoto a la API se discute en el documento <a href=/docs/reference/access-authn-authz/controlling-access/>Controlando el acceso a la API</a>.</p><p>La API de Kubernetes sirve como base para el esquema de configuración declarativa del sistema. La herramienta de
línea de comandos <a href=/docs/reference/kubectl/overview/>kubectl</a> puede ser usada para crear, actualizar, eliminar y consultar objetos a través de la API.</p><p>Kubernetes también almacena el estado de los recursos de la API en forma serializada. Actualmente esto se hace en <a href=https://coreos.com/docs/distributed-configuration/getting-started-with-etcd/>etcd</a>.</p><p>Kubernetes está compuesto, en si mismo, por varios componentes que interactúan a través de su API.</p><h2 id=cambios-a-la-api>Cambios a la API</h2><p>En nuestra experiencia, cualquier sistema exitoso necesita crecer y evolucionar al cambiar o emerger nuevos casos de uso. Por lo tanto, esperamos que la API de Kubernetes cambie y crezca continuamente. Dicho esto, nuestro objetivo es no romper la compatibilidad con los clientes ya existentes, por un período de tiempo razonable. En general, podemos esperar que se agreguen nuevos recursos y propiedades con cierta frecuencia. Para eliminar un recurso o propiedad, se requiere seguir la <a href=/docs/reference/using-api/deprecation-policy/>política de obsolescencia de la API</a>.</p><p>En el documento de <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md>cambios a la API</a> describimos como cambiar la API y definimos lo que es considerado como un cambio compatible.</p><h2 id=definiciones-openapi-y-swagger>Definiciones OpenAPI y Swagger</h2><p>Los detalles completos de la API se documentan usando <a href=https://www.openapis.org/>OpenAPI</a>.</p><p>A partir de Kubernetes 1.10, el servidor de API de Kubernetes provee una especificación OpenAPI en el endpoint <code>/openapi/v2</code>.</p><p>Se puede solicitar un formato en particular utilizando las siguientes cabeceras HTTP:</p><table><thead><tr><th>Cabecera</th><th>Valores admitidos</th></tr></thead><tbody><tr><td>Accept</td><td><code>application/json</code>, <code>application/com.github.proto-openapi.spec.v2@v1.0+protobuf</code> (el content-type predeterminado es <code>application/json</code> si esta cabecera contiene <code>*/*</code> o se omite)</td></tr><tr><td>Accept-Encoding</td><td><code>gzip</code> (esta cabecera es opcional)</td></tr></tbody></table><p>Antes de 1.14, los endpoints separados por formato (<code>/swagger.json</code>, <code>/swagger-2.0.0.json</code>, <code>/swagger-2.0.0.pb-v1</code>, <code>/swagger-2.0.0.pb-v1.gz</code>)
servían la especificación OpenAPI en distintos formatos. Estos endpoints se consideran obsoletos y serán removidos en Kubernetes 1.14.</p><p><strong>Ejemplos</strong>:</p><table><thead><tr><th>Antes de 1.10</th><th>A partir de 1.10</th></tr></thead><tbody><tr><td>GET /swagger.json</td><td>GET /openapi/v2 <strong>Accept</strong>: application/json</td></tr><tr><td>GET /swagger-2.0.0.pb-v1</td><td>GET /openapi/v2 <strong>Accept</strong>: <a href=mailto:application/com.github.proto-openapi.spec.v2@v1.0>application/com.github.proto-openapi.spec.v2@v1.0</a>+protobuf</td></tr><tr><td>GET /swagger-2.0.0.pb-v1.gz</td><td>GET /openapi/v2 <strong>Accept</strong>: <a href=mailto:application/com.github.proto-openapi.spec.v2@v1.0>application/com.github.proto-openapi.spec.v2@v1.0</a>+protobuf <strong>Accept-Encoding</strong>: gzip</td></tr></tbody></table><p>Kubernetes implementa un formato alternativo de serialización basado en <a href=https://developers.google.com/protocol-buffers/>Protocol Buffer (<em>Protobuf</em>)</a> diseñado principalmente para las comunicaciones dentro del clúster. Este formato está documentado en su <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/protobuf.md>propuesta de diseño</a> y los archivos IDL de cada esquema se encuentran en los paquetes de Go que definen los objetos de la API.</p><p>Antes de 1.14, el <code>apiserver</code> de Kubernetes ofrecía una API para obtener la especificación <a href=http://swagger.io/>Swagger v1.2</a> de la API de Kubernetes en <code>/swaggerapi</code>. Este endpoint se considera obsoleto y será removido en Kubernetes 1.14.</p><h2 id=versionado-de-la-api>Versionado de la API</h2><p>Para facilitar la eliminación de propiedades o reestructurar la representación de un recurso, Kubernetes soporta múltiples versiones de la API en distintas rutas como <code>/api/v1</code> or <code>/apis/extensions/v1beta1</code>.</p><p>Se versiona a nivel de la API en vez de a nivel de los recursos o propiedades para asegurarnos de que la API presenta una visión clara y consistente de los recursos y el comportamiento del sistema, y para controlar el acceso a las APIs experimentales o que estén terminando su ciclo de vida. Los esquemas de serialización JSON y Protobuf siguen los mismos lineamientos para los cambios, es decir, estas descripciones cubren ambos formatos.</p><p>Se ha de tener en cuenta que hay una relación indirecta entre el versionado de la API y el versionado del resto del software. La propuesta de <a href=https://git.k8s.io/design-proposals-archive/release/versioning.md>versionado de la API y releases</a> describe esta relación.</p><p>Las distintas versiones de la API implican distintos niveles de estabilidad y soporte. El criterio para cada nivel se describe en detalle en la documentación de <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#alpha-beta-and-stable-versions>Cambios a la API</a>. A continuación se ofrece un resumen:</p><ul><li>Nivel "alpha":<ul><li>El nombre de la versión contiene <code>alpha</code> (p. ej., <code>v1alpha1</code>).</li><li>Puede tener muchos errores. Activar esta característica podría exponer errores. Desactivada por defecto.</li><li>El soporte para esta característica podría ser eliminado sin previo aviso.</li><li>La API podría volverse incompatible en el futuro y sin previo aviso.</li><li>Se recomienda su uso solo en clústers efímeros y de prueba ya que hay mayor riesgo de errores y carece de soporte a largo plazo</li></ul></li><li>Nivel "beta":<ul><li>El nombre de la versión contiene <code>beta</code> (p. ej., <code>v2beta3</code>).</li><li>El código ha sido probado. Activar esta característica es seguro. Activada por defecto.</li><li>Los detalles de esta característica podrían cambiar, pero se mantendrá el soporte.</li><li>El esquema y/o la semántica de un objeto podría volverse incompatible en el futuro. Si esto pasa, se ofrecerán instrucciones para migrar a una nueva versión. Esto podría requerir eliminar, editar o volver a crear objetos. El proceso de edición podría requerir planificación, incluyendo tiempo de inactividad para aplicaciones que usaban esta característica.</li><li>No se recomienda para aplicaciones críticas de negocio ya que podría volverse incompatible en futuras versiones. Si tiene múltiples clústeres que pueden actualizarse de forma independiente se podría decidir correr este riesgo.</li><li><strong>Por favor, ¡pruebe las características en fase beta y comparta sus comentarios! Una vez que salgan de la fase beta, sería más difícil hacer cambios.</strong></li></ul></li><li>Nivel estable:<ul><li>El nombre de la versión es <code>vX</code> donde <code>X</code> es un entero.</li><li>Las versiones estables de las características aparecerán en los siguientes releases.</li></ul></li></ul><h2 id=grupos-de-api>Grupos de API</h2><p>Para que sea más fácil extender la API de Kubernetes, se han creado los <a href=https://git.k8s.io/design-proposals-archive/api-machinery/api-group.md><em>grupos de API</em></a>.
Estos grupos se especifican en una ruta REST y en la propiedad <code>apiVersion</code> de un objeto serializado.</p><p>Actualmente hay varios grupos de API en uso:</p><ol><li><p>El grupo <em>core</em> (o <em>group</em>) en la ruta REST <code>/api/v1</code> y usa <code>apiVersion: v1</code>.</p></li><li><p>Los grupos con entidad propia están en la ruta REST <code>/apis/$NOMBRE_GRUPO/$VERSION</code> y usan <code>apiVersion: $NOMBRE_GRUPO/$VERSION</code>.
(p. ej., <code>apiVersion: batch/v1</code>). La lista completa de los grupos soportados está disponible en la <a href=/es/docs/reference/>Referencia de la API</a>.</p></li></ol><p>Hay dos rutas soportadas para extender la API con <a href=/docs/concepts/api-extension/custom-resources/>recursos personalizados</a>:</p><ol><li><a href=/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/>CustomResourceDefinition</a>
es para los usuarios que tengan necesidades CRUD muy básicas.</li><li>Los usuarios que necesiten la semántica completa de la API pueden implementar su propio <code>apiserver</code>
usando el <a href=/docs/tasks/access-kubernetes-api/configure-aggregation-layer/>agregador</a> para hacerlo
transparente para los clientes.</li></ol><h2 id=activar-los-grupos-de-api>Activar los grupos de API</h2><p>Ciertos recursos y grupos de API están activados por defecto. Pueden activarse o desactivarse con la opción <code>--runtime-config</code> en <code>apiserver</code>. <code>--runtime-config</code> acepta valores separados por coma. Por ejemplo, para desactivar <code>batch/v1</code>, use la opción
<code>--runtime-config=batch/v1=false</code>. Para activar <code>batch/v2alpha1</code>, pase la opción <code>--runtime-config=batch/v2alpha1</code>.
Esta opción acepta pares de <code>clave=valor</code> separados por coma que describen la configuración operativa del <code>apiserver</code>.</p><p><strong>IMPORTANTE:</strong> Activar o desactivar grupos o recursos requiere reiniciar el <code>apiserver</code> y el controller-manager para que estos reconozcan los cambios a <code>--runtime-config</code>.</p><h2 id=activar-recursos-en-los-grupos>Activar recursos en los grupos</h2><p>Los <code>DaemonSets</code>, <code>Deployments</code>, <code>HorizontalPodAutoscalers</code>, <code>Ingresses</code>, <code>Jobs</code> y <code>ReplicaSets</code> están activados por defecto.</p><p>Se pueden activar otros recursos con la opción <code>--runtime-config</code> del <code>apiserver</code>. Por ejemplo, como <code>--runtime-config</code> acepta valores separados por coma, puede desactivar los Deployments y los Ingress con la opción
<code>--runtime-config=extensions/v1beta1/deployments=false,extensions/v1beta1/ingresses=false</code></p></div><div class=td-content style=page-break-before:always><h1 id=pg-110f33530cf761140cb1dab536baef04>1.4 - Objetos de Kubernetes</h1></div><div class=td-content><h1 id=pg-9f5adfa77f48c50d5cc81155a3cecb98>1.4.1 - Entender los Objetos de Kubernetes</h1><p>Esta página explica cómo se representan los objetos de Kubernetes en la API de Kubernetes, y cómo puedes definirlos en formato <code>.yaml</code>.</p><h2 id=entender-los-objetos-de-kubernetes>Entender los Objetos de Kubernetes</h2><p>Los <em>Objetos de Kubernetes</em> son entidades persistentes dentro del sistema de Kubernetes. Kubernetes utiliza estas entidades para representar el estado de tu clúster. Específicamente, pueden describir:</p><ul><li>Qué aplicaciones corren en contenedores (y en qué nodos)</li><li>Los recursos disponibles para dichas aplicaciones</li><li>Las políticas acerca de cómo dichas aplicaciones se comportan, como las políticas de reinicio, actualización, y tolerancia a fallos</li></ul><p>Un objeto de Kubernetes es un "registro de intención" -- una vez que has creado el objeto, el sistema de Kubernetes se pondrá en marcha para asegurar que el objeto existe. Al crear un objeto, en realidad le estás diciendo al sistema de Kubernetes cómo quieres que sea la carga de trabajo de tu clúster; esto es, el <strong>estado deseado</strong> de tu clúster.</p><p>Para trabajar con los objetos de Kubernetes -- sea para crearlos, modificarlos, o borrarlos -- necesitarás usar la <a href=/docs/concepts/overview/kubernetes-api/>API de Kubernetes</a>. Cuando utilizas el interfaz de línea de comandos <code>kubectl</code>, por ejemplo, este realiza las llamadas necesarias a la API de Kubernetes en tu lugar. También puedes usar la API de Kubernetes directamente desde tus programas utilizando alguna de las <a href=/docs/reference/using-api/client-libraries/>Librerías de Cliente</a>.</p><h3 id=alcance-y-estado-de-un-objeto>Alcance y Estado de un Objeto</h3><p>Cada objeto de Kubernetes incluye dos campos como objetos anidados que determinan la configuración del objeto: el campo de objeto <em>spec</em> y el campo de objeto <em>status</em>. El campo <em>spec</em>, que es obligatorio, describe el <em>estado deseado</em> del objeto -- las características que quieres que tenga el objeto. El campo <em>status</em> describe el <em>estado actual</em> del objeto, y se suministra y actualiza directamente por el sistema de Kubernetes. En cualquier momento, el Plano de Control de Kubernetes gestiona de forma activa el estado actual del objeto para que coincida con el estado deseado requerido.</p><p>Por ejemplo, un Deployment de Kubernetes es un objeto que puede representar una aplicación de tu clúster. Cuando creas el Deployment, puedes especificar en el spec del Deployment que quieres correr tres réplicas de la aplicación. El sistema de Kubernetes lee el spec del Deployment y comienza a instanciar réplicas de tu aplicación -- actualizando el estado para conciliarlo con tu spec. Si cualquiera de las instancias falla (un cambio de estado), el sistema de Kubernetes soluciona la diferencia entre la spec y el estado llevando a cabo una correción -- en este caso, iniciando otra instancia de reemplazo.</p><p>Para obtener más información acerca de la spec, el status, y los metadatos de los objetos, echa un vistazo a las <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md>Normas de la API de Kubernetes</a>.</p><h3 id=describir-un-objeto-de-kubernetes>Describir un Objeto de Kubernetes</h3><p>Cuando creas un objeto en Kubernetes, debes especificar la spec del objeto que describe su estado deseado, así como información básica del mismo (como el nombre). Cuando usas la API de Kubernetes para crear el objeto (bien de forma directa o usando <code>kubectl</code>), dicha petición a la API debe incluir toda la información en formato JSON en el cuerpo de la petición. <strong>A menudo, le proporcionas la información a <code>kubectl</code> como un archivo .yaml.</strong> <code>kubectl</code> convierte esa información a JSON cuando realiza la llamada a la API.</p><p>Aquí hay un ejemplo de un archivo <code>.yaml</code> que muestra los campos requeridos y la spec del objeto Deployment de Kubernetes:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/application/deployment.yaml download=application/deployment.yaml><code>application/deployment.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("application-deployment-yaml")' title="Copy application/deployment.yaml to clipboard"></img></div><div class=includecode id=application-deployment-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb> </span><span style=color:#080;font-style:italic># Usa apps/v1beta2 para versiones anteriores a 1.9.0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># indica al controlador que ejecute 2 pods</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.7.9<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Una forma de crear un Deployment utilizando un archivo <code>.yaml</code> como el indicado arriba sería ejecutar el comando
<a href=/docs/reference/generated/kubectl/kubectl-commands#apply><code>kubectl apply</code></a>
en el interfaz de línea de comandos, pasándole el archivo <code>.yaml</code> como argumento. Aquí tienes un ejemplo de cómo hacerlo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/deployment.yaml --record
</span></span></code></pre></div><p>La salida del comando sería algo parecido a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/nginx-deployment created
</span></span></code></pre></div><h3 id=campos-requeridos>Campos requeridos</h3><p>En el archivo <code>.yaml</code> del objeto de Kubernetes que quieras crear, obligatoriamente tendrás que indicar los valores de los siguientes campos (como mínimo):</p><ul><li><code>apiVersion</code> - Qué versión de la API de Kubernetes estás usando para crear este objeto</li><li><code>kind</code> - Qué clase de objeto quieres crear</li><li><code>metadata</code> - Datos que permiten identificar unívocamente al objeto, incluyendo una cadena de texto para el <code>name</code>, UID, y opcionalmente el <code>namespace</code></li></ul><p>También deberás indicar el campo <code>spec</code> del objeto. El formato del campo <code>spec</code> es diferente según el tipo de objeto de Kubernetes, y contiene campos anidados específicos de cada objeto. La <a href=/docs/reference/generated/kubernetes-api/v1.25/>Referencia de la API de Kubernetes</a> puede servirte de ayuda para encontrar el formato de la spec para cada uno de los objetos que puedes crear usando Kubernetes.
Por ejemplo, el formato de la <code>spec</code> para un objeto de tipo <code>Pod</code> lo puedes encontrar
<a href=/docs/reference/generated/kubernetes-api/v1.25/#podspec-v1-core>aquí</a>,
y el formato de la <code>spec</code> para un objeto de tipo <code>Deployment</code> lo puedes encontrar
<a href=/docs/reference/generated/kubernetes-api/v1.25/#deploymentspec-v1-apps>aquí</a>.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Aprender más acerca de los objetos básicos más importantes de Kubernetes, como el <a href=/docs/concepts/workloads/pods/pod-overview/>Pod</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f37749a83c2916b63279ea60f3cfe53e>1.4.2 - Nombres</h1><p>Todos los objetos de la API REST de Kubernetes se identifica de forma inequívoca mediante un Nombre y un UID.</p><p>Para aquellos atributos provistos por el usuario que no son únicos, Kubernetes provee de <a href=/docs/user-guide/labels>etiquetas</a> y <a href=/docs/concepts/overview/working-with-objects/annotations/>anotaciones</a>.</p><p>Echa un vistazo al <a href=https://git.k8s.io/design-proposals-archive/architecture/identifiers.md>documento de diseño de identificadores</a> para información precisa acerca de las reglas sintácticas de los Nombres y UIDs.</p><h2 id=nombres>Nombres</h2><p>Una cadena de caracteres proporcionada por el cliente que identifica un objeto en la URL de un recurso, como por ejemplo, <code>/api/v1/pods/nombre-del-objeto</code>.</p><p>Los nombres de los objetos son únicos para cada tipo de objeto. Sin embargo, si se elimina el objeto, se puede crear un nuevo objeto con el mismo nombre.</p><p>Por regla general, los nombres de los recursos de Kubernetes no deben exceder la longitud máxima de 253 caracteres y deben incluir caracteres alfanuméricos en minúscula, <code>-</code>, y <code>.</code>; aunque algunos recursos tienen restricciones más específicas.</p><h2 id=uids>UIDs</h2><p>Una cadena de caracteres generada por Kubernetes para identificar objetos de forma única.</p><p>Cada objeto creado a lo largo de toda la vida de un clúster Kubernetes tiene un UID distinto. Está pensado para distinguir entre ocurrencias históricas de entidades similares.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1127165f472b7181b9c1d5a0b187d620>1.4.3 - Espacios de nombres</h1><p>Kubernetes soporta múltiples clústeres virtuales respaldados por el mismo clúster físico.
Estos clústeres virtuales se denominan espacios de nombres (namespaces).</p><h2 id=cuándo-usar-múltiple-espacios-de-nombre>Cuándo Usar Múltiple Espacios de Nombre</h2><p>Los espacios de nombres están pensados para utilizarse en entornos con muchos usuarios
distribuidos entre múltiples equipos, o proyectos. Para aquellos clústeres con
unas pocas decenas de usuarios, no deberías necesitar crear o pensar en espacios de
nombres en absoluto. Empieza a usarlos solamente si necesitas las características
que proporcionan.</p><p>Los espacios de nombres proporcionan un campo de acción para los nombres. Los nombres de los recursos
tienen que ser únicos dentro de cada espacio de nombres, pero no entre dichos espacios de nombres.</p><p>Los espacios de nombres son una forma de dividir los recursos del clúster
entre múltiples usuarios (via <a href=/docs/concepts/policy/resource-quotas/>cuotas de recursos</a>).</p><p>En futuras versiones de Kubernetes, los objetos de un mismo espacio de nombres
tendrán las mismas políticas de control de acceso por defecto.</p><p>No es necesario usar múltiples espacios de nombres sólo para separar recursos
ligeramente diferentes, como versiones diferentes de la misma aplicación: para ello
utiliza <a href=/docs/user-guide/labels>etiquetas</a> para distinguir tus recursos dentro
del mismo espacio de nombres.</p><h2 id=trabajar-con-espacios-de-nombres>Trabajar con Espacios de Nombres</h2><p>La creación y borrado de espacios de nombres se describe en la <a href=/docs/admin/namespaces>documentación de la Guía de Administración para espacios de nombres</a>.</p><h3 id=ver-espacios-de-nombre>Ver espacios de nombre</h3><p>Puedes listar los espacios de nombres actuales dentro de un clúster mediante:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get namespaces
</span></span></code></pre></div><pre tabindex=0><code>NAME          STATUS    AGE
default       Active    1d
kube-system   Active    1d
kube-public   Active    1d
</code></pre><p>Kubernetes arranca con tres espacios de nombres inicialmente:</p><ul><li><code>default</code> El espacio de nombres por defecto para aquellos objetos que no especifican ningún espacio de nombres</li><li><code>kube-system</code> El espacio de nombres para aquellos objetos creados por el sistema de Kubernetes</li><li><code>kube-public</code> Este espacio de nombres se crea de forma automática y es legible por todos los usuarios (incluyendo aquellos no autenticados).
Este espacio de nombres se reserva principalmente para uso interno del clúster, en caso de que algunos recursos necesiten ser visibles y legibles de forma pública para todo el clúster.
La naturaleza pública de este espacio de nombres es simplemente por convención, no es un requisito.</li></ul><h3 id=establecer-el-espacio-de-nombres-para-una-petición>Establecer el espacio de nombres para una petición</h3><p>Para indicar de forma temporal el espacio de nombres para una petición, usa la opción <code>--namespace</code>.</p><p>Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl --namespace<span style=color:#666>=</span>&lt;insert-namespace-name-here&gt; run nginx --image<span style=color:#666>=</span>nginx
</span></span><span style=display:flex><span>kubectl --namespace<span style=color:#666>=</span>&lt;insert-namespace-name-here&gt; get pods
</span></span></code></pre></div><h3 id=establecer-la-preferencia-de-espacio-de-nombres>Establecer la preferencia de espacio de nombres</h3><p>Puedes indicar de forma permanente el espacio de nombres para todas las llamadas futuras a comandos kubectl
en dicho contexto.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl config set-context --current --namespace<span style=color:#666>=</span>&lt;insert-namespace-name-here&gt;
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Validate it</span>
</span></span><span style=display:flex><span>kubectl config view | grep namespace:
</span></span></code></pre></div><h2 id=espacios-de-nombres-y-dns>Espacios de nombres y DNS</h2><p>Cuando creas un <a href=/docs/user-guide/services>Servicio</a>, se crea una <a href=/docs/concepts/services-networking/dns-pod-service/>entrada DNS</a> correspondiente.
Esta entrada tiene la forma <code>&lt;service-name>.&lt;namespace-name>.svc.cluster.local</code>, que significa
que si un contenedor simplemente usa <code>&lt;service-name></code>, se resolverá al servicio
que sea local al espacio de nombres. Esto es de utilidad para poder emplear la misma
configuración entre múltiples espacios de nombres como Development, Staging y Production.
Si quieres referenciar recursos entre distintos espacios de nombres, entonces
debes utilizar el nombre cualificado completo de dominio (FQDN).</p><h2 id=no-todos-los-objetos-están-en-un-espacio-de-nombres>No Todos los Objetos están en un Espacio de nombres</h2><p>La mayoría de los recursos de Kubernetes (ej. pods, services, replication controllers, y otros) están
en algunos espacios de nombres. Sin embargo, los recursos que representan a los propios
espacios de nombres no están a su vez en espacios de nombres.
De forma similar, los recursos de bajo nivel, como los <a href=/docs/admin/node>nodos</a> y
los volúmenes persistentes, no están en ningún espacio de nombres.</p><p>Para comprobar qué recursos de Kubernetes están y no están en un espacio de nombres:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># In a namespace</span>
</span></span><span style=display:flex><span>kubectl api-resources --namespaced<span style=color:#666>=</span><span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Not in a namespace</span>
</span></span><span style=display:flex><span>kubectl api-resources --namespaced<span style=color:#666>=</span><span style=color:#a2f>false</span>
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-f1dec4557fb8ffbac9f11390aaaf9fa4>1.4.4 - Etiquetas y Selectores</h1><p>Las <em>etiquetas</em> son pares de clave/valor que se asocian a los objetos, como los pods.
El propósito de las etiquetas es permitir identificar atributos de los objetos que son relevantes y significativos para los usuarios, pero que no tienen significado para el sistema principal.
Se puede usar las etiquetas para organizar y seleccionar subconjuntos de objetos. Las etiquetas se pueden asociar a los objetos a la hora de crearlos y posteriormente modificarlas o añadir nuevas.
Cada objeto puede tener un conjunto de etiquetas clave/valor definidas, donde cada clave debe ser única para un mismo objeto.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;metadata&#34;</span><span>:</span> {
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;key1&#34;</span> : <span style=color:#b44>&#34;value1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;key2&#34;</span> : <span style=color:#b44>&#34;value2&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Las etiquetas permiten consultar y monitorizar los objetos de forma más eficiente y son ideales para su uso en UIs y CLIs. El resto de información no identificada debe ser registrada usando <a href=/docs/concepts/overview/working-with-objects/annotations/>anotaciones</a>.</p><h2 id=motivación>Motivación</h2><p>Las etiquetas permiten que los usuarios mapeen sus estructuras organizacionales en los propios objetos sin acoplamiento, sin forzar a los clientes a almacenar estos mapeos.</p><p>Los despliegues de servicios y los procesos en lotes suelen requerir a menudo la gestión de entidades multi-dimensionales (ej., múltiples particiones o despliegues, múltiples entregas, múltiples capas, múltiples microservicios por capa). Tal gestión a menudo requiere de operaciones horizontales que rompen la encapsulación de representaciones estrictamente jerárquicas, especialmente jerarquías rígidas determinadas por la infraestructura en vez de por los usuarios.</p><p>Ejemplos de etiquetas:</p><ul><li><code>"release" : "stable"</code>, <code>"release" : "canary"</code></li><li><code>"environment" : "dev"</code>, <code>"environment" : "qa"</code>, <code>"environment" : "production"</code></li><li><code>"tier" : "frontend"</code>, <code>"tier" : "backend"</code>, <code>"tier" : "cache"</code></li><li><code>"partition" : "customerA"</code>, <code>"partition" : "customerB"</code></li><li><code>"track" : "daily"</code>, <code>"track" : "weekly"</code></li></ul><p>Estos son sólo algunos ejemplos de etiquetas de uso común; eres libre de establecer tus propias normas. Ten en cuenta que la clave de cada etiqueta debe ser única dentro de cada objeto.</p><h2 id=sintaxis-y-conjunto-de-caracteres>Sintaxis y conjunto de caracteres</h2><p>Las <em>etiquetas</em> son pares de clave/valor. Las claves válidas de etiqueta tienen dos partes: un prefijo opcional y un nombre, separados por una barra (<code>/</code>). La parte del nombre es obligatoria y debe ser menor o igual a 63 caracteres, empezando y terminando con un carácter alfanumérico (<code>[a-z0-9A-Z]</code>), con guiones (<code>-</code>), guiones bajos (<code>_</code>), puntos (<code>.</code>), y cualquier carácter alfanumérico en medio. El prefijo es opcional. Si se indica, este debe ser un subdominio DNS: una serie de etiquetas DNS separadas por puntos (<code>.</code>), no mayores de 253 caracteres en total, seguidas de una barra (<code>/</code>).</p><p>Si se omite el prefijo, la clave de la etiqueta se entiende que es privada para el usuario. Los componentes automatizados del sistema (ej. <code>kube-scheduler</code>, <code>kube-controller-manager</code>, <code>kube-apiserver</code>, <code>kubectl</code>, u otros de terceras partes) que añaden etiquetas a los objetos de usuario deben especificar obligatoriamente un prefijo.</p><p>Los prefijos <code>kubernetes.io/</code> y <code>k8s.io/</code> están reservados para el sistema de Kubernetes.</p><p>Los valores de etiqueta válidos deben tener como máximo 63 caracteres y empezar y terminar con un carácter alfanumérico (<code>[a-z0-9A-Z]</code>), con guiones (<code>-</code>), guiones bajos (<code>_</code>), puntos (<code>.</code>), y cualquier carácter alfanumérico en medio.</p><h2 id=selectores-de-etiquetas>Selectores de etiquetas</h2><p>Al contrario que los <a href=/docs/user-guide/identifiers>nombres y UIDs</a>, las etiquetas no garantizan la unicidad. En general, se espera que muchos objetos compartan la(s) misma(s) etiqueta(s).</p><p>A través del <em>selector de etiqueta</em>, el cliente/usuario puede identificar un conjunto de objetos. El selector de etiqueta es la primitiva principal de agrupación en Kubernetes.</p><p>La API actualmente soporta dos tipos de selectores: <em>basados en igualdad</em> y <em>basados en conjunto</em>.
Un selector de etiqueta puede componerse de múltiples <em>requisitos</em> separados por coma. En el caso de múltiples requisitos, todos ellos deben ser satisfechos de forma que las comas actúan como operadores <em>AND</em> (<code>&&</code>) lógicos.</p><p>La semántica de selectores vacíos o no espefificados es dependiente del contexto,
y los tipos de la API que utilizan los selectores deberían documentar su propia validación y significado.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Para algunos tipos de la API, como los ReplicaSets, los selectores de etiqueta de dos instancias no deben superponerse dentro del mismo espacio de nombres, ya que el controlador puede interpretarlos como un conflicto y no ser capaz de determinar cuántas réplicas debería haber finalmente.</div><h3 id=requisito-basado-en-igualdad>Requisito <em>basado en Igualdad</em></h3><p>Los requisitos basados en <em>Igualdad</em> o <em>Desigualdad</em> permiten filtrar por claves y valores de etiqueta. Los objetos coincidentes deben satisfacer todas y cada una de las etiquetas indicadas, aunque puedan tener otras etiquetas adicionalmente.
Se permiten tres clases de operadores <code>=</code>,<code>==</code>,<code>!=</code>. Los dos primeros representan la <em>igualdad</em> (y son simplemente sinónimos), mientras que el último representa la <em>desigualdad</em>. Por ejemplo:</p><pre tabindex=0><code>environment = production
tier != frontend
</code></pre><p>El primero selecciona todos los recursos cuya clave es igual a <code>environment</code> y su valor es igual a <code>production</code>.
El último selecciona todos los recursos cuya clave es igual a <code>tier</code> y su valor distinto de <code>frontend</code>, y todos los recursos que no tengan etiquetas con la clave <code>tier</code>.
Se podría filtrar los recursos de <code>production</code> que excluyan <code>frontend</code> usando comas: <code>environment=production,tier!=frontend</code></p><p>Un escenario de uso de requisitos basados en igualdad es aquel donde los Pods pueden especificar
los criterios de selección de nodo. Por ejemplo, el Pod de abajo selecciona aquellos nodos con
la etiqueta "<code>accelerator=nvidia-tesla-p100</code>".</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cuda-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cuda-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;k8s.gcr.io/cuda-vector-add:v0.1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>nvidia.com/gpu</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>accelerator</span>:<span style=color:#bbb> </span>nvidia-tesla-p100<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=requisito-basado-en-conjunto>Requisito <em>basado en Conjunto</em></h3><p>Los requisitos de etiqueta basados en <em>Conjuntos</em> permiten el filtro de claves en base a un conjunto de valores. Se puede utilizar tres tipos de operadores: <code>in</code>,<code>notin</code> y <code>exists</code> (sólo el identificador clave). Por ejemplo:</p><pre tabindex=0><code>environment in (production, qa)
tier notin (frontend, backend)
partition
!partition
</code></pre><p>El primer ejemplo selecciona todos los recursos cuya clave es igual a <code>environment</code> y su valor es igual a <code>production</code> o <code>qa</code>.
El segundo ejemplo selecciona todos los recursos cuya clave es igual a <code>tier</code> y sus valores son distintos de <code>frontend</code> y <code>backend</code>, y todos los recursos que no tengan etiquetas con la clave<code>tier</code>.
El tercer ejemplo selecciona todos los recursos que incluyan una etiqueta con la clave <code>partition</code>; sin comprobar los valores.
El cuarto ejemplo selecciona todos los recursos que no incluyan una etiqueta con la clave <code>partition</code>; sin comprobar los valores.
De forma similar, el separador de coma actúa como un operador <em>AND</em> . Así, el filtro de recursos con una clave igual a <code>partition</code> (sin importar el valor) y con un <code>environment</code> distinto de <code>qa</code> puede expresarse como <code>partition,environment notin (qa)</code>.
El selector <em>basado en conjunto</em> es una forma genérica de igualdad puesto que <code>environment=production</code> es equivalente a <code>environment in (production)</code>; y lo mismo aplica para <code>!=</code> y <code>notin</code>.</p><p>Los requisitos <em>basados en conjunto</em> pueden alternarse con aquellos <em>basados en igualdad</em>. Por ejemplo: <code>partition in (customerA, customerB),environment!=qa</code>.</p><h2 id=api>API</h2><h3 id=filtro-con-list-y-watch>Filtro con LIST y WATCH</h3><p>Las operaciones LIST y WATCH pueden especificar selectores de etiqueta para filtrar el conjunto de objetos devueltos usando un parámetro de consulta. Ambos requisitos están permitidos (se presentan aquí como aparecerían en la cadena URL de consulta):</p><ul><li>requisitios <em>basados en igualdad</em>: <code>?labelSelector=environment%3Dproduction,tier%3Dfrontend</code></li><li>requisitios <em>basados en conjunto</em>: <code>?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29</code></li></ul><p>Es posible utilizar ambos estilos de selección de etiquetas para mostrar u observar recursos con un cliente REST. Por ejemplo, para enfocarse en <code>apiserver</code> con <code>kubectl</code> usando el estilo <em>basado en igualdad</em> se puede ejecutar:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>environment</span><span style=color:#666>=</span>production,tier<span style=color:#666>=</span>frontend
</span></span></code></pre></div><p>o usando requisitos <em>basados en conjunto</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b44>&#39;environment in (production),tier in (frontend)&#39;</span>
</span></span></code></pre></div><p>Como ya se ha comentado, los requisitos <em>basados en conjunto</em> son más expresivos.  Por ejemplo, se puede implementar el operador <em>OR</em> sobre valores:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b44>&#39;environment in (production, qa)&#39;</span>
</span></span></code></pre></div><p>o restringir la coincidencia negativa mediante el operador <em>exists</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b44>&#39;environment,environment notin (frontend)&#39;</span>
</span></span></code></pre></div><h3 id=establecer-referencias-en-los-objetos-de-la-api>Establecer referencias en los objetos de la API</h3><p>Algunos objetos de Kubernetes, como los <a href=/docs/user-guide/services><code>services</code></a> y los <a href=/docs/user-guide/replication-controller><code>replicationcontrollers</code></a>, también hacen uso de los selectores de etiqueta para referirse a otros conjuntos de objetos, como los <a href=/docs/user-guide/pods>pods</a>.</p><h4 id=service-y-replicationcontroller>Service y ReplicationController</h4><p>El conjunto de pods que un <code>service</code> expone se define con un selector de etiqueta. De forma similar, el conjunto de pods que un <code>replicationcontroller</code> debería gestionar se define con un selector de etiqueta.</p><p>Los selectores de etiqueta para ambos objetos se definen en los archivos <code>json</code> o <code>yaml</code> usando mapas, y únicamente se permite los selectores <em>basados en igualad</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;selector&#34;</span><span>:</span> {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;component&#34;</span> : <span style=color:#b44>&#34;redis&#34;</span>,
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>ó</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span></code></pre></div><p>este selector (respectivamente en formato <code>json</code> o <code>yaml</code>) es equivalente a <code>component=redis</code> o <code>component in (redis)</code>.</p><h4 id=recursos-que-soportan-requisitos-basados-en-conjunto>Recursos que soportan requisitos basados en conjunto</h4><p>Algunos recursos más recientes, como el <a href=/docs/concepts/jobs/run-to-completion-finite-workloads/><code>Job</code></a>, el <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a>, el <a href=/docs/concepts/workloads/controllers/replicaset/><code>Replica Set</code></a>, y el <a href=/docs/concepts/workloads/controllers/daemonset/><code>Daemon Set</code></a>, sí permiten requisitos <em>basados en conjunto</em>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- {<span style=color:green;font-weight:700>key: tier, operator: In, values</span>:<span style=color:#bbb> </span>[cache]}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- {<span style=color:green;font-weight:700>key: environment, operator: NotIn, values</span>:<span style=color:#bbb> </span>[dev]}<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>matchLabels</code> es un mapa de pares <code>{key,value}</code>. Una única combinación de <code>{key,value}</code> en el mapa <code>matchLabels</code> es equivalente a un elemento en <code>matchExpressions</code> donde el campo <code>key</code> es "key", el <code>operator</code> es "In", y la matriz <code>values</code> contiene únicamente "value". <code>matchExpressions</code> es una lista de requisitos de selección de pod. Los operadores permitidos son In, NotIn, Exists, y DoesNotExist. El conjunto de valores no puede ser vacío en el caso particular de In y NotIn. Todos los requisitos, tanto de <code>matchLabels</code> como de <code>matchExpressions</code> se combinan entre sí con el operador AND -- todos ellos deben ser satisfechos.</p><h4 id=seleccionar-conjuntos-de-objetos>Seleccionar conjuntos de objetos</h4><p>Un caso de uso de selección basada en etiquetas es la posibilidad de limitar los nodos en los que un pod puede desplegarse.
Ver la documentación sobre <a href=/docs/concepts/configuration/assign-pod-node/>selección de nodo</a> para más información.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-93cd7a1d4e1623e2bf01afc49a5af69c>1.4.5 - Anotaciones</h1><p>Puedes usar las anotaciones de Kubernetes para adjuntar metadatos arbitrarios a los objetos, de tal forma que clientes como herramientas y librerías puedan obtener fácilmente dichos metadatos.</p><h2 id=adjuntar-metadatos-a-los-objetos>Adjuntar metadatos a los objetos</h2><p>Puedes usar las etiquetas o anotaciones para adjuntar metadatos a los objetos de Kubernetes.
Las etiquetas pueden utilizarse para seleccionar objetos y para encontrar colecciones de objetos que satisfacen ciertas condiciones.
Por el contrario, las anotaciones no se utilizan para identificar y seleccionar objetos.
Los metadatos de una anotación pueden ser pequeños o grandes, estructurados o no estructurados,
y pueden incluir caracteres no permitidos en las etiquetas.</p><p>Las anotaciones, al igual que las etiquetas, son mapas de clave/valor:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;metadata&#34;</span><span>:</span> {
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;annotations&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;key1&#34;</span> : <span style=color:#b44>&#34;value1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;key2&#34;</span> : <span style=color:#b44>&#34;value2&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Aquí se presentan algunos ejemplos de información que podría ser indicada como anotaciones:</p><ul><li><p>Campos gestionados por una capa de configuración declarativa.
Adjuntando dichos campos como anotaciones permitiría diferenciarlos de los
valores por defecto establecidos por clientes o servidores, además de los
campos auto-generados y los campos modificados por sistemas de auto-escalado.</p></li><li><p>Información acerca de la construcción, entrega, o imagen como marcas de fecha, IDs de entrega, rama de Git,
número de PR, funciones hash de imágenes, y direcciones de registro.</p></li><li><p>Referencias a los repositorios de trazas, monitorización, analíticas, o auditoría.</p></li><li><p>Información de librería de cliente o herramienta que puede usarse con fines de depuración de código:
por ejemplo, nombre, versión, e información de construcción.</p></li><li><p>Información de usuario o procedencia de herramienta/sistema, como las URLs de los
objetos provenientes de otros componentes del ecosistema.</p></li><li><p>Metadatos para una herramienta ligera de lanzamiento de aplicaciones: por ejemplo, configuración o puntos de control.</p></li><li><p>Número de teléfono o contacto de las personas a cargo, o entradas de directorio que
especifican dónde puede encontrarse dicha información, como la página web de un equipo de trabajo.</p></li><li><p>Directivas del usuario final a las implementaciones para modificar el comportamiento
o solicitar funcionalidades no estándar.</p></li></ul><p>En vez de usar anotaciones, podrías almacenar este tipo de información en una
base de datos externa o un directorio, pero eso complicaría enormemente la posibilidad
de crear librerías compartidas de cliente, así como herramientas para el
despliegue, gestión, introspección, y similares.</p><h2 id=sintaxis-y-conjunto-de-caracteres>Sintaxis y conjunto de caracteres</h2><p>Las <em>Anotaciones</em> son entradas clave/valor. Una clave válida para una anotación tiene dos partes: un prefijo opcional y un nombre, separados por una barra (<code>/</code>). La parte del nombre es obligatoria y debe tener 63 caracteres o menos, empezando y terminando con un carácter alfanumérico (<code>[a-z0-9A-Z]</code>) con guiones (<code>-</code>), guiones bajos (<code>_</code>), puntos (<code>.</code>) en medio. El prefijo es opcional. Si se indica,
el prefijo debe ser un subdominio DNS: una serie de etiquetas DNS separadas por puntos (<code>.</code>), no superior a 253 caracteres en total, seguida de una barra (<code>/</code>).</p><p>Si se omite el prefijo, la clave de la anotación se entiende que es privada para el usuario. Los componentes automatizados del sistema (e.g. <code>kube-scheduler</code>, <code>kube-controller-manager</code>, <code>kube-apiserver</code>, <code>kubectl</code>, u otros de terceros) que añaden anotaciones a los objetos de usuario deben, pues, especificar un prefijo.</p><p>Los prefijos <code>kubernetes.io/</code> y <code>k8s.io/</code> se reservan para el uso exclusivo de los componentes principales de Kubernetes.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><p>Aprende más acerca de las <a href=/docs/concepts/overview/working-with-objects/labels/>Etiquetas y Selectores</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-046c03090d47bc4b89b818dc645c3865>1.4.6 - Selectores de Campo</h1><p>Los <em>selectores de campo</em> te permiten <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects>seleccionar recursos de Kubernetes</a> basados en el valor de uno o más campos del recurso. Aquí se presentan varios ejemplos de consultas de selectores de campo:</p><ul><li><code>metadata.name=my-service</code></li><li><code>metadata.namespace!=default</code></li><li><code>status.phase=Pending</code></li></ul><p>Este comando <code>kubectl</code> selecciona todos los Pods para los cuales el valor del campo <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase><code>status.phase</code></a> es igual a <code>Running</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods --field-selector status.phase<span style=color:#666>=</span>Running
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Los selectores de campo son esencialmente <em>filtros</em> de recursos. Por defecto, no se aplica ningún selector/filtro, lo que significa que todos los tipos de recursos son seleccionados. Esto hace que las siguientes consultas con <code>kubectl</code> sean equivalentes:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span><span style=display:flex><span>kubectl get pods --field-selector <span style=color:#b44>&#34;&#34;</span>
</span></span></code></pre></div></div><h2 id=campos-soportados>Campos soportados</h2><p>Los selectores de campos soportados varían según el tipo de recursos de Kubernetes. Todos los tipos de recursos permiten los campos <code>metadata.name</code> y <code>metadata.namespace</code>. El uso de un selector de campo no soportado provoca un error. Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get ingress --field-selector foo.bar<span style=color:#666>=</span>baz
</span></span></code></pre></div><pre tabindex=0><code>Error from server (BadRequest): Unable to find &#34;ingresses&#34; that match label selector &#34;&#34;, field selector &#34;foo.bar=baz&#34;: &#34;foo.bar&#34; is not a known field selector: only &#34;metadata.name&#34;, &#34;metadata.namespace&#34;
</code></pre><h2 id=operadores-soportados>Operadores soportados</h2><p>Puedes usar los operadores <code>=</code>, <code>==</code>, y <code>!=</code> en los selectores de campo (<code>=</code> y <code>==</code> significan lo mismo). Este comando de <code>kubectl</code>, por ejemplo, selecciona todos los servicios de Kubernetes que no están en el espacio de nombres <code>default</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get services --field-selector metadata.namespace!<span style=color:#666>=</span>default
</span></span></code></pre></div><h2 id=selectores-anidados>Selectores anidados</h2><p>De la misma manera que con una <a href=/docs/concepts/overview/working-with-objects/labels>etiqueta</a> y otros selectores, los selectores de campo pueden anidarse como una lista de elementos separados por coma. Este comando de <code>kubectl</code> selecciona todos los Pods para los que el campo <code>status.phase</code> no es igual a <code>Running</code> y el campo <code>spec.restartPolicy</code> es igual a <code>Always</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods --field-selector<span style=color:#666>=</span>status.phase!<span style=color:#666>=</span>Running,spec.restartPolicy<span style=color:#666>=</span>Always
</span></span></code></pre></div><h2 id=múltiples-tipos-de-recursos>Múltiples tipos de recursos</h2><p>Puedes usar los selectores de campo entre múltiples tipos de recursos. Este comando de <code>kubectl</code> selecciona todos los Statefulsets y Services que no están en el espacio de nombres <code>default</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!<span style=color:#666>=</span>default
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-5dd62c6a4a481b4cf1ac50f6799eb581>1.4.7 - Etiquetas recomendadas</h1><p>Puedes visualizar y gestionar los objetos de Kubernetes con herramientas adicionales a kubectl
y el propio tablero de control. Un conjunto común de etiquetas permite a dichas herramientas
trabajar de forma interoperable, describiendo los objetos de una forma común que todas las
herramientas puedan entender.</p><p>Además del soporte a herramientas, las etiquetas recomendadas describen las aplicaciones
de forma que puedan ser consultadas.</p><p>Los metadatos se organizan en torno al concepto de una <em>aplicación</em>. Kubernetes no es
una plataforma como servicio (PaaS) y ni tiene o restringe la definición formal de una aplicación.
Al contrario, las aplicaciones son informales y se describen mediante el uso de los metadatos.
La definición de lo que contiene una aplicación es imprecisa.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Estas son las etiquetas recomendadas. Estas facilitan la gestión de aplicaciones,
pero no son obligatorias para las herramientas en general.</div><p>Las etiquetas compartidas y las anotaciones comparten un prefijo común: <code>app.kubernetes.io</code>.
Las etiquetas sin un prefijo son privadas para los usuarios. El prefijo compartido
garantiza que las etiquetas compartidas no entran en conflicto con las etiquetas
personalizadas de usuario.</p><h2 id=etiquetas>Etiquetas</h2><p>Para beneficiarse al máximo del uso de estas etiquetas, estas deberían aplicarse a cada objeto de recurso.</p><table><thead><tr><th>Clave</th><th>Descripción</th><th>Ejemplo</th><th>Tipo</th></tr></thead><tbody><tr><td><code>app.kubernetes.io/name</code></td><td>El nombre de la aplicación</td><td><code>mysql</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/instance</code></td><td>Un nombre único que identifique la instancia de la aplicación</td><td><code>wordpress-abcxzy</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/version</code></td><td>La versión actual de la aplicación (ej., la versión semántica, cadena hash de revisión, etc.)</td><td><code>5.7.21</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/component</code></td><td>El componente dentro de la arquitectura</td><td><code>database</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/part-of</code></td><td>El nombre de una aplicación de nivel superior de la cual es parte esta aplicación</td><td><code>wordpress</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/managed-by</code></td><td>La herramienta usada para gestionar la operativa de una aplicación</td><td><code>helm</code></td><td>string</td></tr></tbody></table><p>Para ilustrar estas etiquetas en acción, consideremos el siguiente objeto StatefulSet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>wordpress-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5.7.21&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>database<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=aplicaciones-e-instancias-de-aplicaciones>Aplicaciones e Instancias de Aplicaciones</h2><p>Una misma aplicación puede desplegarse una o más veces en un clúster de Kubernetes e,
incluso, el mismo espacio de nombres. Por ejemplo, wordpress puede instalarse más de una
vez de forma que sitios web diferentes sean instalaciones diferentes de wordpress.</p><p>El nombre de una aplicación y el nombre de la instancia se almacenan de forma separada.
Por ejemplo, WordPress tiene un <code>app.kubernetes.io/name</code> igual a <code>wordpress</code> mientras que
tiene un nombre de instancia, representado como <code>app.kubernetes.io/instance</code> con un valor de
<code>wordpress-abcxzy</code>. Esto permite identificar tanto a la aplicación como a sus instancias.
Cada instancia de una aplicación tiene su propio nombre único.</p><h2 id=ejemplos>Ejemplos</h2><p>Para ilustrar las diferentes formas en que se puede utilizar las etiquetas, los siguientes ejemplos presentan distintas complejidades.</p><h3 id=un-servicio-simple-sin-estado>Un Servicio Simple sin Estado</h3><p>Considera el caso de un servicio simple sin estado desplegado mediante el uso de un objeto <code>Deployment</code> y <code>Service</code>. Los dos siguientes extractos de código representan cómo usar las etiquetas de la forma más sencilla.</p><p>El objeto <code>Deployment</code> se utiliza para supervisar los pods que ejecutan la propia aplicación.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>myservice-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>El objeto <code>Service</code> se utiliza para exponer la aplicación.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>myservice-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=aplicación-web-con-una-base-de-datos>Aplicación Web con una Base de Datos</h3><p>Considera una aplicación un poco más complicada: una aplicación web (WordPress)
que utiliza una base de datos (MySQL), instalada utilizando Helm. Los siguientes extractos
de código ilustran la parte inicial de los objetos utilizados para desplegar esta aplicación.</p><p>El comienzo del objeto <code>Deployment</code> siguiente se utiliza para WordPress:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>wordpress-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4.9.4&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>El objeto <code>Service</code> se emplea para exponer WordPress:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>wordpress-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4.9.4&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>MySQL se expone como un objeto <code>StatefulSet</code> con metadatos tanto para sí mismo como para la aplicación global que lo contiene:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>wordpress-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>database<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5.7.21&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>El objeto <code>Service</code> se usa para exponer MySQL como parte de WordPress:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>wordpress-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>database<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5.7.21&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Con los objetos <code>StatefulSet</code> y <code>Service</code> de MySQL te darás cuenta que se incluye la información acerca de MySQL y Wordpress, la aplicación global.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-796ad76ff5c9b7f23bc99ef9a33fba72>1.5 - Gestión de objetos usando kubectl</h1></div><div class=td-content style=page-break-before:always><h1 id=pg-2bf36ccd6b3dbeafecf87c39761b07c7>2 - Arquitectura de Kubernetes</h1></div><div class=td-content><h1 id=pg-9ef2890698e773b6c0d24fd2c20146f5>2.1 - Nodos</h1><p>Un nodo es una máquina de trabajo en Kubernetes, previamente conocida como <code>minion</code>. Un nodo puede ser una máquina virtual o física, dependiendo del tipo de clúster. Cada nodo está gestionado por el componente máster y contiene los servicios necesarios para ejecutar <a href=/docs/concepts/workloads/pods/pod>pods</a>. Los servicios en un nodo incluyen el <a href=/docs/concepts/overview/components/#node-components>container runtime</a>, kubelet y el kube-proxy. Accede a la sección <a href=https://git.k8s.io/design-proposals-archive/architecture/architecture.md#the-kubernetes-node>The Kubernetes Node</a> en el documento de diseño de arquitectura para más detalle.</p><h2 id=estado-del-nodo>Estado del Nodo</h2><p>El estado de un nodo comprende la siguiente información:</p><ul><li><a href=#direcciones>Direcciones</a></li><li><a href=#estados>Estados</a></li><li><a href=#capacidad>Capacidad</a></li><li><a href=#informaci%C3%B3n>Información</a></li></ul><h3 id=direcciones>Direcciones</h3><p>El uso de estos campos varía dependiendo del proveedor de servicios en la nube y/o de la configuración en máquinas locales.</p><ul><li><code>HostName</code>: El nombre de la máquina huésped como aparece en el kernel del nodo. Puede ser reconfigurado a través del kubelet usando el parámetro <code>--hostname-override</code>.</li><li><code>ExternalIP</code>: La dirección IP del nodo que es accesible externamente (que está disponible desde fuera del clúster).</li><li><code>InternalIP</code>: La dirección IP del nodo que es accesible únicamente desde dentro del clúster.</li></ul><h3 id=estados>Estados</h3><p>El campo <code>conditions</code> describe el estado de todos los nodos en modo <code>Running</code>.</p><table><thead><tr><th>Estado</th><th>Descripción</th></tr></thead><tbody><tr><td><code>OutOfDisk</code></td><td><code>True</code> si no hay espacio suficiente en el nodo para añadir nuevos pods; sino <code>False</code></td></tr><tr><td><code>Ready</code></td><td><code>True</code> si el nodo está en buen estado y preparado para aceptar nuevos pods, <code>Falso</code> si no puede aceptar nuevos pods, y <code>Unknown</code> si el controlador aún no tiene constancia del nodo después del último <code>node-monitor-grace-period</code> (por defecto cada 40 segundos)</td></tr><tr><td><code>MemoryPressure</code></td><td><code>True</code> si hay presión en la memoria del nodo -- es decir, si el consumo de memoria en el nodo es elevado; sino <code>False</code></td></tr><tr><td><code>PIDPressure</code></td><td><code>True</code> si el número de PIDs consumidos en el nodo es alto -- es decir, si hay demasiados procesos en el nodo; sino <code>False</code></td></tr><tr><td><code>DiskPressure</code></td><td><code>True</code> si hay presión en el tamaño del disco -- esto es, si la capacidad del disco es baja; sino <code>False</code></td></tr><tr><td><code>NetworkUnavailable</code></td><td><code>True</code> si la configuración de red del nodo no es correcta; sino <code>False</code></td></tr></tbody></table><p>El estado del nodo se representa como un objeto JSON. Por ejemplo, la siguiente respuesta describe un nodo en buen estado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;conditions&#34;</span><span>:</span> [
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;Ready&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;True&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>Si el <code>status</code> de la condición <code>Ready</code> se mantiene como <code>Unknown</code> o <code>False</code> por más tiempo de lo que dura un <code>pod-eviction-timeout</code>, se pasa un argumento al <a href=/docs/admin/kube-controller-manager/>kube-controller-manager</a> y todos los pods en el nodo se marcan para borrado por el controlador de nodos. El tiempo de desalojo por defecto es de <strong>cinco minutos</strong>. En algunos casos, cuando el nodo se encuentra inaccesible, el API Server no puede comunicar con el kubelet del nodo. La decisión de borrar pods no se le puede hacer llegar al kubelet hasta que la comunicación con el API Server se ha restablecido. Mientras tanto, los pods marcados para borrado pueden continuar ejecutándose en el nodo aislado.</p><p>En versiones de Kubernetes anteriores a 1.5, el controlador de nodos <a href=/docs/concepts/workloads/pods/pod/#force-deletion-of-pods>forzaba el borrado</a> de dichos pods inaccesibles desde el API Server. Sin embargo, desde la versión 1.5, el nodo controlador no fuerza el borrado de pods hasta que se confirma que dichos pods han dejado de ejecutarse en el clúster. Pods que podrían estar ejecutándose en un nodo inalcanzable se muestran como <code>Terminating</code> o <code>Unknown</code>. En aquellos casos en los que Kubernetes no puede deducir si un nodo ha abandonado el clúster de forma permanente, puede que sea el administrador el que tenga que borrar el nodo de forma manual. Borrar un objeto <code>Node</code> en un clúster de Kubernetes provoca que los objetos Pod que se ejecutaban en el nodo sean eliminados en el API Server y libera sus nombres.</p><p>En la versión 1.12, la funcionalidad <code>TaintNodesByCondition</code> se eleva a beta, de forma que el controlador del ciclo de vida de nodos crea <a href=/docs/concepts/configuration/taint-and-toleration/>taints</a> de forma automática, que representan estados de nodos.
De forma similar, el planificador de tareas ignora estados cuando evalúa un nodo; en su lugar mira los taints del nodo y las tolerancias de los pods.</p><p>En la actualidad, los usuarios pueden elegir entre la versión de planificación antigua y el nuevo, más flexible, modelo de planificación.
Un pod que no tiene definida ninguna tolerancia es planificado utilizando el modelo antiguo, pero si un nodo tiene definidas ciertas tolerancias, sólo puede ser asignado a un nodo que lo permita.</p><div class="alert alert-warning caution callout" role=alert><strong>Precaución:</strong> Habilitar esta funcionalidad crea una pequeña demora entre que una condición es evaluada y un taint creado. Esta demora suele ser inferior a un segundo, pero puede incrementar el número de pods que se planifican con éxito pero que luego son rechazados por el kubelet.</div><h3 id=capacidad>Capacidad</h3><p>Describe los recursos disponibles en el nodo: CPU, memoria, y el número máximo de pods que pueden ser planificados dentro del nodo.</p><h3 id=información>Información</h3><p>Información general sobre el nodo: versión del kernel, versión de Kubernetes (versiones del kubelet y del kube-proxy), versión de Docker (si se utiliza), nombre del sistema operativo. Toda esta información es recogida por el kubelet en el nodo.</p><h2 id=gestión>Gestión</h2><p>A diferencia de <a href=/docs/concepts/workloads/pods/pod/>pods</a> y <a href=/docs/concepts/services-networking/service/>services</a>, los nodos no son creados por Kubernetes de forma inherente; o son creados de manera externa por los proveedores de servicios en la nube como Google Compute Engine, o existen en la colección de máquinas virtuales o físicas. De manera que cuando Kubernetes crea un nodo, crea un objeto que representa el nodo. Después de ser creado, Kubernetes comprueba si el nodo es válido o no. Por ejemplo, si intentas crear un nodo con el siguiente detalle:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Node&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;10.240.79.157&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;my-first-k8s-node&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Kubernetes crea un objeto <code>Node</code> internamente (la representación), y valida el nodo comprobando su salud en el campo <code>metadata.name</code>. Si el nodo es válido -- es decir, si todos los servicios necesarios están ejecutándose -- el nodo es elegible para correr un pod. Sino, es ignorado para cualquier actividad del clúster hasta que se convierte en un nodo válido.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Kubernetes conserva el objeto de un nodo inválido y continúa probando por si el nodo, en algún momento, entrase en servicio.
Para romper este ciclo deberás borrar el objeto <code>Node</code> explícitamente.</div><p>Actualmente, hay tres componentes que interactúan con la interfaz de nodos de Kubernetes: controlador de nodos, kubelet y kubectl.</p><h3 id=controlador-de-nodos>Controlador de Nodos</h3><p>El controlador de nodos es un componente maestro en Kubernetes que gestiona diferentes aspectos de los nodos.</p><p>El controlador juega múltiples papeles en la vida de un nodo. El primero es asignar un bloque CIDR (Class Inter-Domain Routing) al nodo cuando este se registra (si la asignación CIDR está activada) que contendrá las IPs disponibles para asignar a los objetos que se ejecutarán en ese nodo.</p><p>El segundo es mantener actualizada la lista interna del controlador con la lista de máquinas disponibles a través del proveedor de servicios en la nube. Cuando Kubernetes se ejecuta en la nube, si un nodo deja de responder, el controlador del nodo preguntará al proveedor si la máquina virtual de dicho nodo continúa estando disponible. Si no lo está, el controlador borrará dicho nodo de su lista interna.</p><p>El tercero es el de monitorizar la salud de los nodos. El controlador de nodos es el responsable de actualizar la condición <code>NodeReady</code> del campo <code>NodeStatus</code> a <code>ConditionUnknown</code> cuando un nodo deja de estar accesible (por ejemplo, si deja de recibir señales de vida del nodo indicando que está disponible, conocidas como latidos o <code>hearbeats</code> en inglés) y, también es responsable de posteriormente desalojar todos los pods del nodo si este continúa estando inalcanzable. Por defecto, cuando un nodo deja de responder, el controlador sigue reintentando contactar con el nodo durante 40 segundos antes de marcar el nodo con <code>ConditionUnknown</code> y, si el nodo no se recupera de ese estado pasados 5 minutos, empezará a drenar los pods del nodo para desplegarlos en otro nodo que esté disponible. El controlador comprueba el estado de cada nodo cada <code>--node-monitor-period</code> segundos.</p><p>En versiones de Kubernetes previas a 1.13, <code>NodeStatus</code> es el <code>heartbeat</code> del nodo. Empezando con 1.13 la funcionalidad de <code>node lease</code> se introduce como alfa (<code>NodeLease</code>,
<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/0009-node-heartbeat.md>KEP-0009</a>). Cuando la funcionalidad está habilitada, cada nodo tiene un objeto <code>Lease</code> asociado en el namespace <code>kube-node-lease</code> que se renueva periódicamente y ambos, el <code>NodeStatus</code> y el <code>Lease</code> son considerados como <code>hearbeats</code> del nodo. <code>Node leases</code> se renuevan con frecuencia, mientras que <code>NodeStatus</code> se transmite desde el nodo al máster únicamente si hay cambios o si ha pasado cierto tiempo (por defecto, 1 minuto, que es más que la cuenta atrás por defecto de 40 segundos que marca un nodo como inalcanzable). Al ser los <code>node lease</code> más ligeros que <code>NodeStatus</code>, los <code>hearbeats</code> resultan más económicos desde las perspectivas de escalabilidad y de rendimiento.</p><p>En Kubernetes 1.4, se actualizó la lógica del controlador de nodos para gestionar mejor los casos en los que un gran número de nodos tiene problemas alcanzando el nodo máster (Por ejemplo, cuando el nodo máster es el que tiene un problema de red). Desde 1.4, el controlador de nodos observa el estado de todos los nodos en el clúster cuando toma decisiones sobre desalojo de pods.</p><p>En la mayoría de los casos, el controlador de nodos limita el ritmo de desalojo <code>--node-eviction-rate</code> (0.1 por defecto) por segundo, lo que significa que no desalojará pods de más de un nodo cada diez segundos.</p><p>El comportamiento de desalojo de nodos cambia cuando un nodo en una zona de disponibilidad tiene problemas. El controlador de nodos comprobará qué porcentaje de nodos en la zona no se encuentran en buen estado (es decir, que su condición <code>NodeReady</code> tiene un valor <code>ConditionUnknown</code> o <code>ConditionFalse</code>) al mismo tiempo. Si la fracción de nodos con problemas es de al menos <code>--unhealthy-zone-threshold</code> (0.55 por defecto) entonces se reduce el ratio de desalojos: si el clúster es pequeño (por ejemplo, tiene menos o los mismos nodos que <code>--large-cluster-size-threshold</code> - 50 por defecto) entonces los desalojos se paran. Sino, el ratio se reduce a <code>--secondary-node-eviction-rate</code> (0.01 por defecto) por segundo. La razón por la que estas políticas se implementan por zonas de disponibilidad es debido a que una zona puede quedarse aislada del nodo máster mientras que las demás continúan conectadas. Si un clúster no comprende más de una zona, todo el clúster se considera una única zona.</p><p>La razón principal por la que se distribuyen nodos entre varias zonas de disponibilidad es para que el volumen de trabajo se transfiera a aquellas zonas que se encuentren en buen estado cuando una de las zonas se caiga.
Por consiguiente, si todos los nodos de una zona se encuentran en mal estado, el nodo controlador desaloja al ritmo normal <code>--node-eviction-rate</code>. En el caso extremo de que todas las zonas se encuentran en mal estado (es decir, no responda ningún nodo del clúster), el controlador de nodos asume que hay algún tipo de problema con la conectividad del nodo máster y paraliza todos los desalojos hasta que se restablezca la conectividad.</p><p>Desde la versión 1.6 de Kubernetes el controlador de nodos también es el responsable de desalojar pods que están ejecutándose en nodos con <code>NoExecute</code> taints, cuando los pods no permiten dichos taints. De forma adicional, como una funcionalidad alfa que permanece deshabilitada por defecto, el <code>NodeController</code> es responsable de añadir taints que se corresponden con problemas en los nodos del tipo nodo inalcanzable o nodo no preparado. En <a href=/docs/concepts/configuration/taint-and-toleration/>esta sección de la documentación</a> hay más detalles acerca de los taints <code>NoExecute</code> y de la funcionalidad alfa.</p><p>Desde la versión 1.8, el controlador de nodos puede ser responsable de la creación de taints que representan condiciones de nodos. Esta es una funcionalidad alfa en 1.8.</p><h3 id=auto-registro-de-nodos>Auto-Registro de Nodos</h3><p>Cuando el atributo del kubelet <code>--register-node</code> está habilitado (el valor por defecto), el kubelet intentará auto-registrarse con el API Server. Este es el patrón de diseño preferido, y utilizado por la mayoría de distribuciones.</p><p>Para auto-registro, el kubelet se inicia con las siguientes opciones:</p><ul><li><code>--kubeconfig</code> - La ruta a las credenciales para autentificarse con el API Server.</li><li><code>--cloud-provider</code> - Cómo comunicarse con un proveedor de servicios para leer meta-datos sobre si mismo.</li><li><code>--register-node</code> - Registro automático con el API Server.</li><li><code>--register-with-taints</code> - Registro del nodo con la lista de taints proporcionada (separada por comas <code>&lt;key>=&lt;value>:&lt;effect></code>). Esta opción se ignora si el atributo<code>--register-node</code> no está habilitado.</li><li><code>--node-ip</code> - La dirección IP del nodo.</li><li><code>--node-labels</code> - Etiquetas para añadir al nodo durante el registro en el clúster (ver las restricciones que impone el <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction admission plugin</a> en 1.13+).</li><li><code>--node-status-update-frequency</code> - Especifica la frecuencia con la que el nodo envía información de estado al máster.</li></ul><p>Cuando el <a href=/docs/reference/access-authn-authz/node/>Node authorization mode</a> y el <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction admission plugin</a> están habilitados, los kubelets sólo tienen permisos para crear/modificar su propio objeto <code>Node</code>.</p><h3 id=administración-manual-de-nodos>Administración Manual de Nodos</h3><p>Los administradores del clúster pueden crear y modificar objetos <code>Node</code>.</p><p>Si un administrador desea crear objetos <code>Node</code> de forma manual, debe levantar kubelet con el atributo <code>--register-node=false</code>.</p><p>Los administradores del clúster pueden modificar recursos <code>Node</code> (independientemente del valor de <code>--register-node</code>). Dichas modificaciones incluyen crear etiquetas en el nodo y/o marcarlo como no-planificable (de forma que pods no pueden ser planificados para instalación en el nodo).</p><p>Etiquetas y selectores de nodos pueden utilizarse de forma conjunta para controlar las tareas de planificación, por ejemplo, para determinar un subconjunto de nodos elegibles para ejecutar un pod.</p><p>Marcar un nodo como no-planificable impide que nuevos pods sean planificados en dicho nodo, pero no afecta a ninguno de los pods que existían previamente en el nodo. Esto resulta de utilidad como paso preparatorio antes de reiniciar un nodo, etc. Por ejemplo, para marcar un nodo como no-planificable, se ejecuta el siguiente comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl cordon <span style=color:#b8860b>$NODENAME</span>
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los pods creados por un controlador DaemonSet ignoran el planificador de Kubernetes y no respetan el atributo no-planificable de un nodo. Se asume que los daemons pertenecen a la máquina huésped y que se ejecutan incluso cuando esta está siendo drenada de aplicaciones en preparación de un reinicio.</div><h3 id=capacidad-del-nodo>Capacidad del Nodo</h3><p>La capacidad del nodo (número de CPUs y cantidad de memoria) es parte del objeto <code>Node</code>.
Normalmente, nodos se registran a sí mismos y declaran sus capacidades cuando el objeto <code>Node</code> es creado. Si se está haciendo <a href=#administraci%C3%B3n-manual-de-nodos>administración manual</a>, las capacidades deben configurarse en el momento de añadir el nodo.</p><p>El planificador de Kubernetes asegura la existencia de recursos suficientes para todos los pods que se ejecutan en un nodo. Comprueba que la suma recursos solicitados por los pods no exceda la capacidad del nodo. Incluye todos los pods iniciados por el kubelet, pero no tiene control sobre contenedores iniciados directamente por el <a href=/docs/concepts/overview/components/#node-components>runtime de contenedores</a> ni sobre otros procesos que corren fuera de contenedores.</p><p>Para reservar explícitamente recursos en la máquina huésped para procesos no relacionados con pods, sigue este tutorial <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved>reserva de recursos para daemons de sistema</a>.</p><h2 id=objeto-api>Objeto API</h2><p>Un nodo es un recurso principal dentro de la REST API de Kubernetes. Más detalles sobre el objeto en la API se puede encontrar en: <a href=/docs/reference/generated/kubernetes-api/v1.25/#node-v1-core>Object Node API</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-63e7fdf87ba61eb2586bb8c625c23506>2.2 - Comunicación Nodo-Maestro</h1><p>Este documento cataloga las diferentes vías de comunicación entre el nodo máster (en realidad el apiserver) y el clúster de Kubernetes. La intención es permitir a los usuarios personalizar sus instalaciones para proteger sus configuraciones de red de forma que el clúster pueda ejecutarse en una red insegura. (o en un proveedor de servicios en la nube con direcciones IP públicas)</p><h3 id=clúster-a-máster>Clúster a Máster</h3><p>Todos los canales de comunicación desde el clúster hacia el máster terminan en el apiserver (ningún otro componente del máster está diseñado para exponer servicios remotos). En un despliegue típico, el apiserver está configurado para escuchar conexiones remotas en un canal seguro cómo HTTPS en el puerto (443) con una o más formas de <a href=/docs/reference/access-authn-authz/authentication/>autenticación de clientes</a> habilitada. Una o más formas de <a href=/docs/reference/access-authn-authz/authorization/>autorización</a> deberían ser habilitadas, especialmente si se permiten <a href=/docs/reference/access-authn-authz/authentication/#anonymous-requests>peticiones anónimas</a>
o <a href=/docs/reference/access-authn-authz/authentication/#service-account-tokens>tokens de cuenta de servicio</a>.</p><p>Los nodos deben ser aprovisionados con el certificado raíz público del clúster de forma que puedan conectar de forma segura con el apiserver en conjunción con credenciales de cliente válidas. Por ejemplo, en un despliegue por defecto en GKE, las credenciales de cliente proporcionadas al kubelet están en la forma de certificados de cliente. Accede <a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>kubelet TLS bootstrapping</a> para ver cómo aprovisionar certificados de cliente de forma automática.</p><p>Aquellos Pods que deseen conectar con el apiserver pueden hacerlo de forma segura a través de una cuenta de servicio, de esta forma Kubernetes inserta de forma automática el certificado raíz público y un bearer token válido en el pod cuando este es instanciado. El servicio <code>kubernetes</code> (en todos los namespaces) se configura con una dirección IP virtual que es redireccionada (via kube-proxy) al punto de acceso HTTPS en el apiserver.</p><p>Los componentes máster también se comunican con el apiserver del clúster a través de un puerto seguro.</p><p>Como resultado, el modo de operación por defecto para conexiones desde el clúster (nodos y pods ejecutándose en nodos) al máster es seguro por defecto y puede correr en redes públicas y/o inseguras.</p><h2 id=máster-a-clúster>Máster a Clúster</h2><p>Hay dos vías de comunicación primaria desde el máster (apiserver) al clúster. La primera es desde el apiserver al proceso kubelet que se ejecuta en cada nodo del clúster. La segunda es desde el apiserver a cualquier nodo, pod, o servicio a través de la funcionalidad proxy del apiserver.</p><h3 id=apiserver-al-kubelet>apiserver al kubelet</h3><p>Las conexiones del apiserver al kubelet se utilizan para:</p><ul><li>Recoger entradas de registro de pods.</li><li>Conectar (a través de <code>kubectl</code>) con pods en ejecución.</li><li>Facilitar la funcionalidad <code>port-forwarding</code> del kubelet.</li></ul><p>Estas conexiones terminan en el endpoint HTTPS del kubelet. Por defecto, el apiserver no verifica el certificado del kubelet, por lo que la conexión es vulnerable a ataques del tipo <code>man-in-the-middle</code>, e <strong>insegura</strong> para conectar a través de redes públicas o de no confianza.</p><p>Para verificar esta conexión, se utiliza el atributo <code>--kubeket-certificate-authority</code> que provee el apiserver con un certificado raíz con el que verificar el certificado del kubelet.
Si esto no es posible, se utiliza un <a href=/docs/concepts/architecture/master-node-communication/#ssh-tunnels>túnel SSH</a> entre el apiserver y el kubelet para conectar a través de redes públicas o de no confianza.</p><p>Finalmente, <a href=/docs/admin/kubelet-authentication-authorization/>autenticación y/o autorización al kubelet</a> debe ser habilitada para proteger su API.</p><h3 id=apiserver-a-nodos-pods-y-servicios>apiserver a nodos, pods y servicios</h3><p>Las conexiones desde el apiserver a un nodo, pod o servicio se realizan por defecto con HTTP y, por consiguiente, no son autentificadas o encriptadas. Pueden ser ejecutadas en una conexión HTTPS segura añadiendo el prefijo <code>https:</code> al nodo, pod o nombre de servicio en la API URL, pero los receptores no validan el certificado provisto por el endpoint HTTPS ni facilitan credenciales de cliente asi que, aunque la conexión esté encriptada, esta no ofrece garantía de integridad. Estas conexiones <strong>no son seguras</strong> para conectar a través de redes públicas o inseguras.</p><h3 id=túneles-ssh>Túneles SSH</h3><p>Kubernetes ofrece soporte para túneles SSH que protegen la comunicación Máster -> Clúster. En este modo de configuración, el apiserver inicia un túnel SSH a cada nodo en el clúster (conectando al servidor SSH en el puerto 22) y transfiere todo el tráfico destinado a un kubelet, nodo, pod o servicio a través del túnel. El túnel garantiza que dicho tráfico no es expuesto fuera de la red en la que se ejecutan los nodos.</p><p>Los túneles SSH se consideran obsoletos, y no deberían utilizarse a menos que se sepa lo que se está haciendo. Se está diseñando un reemplazo para este canal de comunicación.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bc804b02614d67025b4c788f1ca87fbc>2.3 - Conceptos subyacentes del Cloud Controller Manager</h1><p>El concepto del Cloud Controller Manager (CCM) (no confundir con el ejecutable) fue creado originalmente para permitir que Kubernetes y el código específico de proveedores de servicios en la nube evolucionen de forma independiente. El Cloud Controller Manager se ejecuta a la par con otros componentes maestros como el Kubernetes Controller Manager, el API Server y el planificador. También puede ejecutarse como un extra, en cuyo caso se ejecuta por encima de Kubernetes.</p><p>El diseño del Cloud Controller Manager está basado en un sistema de plugins, lo que permite a nuevos proveedores de servicios integrarse de forma fácil con Kubernetes. Se está trabajando en implementar nuevos proveedores de servicios y para migrar los existentes del antiguo modelo al nuevo CCM.</p><p>Este documento describe los conceptos tras el Cloud Controller Manager y detalla sus funciones asociadas.</p><p>En la siguiente imagen, se puede visualizar la arquitectura de un cluster de Kubernetes que no utiliza el Cloud Controller Manager:</p><p><img src=/images/docs/pre-ccm-arch.png alt="Arquitectura previa a CCM"></p><h2 id=diseño>Diseño</h2><p>En el diagrama anterior, Kubernetes y el proveedor de servicios en la nube están integrados a través de diferentes componentes:</p><ul><li>Kubelet</li><li>Kubernetes controller manager</li><li>Kubernetes API server</li></ul><p>El CCM consolida toda la lógica dependiente de la nube de estos tres componentes para crear un punto de integración único. La nueva arquitectura con CCM se muestra a continuación:</p><p><img src=/images/docs/post-ccm-arch.png alt="Arquitectura CCM"></p><h2 id=componentes-del-ccm>Componentes del CCM</h2><p>El CCM secciona parte de la funcionalidad del Kubernetes Controller Manager (KCM) y la ejecuta como procesos independientes. Específicamente, aquellos controladores en el KCM que son dependientes de la nube:</p><ul><li>Controlador de Nodos</li><li>Controlador de Volúmenes</li><li>Controlador de Rutas</li><li>Controlador de Servicios</li></ul><p>En la versión 1.9, el CCM se encarga de la ejecución de los siguientes controladores:</p><ul><li>Controlador de Nodos</li><li>Controlador de Rutas</li><li>Controlador de Servicios</li></ul><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El controlador de volúmenes se dejó fuera del CCM de forma explícita. Debido a la complejidad que ello requería y a los esfuerzos existentes para abstraer lógica de volúmenes específica a proveedores de servicios, se decidió que el controlador de volúmenes no fuese movido al CCM.</div><p>El plan original para habilitar volúmenes en CCM era utilizar volúmenes Flex con soporte para volúmenes intercambiables. Sin embargo, otro programa conocido como CSI (Container Storage Interface) se está planeando para reemplazar Flex.</p><p>Considerando todo lo anterior, se ha decidido esperar hasta que CSI esté listo.</p><h2 id=funciones-del-ccm>Funciones del CCM</h2><p>El CCM hereda sus funciones de componentes que son dependientes de un proveedor de servicios en la nube. Esta sección se ha estructurado basado en dichos componentes:</p><h3 id=1-kubernetes-controller-manager>1. Kubernetes Controller Manager</h3><p>La mayoría de las funciones del CCM derivan del KCM. Como se ha mencionado en la sección anterior, el CCM es responsable de los siguientes circuitos de control:</p><ul><li>Controlador de Nodos</li><li>Controlador de Rutas</li><li>Controlador de Servicios</li></ul><h4 id=controlador-de-nodos>Controlador de Nodos</h4><p>El controlador de nodos es responsable de inicializar un nodo obteniendo información del proveedor de servicios sobre los nodos ejecutándose en el clúster. El controlador de nodos lleva a cabo las siguientes funciones:</p><ol><li>Inicializa un nodo con etiquetas de región y zona específicas del proveedor.</li><li>Inicializa un nodo con detalles de la instancia específicos del proveedor, como por ejemplo, el tipo o el tamaño.</li><li>Obtiene las direcciones de red del nodo y su hostname.</li><li>En caso de que el nodo deje de responder, comprueba la nube para ver si el nodo ha sido borrado. Si lo ha sido, borra el objeto nodo en Kubernetes.</li></ol><h4 id=controlador-de-rutas>Controlador de Rutas</h4><p>El controlador de Rutas es responsable de configurar rutas en la nube para que contenedores en diferentes nodos dentro de un clúster kubernetes se puedan comunicar entre sí.</p><h4 id=controlador-de-servicios>Controlador de Servicios</h4><p>El controlador de servicios es responsable de monitorizar eventos de creación, actualización y borrado de servicios. Basándose en el estado actual de los servicios en el clúster Kubernetes, configura balanceadores de carga del proveedor (como Amazon ELB, Google LB, or Oracle Cloud Infrastructure Lb) de forma que estos reflejen los servicios definidos en Kubernetes. Adicionalmente, se asegura de que los sistemas de apoyo de servicios para balanceadores de carga en la nube se encuentren actualizados.</p><h3 id=2-kubelet>2. Kubelet</h3><p>El controlador de nodos incluye la funcionalidad del kubelet que es dependiente de la nube. Previa a la introducción de CCM, el kubelet era responsable de inicializar un nodo con detalles específicos al proveedor como direcciones IP, etiquetas de región/zona y tipo de instancia. La introduccion de CCM transfiere esta inicialización del kubelet al CCM.</p><p>En este nuevo modelo, el kubelet inicializa un nodo sin información especifica del proveedor de servicios. Sin embargo, añade un <code>taint</code> al nodo recién creado de forma que este no esté disponible para el planificador hasta que el CCM completa el nodo con la información específica del proveedor. Sólo entonces elimina el <code>taint</code> y el nodo se vuelve accesible.</p><h2 id=mecanismo-de-plugins-extensiones>Mecanismo de Plugins (extensiones)</h2><p>El Cloud Controller Manager utiliza interfaces Go(lang), lo que permite que implementaciones de cualquier proveedor de servicios sean conectadas. Específicamente, utiliza el CloudProvider Interface definido <a href=https://github.com/kubernetes/cloud-provider/blob/9b77dc1c384685cb732b3025ed5689dd597a5971/cloud.go#L42-L62>aquí</a>.</p><p>La implementación de los cuatro controladores referenciados en este documento, algunas estructuras de inicialización junto con el interface CloudProvider, permanecerán como parte del núcleo de Kubernetes.</p><p>Para más información sobre el desarrollo de extensiones/plugins, consultar <a href=https://kubernetes.io/docs/tasks/administer-cluster/developing-cloud-controller-manager/>Desarrollo del CCM</a>.</p><h2 id=autorización>Autorización</h2><p>Esta sección divide el nivel de acceso requerido por varios objetos API para que el CCM pueda llevar acabo sus operaciones.</p><h3 id=controlador-de-nodos-1>Controlador de Nodos</h3><p>El controlador de nodos sólo opera con objetos Nodo. Necesita de acceso total para obtener, listar, crear, actualizar, arreglar, monitorizar y borrar objetos Nodo.</p><p>v1/Node:</p><ul><li>Get</li><li>List</li><li>Create</li><li>Update</li><li>Patch</li><li>Watch</li><li>Delete</li></ul><h3 id=controlador-de-rutas-1>Controlador de Rutas</h3><p>El controlador de rutas permanece a la escucha de eventos de creación de nodos y configura sus rutas. Necesita acceso a los objetos Nodo.</p><p>v1/Node:</p><ul><li>Get</li></ul><h3 id=controlador-de-servicios-1>Controlador de Servicios</h3><p>El controlador de servicios permanece a la escucha de eventos de creación, actualización y borrado de objetos Servicio, y se encarga de configurar los endpoints para dichos servicios.</p><p>Para acceder a los objetos Servicio, necesita permisos para listar y monitorizar. Para el mantenimiento de servicios necesita permisos para parchear y actualizar.</p><p>Para configurar endpoints para los servicios necesita permisos para crear, listar, obtener, monitorizar y actualizar.</p><p>v1/Service:</p><ul><li>List</li><li>Get</li><li>Watch</li><li>Patch</li><li>Update</li></ul><h3 id=otros>Otros</h3><p>La implementación del núcleo de CCM requiere acceso para crear eventos, y para asegurar la seguridad de operaciones; necesita acceso para crear ServiceAccounts.</p><p>v1/Event:</p><ul><li>Create</li><li>Patch</li><li>Update</li></ul><p>v1/ServiceAccount:</p><ul><li>Create</li></ul><p>El RBAC ClusterRole para CCM se muestra a continuación:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- events<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#39;*&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes/status<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- services<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- serviceaccounts<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- persistentvolumes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- endpoints<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=implementaciones-de-proveedores>Implementaciones de Proveedores</h2><p>Los siguientes proveedores de servicios en la nube han implementado CCMs:</p><ul><li><a href=https://github.com/digitalocean/digitalocean-cloud-controller-manager>Digital Ocean</a></li><li><a href=https://github.com/oracle/oci-cloud-controller-manager>Oracle</a></li><li><a href=https://github.com/kubernetes/cloud-provider-azure>Azure</a></li><li><a href=https://github.com/kubernetes/cloud-provider-gcp>GCP</a></li><li><a href=https://github.com/kubernetes/cloud-provider-aws>AWS</a></li><li><a href=https://github.com/baidu/cloud-provider-baiducloud>BaiduCloud</a></li><li><a href=https://github.com/linode/linode-cloud-controller-manager>Linode</a></li></ul><h2 id=administración-del-clúster>Administración del Clúster</h2><p>Instrucciones para configurar y ejecutar el CCM pueden encontrarse <a href=/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager>aquí</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a5f7383c83ab9eb9cd0e3c4c020b3ae6>3 - Contenedores</h1></div><div class=td-content><h1 id=pg-a858027489648786a3b16264e451272b>3.1 - RuntimeClass</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>Esta página describe el recurso RuntimeClass y el mecanismo de selección del
motor de ejecución.</p><p>RuntimeClass es una característica que permite seleccionar la configuración del
motor de ejecución para los contenedores. La configuración del motor de ejecución para
los contenedores se utiliza para ejecutar los contenedores de un Pod.</p><h2 id=motivación>Motivación</h2><p>Se puede seleccionar un RuntimeClass diferente entre diferentes Pods para
proporcionar equilibrio entre rendimiento y seguridad. Por ejemplo, si parte de
la carga de trabajo requiere un alto nivel de garantía de seguridad, se podrían
planificar esos Pods para ejecutarse en un motor de ejecución que use
virtualización de hardware. Así se beneficiaría con un mayor aislamiento del motor
de ejecución alternativo, con el coste de alguna sobrecarga adicional.</p><p>También se puede utilizar el RuntimeClass para ejecutar distintos Pods con el
mismo motor de ejecución pero con distintos parámetros.</p><h2 id=configuración>Configuración</h2><ol><li>Configurar la implementación del CRI en los nodos (depende del motor de
ejecución)</li><li>Crear los recursos RuntimeClass correspondientes.</li></ol><h3 id=1-configurar-la-implementación-del-cri-en-los-nodos>1. Configurar la implementación del CRI en los nodos</h3><p>La configuración disponible utilizando RuntimeClass dependen de la
implementación de la Interfaz del Motor de ejecución de Containers (CRI). Véase
la sección <a href=#cri-configuration>Configuración del CRI</a> para más
información sobre cómo configurar la implementación del CRI.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> RuntimeClass por defecto asume una configuración de nodos homogénea para todo el
clúster (lo que significa que todos los nodos están configurados de la misma
forma para el motor de ejecución de los contenedores). Para soportar configuraciones
heterogéneas de nodos, véase <a href=#scheduling>Planificación</a> más abajo.</div><p>Las configuraciones tienen un nombre de <code>handler</code> (manipulador) correspondiente, referenciado
por la RuntimeClass. El <code>handler</code> debe ser una etiqueta DNS 1123 válida
(alfanumérico + caracter <code>-</code>).</p><h3 id=2-crear-los-recursos-runtimeclass-correspondientes>2. Crear los recursos RuntimeClass correspondientes.</h3><p>Cada configuración establecida en el paso 1 tiene un nombre de <code>handler</code>, que
identifica a dicha configuración. Para cada <code>handler</code>, hay que crear un objeto
RuntimeClass correspondiente.</p><p>Actualmente el recurso RuntimeClass sólo tiene dos campos significativos: el
nombre del RuntimeClass (<code>metadata.name</code>) y el <code>handler</code>. La
definición del objeto se parece a ésta:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1 <span style=color:#bbb> </span><span style=color:#080;font-style:italic># La RuntimeClass se define en el grupo node.k8s.io</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myclass <span style=color:#bbb> </span><span style=color:#080;font-style:italic># Nombre por el que se referenciará la RuntimeClass</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># no contiene espacio de nombres</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>myconfiguration <span style=color:#bbb> </span><span style=color:#080;font-style:italic># El nombre de la configuración CRI correspondiente</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>El nombre de un objeto RuntimeClass debe ser un <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nombre de subdominio
DNS</a>
válido.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Se recomienda que las operaciones de escritura de la RuntimeClass
(creación/modificación/parcheo/elimiación) se restrinjan al administrador del
clúster. Habitualmente es el valor por defecto. Véase <a href=/docs/reference/access-authn-authz/authorization/>Visión general de la
Autorización</a> para más
detalles.</div><h2 id=uso>Uso</h2><p>Una vez se han configurado las RuntimeClasses para el clúster, el utilizarlas es
muy sencillo. Solo se especifica un <code>runtimeClassName</code> en la especificación del Pod.
Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>myclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># ...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Así se informa a Kubelet del nombre de la RuntimeClass a utilizar para
este pod. Si dicha RuntimeClass no existe, o el CRI no puede ejecutar el
<code>handler</code> correspondiente, el pod entrará en la
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase>fase</a> final <code>Failed</code>.
Se puede buscar por el correspondiente
<a href=/docs/tasks/debug-application-cluster/debug-application-introspection/>evento</a>
con el mensaje de error.</p><p>Si no se especifica ninguna <code>runtimeClassName</code>, se usará el RuntimeHandler por
defecto, lo que equivale al comportamiento cuando la opción RuntimeClass está
deshabilitada.</p><h3 id=configuración-del-cri>Configuración del CRI</h3><p>Para más detalles sobre cómo configurar los motores de ejecución del CRI, véase
<a href=/docs/setup/production-environment/container-runtimes/>instalación del CRI</a>.</p><h4 id=dockershim>dockershim</h4><p>El CRI dockershim incorporado por Kubernetes no soporta manejadores del motor de
ejecución.</p><h4 id=hahahugoshortcode-s3-hbhb><a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></h4><p>Los <code>handlers</code> del motor de ejecución se configuran mediante la configuración
de containerd en <code>/etc/containerd/config.toml</code>. Los <code>handlers</code> válidos se
configuran en la sección de motores de ejecución:</p><pre tabindex=0><code>[plugins.&#34;io.containerd.grpc.v1.cri&#34;.containerd.runtimes.${HANDLER_NAME}]
</code></pre><p>Véase la configuración de containerd para más detalles:
<a href=https://github.com/containerd/containerd/blob/main/docs/cri/config.md>https://github.com/containerd/containerd/blob/main/docs/cri/config.md</a></p><h4 id=hahahugoshortcode-s4-hbhb><a class=glossary-tooltip title='A lightweight container runtime specifically for Kubernetes' data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></h4><p>Los <code>handlers</code> del motor de ejecución se configuran a través de la
configuración del CRI-O en <code>/etc/crio/crio.conf</code>. Los manejadores válidos se
configuran en la <a href=https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table>tabla
crio.runtime</a></p><pre tabindex=0><code>[crio.runtime.runtimes.${HANDLER_NAME}]
  runtime_path = &#34;${PATH_TO_BINARY}&#34;
</code></pre><p>Véase la <a href=https://raw.githubusercontent.com/cri-o/cri-o/9f11d1d/docs/crio.conf.5.md>documentación de la
configuración</a>
de CRI-O para más detalles.</p><h2 id=planificación>Planificación</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code></div><p>Especificando el campo <code>scheduling</code> en una RuntimeClass se pueden establecer
restricciones para asegurar que los Pods ejecutándose con dicha RuntimeClass se
planifican en los nodos que la soportan.</p><p>Para asegurar que los pods sean asignados en nodos que soportan una RuntimeClass
determinada, ese conjunto de nodos debe tener una etiqueta común que se
selecciona en el campo <code>runtimeclass.scheduling.nodeSelector</code>. El nodeSelector
de la RuntimeClass se combina con el nodeSelector del pod durante la admisión,
haciéndose efectiva la intersección del conjunto de nodos seleccionados por
ambos. Si hay conflicto, el pod se rechazará.</p><p>Si los nodos soportados se marcan para evitar que los pods con otra RuntimeClass
se ejecuten en el nodo, se pueden añadir <code>tolerations</code> al RuntimeClass. Igual
que con el <code>nodeSelector</code>, las tolerancias se mezclan con las tolerancias del
pod durante la admisión, haciéndose efectiva la unión del conjunto de nodos
tolerados por ambos.</p><p>Para saber más sobre configurar el selector de nodos y las tolerancias, véase
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/>Asignando Pods a Nodos</a>.</p><h3 id=sobrecarga-del-pod>Sobrecarga del Pod</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code></div><p>Se pueden especificar recursos de <em>sobrecarga</em> adicional que se asocian a los
Pods que estén ejecutándose. Declarar la sobrecarga permite al clúster (incluido
el planificador) contabilizarlo al tomar decisiones sobre los Pods y los
recursos. Para utilizar la sobrecarga de pods, se debe haber habilitado la
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
PodOverhead (lo está por defecto).</p><p>La sobrecarga de pods se define en la RuntimeClass a través del los campos de
<code>overhead</code>. Con estos campos se puede especificar la sobrecarga de los pods en
ejecución que utilizan esta RuntimeClass para asegurar que estas sobrecargas se
cuentan en Kubernetes.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md>Diseño de RuntimeClass</a></li><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling>Diseño de programación de RuntimeClass</a></li><li>Leer sobre el concepto de <a href=/docs/concepts/scheduling-eviction/pod-overhead/>Pod Overhead</a></li><li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>Diseño de capacidad de PodOverhead</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e92055f79467a0422ecbc6d5169fcd38>3.2 - Variables de entorno de un Container</h1><p>Esta página explica los recursos disponibles para Containers dentro del entorno de un Container.</p><h2 id=entorno-del-container>Entorno del Container</h2><p>El entorno de los Containers de Kubernetes, añade múltiples recursos importantes a los Containers:</p><ul><li>Un sistema de ficheros que es la combinación de una <a href=/docs/concepts/containers/images/>imagen</a> y uno o más <a href=/docs/concepts/storage/volumes/>volúmenes</a>.</li><li>Información sobre el propio Container.</li><li>Información sobre otros objetos en el clúster.</li></ul><h3 id=información-del-container>Información del Container</h3><p>El <em>hostname</em> de un Container es el nombre del Pod donde el Container está funcionando.
Está disponible a través del comando <code>hostname</code> o con la función <a href=http://man7.org/linux/man-pages/man2/gethostname.2.html><code>gethostname</code></a> de la libc.</p><p>El nombre del Pod y el namespace están disponibles como variables de entorno a través de la
<a href=/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/>downward API</a>.</p><p>Las variables de entorno definidas por el usuario en la definición del Pod están también disponibles en el Container,
así como cualquier variable de entorno definida de forma estática en la imagen de Docker.</p><h3 id=información-del-cluster>Información del Cluster</h3><p>Una lista de todos los servicios que se ejecutaban cuando se creó el Container está disponible a través de variables de entorno.
La sintaxis de estas variables de entorno coincide con la de los links de Docker.</p><p>Para un servicio llamado <em>foo</em> que mapea un Container llamado <em>bar</em>,
las siguientes variables de entorno estan definidas:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_HOST</span><span style=color:#666>=</span>&lt;El host donde está funcionando el servicio&gt;
</span></span><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_PORT</span><span style=color:#666>=</span>&lt;El puerto dónde está funcionando el servicio&gt;
</span></span></code></pre></div><p>Los servicios tienen direcciones IP dedicadas y están disponibles para el Container a través de DNS,
si el <a href=http://releases.k8s.io/master/cluster/addons/dns/>complemento para DNS</a> está habilitado.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Más información sobre cómo ejecutar código en respuesta a los cambios de etapa durante ciclo de vida de un contenedor la puedes encontrar en <a href=/docs/concepts/containers/container-lifecycle-hooks/>Container lifecycle hooks</a>.</li><li>Practica <a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>añadiendo handlers a los lifecycle events de un Container </a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e6941d969d81540208a3e78bc56f43bc>3.3 - Container Lifecycle Hooks</h1><p>Esta página describe como los contenedores gestionados por kubelet pueden utilizar el framework <em>Container lifecycle hook</em> (hook del ciclo de vida del contenedor)
para ejecutar código disparado por eventos durante la gestión de su ciclo de vida (lifecycle).</p><h2 id=introducción>Introducción</h2><p>De manera análoga a muchos frameworks de lenguajes de programación que tienen componentes hooks de lifecycle, como Angular,
Kubernetes también proporciona esta funcionalidad para los contenedores.
Los hooks permiten a los contenedores conocer los eventos en su gestión de ciclo de vida
y ejecutar el código implementado en un controlador cuando el hook de ciclo de vida correspondiente es ejecutado.</p><h2 id=hooks-de-contenedores>Hooks de contenedores</h2><p>Hay dos hooks expuestos en los contenedores:</p><p><code>PostStart</code></p><p>Este hook se ejecuta inmediatamente después de crear un contenedor.
Sin embargo, no es posible garantizar que el hook se ejecute antes del ENTRYPOINT del contenedor.
No se le pasa ningún parámetro.</p><p><code>PreStop</code></p><p>Este hook se llama inmediatamente antes de que se finalice un contenedor debido a una solicitud de API o evento de gestión como un fallo liveness, o contención de recursos entre otros. Una llamada al hook de Prestop falla si el contenedor ya está en estado terminated (finalizado) o completed (completado).
Es bloqueante, lo que significa que es sincrónico,
por lo que debe completarse antes de que la llamada para eliminar el contenedor pueda ser enviada.
No se le pasa ningún parámetro.</p><p>Puedes encontrar información más detallada sobre el comportamiento de finalización de un contenedor
<a href=/docs/concepts/workloads/pods/pod/#termination-of-pods>Finalización de Pods</a>.</p><h3 id=implementación-de-controladores-de-hooks>Implementación de controladores de hooks</h3><p>Los contenedores pueden acceder a un hook implementando y registrado en un controlador de este hook.
Hay dos tipos de controladores de hooks que se pueden implementar para los contenedores:</p><ul><li>Exec: ejecuta un comando específico, como <code>pre-stop.sh</code>, dentro de cgroups y namespaces del contenedor.
Los recursos consumidos por el comando serán tomados en cuenta para el contenedor.</li><li>HTTP: ejecuta una petición HTTP contra un endpoint específico dentro del contenedor.</li></ul><h3 id=ejecución-de-controladores-de-hooks>Ejecución de controladores de hooks</h3><p>Cuando se llama un hook de gestión de ciclo de vida de un contenedor,
el sistema de gestión de Kubernetes ejecuta el controlador en el contenedor registrado para este hook.</p><p>Las llamadas al controlador de hooks son síncronas dentro del contexto del Pod que contiene el contenedor.
Esto significa que para un hook <code>PostStart</code>,
el ENTRYPOINT del contenedor y el hook se disparan de forma asíncrona.
Sin embargo, si el hook tarda demasiado en ejecutarse o se cuelga,
el contenedor no puede alcanzar el estado de <code>running</code> (en ejecución).</p><p>El comportamiento es similar para un hook <code>PreStop</code>.
Si el hook se cuelga durante la ejecución,
la fase del Pod permanece en un estado de <code>terminating</code> (finalizando) y se cancela después del <code>terminationGracePeriodSeconds</code> (finalización después del periodo de gracia) del pod en cuestión.
Si un hook <code>PostStart</code> o <code>PreStop</code> falla, se mata el contenedor.</p><p>Los usuarios deben hacer que sus controladores de hooks sean lo más livianos posible.
Hay casos, sin embargo, que los comandos de larga ejecución tienen sentido,
como cuando se guarda el estado antes de detener un contenedor.</p><h3 id=garantías-de-entrega-de-hooks>Garantías de entrega de hooks</h3><p>La entrega de un hook está destinada a ser enviada <em>al menos una vez</em>,
lo que significa que un hook puede ser llamado varias veces para cualquier evento dado,
tanto para <code>PostStart</code> como para <code>PreStop</code>.
Depende de la implementación del hook manejar esto correctamente.</p><p>En general, solo se realizan entregas individuales.
Si, por ejemplo, un receptor hook HTTP está inactivo y no puede recibir tráfico,
no hay ningún reintento.
Sin embargo, en algunos casos puede ocurrir una doble entrega.
Por ejemplo, si un Kubelet se reinicia durante la ejecución de envio de un hook,
el hook puede volver a enviarse después de que el kubelet se levante.</p><h3 id=depurando-controladores-de-hooks>Depurando controladores de hooks</h3><p>Los logs de un controlador de hooks no son expuestos en los eventos del Pod.
Si un controlador falla por alguna razón, emite un evento.
Para <code>PostStart</code>, es el evento <code>FailedPostStartHook</code>,
y para <code>PreStop</code>, el evento <code>FailedPreStopHook</code>.
Puedes ver que eventos están en ejecución con el comando <code>kubectl describe pod &lt;pod_name></code>.
El siguiente ejemplo muestra los eventos en ejecución a través del comando anterior:</p><pre tabindex=0><code>Events:
  FirstSeen  LastSeen  Count  From                                                   SubobjectPath          Type      Reason               Message
  ---------  --------  -----  ----                                                   -------------          --------  ------               -------
  1m         1m        1      {default-scheduler }                                                          Normal    Scheduled            Successfully assigned test-1730497541-cq1d2 to gke-test-cluster-default-pool-a07e5d30-siqd
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulling              pulling image &#34;test:1.0&#34;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Created              Created container with docker id 5c6a256a2567; Security:[seccomp=unconfined]
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulled               Successfully pulled image &#34;test:1.0&#34;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Started              Started container with docker id 5c6a256a2567
  38s        38s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 5c6a256a2567: PostStart handler: Error executing in Docker Container: 1
  37s        37s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 8df9fdfd7054: PostStart handler: Error executing in Docker Container: 1
  38s        37s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}                         Warning   FailedSync           Error syncing pod, skipping: failed to &#34;StartContainer&#34; for &#34;main&#34; with RunContainerError: &#34;PostStart handler: Error executing in Docker Container: 1&#34;
  1m         22s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Warning   FailedPostStartHook
</code></pre><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Aprende más sobre <a href=/docs/concepts/containers/container-environment-variables/>variables de entorno de contenedores</a>.</li><li>Practica
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>adjuntando controladores a los eventos de lifecycle de los contenedores</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d52aadda80edd9f8c514cfe2321363c2>4 - Cargas de trabajo</h1></div><div class=td-content><h1 id=pg-4d68b0ccf9c683e6368ffdcc40c838d4>4.1 - Pods</h1></div><div class=td-content><h1 id=pg-99cce294fe789317ee684a6e1f07f20f>4.1.1 - Pods</h1><p>Los <em>Pods</em> son las unidades de computación desplegables más pequeñas que se pueden crear y gestionar en Kubernetes.</p><h2 id=qué-és-un-pod>¿Qué és un Pod?</h2><p>Un <em>Pod</em> (como en una vaina de ballenas o vaina de guisantes) es un grupo de uno o más contenedores (como contenedores Docker), con almacenamiento/red compartidos, y unas especificaciones de cómo ejecutar los contenedores. Los contenidos de un Pod son siempre coubicados, coprogramados y ejecutados en un contexto compartido. Un Pod modela un "host lógico" específico de la aplicación: contiene uno o más contenedores de aplicaciones relativamente entrelazados. Antes de la llegada de los contenedores, ejecutarse en la misma máquina física o virtual significaba ser ejecutado en el mismo host lógico.</p><p>Mientras que Kubernetes soporta más <a class=glossary-tooltip title='El Container Runtime, entorno de ejecución de un contenedor, es el software responsable de ejecutar contenedores.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='runtimes de contenedores'>runtimes de contenedores</a> a parte de Docker, este último es el más conocido y ayuda a describir Pods en términos de Docker.</p><p>El contexto compartido de un Pod es un conjunto de namespaces de Linux, cgroups y, potencialmente, otras facetas de aislamiento, las mismas cosas que aíslan un contenedor Docker. Dentro del contexto de un Pod, las aplicaciones individuales pueden tener más subaislamientos aplicados.</p><p>Los contenedores dentro de un Pod comparten dirección IP y puerto, y pueden encontrarse a través de <code>localhost</code>. También pueden comunicarse entre sí mediante comunicaciones estándar entre procesos, como semáforos de SystemV o la memoria compartida POSIX. Los contenedores en diferentes Pods tienen direcciones IP distintas y no pueden comunicarse por IPC sin <a href=/docs/concepts/policy/pod-security-policy/>configuración especial</a>.
Estos contenedores normalmente se comunican entre sí a través de las direcciones IP del Pod.</p><p>Las aplicaciones dentro de un Pod también tienen acceso a <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volúmenes>volúmenes</a> compartidos, que se definen como parte de un Pod y están disponibles para ser montados en el sistema de archivos de cada aplicación.</p><p>En términos de <a href=https://www.docker.com/>Docker</a>, un Pod se modela como un grupo de contenedores de Docker con namespaces y volúmenes de sistemas de archivos compartidos.</p><p>Al igual que los contenedores de aplicaciones individuales, los Pods se consideran entidades relativamente efímeras (en lugar de duraderas). Como se explica en <a href=/docs/concepts/workloads/pods/pod-lifecycle/>ciclo de vida del pod</a>, los Pods se crean, se les asigna un identificador único (UID) y se planifican en nodos donde permanecen hasta su finalización (según la política de reinicio) o supresión. Si un <a class=glossary-tooltip title='Un Node, nodo en castellano, es una de las máquinas del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodo>nodo</a> muere, los Pods programados para ese nodo se programan para su eliminación después de un período de tiempo de espera. Un Pod dado (definido por su UID) no se "replanifica" a un nuevo nodo; en su lugar, puede reemplazarse por un Pod idéntico, con incluso el mismo nombre si lo desea, pero con un nuevo UID (consulte <a href=/es/docs/concepts/workloads/controllers/replicationcontroller/>controlador de replicación</a> para obtener más detalles).</p><p>Cuando se dice que algo tiene la misma vida útil que un Pod, como un volumen, significa que existe mientras exista ese Pod (con ese UID). Si ese Pod se elimina por cualquier motivo, incluso si se crea un reemplazo idéntico, el recurso relacionado (por ejemplo, el volumen) también se destruye y se crea de nuevo.<figure><img src=/images/docs/pod.svg width=50%><figcaption><h4>diagrama de Pod</h4></figcaption></figure></p><p><em>Un Pod de múltiples contenedores que contiene un extractor de archivos y un servidor web que utiliza un volumen persistente para el almacenamiento compartido entre los contenedores.</em></p><h2 id=motivación-para-los-pods>Motivación para los Pods</h2><h3 id=gestión>Gestión</h3><p>Los Pods son un modelo del patrón de múltiples procesos de cooperación que forman una unidad de servicio cohesiva. Simplifican la implementación y la administración de las aplicaciones proporcionando una abstracción de mayor nivel que el conjunto de las aplicaciones que lo constituyen. Los Pods sirven como unidad de despliegue, escalado horizontal y replicación. La colocación (coprogramación), el destino compartido (por ejemplo, la finalización), la replicación coordinada, el uso compartido de recursos y la gestión de dependencias se controlan automáticamente para los contenedores en un Pod.</p><h3 id=recursos-compartidos-y-comunicación>Recursos compartidos y comunicación</h3><p>Los Pods permiten el intercambio de datos y la comunicación entre los contenedores que lo constituyen.</p><p>Todas las aplicaciones en un Pod utilizan el mismo namespace de red (la misma IP y puerto) y, por lo tanto, pueden "encontrarse" entre sí y comunicarse utilizando <code>localhost</code>.
Debido a esto, las aplicaciones en un Pod deben coordinar su uso de puertos. Cada Pod tiene una dirección IP en un espacio de red compartido que tiene comunicación completa con otros servidores físicos y Pods a través de la red.</p><p>Los contenedores dentro del Pod ven que el hostname del sistema es el mismo que el <code>nombre</code> configurado para el Pod. Hay más información sobre esto en la sección <a href=/docs/concepts/cluster-administration/networking/>networking</a>.</p><p>Además de definir los contenedores de aplicaciones que se ejecutan en el Pod, el Pod especifica un conjunto de volúmenes de almacenamiento compartido. Los volúmenes permiten que los datos sobrevivan a reinicios de contenedores y se compartan entre las aplicaciones dentro del Pod.</p><h2 id=usos-de-pods>Usos de Pods</h2><p>Los Pods pueden ser usados para alojar pilas de aplicaciones integradas (por ejemplo, LAMP), pero su objetivo principal es apoyar los programas de ayuda coubicados y coadministrados, como:</p><ul><li>sistemas de gestión de contenido, loaders de datos y archivos, gestores de caché locales, etc.</li><li>copia de seguridad de registro y punto de control, compresión, rotación, captura de imágenes, etc.</li><li>observadores de cambio de datos, adaptadores de registro y monitoreo, publicadores de eventos, etc.</li><li>proxies, bridges y adaptadores.</li><li>controladores, configuradores y actualizadores.</li></ul><p>Los Pods individuales no están diseñados para ejecutar varias instancias de la misma aplicación, en general.</p><p>Para una explicación más detallada, ver <a href=https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns>El sistema distribuido ToolKit: Patrones para Contenedores multiaplicación</a>.</p><h2 id=alternativas>Alternativas</h2><p><em>¿Por qué simplemente no ejecutar múltiples programas en un solo contenedor de Docker?</em></p><ol><li>Transparencia. Hacer visibles los contenedores dentro del Pod
a la infraestructura permite que esta brinde servicios, como gestión de procesos
y monitoreo de recursos, a los contenedores, facilitando una
serie de comodidades a los usuarios.</li><li>Desacople de dependencias de software. Los contenedores individuales pueden ser
versionados, reconstruidos y redistribuidos independientemente. Kubernetes podría incluso apoyar
actualizaciones en vivo de contenedores individuales en un futuro.</li><li>Facilidad de uso. Los usuarios no necesitan ejecutar sus propios administradores de procesos,
para propagación de señales, códigos de salida, etc.</li><li>Eficiencia. Debido a que la infraestructura asume más responsabilidad,
los contenedores pueden ser más livianos.</li></ol><p><em>¿Por qué no admitir la planificación conjunta de contenedores por afinidad?</em></p><p>Ese enfoque proporcionaría la ubicación conjunta, pero no la mayor parte de
beneficios de los Pods, como compartir recursos, IPC, compartir el destino garantizado y
gestión simplificada.</p><h2 id=durabilidad-de-pods-o-su-ausencia>Durabilidad de pods (o su ausencia)</h2><p>Los Pods no están destinados a ser tratados como entidades duraderas. No sobrevivirán a errores de planificación, caídas de nodo u otros desalojos, ya sea por falta de recursos o en el caso de mantenimiento de nodos.</p><p>En general, los usuarios no deberían necesitar crear Pods directamente, deberían
usar siempre controladores incluso para Pods individuales, como por ejemplo, los
<a href=/es/docs/concepts/workloads/controllers/deployment/>Deployments</a>.
Los controladores proporcionan autorecuperación con un alcance de clúster, así como replicación
y gestión de despliegue.
Otros controladores como los <a href=/es/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>
pueden tambien proporcionar soporte para Pods que necesiten persistir el estado.</p><p>El uso de API colectivas como la principal primitiva de cara al usuario es relativamente común entre los sistemas de planificación de clúster, incluyendo <a href=https://research.google.com/pubs/pub43438.html>Borg</a>, <a href=https://mesosphere.github.io/marathon/docs/rest-api.html>Marathon</a>, <a href=http://aurora.apache.org/documentation/latest/reference/configuration/#job-schema>Aurora</a>, y <a href=http://www.slideshare.net/Docker/aravindnarayanan-facebook140613153626phpapp02-37588997>Tupperware</a>.</p><p>El Pod se expone como primitiva para facilitar:</p><ul><li>planificación y capacidad de conexión del controlador</li><li>soporte para operaciones a nivel de Pod sin la necesidad de "proxy" a través de las API del controlador</li><li>desacople de la vida útil del Pod de la vida útil del controlador, como para el arranque</li><li>desacople de controladores y servicios, el endpoint del controlador solo mira Pods</li><li>composición limpia de funcionalidad a nivel de Kubelet con funcionalidad a nivel de clúster, Kubelet es efectivamente el "controlador de Pod"</li><li>aplicaciones en alta disponibilidad, que esperan que los Pods sean reemplazados antes de su finalización y ciertamente antes de su eliminación, como en el caso de desalojos planificados o descarga previa de imágenes.</li></ul><h2 id=finalización-de-pods>Finalización de Pods</h2><p>Debido a que los Pods representan procesos en ejecución en los nodos del clúster, es importante permitir que esos procesos finalicen de forma correcta cuando ya no se necesiten (en lugar de ser detenidos bruscamente con una señal de KILL). Los usuarios deben poder solicitar la eliminación y saber cuándo finalizan los procesos, pero también deben poder asegurarse de que las eliminaciones finalmente se completen. Cuando un usuario solicita la eliminación de un Pod, el sistema registra el período de gracia previsto antes de que el Pod pueda ser eliminado de forma forzada, y se envía una señal TERM al proceso principal en cada contenedor. Una vez que el período de gracia ha expirado, la señal KILL se envía a esos procesos y el Pod se elimina del servidor API. Si se reinicia Kubelet o el administrador de contenedores mientras se espera que finalicen los procesos, la terminación se volverá a intentar con el período de gracia completo.</p><p>Un ejemplo del ciclo de terminación de un Pod:</p><ol><li>El usuario envía un comando para eliminar Pod, con un período de gracia predeterminado (30s)</li><li>El Pod en el servidor API se actualiza con el tiempo a partir del cual el Pod se considera "muerto" junto con el período de gracia.</li><li>El Pod aparece como "Terminando" cuando aparece en los comandos del cliente</li><li>(simultáneo con 3) Cuando el Kubelet ve que un Pod se ha marcado como terminado porque se ha configurado el tiempo en 2, comienza el proceso de apagado del Pod.<ol><li>Si uno de los contenedores del Pod ha definido un <a href=/es/docs/concepts/containers/container-lifecycle-hooks/#hook-details>preStop hook</a>, se invoca dentro del contenedor. Si el hook <code>preStop</code> todavía se está ejecutando después de que expire el período de gracia, el paso 2 se invoca con un pequeño período de gracia extendido (2s).</li><li>El contenedor recibe la señal TERM. Tenga en cuenta que no todos los contenedores en el Pod recibirán la señal TERM al mismo tiempo y cada uno puede requerir un hook <code>preStop</code> si el orden en el que se cierra es importante.</li></ol></li><li>(simultáneo con 3) Pod se elimina de la lista de endponts del servicio, y ya no se considera parte del conjunto de Pods en ejecución para controladores de replicación. Los Pods que se apagan lentamente no pueden continuar sirviendo el tráfico ya que los balanceadores de carga (como el proxy de servicio) los eliminan de sus rotaciones.</li><li>Cuando expira el período de gracia, todos los procesos que todavía se ejecutan en el Pod se eliminan con SIGKILL.</li><li>El Kubelet terminará de eliminar el Pod en el servidor API configurando el período de gracia 0 (eliminación inmediata). El Pod desaparece de la API y ya no es visible desde el cliente.</li></ol><p>Por defecto, todas las eliminaciones se realizan correctamente en 30 segundos. El comando <code>kubectl delete</code> admite la opción<code>--grace-period = &lt;seconds></code>que permite al usuario anular el valor predeterminado y especificar su propio valor. El valor <code>0</code> <a href=/es/docs/concepts/workloads/pods/pod/#forzar-destrucci%C3%B3n-de-pods>forzar eliminación</a> del Pod.
Debe especificar un indicador adicional <code>--force</code> junto con <code>--grace-period = 0</code> para realizar eliminaciones forzadas.</p><h3 id=forzar-destrucción-de-pods>Forzar destrucción de Pods</h3><p>La eliminación forzada de un Pod se define como la eliminación de un Pod del estado del clúster y etcd inmediatamente. Cuando se realiza una eliminación forzada, el apiserver no espera la confirmación del kubelet de que el Pod ha finalizado en el nodo en el que se estaba ejecutando. Elimina el Pod en la API inmediatamente para que se pueda crear un nuevo Pod con el mismo nombre. En el nodo, los Pods que están configurados para terminar de inmediato recibirán un pequeño período de gracia antes de ser forzadas a matar.</p><p>Estas eliminaciones pueden ser potencialmente peligrosas para algunos Pods y deben realizarse con precaución. En el caso de Pods de StatefulSets, consulte la documentación de la tarea para <a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>eliminando Pods de un StatefulSet</a>.</p><h2 id=modo-privilegiado-para-pods>Modo privilegiado para Pods</h2><p>Cualquier contenedor en un Pod puede habilitar el modo privilegiado, utilizando el indicador <code>privilegiado</code> en el <a href=/docs/tasks/configure-pod-container/security-context/>contexto de seguridad</a> de la especificación del contenedor. Esto es útil para contenedores que desean usar capacidades de Linux como manipular la pila de red y acceder a dispositivos. Los procesos dentro del contenedor obtienen casi los mismos privilegios que están disponibles para los procesos fuera de un contenedor. Con el modo privilegiado, debería ser más fácil escribir complementos de red y volumen como Pods separados que no necesitan compilarse en el kubelet.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El <a class=glossary-tooltip title='El Container Runtime, entorno de ejecución de un contenedor, es el software responsable de ejecutar contenedores.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='runtime de contenedores'>runtime de contenedores</a> debe admitir el concepto de un contenedor privilegiado para que esta configuración sea relevante.</div><h2 id=api>API</h2><p>Pod es un recurso de nivel superior en la API REST de Kubernetes.
La definición de <a href=/docs/reference/generated/kubernetes-api/v1.25/#pod-v1-core>objeto de API Pod</a>
describe el objeto en detalle.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1ccbd4eeded6ab138d98b59175bd557e>4.1.2 - Contenedores de Inicialización</h1><p>Esta página proporciona una descripción general de los contenedores de inicialización (init containers): contenedores especializados que se ejecutan
antes de los contenedores de aplicación en un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>.
Los contenedores de inicialización pueden contener utilidades o scripts de instalación no presentes en una imagen de aplicación.</p><p>Tú puedes especificar contenedores de inicialización en la especificación del Pod junto con el arreglo de <code>containers</code>
(el cual describe los contenedores de aplicación).</p><h2 id=entendiendo-los-contenedores-de-inicialización>Entendiendo los contenedores de inicialización</h2><p>Un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> puede tener múltiples contenedores
ejecutando aplicaciones dentro de él, pero también puede tener uno o más contenedores de inicialización
que se ejecutan antes de que se inicien los contenedores de aplicación.</p><p>Los contenedores de inicialización son exactamente iguales a los contenedores regulares excepto por:</p><ul><li>Los contenedores de inicialización siempre se ejecutan hasta su finalización.</li><li>Cada contenedor de inicialiación debe completarse correctamente antes de que comience el siguiente.</li></ul><p>Si el contenedor de inicialización de un Pod falla, kubelet reinicia repetidamente ese contenedor de inicialización hasta que tenga éxito.
Sin embargo, si el Pod tiene una <code>restartPolicy</code> de <code>Never</code> y un contenedor de inicialización falla durante el inicio de ese Pod, Kubernetes trata al Pod en general como fallido.</p><p>Para especificar un contenedor de inicialización para un Pod, agrega el campo <code>initContainers</code> en
la <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec>especificación del Pod</a>,
como un arreglo de elementos <code>container</code> (similar al campo <code>containers</code> de aplicación y su contenido).
Consulta <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container>Container</a> en la
referencia de API para más detalles.</p><p>El estado de los contenedores de inicialización se devuelve en el campo <code>.status.initContainerStatuses</code>
como un arreglo de los estados del contenedor (similar al campo <code>.status.containerStatuses</code>).</p><h3 id=diferencias-con-los-contenedores-regulares>Diferencias con los contenedores regulares</h3><p>Los contenedores de inicialización admiten todos los campos y características de los contenedores de aplicaciones,
incluidos los límites de recursos, los volúmenes y la configuración de seguridad. Sin embargo, las
solicitudes de recursos y los límites para un contenedor de inicialización se manejan de manera diferente,
como se documenta en <a href=#resources>Recursos</a>.</p><p>Además, los contenedores de inicialización no admiten <code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code> o
<code>startupProbe</code> porque deben de ejecutarse hasta su finalización antes de que el Pod pueda estar listo.</p><p>Si especificas varios contenedores de inicialización para un Pod, kubelet ejecuta cada contenedor
de inicialización secuencialmente. Cada contenedor de inicialización debe tener éxito antes de que se pueda ejecutar el siguiente.
Cuando todos los contenedores de inicialización se hayan ejecutado hasta su finalización, kubelet inicializa
los contenedores de aplicación para el Pod y los ejecuta como de costumbre.</p><h3 id=usando-contenedores-de-inicialización>Usando contenedores de inicialización</h3><p>Dado que los contenedores de inicialización tienen imágenes separadas de los contenedores de aplicaciones, estos
tienen algunas ventajas sobre el código relacionado de inicio:</p><ul><li>Los contenedores de inicialización pueden contener utilidades o código personalizado para la configuración que no están presentes en una
imagen de aplicación. Por ejemplo, no hay necesidad de hacer una imagen <code>FROM</code> de otra imagen solo para usar una herramienta como
<code>sed</code>, <code>awk</code>, <code>python</code> o <code>dig</code> durante la instalación.</li><li>Los roles de constructor e implementador de imágenes de aplicación pueden funcionar de forma independiente sin
la necesidad de construir conjuntamente una sola imagen de aplicación.</li><li>Los contenedores de inicialización pueden ejecutarse con una vista diferente al sistema de archivos que los contenedores de aplicaciones en
el mismo Pod. En consecuencia, se les puede dar acceso a
<a class=glossary-tooltip title='Almacena información sensible, como contraseñas, tokens OAuth o claves ssh.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secrets>Secrets</a> a los que los contenedores de aplicaciones no pueden acceder.</li><li>Debido a que los contenedores de inicialización se ejecutan hasta su finalización antes de que se inicien los contenedores de aplicaciones, los contenedores de inicialización ofrecen
un mecanismo para bloquear o retrasar el inicio del contenedor de aplicación hasta que se cumplan una serie de condiciones previas. Una vez
que las condiciones previas se cumplen, todos los contenedores de aplicaciones de un Pod pueden iniciarse en paralelo.</li><li>Los contenedores de inicialización pueden ejecutar de forma segura utilidades o código personalizado que de otro modo harían a una imagen de aplicación
de contenedor menos segura. Si mantiene separadas herramientas innecesarias, puede limitar la superficie de ataque
a la imagen del contenedor de aplicación.</li></ul><h3 id=ejemplos>Ejemplos</h3><p>A continuación, se muestran algunas ideas sobre cómo utilizar los contenedores de inicialización:</p><ul><li><p>Esperar a que se cree un <a class=glossary-tooltip title='Un Service, servicio en castellano, es el objeto de la API de Kubernetes que describe cómo se accede a las aplicaciones, tal como un conjunto de Pods, y que puede describir puertos y balanceadores de carga.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>
usando una sola linea de comando de shell:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#666>{</span>1..100<span style=color:#666>}</span>; <span style=color:#a2f;font-weight:700>do</span> sleep 1; <span style=color:#a2f;font-weight:700>if</span> dig myservice; <span style=color:#a2f;font-weight:700>then</span> <span style=color:#a2f>exit</span> 0; <span style=color:#a2f;font-weight:700>fi</span>; <span style=color:#a2f;font-weight:700>done</span>; <span style=color:#a2f>exit</span> <span style=color:#666>1</span>
</span></span></code></pre></div></li><li><p>Registrar este Pod con un servidor remoto desde la downward API con un comando como:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -X POST http://<span style=color:#b8860b>$MANAGEMENT_SERVICE_HOST</span>:<span style=color:#b8860b>$MANAGEMENT_SERVICE_PORT</span>/register -d <span style=color:#b44>&#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;</span>
</span></span></code></pre></div></li><li><p>Esperar algo de tiempo antes de iniciar el contenedor de aplicación con un comando como:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sleep <span style=color:#666>60</span>
</span></span></code></pre></div></li><li><p>Clonar un repositorio de Git en un <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volume>Volume</a></p></li><li><p>Colocar valores en un archivo de configuración y ejecutar una herramienta de plantilla para generar
dinámicamente un archivo de configuración para el contenedor de aplicación principal. Por ejemplo,
colocar el valor <code>POD_IP</code> en una configuración y generar el archivo de configuración
de la aplicación principal usando Jinja.</p></li></ul><h4 id=contenedores-de-inicialización-en-uso>Contenedores de inicialización en uso</h4><p>Este ejemplo define un simple Pod que tiene dos contenedores de inicialización.
El primero espera por <code>myservice</code> y el segundo espera por <code>mydb</code>. Una vez que ambos
contenedores de inicialización se completen, el Pod ejecuta el contenedor de aplicación desde su sección <code>spec</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;echo ¡La aplicación se está ejecutando! &amp;&amp; sleep 3600&#39;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>initContainers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo esperando a myservice; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-mydb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo esperando a mydb; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div><p>Puedes iniciar este Pod ejecutando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f myapp.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pod/myapp-pod created
</span></span></code></pre></div><p>Y verificar su estado con:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME        READY     STATUS     RESTARTS   AGE
</span></span><span style=display:flex><span>myapp-pod   0/1       Init:0/2   <span style=color:#666>0</span>          6m
</span></span></code></pre></div><p>o para más detalles:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe -f myapp.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Name:          myapp-pod
</span></span><span style=display:flex><span>Namespace:     default
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>Labels:        app.kubernetes.io/name<span style=color:#666>=</span>MyApp
</span></span><span style=display:flex><span>Status:        Pending
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>Init Containers:
</span></span><span style=display:flex><span>  init-myservice:
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>    State:         Running
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>  init-mydb:
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>    State:         Waiting
</span></span><span style=display:flex><span>      Reason:      PodInitializing
</span></span><span style=display:flex><span>    Ready:         False
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>Containers:
</span></span><span style=display:flex><span>  myapp-container:
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>    State:         Waiting
</span></span><span style=display:flex><span>      Reason:      PodInitializing
</span></span><span style=display:flex><span>    Ready:         False
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
</span></span><span style=display:flex><span>  ---------    --------    -----    ----                      -------------                           --------      ------        -------
</span></span><span style=display:flex><span>  16s          16s         <span style=color:#666>1</span>        <span style=color:#666>{</span>default-scheduler <span style=color:#666>}</span>                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
</span></span><span style=display:flex><span>  16s          16s         <span style=color:#666>1</span>        <span style=color:#666>{</span>kubelet 172.17.4.201<span style=color:#666>}</span>    spec.initContainers<span style=color:#666>{</span>init-myservice<span style=color:#666>}</span>     Normal        Pulling       pulling image <span style=color:#b44>&#34;busybox&#34;</span>
</span></span><span style=display:flex><span>  13s          13s         <span style=color:#666>1</span>        <span style=color:#666>{</span>kubelet 172.17.4.201<span style=color:#666>}</span>    spec.initContainers<span style=color:#666>{</span>init-myservice<span style=color:#666>}</span>     Normal        Pulled        Successfully pulled image <span style=color:#b44>&#34;busybox&#34;</span>
</span></span><span style=display:flex><span>  13s          13s         <span style=color:#666>1</span>        <span style=color:#666>{</span>kubelet 172.17.4.201<span style=color:#666>}</span>    spec.initContainers<span style=color:#666>{</span>init-myservice<span style=color:#666>}</span>     Normal        Created       Created container with docker id 5ced34a04634; Security:<span style=color:#666>[</span><span style=color:#b8860b>seccomp</span><span style=color:#666>=</span>unconfined<span style=color:#666>]</span>
</span></span><span style=display:flex><span>  13s          13s         <span style=color:#666>1</span>        <span style=color:#666>{</span>kubelet 172.17.4.201<span style=color:#666>}</span>    spec.initContainers<span style=color:#666>{</span>init-myservice<span style=color:#666>}</span>     Normal        Started       Started container with docker id 5ced34a04634
</span></span></code></pre></div><p>Para ver los logs de los contenedores de inicialización en este Pod ejecuta:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs myapp-pod -c init-myservice <span style=color:#080;font-style:italic># Inspecciona el primer contenedor de inicialización</span>
</span></span><span style=display:flex><span>kubectl logs myapp-pod -c init-mydb      <span style=color:#080;font-style:italic># Inspecciona el segundo contenedor de inicialización</span>
</span></span></code></pre></div><p>En este punto, estos contenedores de inicialización estarán esperando para descubrir los Servicios denominados
<code>mydb</code> y <code>myservice</code>.</p><p>Aquí hay una configuración que puedes usar para que aparezcan esos Servicios:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9377</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Para crear los servicios de <code>mydb</code> y <code>myservice</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f services.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>service/myservice created
</span></span><span style=display:flex><span>service/mydb created
</span></span></code></pre></div><p>Luego verás que esos contenedores de inicialización se completan y que el Pod <code>myapp-pod</code>
pasa al estado <code>Running</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME        READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>myapp-pod   1/1       Running   <span style=color:#666>0</span>          9m
</span></span></code></pre></div><p>Este sencillo ejemplo debería servirte de inspiración para crear tus propios
contenedores de inicialización. <a href=#what-s-next>¿Qué es lo que sigue?</a> contiene un enlace a un ejemplo más detallado.</p><h2 id=comportamiento-detallado>Comportamiento detallado</h2><p>Durante el inicio del Pod, kubelet retrasa la ejecución de contenedores de inicialización hasta que la red
y el almacenamiento estén listos. Después, kubelet ejecuta los contenedores de inicialización del Pod en el orden que
aparecen en la especificación del Pod.</p><p>Cada contenedor de inicialización debe salir correctamente antes de que
comience el siguiente contenedor. Si un contenedor falla en iniciar debido al tiempo de ejecución o
sale con una falla, se vuelve a intentar de acuerdo con el <code>restartPolicy</code> del Pod. Sin embargo,
si el <code>restartPolicy</code> del Pod se establece en <code>Always</code>, los contenedores de inicialización usan
el <code>restartPolicy</code> como <code>OnFailure</code>.</p><p>Un Pod no puede estar <code>Ready</code> sino hasta que todos los contenedores de inicialización hayan tenido éxito. Los puertos en un
contenedor de inicialización no se agregan a un Servicio. Un Pod que se está inicializando,
está en el estado de <code>Pending</code>, pero debe tener una condición <code>Initialized</code> configurada como falsa.</p><p>Si el Pod <a href=#pod-restart-reasons>se reinicia</a> o es reiniciado, todos los contenedores de inicialización
deben ejecutarse de nuevo.</p><p>Los cambios en la especificación del contenedor de inicialización se limitan al campo de la imagen del contenedor.
Alterar un campo de la imagen del contenedor de inicialización equivale a reiniciar el Pod.</p><p>Debido a que los contenedores de inicialización se pueden reiniciar, reintentar o volverse a ejecutar, el código del contenedor de inicialización
debe ser idempotente. En particular, el código que escribe en archivos en <code>EmptyDirs</code>
debe estar preparado para la posibilidad de que ya exista un archivo de salida.</p><p>Los contenedores de inicialización tienen todos los campos de un contenedor de aplicaciones. Sin embargo, Kubernetes
prohíbe el uso de <code>readinessProbe</code> porque los contenedores de inicialización no pueden
definir el <code>readiness</code> distinto de la finalización. Esto se aplica durante la validación.</p><p>Usa <code>activeDeadlineSeconds</code> en el Pod para prevenir que los contenedores de inicialización fallen por siempre.
La fecha límite incluye contenedores de inicialización.
Sin embargo, se recomienda utilizar <code>activeDeadlineSeconds</code> si el usuario implementa su aplicación
como un <code>Job</code> porque <code>activeDeadlineSeconds</code> tiene un efecto incluso después de que <code>initContainer</code> finaliza.
El Pod que ya se está ejecutando correctamente sería eliminado por <code>activeDeadlineSeconds</code> si lo estableces.</p><p>El nombre de cada aplicación y contenedor de inicialización en un Pod debe ser único; un
error de validación es arrojado para cualquier contenedor que comparta un nombre con otro.</p><h3 id=recursos>Recursos</h3><p>Dado el orden y la ejecución de los contenedores de inicialización, las siguientes reglas
para el uso de recursos se aplican:</p><ul><li>La solicitud más alta de cualquier recurso o límite particular definido en todos los contenedores
de inicialización es la <em>solicitud/límite de inicialización efectiva</em>. Si algún recurso no tiene un
límite de recursos especificado éste se considera como el límite más alto.</li><li>La <em>solicitud/límite efectiva</em> para un recurso es la más alta entre:<ul><li>la suma de todas las solicitudes/límites de los contenedores de aplicación, y</li><li>la solicitud/límite de inicialización efectiva para un recurso</li></ul></li><li>La planificación es hecha con base en las solicitudes/límites efectivos, lo que significa
que los contenedores de inicialización pueden reservar recursos para la inicialización que no se utilizan
durante la vida del Pod.</li><li>El nivel de <code>QoS</code> (calidad de servicio) del <em>nivel de <code>QoS</code> efectivo</em> del Pod es el
nivel de <code>QoS</code> tanto para los contenedores de inicialización como para los contenedores de aplicación.</li></ul><p>La cuota y los límites son aplicados con base en la solicitud y límite efectivos de Pod.</p><p>Los grupos de control de nivel de Pod (cgroups) se basan en la solicitud y el límite de Pod efectivos, al igual que el planificador de Kubernetes (<a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a>).</p><h3 id=razones-de-reinicio-del-pod>Razones de reinicio del Pod</h3><p>Un Pod puede reiniciarse, provocando la re-ejecución de los contenedores de inicialización por las siguientes razones:</p><ul><li>Se reinicia el contenedor de infraestructura del Pod. Esto es poco común y debería hacerlo alguien con acceso de root a los nodos.</li><li>Todos los contenedores en un Pod son terminados mientras <code>restartPolicy</code> esté configurado en <code>Always</code>,
forzando un reinicio y el registro de finalización del contenedor de inicialización se ha perdido debido a
la recolección de basura.</li></ul><p>El Pod no se reiniciará cuando se cambie la imagen del contenedor de inicialización o cuando
se pierda el registro de finalización del contenedor de inicialización debido a la recolección de basura. Esto
se aplica a Kubernetes v1.20 y posteriores. Si estás utilizando una versión anterior de
Kubernetes, consulta la documentación de la versión que estás utilizando.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Lee acerca de <a href=/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container>creando un Pod que tiene un contenedor de inicialización</a></li><li>Aprende cómo <a href=/docs/tasks/debug/debug-application/debug-init-containers/>depurar contenedores de inicialización</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4e9b9cbc9776b12e7335c53da377c9c8>4.1.3 - Pod Preset</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.6 [alpha]</code></div><p>Esta página provee una descripción general de los PodPresets, los cuales son
los objetos que se utilizan para inyectar cierta información en los Pods en
el momento de la creación. Esta información puede incluir secretos, volúmenes,
montajes de volúmenes y variables de entorno.</p><h2 id=entendiendo-los-pod-presets>Entendiendo los Pod Presets</h2><p>Un PodPreset es un recurso de la API utilizado para poder inyectar requerimientos
adicionales de tiempo de ejecución en un Pod en el momento de la creación.
Se utilizan los <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>selectores de etiquetas</a>
para especificar los Pods a los que se aplica un PodPreset determinado.</p><p>El uso de un PodPreset permite a los autores de plantillas de Pods no tener que proporcionar
explícitamente toda la información de cada Pod. De esta manera, los autores de plantillas de
Pods que consuman un determinado servicio no tendrán que conocer todos los detalles de ese servicio.</p><h2 id=habilitando-un-podpreset-en-su-clúster>Habilitando un PodPreset en su clúster</h2><p>Con el fin de utilizar los Pod Presets en un clúster debe asegurarse de lo siguiente:</p><ol><li><p>Que se ha configurado el tipo de API <code>settings.k8s.io/v1alpha1/podpreset</code>. Esto se puede hacer,
por ejemplo, incluyendo <code>settings.k8s.io/v1alpha1=true</code> como valor de la opción <code>--runtime-config</code>
en el servidor API. En minikube se debe añadir el flag
<code>--extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true</code> cuando el clúster
se está iniciando.</p></li><li><p>Que se ha habilitado el controlador de admisión <code>PodPreset</code>. Una forma de hacer esto es incluir
<code>PodPreset</code> como valor de la opción <code>--enable-admission-plugins</code> especificada
para el servidor API. En minikube se debe añadir el flag</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>--extra-config<span style=color:#666>=</span>apiserver.enable-admission-plugins<span style=color:#666>=</span>NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset
</span></span></code></pre></div><p>cuando el clúster se está iniciando.</p></li></ol><h2 id=cómo-funciona>Cómo funciona</h2><p>Kubernetes provee un controlador de admisión (<code>PodPreset</code>) que, cuando está habilitado,
aplica los Pod Presets a las peticiones de creación de Pods entrantes.
Cuando se realiza una solicitud de creación de Pods, el sistema hace lo siguiente:</p><ol><li>Obtiene todos los <code>PodPresets</code> disponibles para usar.</li><li>Verifica si los selectores de etiquetas de cualquier <code>PodPreset</code> correspondan
con las etiquetas del Pod que se está creando.</li><li>Intenta fusionar los diversos recursos definidos por el <code>PodPreset</code> dentro del Pod
que se está creando.</li><li>Si se llegase a producir un error al intentar fusionar los recursos dentro del Pod,
lanza un evento que documente este error, luego crea el Pod <em>sin</em> ningún recurso que se
inyecte desde el <code>PodPreset</code>.</li><li>Escribe una nota descriptiva de la especificación de Pod modificada resultante para
indicar que ha sido modificada por un <code>PodPreset</code>. La nota descriptiva presenta la forma
<code>podpreset.admission.kubernetes.io/podpreset-&lt;pod-preset name>: "&lt;resource version>"</code>.</li></ol><p>Cada Pod puede ser correspondido por cero o más Pod Presets; y cada <code>PodPreset</code> puede ser
aplicado a cero o más Pods. Cuando se aplica un <code>PodPreset</code> a una o más Pods, Kubernetes
modifica la especificación del Pod. Para los cambios a <code>env</code>, <code>envFrom</code>, y <code>volumeMounts</code>,
Kubernetes modifica la especificación del Container para todos los Containers en el Pod;
para los cambios a <code>volumes</code>, Kubernetes modifica la especificación del Pod.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Un Pod Preset es capaz de modificar los siguientes campos en las especificaciones de un Pod
en caso de ser necesario:</p><ul><li>El campo <code>.spec.containers</code>.</li><li>El campo <code>.spec.initContainers</code></li></ul></div><h3 id=deshabilitar-un-pod-preset-para-un-pod-específico>Deshabilitar un Pod Preset para un Pod específico</h3><p>Puede haber casos en los que se desee que un Pod no se vea alterado por ninguna posible
modificación del Pod Preset. En estos casos, se puede añadir una observación en el Pod
<code>.spec</code> de la siguiente forma: <code>podpreset.admission.kubernetes.io/exclude: "true"</code>.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><p>Ver <a href=/docs/tasks/inject-data-application/podpreset/>Inyectando datos en un Pod usando PodPreset</a></p><p>Para más información sobre los detalles de los trasfondos, consulte la <a href=https://git.k8s.io/design-proposals-archive/service-catalog/pod-preset.md>propuesta de diseño de PodPreset</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4aaf43c715cd764bc8ed4436f3537e68>4.1.4 - Interrupciones</h1><p>Esta guía es para los dueños de aplicaciones que quieren crear
aplicaciones con alta disponibilidad y que necesitan entender
que tipos de interrupciones pueden suceder en los Pods.</p><p>También es para los administradores de clústers que quieren aplicar acciones
automatizadas en sus clústers, cómo actualizar o autoescalar los clústers.</p><h2 id=interrupciones-voluntarias-e-involuntarias>Interrupciones voluntarias e involuntarias</h2><p>Los Pods no desaparecen hasta que algo (una persona o un controlador) los destruye
ó hay problemas de hardware ó software que son inevitables.</p><p>Nosotros llamamos a esos casos inevitables <em>interrupciones involuntarias</em> de
una aplicación. Algunos ejemplos:</p><ul><li>Una falla en hardware de la máquina física del nodo</li><li>Un administrador del clúster borra una VM (instancia) por error</li><li>El proveedor de la nube o el hipervisor falla y hace desaparecer la VM</li><li>Un kernel panic</li><li>El nodo desaparece del clúster por un problema de red que lo separa del clúster</li><li>Una remoción del Pod porque el nodo <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>no tiene recursos suficientes</a>.</li></ul><p>A excepción de la condición sin recursos suficientes, todas estas condiciones
deben ser familiares para la mayoría de los usuarios, no son específicas
de Kubernetes</p><p>Nosotros llamamos a los otros casos <em>interrupciones voluntarias</em>. Estas incluyen
las acciones iniciadas por el dueño de la aplicación y aquellas iniciadas por el Administrador
del Clúster. Las acciones típicas de los dueños de la aplicación incluye:</p><ul><li>borrar el Deployment u otro controlador que maneja el Pod</li><li>actualizar el Deployment del Pod que causa un reinicio</li><li>borrar un Pod (por ejemplo, por accidente)</li></ul><p>Las acciones del administrador del clúster incluyen:</p><ul><li><a href=/docs/tasks/administer-cluster/safely-drain-node/>Drenar un nodo</a> para reparar o actualizar.</li><li>Drenar un nodo del clúster para reducir el clúster (aprenda acerca de <a href=https://github.com/kubernetes/autoscaler/#readme>Autoescalamiento de Clúster</a>
).</li><li>Remover un Pod de un nodo para permitir que otra cosa pueda ingresar a ese nodo.</li></ul><p>Estas acciones pueden ser realizadas directamente por el administrador del clúster, por
tareas automatizadas del administrador del clúster ó por el proveedor del clúster.</p><p>Consulte al administrador de su clúster, a su proveedor de la nube ó a la documentación de su distribución
para determinar si alguna de estas interrupciones voluntarias están habilitadas en su clúster.
Si ninguna se encuentra habilitada, puede omitir la creación del presupuesto de Interrupción de Pods.</p><div class="alert alert-warning caution callout" role=alert><strong>Precaución:</strong> No todas las interrupciones voluntarias son consideradas por el presupuesto de interrupción de Pods. Por ejemplo,
borrar un Deployment o Pods que evitan el uso del presupuesto.</div><h2 id=tratando-con-las-interrupciones>Tratando con las interrupciones</h2><p>Estas son algunas de las maneras para mitigar las interrupciones involuntarias:</p><ul><li>Asegurarse que el Pod <a href=/docs/tasks/configure-pod-container/assign-memory-resource>solicite los recursos</a> que necesita.</li><li>Replique su aplicación si usted necesita alta disponibilidad. (Aprenda sobre correr aplicaciones replicadas
<a href=/docs/tasks/run-application/run-stateless-application-deployment/>stateless</a>
y <a href=/docs/tasks/run-application/run-replicated-stateful-application/>stateful</a></li><li>Incluso, para una alta disponibilidad mayor cuando se corren aplicaciones replicadas,
propague las aplicaciones por varios racks (usando
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>anti-affinity</a>)
o usando zonas (si usa un <a href=/docs/setup/multiple-zones>clúster multi-zona</a>.)</li></ul><p>La frecuencia de las interrupciones voluntarias varía. En un clúster basico de Kubernetes, no hay
interrupciones voluntarias automáticas (solo el usuario las genera). Sin embargo, su administrador del clúster o proveedor de alojamiento
puede correr algun servicio adicional que pueda causar estas interrupciones voluntarias. Por ejemplo,
desplegando una actualización de software en los nodos puede causar interrupciones. También, algunas implementaciones
de clústers con autoescalamiento de nodos puede causar interrupciones para defragmentar o compactar los nodos.
Su administrador de clúster o proveedor de alojamiento debe tener documentado cuál es el nivel de interrupciones
voluntarias esperadas, sí es que las hay. Ciertas opciones de configuración, como ser
<a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>usar PriorityClasses</a>
en las especificaciones de su Pod pueden también causar interrupciones voluntarias (o involuntarias).</p><h2 id=presupuesto-de-interrupción-de-pods>Presupuesto de Interrupción de Pods</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>Kubernetes ofrece carácteristicas para ayudar a ejecutar aplicaciones con alta disponibliidad, incluso cuando usted
introduce interrupciones voluntarias frecuentes.</p><p>Como dueño de la aplicación, usted puede crear un presupuesto de interrupción de Pods (PDB por sus siglas en inglés) para cada aplicación.
Un PDB limita el numero de Pods de una aplicación replicada, que estan caídos de manera simultánea por
interrupciones voluntarias. Por ejemplo, una aplicación basada en quórum puede
asegurarse que el número de réplicas corriendo nunca es menor al
número necesitado para obtener el quórum. Una web de tipo front end puede querer
asegurarse que el número de réplicas atendiendo al tráfico nunca puede caer bajo un cierto
porcentaje del total.</p><p>Los administradores del clúster y proveedores de hosting pueden usar herramientas que
respeten el presupuesto de interrupción de Pods utilizando la <a href=/docs/tasks/administer-cl%C3%BAster/safely-drain-node/#eviction-api>API de Desalojo</a>
en vez de directamente borrar Pods o Deployments.</p><p>Por ejemplo, el subcomando <code>kubectl drain</code> le permite marcar un nodo a un modo fuera de
servicio. Cuando se ejecuta <code>kubectl drain</code>, la herramienta trata de quitar a todos los Pods en
el nodo que se esta dejando fuera de servicio. La petición de desalojo que <code>kubectl</code> solicita en
su nombre puede ser temporalmente denegado, entonces la herramienta periodicamente reintenta todas las
peticiones fallidas hasta que todos los Pods en el nodo afectado son terminados ó hasta que el tiempo de espera,
que puede ser configurado, es alcanzado.</p><p>Un PDB especifica el número de réplicas que una aplicación puede tolerar, relativo a cuantas
se pretende tener. Por ejemplo, un Deployment que tiene un <code>.spec.replicas: 5</code> se
supone que tiene 5 Pods en cualquier momento. Si su PDB permite tener 4 a la vez,
entonces la API de Desalojo va a permitir interrupciones voluntarias de un (pero no a dos) Pod a la vez.</p><p>El grupo de Pods que comprende a la aplicación esta especificada usando una etiqueta selectora, la misma
que es usada por el controlador de aplicación (deployment, stateful-set, etc).</p><p>El numero de Pods "deseado" es calculado a partir de <code>.spec.replicas</code> de el recurso de Workload
que es manejado para esos Pods. El plano de control descubre el recurso Workload perteneciente a el
examinando las <code>.metadata.ownerReferences</code> del Pod.</p><p>Las <a href=#voluntary-and-involuntary-disruptions>Interrupciones Involuntarias</a> no pueden ser prevenidas por los PDB; pero si
son contabilizadas a partir este presupuesto.</p><p>Los Pods que son borrados o no estan disponibles debido a una actualización continua de una aplicación forman parte del presupuesto de interrupciones, pero los recursos Workload (como los Deployments y StatefulSet)
no están limitados por los PDBs cuando se hacen actualizaciones continuas. En cambio, la administración de fallas
durante la actualización de la aplicación es configurada en la especificación para este recurso Workload específico.</p><p>Cuando un Pod es quitado usando la API de desalojo, este es
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>terminado</a> correctamente, haciendo honor al
<code>terminationGracePeriodSeconds</code> configurado en su <a href=/docs/reference/generated/kubernetes-api/v1.25/#podspec-v1-core>PodSpec</a>.</p><h2 id=pdb-example>Ejemplo de Presupuesto de Interrupción de POD</h2><p>Considere un clúster con 3 nodos, <code>nodo-1</code> hasta <code>nodo-3</code>.
El clúster esta corriendo varias aplicaciones. Uno de ellos tiene 3 replicas, que llamaremos
<code>pod-a</code>, <code>pod-b</code>, y <code>pod-c</code>. Otro Pod no relacionado y sin PDB, llamado <code>pod-x</code>, también se muestra.</p><p>Inicialmente los pods estan distribuidos de esta manera:</p><table><thead><tr><th style=text-align:center>nodo-1</th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center>pod-a <em>available</em></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center>pod-x <em>available</em></td><td style=text-align:center></td><td style=text-align:center></td></tr></tbody></table><p>Los 3 Pods son parte de un Deployment, ellos colectivamente tienen un PDB que requiere
que por lo menos 2 de los 3 Pods esten disponibles todo el tiempo.</p><p>Por ejemplo, supongamos que el administrador del clúster quiere reiniciar para actualizar el kernel y arreglar un bug.
El administrador del clúster primero intenta desocupar el <code>nodo-1</code> usando el comando <code>kubectl drain</code>.
La herramienta intenta desalojar a los pods <code>pod-a</code> y <code>pod-x</code>. Esto tiene éxito inmediatamente.
Ambos Pods van al estado <code>terminating</code> al mismo tiempo.
Pone al clúster en el siguiente estado:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>draining</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center>pod-a <em>terminating</em></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center>pod-x <em>terminating</em></td><td style=text-align:center></td><td style=text-align:center></td></tr></tbody></table><p>El Deployment detecta que uno de los Pods esta terminando, entonces crea un reemplazo
llamado <code>pod-d</code>. Como el <code>nodo-1</code> esta bloqueado, el pod termina en otro nodo. Algo más, adicionalmente
a creado el pod <code>pod-y</code> como un reemplazo del <code>pod-x</code> .</p><p>(Nota: para un StatefulSet, <code>pod-a</code>, el cual debería ser llamado algo como <code>pod-0</code>, necesitaría ser terminado completamente antes de su remplazo, el cual también es llamado <code>pod-0</code> pero tiene un UID diferente, podría ser creado. De lo contrario, el ejemplo también aplica a un StatefulSet.)</p><p>Ahora el clúster esta en este estado:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>draining</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center>pod-a <em>terminating</em></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center>pod-x <em>terminating</em></td><td style=text-align:center>pod-d <em>starting</em></td><td style=text-align:center>pod-y</td></tr></tbody></table><p>En algún punto, los Pods finalizan y el clúster se ve de esta forma:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>drained</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center></td><td style=text-align:center>pod-d <em>starting</em></td><td style=text-align:center>pod-y</td></tr></tbody></table><p>En este estado, si un administrador del clúster impaciente intenta desalojar el <code>nodo-2</code> ó el
<code>nodo-3</code>, el comando drain va a ser bloqueado, porque hay solamente 2 Pods disponibles para
el Deployment y el PDB requiere por lo menos 2. Después de pasado un tiempo el <code>pod-d</code> esta disponible.</p><p>El estado del clúster ahora se ve así:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>drained</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center></td><td style=text-align:center>pod-d <em>available</em></td><td style=text-align:center>pod-y</td></tr></tbody></table><p>Ahora, el administrador del clúster desaloja el <code>nodo-2</code>.
El comando drain tratará de desalojar a los 2 Pods con algún orden, digamos
primero el <code>pod-b</code> y después el <code>pod-d</code>. Va a tener éxito en quitar el <code>pod-b</code>.
Pero cuando intente desalojar al <code>pod-d</code>, va a ser rechazado porque esto va a dejar
un Pod solamente disponible para el Deployment.</p><p>El Deployment crea un reemplazo para el <code>pod-b</code> llamado <code>pod-e</code>.
Porque no hay recursos suficientes disponibles en el clúster para programar
el <code>pod-e</code> el desalojo será bloqueado nuevamente. El clúster va a terminar en este
estado:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>drained</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th><th style=text-align:center><em>no node</em></th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>pod-b <em>terminating</em></td><td style=text-align:center>pod-c <em>available</em></td><td style=text-align:center>pod-e <em>pending</em></td></tr><tr><td style=text-align:center></td><td style=text-align:center>pod-d <em>available</em></td><td style=text-align:center>pod-y</td><td style=text-align:center></td></tr></tbody></table><p>Ahora, el administrador del clúster necesita
agregar un nuevo nodo en el clúster para continuar con la actualización.</p><p>Usted puede ver como Kubernetes varia la tasa a la que las interrupciones
pueden suceder, en función de:</p><ul><li>cuantas réplicas una aplicación necesita</li><li>cuanto toma apagar una instancia de manera correcta</li><li>cuanto tiempo toma que una nueva instancia inicie</li><li>el tipo de controlador</li><li>la capacidad de recursos del clúster</li></ul><h2 id=separando-al-dueño-del-clúster-y-los-roles-de-dueños-de-la-aplicación>Separando al dueño del Clúster y los roles de dueños de la Aplicación</h2><p>Muchas veces es útil pensar en el Administrador del Clúster
y al dueño de la aplicación como roles separados con conocimiento limitado
el uno del otro. Esta separación de responsabilidades
puede tener sentido en estos escenarios:</p><ul><li>Cuando hay muchas equipos con aplicaciones compartiendo un clúster de Kubernetes y
hay una especialización natural de roles</li><li>Cuando una herramienta de terceros o servicio es usado para automatizar el control del clúster</li></ul><p>El presupuesto de interrupción de Pods soporta esta separación de roles, ofreciendo
una interfaz entre los roles.</p><p>Si no se tiene tal separación de responsabilidades en la organización,
posiblemente no se necesite el Presupuesto de Interrupción de Pods.</p><h2 id=como-realizar-acciones-disruptivas-en-el-clúster>Como realizar Acciones Disruptivas en el Clúster</h2><p>Si usted es el Administrador del Clúster y necesita realizar una acción disruptiva en todos
los nodos en el clúster, como ser una actualización de nodo o de software, estas son algunas de las opciones:</p><ul><li>Aceptar el tiempo sin funcionar mientras dura la actualización.</li><li>Conmutar a otra replica completa del clúster.<ul><li>No hay tiempo sin funcionar, pero puede ser costoso tener duplicados los nodos
y tambien un esfuerzo humano para orquestar dicho cambio.</li></ul></li><li>Escribir la toleracia a la falla de la aplicación y usar PDBs.<ul><li>No hay tiempo sin funcionar.</li><li>Duplicación de recursos mínimo.</li><li>Permite mucha más automatización de la administración del clúster.</li><li>Escribir aplicaciones que tengan tolerancia a fallas es complicado, pero el trabajo para tolerar interrupciones
involuntarias, largamente se sobrepone con el trabajo que es dar soporte a autoescalamientos y tolerar
interrupciones involuntarias.</li></ul></li></ul><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li><p>Siga los pasos para proteger su aplicación con <a href=/docs/tasks/run-application/configure-pdb/>configurar el Presupuesto de Interrupciones de Pods</a>.</p></li><li><p>Aprenda más sobre <a href=/docs/tasks/administer-cl%C3%BAster/safely-drain-node/>desalojar nodos</a></p></li><li><p>Aprenda sobre <a href=/docs/concepts/workloads/controllers/deployment/#updating-a-deployment>actualizar un Deployment</a>
incluyendo los pasos para mantener su disponibilidad mientras dura la actualización.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-53a1005011e1bda2ce81819aad7c8b32>4.1.5 - Containers Efímeros</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><p>Esta página proporciona una descripción general de los Containers efímeros: un tipo especial de Container
que se ejecuta temporalmente en un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> ya existente para cumplir las
acciones iniciadas por el usuario, como por ejemplo, la solución de problemas. En vez de ser utilizadas para
crear aplicaciones, los Containers efímeros se utilizan para examinar los servicios.</p><div class="alert alert-danger warning callout" role=alert><strong>Advertencia:</strong> Los Containers efímeros se encuentran en una fase alfa inicial y no son aptos para clústers
de producción. Es de esperar que esta característica no funcione en algunas situaciones, por
ejemplo, al seleccionar los Namespaces de un Container. De acuerdo con la <a href=/docs/reference/using-api/deprecation-policy/>Política de
Deprecación de Kubernetes</a>, esta característica
alfa puede variar significativamente en el futuro o ser eliminada por completo.</div><h2 id=entendiendo-los-containers-efímeros>Entendiendo los Containers efímeros</h2><p><a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> son el componente fundamental de las
aplicaciones de Kubernetes. Puesto que los Pods están previstos para ser desechables
y reemplazables, no se puede añadir un Container a un Pod una vez creado. Sin embargo, por lo
general se eliminan y se reemplazan los Pods de manera controlada utilizando
<a class=glossary-tooltip title='Un objeto API que gestiona una aplicación replicada.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployments>Deployments</a>.</p><p>En ocasiones es necesario examinar el estado de un Pod existente, como por ejemplo,
para poder solucionar un error difícil de reproducir. Puede ejecutar en estos casos
un Container efímero en un Pod ya existente para examinar su estado y para ejecutar
comandos de manera arbitraria.</p><h3 id=qué-es-un-container-efímero>Qué es un Container efímero?</h3><p>Los Containers efímeros se diferencian de otros Containers en que no garantizan ni los
recursos ni la ejecución, y en que nunca se reiniciarán automáticamente, de modo que no
son aptos para la construcción de aplicaciones. Los Containers efímeros se describen
usando la misma <a href=/docs/reference/generated/kubernetes-api/v1.25/#container-v1-core>ContainerSpec</a> que los Containers regulares, aunque muchos campos son
incompatibles y no están habilitados para los Containers efímeros.</p><ul><li>Los Containers efímeros no pueden tener puertos, por lo que campos como <code>ports</code>,
<code>livenessProbe</code>, <code>readinessProbe</code> no están habilitados.</li><li>Las asignaciones de recursos del Pod son inmutables, por lo que no esta habilitado
configurar "resources".</li><li>Para obtener una lista completa de los campos habilitados, consulte la documentación
de referencia [EphemeralContainer] (/docs/reference/generated/kubernetes-api/v1.25/#ephemeralcontainer-v1-core).</li></ul><p>En vez de añadirlos de forma directa al <code>pod.spec</code>, los Containers efímeros se crean usando un
controlador especial de la API, <code>ephemeralcontainers</code>, por lo tanto no es posible añadir un
Container efímero utilizando <code>kubectl edit</code>.</p><p>Al igual en el caso de los Containers regulares, no se puede modificar o remover un Container
efímero después de haberlo agregado a un Pod.</p><h2 id=casos-de-uso-para-los-containers-efímeros>Casos de uso para los Containers efímeros</h2><p>Los Containers efímeros resultan útiles para la solución interactiva de incidencias cuando
<code>kubectl exec</code> es insuficiente tanto porque un container se ha caído, como porque la imagen de un
Container no incluye las utilidades de depuración.</p><p>En particular, las <a href=https://github.com/GoogleContainerTools/distroless>imágenes distroless</a>
le permiten desplegar imágenes de Containers mínimos que disminuyen la superficie de ataque
y la exposición a errores y vulnerabilidades. Ya que las imágenes distroless no contienen un
shell ni ninguna utilidad de depuración, resulta difícil solucionar los problemas de las imágenes
distroless usando solamente <code>kubectl exec</code>.</p><p>Cuando utilice Containers efímeros, es conveniente habilitar el <a href=/docs/tasks/configure-pod-container/share-process-namespace/>proceso Namespace de uso
compartido</a> para poder ver los
procesos en otros containers.</p><h3 id=ejemplos>Ejemplos</h3><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los ejemplos de esta sección requieren que los <code>EphemeralContainers</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature
gate</a> estén habilitados
y que tanto el cliente como el servidor de Kubernetes tengan la version v1.16 o posterior.</div><p>En los ejemplos de esta sección muestran la forma en que los Containers efímeros se
presentan en la API. Los usuarios normalmente usarían un plugin <code>kubectl</code> para la solución
de problemas que automatizaría estos pasos.</p><p>Los Containers efímeros son creados utilizando el subrecurso <code>ephemeralcontainers</code> del Pod,
que puede ser visto utilizando <code>kubectl --raw</code>. En primer lugar describa el Container
efímero a añadir como una lista de <code>EphemeralContainers</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;EphemeralContainers&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;example-pod&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;ephemeralContainers&#34;</span>: [{
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;command&#34;</span>: [
</span></span><span style=display:flex><span>            <span style=color:#b44>&#34;sh&#34;</span>
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;image&#34;</span>: <span style=color:#b44>&#34;busybox&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;imagePullPolicy&#34;</span>: <span style=color:#b44>&#34;IfNotPresent&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;debugger&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;stdin&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;tty&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;terminationMessagePolicy&#34;</span>: <span style=color:#b44>&#34;File&#34;</span>
</span></span><span style=display:flex><span>    }]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Para actualizar los Containers efímeros de los <code>example-pod</code> en ejecución:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl replace --raw /api/v1/namespaces/default/pods/example-pod/ephemeralcontainers  -f ec.json
</span></span></code></pre></div><p>Esto devolverá una nueva lista de Containers efímeros:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;kind&#34;</span>:<span style=color:#b44>&#34;EphemeralContainers&#34;</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>:<span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;metadata&#34;</span>:{
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>:<span style=color:#b44>&#34;example-pod&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>:<span style=color:#b44>&#34;default&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;selfLink&#34;</span>:<span style=color:#b44>&#34;/api/v1/namespaces/default/pods/example-pod/ephemeralcontainers&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;uid&#34;</span>:<span style=color:#b44>&#34;a14a6d9b-62f2-4119-9d8e-e2ed6bc3a47c&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;resourceVersion&#34;</span>:<span style=color:#b44>&#34;15886&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;creationTimestamp&#34;</span>:<span style=color:#b44>&#34;2019-08-29T06:41:42Z&#34;</span>
</span></span><span style=display:flex><span>   },
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;ephemeralContainers&#34;</span>:[
</span></span><span style=display:flex><span>      {
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;name&#34;</span>:<span style=color:#b44>&#34;debugger&#34;</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;image&#34;</span>:<span style=color:#b44>&#34;busybox&#34;</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;command&#34;</span>:[
</span></span><span style=display:flex><span>            <span style=color:#b44>&#34;sh&#34;</span>
</span></span><span style=display:flex><span>         ],
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;resources&#34;</span>:{
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>         },
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;terminationMessagePolicy&#34;</span>:<span style=color:#b44>&#34;File&#34;</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;imagePullPolicy&#34;</span>:<span style=color:#b44>&#34;IfNotPresent&#34;</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;stdin&#34;</span>:<span style=color:#a2f;font-weight:700>true</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;tty&#34;</span>:<span style=color:#a2f;font-weight:700>true</span>
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>   ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Se puede ver el estado del Container efímero creado usando <code>kubectl describe</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pod example-pod
</span></span></code></pre></div><pre tabindex=0><code>...
Ephemeral Containers:
  debugger:
    Container ID:  docker://cf81908f149e7e9213d3c3644eda55c72efaff67652a2685c1146f0ce151e80f
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
    State:          Running
      Started:      Thu, 29 Aug 2019 06:42:21 +0000
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:         &lt;none&gt;
...
</code></pre><p>Se puede conectar al nuevo Container efímero usando <code>kubectl attach</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl attach -it example-pod -c debugger
</span></span></code></pre></div><p>Si el proceso Namespace de uso compartido está habilitado, se pueden visualizar los procesos de todos los Containers de ese Pod.
Por ejemplo, después de haber conectado, ejecute <code>ps</code> en el debugger del container:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ps auxww
</span></span></code></pre></div><p>La respuesta es semejante a:</p><pre tabindex=0><code>PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    6 root      0:00 nginx: master process nginx -g daemon off;
   11 101       0:00 nginx: worker process
   12 101       0:00 nginx: worker process
   13 101       0:00 nginx: worker process
   14 101       0:00 nginx: worker process
   15 101       0:00 nginx: worker process
   16 101       0:00 nginx: worker process
   17 101       0:00 nginx: worker process
   18 101       0:00 nginx: worker process
   19 root      0:00 /pause
   24 root      0:00 sh
   29 root      0:00 ps auxww
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-89637410cacae45a36ab1cc278c482eb>4.2 - Controladores</h1></div><div class=td-content><h1 id=pg-d459b930218774655fa7fd1620625539>4.2.1 - ReplicaSet</h1><p>El objeto de un ReplicaSet es el de mantener un conjunto estable de réplicas de Pods ejecutándose
en todo momento. Así, se usa en numerosas ocasiones para garantizar la disponibilidad de un
número específico de Pods idénticos.</p><h2 id=cómo-funciona-un-replicaset>Cómo funciona un ReplicaSet</h2><p>Un ReplicaSet se define con campos, incluyendo un selector que indica cómo identificar a los Pods que puede adquirir,
un número de réplicas indicando cuántos Pods debería gestionar, y una plantilla pod especificando los datos de los nuevos Pods
que debería crear para conseguir el número de réplicas esperado. Un ReplicaSet alcanza entonces su propósito
mediante la creación y eliminación de los Pods que sea necesario para alcanzar el número esperado.
Cuando un ReplicaSet necesita crear nuevos Pods, utiliza su plantilla Pod.</p><p>El enlace que un ReplicaSet tiene hacia sus Pods es a través del campo del Pod denominado <a href=/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents>metadata.ownerReferences</a>,
el cual indica qué recurso es el propietario del objeto actual. Todos los Pods adquiridos por un ReplicaSet tienen su propia
información de identificación del ReplicaSet en su campo ownerReferences. Y es a través de este enlace
cómo el ReplicaSet conoce el estado de los Pods que está gestionando y actúa en consecuencia.</p><p>Un ReplicaSet identifica los nuevos Pods a adquirir usando su selector. Si hay un Pod que no tiene OwnerReference
o donde OwnerReference no es un controlador, pero coincide con el selector del ReplicaSet,
este será inmediatamente adquirido por dicho ReplicaSet.</p><h2 id=cuándo-usar-un-replicaset>Cuándo usar un ReplicaSet</h2><p>Un ReplicaSet garantiza que un número específico de réplicas de un pod se está ejecutando en todo momento.
Sin embargo, un Deployment es un concepto de más alto nivel que gestiona ReplicaSets y
proporciona actualizaciones de forma declarativa de los Pods junto con muchas otras características útiles.
Por lo tanto, se recomienda el uso de Deployments en vez del uso directo de ReplicaSets, a no ser
que se necesite una orquestración personalizada de actualización o no se necesite las actualizaciones en absoluto.</p><p>En realidad, esto quiere decir que puede que nunca necesites manipular los objetos ReplicaSet:
en vez de ello, usa un Deployment, y define tu aplicación en la sección spec.</p><h2 id=ejemplo>Ejemplo</h2><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/frontend.yaml download=controllers/frontend.yaml><code>controllers/frontend.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-frontend-yaml")' title="Copy controllers/frontend.yaml to clipboard"></img></div><div class=includecode id=controllers-frontend-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># modifica las réplicas según tu caso de uso</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>php-redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google_samples/gb-frontend:v3<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Si guardas este manifiesto en un archivo llamado <code>frontend.yaml</code> y lo lanzas en un clúster de Kubernetes,
se creará el ReplicaSet definido y los Pods que maneja.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f http://k8s.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>Puedes ver los ReplicaSets actuales desplegados:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>Y ver el frontend que has creado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME       DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>frontend   <span style=color:#666>3</span>         <span style=color:#666>3</span>         <span style=color:#666>3</span>       6s
</span></span></code></pre></div><p>También puedes comprobar el estado del replicaset:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe rs/frontend
</span></span></code></pre></div><p>Y verás una salida parecida a la siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Name:		frontend
</span></span><span style=display:flex><span>Namespace:	default
</span></span><span style=display:flex><span>Selector:	<span style=color:#b8860b>tier</span><span style=color:#666>=</span>frontend,tier in <span style=color:#666>(</span>frontend<span style=color:#666>)</span>
</span></span><span style=display:flex><span>Labels:		<span style=color:#b8860b>app</span><span style=color:#666>=</span>guestbook
</span></span><span style=display:flex><span>		<span style=color:#b8860b>tier</span><span style=color:#666>=</span>frontend
</span></span><span style=display:flex><span>Annotations:	&lt;none&gt;
</span></span><span style=display:flex><span>Replicas:	<span style=color:#666>3</span> current / <span style=color:#666>3</span> desired
</span></span><span style=display:flex><span>Pods Status:	<span style=color:#666>3</span> Running / <span style=color:#666>0</span> Waiting / <span style=color:#666>0</span> Succeeded / <span style=color:#666>0</span> Failed
</span></span><span style=display:flex><span>Pod Template:
</span></span><span style=display:flex><span>  Labels:       <span style=color:#b8860b>app</span><span style=color:#666>=</span>guestbook
</span></span><span style=display:flex><span>                <span style=color:#b8860b>tier</span><span style=color:#666>=</span>frontend
</span></span><span style=display:flex><span>  Containers:
</span></span><span style=display:flex><span>   php-redis:
</span></span><span style=display:flex><span>    Image:      gcr.io/google_samples/gb-frontend:v3
</span></span><span style=display:flex><span>    Port:       80/TCP
</span></span><span style=display:flex><span>    Requests:
</span></span><span style=display:flex><span>      cpu:      100m
</span></span><span style=display:flex><span>      memory:   100Mi
</span></span><span style=display:flex><span>    Environment:
</span></span><span style=display:flex><span>      GET_HOSTS_FROM:   dns
</span></span><span style=display:flex><span>    Mounts:             &lt;none&gt;
</span></span><span style=display:flex><span>  Volumes:              &lt;none&gt;
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
</span></span><span style=display:flex><span>  ---------    --------    -----    ----                -------------    --------    ------            -------
</span></span><span style=display:flex><span>  1m           1m          <span style=color:#666>1</span>        <span style=color:#666>{</span>replicaset-controller <span style=color:#666>}</span>             Normal      SuccessfulCreate  Created pod: frontend-qhloh
</span></span><span style=display:flex><span>  1m           1m          <span style=color:#666>1</span>        <span style=color:#666>{</span>replicaset-controller <span style=color:#666>}</span>             Normal      SuccessfulCreate  Created pod: frontend-dnjpy
</span></span><span style=display:flex><span>  1m           1m          <span style=color:#666>1</span>        <span style=color:#666>{</span>replicaset-controller <span style=color:#666>}</span>             Normal      SuccessfulCreate  Created pod: frontend-9si5l
</span></span></code></pre></div><p>Y por último, puedes comprobar los Pods que ha arrancado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get Pods
</span></span></code></pre></div><p>Deberías ver la información de cada Pod similar a:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-9si5l   1/1       Running   <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-dnjpy   1/1       Running   <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-qhloh   1/1       Running   <span style=color:#666>0</span>          1m
</span></span></code></pre></div><p>También puedes verificar que la referencia de propietario de dichos pods está puesta al ReplicaSet frontend.
Para ello, obtén el yaml de uno de los Pods ejecutándose:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods frontend-9si5l -o yaml
</span></span></code></pre></div><p>La salida será parecida a esta, donde la información sobre el ReplicaSet aparece en el campo ownerReferences de los metadatos:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  creationTimestamp: 2019-01-31T17:20:41Z
</span></span><span style=display:flex><span>  generateName: frontend-
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    tier: frontend
</span></span><span style=display:flex><span>  name: frontend-9si5l
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: extensions/v1beta1
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    controller: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    kind: ReplicaSet
</span></span><span style=display:flex><span>    name: frontend
</span></span><span style=display:flex><span>    uid: 892a2330-257c-11e9-aecd-025000000001
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=adquisiciones-de-pods-fuera-de-la-plantilla>Adquisiciones de Pods fuera de la plantilla</h2><p>Aunque puedes crear Pods simples sin problemas, se recomienda encarecidamente asegurarse de que dichos Pods no tienen
etiquetas que puedan coincidir con el selector de alguno de tus ReplicaSets.
La razón de esta recomendación es que un ReplicaSet no se limita a poseer los Pods
especificados en su plantilla -- sino que puede adquirir otros Pods como se explicó en secciones anteriores.</p><p>Toma el ejemplo anterior del ReplicaSet frontend, y los Pods especificados en el siguiente manifiesto:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/pods/pod-rs.yaml download=pods/pod-rs.yaml><code>pods/pod-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-rs-yaml")' title="Copy pods/pod-rs.yaml to clipboard"></img></div><div class=includecode id=pods-pod-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:2.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:1.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Como estos Pods no tienen un Controlador (o cualquier otro objeto) como referencia de propietario
y como además su selector coincide con el del ReplicaSet frontend, este último los terminará adquiriendo de forma inmediata.</p><p>Supón que creas los Pods después de que el ReplicaSet frontend haya desplegado los suyos
para satisfacer su requisito de cuenta de réplicas:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f http://k8s.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>Los nuevos Pods serán adquiridos por el ReplicaSet, e inmediatamente terminados ya que
el ReplicaSet estaría por encima del número deseado.</p><p>Obtener los Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get Pods
</span></span></code></pre></div><p>La salida muestra que los nuevos Pods se han terminado, o están en el proceso de terminarse:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY   STATUS        RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-9si5l   1/1     Running       <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-dnjpy   1/1     Running       <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-qhloh   1/1     Running       <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>pod2             0/1     Terminating   <span style=color:#666>0</span>          4s
</span></span></code></pre></div><p>Si creas primero los Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f http://k8s.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>Y entonces creas el ReplicaSet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f http://k8s.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>Verás que el ReplicaSet ha adquirido dichos Pods y simplemente ha creado tantos nuevos
como necesarios para cumplir con su especificación hasta que el número de
sus nuevos Pods y los originales coincidan con la cuenta deseado. Al obtener los Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get Pods
</span></span></code></pre></div><p>Veremos su salida:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-pxj4r   1/1     Running   <span style=color:#666>0</span>          5s
</span></span><span style=display:flex><span>pod1             1/1     Running   <span style=color:#666>0</span>          13s
</span></span><span style=display:flex><span>pod2             1/1     Running   <span style=color:#666>0</span>          13s
</span></span></code></pre></div><p>De esta forma, un ReplicaSet puede poseer un conjunto no homogéneo de Pods</p><h2 id=escribir-un-manifiesto-de-replicaset>Escribir un manifiesto de ReplicaSet</h2><p>Al igual que con el esto de los objeto de la API de Kubernetes, un ReplicaSet necesita los campos
<code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>. Para los ReplicaSets, el tipo es siempre ReplicaSet.
En la versión 1.9 de Kubernetes, la versión <code>apps/v1</code> de la API en un tipo ReplicaSet es la versión actual y está habilitada por defecto.
La versión <code>apps/v1beta2</code> de la API se ha desaprobado.
Consulta las primeras líneas del ejemplo <code>frontend.yaml</code> como guía.</p><p>Un ReplicaSet también necesita una <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>sección <code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/Pods/pod-overview/#pod-templates>plantilla pod</a> que es
también necesita obligatoriamente tener etiquetas definidas. En nuestro ejemplo <code>frontend.yaml</code> teníamos una etiqueta: <code>tier: frontend</code>.
Lleva cuidado de que no se entremezcle con los selectores de otros controladores, no sea que traten de adquirir este Pod.</p><p>Para el campo de <a href=/docs/concepts/workloads/Pods/pod-lifecycle/#restart-policy>regla de reinicio</a> de la plantilla,
<code>.spec.template.spec.restartPolicy</code>, el único valor permitido es <code>Always</code>, que es el valor predeterminado.</p><h3 id=selector-de-pod>Selector de Pod</h3><p>El campo <code>.spec.selector</code> es un <a href=/docs/concepts/overview/working-with-objects/labels/>selector de etiqueta</a>.
Como se explicó <a href=#how-a-replicaset-works>anteriormente</a>, estas son las etiquetas que se usan para
identificar los Pods potenciales a adquirir. En nuestro ejemplo <code>frontend.yaml</code>, el selector era:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>matchLabels:
</span></span><span style=display:flex><span>	tier: frontend
</span></span></code></pre></div><p>El el ReplicaSet, <code>.spec.template.metadata.labels</code> debe coincidir con <code>spec.selector</code>, o será
rechazado por la API.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Cuando 2 ReplicaSets especifican el mismo campo <code>.spec.selector</code>, pero los campos
<code>.spec.template.metadata.labels</code> y <code>.spec.template.spec</code> diferentes, cada ReplicaSet
ignora los Pods creados por el otro ReplicaSet.</div><h3 id=réplicas>Réplicas</h3><p>Puedes configurar cuántos Pods deberían ejecutarse de forma concurrente indicando el campo <code>.spec.replicas</code>.
El ReplicaSet creará/eliminará sus Pods para alcanzar este número.</p><p>Si no indicas el valor del campo <code>.spec.replicas</code>, entonces por defecto se inicializa a 1.</p><h2 id=trabajar-con-replicasets>Trabajar con ReplicaSets</h2><h3 id=eliminar-un-replicaset-y-sus-pods>Eliminar un ReplicaSet y sus Pods</h3><p>Para eliminar un ReplicaSet y todos sus Pods, utiliza el comando <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>.
El <a href=/docs/concepts/workloads/controllers/garbage-collection/>Recolector de basura</a> eliminará automáticamente
todos los Pods subordinados por defecto.</p><p>Cuando se usa la API REST o la librería <code>client-go</code>, se debe poner el valor de <code>propagationPolicy</code> a <code>Background</code> o
<code>Foreground</code> en la opción -d.
Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><h3 id=eliminar-sólo-un-replicaset>Eliminar sólo un ReplicaSet</h3><p>Se puede eliminar un ReplicaSet sin afectar a ninguno de sus Pods usando el comando <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a> con la opción <code>--cascade=false</code>.
Cuando se usa la API REST o la librería <code>client-go</code>, se debe poner <code>propagationPolicy</code> a <code>Orphan</code>.
Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>Una vez que se ha eliminado el original, se puede crear un nuevo ReplicaSet para sustituirlo.
Mientras el viejo y el nuevo <code>.spec.selector</code> sean el mismo, el nuevo adoptará a los viejos Pods.
Sin embargo, no se esforzará en conseguir que los Pods existentes coincidan con una plantilla pod nueva, diferente.
Para actualizar dichos Pods a la nueva especificación de forma controlada,
usa una <a href=#rolling-updates>actualización en línea</a>.</p><h3 id=aislar-pods-de-un-replicaset>Aislar Pods de un ReplicaSet</h3><p>Es posible aislar Pods de un ReplicaSet cambiando sus etiquetas. Esta técnica puede usarse
para eliminar Pods de un servicio para poder depurar, recuperar datos, etc. Los Pods
que se eliminar de esta forma serán sustituidos de forma automática (siempre que el
número de réplicas no haya cambiado).</p><h3 id=escalar-un-replicaset>Escalar un ReplicaSet</h3><p>Se puede aumentar o reducir fácilmente un ReplicaSet simplemente actualizando el campo <code>.spec.replicas</code>.
El controlador del ReplicaSet se asegura de que el número deseado de Pods con un selector
de etiquetas coincidente está disponible y operacional.</p><h3 id=replicaset-como-blanco-de-un-horizontal-pod-autoscaler>ReplicaSet como blanco de un Horizontal Pod Autoscaler</h3><p>Un ReplicaSet puede también ser el blanco de un
<a href=/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscalers (HPA)</a>. Esto es,
un ReplicaSet puede auto-escalarse mediante un HPA. Aquí se muestra un ejemplo de HPA dirigido
al ReplicaSet que creamos en el ejemplo anterior.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/hpa-rs.yaml download=controllers/hpa-rs.yaml><code>controllers/hpa-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-hpa-rs-yaml")' title="Copy controllers/hpa-rs.yaml to clipboard"></img></div><div class=includecode id=controllers-hpa-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>autoscaling/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>HorizontalPodAutoscaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-scaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scaleTargetRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>minReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>maxReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>targetCPUUtilizationPercentage</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Si guardas este manifiesto en un archivo <code>hpa-rs.yaml</code> y lo lanzas contra el clúster de Kubernetes,
debería crear el HPA definido que auto-escala el ReplicaSet destino dependiendo del uso
de CPU de los Pods replicados.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
</span></span></code></pre></div><p>Alternativamente, puedes usar el comando <code>kubectl autoscale</code> para conseguir el mismo objetivo
(¡y mucho más fácil!)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale rs frontend --max<span style=color:#666>=</span><span style=color:#666>10</span>
</span></span></code></pre></div><h2 id=alternativas-al-replicaset>Alternativas al ReplicaSet</h2><h3 id=deployment-recomendado>Deployment (recomendado)</h3><p>Un<a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> es un objeto que puede poseer ReplicaSets
y actualizar a estos y a sus Pods mediante actualizaciones en línea declarativas en el servidor.
Aunque que los ReplicaSets puede usarse independientemente, hoy en día se usan principalmente a través de los Deployments
como el mecanismo para orquestrar la creación, eliminación y actualización de los Pods.
Cuando usas Deployments no tienes que preocuparte de gestionar los ReplicaSets que crean.
Los Deployments poseen y gestionan sus ReplicaSets.
Por tanto, se recomienda que se use Deployments cuando se quiera ReplicaSets.</p><h3 id=pods-simples>Pods simples</h3><p>A diferencia del caso en que un usuario creaba Pods de forma directa, un ReplicaSet sustituye los Pods que se eliminan
o se terminan por la razón que sea, como en el caso de un fallo de un nodo o
una intervención disruptiva de mantenimiento, como una actualización de kernel.
Por esta razón, se recomienda que se use un ReplicaSet incluso cuando la aplicación
sólo necesita un único Pod. Entiéndelo de forma similar a un proceso supervisor,
donde se supervisa múltiples Pods entre múltiples nodos en vez de procesos individuales
en un único nodo. Un ReplicaSet delega los reinicios del contenedor local a algún agente
del nodo (por ejemplo, Kubelet o Docker).</p><h3 id=job>Job</h3><p>Usa un <a href=/docs/concepts/jobs/run-to-completion-finite-workloads/><code>Job</code></a> en vez de un ReplicaSet para
aquellos Pods que se esperan que terminen por ellos mismos (esto es, trabajos por lotes).</p><h3 id=daemonset>DaemonSet</h3><p>Usa un <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> en vez de un ReplicaSet para aquellos
Pods que proporcionan funcionalidad a nivel de servidor, como monitorización de servidor o
logging de servidor. Estos Pods tienen un ciclo de vida asociado al del servidor mismo:
el Pod necesita ejecutarse en el servidor antes de que los otros Pods comiencen, y es seguro
que terminen cuando el servidor esté listo para ser reiniciado/apagado.</p><h3 id=replicationcontroller>ReplicationController</h3><p>Los ReplicaSets son los sucesores de los <a href=/docs/concepts/workloads/controllers/replicationcontroller/><em>ReplicationControllers</em></a>.
Los dos sirven al mismo propósito, y se comportan de forma similar, excepto porque un ReplicationController
no soporta los requisitos del selector basado en conjunto, como se describe en la <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>guía de usuario de etiquetas</a>.
Por ello, se prefiere los ReplicaSets a los ReplicationControllers.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-27f1331d515d95f76aa1156088b4ad91>4.2.2 - ReplicationController</h1><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> hoy en día la forma recomendada de configurar la replicación es con un <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> que configura un <a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a>.</div><p>Un <em>ReplicationController</em> garantiza que un número determinado de réplicas se estén ejecutando
en todo momento. En otras palabras, un ReplicationController se asegura que un pod o un conjunto homogéneo de pods
siempre esté arriba y disponible.</p><h2 id=cómo-funciona-un-replicationcontroller>Cómo Funciona un ReplicationController</h2><p>Si hay muchos pods, el ReplicationController termina los pods extra. Si hay muy pocos, el
ReplicationController arranca más pods. A difrencia de los pods creados manualmente, los pods mantenidos por un
ReplicationController se sustituyen de forma automática si fallan, se borran, o se terminan.
Por ejemplo, tus pods se re-crean en un nodo durante una intervención disruptiva de mantenimiento como una actualización del kernel.
Por esta razón, deberías usar un ReplicationController incluso cuando tu aplicación sólo necesita
un único pod. Un ReplicationController es parecido a un supervisor de procesos,
pero en vez de supervisar procesos individuales en un único nodo,
el ReplicationController supervisa múltiples pods entre múltiples nodos.</p><p>A menudo nos referimos a un ReplicationController de forma abreviada como "rc" o "rcs", así como
atajo en los comandos de kubectl.</p><p>Un caso simple es crear un objeto ReplicationController para ejecutar de manera fiable una instancia
de un Pod indefinidamente. Un caso de uso más complejo es ejecutar varias réplicas idénticas
de un servicio replicado, como los servidores web.</p><h2 id=ejecutar-un-ejemplo-de-replicationcontroller>Ejecutar un ejemplo de ReplicationController</h2><p>Esta configuración de un ReplicationController de ejemplo ejecuta tres copias del servidor web nginx.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/replication.yaml download=controllers/replication.yaml><code>controllers/replication.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-replication-yaml")' title="Copy controllers/replication.yaml to clipboard"></img></div><div class=includecode id=controllers-replication-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicationController<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Ejecuta el ejemplo descargando el archivo de ejemplo y ejecutando este comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
</span></span></code></pre></div><pre tabindex=0><code>replicationcontroller/nginx created
</code></pre><p>Comprueba el estado del ReplicationController con este comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe replicationcontrollers/nginx
</span></span></code></pre></div><pre tabindex=0><code>Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
</code></pre><p>Como se puede observar, se han creado tres pods, pero ninguno se está ejecutándose todavía,
puede que porque la imagen todavía se está descargando.
Unos momentos después, el mismo comando puede que muestre:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Pods Status:    <span style=color:#666>3</span> Running / <span style=color:#666>0</span> Waiting / <span style=color:#666>0</span> Succeeded / <span style=color:#666>0</span> Failed
</span></span></code></pre></div><p>Para listar todos los pods que pertenecen al ReplicationController de forma legible,
puedes usar un comando como el siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>pods</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl get pods --selector<span style=color:#666>=</span><span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx --output<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>={</span>.items..metadata.name<span style=color:#666>}</span><span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><pre tabindex=0><code>nginx-3ntk0 nginx-4ok8v nginx-qrm3m
</code></pre><p>Como se puede ver, el selector es el mismo que el selector del ReplicationController (mostrado en la salida de
<code>kubectl describe</code>), y con una forma diferente a lo definido en el archivo <code>replication.yaml</code>.
La opción <code>--output=jsonpath</code> especifica una expresión que simplemente muestra el nombre
de cada pod en la lista devuelta.</p><h2 id=escribir-una-especificación-de-replicationcontroller>Escribir una especificación de ReplicationController</h2><p>Al igual que con el resto de configuraciones de Kubernetes, un ReplicationController necesita los campos <code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>.
Para información general acerca del trabajo con archivos de configuración, ver la <a href=/docs/concepts/overview/object-management-kubectl/overview/>gestión de objetos</a>.</p><p>Un ReplicationController también necesita un <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>sección <code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>El campo <code>.spec.template</code> es el único campo requerido de <code>.spec</code>.</p><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/pods/pod-overview/#pod-templates>plantilla pod</a>.
Tiene exactamente el mismo esquema que un <a href=/docs/concepts/workloads/pods/pod/>pod</a>, excepto por el hecho de que está anidado y no tiene los campos <code>apiVersion</code> ni <code>kind</code>.</p><p>Además de los campos obligatorios de un Pod, una plantilla pod de un ReplicationController debe especificar las etiquetas apropiadas
y la regla de reinicio apropiada. En el caso de las etiquetas, asegúrate que no se entremezclan con otros controladores. Ver el <a href=#pod-selector>selector de pod</a>.</p><p>Sólo se permite el valor <code>Always</code> para el campo <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>.spec.template.spec.restartPolicy</code></a>,
que es el valor predeterminado si no se indica.</p><p>Para los reinicios locales de los contenedores, los ReplicationControllers delegan en los agentes del nodo,
por ejmplo el <a href=/docs/admin/kubelet/>Kubelet</a> o Docker.</p><h3 id=etiquetas-en-los-replicationcontroller>Etiquetas en los ReplicationController</h3><p>los ReplicationController puede tener sus propias (<code>.metadata.labels</code>). Normalmente, se indicaría dichas etiquetas
con los mismos valores que el campo <code>.spec.template.metadata.labels</code>; si el campo <code>.metadata.labels</code> no se indica,
entonces se predetermina al valor de <code>.spec.template.metadata.labels</code>. Sin embargo, se permite que sean diferentes,
y el valor de <code>.metadata.labels</code> no afecta al comportamiento del ReplicationController.</p><h3 id=selector-de-pod>Selector de Pod</h3><p>El campo <code>.spec.selector</code> es un <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>selector de etiqueta</a>. Un ReplicationController
gestiona todos los pods con etiquetas que coinciden con el selector. No distingue entre
pods que creó o eliminó, y pods que otra persona o proceso creó o eliminó. Esto permite sustituir al ReplicationController sin impactar a ninguno de sus pods que se esté ejecutando.</p><p>Si se indica, el valor de <code>.spec.template.metadata.labels</code> debe ser igual al de <code>.spec.selector</code>, o será rechazado por la API.
Si no se indica el valor de <code>.spec.selector</code>, se tomará como predeterminado el de <code>.spec.template.metadata.labels</code>.</p><p>Tampoco deberías crear ningún pod cuyas etiquetas coincidan con las de este selector, ni directamente con
otro ReplicationController, ni con otro controlador como un Job. Si lo haces, el
ReplicationController piensa que el creó también los otros pods. Kubernetes no te impide hacerlo.</p><p>Si al final terminas con múltiples controladores que tienen selectores que se entremezclan,
tendrás que gestionar la eliminación tú mismo (ver <a href=#working-with-replicationcontrollers>abajo</a>).</p><h3 id=múltiples-réplicas>Múltiples Réplicas</h3><p>Puedes configurar cuántos pods deberían ejecutarse de forma concurrente poniendo el valor de <code>.spec.replicas</code> al número
de pods que te gustaría tener ejecutándose a la vez. El número de ejecuciones en cualquier momento puede que sea superior
o inferior, dependiendo de si las réplicas se han incrementado o decrementado, o si un pod se ha apagado de forma controlada,
y su sustituto arranca más pronto.</p><p>Si no se indica el valor de <code>.spec.replicas</code>, entonces se predetermina a 1.</p><h2 id=trabajar-con-replicationcontrollers>Trabajar con ReplicationControllers</h2><h3 id=eliminar-un-replicationcontroller-y-sus-pods>Eliminar un ReplicationController y sus Pods</h3><p>Para eliminar un ReplicationController y todos sus pods, usa el comando <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>. Kubectl reducirá el ReplicationController a cero y esperará
que elimine cada pod antes de eliminar al ReplicationController mismo. Si este comando kubectl
se interrumpe, puede ser reiniciado.</p><p>Cuando uses la API REST o la librería Go, necesitas realizar los pasos de forma explícita (reducir las réplicas a cero,
esperar a que se eliminen los pods, y entonces eliminar el ReplicationController).</p><h3 id=eliminar-sólo-el-replicationcontroller>Eliminar sólo el ReplicationController</h3><p>Puedes eliminar un ReplicationController sin impactar a ninguno de sus Pods.</p><p>Usando kubectl, indica la opción <code>--cascade=false</code> en el comando <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>.</p><p>Cuando uses la API REST o la librería Go, simplemente elimina objeto ReplicationController.</p><p>Una vez que el objeto original se ha eliminado, puedes crear un nuevo ReplicationController para sustituirlo.
Mientras el viejo y el nuevo valor del <code>.spec.selector</code> sea el mismo, el nuevo adoptará a los viejos pods.
Sin embargo, no se molestará en hacer que los pods actuales coincidan con una plantilla pod nueva, diferente.
Para actualizar los pods con una nueva especificación de forma controlada, utiliza la <a href=#rolling-updates>actualización en línea</a>.</p><h3 id=aislar-pods-de-un-replicationcontroller>Aislar pods de un ReplicationController</h3><p>Se puede aislar Pods del conjunto destino de un ReplicationController cambiando sus etiquetas.
Esta técnica puede usarse para eliminar pods de un servicio para poder depurarlos, recuperar datos, etc.
Los Pods que se eliminan de esta forma serán sustituidos de forma automática (asumiendo que el número de réplicas no ha cambiado tampoco).</p><h2 id=patrones-comunes-de-uso>Patrones comunes de uso</h2><h3 id=reprogramación>Reprogramación</h3><p>Como se comentó arriba, cuando tienes 1 pod que quieres mantener ejecutándose, o 1000, un ReplicationController se asegura de que el número indicado de pods exista,
incluso si falla un nodo o se termina algún pod (por ejemplo, debido a alguna acción de otro agente de control).</p><h3 id=escalado>Escalado</h3><p>El ReplicationController facilita el escalado del número de réplicas tanto para su aumento como para su disminución,
bien manualmente o mediante un agente de auto-escalado, simplemente actualizando el campo <code>replicas</code>.</p><h3 id=actualizaciones-en-línea>Actualizaciones en línea</h3><p>El ReplicationController se ha diseñado para facilitar las actualizaciones en línea de un servicio mediante la sustitución de sus pods uno por uno.</p><p>Cómo se explicó en <a href=http://issue.k8s.io/1353>#1353</a>, la estrategia recomendada es crear un nuevo ReplicationController con 1 réplica,
escalar el nuevo (+1) y el viejo (-1) controlador uno por uno, y entonces eliminar el viejo controlador una vez que alcanza las 0 réplicas.
Esto actualiza de forma predecible el conjunto de pods independientemente de que se produzcan fallos inesperados.</p><p>De forma ideal, el controlador de actualización en línea tendrá en cuenta si la aplicación está lista, y
se asegurará de que un número suficiente de pods está en servicio en todo momento.</p><p>Los dos ReplicationControllers necesitarán crear pods con al menos una etiqueta diferenciadora, como la etiqueta de imagen del contenedor primario del pod,
ya que las actualizaciones de imagen son las que normalmente desencadenan las actualizaciones en línea.</p><p>La actualización en línea se implementa a través de la herramienta cliente mediante
<a href=/docs/reference/generated/kubectl/kubectl-commands#rolling-update><code>kubectl rolling-update</code></a>.
Echa un vistazo a la <a href=/docs/tasks/run-application/rolling-update-replication-controller/>tarea <code>kubectl rolling-update</code></a> para más ejemplos concretos.</p><h3 id=múltiples-operaciones-de-despliegue>Múltiples operaciones de despliegue</h3><p>Además de llevar a cabo múltiples despliegues de una aplicación cuando una actualización en línea está en progreso,
es común ejecutar varios despliegues durante un período extendido de tiempo, o incluso de forma contínua, usando múltiples operaciones de despliegue. Dichas operaciones se diferenciarían por etiquetas.</p><p>Por ejemplo, un servicio puede que exponga todos los pods con etiquetas <code>tier in (frontend), environment in (prod)</code>. Ahora digamos que tenemos 10 pods replicados que forman este grupo.
Pero queremos poder desplegar una nueva versión 'canary' de este component. Se podría configurar un ReplicationController con el valor de <code>replicas</code> puesto a 9 para la mayor parte de las réplicas,
con etiquetas <code>tier=frontend, environment=prod, track=stable</code>, y otro ReplicationController con el valor de <code>replicas</code> puesto a 1 para el 'canary',
con las etiquetas <code>tier=frontend, environment=prod, track=canary</code>. Así el servicio cubriría tanto los pods canary como el resto.
Pero también es posible trastear con los ReplicationControllers de forma separada para probar cosas, monitorizar los resultados, etc.</p><h3 id=usar-replicationcontrollers-con-servicios>Usar ReplicationControllers con servicios</h3><p>Un único servicio puede exponer múltiples ReplicationControllers, de forma que, por ejemplo, algo de tráfico
vaya a la versión vieja, y otro tanto vaya a la versión nueva.</p><p>Un ReplicationController nunca se terminará por sí mismo, pero tampoco se espera que se ejecute permanentemente como los servicios.
Los servicios puede que estén compuestos de pods controlados por múltiples ReplicationControllers,
y se espera que muchos ReplicationControllers se creen y se destruyan durante el ciclo de vida de un servicio (por ejemplo,
para realizar una actualización de los pods que ejecutan el servicio). Ambos servicios mismos y sus clientes deberían permanecer
ajenos a los ReplicationControllers que mantienen los pods que proporcionan los servicios.</p><h2 id=escribir-aplicaciones-que-se-repliquen>Escribir aplicaciones que se repliquen</h2><p>Los Pods creados por un ReplicationController están pensados para que sean intercambiables y semánticamente idénticos,
aunque sus configuraciones puede que sean heterogéneas a lo largo del tiempo. Este es un ajuste obvio para los servidores sin estado replicados,
pero los ReplicationControllers también pueden utilizarse para mantener la disponibilidad de aplicaciones que se elijen por un maestro, las particionadas, y las de grupos de trabajadores.
Dichas aplicaciones deberían usar los mecanismos de asignación dinámica de trabajo, como las <a href=https://www.rabbitmq.com/tutorials/tutorial-two-python.html>colas de trabajo RabbitMQ</a>,
en vez de la personalización estática/de una sola vez en la configuración de cada pod,
ya que se considera un anti-patrón. Cualquier personalización de pod que se haga, como el calibrado vertical automático de recursos (por ejemplo, cpu o memoria),
debería realizarse a través de otro proceso de controlador en línea, no con el mismo ReplicationController.</p><h2 id=responsabilidades-de-un-replicationcontroller>Responsabilidades de un ReplicationController</h2><p>El ReplicationController simplemente garantiza que el número deseado de pods coincide con su selector de etiqueta y que son operacionales.
Actualmente, sólo los pods que han terminado se excluyen de la cuenta. En el futuro, la <a href=http://issue.k8s.io/620>disponibilidad</a> y otra información disponible en el sistema
se tendrá en cuenta, se añadirá más controles sobre la regla de sussitución, y se está planificando
emitir eventos que podrían ser aprovechados por clientes externos para implementar reglas complejas de sustitución y escalado de forma arbitraria.</p><p>El ReplicationController está siempre condicionado a esta reducida responsabilidad.
Él mismo no llevará a cabo ni pruebas de estar listo ni vivo. En vez de aplicar el auto-escalado,
se pretende que este sea realizado por un auto-escalador externo (como se vio en <a href=http://issue.k8s.io/492>#492</a>), que sería el encargado de cambiar su campo <code>replicas</code>.
No se añadirá reglas de programación (por ejemplo, <a href=http://issue.k8s.io/367#issuecomment-48428019>propagación</a>) al ReplicationController.
Ni se debería validar que los pods controlados coincidan con la plantilla actual especificada, ya que eso obstruiría el auto-calibrado y otros procesos automáticos.
De forma similar, los vencimientos de término, las dependencias de orden, la extensión de la configuración, y otras características se aplican en otro lado.
Incluso se plantea excluir el mecanismo de creación de pods a granel (<a href=http://issue.k8s.io/170>#170</a>).</p><p>El ReplicationController está pensado para ser una primitiva de bloques is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The "macro" operations currently supported by kubectl (run, scale, rolling-update) are proof-of-concept examples of this. For instance, we could imagine something like <a href=http://techblog.netflix.com/2012/06/asgard-web-based-cloud-management-and.html>Asgard</a> managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.</p><h2 id=objeto-api>Objeto API</h2><p>El ReplicationController es un recurso de alto nivel en la API REST de Kubernetes. Más detalles acerca del
objeto API se pueden encontrar aquí:
<a href=/docs/reference/generated/kubernetes-api/v1.25/#replicationcontroller-v1-core>Objeto API ReplicationController</a>.</p><h2 id=alternativas-al-replicationcontroller>Alternativas al ReplicationController</h2><h3 id=replicaset>ReplicaSet</h3><p>El <a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a> es el ReplicationController de nueva generación que soporta el nuevo <a href=/docs/concepts/overview/working-with-objects/labels/#set-based-requirement>selector de etiqueta basado en conjunto</a>.
Se usa principalmente por el <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> como un mecanismo para orquestrar la creación de pods, la eliminación y las actualizaciones.
Nótese que se recomienda usar Deployments en vez de directamente usar los ReplicaSets, a menos que necesites una orquestración personalizada de actualizaciones o no quieras actualizaciones en absoluto.</p><h3 id=deployment-recomendado>Deployment (Recomendado)</h3><p>El <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> es un objeto de alto nivel de la API que actualiza sus ReplicaSets subyacenetes y sus Pods
de forma similar a cómo lo hace el comando <code>kubectl rolling-update</code>. Se recomienda el uso de Deployments si se quiere esta functionalidad de actualización en línea,
porque a diferencia del comando <code>kubectl rolling-update</code>, son declarativos, se ejecutan del lado del servidor, y tienen características adicionales.</p><h3 id=pods-simples>Pods simples</h3><p>A diferencia del caso en que un usuario ha creado directamente pods, un ReplicationController sustituye los pods que han sido eliminador o terminados por cualquier motivo,
como en el caso de un fallo de un nodo o una intervención disruptiva de mantenimiento, como la actualización del kernel.
Por esta razón, se recomienda que se usa un ReplicationController incluso si tu aplicación sólo necesita un único pod.
Piensa que es similar a un supervisor de proceso, sólo que supervisa múltiples pods entre múltiples nodos en vez de
procesos individuales en un único nodo. Un ReplicationController delega los reinicios locales de
los contenedores a algún agente del nodo (por ejemplo, Kubelet o Docker).</p><h3 id=job>Job</h3><p>Usa un <a href=/docs/concepts/jobs/run-to-completion-finite-workloads/><code>Job</code></a> en vez de un ReplicationController para aquellos pods que se espera que terminen por sí mismos
(esto es, trabajos por lotes).</p><h3 id=daemonset>DaemonSet</h3><p>Usa un <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> en vez de un ReplicationController para aquellos pods que proporcionan
una función a nivel de servidor, como la monitorización o el loggin de servidor. Estos pods tienen un ciclo de vida que está asociado
al del servidor: el pod necesita ejecutarse en el servidor antes que los otros pods arranquen, y es seguro
terminarlo cuando el servidor está listo para reiniciarse/apagarse.</p><h2 id=para-más-información>Para más información</h2><p>Lee <a href=/docs/tutorials/stateless-application/run-stateless-ap-replication-controller/>Ejecutar Aplicaciones sin Estado con un ReplicationController</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a2dc0393e0c4079e1c504b6429844e86>4.2.3 - Deployment</h1><p>Un controlador de <em>Deployment</em> proporciona actualizaciones declarativas para los <a href=/docs/concepts/workloads/pods/pod/>Pods</a> y los
<a href=/docs/concepts/workloads/controllers/replicaset/>ReplicaSets</a>.</p><p>Cuando describes el <em>estado deseado</em> en un objeto Deployment, el controlador del Deployment se encarga de cambiar el estado actual al estado deseado de forma controlada.
Puedes definir Deployments para crear nuevos ReplicaSets, o eliminar Deployments existentes y adoptar todos sus recursos con nuevos Deployments.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> No deberías gestionar directamente los ReplicaSets que pertenecen a un Deployment.
Todos los casos de uso deberían cubrirse manipulando el objeto Deployment.
Considera la posibilidad de abrir un incidente en el repositorio principal de Kubernetes si tu caso de uso no está soportado por el motivo que sea.</div><h2 id=casos-de-uso>Casos de uso</h2><p>A continuación se presentan los casos de uso típicos de los Deployments:</p><ul><li><a href=#creating-a-deployment>Crear un Deployment para desplegar un ReplicaSet</a>. El ReplicaSet crea los Pods en segundo plano. Comprueba el estado del despliegue para comprobar si es satisfactorio o no.</li><li><a href=#updating-a-deployment>Declarar el nuevo estado de los Pods</a> actualizando el PodTemplateSpec del Deployment. Ello crea un nuevo ReplicaSet y el Deployment gestiona el cambio de los Pods del viejo ReplicaSet al nuevo de forma controlada. Cada nuevo ReplicaSet actualiza la revisión del Deployment.</li><li><a href=#rolling-back-a-deployment>Retroceder a una revisión anterior del Deployment</a> si el estado actual de un Deployment no es estable. Cada retroceso actualiza la revisión del Deployment.</li><li><a href=#scaling-a-deployment>Escalar horizontalmente el Deployment para soportar más carga</a>.</li><li><a href=#pausing-and-resuming-a-deployment>Pausar el Deployment</a> para aplicar múltiples arreglos a su PodTemplateSpec y, a continuación, reanúdalo para que comience un nuevo despliegue.</li><li><a href=#deployment-status>Usar el estado del Deployment</a> como un indicador de que el despliegue se ha atascado.</li><li><a href=#clean-up-policy>Limpiar los viejos ReplicaSets</a> que no necesites más.</li></ul><h2 id=crear-un-deployment>Crear un Deployment</h2><p>El siguiente ejemplo de un Deployment crea un ReplicaSet para arrancar tres Pods con <code>nginx</code>:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/nginx-deployment.yaml download=controllers/nginx-deployment.yaml><code>controllers/nginx-deployment.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-nginx-deployment-yaml")' title="Copy controllers/nginx-deployment.yaml to clipboard"></img></div><div class=includecode id=controllers-nginx-deployment-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.7.9<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>En este ejemplo:</p><ul><li><p>Se crea un Deployment denominado <code>nginx-deployment</code>, indicado a través del campo <code>.metadata.name</code>.</p></li><li><p>El Deployment crea tres Pods replicados, indicado a través del campo <code>replicas</code>.</p></li><li><p>El campo <code>selector</code> define cómo el Deployment identifica los Pods que debe gestionar.
En este caso, simplemente seleccionas una etiqueta que se define en la plantilla Pod (<code>app: nginx</code>).
Sin embargo, es posible definir reglas de selección más sofisticadas,
siempre que la plantilla Pod misma satisfaga la regla.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> <code>matchLabels</code> es un mapa de entradas {clave,valor}. Una entrada simple {clave,valor} en el mapa <code>matchLabels</code>
es equivalente a un elemento de <code>matchExpressions</code> cuyo campo sea la "clave", el operador sea "In",
y la matriz de valores contenga únicamente un "valor". Todos los requisitos se concatenan con AND.</div></li><li><p>El campo <code>template</code> contiene los siguientes sub-campos:</p><ul><li>Los Pods se etiquetan como <code>app: nginx</code> usando el campo <code>labels</code>.</li><li>La especificación de la plantilla Pod, o el campo <code>.template.spec</code>, indica
que los Pods ejecutan un contenedor, <code>nginx</code>, que utiliza la versión 1.7.9 de la imagen de <code>nginx</code> de
<a href=https://hub.docker.com/>Docker Hub</a>.</li><li>Crea un contenedor y lo llamar <code>nginx</code> usando el campo <code>name</code>.</li><li>Ejecuta la imagen <code>nginx</code> en su versión <code>1.7.9</code>.</li><li>Abre el puerto <code>80</code> para que el contenedor pueda enviar y recibir tráfico.</li></ul></li></ul><p>Para crear este Deployment, ejecuta el siguiente comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes indicar el parámetro <code>--record</code> para registrar el comando ejecutado en la anotación de recurso <code>kubernetes.io/change-cause</code>.
Esto es útil para futuras introspecciones, por ejemplo para comprobar qué comando se ha ejecutado en cada revisión del Deployment.</div><p>A continuación, ejecuta el comando <code>kubectl get deployments</code>. La salida debe ser parecida a la siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
</span></span><span style=display:flex><span>nginx-deployment   3/3     <span style=color:#666>3</span>            <span style=color:#666>3</span>           1s  
</span></span></code></pre></div><p>Cuando inspeccionas los Deployments de tu clúster, se muestran los siguientes campos:</p><ul><li><code>NAME</code> enumera los nombre de los Deployments del clúster.</li><li><code>READY</code> muestra cuántas réplicas de la aplicación están disponibles para sus usuarios. Sigue el patrón número de réplicas <code>listas/deseadas</code>.</li><li><code>UP-TO-DATE</code> muestra el número de réplicas que se ha actualizado para alcanzar el estado deseado.</li><li><code>AVAILABLE</code> muestra cuántas réplicas de la aplicación están disponibles para los usuarios.</li><li><code>AGE</code> muestra la cantidad de tiempo que la aplicación lleva ejecutándose.</li></ul><p>Nótese cómo los valores de cada campo corresponden a los valores de la especificación del Deployment:</p><ul><li>El número de réplicas deseadas es 3 de acuerdo con el campo <code>.spec.replicas</code>.</li><li>El número de réplicas actuales es 0 de acuerdo con el campo <code>.status.replicas</code>.</li><li>El número de réplicas actualizadas es 0 de acuerdo con el campo <code>.status.updatedReplicas</code>.</li><li>El número de réplicas disponibles es 0 de acuerdo con el campo <code>.status.availableReplicas</code>.</li></ul><p>Si deseamos obtener más información del Deployment utilice el parámetro '-o wide', ejecutando el comando 'kubectl get deployments -o wide'. La salida será parecida a la siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
</span></span><span style=display:flex><span>nginx-deployment   3/3     <span style=color:#666>3</span>            <span style=color:#666>3</span>           10s   nginx        nginx:1.7.9   <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</span></span></code></pre></div><p>Ejecutando el comando anterior se muestran los siguientes campos adicionales:</p><ul><li><code>CONTAINERS</code> muestra los nombres de los contenedores declarados en <code>.spec.template.spec.containers.[name]</code>.</li><li><code>IMAGES</code> muestra los nombres de las imágenes declaradas en <code>.spec.template.spec.containers.[image]</code>.</li><li>'SELECTOR' muestra el Label selector que se declaró en matchLabels o matchExpressions.</li></ul><p>Para ver el estado del Deployment, ejecuta el comando <code>kubectl rollout status deployment.v1.apps/nginx-deployment</code>. Este comando devuelve el siguiente resultado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Waiting <span style=color:#a2f;font-weight:700>for</span> rollout to finish: <span style=color:#666>2</span> out of <span style=color:#666>3</span> new replicas have been updated...
</span></span><span style=display:flex><span>deployment <span style=color:#b44>&#34;nginx-deployment&#34;</span> successfully rolled out
</span></span></code></pre></div><p>Ejecuta de nuevo el comando <code>kubectl get deployments</code> unos segundos más tarde:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
</span></span><span style=display:flex><span>nginx-deployment   3/3     <span style=color:#666>3</span>            <span style=color:#666>3</span>           18s  
</span></span></code></pre></div><p>Fíjate que el Deployment ha creado todas las tres réplicas, y que todas las réplicas están actualizadas (contienen
la última plantilla Pod) y están disponibles (el estado del Pod tiene el valor Ready al menos para el campo <code>.spec.minReadySeconds</code> del Deployment).</p><p>Para ver el ReplicaSet (<code>rs</code>) creado por el Deployment, ejecuta el comando <code>kubectl get rs</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                          DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>nginx-deployment-75675f5897   <span style=color:#666>3</span>         <span style=color:#666>3</span>         <span style=color:#666>3</span>       18s
</span></span></code></pre></div><p>Fíjate que el nombre del ReplicaSet siempre se formatea con el patrón <code>[DEPLOYMENT-NAME]-[RANDOM-STRING]</code>. La cadena aleatoria se
genera de forma aleatoria y usa el pod-template-hash como semilla.</p><p>Para ver las etiquetas generadas automáticamente en cada pod, ejecuta el comando <code>kubectl get pods --show-labels</code>. Se devuelve la siguiente salida:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE       LABELS
</span></span><span style=display:flex><span>nginx-deployment-75675f5897-7ci7o   1/1       Running   <span style=color:#666>0</span>          18s       <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx,pod-template-hash<span style=color:#666>=</span><span style=color:#666>3123191453</span>
</span></span><span style=display:flex><span>nginx-deployment-75675f5897-kzszj   1/1       Running   <span style=color:#666>0</span>          18s       <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx,pod-template-hash<span style=color:#666>=</span><span style=color:#666>3123191453</span>
</span></span><span style=display:flex><span>nginx-deployment-75675f5897-qqcnn   1/1       Running   <span style=color:#666>0</span>          18s       <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx,pod-template-hash<span style=color:#666>=</span><span style=color:#666>3123191453</span>
</span></span></code></pre></div><p>El ReplicaSet creado garantiza que hay tres Pods de <code>nginx</code> ejecutándose en todo momento.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> En un Deployment, debes especificar un selector apropiado y etiquetas de plantilla Pod (en este caso,
<code>app: nginx</code>). No entremezcles etiquetas o selectores con otros controladores (incluyendo otros Deployments y StatefulSets).
Kubernetes no te impide que lo hagas, pero en el caso de que múltiples controladores tengan selectores mezclados, dichos controladores pueden entrar en conflicto y provocar resultados inesperados.</div><h3 id=etiqueta-pod-template-hash>Etiqueta pod-template-hash</h3><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> No cambies esta etiqueta.</div><p>La etiqueta <code>pod-template-hash</code> es añadida por el controlador del Deployment a cada ReplicaSet que el Deployment crea o adopta.</p><p>Esta etiqueta garantiza que todos los hijos ReplicaSets de un Deployment no se entremezclan. Se genera mediante una función hash aplicada al <code>PodTemplate</code> del ReplicaSet
y usando el resultado de la función hash como el valor de la etiqueta que se añade al selector del ReplicaSet, en las etiquetas de la plantilla Pod,
y en cualquier Pod existente que el ReplicaSet tenga.</p><h2 id=actualizar-un-deployment>Actualizar un Deployment</h2><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El lanzamiento de un Deployment se activa si y sólo si la plantilla Pod del Deployment (esto es, <code>.spec.template</code>)
se cambia, por ejemplo si se actualiza las etiquetas o las imágenes de contenedor de la plantilla.
Otras actualizaciones, como el escalado del Deployment, no conllevan un lanzamiento de despliegue.</div><p>Asumiendo que ahora quieres actualizar los Pods nginx para que usen la imagen <code>nginx:1.9.1</code>
en vez de la imagen <code>nginx:1.7.9</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl --record deployment.apps/nginx-deployment <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.9.1
</span></span></code></pre></div><pre tabindex=0><code>image updated
</code></pre><p>De forma alternativa, puedes <code>editar</code> el Deployment y cambiar el valor del campo <code>.spec.template.spec.containers[0].image</code> de <code>nginx:1.7.9</code> a <code>nginx:1.9.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment edited
</code></pre><p>Para ver el estado del despliegue, ejecuta:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment &#34;nginx-deployment&#34; successfully rolled out
</code></pre><p>Cuando el despliegue funciona, puede que quieras <code>obtener</code> el Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployments
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   3/3     3            3           36s 
</code></pre><p>El número de réplicas actualizadas indica que el Deployment ha actualizado las réplicas según la última configuración.
Las réplicas actuales indican el total de réplicas que gestiona este Deployment, y las réplicas disponibles indican
el número de réplicas actuales que están disponibles.</p><p>Puedes ejecutar el comando <code>kubectl get rs</code> para ver que el Deployment actualizó los Pods creando un nuevo ReplicaSet y escalándolo
hasta las 3 réplicas, así como escalando el viejo ReplicaSet a 0 réplicas.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
</code></pre><p>Si ejecutas el comando <code>get pods</code> deberías ver los nuevos Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><pre tabindex=0><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</code></pre><p>La próxima vez que quieras actualizar estos Pods, sólo necesitas actualizar la plantilla Pod del Deployment otra vez.</p><p>El Deployment permite garantizar que sólo un número determinado de Pods puede eliminarse mientras se están actualizando.
Por defecto, garantiza que al menos el 25% menos del número deseado de Pods se está ejecutando (máx. 25% no disponible).</p><p>El Deployment también permite garantizar que sólo un número determinado de Pods puede crearse por encima del número deseado de
Pods. Por defecto, garantiza que al menos el 25% más del número deseado de Pods se está ejecutando (máx. 25% de aumento).</p><p>Por ejemplo, si miras detenidamente el Deployment de arriba, verás que primero creó un Pod,
luego eliminó algunos viejos Pods y creó otros nuevos. No elimina los viejos Pods hasta que un número suficiente de
nuevos Pods han arrancado, y no crea nuevos Pods hasta que un número suficiente de viejos Pods se han eliminado.
De esta forma, asegura que el número de Pods disponibles siempre es al menos 2, y el número de Pods totales es cómo máximo 4.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployments
</span></span></code></pre></div><pre tabindex=0><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
  Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
</code></pre><p>Aquí puedes ver que cuando creaste por primera vez el Deployment, este creó un ReplicaSet (nginx-deployment-2035384211)
y lo escaló a 3 réplicas directamente. Cuando actualizaste el Deployment, creó un nuevo ReplicaSet
(nginx-deployment-1564180365) y lo escaló a 1 y entonces escaló el viejo ReplicaSet a 2, de forma que al menos
hubiera 2 Pods disponibles y como mucho 4 Pods en total en todo momento. Entonces, continuó escalando
el nuevo y el viejo ReplicaSet con la misma estrategia de actualización continua. Finalmente, el nuevo ReplicaSet acaba con 3 réplicas
disponibles, y el viejo ReplicaSet se escala a 0.</p><h3 id=sobrescritura-o-sea-múltiples-actualizaciones-a-la-vez>Sobrescritura (o sea, múltiples actualizaciones a la vez)</h3><p>Cada vez que el controlador del Deployment observa un nuevo objeto de despliegue, se crea un ReplicaSet para arrancar
los Pods deseados si es que no existe otro ReplicaSet haciéndolo. Los ReplicaSet existentes que controlan los Pods cuyas etiquetas
coinciden con el valor del campo <code>.spec.selector</code>, pero cuya plantilla no coincide con el valor del campo <code>.spec.template</code> se reducen. Al final,
el nuevo ReplicaSet se escala hasta el valor del campo <code>.spec.replicas</code> y todos los viejos ReplicaSets se escalan a 0.</p><p>Si actualizas un Deployment mientras otro despliegue está en curso, el Deployment creará un nuevo ReplicaSet
como consecuencia de la actualización y comenzará a escalarlo, y sobrescribirá al ReplicaSet que estaba escalando anteriormente
-- lo añadirá a su lista de viejos ReplicaSets y comenzará a reducirlos.</p><p>Por ejemplo, supongamos que creamos un Deployment para crear 5 réplicas de <code>nginx:1.7.9</code>,
pero entonces actualizamos el Deployment para crear 5 réplicas de <code>nginx:1.9.1</code> cuando sólo se ha creado 3
réplicas de <code>nginx:1.7.9</code>. En este caso, el Deployment comenzará automáticamente a matar los 3 Pods de <code>nginx:1.7.9</code>
que había creado, y empezará a crear los Pods de <code>nginx:1.9.1</code>. Es decir, no esperará a que se creen las 5 réplicas de <code>nginx:1.7.9</code>
antes de aplicar la nueva configuración.</p><h3 id=actualizaciones-del-selector-de-etiquetas>Actualizaciones del selector de etiquetas</h3><p>No se recomienda hacer cambios al selector del etiquetas y, por ello, se aconseja encarecidamente planificar el valor de dichos selectores por adelantado.
En cualquier caso, si necesitas cambiar un selector de etiquetas, hazlo con mucho cuidado y asegúrate que entiendes todas sus implicaciones.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> En la versión <code>apps/v1</code> de la API, el selector de etiquetas del Deployment es inmutable una vez se ha creado.</div><ul><li>Las adiciones posteriores al selector obligan también a actualizar las etiquetas de la plantilla Pod en la especificación del Deployment con los nuevos valores,
ya que de lo contrario se devolvería un error. Este cambio no es de superposición, es decir, que el nuevo selector
no selecciona los ReplicaSets y Pods creados con el viejo selector, lo que provoca que todos los viejos ReplicaSets se marquen como huérfanos y
la creación de un nuevo ReplicaSet.</li><li>Las actualizaciones de selector -- esto es, cambiar el valor actual en una clave de selector -- provocan el mismo comportamiento que las adiciones.</li><li>Las eliminaciones de selector -- esto es, eliminar una clave actual del selector del Deployment -- no necesitan de cambios en las etiquetas de la plantilla Pod.
No se marca ningún ReplicaSet existente como huérfano, y no se crea ningún ReplicaSet nuevo, pero debe tenerse en cuenta que
la etiqueta eliminada todavía existe en los Pods y ReplicaSets que se están ejecutando.</li></ul><h2 id=revertir-un-deployment>Revertir un Deployment</h2><p>En ocasiones necesitas revertir un Deployment; por ejemplo, cuando el Deployment no es estable, como cuando no para de reiniciarse.
Por defecto, toda la historia de despliegue del Deployment se mantiene en el sistema de forma que puedes revertir en cualquier momento
(se puede modificar este comportamiento cambiando el límite de la historia de revisiones de modificaciones).</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Cuando se lanza el despligue de un Deployment, se crea una nueva revisión. Esto quiere decir que
la nueva revisión se crea si y sólo si la plantilla Pod del Deployment (<code>.spec.template</code>) se cambia;
por ejemplo, si cambias las etiquetas o la imagen del contenedor de la plantilla.
Otras actualizaciones, como escalar el Deployment,
no generan una nueva revisión del Deployment, para poder facilitar el escalado manual simultáneo - o auto-escalado.
Esto significa que cuando reviertes a una versión anterior, sólo la parte de la plantilla Pod del Deployment se revierte.</div><p>Vamos a suponer que hemos cometido un error al actualizar el Deployment, poniendo como nombre de imagen <code>nginx:1.91</code> en vez de <code>nginx:1.9.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.91 --record<span style=color:#666>=</span><span style=color:#a2f>true</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre><p>El despliegue se atasca y no progresa.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
</code></pre><p>Presiona Ctrl-C para detener la monitorización del despliegue de arriba. Para obtener más información sobre despliegues atascados,
<a href=#deployment-status>lee más aquí</a>.</p><p>Verás que el número de réplicas viejas (nginx-deployment-1564180365 y nginx-deployment-2035384211) es 2, y el número de nuevas réplicas (nginx-deployment-3066724191) es 1.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s
</code></pre><p>Echando un vistazo a los Pods creados, verás que uno de los Pods creados por el nuevo ReplicaSet está atascado en un bucle intentando bajar la imagen:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><pre tabindex=0><code>NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
</code></pre><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El controlador del Deployment parará el despliegue erróneo de forma automática, y detendrá el escalado del nuevo
ReplicaSet. Esto depende de los parámetros del rollingUpdate (<code>maxUnavailable</code> específicamente) que hayas configurado.
Kubernetes por defecto establece el valor en el 25%.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment
</span></span></code></pre></div><pre tabindex=0><code>Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.91
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubobjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
</code></pre><p>Para arreglar este problema, necesitas volver a una revisión previa del Deployment que sea estable.</p><h3 id=comprobar-la-historia-de-despliegues-de-un-deployment>Comprobar la Historia de Despliegues de un Deployment</h3><p>Primero, comprobemos las revisiones de este despliegue:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployments &#34;nginx-deployment&#34;
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true
</code></pre><p>En el momento de la creación, el mensaje en <code>CHANGE-CAUSE</code> se copia de la anotación <code>kubernetes.io/change-cause</code> del Deployment a sus revisiones. Podrías indicar el mensaje <code>CHANGE-CAUSE</code>:</p><ul><li>Anotando el Deployment con el comando <code>kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause="image updated to 1.9.1"</code></li><li>Añadiendo el parámetro <code>--record</code> para registrar el comando <code>kubectl</code> que está haciendo cambios en el recurso.</li><li>Manualmente editando el manifiesto del recursos.</li></ul><p>Para ver más detalles de cada revisión, ejecuta:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment.v1.apps/nginx-deployment --revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><pre tabindex=0><code>deployments &#34;nginx-deployment&#34; revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
  Containers:
   nginx:
    Image:      nginx:1.9.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &lt;none&gt;
  No volumes.
</code></pre><h3 id=retroceder-a-una-revisión-previa>Retroceder a una Revisión Previa</h3><p>Ahora has decidido que quieres deshacer el despliegue actual y retrocederlo a la revisión previa:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment
</code></pre><p>Alternativamente, puedes retroceder a una revisión específica con el parámetro <code>--to-revision</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment
</code></pre><p>Para más detalles acerca de los comandos relacionados con las revisiones de un Deployment, echa un vistazo a <a href=/docs/reference/generated/kubectl/kubectl-commands#rollout><code>kubectl rollout</code></a>.</p><p>El Deployment se ha revertido ahora a una revisión previa estable. Como se puede comprobar, el controlador del Deployment genera un evento <code>DeploymentRollback</code>
al retroceder a la revisión 2.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   3/3     3            3           30m 
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &#34;nginx-deployment&#34; to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
</code></pre><h2 id=escalar-un-deployment>Escalar un Deployment</h2><p>Puedes escalar un Deployment usando el siguiente comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment.v1.apps/nginx-deployment --replicas<span style=color:#666>=</span><span style=color:#666>10</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment scaled
</code></pre><p>Asumiendo que se ha habilitado el <a href=/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/>escalado horizontal de pod</a>
en tu clúster, puedes configurar un auto-escalado para tu Deployment y elegir el mínimo y máximo número de Pods
que quieres ejecutar en base al uso de CPU de tus Pods actuales.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale deployment.v1.apps/nginx-deployment --min<span style=color:#666>=</span><span style=color:#666>10</span> --max<span style=color:#666>=</span><span style=color:#666>15</span> --cpu-percent<span style=color:#666>=</span><span style=color:#666>80</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment scaled
</code></pre><h3 id=escalado-proporcional>Escalado proporcional</h3><p>La actualización continua de los Deployments permite la ejecución de múltiples versiones de una aplicación al mismo tiempo.
Cuando tú o un auto-escalado escala un Deployment con actualización continua que está en medio de otro despliegue (bien en curso o pausado),
entonces el controlador del Deployment balanceará las réplicas adicionales de los ReplicaSets activos (ReplicaSets con Pods)
para así poder mitigar el riesgo. Esto se conoce como <em>escalado proporcional</em>.</p><p>Por ejemplo, imagina que estás ejecutando un Deployment con 10 réplicas, donde <a href=#max-surge>maxSurge</a>=3, y <a href=#max-unavailable>maxUnavailable</a>=2.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   10/10   10           10          50s 
</code></pre><p>Si actualizas a una nueva imagen que no puede descargarse desde el clúster:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:sometag
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre><p>La actualización de la imagen arranca un nuevo despliegue con el ReplicaSet nginx-deployment-1989198191,
pero se bloquea debido al requisito <code>maxUnavailable</code> indicado arriba:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
</code></pre><p>Y entonces se origina una nueva petición de escalado para el Deployment. El auto-escalado incrementa las réplicas del Deployment
a 15. El controlador del Deployment necesita ahora decidir dónde añadir esas nuevas 5 réplicas.
Si no estuvieras usando el escalado proporcional, las 5 se añadirían al nuevo ReplicaSet. Pero con el escalado proporcional,
las réplicas adicionales se distribuyen entre todos los ReplicaSets. Las partes más grandes van a los ReplicaSets
con el mayor número de réplicas y las partes más pequeñas van a los ReplicaSets con menos réplicas. Cualquier resto sobrante se añade
al ReplicaSet con mayor número de réplicas. Aquellos ReplicaSets con 0 réplicas no se escalan.</p><p>En nuestro ejemplo anterior, se añadirán 3 réplicas al viejo ReplicaSet y 2 réplicas al nuevo ReplicaSet.
EL proceso de despliegue debería al final mover todas las réplicas al nuevo ReplicaSet, siempre que las nuevas
réplicas arranquen positivamente.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   18/15   7            8           7m 
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
</code></pre><h2 id=pausar-y-reanudar-un-deployment>Pausar y Reanudar un Deployment</h2><p>Puedes pausar un Deployment antes de arrancar una o más modificaciones y luego reanudarlo. Esto te permite aplicar múltiples arreglos
entre la pausa y la reanudación sin necesidad de arrancar despliegues innecesarios.</p><p>Por ejemplo, con un Deployment que acaba de crearse:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   3/3     3            3           1m 
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
</code></pre><p>Lo pausamos ejecutando el siguiente comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout pause deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment paused
</code></pre><p>Y luego actualizamos la imagen del Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.9.1
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre><p>Nótese que no se arranca ningún despliegue nuevo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployments &#34;nginx&#34;
REVISION  CHANGE-CAUSE
1   &lt;none&gt;
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m
</code></pre><p>Puedes realizar tantas modificaciones como quieras, por ejemplo, para actualizar los recursos a utilizar:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> resources deployment.v1.apps/nginx-deployment -c<span style=color:#666>=</span>nginx --limits<span style=color:#666>=</span><span style=color:#b8860b>cpu</span><span style=color:#666>=</span>200m,memory<span style=color:#666>=</span>512Mi
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment resource requirements updated
</code></pre><p>El estado inicial del Deployment anterior a la pausa continuará su función, pero las nuevas modificaciones
del Deployment no tendrán efecto ya que el Deployment está pausado.</p><p>Al final, reanuda el Deployment y observa cómo se genera un nuevo ReplicaSet con todos los cambios:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout resume deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment resumed
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs -w
</span></span></code></pre></div><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
</code></pre><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> No se puede revertir un Deployment pausado hasta que se vuelve a reanudar.</div><h2 id=estado-del-deployment>Estado del Deployment</h2><p>Un Deployment pasa por varios estados a lo largo de su ciclo de vida. Así, puede estar <a href=#progressing-deployment>progresando</a> mientras
se despliega un nuevo ReplicaSet, puede estar <a href=#complete-deployment>completo</a>, o puede quedar en estado <a href=#failed-deployment>fallido</a>.</p><h3 id=progresar-un-deployment>Progresar un Deployment</h3><p>Kubernetes marca un Deployment como <em>progresando</em> cuando se realiza cualquiera de las siguientes tareas:</p><ul><li>El Deployment crea un nuevo ReplicaSet.</li><li>El Deployment está escalando su ReplicaSet más nuevo.</li><li>El Deployment está reduciendo su(s) ReplicaSet(s) más antiguo(s).</li><li>Hay nuevos Pods disponibles y listos (listo por lo menos <a href=#min-ready-seconds>MinReadySeconds</a>).</li></ul><p>Puedes monitorizar el progreso de un Deployment usando el comando <code>kubectl rollout status</code>.</p><h3 id=completar-un-deployment>Completar un Deployment</h3><p>Kubernetes marca un Deployment como <em>completado</em> cuando presenta las siguientes características:</p><ul><li>Todas las réplicas asociadas con el Deployment han sido actualizadas a la última versión indicada, lo cual quiere decir
que todas las actualizaciones se han completado.</li><li>Todas las réplicas asociadas con el Deployment están disponibles.</li><li>No están ejecutándose viejas réplicas del Deployment.</li></ul><p>Puedes comprobar si un Deployment se ha completado usando el comando <code>kubectl rollout status</code>. Si el despliegue se ha completado
de forma satisfactoria, el comando <code>kubectl rollout status</code> devuelve un código 0 de salida.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment &#34;nginx-deployment&#34; successfully rolled out
$ echo $?
0
</code></pre><h3 id=deployment-fallido>Deployment fallido</h3><p>Tu Deployment puede quedarse bloqueado intentando desplegar su nuevo ReplicaSet sin nunca completarse. Esto puede ocurrir
debido a algunos de los factores siguientes:</p><ul><li>Cuota insuficiente</li><li>Fallos en la prueba de estar listo</li><li>Errores en la descarga de imágenes</li><li>Permisos insuficientes</li><li>Rangos de límites de recursos</li><li>Mala configuración del motor de ejecución de la aplicación</li></ul><p>Una forma de detectar este tipo de situación es especificar un parámetro de vencimiento en la especificación de tu Deployment:
(<a href=#progress-deadline-seconds><code>.spec.progressDeadlineSeconds</code></a>). <code>.spec.progressDeadlineSeconds</code> denota el número
de segundos que el controlador del Deployment debe esperar antes de indicar (en el estado del Deployment) que el
Deployment no avanza.</p><p>El siguiente comando <code>kubectl</code> configura el campo <code>progressDeadlineSeconds</code> para forzar al controlador a
informar de la falta de avance de un Deployment después de 10 minutos:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl patch deployment.v1.apps/nginx-deployment -p <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;progressDeadlineSeconds&#34;:600}}&#39;</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment patched
</code></pre><p>Una vez que se ha excedido el vencimiento, el controlador del Deployment añade una DeploymentCondition
con los siguientes atributos al campo <code>.status.conditions</code> del Deployment:</p><ul><li>Type=Progressing</li><li>Status=False</li><li>Reason=ProgressDeadlineExceeded</li></ul><p>Ver las <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties>convenciones de la API de Kubernetes</a> para más información acerca de las condiciones de estado.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Kubernetes no emprenderá ninguna acción ante un Deployment parado que no sea la de reportar el estado mediante
<code>Reason=ProgressDeadlineExceeded</code>. Los orquestradores de alto nivel pueden aprovecharse y actuar consecuentemente, por ejemplo,
retrocediendo el Deployment a su versión previa.</div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Si pausas un Deployment, Kubernetes no comprueba el avance en base al vencimiento indicado. Así, es posible pausar
de forma segura un Deployment en medio de un despliegue y reanudarlo sin que se arranque el estado de exceso de vencimiento.</div><p>Puede que notes errores transitorios en tus Deployments, bien debido a un tiempo de vencimiento muy pequeño que hayas configurado
o bien a cualquier otro tipo de error que puede considerarse como transitorio. Por ejemplo,
supongamos que no tienes suficiente cuota. Si describes el Deployment, te darás cuenta de la sección siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
</code></pre><p>Si ejecutas el comando <code>kubectl get deployment nginx-deployment -o yaml</code>, el estado del Deployment puede parecerse a:</p><pre tabindex=0><code>status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &#34;nginx-deployment-4262182780&#34; is progressing.
    reason: ReplicaSetUpdated
    status: &#34;True&#34;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &#34;True&#34;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: &#39;Error creating: pods &#34;nginx-deployment-4262182780-&#34; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2&#39;
    reason: FailedCreate
    status: &#34;True&#34;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
</code></pre><p>Al final, una vez que se supera el vencimiento del progreso del Deployment, Kubernetes actualiza el estado
y la razón de el estado de progreso:</p><pre tabindex=0><code>Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
</code></pre><p>Puedes solucionar un problema de cuota insuficiente simplemente reduciendo el número de réplicas de tu Deployment, reduciendo
otros controladores que puedas estar ejecutando, o incrementando la cuota en tu espacio de nombres. Si una vez satisfechas las condiciones de tu cuota,
el controlador del Deployment completa el despliegue, entonces verás que el estado del Deployment se actualiza al estado satisfactorio (<code>Status=True</code> y <code>Reason=NewReplicaSetAvailable</code>).</p><pre tabindex=0><code>Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
</code></pre><p><code>Type=Available</code> con <code>Status=True</code> significa que tu Deployment tiene disponibilidad mínima. La disponibilidad mínima se prescribe
mediante los parámetros indicados en la estrategia de despligue. <code>Type=Progressing</code> con <code>Status=True</code> significa que tu Deployment
está bien en medio de un despliegue y está progresando o bien que se ha completado de forma satisfactoria y el número mínimo
requerido de nuevas réplicas ya está disponible (ver la Razón del estado para cada caso particular - en nuestro caso
<code>Reason=NewReplicaSetAvailable</code> significa que el Deployment se ha completado).</p><p>Puedes comprobar si un Deployment ha fallado en su progreso usando el comando <code>kubectl rollout status</code>. <code>kubectl rollout status</code>
devuelve un código de salida distinto de 0 si el Deployment ha excedido su tiempo de vencimiento.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &#34;nginx&#34; exceeded its progress deadline
$ echo $?
1
</code></pre><h3 id=actuar-ante-un-despliegue-fallido>Actuar ante un despliegue fallido</h3><p>Todas las acciones que aplican a un Deployment completado también aplican a un Deployment fallido. Puedes escalarlo/reducirlo, retrocederlo
a una revisión previa, o incluso pausarlo si necesitas realizar múltiples cambios a la plantilla Pod del Deployment.</p><h2 id=regla-de-limpieza>Regla de Limpieza</h2><p>Puedes configurar el campo <code>.spec.revisionHistoryLimit</code> de un Deployment para especificar cuántos ReplicaSets viejos quieres conservar
para este Deployment. El resto será eliminado en segundo plano. Por defecto, es 10.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Poner este campo de forma explícita a 0 resulta en la limpieza de toda la historia de tu Deployment,
por lo que tu Deployment no podrá retroceder a revisiones previas.</div><h2 id=casos-de-uso-1>Casos de Uso</h2><h3 id=despligue-canary>Despligue Canary</h3><p>Si quieres desplegar nuevas versiones a un sub-conjunto de usuarios o servidores usando el Deployment,
puedes hacerlo creando múltiples Deployments, uno para cada versión nueva, siguiendo el patrón canary descrito en
<a href=/docs/concepts/cluster-administration/manage-deployment/#canary-deployments>gestionar recursos</a>.</p><h2 id=escribir-una-especificación-de-deployment>Escribir una especificación de Deployment</h2><p>Al igual que con el resto de configuraciones de Kubernetes, un Deployment requiere los campos <code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>.
Para información general acerca de cómo trabajar con ficheros de configuración, ver los documentos acerca de <a href=/docs/tutorials/stateless-application/run-stateless-application-deployment/>desplegar aplicaciones</a>,
configurar contenedores, y <a href=/docs/concepts/overview/object-management-kubectl/overview/>usar kubectl para gestionar recursos</a>.</p><p>Un Deployment también necesita una <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>sección <code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>Tanto <code>.spec.template</code> como <code>.spec.selector</code> sin campos obligatorios dentro de <code>.spec</code>.</p><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/pods/pod-overview/#pod-templates>plantilla Pod</a>. Tiene exactamente el mismo esquema que un <a href=/docs/concepts/workloads/pods/pod/>Pod</a>,
excepto por el hecho de que está anidado y no tiene <code>apiVersion</code> ni <code>kind</code>.</p><p>Junto con los campos obligatorios de un Pod, una plantilla Pod de un Deployment debe indicar las etiquetas
y las reglas de reinicio apropiadas. Para el caso de las etiquetas, asegúrate que no se entremezclan con otros controladores. Ver <a href=#selector>selector</a>).</p><p>Únicamente se permite una <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>.spec.template.spec.restartPolicy</code></a> igual a <code>Always</code>,
que es el valor por defecto si no se indica.</p><h3 id=réplicas>Réplicas</h3><p><code>.spec.replicas</code> es un campo opcional que indica el número de Pods deseados. Su valor por defecto es 1.</p><h3 id=selector>Selector</h3><p><code>.spec.selector</code> es un campo opcional que indica un <a href=/docs/concepts/overview/working-with-objects/labels/>selector de etiquetas</a>
para los Pods objetivo del deployment.</p><p><code>.spec.selector</code> debe coincidir con <code>.spec.template.metadata.labels</code>, o será descartado por la API.</p><p>A partir de la versión <code>apps/v1</code> de la API, <code>.spec.selector</code> y <code>.metadata.labels</code> no toman como valor por defecto el valor de <code>.spec.template.metadata.labels</code> si no se indica.
Por ello, debe especificarse de forma explícita. Además hay que mencionar que <code>.spec.selector</code> es inmutable tras la creación del Deployment en <code>apps/v1</code>.</p><p>Un Deployment puede finalizar aquellos Pods cuyas etiquetas coincidan con el selector si su plantilla es diferente
de <code>.spec.template</code> o si el número total de dichos Pods excede <code>.spec.replicas</code>. Arranca nuevos
Pods con <code>.spec.template</code> si el número de Pods es menor que el número deseado.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> No deberías crear otros Pods cuyas etiquetas coincidan con este selector, ni directamente creando
otro Deployment, ni creando otro controlador como un ReplicaSet o un ReplicationController. Si lo haces,
el primer Deployment pensará que también creó esos otros Pods. Kubernetes no te impide hacerlo.</div><p>Si tienes múltiples controladores que entremezclan sus selectores, dichos controladores competirán entre ellos
y no se comportarán de forma correcta.</p><h3 id=estrategia>Estrategia</h3><p><code>.spec.strategy</code> especifica la estrategia usada para remplazar los Pods viejos con los nuevos.
<code>.spec.strategy.type</code> puede tener el valor "Recreate" o "RollingUpdate". "RollingUpdate" el valor predeterminado.</p><h4 id=despliegue-mediante-recreación>Despliegue mediante recreación</h4><p>Todos los Pods actuales se eliminan antes de que los nuevos se creen cuando <code>.spec.strategy.type==Recreate</code>.</p><h4 id=despliegue-mediante-actualización-continua>Despliegue mediante actualización continua</h4><p>El Deployment actualiza los Pods en modo de <a href=/docs/tasks/run-application/rolling-update-replication-controller/>actualización continua</a>
cuando <code>.spec.strategy.type==RollingUpdate</code>. Puedes configurar los valores de <code>maxUnavailable</code> y <code>maxSurge</code>
para controlar el proceso de actualización continua.</p><h5 id=número-máximo-de-pods-no-disponibles>Número máximo de pods no disponibles</h5><p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> es un campo opcional que indica el número máximo
de Pods que pueden no estar disponibles durante el proceso de actualización. El valor puede ser un número absoluto (por ejemplo, 5)
o un porcentaje de los Pods deseados (por ejemplo, 10%). El número absoluto se calcula a partir del porcentaje
con redondeo a la baja. El valor no puede ser 0 si <code>.spec.strategy.rollingUpdate.maxSurge</code> es 0. El valor predeterminado es 25%.</p><p>Por ejemplo, cuando este valor es 30%, el ReplicaSet viejo puede escalarse al 70% de los
Pods deseados de forma inmediata tras comenzar el proceso de actualización. Una vez que los Pods están listos,
el ReplicaSet viejo puede reducirse aún mas, seguido de un escalado del nuevo ReplicaSet,
asegurándose que el número total de Pods disponibles en todo momento durante la actualización
es de al menos el 70% de los Pods deseados.</p><h5 id=número-máximo-de-pods-por-encima-del-número-deseado>Número máximo de pods por encima del número deseado</h5><p><code>.spec.strategy.rollingUpdate.maxSurge</code> es un campo opcional que indica el número máximo de Pods
que puede crearse por encima del número deseado de Pods. El valor puede ser un número absoluto (por ejemplo, 5)
o un porcentaje de los Pods deseados (por ejemplo, 10%). El valor no puede ser 0 si <code>MaxUnavailable</code> es 0.
El número absoluto se calcula a partir del porcentaje con redondeo al alza. El valor predeterminado es 25%.</p><p>Por ejemplo, cuando este valor es 30%, el nuevo ReplicaSet puede escalarse inmediatamente cuando
comienza la actualización continua, de forma que el número total de Pods viejos y nuevos no
excede el 130% de los Pods deseados. Una vez que los viejos Pods se han eliminado, el nuevo ReplicaSet
puede seguir escalándose, asegurándose que el número total de Pods ejecutándose en todo momento
durante la actualización es como mucho del 130% de los Pods deseados.</p><h3 id=segundos-para-vencimiento-del-progreso>Segundos para vencimiento del progreso</h3><p><code>.spec.progressDeadlineSeconds</code> es un campo opcional que indica el número de segundos que quieres
esperar a que tu Deployment avance antes de que el sistema reporte que dicho Deployment
<a href=#failed-deployment>ha fallado en su avance</a> - expresado como un estado con <code>Type=Progressing</code>, <code>Status=False</code>.
y <code>Reason=ProgressDeadlineExceeded</code> en el recurso. El controlador del Deployment seguirá intentando
el despliegue. En el futuro, una vez que se implemente el retroceso automático, el controlador del Deployment
retrocederá el despliegue en cuanto detecte ese estado.</p><p>Si se especifica, este campo debe ser mayor que <code>.spec.minReadySeconds</code>.</p><h3 id=tiempo-mínimo-para-considerar-el-pod-disponible>Tiempo mínimo para considerar el Pod disponible</h3><p><code>.spec.minReadySeconds</code> es un campo opcional que indica el número mínimo de segundos en que
un Pod recién creado debería estar listo sin que falle ninguno de sus contenedores, para que se considere disponible.
Por defecto su valor es 0 (el Pod se considera disponible en el momento que está listo). Para aprender más acerca de
cuándo un Pod se considera que está listo, ver las <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>pruebas de contenedor</a>.</p><h3 id=vuelta-atrás>Vuelta atrás</h3><p>El campo <code>.spec.rollbackTo</code> se ha quitado de las versiones <code>extensions/v1beta1</code> y <code>apps/v1beta1</code> de la API, y ya no se permite en las versiones de la API a partir de <code>apps/v1beta2</code>.
En su caso, se debería usar <code>kubectl rollout undo</code>, tal y como se explicó en <a href=#rolling-back-to-a-previous-revision>Retroceder a una Revisión Previa</a>.</p><h3 id=límite-del-histórico-de-revisiones>Límite del histórico de revisiones</h3><p>La historia de revisiones de un Deployment se almacena en los ReplicaSets que este controla.</p><p><code>.spec.revisionHistoryLimit</code> es un campo opcional que indica el número de ReplicaSets viejos a retener
para permitir los retrocesos. Estos ReplicaSets viejos consumen recursos en <code>etcd</code> y rebosan la salida de <code>kubectl get rs</code>.
La configuración de cada revisión de Deployment se almacena en sus ReplicaSets;
por lo tanto, una vez que se elimina el ReplicaSet viejo, se pierde la posibilidad de retroceder a dicha revisión del Deployment.
Por defecto, se retienen hasta 10 ReplicaSets viejos; pero su valor ideal depende de la frecuencia y la estabilidad de los nuevos Deployments.</p><p>De forma más específica, si ponemos este campo a cero quiere decir que todos los ReplicaSets viejos con 0 réplicas se limpiarán.
En este caso, el nuevo despliegue del Deployment no se puede deshacer, ya que su historia de revisiones se habrá limpiado.</p><h3 id=pausa>Pausa</h3><p><code>.spec.paused</code> es un campo booleano opcional para pausar y reanudar un Deployment. La única diferencia entre
un Deployment pausado y otro que no lo está es que cualquier cambio al PodTemplateSpec del Deployment pausado
no generará nuevos despliegues mientras esté pausado. Un Deployment se pausa de forma predeterminada cuando se crea.</p><h2 id=alternativa-a-los-deployments>Alternativa a los Deployments</h2><h3 id=kubectl-rolling-update>kubectl rolling update</h3><p><a href=/docs/reference/generated/kubectl/kubectl-commands#rolling-update><code>kubectl rolling update</code></a> actualiza los Pods y los ReplicationControllers
de forma similar. Pero se recomienda el uso de Deployments porque se declaran del lado del servidor, y proporcionan características adicionales
como la posibilidad de retroceder a revisiones anteriores incluso después de haber terminado una actualización continua.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6d72299952c37ca8cc61b416e5bdbcd4>4.2.4 - StatefulSets</h1><p>Un StatefulSet es el objeto de la API workload que se usa para gestionar aplicaciones con estado.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los StatefulSets son estables (GA) en la versión 1.9.</div><p>Gestiona el despliegue y escalado de un conjunto de <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>,
<em>y garantiza el orden y unicidad</em> de dichos Pods.</p><p>Al igual que un <a class=glossary-tooltip title='Un objeto API que gestiona una aplicación replicada.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>, un StatefulSet gestiona Pods
que se basan en una especificación idéntica de contenedor. A diferencia de un Deployment, un
StatefulSet mantiene una identidad asociada a sus Pods. Estos pods se crean a partir de la
misma especificación, pero no pueden intercambiarse; cada uno tiene su propio identificador persistente
que mantiene a lo largo de cualquier re-programación.</p><p>Un StatefulSet opera bajo el mismo patrón que cualquier otro controlador.
Se define el estado deseado en un <em>objeto</em> StatefulSet, y el <em>controlador</em> del StatefulSet efectúa
las actualizaciones que sean necesarias para alcanzarlo a partir del estado actual.</p><h2 id=usar-statefulsets>Usar StatefulSets</h2><p>Los StatefulSets son valiosos para aquellas aplicaciones que necesitan uno o más de los siguientes:</p><ul><li>Identificadores de red estables, únicos.</li><li>Almacenamiento estable, persistente.</li><li>Despliegue y escalado ordenado, controlado.</li><li>Actualizaciones en línea ordenadas, automatizadas.</li></ul><p>De los de arriba, estable es sinónimo de persistencia entre (re)programaciones de Pods.
Si una aplicación no necesita ningún identificador estable o despliegue,
eliminación, o escalado ordenado, deberías desplegar tu aplicación con un controlador que
proporcione un conjunto de réplicas sin estado, como un
<a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a> o un
<a href=/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a>, ya que están mejor preparados
para tus necesidades sin estado.</p><h2 id=limitaciones>Limitaciones</h2><ul><li>El almacenamiento de un determinado Pod debe provisionarse por un <a href=https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md>Provisionador de PersistentVolume</a> basado en la <code>storage class</code> requerida, o pre-provisionarse por un administrador.</li><li>Eliminar y/o reducir un StatefulSet <em>no</em> eliminará los volúmenes asociados con el StatefulSet. Este comportamiento es intencional y sirve para garantizar la seguridad de los datos, que da más valor que la purga automática de los recursos relacionados del StatefulSet.</li><li>Los StatefulSets actualmente necesitan un <a href=/docs/concepts/services-networking/service/#headless-services>Servicio Headless</a> como responsable de la identidad de red de los Pods. Es tu responsabilidad crear este Service.</li><li>Los StatefulSets no proporcionan ninguna garantía de la terminación de los pods cuando se elimina un StatefulSet. Para conseguir un término de los pods ordenado y controlado en el StatefulSet, es posible reducir el StatefulSet a 0 réplicas justo antes de eliminarlo.</li><li>Cuando se usan las <a href=#rolling-updates>Actualizaciones en línea</a> con la
<a href=#pod-management-policies>Regla de Gestión de Pod</a> (<code>OrderedReady</code>) por defecto,
es posible entrar en un estado inconsistente que requiere de una
<a href=#retroceso-forzado>intervención manual para su reparación</a>.</li></ul><h2 id=componentes>Componentes</h2><p>El ejemplo de abajo demuestra los componentes de un StatefulSet:</p><ul><li>Un servicio Headless, llamado nginx, se usa para controlar el dominio de red.</li><li>Un StatefulSet, llamado web, que tiene una especificación que indica que se lanzarán 3 réplicas del contenedor nginx en Pods únicos.</li><li>Un volumeClaimTemplate que proporciona almacenamiento estable por medio de <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a> provisionados por un provisionador de tipo PersistentVolume.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span>None<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># tiene que coincidir con .spec.template.metadata.labels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;nginx&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># por defecto es 1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># tiene que coincidir con .spec.selector.matchLabels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/nginx-slim:0.8<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/usr/share/nginx/html<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeClaimTemplates</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;ReadWriteOnce&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;my-storage-class&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=selector-de-pod>Selector de Pod</h2><p>Debes poner el valor del campo <code>.spec.selector</code> de un StatefulSet para que coincida con las etiquetas de su campo <code>.spec.template.metadata.labels</code>. Antes de Kubernetes 1.8,
el campo <code>.spec.selector</code> se predeterminaba cuando se omitía. A partir de la versión 1.8, si no se especifica un selector de coincidencia de Pods, se produce un error de validación
durante la creación del StatefulSet.</p><h2 id=identidad-de-pod>Identidad de Pod</h2><p>Los Pods de un StatefulSet tienen una identidad única que está formada por un ordinal,
una identidad estable de red, y almacenamiento estable. La identidad se asocia al Pod,
independientemente del nodo en que haya sido (re)programado.</p><h3 id=índice-ordinal>Índice Ordinal</h3><p>Para un StatefulSet con N réplicas, a cada Pod del StatefulSet se le asignará
un número entero ordinal, desde 0 hasta N-1, y que es único para el conjunto.</p><h3 id=id-estable-de-red>ID estable de Red</h3><p>El nombre de anfitrión (hostname) de cada Pod de un StatefulSet se deriva del nombre del StatefulSet
y del número ordinal del Pod. El patrón para construir dicho hostname
es <code>$(statefulset name)-$(ordinal)</code>. Así, el ejemplo de arriba creará tres Pods
denominados <code>web-0,web-1,web-2</code>.
Un StatefulSet puede usar un <a href=/docs/concepts/services-networking/service/#headless-services>Servicio Headless</a>
para controlar el nombre de dominio de sus Pods. El nombre de dominio gestionado por este Service tiene la forma:
<code>$(service name).$(namespace).svc.cluster.local</code>, donde "cluster.local" es el nombre de dominio del clúster.
Conforme se crea cada Pod, se le asigna un nombre DNS correspondiente de subdominio, que tiene la forma:
<code>$(podname).$(governing service domain)</code>, donde el servicio en funciones se define por el campo
<code>serviceName</code> del StatefulSet.</p><p>Como se indicó en la sección <a href=#limitaciones>limitaciones</a>, la creación del
<a href=/docs/concepts/services-networking/service/#headless-services>Servicio Headless</a>
encargado de la identidad de red de los pods es enteramente tu responsabilidad.</p><p>Aquí se muestran algunos ejemplos de elecciones de nombres de Cluster Domain, nombres de Service,
nombres de StatefulSet, y cómo impactan en los nombres DNS de los Pods del StatefulSet:</p><table><thead><tr><th>Cluster Domain</th><th>Service (ns/nombre)</th><th>StatefulSet (ns/nombre)</th><th>StatefulSet Domain</th><th>Pod DNS</th><th>Pod Hostname</th></tr></thead><tbody><tr><td>cluster.local</td><td>default/nginx</td><td>default/web</td><td>nginx.default.svc.cluster.local</td><td>web-{0..N-1}.nginx.default.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>cluster.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.cluster.local</td><td>web-{0..N-1}.nginx.foo.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>kube.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.kube.local</td><td>web-{0..N-1}.nginx.foo.svc.kube.local</td><td>web-{0..N-1}</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El valor de Cluster Domain se pondrá a <code>cluster.local</code> a menos que
<a href=/docs/concepts/services-networking/dns-pod-service/>se configure de otra forma</a>.</div><h3 id=almacenamiento-estable>Almacenamiento estable</h3><p>Kubernetes crea un <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolume</a> para cada
VolumeClaimTemplate. En el ejemplo de nginx de arriba, cada Pod recibirá un único PersistentVolume
con una StorageClass igual a <code>my-storage-class</code> y 1 Gib de almacenamiento provisionado. Si no se indica ninguna StorageClass,
entonces se usa la StorageClass por defecto. Cuando un Pod se (re)programa
en un nodo, sus <code>volumeMounts</code> montan los PersistentVolumes asociados con sus
PersistentVolume Claims. Nótese que los PersistentVolumes asociados con los
PersistentVolume Claims de los Pods no se eliminan cuando los Pods, o los StatefulSet se eliminan.
Esto debe realizarse manualmente.</p><h3 id=etiqueta-de-nombre-de-pod>Etiqueta de Nombre de Pod</h3><p>Cuando el controlador del StatefulSet crea un Pod, añade una etiqueta, <code>statefulset.kubernetes.io/pod-name</code>,
que toma el valor del nombre del Pod. Esta etiqueta te permite enlazar un Service a un Pod específico
en el StatefulSet.</p><h2 id=garantías-de-despliegue-y-escalado>Garantías de Despliegue y Escalado</h2><ul><li>Para un StatefulSet con N réplicas, cuando los Pods se despliegan, se crean secuencialmente, en orden de {0..N-1}.</li><li>Cuando se eliminan los Pods, se terminan en orden opuesto, de {N-1..0}.</li><li>Antes de que una operación de escalado se aplique a un Pod, todos sus predecesores deben estar Running y Ready.</li><li>Antes de que se termine un Pod, todos sus sucesores deben haberse apagado completamente.</li></ul><p>El StatefulSet no debería tener que indicar un valor 0 para el campo <code>pod.Spec.TerminationGracePeriodSeconds</code>.
Esta práctica no es segura y se aconseja no hacerlo. Para una explicación más detallada, por favor echa un vistazo a cómo <a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>forzar la eliminación de Pods de un StatefulSet</a>.</p><p>Cuando el ejemplo nginx de arriba se crea, se despliegan tres Pods en el orden
web-0, web-1, web-2. web-1 no se desplegará hasta que web-0 no esté
<a href=/docs/user-guide/pod-states/>Running y Ready</a>, y web-2 no se desplegará hasta que
web-1 esté Running y Ready. En caso de que web-0 fallase, después de que web-1 estuviera Running y Ready, pero antes
de que se desplegara web-2, web-2 no se desplegaría hasta que web-0 se redesplegase con éxito y estuviera
Running y Ready.</p><p>Si un usuario fuera a escalar el ejemplo desplegado parcheando el StatefulSet de forma que
<code>replicas=1</code>, web-2 se terminaría primero. web-1 no se terminaría hasta que web-2
no se hubiera apagado y eliminado por completo. Si web-0 fallase después de que web-2 se hubiera terminado y
apagado completamente, pero antes del término de web-1, entonces web-1 no se terminaría hasta
que web-0 estuviera Running y Ready.</p><h3 id=reglas-de-gestión-de-pods>Reglas de Gestión de Pods</h3><p>En Kubernetes 1.7 y versiones posteriores, el StatefulSet permite flexibilizar sus garantías de ordenación
al mismo tiempo que preservar su garantía de singularidad e identidad a través del campo <code>.spec.podManagementPolicy</code>.</p><h4 id=gestión-de-tipo-orderedready-de-pods>Gestión de tipo OrderedReady de Pods</h4><p>La gestión de tipo <code>OrderedReady</code> de pods es la predeterminada para los StatefulSets. Implementa el comportamiento
descrito <a href=#deployment-and-scaling-guarantees>arriba</a>.</p><h4 id=gestión-de-tipo-parallel-de-pods>Gestión de tipo Parallel de Pods</h4><p>La gestión de tipo <code>Parallel</code> de pods le dice al controlador del StatefulSet que lance y termine
todos los Pods en paralelo, y que no espere a que los Pods estén Running
y Ready o completamente terminados antes de lanzar o terminar otro Pod.</p><h2 id=estrategias-de-actualización>Estrategias de Actualización</h2><p>En Kubernetes 1.7 y a posteriori, el campo <code>.spec.updateStrategy</code> del StatefulSet permite configurar
y deshabilitar las actualizaciones automátizadas en línea para los contenedores, etiquetas, peticiones/límites de recursos,
y anotaciones de los Pods del StatefulSet.</p><h3 id=on-delete>On Delete</h3><p>La estrategia de actualización <code>OnDelete</code> implementa el funcionamiento tradicional (1.6 y previo). Cuando el campo
<code>.spec.updateStrategy.type</code> de un StatefulSet se pone al valor <code>OnDelete</code>, el controlador del StatefulSet no actualizará automáticamente
los Pods del StatefulSet. Los usuarios deben eliminar manualmente los Pods para forzar al controlador a crear
nuevos Pods que reflejen las modificaciones hechas al campo <code>.spec.template</code> del StatefulSet.</p><h3 id=rolling-updates>Rolling Updates</h3><p>La estrategia de actualización <code>RollingUpdate</code> implementa una actualización automatizada en línea de los Pods del
StatefulSet. Es la estrategia por defecto cuando el campo <code>.spec.updateStrategy</code> se deja sin valor. Cuando el campo <code>.spec.updateStrategy.type</code> de un StatefulSet
se pone al valor <code>RollingUpdate</code>, el controlador del StatefulSet lo eliminará y recreará cada Pod en el StatefulSet. Procederá
en el mismo orden en que ha terminado los Pod (del número ordinal más grande al más pequeño), actualizando
cada Pod uno por uno. Esperará a que el Pod actualizado esté Running y Ready antes de
actualizar su predecesor.</p><h4 id=particiones>Particiones</h4><p>La estrategia de actualización <code>RollingUpdate</code> puede particionarse, indicando el valor del campo
<code>.spec.updateStrategy.rollingUpdate.partition</code>. Si se indica una partición, todos los Pods con un
número ordinal mayor o igual que el de la partición serán actualizados cuando el campo <code>.spec.template</code>
del StatefulSet se actualice. Todos los Pods con un número ordinal que sea menor que el de la partición
no serán actualizados, e incluso si son eliminados, serán recreados con la versión anterior. Si el campo
<code>.spec.updateStrategy.rollingUpdate.partition</code> de un StatefulSet es mayor que el valor del campo <code>.spec.replicas</code>,
las modificaciones al campo <code>.spec.template</code> no se propagarán a sus Pods.
En la mayoría de ocasiones, no necesitarás usar una partición, pero pueden resultar útiles si quieres preparar una actualización,
realizar un despliegue tipo canary, o llevar a cabo un despliegue en fases.</p><h4 id=retroceso-forzado>Retroceso Forzado</h4><p>Cuando se usa <a href=#rolling-updates>Actualizaciones en línea</a> con el valor de la
<a href=#pod-management-policies>Regla de Gestión de Pod</a> (<code>OrderedReady</code>) por defecto,
es posible acabar en un estado inconsistente que requiera de una intervención manual para arreglarlo.</p><p>Si actualizas la plantilla Pod a una configuración que nunca llega a Running y
Ready (por ejemplo, debido a un binario incorrecto o un error de configuración a nivel de aplicación),
el StatefulSet detendrá el despliegue y esperará.</p><p>En este estado, no es suficiente con revertir la plantilla Pod a la configuración buena.
Debido a un <a href=https://github.com/kubernetes/kubernetes/issues/67250>problema conocido</a>,
el StatefulSet seguirá esperando a que los Pod estropeados se pongan en Ready
(lo que nunca ocurre) antes de intentar revertirla a la configuración que funcionaba.</p><p>Antes de revertir la plantilla, debes también eliminar cualquier Pod que el StatefulSet haya
intentando ejecutar con la configuración incorrecta.
El StatefulSet comenzará entonces a recrear los Pods usando la plantilla revertida.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Sigue el ejemplo de cómo <a href=/docs/tutorials/stateful-application/basic-stateful-set/>desplegar un aplicación con estado</a>.</li><li>Sigue el ejemplo de cómo <a href=/docs/tutorials/stateful-application/cassandra/>desplegar Cassandra con StatefulSets</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-41600eb8b6631c88848156f381e9d588>4.2.5 - DaemonSet</h1><p>Un <em>DaemonSet</em> garantiza que todos (o algunos) de los nodos ejecuten una copia de un Pod. Conforme se añade más nodos
al clúster, nuevos Pods son añadidos a los mismos. Conforme se elimina nodos del clúster, dichos Pods se destruyen.
Al eliminar un DaemonSet se limpian todos los Pods que han sido creados.</p><p>Algunos casos de uso típicos de un DaemonSet son:</p><ul><li>Ejecutar un proceso de almacenamiento en el clúster.</li><li>Ejecutar un proceso de recolección de logs en cada nodo.</li><li>Ejecutar un proceso de monitorización de nodos en cada nodo.</li></ul><p>De forma básica, se debería usar un DaemonSet, cubriendo todos los nodos, por cada tipo de proceso.
En configuraciones más complejas se podría usar múltiples DaemonSets para un único tipo de proceso,
pero con diferentes parámetros y/o diferentes peticiones de CPU y memoria según el tipo de hardware.</p><h2 id=escribir-una-especificación-de-daemonset>Escribir una especificación de DaemonSet</h2><h3 id=crear-un-daemonset>Crear un DaemonSet</h3><p>Un DaemonSet se describe por medio de un archivo YAML. Por ejemplo, el archivo <code>daemonset.yaml</code> de abajo describe un DaemonSet que ejecuta la imagen Docker de fluentd-elasticsearch:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/daemonset.yaml download=controllers/daemonset.yaml><code>controllers/daemonset.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-daemonset-yaml")' title="Copy controllers/daemonset.yaml to clipboard"></img></div><div class=includecode id=controllers-daemonset-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DaemonSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>fluentd-logging<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node-role.kubernetes.io/master<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Exists<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/fluentd-elasticsearch/fluentd:v2.5.1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlibdockercontainers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/docker/containers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>30</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlibdockercontainers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/lib/docker/containers<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><ul><li>Crear un DaemonSet basado en el archivo YAML:</li></ul><pre tabindex=0><code>kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
</code></pre><h3 id=campos-requeridos>Campos requeridos</h3><p>Como con cualquier otra configuración de Kubernetes, un DaemonSet requiere los campos <code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>.
Para información general acerca de cómo trabajar con ficheros de configuración, ver los documentos <a href=/docs/user-guide/deploying-applications/>desplegar aplicaciones</a>,
<a href=/docs/tasks/>configurar contenedores</a>, y <a href=/docs/concepts/overview/object-management-kubectl/overview/>gestión de objetos usando kubectl</a>.</p><p>Un DaemonSet también necesita un sección <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>El campo <code>.spec.template</code> es uno de los campos obligatorios de la sección <code>.spec</code>.</p><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/pods/pod-overview/#pod-templates>plantilla Pod</a>. Tiene exactamente el mismo esquema que un <a href=/docs/concepts/workloads/pods/pod/>Pod</a>,
excepto por el hecho de que está anidado y no tiene los campos <code>apiVersion</code> o <code>kind</code>.</p><p>Además de los campos obligatorios de un Pod, la plantilla Pod para un DaemonSet debe especificar
las etiquetas apropiadas (ver <a href=#pod-selector>selector de pod</a>).</p><p>Una plantilla Pod para un DaemonSet debe tener una <a href=/docs/user-guide/pod-states><code>RestartPolicy</code></a>
igual a <code>Always</code>, o no indicarse, lo cual asume por defecto el valor <code>Always</code>.</p><h3 id=selector-de-pod>Selector de Pod</h3><p>El campo <code>.spec.selector</code> es un selector de pod. Funciona igual que el campo <code>.spec.selector</code>
de un <a href=/docs/concepts/jobs/run-to-completion-finite-workloads/>Job</a>.</p><p>A partir de Kubernetes 1.8, se debe configurar un selector de pod que coincida con las
etiquetas definidas en el <code>.spec.template</code>. Así, el selector de pod ya no asume valores por defecto cuando no se indica.
Dichos valores por defecto no eran compatibles con <code>kubectl apply</code>. Además, una vez que se ha creado el DaemonSet,
su campo <code>.spec.selector</code> no puede alterarse porque, si fuera el caso, ello podría resultar
en Pods huérfanos, lo cual confundiría a los usuarios.</p><p>El campo <code>.spec.selector</code> es un objeto que, a su vez, consiste en dos campos:</p><ul><li><code>matchLabels</code> - funciona igual que el campo <code>.spec.selector</code> de un <a href=/docs/concepts/workloads/controllers/replicationcontroller/>ReplicationController</a>.</li><li><code>matchExpressions</code> - permite construir selectores más sofisticados indicando la clave,
la lista de valores y un operador para relacionar la clave y los valores.</li></ul><p>Cuando se configura ambos campos, el resultado es conjuntivo (AND).</p><p>Si se especifica el campo <code>.spec.selector</code>, entonces debe coincidir con el campo <code>.spec.template.metadata.labels</code>. Aquellas configuraciones que no coinciden, son rechazadas por la API.</p><p>Además, normalmente no se debería crear ningún Pod con etiquetas que coincidan con el selector, bien sea de forma directa, via otro
DaemonSet, o via otro controlador como un ReplicaSet. De ser así, el controlador del DaemonSet
pensará que dichos Pods fueron en realidad creados por él mismo. Kubernetes, en cualquier caso, no te impide realizar esta
operación. Un caso donde puede que necesites hacer esto es cuando quieres crear manualmente un Pod con un valor diferente en un nodo para pruebas.</p><h3 id=ejecutar-pods-sólo-en-nodos-seleccionados>Ejecutar Pods sólo en Nodos seleccionados</h3><p>Si se configura un <code>.spec.template.spec.nodeSelector</code>, entonces el controlador del DaemonSet
creará los Pods en aquellos nodos que coincidan con el <a href=/docs/concepts/configuration/assign-pod-node/>selector de nodo</a> indicado.
De forma similar, si se configura una <code>.spec.template.spec.affinity</code>,
entonces el controlador del DaemonSet creará los Pods en aquellos nodos que coincidan con la <a href=/docs/concepts/configuration/assign-pod-node/>afinidad de nodo</a> indicada.
Si no se configura ninguno de los dos, entonces el controlador del DaemonSet creará los Pods en todos los nodos.</p><h2 id=cómo-se-planifican-los-pods-procesos>Cómo se planifican los Pods procesos</h2><h3 id=planificados-por-el-controlador-del-daemonset-deshabilitado-por-defecto-a-partir-de-1-12>Planificados por el controlador del DaemonSet (deshabilitado por defecto a partir de 1.12)</h3><p>Normalmente, el planificador de Kubernetes determina la máquina donde se ejecuta un Pod. Sin embargo, los Pods
creados por el controlador del DaemonSet ya tienen la máquina seleccionada (puesto que cuando se crea el Pod,
se indica el campo <code>.spec.nodeName</code>, y por ello el planificador los ignora). Por lo tanto:</p><ul><li>El controlador del DaemonSet no tiene en cuenta el campo <a href=/docs/admin/node/#manual-node-administration><code>unschedulable</code></a> de un nodo.</li><li>El controlador del DaemonSet puede crear Pods incluso cuando el planificador no ha arrancado, lo cual puede ayudar en el arranque del propio clúster.</li></ul><h3 id=planificados-por-el-planificador-por-defecto-de-kubernetes-habilitado-por-defecto-desde-1-12>Planificados por el planificador por defecto de Kubernetes (habilitado por defecto desde 1.12)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [beta]</code></div><p>Un DaemonSet garantiza que todos los nodos elegibles ejecuten una copia de un Pod.
Normalmente, es el planificador de Kubernetes quien determina el nodo donde se ejecuta un Pod. Sin embargo,
los pods del DaemonSet son creados y planificados por el mismo controlador del DaemonSet.
Esto introduce los siguientes inconvenientes:</p><ul><li>Comportamiento inconsistente de los Pods: Los Pods normales que están esperando
a ser creados, se encuentran en estado <code>Pending</code>, pero los pods del DaemonSet no pasan por el estado <code>Pending</code>.
Esto confunde a los usuarios.</li><li>La <a href=/docs/concepts/configuration/pod-priority-preemption/>prioridad y el comportamiento de apropiación de Pods</a>
se maneja por el planificador por defecto. Cuando se habilita la contaminación, el controlador del DaemonSet
tomará la decisiones de planificación sin considerar ni la prioridad ni la contaminación del pod.</li></ul><p><code>ScheduleDaemonSetPods</code> permite planificar DaemonSets usando el planificador por defecto
en vez del controlador del DaemonSet, añadiendo la condición <code>NodeAffinity</code>
a los pods del DaemonSet, en vez de la condición <code>.spec.nodeName</code>. El planificador por defecto
se usa entonces para asociar el pod a su servidor destino. Si la afinidad de nodo del
pod del DaemonSet ya existe, se sustituye. El controlador del DaemonSet sólo realiza
estas operaciones cuando crea o modifica los pods del DaemonSet, y no se realizan cambios
al <code>spec.template</code> del DaemonSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>matchFields</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>metadata.name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- target-host-name<span style=color:#bbb>
</span></span></span></code></pre></div><p>Adicionalmente, se añade de forma automática la tolerancia <code>node.kubernetes.io/unschedulable:NoSchedule</code>
a los Pods del DaemonSet. Así, el planificador por defecto ignora los nodos
<code>unschedulable</code> cuando planifica los Pods del DaemonSet.</p><h3 id=contaminaciones-taints-y-tolerancias-tolerations>Contaminaciones (taints) y Tolerancias (tolerations)</h3><p>A pesar de que los Pods de proceso respetan las
<a href=/docs/concepts/configuration/taint-and-toleration>contaminaciones y tolerancias</a>,
la siguientes tolerancias son añadidas a los Pods del DaemonSet de forma automática
según las siguientes características:</p><table><thead><tr><th>Clave de tolerancia</th><th>Efecto</th><th>Versión</th><th>Descripción</th></tr></thead><tbody><tr><td><code>node.kubernetes.io/not-ready</code></td><td>NoExecute</td><td>1.13+</td><td>Los pods del DaemonSet no son expulsados cuando hay problemas de nodo como una partición de red.</td></tr><tr><td><code>node.kubernetes.io/unreachable</code></td><td>NoExecute</td><td>1.13+</td><td>Los pods del DaemonSet no son expulsados cuando hay problemas de nodo como una partición de red.</td></tr><tr><td><code>node.kubernetes.io/disk-pressure</code></td><td>NoSchedule</td><td>1.8+</td><td>Los pods del DaemonSet no son expulsados cuando hay problemas de nodo como la falta de espacio en disco.</td></tr><tr><td><code>node.kubernetes.io/memory-pressure</code></td><td>NoSchedule</td><td>1.8+</td><td>Los pods del DaemonSet no son expulsados cuando hay problemas de nodo como la falta de memoria.</td></tr><tr><td><code>node.kubernetes.io/unschedulable</code></td><td>NoSchedule</td><td>1.12+</td><td>Los pods del DaemonSet toleran los atributos unschedulable del planificador por defecto.</td></tr><tr><td><code>node.kubernetes.io/network-unavailable</code></td><td>NoSchedule</td><td>1.12+</td><td>Los pods del DaemonSet, que usan la red del servidor anfitrión, toleran los atributos network-unavailable del planificador por defecto.</td></tr></tbody></table><h2 id=comunicarse-con-los-pods-de-los-daemonsets>Comunicarse con los Pods de los DaemonSets</h2><p>Algunos patrones posibles para la comunicación con los Pods de un DaemonSet son:</p><ul><li><strong>Push</strong>: Los Pods del DaemonSet se configuran para enviar actualizaciones a otro servicio,
como una base de datos de estadísticas. No tienen clientes.</li><li><strong>NodeIP y Known Port</strong>: Los Pods del DaemonSet pueden usar un <code>hostPort</code>, de forma que se les puede alcanzar via las IPs del nodo. Los clientes conocen la lista de IPs del nodo de algún modo,
y conocen el puerto acordado.</li><li><strong>DNS</strong>: Se crea un <a href=/docs/concepts/services-networking/service/#headless-services>servicio headless</a> con el mismo selector de pod,
y entonces se descubre a los DaemonSets usando los recursos <code>endpoints</code> o mediante múltiples registros de tipo A en el DNS.</li><li><strong>Service</strong>: Se crea un servicio con el mismo selector de Pod, y se usa el servicio para llegar al proceso de uno de los nodos. (No hay forma de determinar el nodo exacto.)</li></ul><h2 id=actualizar-un-daemonset>Actualizar un DaemonSet</h2><p>Si se cambian las etiquetas de nodo, el DaemonSet comenzará de forma inmediata a añadir Pods a los nuevos nodos que coincidan y a eliminar
los Pods de aquellos nuevos nodos donde no coincidan.</p><p>Puedes modificar los Pods que crea un DaemonSet. Sin embargo, no se permite actualizar todos los campos de los Pods.
Además, el controlador del DaemonSet utilizará la plantilla original la próxima vez que se cree un nodo (incluso con el mismo nombre).</p><p>Puedes eliminar un DaemonSet. Si indicas el parámetro <code>--cascade=false</code> al usar <code>kubectl</code>,
entonces los Pods continuarán ejecutándose en los nodos. Así, puedes crear entonces un nuevo DaemonSet con una plantilla diferente.
El nuevo DaemonSet con la plantilla diferente reconocerá a todos los Pods existentes que tengan etiquetas coincidentes y
no modificará o eliminará ningún Pod aunque la plantilla no coincida con los Pods desplegados.
Entonces, deberás forzar la creación del nuevo Pod eliminando el Pod mismo o el nodo.</p><p>A partir de las versión 1.6 de Kubernetes, puedes <a href=/docs/tasks/manage-daemon/update-daemon-set/>llevar a cabo una actualización continua</a> en un DaemonSet.</p><h2 id=alternativas-al-daemonset>Alternativas al DaemonSet</h2><h3 id=secuencias-de-comandos-de-inicialización>Secuencias de comandos de inicialización</h3><p>Aunque es perfectamente posible ejecutar procesos arrancándolos directamente en un nodo (ej. usando
<code>init</code>, <code>upstartd</code>, o <code>systemd</code>), existen numerosas ventajas si se realiza via un DaemonSet:</p><ul><li>Capacidad de monitorizar y gestionar los logs de los procesos del mismo modo que para las aplicaciones.</li><li>Mismo lenguaje y herramientas de configuración (ej. plantillas de Pod, <code>kubectl</code>) tanto para los procesos como para las aplicaciones.</li><li>Los procesos que se ejecutan en contenedores con límitaciones de recursos aumentan el aislamiento entre dichos procesos y el resto de contenedores de aplicaciones.
Sin embargo, esto también se podría conseguir ejecutando los procesos en un contenedor en vez de un Pod
(ej. arrancarlos directamente via Docker).</li></ul><h3 id=pods-individuales>Pods individuales</h3><p>Es posible crear Pods directamente sin indicar el nodo donde ejecutarse. Sin embargo,
la ventaja del DaemonSet es que sustituye los Pods que se eliminan o terminan por cualquier razón, como en el caso
de un fallo del nodo o una intervención disruptiva de mantenimiento del nodo, como la actualización del kernel.
Por esta razón, deberías siempre utilizar un DaemonSet en vez de crear Pods individuales.</p><h3 id=pods-estáticos>Pods estáticos</h3><p>Es posible crear Pods a partir de archivos en el directorio donde está escuchando el proceso Kubelet.
Este tipo de Pods se denomina <a href=/docs/concepts/cluster-administration/static-pod/>pods estáticos</a>.
A diferencia del DaemonSet, los Pods estáticos no se pueden gestionar con kubectl
o cualquier otro cliente de la API de Kubernetes. Los Pods estáticos no dependen del apiserver, lo cual los hace
convenientes para el arranque inicial del clúster. Además, puede que los Pods estáticos se deprecien en el futuro.</p><h3 id=deployments>Deployments</h3><p>Los DaemonSets son similares a los <a href=/docs/concepts/workloads/controllers/deployment/>Deployments</a> en el sentido que
ambos crean Pods, y que dichos Pods tienen procesos que no se espera que terminen (ej. servidores web,
servidores de almacenamiento).</p><p>Utiliza un Deployment para definir servicios sin estado, como las interfaces de usuario, donde el escalado vertical y horizontal
del número de réplicas y las actualizaciones continuas son mucho más importantes que el control exacto del servidor donde se ejecuta el Pod.
Utiliza un DaemonSet cuando es importante que una copia de un Pod siempre se ejecute en cada uno de los nodos,
y cuando se necesite que arranque antes que el resto de Pods.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9add0d2120634b63073ad08dc8683bd6>4.2.6 - Recolección de Basura</h1><p>El papel del recolector de basura de Kubernetes es el de eliminar determinados objetos
que en algún momento tuvieron un propietario, pero que ahora ya no.</p><h2 id=propietarios-y-subordinados>Propietarios y subordinados</h2><p>Algunos objetos de Kubernetes son propietarios de otros objetos. Por ejemplo, un ReplicaSet
es el propietario de un conjunto de Pods. Los objetos que se poseen se denominan <em>subordinados</em> del
objeto propietario. Cada objeto subordinado tiene un campo <code>metadata.ownerReferences</code>
que apunta al objeto propietario.</p><p>En ocasiones, Kubernetes pone el valor del campo <code>ownerReference</code> automáticamente.
Por ejemplo, cuando creas un ReplicaSet, Kubernetes automáticamente pone el valor del campo
<code>ownerReference</code> de cada Pod en el ReplicaSet. A partir de la versión 1.8, Kubernetes
automáticamente pone el valor de <code>ownerReference</code> para los objetos creados o adoptados
por un ReplicationController, ReplicaSet, StatefulSet, DaemonSet, Deployment, Job
y CronJob.</p><p>También puedes configurar las relaciones entre los propietarios y sus subordinados
de forma manual indicando el valor del campo <code>ownerReference</code>.</p><p>Aquí se muestra un archivo de configuración para un ReplicaSet que tiene tres Pods:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/replicaset.yaml download=controllers/replicaset.yaml><code>controllers/replicaset.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-replicaset-yaml")' title="Copy controllers/replicaset.yaml to clipboard"></img></div><div class=includecode id=controllers-replicaset-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-repset<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pod-is-for</span>:<span style=color:#bbb> </span>garbage-collection-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pod-is-for</span>:<span style=color:#bbb> </span>garbage-collection-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Si se crea el ReplicaSet y entonces se muestra los metadatos del Pod, se puede
observar el campo OwnerReferences:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/replicaset.yaml
</span></span><span style=display:flex><span>kubectl get pods --output<span style=color:#666>=</span>yaml
</span></span></code></pre></div><p>La salida muestra que el propietario del Pod es el ReplicaSet denominado <code>my-repset</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: apps/v1
</span></span><span style=display:flex><span>    controller: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    kind: ReplicaSet
</span></span><span style=display:flex><span>    name: my-repset
</span></span><span style=display:flex><span>    uid: d9607e19-f88f-11e6-a518-42010a800195
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>No se recomienda el uso de OwnerReferences entre Namespaces por diseño. Esto quiere decir que:</p><ol><li>Los subordinados dentro del ámbito de Namespaces sólo pueden definir propietarios en ese mismo Namespace,
y propietarios dentro del ámbito de clúster.</li><li>Los subordinados dentro del ámbito del clúster sólo pueden definir propietarios dentro del ámbito del clúster, pero no
propietarios dentro del ámbito de Namespaces.</li></ol></div><h2 id=controlar-cómo-el-recolector-de-basura-elimina-los-subordinados>Controlar cómo el recolector de basura elimina los subordinados</h2><p>Cuando eliminas un objeto, puedes indicar si sus subordinados deben eliminarse también
de forma automática. Eliminar los subordinados automáticamente se denomina <em>borrado en cascada</em>.<br>Hay dos modos de <em>borrado en cascada</em>: <em>en segundo plano</em> y <em>en primer plano</em>.</p><p>Si eliminas un objeto sin borrar sus subordinados de forma automática,
dichos subordinados se convierten en <em>huérfanos</em>.</p><h3 id=borrado-en-cascada-en-primer-plano>Borrado en cascada en primer plano</h3><p>En el <em>borrado en cascada en primer plano</em>, el objeto raíz primero entra en un estado
llamado "deletion in progress". En este estado "deletion in progress",
se cumplen las siguientes premisas:</p><ul><li>El objeto todavía es visible a través de la API REST</li><li>Se pone el valor del campo <code>deletionTimestamp</code> del objeto</li><li>El campo <code>metadata.finalizers</code> del objeto contiene el valor "foregroundDeletion".</li></ul><p>Una vez que se pone el estado "deletion in progress", el recolector de basura elimina
los subordinados del objeto. Una vez que el recolector de basura ha eliminado todos
los subordinados "bloqueantes" (los objetos con <code>ownerReference.blockOwnerDeletion=true</code>), elimina
el objeto propietario.</p><p>Cabe mencionar que usando "foregroundDeletion", sólo los subordinados con valor en
<code>ownerReference.blockOwnerDeletion</code> bloquean la eliminación del objeto propietario.
A partir de la versión 1.7, Kubernetes añadió un <a href=/docs/reference/access-authn-authz/admission-controllers/#ownerreferencespermissionenforcement>controlador de admisión</a>
que controla el acceso de usuario cuando se intenta poner el campo <code>blockOwnerDeletion</code> a true
con base a los permisos de borrado del objeto propietario, de forma que aquellos subordinados no autorizados
no puedan retrasar la eliminación del objeto propietario.</p><p>Si un controlador (como un Deployment o un ReplicaSet) establece el valor del campo <code>ownerReferences</code> de un objeto,
se pone blockOwnerDeletion automáticamente y no se necesita modificar de forma manual este campo.</p><h3 id=borrado-en-cascada-en-segundo-plano>Borrado en cascada en segundo plano</h3><p>En el <em>borrado en cascada en segundo plano</em>, Kubernetes elimina el objeto propietario
inmediatamente y es el recolector de basura quien se encarga de eliminar los subordinados en segundo plano.</p><h3 id=configurar-la-regla-de-borrado-en-cascada>Configurar la regla de borrado en cascada</h3><p>Para controlar la regla de borrado en cascada, configura el campo <code>propagationPolicy</code>
del parámetro <code>deleteOptions</code> cuando elimines un objeto. Los valores posibles incluyen "Orphan",
"Foreground", o "Background".</p><p>Antes de la versión 1.9 de Kubernetes, la regla predeterminada del recolector de basura para la mayoría de controladores era <code>orphan</code>.
Esto incluía al ReplicationController, ReplicaSet, StatefulSet, DaemonSet, y al Deployment.
Para los tipos dentro de las versiones de grupo <code>extensions/v1beta1</code>, <code>apps/v1beta1</code>, y <code>apps/v1beta2</code>, a menos que
se indique de otra manera, los objetos subordinados se quedan huérfanos por defecto.
En Kubernetes 1.9, para todos los tipos de la versión de grupo <code>apps/v1</code>, los objetos subordinados se eliminan por defecto.</p><p>Aquí se muestra un ejemplo que elimina los subordinados en segundo plano:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Background&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>Aquí se muestra un ejemplo que elimina los subordinados en primer plano:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>Aquí se muestra un ejemplo de subordinados huérfanos:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>kubectl también permite el borrado en cascada.
Para eliminar los subordinados automáticamente, utiliza el parámetro <code>--cascade</code> a true.
Usa false para subordinados huérfanos. Por defecto, el valor de <code>--cascade</code>
es true.</p><p>Aquí se muestra un ejemplo de huérfanos de subordinados de un ReplicaSet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete replicaset my-repset --cascade<span style=color:#666>=</span><span style=color:#a2f>false</span>
</span></span></code></pre></div><h3 id=nota-adicional-sobre-los-deployments>Nota adicional sobre los Deployments</h3><p>Antes de la versión 1.7, cuando se usaba el borrado en cascada con Deployments se <em>debía</em> usar <code>propagationPolicy: Foreground</code>
para eliminar no sólo los ReplicaSets creados, sino también sus Pods correspondientes. Si este tipo de <em>propagationPolicy</em>
no se usa, solo se elimina los ReplicaSets, y los Pods se quedan huérfanos.
Ver <a href=https://github.com/kubernetes/kubeadm/issues/149#issuecomment-284766613>kubeadm/#149</a> para más información.</p><h2 id=problemas-conocidos>Problemas conocidos</h2><p>Seguimiento en <a href=https://github.com/kubernetes/kubernetes/issues/26120>#26120</a></p><h2 id=siguientes-pasos>Siguientes pasos</h2><p><a href=https://git.k8s.io/design-proposals-archive/api-machinery/garbage-collection.md>Documento de Diseño 1</a></p><p><a href=https://git.k8s.io/design-proposals-archive/api-machinery/synchronous-garbage-collection.md>Documento de Diseño 2</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4de50a37ebb6f2340484192126cb7a04>4.2.7 - Controlador TTL para Recursos Finalizados</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.12 [alpha]</code></div><p>El controlador TTL proporciona un mecanismo TTL para limitar el tiempo de vida de los objetos
de recurso que ya han terminado su ejecución. El controlador TTL sólo se ocupa de los
<a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/>Jobs</a> por el momento,
y puede que se extienda para gestionar otros recursos que terminen su ejecución,
como los Pods y los recursos personalizados.</p><p>Descargo de responsabilidad Alpha: esta característica está actualmente en versión alpha, y puede habilitarse mediante el
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
<code>TTLAfterFinished</code>.</p><h2 id=controlador-ttl>Controlador TTL</h2><p>El controlador TTL sólo soporta los Jobs por ahora. Un operador del clúster puede usar esta funcionalidad para limpiar
los Jobs terminados (bien <code>Complete</code> o <code>Failed</code>) automáticamente especificando el valor del campo
<code>.spec.ttlSecondsAfterFinished</code> del Job, como en este
<a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/#clean-up-finished-jobs-automatically>ejemplo</a>.
El controlador TTL asumirá que un recurso es candidato a ser limpiado
TTL segundos después de que el recurso haya terminado; dicho de otra forma, cuando el TTL haya expirado.
Cuando el controlador TTL limpia un recursos, lo elimina en cascada, esto es, borra
sus objetos subordinados juntos. Nótese que cuando se elimina un recurso,
se respetan las garantías de su ciclo de vida, como con los finalizadores.</p><p>Los segundos TTL pueden ser configurados en cualquier momento. Aquí se muestran algunos ejemplos para poner valores al campo
<code>.spec.ttlSecondsAfterFinished</code> de un Job:</p><ul><li>Indicando este campo en el manifiesto de los recursos, de forma que se pueda limpiar un Job
automáticamente un tiempo después de que haya finalizado.</li><li>Haciendo que el campo de los recursos existentes, ya finalizados, adopte esta nueva característica.</li><li>Usando un <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>mutating admission webhook</a>
para poner el valor de este campo dinámicamente en el momento de la creación del recursos. Los administradores del clúster pueden
usar este enfoque para forzar una regla TTL para los recursos terminados.</li><li>Usando un
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>mutating admission webhook</a>
para poner el valor de este campo dinámicamente después de que el recurso haya terminado,
y eligiendo diferentes valores TTL basados en los estados de los recursos, etiquetas, etc.</li></ul><h2 id=advertencia>Advertencia</h2><h3 id=actualizar-los-segundos-ttl>Actualizar los segundos TTL</h3><p>Cabe señalar que el período TTL , ej. campo <code>.spec.ttlSecondsAfterFinished</code> de los Jobs,
puede modificarse después de que el recurso haya sido creado o terminado. Sin embargo, una vez
que el Job se convierte en candidato para ser eliminado (cuando el TTL ha expirado), el sistema
no garantiza que se mantendrán los Jobs, incluso si una modificación para extender el TTL
devuelve una respuesta API satisfactoria.</p><h3 id=diferencia-horaria>Diferencia horaria</h3><p>Como el controlador TTL usa marcas de fecha/hora almacenadas en los recursos de Kubernetes
para determinar si el TTL ha expirado o no, esta funcionalidad es sensible a las
diferencias horarias del clúster, lo que puede provocar que el controlador TTL limpie recursos
en momentos equivocados.</p><p>En Kubernetes, se necesita ejecutar NTP en todos los nodos
(ver <a href=https://github.com/kubernetes/kubernetes/issues/6159#issuecomment-93844058>#6159</a>)
para evitar este problema. Los relojes no siempre son correctos, pero la diferencia debería ser muy pequeña.
Ten presente este riesgo cuando pongas un valor distinto de cero para el TTL.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><p><a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/#clean-up-finished-jobs-automatically>Limpiar Jobs automáticamente</a></p><p><a href=https://github.com/kubernetes/community/blob/master/keps/sig-apps/0026-ttl-after-finish.md>Documento de diseño</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-230b370ed7a1dedf04163f02fa701802>4.2.8 - Jobs - Ejecución hasta el final</h1><p>Un Job crea uno o más Pods y se asegura de que un número específico de ellos termina de forma satisfactoria.
Conforme los pods terminan satisfactoriamente, el Job realiza el seguimiento de las ejecuciones satisfactorias.
Cuando se alcanza un número específico de ejecuciones satisfactorias, la tarea (esto es, el Job) se completa.
Al eliminar un Job se eliminan los Pods que haya creado.</p><p>Un caso simple de uso es crear un objeto Job para que se ejecute un Pod de manera fiable hasta el final.
El objeto Job arrancará un nuevo Pod si el primer Pod falla o se elimina (por ejemplo
como consecuencia de un fallo de hardware o un reinicio en un nodo).</p><p>También se puede usar un Job para ejecutar múltiples Pods en paralelo.</p><h2 id=ejecutar-un-job-de-ejemplo>Ejecutar un Job de ejemplo</h2><p>Aquí se muestra un ejemplo de configuración de Job. Este ejemplo calcula los primeros 2000 decimales de π y los imprime por pantalla.
Tarda unos 10s en completarse.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/job.yaml download=controllers/job.yaml><code>controllers/job.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-job-yaml")' title="Copy controllers/job.yaml to clipboard"></img></div><div class=includecode id=controllers-job-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>4</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Puedes ejecutar el ejemplo con este comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/job.yaml
</span></span></code></pre></div><pre tabindex=0><code>job &#34;pi&#34; created
</code></pre><p>Comprueba el estado del Job con <code>kubectl</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe jobs/pi
</span></span></code></pre></div><pre tabindex=0><code>Name:             pi
Namespace:        default
Selector:         controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495
Labels:           controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495
                  job-name=pi
Annotations:      &lt;none&gt;
Parallelism:      1
Completions:      1
Start Time:       Tue, 07 Jun 2016 10:56:16 +0200
Pods Statuses:    0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:       controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495
                job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen    LastSeen    Count    From            SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----            -------------    --------    ------            -------
  1m           1m          1        {job-controller }                Normal      SuccessfulCreate  Created pod: pi-dtn4q
</code></pre><p>Para ver los Pods de un Job que se han completado, usa <code>kubectl get pods</code>.</p><p>Para listar todos los Pods que pertenecen a un Job de forma que sea legible, puedes usar un comando como:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>pods</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl get pods --selector<span style=color:#666>=</span>job-name<span style=color:#666>=</span>pi --output<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.items[*].metadata.name}&#39;</span><span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><pre tabindex=0><code>pi-aiw0a
</code></pre><p>En este caso, el selector es el mismo que el selector del Job. La opción <code>--output=jsonpath</code> indica un expresión
que simplemente obtiene el nombre de cada Pod en la lista devuelta.</p><p>Mira la salida estándar de uno de los Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ kubectl logs <span style=color:#b8860b>$pods</span>
</span></span><span style=display:flex><span>3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
</span></span></code></pre></div><h2 id=escribir-una-especificación-de-job>Escribir una especificación de Job</h2><p>Como con el resto de configuraciones de Kubernetes, un Job necesita los campos <code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>.</p><p>Un Job también necesita la <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>sección <code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>El campo <code>.spec.template</code> es el único campo obligatorio de <code>.spec</code>.</p><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/pods/pod-overview/#pod-templates>plantilla Pod</a>. Tiene exactamente el mismo esquema que un <a href=/docs/user-guide/pods>pod</a>,
excepto por el hecho de que está anidado y no tiene el campo <code>apiVersion</code> o <code>kind</code>.</p><p>Además de los campos olbigatorios de un Pod, una plantilla Pod de un Job debe indicar las etiquetas apropiadas
(ver <a href=#pod-selector>selector de pod</a>) y una regla de reinicio apropiada.</p><p>Sólo se permite los valores <code>Never</code> o <code>OnFailure</code> para <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>RestartPolicy</code></a>.</p><h3 id=selector-de-pod>Selector de Pod</h3><p>El campo <code>.spec.selector</code> es opcional. En la práctica mayoría de los casos no deberías configurarlo.
Mira la sección sobre <a href=#specifying-your-own-pod-selector>configurar tu propio selector de pod</a>.</p><h3 id=jobs-en-paralelo>Jobs en paralelo</h3><p>Hay tres tipos principales de tarea aptos para ejecutarse como un Job:</p><ol><li>Jobs no paralelos</li></ol><ul><li>normalmente, sólo se arranca un Pod, a menos que el Pod falle.</li><li>el Job se completa tan pronto como su Pod termine de forma satisfactoria.</li></ul><ol><li>Jobs en paralelo con un <em>cupo fijo de terminación</em>:</li></ol><ul><li>se configura un valor positivo distinto de cero para el campo <code>.spec.completions</code>.</li><li>el Job representa la tarea en general, y se completa cuando hay una ejecución satisfactoria de un Pod por cada valor dentro del rango de 1 a <code>.spec.completions</code>.</li><li><strong>no implementado todavía:</strong> A cada Pod se le pasa un índice diferenente dentro del rango de 1 a <code>.spec.completions</code>.</li></ul><ol><li>Jobs en paralelo con una <em>cola de trabajo</em>:</li></ol><ul><li>no se especifica el campo <code>.spec.completions</code>, por defecto <code>.spec.parallelism</code>.</li><li>los Pods deben coordinarse entre ellos mismos o a través de un servicio externo que determine quién debe trabajar en qué.
Por ejemplo, un Pod podría ir a buscar un lote de hasta N ítems de una cola de trabajo.</li><li>cada Pod es capaz de forma independiente de determinar si sus compañeros han terminado o no, y como consecuencia el Job entero ha terminado.</li><li>cuando <em>cualquier</em> Pod del Job termina con éxito, no se crean nuevos Pods.</li><li>una vez que al menos uno de los Pods ha terminado con éxito y todos los Pods han terminado, entonces el Job termina con éxito.</li><li>una vez que cualquier Pod ha terminado con éxito, ningún otro Pod debería continuar trabajando en la misma tarea o escribiendo ningún resultado. Todos ellos deberían estar en proceso de terminarse.</li></ul><p>En un Job <em>no paralelo</em>, no debes indicar el valor de <code>.spec.completions</code> ni <code>.spec.parallelism</code>. Cuando ambos se dejan
sin valor, ambos se predeterminan a 1.</p><p>En un Job con <em>cupo fijo de terminación</em>, deberías poner el valor de <code>.spec.completions</code> al número de terminaciones que se necesiten.
Puedes dar un valor a <code>.spec.parallelism</code>, o dejarlo sin valor, en cuyo caso se predetermina a 1.</p><p>En un Job con <em>cola de trabajo</em>, no debes indicar el valor de <code>.spec.completions</code>, y poner el valor de <code>.spec.parallelism</code> a
un entero no negativo.</p><p>Para más información acerca de cómo usar los distintos tipos de Job, ver la sección de <a href=#job-patterns>patrones de job</a>.</p><h4 id=controlar-el-paralelismo>Controlar el paralelismo</h4><p>El paralelismo solicitado (<code>.spec.parallelism</code>) puede usar cualquier valor no negativo.
Si no se indica, se predeterminad a 1.
Si se indica como 0, entonces el Job se pausa de forma efectiva hasta que se incremente.</p><p>El paralelismo actual (número de pods ejecutándose en cada momento) puede que sea mayor o menor que el solicitado,
por los siguientes motivos:</p><ul><li>Para los Jobs con <em>cupo fijo de terminaciones</em>, el número actual de pods ejecutándose en paralelo no excede el número de terminaciones pendientes.
Los valores superiores de <code>.spec.parallelism</code> se ignoran.</li><li>Para los Jobs con <em>cola de trabajo</em>, no se arranca nuevos Pods después de que cualquier Pod se haya completado -- sin embargo, se permite que se completen los Pods pendientes.</li><li>Cuando el controlador no ha tenido tiempo para reaccionar.</li><li>Cuando el controlador no pudo crear los Pods por el motivo que fuera (falta de <code>ResourceQuota</code>, falta de permisos, etc.),
entonces puede que haya menos pods que los solicitados.</li><li>El controlador puede que regule la creación de nuevos Pods debido al excesivo número de fallos anteriores en el mismo Job.</li><li>Cuando un Pod se para de forma controlada, lleva tiempo pararlo.</li></ul><h2 id=gestionar-fallos-de-pod-y-contenedor>Gestionar Fallos de Pod y Contenedor</h2><p>Un contenedor de un Pod puede fallar por cualquier motivo, como porque el proceso que se estaba ejecutando termina con un código de salida distinto de cero,
o porque se mató el contenedor por exceder un límite de memoria, etc. Si esto ocurre, y se tiene
<code>.spec.template.spec.restartPolicy = "OnFailure"</code>, entonces el Pod permance en el nodo,
pero el contenedor se vuelve a ejecutar. Por lo tanto, tu aplicación debe poder gestionar el caso en que se reinicia de forma local,
o bien especificar <code>.spec.template.spec.restartPolicy = "Never"</code>.
Ver el <a href=/docs/concepts/workloads/pods/pod-lifecycle/#example-states>ciclo de vida de un pod</a> para más información sobre <code>restartPolicy</code>.</p><p>Un Pod entero puede también fallar por cualquier motivo, como cuando se expulsa al Pod del nodo
(porque el nodo se actualiza, reinicia, elimina, etc.), o si un contenedor del Pod falla
cuando <code>.spec.template.spec.restartPolicy = "Never"</code>. Cuando un Pod falla, entonces el controlador del Job
arranca un nuevo Pod. Esto quiere decir que tu aplicación debe ser capaz de gestionar el caso en que se reinicia en un nuevo pod.
En particular, debe ser capaz de gestionar los ficheros temporales, los bloqueos, los resultados incompletos, y cualquier otra dependencia
de ejecuciones previas.</p><p>Nótese que incluso si se configura <code>.spec.parallelism = 1</code> y <code>.spec.completions = 1</code> y
<code>.spec.template.spec.restartPolicy = "Never"</code>, el mismo programa puede arrancarse dos veces.</p><p>Si se especifica <code>.spec.parallelism</code> y <code>.spec.completions</code> con valores mayores que 1,
entonces puede que haya múltiples pods ejecutándose a la vez. Por ello, tus pods deben tolerar la concurrencia.</p><h3 id=regla-de-retroceso-de-pod-por-fallo>Regla de retroceso de Pod por fallo</h3><p>Hay situaciones en que quieres que el Job falle después de intentar ejecutarlo unas cuantas veces debido
a un error lógico en la configuración, etc.
Para hacerlo, pon el valor de <code>.spec.backoffLimit</code> al número de reintentos que quieres
antes de considerar el Job como fallido. El límite de retroceso se predetermina a 6.
Los Pods fallidos asociados al Job son recreados por el controlador del Job con un
retroceso exponencial (10s, 20s, 40s ...) limitado a seis minutos. El contador
de retroceso se resetea si no aparecen Pods fallidos antes del siguiente chequeo de estado del Job.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El problema <a href=https://github.com/kubernetes/kubernetes/issues/54870>#54870</a> todavía existe en las versiones de Kubernetes anteriores a la versión 1.12</div><h2 id=terminación-y-limpieza-de-un-job>Terminación y Limpieza de un Job</h2><p>Cuando un Job se completa, ya no se crea ningún Pod, pero tampoco se elimina los Pods. Guardarlos permite
ver todavía los logs de los pods acabados para comprobar errores, avisos, o cualquier otro resultado de diagnóstico.
El objeto job también se conserva una vez que se ha completado para que se pueda ver su estado. Es decisión del usuario si elimina
los viejos jobs después de comprobar su estado. Eliminar el job con el comando <code>kubectl</code> (ej. <code>kubectl delete jobs/pi</code> o <code>kubectl delete -f ./job.yaml</code>).
Cuando eliminas un job usando el comando <code>kubectl</code>, todos los pods que creó se eliminan también.</p><p>Por defecto, un Job se ejecutará de forma ininterrumpida a menos que uno de los Pods falle, en cuyo caso el Job se fija en el valor de
<code>.spec.backoffLimit</code> descrito arriba. Otra forma de acabar un Job es poniéndole un vencimiento activo.
Haz esto poniendo el valor del campo <code>.spec.activeDeadlineSeconds</code> del Job a un número de segundos.</p><p>El campo <code>activeDeadlineSeconds</code> se aplica a la duración del job, independientemente de cuántos Pods se hayan creado.
Una vez que el Job alcanza <code>activeDeadlineSeconds</code>, se terminan todos sus Pods y el estado del Job se pone como <code>type: Failed</code> con <code>reason: DeadlineExceeded</code>.</p><p>Fíjate que el campo <code>.spec.activeDeadlineSeconds</code> de un Job tiene precedencia sobre el campo <code>.spec.backoffLimit</code>.
Por lo tanto, un Job que está reintentando uno o más Pods fallidos no desplegará nuevos Pods una vez que alcance el límite de tiempo especificado por <code>activeDeadlineSeconds</code>,
incluso si todavía no se ha alcanzado el <code>backoffLimit</code>.</p><p>Ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi-with-timeout<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>activeDeadlineSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><p>Fíjate que tanto la especificación del Job como la <a href=/docs/concepts/workloads/pods/init-containers/#detailed-behavior>especificación de la plantilla Pod</a>
dentro del Job tienen un campo <code>activeDeadlineSeconds</code>. Asegúrate que pones el valor de este campo de forma adecuada.</p><h2 id=limpiar-los-jobs-terminados-automáticamente>Limpiar los Jobs terminados automáticamente</h2><p>Normalmente, los Jobs que han terminado ya no se necesitan en el sistema. Conservarlos sólo añade
más presión al servidor API. Si dichos Jobs no se gestionan de forma directa por un controlador de más alto nivel,
como los <a href=/docs/concepts/workloads/controllers/cron-jobs/>CronJobs</a>, los Jobs pueden
limpiarse por medio de CronJobs en base a la regla de limpieza basada en capacidad que se haya especificado.</p><h3 id=mecanismo-ttl-para-jobs-terminados>Mecanismo TTL para Jobs terminados</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.12 [alpha]</code></div><p>Otra forma de limpiar los Jobs terminados (bien <code>Complete</code> o <code>Failed</code>)
de forma automática es usando un mecanismo TTL proporcionado por un
<a href=/docs/concepts/workloads/controllers/ttlafterfinished/>controlador TTL</a> de recursos finalizados,
indicando el valor <code>.spec.ttlSecondsAfterFinished</code> del Job.</p><p>Cuando el controlador TTL limpia el Job, lo eliminará en cascada,
esto es, eliminará sus objetos subordinados, como Pods, junto con el Job. Nótese
que cuando se elimina el Job, sus garantías de ciclo de vida, como los finalizadores,
se tendrán en cuenta.</p><p>Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi-with-ttl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ttlSecondsAfterFinished</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><p>Aquí el Job <code>pi-with-ttl</code> será candidato a ser automáticamente eliminado, <code>100</code>
segundos después de que termine.</p><p>Si el campo se pone a <code>0</code>, el Job será candidato a ser automáticamente eliminado
inmediatamente después de haber terminado. Si no se pone valor al campo, este Job no será eliminado
por el controlador TTL una vez concluya.</p><p>Nótese que este mecanismo TTL está todavía en alpha, a través de la característica denominada <code>TTLAfterFinished</code>.
Para más información, ver la documentación del <a href=/docs/concepts/workloads/controllers/ttlafterfinished/>controlador TTL</a> para
recursos terminados.</p><h2 id=patrones-de-job>Patrones de Job</h2><p>El objeto Job puede usarse para dar soporte a la ejecución fiable de Pods en paralelo. El objeto Job
no se diseñó para dar soporte a procesos paralelos estrechamente comunicados, como los que comúnmente
se encuentran en la computación científica. Eso sí, permite el proceso paralelo de un conjunto de <em>ítems de trabajo</em> independientes, pero relacionados entre sí.
Estos pueden ser correos a enviar, marcos a renderizar, archivos a codificar, rangos de claves en una base de datos NoSQL a escanear, y demás.</p><p>En un sistema complejo, puede haber múltiples diferentes conjuntos de ítems de trabajo. Aquí sólo se está
considerando un conjunto de ítems de trabajo que el usuario quiere gestionar de forma conjunta — un <em>proceso por lotes</em>.</p><p>Hay varios patrones diferentes para computación en paralelo, cada uno con sus fortalezas y sus debilidades.
Los sacrificios a tener en cuenta son:</p><ul><li>Un objeto Job para cada ítem de trabajo vs. un objeto Job simple para todos los ítems de trabajo. El último es mejor
para grandes números de ítems de trabajo. El primero añade sobrecarga para el usuario y para el sistema
al tener que gestionar grandes números de objetos Job.</li><li>El número de pods creados es igual al número de ítems de trabajo vs. cada Pod puede procesar múltiplese ítems de trabajo.
El primero típicamente requiere menos modificaciones al código existente y a los contenedores.
El último es mejor cuanto mayor sea el número de ítems de trabajo, por las mismas razones que antes..</li><li>Varios enfoques usan una cola de trabajo. Ello requiere ejecutar un servicio de colas,
y modificaciones a las aplicaciones o contenedores existentes para que hagan uso de la cola de trabajo.
Otras estrategias son más fáciles de adaptar a una aplicación ya usando contenedores.</li></ul><p>Los sacrificios a tener en cuenta se indican a continuación, donde las columnas 2 a 4 representan los sacrificios de arriba.
Los nombres de los patrones son también enlaces a ejemplos e información más detallada.</p><table><thead><tr><th>Patrón</th><th style=text-align:center>Objeto Job simple</th><th style=text-align:center>¿Menos pods que ítems de trabajo?</th><th style=text-align:center>¿No modificar la aplicación?</th><th style=text-align:center>¿Funciona en Kube 1.1?</th></tr></thead><tbody><tr><td><a href=/docs/tasks/job/parallel-processing-expansion/>Extensión de la Plantilla Job</a></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>✓</td><td style=text-align:center>✓</td></tr><tr><td><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Cola con Pod por Ítem de Trabajo</a></td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>a veces</td><td style=text-align:center>✓</td></tr><tr><td><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Cola con Cuenta Variable de Pods</a></td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>✓</td></tr><tr><td>Job simple con Asignación Estática de Trabajo</td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>✓</td><td style=text-align:center></td></tr></tbody></table><p>Cuando se especifican terminaciones con <code>.spec.completions</code>, cada Pod creado por el controlado del Job
tiene un <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>spec</code></a>idéntico.
Esto significa que todos los pods de una tarea tendrán la misma línea de comandos y la
misma imagne, los mismo volúmenes, y (casi) las mismas variables de entorno.
Estos patrones otorgan diferentes formas de organizar los pods para que trabajen en cosas distintas.</p><p>Esta tabla muestra la configuración necesaria para <code>.spec.parallelism</code> y <code>.spec.completions</code> para cada uno de los patrones.
Aquí, <code>T</code> es el número de ítems de trabajo.</p><table><thead><tr><th>Patrón</th><th style=text-align:center><code>.spec.completions</code></th><th style=text-align:center><code>.spec.parallelism</code></th></tr></thead><tbody><tr><td><a href=/docs/tasks/job/parallel-processing-expansion/>Extensión de la Plantilla Job</a></td><td style=text-align:center>1</td><td style=text-align:center>debería ser 1</td></tr><tr><td><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Cola con Pod por Ítem de Trabajo</a></td><td style=text-align:center>T</td><td style=text-align:center>cualquiera</td></tr><tr><td><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Cola con Cuenta Variable de Pods</a></td><td style=text-align:center>1</td><td style=text-align:center>cualquiera</td></tr><tr><td>Job simple con Asignación Estática de Trabajo</td><td style=text-align:center>T</td><td style=text-align:center>cualquiera</td></tr></tbody></table><h2 id=uso-avanzado>Uso Avanzado</h2><h3 id=especificar-tu-propio-selector-de-pod>Especificar tu propio selector de pod</h3><p>Normalmente, cuando creas un objeto Job, no especificas el campo <code>.spec.selector</code>.
La lógica por defecto del sistema añade este campo cuando se crea el Job.
Se elige un valor de selector que no se entremezcle con otras tareas.</p><p>Sin embargo, en algunos casos, puede que necesites sobreescribir este selector que se configura de forma automática.
Para ello, puedes indicar el valor de <code>.spec.selector</code> en el Job.</p><p>Pero ten mucho cuidado cuando lo hagas. Si configuras un selector de etiquta que no
es único para los pods de ese Job, y que selecciona Pods que no tienen que ver,
entonces estos últimos pueden ser eliminados, o este Job puede contar los otros
Pods para terminarse, o uno o ambos Jobs pueden negarse a crear Pods o ejecutarse hasta el final.
Si se elige un selector que no es único, entonces otros controladores (ej. ReplicationController)
y sus Pods puede comportarse de forma impredecibles también. Kubernetes no te impide cometer un error
especificando el <code>.spec.selector</code>.</p><p>Aquí se muestra un ejemplo de un caso en que puede que necesites usar esta característica.</p><p>Digamos que el Job <code>viejo</code> todavía está ejeuctándose. Quieres que los Pods existentes
sigan corriendo, pero quieres que el resto de los Pods que se creen
usen una plantilla pod diferente y que el Job tenga un nombre nuevo.
Como no puedes modificar el Job porque esos campos no son modificables, eliminas el Job <code>old</code>,
pero <em>dejas sus pods ejecutándose</em> mediante el comando <code>kubectl delete jobs/old --cascade=false</code>.
Antes de eliminarlo, apúntate el selector actual que está usando:</p><pre tabindex=0><code>kind: Job
metadata:
  name: viejo
  ...
spec:
  selector:
    matchLabels:
      job-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
</code></pre><p>Entonces, creas un nuevo Job con el nombre <code>nuevo</code> y le configuras explícitamente el mismo selector.
Puesto que los Pods existentes tienen la etiqueta <code>job-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002</code>,
son controlados por el Job <code>nuevo</code> igualmente.</p><p>Necesitas configurar <code>manualSelector: true</code> en el nuevo Job, ya qye no estás usando
el selector que normalmente se genera de forma automática por el sistema.</p><pre tabindex=0><code>kind: Job
metadata:
  name: nuevo
  ...
spec:
  manualSelector: true
  selector:
    matchLabels:
      job-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
</code></pre><p>El mismo Job nuevo tendrá un uid distinto a <code>a8f3d00d-c6d2-11e5-9f87-42010af00002</code>.
Poniendo <code>manualSelector: true</code> le dice al sistema que sabes lo que estás haciendo
y que te permita hacer este desajuste.</p><h2 id=alternativas>Alternativas</h2><h3 id=pods-simples>Pods simples</h3><p>Cuando el nodo donde un Pod simple se estaba ejecutando se reinicia o falla, dicho pod se termina
y no será reinicado. Sin embargo, un Job creará nuevos Pods para sustituir a los que se han terminando.
Por esta razón, se recomienda que se use un Job en vez de un Pod simple, incluso si tu aplicación
sólo necesita un único Pod.</p><h3 id=replication-controller>Replication Controller</h3><p>Los Jobs son complementarios a los <a href=/docs/user-guide/replication-controller>Replication Controllers</a>.
Un Replication Controller gestiona aquellos Pods que se espera que no terminen (ej. servidores web), y un Job
gestiona aquellos Pods que se espera que terminen (ej. tareas por lotes).</p><p>Como se discutió en el <a href=/docs/concepts/workloads/pods/pod-lifecycle/>Ciclo de vida de un Pod</a>, un <code>Job</code> <em>sólo</em> es apropiado
para aquellos pods con <code>RestartPolicy</code> igual a <code>OnFailure</code> o <code>Never</code>.
(Nota: Si <code>RestartPolicy</code> no se pone, el valor predeterminado es <code>Always</code>.)</p><h3 id=job-simple-arranca-que-arranca-un-controlador-de-pod>Job simple arranca que arranca un controlador de Pod</h3><p>Otro patrón es aquel donde un Job simple crea un Pod que, a su vez, crea otros Pods, actuando como una especie
de controlador personalizado para esos Pods. Esto da la máxima flexibilidad, pero puede que
cueste un poco más de entender y ofrece menos integración con Kubernetes.</p><p>Un ejemplo de este patrón sería un Job que arranca un Pod que ejecuta una secuencia de comandos que, a su vez,
arranca un controlador maestro de Spark (ver el <a href=https://github.com/kubernetes/examples/tree/master/staging/spark/README.md>ejemplo de spark</a>),
ejecuta un manejador de spark, y a continuación lo limpia todo.</p><p>Una ventaja de este enfoque es que el proceso general obtiene la garantía del objeto Job,
además del control completo de los Pods que se crean y cómo se les asigna trabajo.</p><h2 id=cron-jobs>Cron Jobs</h2><p>Puedes utilizar un <a href=/docs/concepts/workloads/controllers/cron-jobs/><code>CronJob</code></a> para crear un Job que se ejecute en una hora/fecha determinadas, de forma similar
a la herramienta <code>cron</code> de Unix.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2e4cec01c525b45eccd6010e21cc76d9>4.2.9 - CronJob</h1><p>Un <em>Cron Job</em> ejecuta tareas, <a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/>Jobs</a>, a intervalos regulares.</p><p>Un objeto CronJob es como una línea de un archivo <em>crontab</em> (tabla cron). Ejecuta un trabajo de forma periódica
según un horario programado escrito en formato <a href=https://en.wikipedia.org/wiki/Cron>Cron</a>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Todos los <code>horarios</code> <strong>CronJob</strong> se basan en la zona horaria del máster donde se inicia el trabajo.</div><p>Para instrucciones sobre cómo crear y trabajar con trabajos programados,
incluyendo definiciones de ejemplo,
puedes consultar <a href=/docs/tasks/job/automated-tasks-with-cron-jobs>Ejecutar tareas automatizadas con trabajos programados</a>.</p><h2 id=limitaciones-de-las-tareas-programados>Limitaciones de las tareas programados</h2><p>Un trabajo programado crea un objeto job <em>como mínimo</em> una vez por cada ejecución de su programación. Decimos "como mínimo" porque
hay determinadas circunstancias bajo las cuales dos trabajos pueden crearse, o ninguno de ellos se crea. Se intenta que estos casos sean residuales,
pero no pueden evitarse completamente. Por lo tanto, los trabajos deberían ser <em>idempotentes</em>, es decir, que se pueden ejecutar más de una vez con el mismo resultado.</p><p>Si el valor de <code>startingDeadlineSeconds</code> se establece a un valor grande o se deja sin especificar (por defecto)
y si el valor de <code>concurrencyPolicy</code> se establece a <code>Allow</code>, los trabajos siempre se ejecutarán por lo menos una vez.</p><p>Para cada CronJob, el controlador de CronJob verifica cuántas programaciones se han perdido desde la última programación hasta el momento actual.
Si hay más de 100 programaciones perdidas, entonces ya no vuelve a ejecutar el trabajo y registra el error:</p><pre tabindex=0><code>Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
</code></pre><p>Es importante destacar que si el campo <code>startingDeadlineSeconds</code> está configurado, es decir, no es nulo (<code>nil</code>), el controlador cuenta cuántos trabajos perdidos se produjeron desde el valor de <code>startingDeadlineSeconds</code>
hasta el momento actual, en vez de la última programación. Por ejemplo, si <code>startingDeadlineSeconds</code> es <code>200</code>, el controlador cuenta cuántos trabajos perdidos se produjeron en los últimos 200 segundos.</p><p>Se cuenta un CronJob como perdido si no se ha podido crear a la hora programada. Por ejemplo, si establecemos el valor de <code>concurrencyPolicy</code> a <code>Forbid</code> y se intentó programar
un CronJob cuando otro previamente programado estaba todavía ejecutándose, entonces contará como perdido.</p><p>Por ejemplo, imagina que un CronJob se configura para programar un nuevo Job cada minuto a partir de las <code>08:30:00</code>, y su campo
<code>startingDeadlineSeconds</code> no se configura. Si el controlador del CronJob no estuviera disponible de <code>08:29:00</code> a <code>10:21:00</code>,
el trabajo no comenzaría porque el número de trabajos perdidos que se habría perdido en su programación sería superior a 100.</p><p>Para ilustrar este concepto mejor, vamos a suponer que programamos un CronJob para que ejecute un nuevo Job cada minuto comenzando a las <code>08:30:00</code>, y establecemos el valor del campo
<code>startingDeadlineSeconds</code> a 200 segundos. Si el controlador del CronJob no se encuentra disponible
durante el mismo período que en el ejemplo anterior (<code>08:29:00</code> a <code>10:21:00</code>,) aún así el Job comenzará a las 10:22:00.
Esto ocurre porque el controlador en este caso comprueba cuántas programaciones perdidas ha habido en los últimos 200 segundos (esto es, 3 programaciones que no se han ejecutado), en vez de comprobarlo a partir de la última programación hasta el momento actual.</p><p>El CronJob es únicamente responsable de crear los Jobs que coinciden con su programación, y
el Job por otro lado es el responsable de gestionar los Pods que representa.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0a0a7eca3e302a3c08f8c85e15d337fd>5 - Servicios, balanceo de carga y redes</h1></div><div class=td-content><h1 id=pg-5701136fd2ce258047b6ddc389112352>5.1 - Service</h1>Un Service, servicio en castellano, es el objeto de la API de Kubernetes que describe cómo se accede a las aplicaciones, tal como un conjunto de <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>, y que puede describir puertos y balanceadores de carga.<p>Con Kubernetes no necesitas modificar tu aplicación para que utilice un mecanismo de descubrimiento de servicios desconocido.
Kubernetes le otorga a sus Pods su propia dirección IP y un nombre DNS para un conjunto de Pods, y puede balancear la carga entre ellos.</p><h2 id=motivación>Motivación</h2><p>Los <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> de Kubernetes son creados y destruidos para coincidir con el estado de tu clúster.
Los Pods son recursos no permanentes. Si utilizas un <a class=glossary-tooltip title='Un objeto API que gestiona una aplicación replicada.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a> para correr tu aplicación, puede crear y
destruir los Pods dinámicamente.</p><p>Cada Pod obtiene su propia dirección IP, sin embargo, en un Deployment, el conjunto de Pods corriendo en un momento dado puede ser diferente al
conjunto de Pods corriendo esa aplicación un momento después.</p><p>Esto conlleva un problema: si un conjunto de Pods (llamémoslos "backends") provee funcionalidad a otros Pods (llamémoslos "frontends") dentro de tu clúster,
¿de qué manera los frontends encuentran y tienen seguimiento de cuál dirección IP conectarse, para que el frontend pueda usar la parte del backend de la carga de trabajo?</p><p>Entran los <em>Services</em>.</p><h2 id=service-resource>Recursos Service</h2><p>En Kubernetes, un Service es una abstracción que define un conjunto lógico de Pods y una política por la cual acceder a ellos
(algunas veces este patrón es llamado micro-servicio). El conjunto de Pods a los que apunta un Servicio se determina usualmente por un <a class=glossary-tooltip title='Permite a los usuarios filtrar recursos por .' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=Selector>Selector</a>.
Para aprender más sobre otras maneras de definir Endpoints para un Service, mira <a href=#services-sin-selectores>Services sin selectores</a>.</p><p>Por ejemplo, considera un backend sin estado para procesar imágenes que está corriendo con 3 réplicas. Estas réplicas son fungibles; a los frontends no les importa
cuál backend usar. Mientras que los Pods actuales que componen el backend pueden cambiar, los clientes del frontend no deberían estar al tanto de ello, ni deberían llevar un seguimiento
del conjunto de backends en sí mismos.</p><p>La abstracción del Service habilita este desacoplamiento.</p><h3 id=descubrimiento-de-servicios-nativos-en-la-nube>Descubrimiento de servicios nativos en la nube</h3><p>Si eres capaz de usar la API de Kubernetes para descubrir servicios en tu aplicación,
puedes hacer una búsqueda en el <a class=glossary-tooltip title='Componente del plano de control que expone la API de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-apiserver/ target=_blank aria-label='servidor API'>servidor API</a> para los Endpoints, que se actualizan cuando cambian el conjunto de Pods en el servicio.</p><p>Para aplicaciones no nativas, Kubernetes ofrece una manera de colocar un puerto de red o un balanceador de carga entre tu aplicación y los Pods del backend.</p><h2 id=definiendo-un-service>Definiendo un Service</h2><p>Un Service en Kubernetes es un objeto REST, similar a un Pod. Como todos los objetos REST, puedes hacer un <code>Post</code> a una definición de un Service al servidor API para crear una nueva instancia.
EL nombre de un objeto Service debe ser un <a href=/docs/concepts/overview/working-with-objects/names#rfc-1035-label-names>nombre RFC 1035 válido</a>.</p><p>Por ejemplo, supongamos que tienes un conjunto de Pods en el que cada uno escucha el puerto TCP 9376 y contiene la etiqueta <code>app.kubernetes.io/name=MyApp</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Esta especificación crea un nuevo objeto Service llamado "mi-servicio", que apunta via TCP al puerto 9376 de cualquier Pod con la etiqueta <code>app.kubernetes.io/name=MyApp</code>.</p><p>Kubernetes asigna una dirección IP a este Service (Algunas veces llamado "Cluster IP"), la cual es usada por los proxies de los Services (mira <a href=#virtual-ips-and-service-proxies>IPs Virtuales y proxies de servicios</a> abajo).</p><p>El controlador para el selector del Service escanea continuamente a los Pods que coincidan con este selector, y luego hace un Post de cualquier actualización a un objeto Endpoint llamado también "mi-servicio".</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Un Service puede mapear <em>cualquier</em> <code>port</code> de entrada a un <code>targetPort</code>. Por defecto y conveniencia, el <code>targetPort</code> se establece con el mismo valor que el campo <code>port</code>.</div><p>Las definiciones de puerto en los Pods tienen nombres, y puedes hacer referencia a estos nombres en el atributo <code>targetPort</code> del Service. Esto funciona incluso si existe una mezcla
de Pods en el Service usando un único nombre configurado, con el mismo protocolo de red disponible via diferentes números de puerto.
Esto ofrece mucha flexibilidad para desplegar y evolucionar tus Services. Por ejemplo, puedes cambiar los números de puertos que los Pods exponen en la siguiente versión de tu software backend, sin romper los clientes.</p><p>El protocolo por defecto para los Services is TCP; también puedes usar cualquier otro <a href=#protocol-support>protocolo soportado</a>.</p><p>Como muchos Services necesitan exponer más de un puerto, Kubernetes soporta múltiples definiciones de puertos en un único objeto Service.
Cada definición de un puerto puede tener el mismo protocolo, o uno diferente</p><h3 id=services-sin-selectores>Services sin selectores</h3><p>Los Services comúnmente abstraen el acceso a los Pods de Kubernetes, pero también pueden abstraer otros tipos de backends.</p><p>Por ejemplo:</p><ul><li>Quieres tener un clúster de base de datos externo en producción, pero en el entorno de pruebas quieres usar tus propias bases de datos.</li><li>Quieres apuntar tu Service a un Service en un <a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=Namespace>Namespace</a> o en un clúster diferente.</li><li>Estás migrando tu carga de trabajo a Kubernetes. Mientras evalúas la aproximación, corres solo una porción de tus backends en Kubernetes.</li></ul><p>En cualquiera de estos escenarios puedes definir un Service <em>sin</em> un selector de Pod.</p><p>Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Debido a que este Service no tiene un selector, el objeto Endpoints no se crea de forma automática.
Puedes mapear manualmente el Service a la dirección de red y puerto donde está corriendo, añadiendo el objeto Endpoints manualmente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Endpoints<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>subsets</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>addresses</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>ip</span>:<span style=color:#bbb> </span><span style=color:#666>192.0.2.42</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>El nombre del objeto Endpoints debe ser un <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nombre de subdominio DNS válido</a>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Las direcciones IPs <em>no deben</em> ser: loopback (127.0.0.0/8 para IPv4, ::1/128 para IPv6), o
link-local (169.254.0.0/16 and 224.0.0.0/24 para IPv4, fe80::/64 para IPv6).</p><p>Las direcciones IP del Endpoint no pueden ser IPs de clúster de otros Services de Kubernetes, debido a que el
<a class=glossary-tooltip title='kube-proxy es un componente de red que se ejecuta en cada nodo del clúster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a> no soporta IPs virtuales como destino.</p></div><p>Acceder a un Service sin un selector funciona de la misma manera que si tuviese un selector.
En el ejemplo de abajo, el tráfico se dirige al único Endpoint definido en el YAML:
<code>192.0.2.42:9376</code> (TCP).</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El servidor de API de Kubernetes no permite hacer proxy a Endpoints que no están mapeados a Pods.
Acciones como <code>kubectl proxy &lt;service-name></code> donde el servicio no tiene un selector fallará debido a esta restricción.
Esto previene que el servidor API de Kubernetes sea utilizado como proxy a endpoints a los que quien llama no tenga acceso autorizado.</div><p>Un Service ExternalName es un caso especial de Service que no tiene selectores y usa nombres DNS en su lugar. Para más información, mira la sección <a href=#externalname>ExternalName</a> en este documento.</p><h3 id=endpoints-de-sobrecapacidad>Endpoints de sobrecapacidad</h3><p>Si un recurso Endpoint tiene más de 1000 endpoints entonces un clúster de Kubernetes v1.22 (o posterior)
anota los Endpoints con <code>endpoints.kubernetes.io/over-capacity: truncated</code>.
Esta anotación indica que el objeto Endpoints afectado está por encima de la capacidad y que
el controlador de endpoints ha truncado el número de endpoints a 1000.</p><h3 id=endpointslices>EndpointSlices</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>Los EndpointSlices son un recurso de la API que pueden proveer una alternativa más escalable a los Endpoints.
Aunque conceptualmente es muy similar a los Endpoints, los EndpointSlices permiten distribuir los endpoints de red a través de múltiples recursos.
Por defecto, un EndpointSlice se considera "full" una vez que alcanza 100 endpoints, punto en el cual un EndpointSlice se creará para almacenar cualquier endpoint adicional.</p><p>Los EndpointSlices proveen atributos adicionales y funcionalidad que se describe en detalle en <a href=/docs/concepts/services-networking/endpoint-slices/>EndpointSlices</a>.</p><h3 id=protocolo-de-aplicación>Protocolo de aplicación</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>El campo <code>appProtocol</code> provee una manera de especificar un protocolo de aplicación para cada puerto de un Service.
El valor de este campo es reflejado por el Endpoint correspondiente y los objetos EndpointSlices.</p><p>Este campo sigue una sintaxis estándar de etiquetas de Kubernetes. Los valores deberían ser <a href=https://www.iana.org/assignments/service-names>nombres de servicio IANA estándar</a>
o nombres de dominio con prefijos tales como <code>mycompany.com/my-custom-protocol</code>.</p><h2 id=ips-virtuales-proxies-de-servicio>IPS Virtuales proxies de servicio</h2><p>Cada nodo en un clúster de Kubernetes ejecuta un <code>kube-proxy</code>. El <code>kube-proxy</code>es el responsable de implementar una forma de IP virtual para los
<code>Services</code> de un tipo distinto al de <a href=#externalname><code>ExternalName</code></a>.</p><h3 id=por-qué-no-usar-dns-round-robin>Por qué no usar DNS round-robin?</h3><p>Una pregunta que surge algunas veces es por qué Kubernetes depende de proxies para redirigir el tráfico de entrada a los backends. ¿Qué hay de otros enfoques?
Por ejemplo, ¿sería posible configurar registros DNS que tengan múltiples valores A (o AAA para IPv6), y depender en la resolución de nombres round-robin?</p><p>Existen algunas razones para usar proxies en los Services:</p><ul><li>Hay una larga historia de implementaciones DNS que no respetan los registros TTLs, y cachean los resultados de la búsqueda de nombres luego de que deberían haber expirado.</li><li>Algunas aplicaciones realizan la búsqueda de DNS solo una vez y almacenan en caché los resultados indefinidamente.</li><li>Incluso si las aplicaciones y las librerías hicieran una resolución apropiada, los TTLs bajos o nulos en los registros DNS podrían imponer una carga alta en los DNS que luego se volvería difícil de manejar.</li></ul><p>Más adelante en esta página puedes leer acerca del trabajo de varias implementaciones de kube-proxy. En general, deberías notar que, cuando ejecutas <code>kube-proxy</code>, los niveles de reglas del kernel podrían modificarse (por ejemplo, podrían crearse reglas iptables), que luego no son limpiados, en algunos casos hasta que reinicias. Por tanto, ejecutar kube-proxy es algo que
solo debería hacer un administrador que entienda las consecuencias de tener un servicio de bajo nivel privilegiado de proxy de red en un computador. Aunque el ejecutable de <code>kube-proxy</code> soporta una función de <code>cleanup</code>, esta función no es una característica oficial y solo está disponible para usarse como está.</p><h3 id=configuración>Configuración</h3><p>Ten en cuenta que el kube-proxy inicia en diferentes modos, los cuales están determinados por su configuración.</p><ul><li>La configuración del kube-proxy se hace via un ConfigMap, y el ConfigMap para el kube-proxy remplaza efectivamente el comportamiento de casi todas las banderas para el kube-proxy.</li><li>La configuración del ConfigMap no soporta la recarga en vivo de la configuración.</li><li>Los parámetros del ConfigMap para el kube-proxy no se pueden validar y verificar en el arranque. Por ejemplo, si tu sistema operativo no permite ejecutar comandos iptables, el kube-proxy del kernel estándar no funcionará. De igual forma, si tienes un sistema operativo que no soporta <code>netsh</code>, no se ejecutará en modo userspace en Windows.</li></ul><h3 id=proxy-mode-userspace>Modo proxy userspace</h3><p>En este modo, el kube-proxy observa la adición y eliminación de objetos Endpoint Service del plano de control de Kubernetes.
Para cada Service se abre un puerto (elegido al azar) en el nodo local. Cualquier conexión a este "puerto proxy" es dirigido a uno de los Pods backend del Servicio (como se reporta via Endpoints). El kube-proxy toma el valor <code>sessionAffinity</code> del Service en cuenta cuando decide cuál Pod del backend utilizar.</p><p>Finalmente, el proxy del userspace instala reglas de iptables que capturan el tráfico al <code>clusterIP</code> (que es virtual) del servicio y el <code>port</code>. Las reglas redirigen el tráfico al puerto proxy que redirige al Pod del backend.</p><p>Por defecto, el kube-proxy en modo userspace elige un backend con un algoritmo round-robin.</p><p><img src=/images/docs/services-userspace-overview.svg alt="Diagrama de descripción general de los Services para el proxy userspace"></p><h3 id=proxy-mode-iptables>Modo proxy <code>iptables</code></h3><p>En este modo, el kube-proxy observa la adición y eliminación de objetos Endpoint Service del panel de control de Kubernetes.
Para Service, instala reglas iptables, las cuales capturan el tráfico al <code>clusterIP</code> y el <code>port</code> del Service, y redirige este tráfico a uno de los conjuntos del backend.
Para cada objeto Endpoint, instala reglas de iptables que seleccionan un Pod del backend.</p><p>Por defecto, el kube-proxy en modo iptables elige un backend al azar.</p><p>Usar iptables para manejar tráfico tiene una sobrecarga más baja del sistema, porque el tráfico es manejado por el netfilter de Linux sin la necesidad de cambiar entre userspace y el espacio del kernel.
Esta aproximación puede llegar a ser más confiable.</p><p>Si el kube-proxy está corriendo en modo iptables y el primer Pod seleccionado no responde, la conexión falla. Esto es diferente del modo userspace: en ese escenario, el kube-proxy detectaría que la conexión al primer Pod ha fallado e intentaría automáticamente con otro Pod del backend.</p><p>Puedes usar <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>readiness probes</a> para verificar que los Pods del backend están funcionando correctamente, para que kube-proxy en modo iptables solo vea los backends que han sido comprobados como sanos. Hacer esto significa que evitas enviar tráfico via kube-proxy a un Pod que se sabe que ha fallado.</p><p><img src=/images/docs/services-iptables-overview.svg alt="Diagrama de descripción general de los Services para el proxy iptables"></p><h3 id=proxy-mode-ipvs>Modo Proxy IPVS</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.11 [stable]</code></div><p>En el modo <code>ipvs</code>, el kube-proxy observa los Services de Kubernetes y los Endpoints, llama la interfaz <code>netlink</code> para crear reglas IPVS respectivamente y sincroniza las reglas IPVS con los Services de Kubernetes y los Endpoints periódicamente. Este ciclo de control asegura que los estados del IPVS coincidan con el estado deseado.</p><p>Cuando accede a un Service, IPVS dirige el tráfico a uno de estos Pods del backend.</p><p>El modo proxy IPVS está basado en la función de enlace netfilter que es similar al modo iptables, pero usa una tabla hash como estructura de datos subyacente y opera en el espacio del kernel.</p><p>Esto significa que el kube-proxy en modo IPVS redirige el tráfico como menor latencia que el kube-proxy en modo iptables, con mejor desempeño cuando sincroniza las reglas proxy. Comparado con otros modos proxy, el modo IPVS también soporta un rendimiento más alto de tráfico de red.</p><p>IPVS provee más opciones para balancear el tráfico a los Pods del backend; estas son:</p><ul><li><code>rr</code>: round-robin</li><li><code>lc</code>: menor conexión (el número más pequeño de conexiones abiertas)</li><li><code>dh</code>: hash de destino</li><li><code>sh</code>: hash de origen</li><li><code>sed</code>: retraso esperado más corto</li><li><code>nq</code>: nunca hacer cola</li></ul><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Para correr kube-proxy en modo IPVS, deber estar disponible IPVS en el nodo antes de iniciar el kube-proxy.</p><p>Cuando kube-proxy inicia en modo IPVS, este verifica si los módulos kernel IPVS están disponibles. Si no se detectan los módulos del kernel IPVS, kube-proxy vuelve al modo proxy iptables.</p></div><p><img src=/images/docs/services-ipvs-overview.svg alt="Diagrama de descripción general de los Services para el proxy IPVS"></p><p>En estos modelos de proxy, el tráfico enlazado para la IP:Port del Service es redirigido al backend apropiado sin que el cliente sepa nada de Kubernetes, Services o Pods.</p><p>Si quieres asegurarte que las conexiones desde un cliente en particular se pasen al mismo Pod cada vez, puedes seleccionar la afinidad de sesión basada en la dirección IP del cliente al establecer <code>service.spec.sessionAffinity</code> a "ClientIP" (por defecto es "None").</p><p>Puedes establecer también el número máximo de tiempo al establecer <code>service.spec.sessionAffinityConfig.clientIP.timeoutSeconds</code> apropiadamente. (El valor por defecto es 10800, que resulta ser unas 3 horas).</p><h2 id=services-multi-puerto>Services multi puerto</h2><p>Para algunos servicios, necesitas exponer más de un puerto. Kubernetes te permite configurar múltiples definiciones puertos en un objeto Service. Cuando usas múltiples puertos para un Service, debes nombrar todos tus puertos para que no sean ambiguos.
Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>MiApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>https<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9377</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Como con los <a class=glossary-tooltip title='Una cadena de caracteres proporcionada por el cliente que identifica un objeto en la URL de un recurso, como por ejemplo, /api/v1/pods/nombre-del-objeto.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/names target=_blank aria-label=nombres>nombres</a> de Kubernetes en general, los nombres para los puertos deben contener alfanuméricos en minúsculas y <code>-</code>. Los nombres de puertos deben comenzar y terminar con un carácter alfanumérico.</p><p>Por ejemplo, los nombres <code>123-abc</code> and <code>web</code> son válidos, pero <code>123_abc</code> y <code>-web</code> no lo son.</p></div><h2 id=eligiendo-tu-propia-dirección-ip>Eligiendo tu propia dirección IP</h2><p>Puedes especificar tu propia dirección IP para el clúster como parte de la petición de creación de un <code>Service</code>. Para hacer esto, establece el campo <code>.spec.clusterIP</code>. Por ejemplo, si ya tienes una entrada DNS existente que quieres reutilizar, o sistemas legacy que están configurados para direcciones IP específicas que son difíciles de reconfigurar.</p><p>La dirección IP que elijas debe ser una dirección IPV4 o IPV6 válida dentro del rango CIDR <code>service-cluster-ip-range</code> que está configurado para el servidor API.
Si intentas crear un Service con una dirección clusterIP inválida, el servidor API devolverá un código de estado 422 para indicar que hay un problema.</p><h2 id=políticas-de-tráfico>Políticas de tráfico</h2><h3 id=política-de-tráfico-externa>Política de tráfico externa</h3><p>Puedes establecer el campo <code>spec.externalTrafficPolicy</code> para controlar cómo se enruta el tráfico de fuentes externas. Los valores válidos son <code>Cluster</code>y <code>Local</code>. Establece el campo a <code>Cluster</code> para enrutar tráfico externo a todos los endpoints listos y <code>Local</code> para enrutar solamente a los endpoints locales del nodo. Si la política de tráfico es <code>Local</code> y no hay endpoints de nodos locales, kube-proxy no redirige ningún tráfico al Service relevante.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code></div><p>Si habilitas el <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> <code>ProxyTerminatingEndpoints</code> para el kube-proxy, el kube-proxy revisa si el nodo tiene endpoints locales y si todos los endpoints locales están marcados como terminando. Si hay endpoints locales y <strong>todos</strong> están terminando, el kube-proxy ignora todo tráfico externo de <code>Local</code>. En cambio, mientras que los endpoints locales del nodo permanecen todos como terminando, el kube-proxy reenvía el tráfico para ese Service para endpoints sanos en otro lugar, como si la política de tráfico externo fuese <code>Cluster</code>.
Este comportamiento de reenvío para endpoints que están terminando existe para permitir que los balanceadores de carga externos terminen las conexiones que están respaldadas por Services <code>NodePort</code> gradualmente, incluso cuando la verificación del estado del puerto del nodo comienza a fallar. De lo contrario, el tráfico puede perderse entre el tiempo en que un nodo está todavía en el grupo de nodos de un balanceador de carga y el tráfico se cae durante el periodo de terminación de un Pod.</div><h3 id=política-de-tráfico-interna>Política de tráfico interna</h3><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code></div>Puedes establecer el campo <code>spec.internalTrafficPolicy</code> para controlar como se enruta el tráfico desde las fuentes internas. Los valores válidos son <code>Cluster</code> y <code>Local</code>. Establece el campo a <code>Cluster</code> para enrutar el tráfico interno a todos los endpoints listos y <code>Local</code> para enrutar solo los endpoints locales del nodo. Si la política de tráfico es <code>Local</code> y no hay endpoints locales de nodo, el tráfico es terminado por el kube-proxy.</p><h2 id=descubriendo-servicios>Descubriendo servicios</h2><p>Kubernetes soporta 2 modos primarios para encontrar un Service - variables de entorno y DNS</p><h3 id=variables-de-entorno>Variables de entorno</h3><p>Cuando un Pod está corriendo en un Node, kubelet añade un conjunto de variables de entorno para cada Service activo. Soporta tanto variables <a href=https://docs.docker.com/userguide/dockerlinks/>Docker links
compatible</a> como variables más sencillas <code>{SVCNAME}_SERVICE_HOST</code> and <code>{SVCNAME}_SERVICE_PORT</code>, donde el nombre del Service está en mayúsculas y los guiones medios se convierten en guiones bajos.</p><p>Por ejemplo, el Service <code>redis-master</code> que expone el puerto TCP 6739 y se le ha asignado una dirección IP de clúster 10.0.0.11, produce las siguientes variables de entorno:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>REDIS_MASTER_SERVICE_HOST</span><span style=color:#666>=</span>10.0.0.11
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_MASTER_SERVICE_PORT</span><span style=color:#666>=</span><span style=color:#666>6379</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_MASTER_PORT</span><span style=color:#666>=</span>tcp://10.0.0.11:6379
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_MASTER_PORT_6379_TCP</span><span style=color:#666>=</span>tcp://10.0.0.11:6379
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_MASTER_PORT_6379_TCP_PROTO</span><span style=color:#666>=</span>tcp
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_MASTER_PORT_6379_TCP_PORT</span><span style=color:#666>=</span><span style=color:#666>6379</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_MASTER_PORT_6379_TCP_ADDR</span><span style=color:#666>=</span>10.0.0.11
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Cuando tienes un Pod que necesita acceso a un Service, y estás usando el método de variable de entorno para publicar el puerto y la dirección clúster IP al Pod cliente, debes crear el Service <em>antes</em> de que los Pods del cliente lleguen a existir. De lo contrario, esos Pods cliente no tendrán las variables de entorno pobladas.</p><p>Si solo usas DNS para descubrir la clúster IP para un Service, no tienes que preocuparte acerca de este tema de ordenación.</p></div><h3 id=dns>DNS</h3><p>Puedes (y casi siempre deberías) configurar un servicio DNS para tu clúster de Kubernetes usando un <a href=/docs/concepts/cluster-administration/addons/>add-on</a>.</p><p>Un servidor DNS consciente del clúster, como CoreDNS, observa la API de Kubernetes por nuevos Services y crea un conjunto de registros DNS para cada uno. Si DNS ha sido habilitado a través de tu clúster entonces todos los Pods automáticamente serán capaces de resolver los Services por su nombre DNS.</p><p>Por ejemplo, si tienes un Service llamado <code>mi-servicio</code> en un namespace <code>mi-ns</code>, el plano de control y el Service DNS crean un registro DNS para <code>mi-servicio.mi-ns</code> conjuntamente. Los Pods en el namespace <code>mi-ns</code> deberían ser capaces de encontrar el Service haciendo una búsqueda de nombre por <code>mi-servicio</code> (<code>mi-servicio.mi-ns</code> también funcionaría)</p><p>Los Pods en otros namespaces deben calificar el nombre como <code>my-service.my-ns</code>. Estos nombres resolverán la clúster IP asignada para el Service.</p><p>Kubernetes también soporta registros DNS SRV (Service) para los puertos nombrados. Si el Service <code>mi-servicio.mi-ns</code> tiene un puerto llamado <code>http</code> con el protocolo fijado a <code>TCP</code>, puedes hacer una consulta DNS SRV a <code>_http._tcp.mi-servicio.mi-ns</code> para descubrir el número de puerto para <code>http</code> así como la dirección IP.</p><p>El servidor DNS de Kubernetes es la única manera de acceder a los Services <code>ExternalName</code>. Puedes encontrar más información sobre la resolución <code>ExternalName</code> en <a href=/docs/concepts/services-networking/dns-pod-service/>Pods y Services DNS</a>.</p><h2 id=servicios-headless>Servicios Headless</h2><p>Algunas veces no necesitas balancear cargas y una IP única. En este caso, puedes crear lo que llamamos Services "headless", especificando <code>"None"</code> para el clúster IP (<code>.spec.clusterIP</code>).</p><p>Puedes usar un Service headless para hacer una interfaz con otros mecanismos de descubrimiento de servicios, sin estar atado a la implementación de Kubernetes.</p><p>Para los <code>Services</code> headless, no se asigna una clúster IP, kube-proxy no maneja estos Services, y no hay balanceo de cargas o redirección por la plataforma para ellos. Cómo se configura el DNS automáticamente depende de si el Service tiene selectores definidos:</p><h3 id=con-selectores>Con selectores</h3><p>Para los Services headless que definen selectores, el controlador de endpoints crea registros <code>Endpoints</code> en la API, y modifica la configuración DNS para devolver registros A (direcciones IP) que apuntan directamente a los <code>Pods</code> que respaldan el <code>Service</code>.</p><h3 id=sin-selectores>Sin selectores</h3><p>Para Services headless que no definen selectores, el controlador de endpoints no crea registros <code>Endpoints</code>. Sin embargo, el sistema DNS busca y configura:</p><ul><li>Registros CNAME para Services del tipo <a href=#externalname><code>ExternalName</code></a>.</li><li>Registros A para cualquier <code>Endpoints</code> que comparten un nombre con el Service, para todos los otros tipos.</li></ul><h2 id=publishing-services-service-types>Publicar Services (ServiceTypes)</h2><p>En algunas partes de tu aplicación (por ejemplo, frontends) puede que necesites exponer un Service a una dirección IP externa, que está fuera de tu clúster local</p><p>Los <code>ServiceTypes</code> de Kubernetes permiten especificar qué tipo de Service quieres. El valor por defecto es <code>ClusterIP</code></p><p>Los valores <code>Type</code> y sus comportamientos son:</p><ul><li><p><code>ClusterIP</code>: Expone el Service en una dirección IP interna del clúster. Al escoger este valor el Service solo es alcanzable desde el clúster. Este es el <code>ServiceType</code> por defecto.</p></li><li><p><a href=#tipo-nodeport><code>NodePort</code></a>: Expone el Service en cada IP del nodo en un puerto estático (el <code>NodePort</code>). Automáticamente se crea un Service <code>ClusterIP</code>, al cual enruta el <code>NodePort</code>del Service. Podrás alcanzar el Service <code>NodePort</code> desde fuera del clúster, haciendo una petición a <code>&lt;NodeIP>:&lt;NodePort></code>.</p></li><li><p><a href=#loadbalancer><code>LoadBalancer</code></a>: Expone el Service externamente usando el balanceador de carga del proveedor de la nube. Son creados automáticamente Services <code>NodePort</code>y <code>ClusterIP</code>, a los cuales el apuntará el balanceador externo.</p></li><li><p><a href=#externalname><code>ExternalName</code></a>: Mapea el Service al contenido del campo <code>externalName</code> (ej. <code>foo.bar.example.com</code>), al devolver un registro <code>CNAME</code> con su valor. No se configura ningún tipo de proxy.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><ul><li>Necesitas la versión 1.7 de <code>kube-dns</code> o CoreDNS versión 0.0.8 o más para usar el tipo <code>ExternalName</code>.</li></ul></div></li></ul><p>También puedes usar un <a href=/docs/concepts/services-networking/ingress/>Ingress</a> para exponer tu Service. Ingress no es un tipo de Service, pero actúa como el punto de entrada de tu clúster. Te permite consolidar tus reglas de enrutamiento en un único recurso, ya que puede exponer múltiples servicios bajo la misma dirección IP.</p><h3 id=tipo-nodeport>Tipo NodePort</h3><p>Si estableces el campo <code>type</code> a <code>NodePort</code>, el plano de control de Kubernetes asigna un puerto desde un rango especificado por la bandera <code>--service-node-port-range</code> (por defecto: 30000-32767).
Cada nodo es un proxy de ese puerto (el mismo número de puerto en cada nodo) hacia tu Service. Tu Service reporta al puerto asignado en el campo <code>.spec.ports[*].nodePort</code></p><p>Si quieres especificar una(s) IP(s) particular(es) para hacer proxy del puerto, puedes establecer la bandera <code>--nodeport-addresses</code> para el kube-proxy o el campo equivalente <code>nodePortAddresses</code> del <a href=/docs/reference/config-api/kube-proxy-config.v1alpha1/>fichero de configuración de kube-proxy</a> para ese bloque particular de IP(s).</p><p>Esta bandera recibe un listado de bloques de IP separados por coma (ej. <code>10.0.0.0/8</code>, <code>192.0.2.0/25</code>) para especificar rangos de direcciones IP que el kube-proxy debería considerar como local para este nodo.</p><p>Por ejemplo, si arrancas el kube-proxy con la bandera <code>--nodeport-addresses=127.0.0.0/8</code>, el kube-proxy solo selecciona la interfaz loopback para los Services NodePort. El valor por defecto es <code>--nodeport-addresses</code> es una lista vacía. Esto significa que el kube-proxy considera todas las interfaces de red disponibles para el NodePort. (Esto es compatible también con versiones más antiguas de Kubernetes).</p><p>Si quieres un número de puerto específico, puedes especificar un valor en el campo <code>nodePort</code>. El plano de control te asignará ese puerto o reportará que la transacción API ha fallado.
Esto significa que necesitas prestar atención a posibles colisiones de puerto por tu cuenta.
También tienes que usar un número de puerto válido, uno que esté dentro del rango configurado para uso del NodePort.</p><p>Usar un NodePort te da libertad para configurar tu propia solución de balanceo de cargas, para configurar entornos que no soportan Kubernetes del todo, o para exponer uno o más IPs del nodo directamente.</p><p>Ten en cuenta que este Service es visible como <code>&lt;NodeIP>:spec.ports[*].nodePort</code> y <code>.spec.clusterIP:spec.ports[*].port</code>.
Si la bandera <code>--nodeport-addresses</code> está configurada para el kube-proxy o para el campo equivalente en el fichero de configuración, <code>&lt;NodeIP></code> sería IP filtrada del nodo. Si</p><p>Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>NodePort<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>MiApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Por defecto y por comodidad, el `TargetPort` tiene el mismo valor que el campo `port.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Campo opcional</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Por defecto y por comodidad, el plano de control de Kubernetes asignará el puerto desde un rango (por defecto: 30000-32767)</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodePort</span>:<span style=color:#bbb> </span><span style=color:#666>30007</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=loadbalancer>Tipo LoadBalancer</h3><p>En proveedores de la nube que soportan balanceadores de carga externos, establecer el campo <code>type</code> a <code>LoadBalancer</code> aprovisiona un balanceador de carga para tu Service. La creación del balanceador de carga ocurre de forma asíncrona, y la información sobre el balanceador de carga provisto se publica en el campo <code>.status.loadBalancer</code> del Service.</p><p>Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>MiApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span><span style=color:#666>10.0.171.239</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>loadBalancer</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>ip</span>:<span style=color:#bbb> </span><span style=color:#666>192.0.2.127</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>El tráfico desde el balanceador de carga externo es dirigido a los Pods del backend. El proveedor de la nube decide cómo balancear la carga.</p><p>Algunos proveedores de la nube te permiten especificar la IP <code>loadBalancerIP</code>. En esos caso, el balanceador de carga es creado con la <code>loadBalancerIP</code> especificada por el usuario. Si el campo <code>loadBalancerIP</code> no se especifica, el balanceador de carga se configura con una dirección IP efímera. Si especificas una <code>loadBalancerIP</code> pero tu proveedor de la nube no soporta esta característica, se ignora el campo <code>loadBalancerIP</code> que has configurado.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>En <strong>Azure</strong>, si quieres usar un tipo <code>loadBalancerIP</code> público definido por el usuario, primero necesitas crear una dirección IP estática y pública. Esta dirección IP pública debería estar en el mismo grupo de recursos que los otros recursos del clúster creados automáticamente.
Por ejemplo, <code>MC_myResourceGroup_myAKSCluster_eastus</code>.</p><p>Especifica la dirección IP asignada como loadBalancerIP. Asegúrate que tienes actualizado el securityGroupName en tu fichero de configuración del proveedor de la nube. Para información sobre cómo resolver problemas de permisos de <code>CreatingLoadBalancerFailed</code>, mira <a href=https://docs.microsoft.com/en-us/azure/aks/static-ip>Usar una IP estática con el balanceador de carga de Azure Kubernetes Service (AKS)</a> o <a href=https://github.com/Azure/AKS/issues/357>CreatingLoadBalancerFailed en un clúster AKS con configuración de red avanzada</a>.</p></div><h4 id=balanceadores-de-carga-con-tipos-de-protocolo-mixtos>Balanceadores de carga con tipos de protocolo mixtos</h4><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code></div>Por defecto, para los tipos de Service LoadBalancer, cuando hay más de un puerto definido, todos los puertos deben tener el mismo protocolo, y el protocolo debe estar soportado por el proveedor de la nube.</p><p>Si la feature gate <code>MixedProtocolLBService</code> está habilitada para el kube-apiserver se permiten usar diferentes protocolos cuando hay más de un puerto definido.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El conjunto de protocolos que pueden ser usados para Services de tipo LoadBalancer es definido por el proveedor de la nube.</div><h4 id=load-balancer-nodeport-allocation>Deshabilitar la asignación NodePort del balanceador de carga</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [alpha]</code></div><p>A partir de v1.20, puedes deshabilitar opcionalmente la asignación del puerto del nodo para un Service de tipo LoadBalancer estableciendo el campo <code>spec.allocateLoadBalancerNodePorts</code> a <code>false</code>. Esto debería ser usado solo para implementaciones de balanceadores de carga que enrutan el tráfico directamente a los Pods al contrario de usar puertos del nodo. Por defecto, <code>spec.allocateLoadBalancerNodePorts</code> es <code>true</code> y los Services de tipo LoadBalancer continuarán asignando puertos. Si <code>spec.allocateLoadBalancerNodePorts</code> es <code>false</code> en un Service existente con puertos asignado, esos puertos del nodo no serán desasignados automáticamente.
Debes quitar explícitamente la entrada <code>nodePorts</code>en cada puerto del Service para desasignar esos puertos del nodo.
Debes habilitar la feature gate <code>ServiceLBNodePortControl</code> para usar este campo.</p><h4 id=load-balancer-class>Especificar la clase de implementación del balanceador de carga</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code></div><p><code>spec.loadBalancerClass</code> te permite usar una implementación del balanceador de carga distinta que la que viene por defecto para el proveedor de la nube. Esta característica está disponible desde v1.21, debes habilitar la feature gate <code>ServiceLoadBalancerClass</code> para usar este campo en v1.21, y la feature gate está habilitada por defecto desde v1.22 en adelante.</p><p>Por defecto, <code>spec.loadBalancerClass</code> es <code>nil</code> y un tipo de Service <code>LoadBalancer</code> usa la implementación por defecto del proveedor de la nube si el clúster está configurado con un proveedor de nube usando la bandera de componente <code>--cloud-provider</code>.</p><p>Si <code>spec.loadBalancerClass</code> está especificado, se asume que una implementación de un balanceador de carga que coincida con la clase especificada está observando los Services. Cualquier implementación por defecto del balanceador de carga (por ejemplo, la que es provista por el proveedor de la nube) ignorará los Services que tienen este campo establecido. <code>spec.loadBalancerClass</code> se puede establecer en cualquier Service de tipo <code>LoadBalancer</code> únicamente. Una vez hecho, no se puede cambiar.
El valor de <code>spec.loadBalancerClass</code> debe ser un identificador de etiqueta, con un prefijo opcional como "<code>internal-vip</code>" o "<code>example.com/internal-vip</code>". Los nombres sin prefijo están reservados para usuarios finales.</p><h4 id=balanceador-de-carga-interno>Balanceador de carga interno</h4><p>En un entorno mixto algunas veces es necesario enrutar el tráfico desde Services dentro del mismo bloque (virtual) de direcciones de red.</p><p>En un entorno de split-horizon DNS necesitarías dos Services para ser capaz de enrutar tanto el tráfico externo como el interno a tus Endpoints.</p><p>Para establecer un balanceador de carga interno, agrega una de las siguientes anotaciones a tu Service dependiendo del proveedor de Service en la nube que estás usando.</p><ul class="nav nav-tabs" id=service-tabs role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#service-tabs-0 role=tab aria-controls=service-tabs-0 aria-selected=true>Default</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-1 role=tab aria-controls=service-tabs-1>GCP</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-2 role=tab aria-controls=service-tabs-2>AWS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-3 role=tab aria-controls=service-tabs-3>Azure</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-4 role=tab aria-controls=service-tabs-4>IBM Cloud</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-5 role=tab aria-controls=service-tabs-5>OpenStack</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-6 role=tab aria-controls=service-tabs-6>Baidu Cloud</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-7 role=tab aria-controls=service-tabs-7>Tencent Cloud</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-8 role=tab aria-controls=service-tabs-8>Alibaba Cloud</a></li></ul><div class=tab-content id=service-tabs><div id=service-tabs-0 class="tab-pane show active" role=tabpanel aria-labelledby=service-tabs-0><p><p>Selecciona una de las pestañas.</p></div><div id=service-tabs-1 class=tab-pane role=tabpanel aria-labelledby=service-tabs-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cloud.google.com/load-balancer-type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Internal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-2 class=tab-pane role=tabpanel aria-labelledby=service-tabs-2><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-internal</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-3 class=tab-pane role=tabpanel aria-labelledby=service-tabs-3><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/azure-load-balancer-internal</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-4 class=tab-pane role=tabpanel aria-labelledby=service-tabs-4><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;private&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-5 class=tab-pane role=tabpanel aria-labelledby=service-tabs-5><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/openstack-internal-load-balancer</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-6 class=tab-pane role=tabpanel aria-labelledby=service-tabs-6><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/cce-load-balancer-internal-vpc</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-7 class=tab-pane role=tabpanel aria-labelledby=service-tabs-7><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.kubernetes.io/qcloud-loadbalancer-internal-subnetid</span>:<span style=color:#bbb> </span>subnet-xxxxx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-8 class=tab-pane role=tabpanel aria-labelledby=service-tabs-8><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;intranet&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h4 id=ssl-support-on-aws>Soporte para TLS en AWS</h4><p>Para soporte parcial de TLS/SSL en clústeres corriendo en AWS, puedes agregar tres anotaciones al servicio <code>LoadBalancer</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-ssl-cert</span>:<span style=color:#bbb> </span>arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012<span style=color:#bbb>
</span></span></span></code></pre></div><p>El primero especifica el ARN del certificado a usar. Este puede ser un certificado de un emisor de un tercero que fue subido en IAM o uno creado dentro del Administrador de Certificados de AWS.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span>:<span style=color:#bbb> </span>(https|http|ssl|tcp)<span style=color:#bbb>
</span></span></span></code></pre></div><p>La segunda anotación especifica cuál protocolo habla el Pod. Para HTTPS y SSL, el ELB espera que el Pod se autentique a sí mismo sobre una conexión encriptada, usando un certificado.</p><p>HTTP y HTTPS seleccionan un proxy de capa 7: el ELB termina la conexión con el usuario, interpreta los encabezados, e inyecta el encabezado <code>X-Forwared-For</code> con la dirección IP del usuario (los Pods solo ven la dirección IP del ELB del otro lado de su conexión) cuando reenvía las peticiones.</p><p>TCP y SSL seleccionan un proxy de capa 4: el ELB reenvía el tráfico sin modificar los encabezados.</p><p>En un entorno mixto donde algunos puertos están asegurados y otros se dejan sin encriptar, puedes usar una de las siguientes anotaciones:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-ssl-ports</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;443,8443&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>En el ejemplo de arriba, si el Service contenía tres puertos, <code>80</code>, <code>443</code> y <code>8443</code> entonces <code>443</code> y <code>8443</code> usarían el certificado SSL, pero <code>80</code>sería HTTP proxy.</p><p>A partir de Kubernetes v1.9 en adelante puedes usar <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html>políticas predefinidas de AWS SSL</a> con listeners HTTPS o SSL para tus Services. Para ver cuáles políticas están disponibles para usar, puedes usar la herramienta de línea de comandos <code>aws</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws elb describe-load-balancer-policies --query <span style=color:#b44>&#39;PolicyDescriptions[].PolicyName&#39;</span>
</span></span></code></pre></div><p>Puedes especificar cualquiera de estas políticas usando la anotación "<code>service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy</code>", por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;ELBSecurityPolicy-TLS-1-2-2017-01&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=soporte-de-protocolo-proxy-en-aws>Soporte de Protocolo PROXY en AWS</h4><p>Para habilitar el soporte para el <a href=https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt>protocolo PROXY</a> en clústeres corriendo en AWS, puedes usar la siguiente anotación para el servicio:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-proxy-protocol</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>A partir de la versión 1.3.0, el uso de esta anotación aplica para todos los puertos proxy del ELB y no puede ser configurado de otra manera.</p><h4 id=acceso-a-los-logs-elb-en-aws>Acceso a los logs ELB en AWS</h4><p>Existen algunas anotaciones para administrar el acceso a los logs para Services ELB en AWS.</p><p>La anotación <code>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</code> controla si el acceso a los logs están habilitados.</p><p>La anotación <code>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</code>
controla el intervalo en minutos para publicar los logs de acceso. Puedes especificar un intervalo de 5 0 60 minutos.</p><p>La anotación <code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</code>
controla el nombre del bucket de Amazon S3 donde se almacenan los logs del balanceador de carga.</p><p>La anotación <code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</code>
especifica la jerarquía lógica que has creado para tu bucket de Amazon S3.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Especifica si está habilitado el acceso a los logs</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;60&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># EL intervalo para publicar los logs de acceso. Puedes especificar un intervalo de 5 o 60 (minutos)</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;my-bucket&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># El nombre del bucket S· de Amazon donde se almacenan los logs de acceso</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;my-bucket-prefix/prod&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># La jerarquía lógica que has creado para tu bucket S3 de Amazon, por ejemplo `my-bucket-prefix/prod`</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=drenaje-de-conexión-en-aws>Drenaje de conexión en AWS</h4><p>Drenaje de conexión para ELBs clásicos se puede administrar con la anotación
<code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</code> fijada al valor <code>"true"</code>.
La anotación <code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</code> puede
ser usada también para establecer el tiempo máximo, en segundos, para mantener las conexiones existentes antes de dar de baja las instancias.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;60&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=otras-anotaciones-elb>Otras anotaciones ELB</h4><p>Existen otras anotaciones para administrar Classic Elastic Load Balancers que se describen abajo.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;60&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># El tiempo, en segundos, que se permite a una conexión estar en reposo (no se han enviado datos sobre la conexión) antes que sea cerrada por el balanceador de carga</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Especifica si el balanceo de cargas entre zonas está habilitado para el balanceador de carga</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;environment=prod,owner=devops&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Un listado separado por comas de valores de clave-valor que será guardados como etiquetas adicionales en el ELB.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># El número de comprobaciones de estado exitosas sucesivas requeridas para considerar sano para el tráfico a un backend.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Por defecto es 2, debe ser entre 2 y 10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;3&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># El número de comprobaciones de estado fallidas requeridas para considerar a un backend no apto para el tráfico.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Por defecto es 6, debe ser entre 2 y 10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;20&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># EL intervalo aproximado, en segundos, entre comprobaciones de estados de una instancia individual.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Por defecto es 10, debe ser entre 5 y 300.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># La cantidad de tiempo, en segundos, durante el cual no recibir respuesta significa una comprobación de estado fallida.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Este valor debe ser menor que el valor de service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Por defecto es 5, debe estar entre 2 y 60</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-security-groups</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;sg-53fae93f&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Un listado de grupos de seguridad existentes para configurar en el ELB creado. A diferencia de la anotación</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># service.beta.kubernetes.io/aws-load-balancer-extra-security-groups, esta reemplaza todos los grupos de seguridad previamente asignados al ELB y también sobreescribe la creación de un grupo de seguridad creado únicamente para este ELB.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># El primer ID grupo de seguridad en esta lista se utiliza para permitir tráfico de entrada a los nodos workers objetivo (tráfico de servicio y comprobaciones de estados).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Si se configuran múltiples ELBs con el mismo grupo de seguridad, solo una única línea de permisos será añadida a los grupos de seguridad del nodo worker, lo que significa que si eliminas cualquiera de esos ELBs removerá la línea de permisos y bloqueará el acceso para todos los ELBs que comparten el mismo ID de seguridad de grupo.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Esto puede ocasionar cortes entre servicios si no se usa apropiadamente</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-extra-security-groups</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;sg-53fae93f,sg-42efd82e&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Un listado adicional de grupos de seguridad para añadir al ELB creado, esto deja un grupo de seguridad creado únicamente, asegurando que cada ELB tiene un ID de grupo de seguridad único que coincide con la línea de permiso para permitir tráfico a los nodos worker objetivo (tráfico de servicio y comprobaciones de estados)</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Grupos de seguridad definidos se pueden compartir entre servicios.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-target-node-labels</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;ingress-gw,gw-name=public-api&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Un listado separado por comas de clave-valor que se utilizan para seleccionar los nodos objetivos para el balanceador de carga</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=aws-nlb-support>Soporte para Balanceador de Carga de Red (NLB) en AWS</h4><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code></div>Para usar un balanceador de carga de Red en AWS, usa la anotación <code>service.beta.kubernetes.io/aws-load-balancer-type</code> con el valor fijado a <code>nlb</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;nlb&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> NLB solo funciona con ciertas clases de instancias; mira la <a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets>documentación AWS</a> sobre balanceo de cargas elástico para un listado de tipos de instancia soportados.</div><p>A diferencia de los balanceadores de cargas, el balanceador de carga de red (NLB) reenvía la dirección IP del cliente a través del nodo. Si el campo <code>.spec.externalTrafficPolicy</code> está fijado a <code>clúster</code>, la dirección IP del cliente no es propagada a los Pods finales.</p><p>Al fijar <code>.spec.externalTrafficPolicy</code> en <code>Local</code>, la dirección IP del cliente se propaga a los Pods finales,
pero esto puede resultar a una distribución de tráfico desigual. Los nodos sin ningún Pod para un Service particular de tipo LoadBalancer fallarán en la comprobación de estado del grupo objetivo del NLB en el puerto <code>.spec.healthCheckNodePort</code> y no recibirán ningún tráfico.</p><p>Para conseguir trafico equilibrado, usa un DaemonSet o especifica <a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>pod anti-affinity</a> para no localizar en el mismo nodo.</p><p>También puedes usar Services NLB con la anotación del <a href=/docs/concepts/services-networking/service/#internal-load-balancer>balanceador de carga interno</a></p><p>Para permitir que el tráfico del cliente alcance las instancias detrás del NLB, los grupos de seguridad del Nodo se modifican con las siguientes reglas de IP:</p><table><thead><tr><th>Regla</th><th>Protocolo</th><th>Puerto(s)</th><th>Rango de IP(s)</th><th>Descripción del Rango de IP</th></tr></thead><tbody><tr><td>Health Check</td><td>TCP</td><td>NodePort(s) (<code>.spec.healthCheckNodePort</code> para <code>.spec.externalTrafficPolicy = Local</code>)</td><td>Subnet CIDR</td><td>kubernetes.io/rule/nlb/health=&lt;loadBalancerName></td></tr><tr><td>Tráfico del Cliente</td><td>TCP</td><td>NodePort(s)</td><td><code>.spec.loadBalancerSourceRanges</code> (por defecto en <code>0.0.0.0/0</code>)</td><td>kubernetes.io/rule/nlb/client=&lt;loadBalancerName></td></tr><tr><td>MTU Discovery</td><td>ICMP</td><td>3,4</td><td><code>.spec.loadBalancerSourceRanges</code> (por defecto en <code>0.0.0.0/0</code>)</td><td>kubernetes.io/rule/nlb/mtu=&lt;loadBalancerName></td></tr></tbody></table><p>Para limitar cuáles IPs del cliente pueden acceder al balanceador de carga de red, especifica <code>loadBalancerSourceRanges</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>loadBalancerSourceRanges</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;143.231.0.0/16&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Si no se establece <code>.spec.loadBalancerSourceRanges</code>, Kubernetes permite el tráfico
desde <code>0.0.0.0/0</code> a los Grupos de Seguridad del Nodo. Si los nodos tienen direcciones IP públicas, ten en cuenta que el tráfico que no viene del NLB
también puede alcanzar todas las instancias en esos grupos de seguridad modificados.</div><h4 id=otras-anotaciones-cls-en-tencent-kubernetes-engine-tke>Otras anotaciones CLS en Tencent Kubernetes Engine (TKE)</h4><p>Hay otras anotaciones para administrar balanceadores de carga en la nube en TKE como se muestra abajo.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Enlaza Loadbalancers con nodos específicos</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/qcloud-loadbalancer-backends-label</span>:<span style=color:#bbb> </span>key in (value1, value2)<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Identificador de un balanceador de carga existente</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>service.kubernetes.io/tke-existed-lbid：lb-6swtxxxx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic>#Parámetros personalizados para el balanceador de cargas (LB), no soporta la modificación del tipo de LB todavía</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/service.extensiveParameters</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Parámetros personalizados para el listener LB</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/service.listenerParameters</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Especifica el tipo de balanceador de carga;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># valores válidos: clásico (Balanceador de Carga clásico) o aplicación (Balanceador de Carga de aplicación de la nube)</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/loadbalance-type</span>:<span style=color:#bbb> </span>xxxxx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Especifica método de pago el ancho de banda de la red pública;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># valores válidos: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) y BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/qcloud-loadbalancer-internet-charge-type</span>:<span style=color:#bbb> </span>xxxxxx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Especifica el valor del ancho de banda (rango valor: [1,2000] Mbps).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Cuando se fija esta anotación, los balanceadores de carga solo registrarán nodos con Pods corriendo en él, de lo contrario todos los nodos serán registrados.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/local-svc-only-bind-node-with-pod</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=externalname>Tipo ExternalName</h3><p>Los Services de tipo ExternalName mapean un Service a un nombre DNS, no a un selector típico como <code>mi-servicio</code> o <code>cassandra</code>. Estos Services se especifican con el parámetro <code>spec.externalName</code>.</p><p>Esta definición de Service, por ejemplo, mapea el Service <code>mi-Servicio</code> en el namespace <code>prod</code> a <code>my.database.example.com</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>prod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>ExternalName<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>externalName</span>:<span style=color:#bbb> </span>my.database.example.com<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> ExternalName acepta una cadena de texto IPv4, pero como un nombre DNS compuesto de dígitos, no como una dirección IP. ExternalNames que se parecen a direcciones IPv4 no se resuelven por el CoreDNS o ingress-nginx, ya que ExternalName se usa para especificar un nombre DNS canónico. Al fijar una dirección IP, considera usar <a href=#headless-services>headless Services</a>.</div><p>Cuando busca el host <code>mi-servicio.prod.svc.cluster.local</code>, el Service DNS del clúster devuelve un registro <code>CNAME</code> con el valor <code>my.database.example.com</code>. Acceder a <code>mi-servicio</code> funciona de la misma manera que otros Services, pero con la diferencia crucial de que la redirección ocurre a nivel del DNS en lugar reenviarlo o redirigirlo. Si posteriormente decides mover tu base de datos al clúster, puedes iniciar sus Pods, agregar selectores apropiados o endpoints, y cambiar el <code>type</code> del Service.</p><div class="alert alert-danger warning callout" role=alert><strong>Advertencia:</strong><p>Podrías tener problemas al usar ExternalName para algunos protocolos comunes, incluyendo HTTP y HTTPS, si usas ExternalName entonces el nombre del host usado por los clientes dentro de tu clúster es diferente del nombre al que hace referencia el ExternalName.</p><p>Para protocolos que usan el nombre del host esta diferencia puede llevar a errores o respuestas inesperadas. Las peticiones HTTP tendrán un encabezado <code>Host:</code> que el servidor de origen no reconocerá; los servidores TLS no serán capaces de proveer un certificado que coincida con el nombre del host al que el cliente está conectado.</p></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Esta sección está en deuda con el artículo de blog <a href=https://akomljen.com/kubernetes-tips-part-1/>Kubernetes Tips - Part 1</a> de <a href=https://akomljen.com/>Alen Komljen</a>.</div><h3 id=ips-externas>IPs Externas</h3><p>Si existen IPs externas que enrutan hacia uno o más nodos del clúster, los Services de Kubernetes pueden ser expuestos en esas <code>externalIPs</code>. El tráfico que ingresa al clúster con la IP externa (como IP de destino), en el puerto del Service, será enrutado a uno de estos endpoints del Service. Las <code>externalIPs</code> no son administradas por Kubernetes y son responsabilidad del administrador del clúster.</p><p>En la especificación del Service, las <code>externalIPs</code> se pueden especificar junto con cualquiera de los <code>ServiceTypes</code>.
En el ejemplo de abajo, "<code>mi-servicio</code>" puede ser accedido por clientes en "<code>80.11.12.10:80</code>" (<code>externalIP:port</code>)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mi-servicio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>externalIPs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#666>80.11.12.10</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=limitaciones>Limitaciones</h2><p>Usar el proxy del userspace for VIPs funciona en pequeña y mediana escala, pero no escalará a clústeres muy grandes con miles de Services. El tópico <a href=https://github.com/kubernetes/kubernetes/issues/1107>original design proposal for portals</a> tiene más detalles sobre esto.</p><p>Usar el proxy del userspace oculta la dirección IP de origen de un paquete que accede al Service. Esto hace que algún tipo de filtrado (firewalling) sea imposible. El modo proxy iptables no oculta IPs de origen en el clúster, pero aún tiene impacto en clientes que vienen desde un balanceador de carga o un node-port.</p><p>El campo <code>Type</code> está diseñado como una funcionalidad anidada - cada nivel se agrega al anterior. Esto no es estrictamente requerido en todos los proveedores de la nube (ej. Google Compute Engine no necesita asignar un <code>NodePort</code> para que funcione el <code>LoadBalancer</code>, pero AWS si) pero la API actual lo requiere.</p><h2 id=the-gory-details-of-virtual-ips>Implementación de IP Virtual</h2><p>La información previa sería suficiente para muchas personas que quieren usar Services. Sin embargo, ocurren muchas cosas detrás de bastidores que valdría la pena entender.</p><h3 id=evitar-colisiones>Evitar colisiones</h3><p>Una de las principales filosofías de Kubernetes es que no debe estar expuesto a situaciones que podrían hacer que sus acciones fracasen por su propia culpa. Para el diseño del recurso de Service, esto significa no obligarlo a elegir su propio número de puerto si esa elección puede colisionar con la de otra persona. Eso es un fracaso de aislamiento.</p><p>Para permitirte elegir un número de puerto en tus Services, debemos asegurarnos que dos Services no puedan colisionar. Kubernetes lo hace asignando a cada Service su propia dirección IP.</p><p>Para asegurarse que cada Service recibe una IP única, un asignador interno actualiza atómicamente el mapa global de asignaciones en <a class=glossary-tooltip title='Almacén de datos persistente, consistente y distribuido de clave-valor utilizado para almacenar toda a la información del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a> antes de crear cada Service. El objeto mapa debe existir en el registro para que los Services obtengan asignaciones de dirección IP, de lo contrario las creaciones fallarán con un mensaje indicando que la dirección IP no pudo ser asignada.</p><p>En el plano de control, un controlador de trasfondo es responsable de crear el mapa (requerido para soportar la migración desde versiones más antiguas de Kubernetes que usaban bloqueo en memoria). Kubernetes también utiliza controladores para revisar asignaciones inválidas (ej. debido a la intervención de un administrador) y para limpiar las direcciones IP que ya no son usadas por ningún Service.</p><h3 id=ips-and-vips>Direcciones IP del Service</h3><p>A diferencia de direcciones IP del Pod, que enrutan a un destino fijo, las IPs del Service no son respondidas por ningún host. En lugar de ello, El kube-proxy usa iptables (lógica de procesamiento de paquetes en Linux) para definir direcciones IP <em>virtuales</em> que se redirigen de forma transparente cuando se necesita. Cuando el cliente se conecta con la VIP, su tráfico es transportado automáticamente al endpoint apropiado. Las variables de entorno y DNS para los Services son pobladas en términos de la dirección IP virtual del Service (y el puerto).</p><p>Kube-proxy soporta tres modos — userspace, iptables e IPVS — los cuales operan ligeramente diferente cada uno.</p><h4 id=userspace>Userspace</h4><p>Por ejemplo, considera la aplicación de procesamiento de imágenes descrita arriba. Cuando el Service del backend es creado, el nodo maestro de Kubernetes asigna una dirección IP virtual, por ejemplo 10.0.0.1. Asumiendo que el puerto del Service es 1234, el Service es observado por todas las instancias del kube-proxy en el clúster. Cuando un proxy mira un nuevo Service, abre un puerto al azar, establece una redirección iptables desde la dirección IP virtual a este nuevo puerto, y comienza a aceptar conexiones a este.</p><p>Cuando un cliente se conecta a la dirección IP virtual del Service, la regla de iptables entra en acción, y redirige los paquetes al propio puerto del proxy. El "proxy del Service" elige un backend, y comienza a redirigir el tráfico desde el cliente al backend.</p><p>Esto quiere decir que los dueños del Service pueden elegir cualquier puerto que quieran sin riesgo de colisión. Los clientes pueden conectarse a una IP y un puerto, sin estar conscientes de a cuáles Pods están accediendo.</p><h4 id=iptables>iptables</h4><p>Nuevamente, considera la aplicación de procesamiento de imágenes descrita arriba. Cuando se crea el Service Backend, el plano de control de Kubernetes asigna una dirección IP virtual, por ejemplo 10.0.0.1. Asumiendo que el puerto del servicio es 1234, el Service es observado por todas las instancias del kube-proxy en el clúster. Cuando un proxy mira un nuevo Service, instala una serie de reglas de iptables que redirigen desde la dirección IP virtual a las reglas del Service. Las reglas del Service enlazan a las reglas del Endpoint que redirigen el tráfico (usando NAT de destino) a los backends.</p><p>Cuando un cliente se conecta a la dirección IP virtual del Service la regla de iptables son aplicadas. A diferencia del modo proxy userspace, el kube-proxy no tiene que estar corriendo para que funcione la dirección IP virtual, y los nodos observan el tráfico que viene desde la dirección IP del cliente sin alteraciones.</p><p>El mismo flujo básico se ejecuta cuando el tráfico viene a través de un node-port o de un balanceador de carga, aunque en estos casos la IP del cliente es alterada.</p><h4 id=ipvs>IPVS</h4><p>Las operaciones iptables ralentizan dramáticamente en un clúster a gran escala, ej. 10.000 Services. IPVS está diseñado para balancear cargas y está basado en tablas hash dentro del kernel. De esta manera puedes alcanzar consistencia en el desempeño en un número grande de Services de un kube-proxy basado en IPVS. Mientras tanto, el kube-proxy basado en IPVS tiene algoritmos de balanceo de cargas más sofisticados (least conns, locality, weighted, persistence).</p><h2 id=objeto-api>Objeto API</h2><p>El Service es un recurso de alto nivel en la API REST de Kubernetes. Puedes encontrar más detalles sobre el objeto API en: <a href=/docs/reference/generated/kubernetes-api/v1.25/#service-v1-core>Objeto API Service API</a>.</p><h2 id=protocol-support>Protocolos soportados</h2><h3 id=tcp>TCP</h3><p>Puedes usar TPC para cualquier tipo de Service, y es el protocolo de red por defecto.</p><h3 id=udp>UDP</h3><p>Puedes usar UDP para la mayoría de los Services. Para Services type=LoadBalancer, el soporte UDP depende del proveedor de la nube que ofrece esta facilidad.</p><h3 id=sctp>SCTP</h3><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div>Cuando usas un plugin de red que soporta tráfico SCTP, puedes usar SCTP para la mayoría de los Services. Para Services type=LoadBalancer, el soporte SCTP depende del proveedor de la nube que ofrece esta facilidad. (La mayoría no lo hace)</p><h4 id=caveat-sctp-overview>Advertencias</h4><h5 id=caveat-sctp-multihomed>Soporte para asociaciones SCTP multihomed</h5><div class="alert alert-danger warning callout" role=alert><strong>Advertencia:</strong><p>El soporte para asociaciones SCTP multihomed requiere que el plugin CNI pueda soportar la asignación de múltiples interfaces y direcciones IP a un Pod.</p><p>NAT para asociaciones SCTP multihomed requiere una lógica especial en los módulos del kernel correspondientes.</p></div><h5 id=caveat-sctp-windows-os>Windows</h5><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> SCTP no está soportado en nodos basados en Windows.</div><h5 id=caveat-sctp-kube-proxy-userspace>Userspace kube-proxy</h5><div class="alert alert-danger warning callout" role=alert><strong>Advertencia:</strong> El kube-proxy no soporta la administración de asociaciones SCTP cuando está en el modo userspace.</div><h3 id=http>HTTP</h3><p>Si tu proveedor de la nube lo soporta, puedes usar un Service en modo LoadBalancer para configurar un proxy invertido HTTP/HTTPS, redirigido a los Endpoints del Service.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> También puedes usar <a class=glossary-tooltip title='An API object that manages external access to the services in a cluster, typically HTTP.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/ingress/ target=_blank aria-label=Ingress>Ingress</a> en lugar de un Service para exponer Services HTTP/HTTPS.</div><h3 id=protocolo-proxy>Protocolo PROXY</h3><p>Si tu proveedor de la nube lo soporta, puedes usar un Service en modo LoadBalancer para configurar un balanceador de carga fuera de Kubernetes mismo, que redirigirá las conexiones prefijadas con <a href=https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt>protocolo PROXY</a>.</p><p>El balanceador de carga enviará una serie inicial de octetos describiendo la conexión entrante, similar a este ejemplo</p><pre tabindex=0><code>PROXY TCP4 192.0.2.202 10.0.42.7 12345 7\r\n
</code></pre><p>Seguido de la data del cliente.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Leer sobre <a href=/docs/concepts/services-networking/connect-applications-service/>Conectar aplicaciones con Services</a></li><li>Leer sobre <a href=/docs/concepts/services-networking/ingress/>Ingress</a></li><li>Leer sobre <a href=/docs/concepts/services-networking/endpoint-slices/>EndpointSlices</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ded1daafdcd293023ee333728007ca61>5.2 - Políticas de red (Network Policies)</h1><p>Si quieres controlar el tráfico de red a nivel de dirección IP o puerto (capa OSI 3 o 4), puedes considerar el uso de Kubernetes NetworkPolicies para las aplicaciones que corren en tu clúster. Las NetworkPolicies son una estructura enfocada en las aplicaciones que permite establecer cómo un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> puede comunicarse con otras "entidades" (utilizamos la palabra "entidad" para evitar sobrecargar términos más comunes como "Endpoint" o "Service", que tienen connotaciones específicas de Kubernetes) a través de la red. Las NetworkPolicies se aplican a uno o ambos extremos de la conexión a un Pod, sin afectar a otras conexiones.</p><p>Las entidades con las que un Pod puede comunicarse son de una combinación de estos 3 tipos:</p><ol><li>Otros Pods permitidos (excepción: un Pod no puede bloquear el acceso a sí mismo)</li><li>Namespaces permitidos</li><li>Bloqueos de IP (excepción: el tráfico hacia y desde el nodo donde se ejecuta un Pod siempre está permitido, independientemente de la dirección IP del Pod o del nodo)</li></ol><p>Cuando se define una NetworkPolicy basada en Pods o Namespaces, se utiliza un <a class=glossary-tooltip title='Permite a los usuarios filtrar recursos por .' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=Selector>Selector</a> para especificar qué tráfico se permite desde y hacia los Pod(s) que coinciden con el selector.</p><p>Por otro lado, cuando se crean NetworkPolicies basadas en IP, se definen políticas basadas en bloques de IP (rangos CIDR).</p><h2 id=prerrequisitos>Prerrequisitos</h2><p>Las políticas de red son implementadas por el <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>plugin de red</a>. Para usar políticas de red, debes estar utilizando una solución de red que soporte NetworkPolicy. Crear un recurso NetworkPolicy sin un controlador que lo habilite no tendrá efecto alguno.</p><h2 id=dos-tipos-de-aislamiento-de-pod>Dos Tipos de Aislamiento de Pod</h2><p>Hay dos tipos de aislamiento para un Pod: el aislamiento para la salida y el aislamiento para la entrada. Estos se refieren a las conexiones que pueden establecerse. El término "Aislamiento" en el contexto de este documento no es absoluto, sino que significa "se aplican algunas restricciones". La alternativa, "no aislado para $dirección", significa que no se aplican restricciones en la dirección descrita. Los dos tipos de aislamiento (o no) se declaran independientemente, y ambos son relevantes para una conexión de un Pod a otro.</p><p>Por defecto, un Pod no está aislado para la salida; todas las conexiones salientes están permitidas. Un Pod está aislado para la salida si hay alguna NetworkPolicy con "Egress" en su <code>policyTypes</code> que seleccione el Pod; decimos que tal política se aplica al Pod para la salida. Cuando un Pod está aislado para la salida, las únicas conexiones permitidas desde el Pod son las permitidas por la lista <code>egress</code> de las NetworkPolicy que se aplique al Pod para la salida. Los valores de esas listas <code>egress</code> se combinan de forma aditiva.</p><p>Por defecto, un Pod no está aislado para la entrada; todas las conexiones entrantes están permitidas. Un Pod está aislado para la entrada si hay alguna NetworkPolicy con "Ingress" en su <code>policyTypes</code> que seleccione el Pod; decimos que tal política se aplica al Pod para la entrada. Cuando un Pod está aislado para la entrada, las únicas conexiones permitidas en el Pod son las del nodo del Pod y las permitidas por la lista <code>ingress</code> de alguna NetworkPolicy que se aplique al Pod para la entrada. Los valores de esas listas de direcciones se combinan de forma aditiva.</p><p>Las políticas de red no entran en conflicto; son aditivas. Si alguna política(s) se aplica a un Pod para una dirección determinada, las conexiones permitidas en esa dirección desde ese Pod es la unión de lo que permiten las políticas aplicables. Por tanto, el orden de evaluación no afecta al resultado de la política.</p><p>Para que se permita una conexión desde un Pod de origen a un Pod de destino, tanto la política de salida del Pod de origen como la de entrada del Pod de destino deben permitir la conexión. Si cualquiera de los dos lados no permite la conexión, ésta no se producirá.</p><h2 id=networkpolicy-resource>El Recurso NetworkPolicy</h2><p>Ver la referencia <a href=/docs/reference/generated/kubernetes-api/v1.25/#networkpolicy-v1-networking-k8s-io>NetworkPolicy</a> para una definición completa del recurso.</p><p>Un ejemplo de NetworkPolicy pudiera ser este:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/service/networking/networkpolicy.yaml download=service/networking/networkpolicy.yaml><code>service/networking/networkpolicy.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-networkpolicy-yaml")' title="Copy service/networking/networkpolicy.yaml to clipboard"></img></div><div class=includecode id=service-networking-networkpolicy-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-network-policy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>db<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- Egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>ipBlock</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cidr</span>:<span style=color:#bbb> </span><span style=color:#666>172.17.0.0</span>/16<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>except</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:#666>172.17.1.0</span>/24<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>project</span>:<span style=color:#bbb> </span>myproject<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>6379</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>to</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>ipBlock</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cidr</span>:<span style=color:#bbb> </span><span style=color:#666>10.0.0.0</span>/24<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>5978</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Enviar esto al API Server de su clúster no tendrá ningún efecto a menos que su solución de red soporte de políticas de red.</div><p><strong>Campos Obligatorios</strong>: Como con todos los otras configuraciones de Kubernetes, una NetworkPolicy
necesita los campos <code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>. Para obtener información general
sobre cómo funcionan esos ficheros de configuración, puedes consultar
<a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>Configurar un Pod para usar un ConfigMap</a>,
y <a href=/docs/concepts/overview/working-with-objects/object-management>Gestión de Objetos</a>.</p><p><strong>spec</strong>: NetworkPolicy <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>spec</a> contiene toda la información necesaria para definir una política de red dado un Namespace.</p><p><strong>podSelector</strong>: Cada NetworkPolicy incluye un <code>podSelector</code> el cual selecciona el grupo de Pods en los cuales aplica la política. La política de ejemplo selecciona Pods con el label "role=db". Un <code>podSelector</code> vacío selecciona todos los Pods en un Namespace.</p><p><strong>policyTypes</strong>: Cada NetworkPolicy incluye una lista de <code>policyTypes</code> la cual puede incluir <code>Ingress</code>, <code>Egress</code>, o ambas. Los campos <code>policyTypes</code> indican si la política aplica o no al tráfico de entrada hacia el Pod seleccionado, el tráfico de salida desde el Pod seleccionado, o ambos. Si no se especifican <code>policyTypes</code> en una NetworkPolicy el valor <code>Ingress</code> será siempre aplicado por defecto y <code>Egress</code> será aplicado si la NetworkPolicy contiene alguna regla de salida.</p><p><strong>ingress</strong>: Cada NetworkPolicy puede incluir una lista de reglas <code>ingress</code> permitidas. Cada regla permite el tráfico con que se relaciona a ambos valores de las secciones de <code>from</code> y <code>ports</code>. La política de ejemplo contiene una única regla, la cual se relaciona con el tráfico sobre un solo puerto, desde uno de los tres orígenes definidos, el primero especificado por el valor <code>ipBlock</code>, el segundo especificado por el valor <code>namespaceSelector</code> y el tercero especificado por el <code>podSelector</code>.</p><p><strong>egress</strong>: Cada NetworkPolicy puede incluir una lista de reglas de <code>egress</code> permitidas. Cada regla permite el tráfico con que se relaciona a ambos valores de las secciones de <code>to</code> and <code>ports</code>. La política de ejemplo contiene una única regla, la cual se relaciona con el tráfico en un único puerto para cualquier destino en el rango de IPs <code>10.0.0.0/24</code>.</p><p>Por lo tanto, la NetworkPolicy de ejemplo:</p><ol><li>Aísla los Pods "role=db" en el Namespace "default" para ambos tipos de tráfico ingress y egress (si ellos no están aún aislados)</li><li>(Reglas Ingress) permite la conexión hacia todos los Pods en el Namespace "default" con el label "role=db" en el puerto TCP 6379 desde los siguientes orígenes:</li></ol><ul><li>cualquier Pod en el Namespace "default" con el label "role=frontend"</li><li>cualquier Pod en un Namespace con el label "project=myproject"</li><li>La dirección IP en los rangos 172.17.0.0–172.17.0.255 y 172.17.2.0–172.17.255.255 (por ejemplo, todo el rango de IPs de 172.17.0.0/16 con excepción del 172.17.1.0/24)</li></ul><ol start=3><li>(Egress rules) permite conexión desde cualquier Pod en el Namespace "default" con el label "role=db" hacia CIDR 10.0.0.0/24 en el puerto TCP 5978</li></ol><p>Ver el artículo de <a href=/docs/tasks/administer-cl%C3%BAster/declare-network-policy/>Declarar Network Policy</a> para más ejemplos.</p><h2 id=comportamiento-de-los-selectores-to-y-from>Comportamiento de los selectores <code>to</code> y <code>from</code></h2><p>Existen cuatro tipos de selectores que pueden ser especificados en una sección de <code>ingress</code> <code>from</code> o en una sección de <code>egress</code> <code>to</code>:</p><p><strong>podSelector</strong>: Este selector selecciona Pods específicos en el mismo Namespace que la NetworkPolicy para permitir el tráfico como origen de entrada o destino de salida.</p><p><strong>namespaceSelector</strong>: Este selector selecciona Namespaces específicos para permitir el tráfico como origen de entrada o destino de salida.</p><p><strong>namespaceSelector</strong> <em>y</em> <strong>podSelector</strong>: Una única entrada <code>to</code>/<code>from</code> que especifica tanto <code>namespaceSelector</code> como <code>podSelector</code> selecciona Pods específicos dentro de Namespaces específicos. Es importante revisar que se utiliza la sintaxis de YAML correcta. A continuación se muestra un ejemplo de esta política:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>alice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>client<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>contiene un elemento <code>from</code> permitiendo conexiones desde los Pods con el label <code>role=client</code> en Namespaces con el label <code>user=alice</code>. Por el contrario, <em>esta</em> política:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>alice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>client<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>contiene dos elementos en el array <code>from</code>, y permite conexiones desde Pods en los Namespaces con el label <code>role=client</code>, <em>o</em> desde cualquier Pod en cualquier Namespace con el label <code>user=alice</code>.</p><p>En caso de duda, utilice <code>kubectl describe</code> para ver cómo Kubernetes ha interpretado la política.</p><p><a name=behavior-of-ipblock-selectors></a>
<strong>ipBlock</strong>: Este selector selecciona rangos CIDR de IP específicos para permitirlas como origen de entrada o destino de salida. Estas IPs deben ser externas al clúster, ya que las IPs de Pod son efímeras e impredecibles.</p><p>Los mecanismos de entrada y salida del clúster a menudo requieren reescribir la IP de origen o destino
de los paquetes. En los casos en los que esto ocurre, no está definido si esto ocurre antes o
después del procesamiento de NetworkPolicy, y el comportamiento puede ser diferente para diferentes
combinaciones de plugin de red, proveedor de nube, implementación de <code>Service</code>, etc.</p><p>En el caso de la entrada, esto significa que en algunos casos se pueden filtrar paquetes
entrantes basándose en la IP de origen real, mientras que en otros casos, la "IP de origen" sobre la que actúa la
la NetworkPolicy actúa puede ser la IP de un <code>LoadBalancer</code> o la IP del Nodo donde este el Pod involucrado, etc.</p><p>Para la salida, esto significa que las conexiones de los Pods a las IPs de <code>Service</code> que se reescriben a
IPs externas al clúster pueden o no estar sujetas a políticas basadas en <code>ipBlock</code>.</p><h2 id=políticas-por-defecto>Políticas por defecto</h2><p>Por defecto, si no existen políticas en un Namespace, se permite todo el tráfico de entrada y salida hacia y desde los Pods de ese Namespace. Los siguientes ejemplos muestran cómo cambiar el comportamiento por defecto en ese Namespace.</p><h3 id=denegar-todo-el-tráfico-de-entrada-por-defecto>Denegar todo el tráfico de entrada por defecto</h3><p>Puedes crear una política que "por defecto" aisle a un Namespace del tráfico de entrada con la creación de una política que seleccione todos los Pods del Namespace pero no permite ningún tráfico de entrada en esos Pods.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/service/networking/network-policy-default-deny-ingress.yaml download=service/networking/network-policy-default-deny-ingress.yaml><code>service/networking/network-policy-default-deny-ingress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-default-deny-ingress-yaml")' title="Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-default-deny-ingress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny-ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Esto asegura que incluso los Pods que no están seleccionados por ninguna otra NetworkPolicy también serán aislados del tráfico de entrada. Esta política no afecta el aislamiento en el tráfico de salida desde cualquier Pod.</p><h3 id=permitir-todo-el-tráfico-de-entrada>Permitir todo el tráfico de entrada</h3><p>Si tu quieres permitir todo el tráfico de entrada a todos los Pods en un Namespace, puedes crear una política que explícitamente permita eso.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/service/networking/network-policy-allow-all-ingress.yaml download=service/networking/network-policy-allow-all-ingress.yaml><code>service/networking/network-policy-allow-all-ingress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-allow-all-ingress-yaml")' title="Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-allow-all-ingress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>allow-all-ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- {}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Con esta política en curso, ninguna política(s) adicional puede hacer que se niegue cualquier conexión entrante a esos Pods. Esta política no tiene efecto sobre el aislamiento del tráfico de salida de cualquier Pod.</p><h3 id=denegar-por-defecto-todo-el-tráfico-de-salida>Denegar por defecto todo el tráfico de salida</h3><p>Puedes crear una política que "por defecto" aisle el tráfico de salida para un Namespace, creando una NetworkPolicy que seleccione todos los Pods pero que no permita ningún tráfico de salida desde esos Pods.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/service/networking/network-policy-default-deny-egress.yaml download=service/networking/network-policy-default-deny-egress.yaml><code>service/networking/network-policy-default-deny-egress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-default-deny-egress-yaml")' title="Copy service/networking/network-policy-default-deny-egress.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-default-deny-egress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny-egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Esto asegura que incluso los Pods que no son seleccionados por ninguna otra NetworkPolicy no tengan permitido el tráfico de salida. Esta política no cambia el comportamiento de aislamiento para el tráfico de entrada de ningún Pod.</p><h3 id=permitir-todo-el-tráfico-de-salida>Permitir todo el tráfico de salida</h3><p>Si quieres permitir todas las conexiones desde todos los Pods de un Namespace, puedes crear una política que permita explícitamente todas las conexiones salientes de los Pods de ese Namespace.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/service/networking/network-policy-allow-all-egress.yaml download=service/networking/network-policy-allow-all-egress.yaml><code>service/networking/network-policy-allow-all-egress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-allow-all-egress-yaml")' title="Copy service/networking/network-policy-allow-all-egress.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-allow-all-egress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>allow-all-egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- {}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Con esta política en vigor, ninguna política(s) adicional puede hacer que se niegue cualquier conexión de salida desde esos Pods. Esta política no tiene efecto sobre el aislamiento para el tráfico de entrada a cualquier Pod.</p><h3 id=denegar-por-defecto-todo-el-tráfico-de-entrada-y-de-salida>Denegar por defecto todo el tráfico de entrada y de salida</h3><p>Puede crear una política que "por defecto" en un Namespace impida todo el tráfico de entrada y de salida creando la siguiente NetworkPolicy en ese Namespace.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/service/networking/network-policy-default-deny-all.yaml download=service/networking/network-policy-default-deny-all.yaml><code>service/networking/network-policy-default-deny-all.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-default-deny-all-yaml")' title="Copy service/networking/network-policy-default-deny-all.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-default-deny-all-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny-all<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Esto asegura que incluso los Pods que no son seleccionados por ninguna otra NetworkPolicy no tendrán permitido el tráfico de entrada o salida.</p><h2 id=soporte-a-sctp>Soporte a SCTP</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>Como característica estable, está activada por defecto. Para deshabilitar SCTP a nivel de clúster, usted (o el administrador de su clúster) tiene que deshabilitar la <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> <code>SCTPSupport</code> para el API Server con el flag <code>--feature-gates=SCTPSupport=false,...</code>.
Cuando esta feature gate está habilitada, puede establecer el campo <code>protocol</code> de una NetworkPolicy como <code>SCTP</code>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes utilizar un plugin de <a class=glossary-tooltip title='Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label=CNI>CNI</a> que soporte el protocolo SCTP NetworkPolicies.</div><h2 id=apuntar-a-un-rango-de-puertos>Apuntar a un rango de puertos</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code></div><p>Cuando se escribe una NetworkPolicy, se puede apuntar a un rango de puertos en lugar de un solo puerto.</p><p>Esto se puede lograr con el uso del campo <code>endPort</code>, como el siguiente ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>multi-port-egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>db<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>to</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>ipBlock</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cidr</span>:<span style=color:#bbb> </span><span style=color:#666>10.0.0.0</span>/24<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>32000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>endPort</span>:<span style=color:#bbb> </span><span style=color:#666>32768</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>La regla anterior permite que cualquier Pod con la etiqueta <code>role=db</code> en el Namespace <code>default</code> se comunique
con cualquier IP dentro del rango <code>10.0.0.0/24</code> sobre el protocolo TCP, siempre que el puerto
esté entre el rango 32000 y 32768.</p><p>Se aplican las siguientes restricciones al utilizar este campo:</p><ul><li>Como característica en estado beta, está activada por defecto. Para desactivar el campo <code>endPort</code> a nivel de clúster, usted (o su administrador de clúster) debe desactivar la <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> <code>NetworkPolicyEndPort</code><br>en el API Server con el flag <code>--feature-gates=NetworkPolicyEndPort=false,...</code>.</li><li>El campo <code>endPort</code> debe ser igual o mayor que el campo <code>port</code>.</li><li>Sólo se puede definir <code>endPort</code> si también se define <code>port</code>.</li><li>Ambos puertos deben ser numéricos.</li></ul><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Su clúster debe utilizar un plugin de <a class=glossary-tooltip title='Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label=CNI>CNI</a> que
soporte el campo <code>endPort</code> en las especificaciones de NetworkPolicy.
Si su <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>plugin de red</a>
no soporta el campo <code>endPort</code> y usted especifica una NetworkPolicy que use este campo,
la política se aplicará sólo para el campo <code>port</code>.</div><h2 id=como-apuntar-a-un-namespace-usando-su-nombre>Como apuntar a un Namespace usando su nombre</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.22 [stable]</code></div><p>El plano de control de Kubernetes establece una etiqueta inmutable <code>kubernetes.io/metadata.name</code> en todos los
Namespaces, siempre que se haya habilitado la <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> <code>NamespaceDefaultLabelName</code>.
El valor de la etiqueta es el nombre del Namespace.</p><p>Aunque NetworkPolicy no puede apuntar a un Namespace por su nombre con algún campo de objeto, puede utilizar la etiqueta estandarizada para apuntar a un Namespace específico.</p><h2 id=que-no-puedes-hacer-con-políticas-de-red-al-menos-aún-no>Que no puedes hacer con políticas de red (al menos, aún no)</h2><p>Actualmente, en Kubernetes 1.25, la siguiente funcionalidad no existe en la API de NetworkPolicy, pero es posible que se puedan implementar soluciones mediante componentes del sistema operativo (como SELinux, OpenVSwitch, IPTables, etc.) o tecnologías de capa 7 (Ingress controllers, implementaciones de Service Mesh) o controladores de admisión. En caso de que seas nuevo en la seguridad de la red en Kubernetes, vale la pena señalar que las siguientes historias de usuario no pueden (todavía) ser implementadas usando la API NetworkPolicy.</p><ul><li>Forzar que el tráfico interno del clúster pase por una puerta de enlace común (esto se puede implementar con una malla de servicios u otro proxy).</li><li>Cualquier cosa relacionada con TLS (se puede implementar con una malla de servicios o un Ingress controllers para esto).</li><li>Políticas específicas de los nodos (se puede utilizar la notación CIDR para esto, pero no se puede apuntar a los nodos por sus identidades Kubernetes específicamente).</li><li>Apuntar Services por nombre (sin embargo, puede orientar los Pods o los Namespaces por su <a class=glossary-tooltip title='Metadatos en forma de clave-valor que permite añadir a los objetos atributos que sean relevantes para los usuarios para identificarlos.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a>, lo que suele ser una solución viable).</li><li>Creación o gestión de "solicitudes de políticas" que son atendidas por un tercero.</li><li>Políticas que por defecto son aplicadas a todos los Namespaces o Pods (hay algunas distribuciones y proyectos de Kubernetes de terceros que pueden hacer esto).</li><li>Consulta avanzada de políticas y herramientas de accesibilidad.</li><li>La capacidad de registrar los eventos de seguridad de la red (por ejemplo, las conexiones bloqueadas o aceptadas).</li><li>La capacidad de negar explícitamente las políticas (actualmente el modelo para NetworkPolicies es negar por defecto, con sólo la capacidad de añadir reglas de permitir).</li><li>La capacidad de impedir el tráfico entrante de Loopback o de Host (actualmente los Pods no pueden bloquear el acceso al host local, ni tienen la capacidad de bloquear el acceso desde su nodo residente).</li></ul><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Leer el artículo de como <a href=/docs/tasks/administer-cl%C3%BAster/declare-network-policy/>Declarar de Políticas de Red</a> para ver más ejemplos.</li><li>Ver más <a href=https://github.com/ahmetb/kubernetes-network-policy-recipes>recetas</a> de escenarios comunes habilitados por los recursos de las NetworkPolicy.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f018f568c6723865753f150c3c59bdda>6 - Almacenamiento</h1></div><div class=td-content><h1 id=pg-27795584640a03bd2024f1fe3b3ab754>6.1 - Volumes</h1><p>Los archivos localizados dentro de un contenedor son efímeros, lo cual presenta problemas
para aplicaciones no triviales cuando se ejecutan en contenedores. Un problema es la
pérdida de archivos cuando el contenedor termina. Kubelet reinicia el contenedor con un estado limpio.
Un segundo problema ocurre cuando compartimos ficheros entre contenedores corriendo juntos dentro de un <code>Pod</code>. La abstracción <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a> de Kubernetes resuelve ambos problemas.
Se sugiere familiaridad con <a href=/docs/concepts/workloads/pods/>Pods</a></p><h2 id=trasfondo>Trasfondo</h2><p>Docker tiene el concepto de <a href=https://docs.docker.com/storage/>volúmenes</a>, aunque es algo más flojo y menos controlado.
Un volumen de Docker es un directorio en disco o en otro contenedor. Docker provee controladores de volúmenes, pero la funcionalidad es algo limitada.</p><p>Kubernetes soporta muchos tipos de volúmenes. Un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>
puede utilizar cualquier número de tipos de volúmenes simultáneamente. Los tipos de volúmenes efímeros tienen el mismo tiempo de vida que un Pod, pero los volúmenes persistentes existen más allá del tiempo de vida de un Pod. Cuando un Pod deja de existir,
Kubernetes destruye los volúmenes efímeros; sin embargo, Kubernetes no destruye los volúmenes persistentes. Para cualquier tipo de volumen en un Pod dado, los datos son preservados a lo largo de los reinicios del contenedor.</p><p>En su núcleo, un volumen es un directorio, posiblemente con algunos datos en este, que puede ser accesible para los contenedores en un Pod. Cómo ese directorio llega a crearse, el medio que lo respalda, y el contenido de este se determinan por el tipo de volumen usado.</p><p>Para usar un volumen, especifica los volúmenes a proveer al por en <code>.spec.volumes</code> y declara
dónde montar estos volúmenes dentro de los contenedores en <code>.spec.containers[*].volumeMounts</code>.
Un proceso en el contenedor observa una vista del sistema de archivos compuesta por la imagen Docker y volúmenes.
La <a href=https://docs.docker.com/userguide/dockerimages/>imagen Docker</a> está en la raíz de la jerarquía del sistema de archivos.
Los volúmenes se montan en las rutas especificadas dentro de la imagen. Los volúmenes no se pueden montar en otros volúmenes o tener enlaces duros a otros volúmenes. Cada contenedor en la configuración del Pod debe especificar de forma independiente donde montar cada volumen.</p><h2 id=volume-types>Tipos de volúmenes</h2><p>Kubernetes soporta varios tipos de volúmenes</p><h3 id=awselasticblockstore>awsElasticBlockStore</h3><p>Un volumen <code>awsElasticBlockStore</code> monta un
<a href=https://aws.amazon.com/ebs/>volumen EBS</a> de Amazon Web Services (AWS) en tu Pod. A diferencia de
<code>emptyDir</code>, que se borra cuando se quita un Pod, el contenido de un volumen EBS es persistido cuando se desmonta el volumen.
Esto significa que un volumen EBS puede ser pre-poblado con datos, y que los datos puedes ser compartidos entre pods.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes crear un volumen EBS usando <code>aws ec2 create-volume</code> o la API de AWS antes de poder usarlo.</div><p>Existen algunas restricciones cuando usas un volumen <code>awsElasticBlockStore</code>:</p><ul><li>Los nodos en los que corren los pods deben ser instances AWS EC2.</li><li>Estas instancias deben estar en la misma región y zona de disponibilidad que el volumen EBS</li><li>EBS solo soporta una única instancia EC2 montando un volumen</li></ul><h4 id=creando-un-volumen-aws-ebs>Creando un volumen AWS EBS</h4><p>Antes poder usar un volumen EBS en un Pod, necesitas crearlo.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>aws ec2 create-volume --availability-zone<span style=color:#666>=</span>eu-west-1a --size<span style=color:#666>=</span><span style=color:#666>10</span> --volume-type<span style=color:#666>=</span>gp2
</span></span></code></pre></div><p>Asegúrate de que la zona coincide con la zona en que has creado el clúster. Revisa que el tamaño y el tipo de
volumen EBS son compatibles para tu uso.</p><h4 id=ejemplo-de-configuración-aws-ebs>Ejemplo de configuración AWS EBS</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-ebs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-ebs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Este volumen EBS debe existir anteriormente.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>awsElasticBlockStore</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeID</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&lt;volume id&gt;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span></code></pre></div><p>Si el volumen EBS está particionado, puedes suministrar el campo opcional <code>partition: "&lt;partition number>"</code> para especificar cuál partición montar.</p><h4 id=migración-csi-aws-ebs-csi>Migración CSI AWS EBS CSI</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [beta]</code></div><p>La función <code>CSIMigration</code> para <code>awsElasticBlockStore</code>, cuando se habilita, redirige todas las operaciones
de complemento desde el complemento existente dentro del árbol existente al controlador de Interfaz de Almacenamiento del Contenedor (CSI) de <code>ebs.csi.aws.com</code>. Para utilizar esta función, el <a href=https://github.com/kubernetes-sigs/aws-ebs-csi-driver>controlador AWS EBS CSI</a> debe ser instalado en el clúster y las características beta
<code>CSIMigration</code> y <code>CSIMigrationAWS</code> deben estar habilitadas.</p><h4 id=migración-csi-aws-ebs-csi-completa>Migración CSI AWS EBS CSI completa</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [alpha]</code></div><p>Para desactivar el complemento de almacenamiento <code>awsElasticBlockStore</code> de ser cargado por el administrador de controladores y el kubelet, establece el atributo <code>CSIMigrationAWSComplete</code> a <code>true</code>. Esta función requiere tener instalado el controlador de interfaz de almacenamiento del contenedor (CSI) en todos los nodos en obreros.</p><h3 id=azuredisk>azureDisk</h3><p>El tipo de volumen <code>azureDisk</code> monta un <a href=https://docs.microsoft.com/en-us/azure/aks/csi-storage-drivers>Data Disk</a> de Microsoft Azure en el Pod.</p><p>Para más detalles, mira el <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk/README.md><code>azureDisk</code> volume plugin</a>.</p><h4 id=migración-csi-azuredisk>Migración CSI azureDisk</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code></div><p>La función <code>CSIMigration</code> para <code>azureDisk</code>, cuando se habilita, redirige todas las operaciones
de complemento desde el complemento existente dentro del árbol existente al controlador de Interfaz de Almacenamiento del Contenedor (CSI) de <code>disk.csi.azure.com</code>. Para utilizar esta función, el <a href=https://github.com/kubernetes-sigs/azuredisk-csi-driver>controlador Azure Disk CSI</a> debe ser instalado en el clúster y las características beta
<code>CSIMigration</code> y <code>CSIMigrationAzureDisk</code> deben estar habilitadas.</p><h3 id=azurefile>azureFile</h3><p>El tipo de volumen <code>azureFile</code> monta un volumen de ficheros de Microsoft Azure (SMB 2.1 and 3.0) en un Pod.</p><p>Para más detalles, mira el <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md><code>azureFile</code> volume plugin</a>.</p><h4 id=migración-csi-azurefile-csi>Migración CSI azureFile CSI</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code></div><p>La función <code>CSIMigration</code> para <code>azureFile</code>, cuando se habilita, redirige todas las operaciones
de complemento desde el complemento existente dentro del árbol existente al controlador de Interfaz de Almacenamiento del Contenedor (CSI) de <code>file.csi.azure.com</code>. Para utilizar esta función, el <a href=https://github.com/kubernetes-sigs/azurefile-csi-driver>controlador Azure File CSI
Driver</a>
debe ser instalado en el clúster y las <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gates</a> <code>CSIMigration</code> y <code>CSIMigrationAzureFile</code> deben estar habilitadas.</p><p>El controlador Azure File CSI no soporta usar el mismo volumen con fsgroups diferentes, si está habilitadla migración CSI Azurefile, usar el mismo volumen con fsgorups diferentes no será compatible en absoluto.</p><h3 id=cephfs>cephfs</h3><p>Un volumen <code>cephfs</code> permite montar un volumen CephFS existente en tu Pod.
A diferencia de <code>emptydir</code>, que es borrado cuando se remueve el Pod, el contenido de un volumen <code>cephfs</code> es preservado y el volumen es meramente desmontado. Esto significa que un volumen <code>cephfs</code>puede ser pre-poblado por múltiples escritores simultáneamente.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes tener tu propio servidor Ceph corriendo con el recurso compartido exportado antes de usarlo.</div><p>Mira el <a href=https://github.com/kubernetes/examples/tree/master/volumes/cephfs/>CephFS example</a> para más detalles.</p><h3 id=cinder>cinder</h3><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Kubernetes no debe ser configurado con el proveedor cloud OpenStack.</div><p>El tipo de volumen <code>cinder</code> se usa para montar un volumen Cinder de OpenStack en tu Pod.</p><h4 id=cinder-volume-configuration-example>Cinder volume configuration example</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-cinder<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-cinder-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-cinder<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Este volumen de  OpenStack debe existir anteriormente.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cinder</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeID</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&lt;volume id&gt;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=migración-csi-openstack>Migración CSI OpenStack</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code></div><p>La función <code>CSIMigration</code> para Cinder está habilitada por defecto en Kubernetes 1.21.
Esta redirige todas las operaciones de complemento desde el complemento existente dentro del árbol existente al controlador de Interfaz de Almacenamiento del Contenedor (CSI) de <code>cinder.csi.openstack.org</code>.
El controlador <a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md>OpenStack Cinder CSI Driver</a> debe estar instalado en el clúster.</p><p>Puedes deshabilitar la migración CSI para tu clúster estableciendo el <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> <code>CSIMigrationOpenStack</code> a <code>false</code>.
Si deshabilitas la función <code>CSIMigrationOpenStack</code>, el complemento del volumen Cinder dentro del árbol toma la responsabilidad para todos los aspectos de la administración del almacenamiento del volumen Cinder.</p><h3 id=configmap>configMap</h3><p>Un <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMap</a>
provee una manera de inyectar datos de configuración a los pods.
Los datos almacenados en un ConfigMap se pueden referenciar en un volumen de tipo <code>configMap</code>
y luego ser consumidos por aplicaciones contenerizadas corriendo en un Pod.</p><p>Cuando haces referencia a un ConfigMap, provees el nombre del ConfigMap en el volumen.
Puedes personalizar la ruta para una entrada específica en el ConfigMap.
La siguiente configuración muestra cómo montar un ConfigMap <code>log-config</code> en un Pod llamado <code>configmap-pod</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>configmap-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>log-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>log_level<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>log_level<span style=color:#bbb>
</span></span></span></code></pre></div><p>El ConfigMap <code>log-config</code> es montado como un volumen, y todo el contenido almacenado en su entrada <code>log_level</code> es
montado en el Pod en la ruta <code>/etc/config/log_level</code>. Ten en cuenta que esta ruta se deriva del <code>mountPath</code>del volumen y el <code>path</code> cuya clave es <code>log_level</code>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><ul><li>Debes crear un <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMap</a> antes de usarlo.</li><li>Un contenedor usando un ConfigMap montado como un volumen <a href=#using-subpath><code>subPath</code></a> no recibirá actualizaciones del ConfigMap</li><li>Los datos de texto son expuestos como ficheros usando la codificación de caracteres UTF-8. Para otras codificaciones de caracteres, use <code>binaryData</code>.</li></ul></div><h3 id=downwardapi>downwardAPI</h3><p>Un volumen de <code>downwardAPI</code> hace que los datos API descendentes estén disponibles para las aplicaciones.
Monta un directorio y escribe los datos solicitados en archivos de texto sin formato.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Un contenedor usando la API descendiente montado como un volumen <a href=#using-subpath><code>subPath</code></a> no recibirá actualizaciones API descendientes.</div><p>Mira el <a href=/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/>downward API example</a> para mayores detalles.</p><h3 id=emptydir>emptyDir</h3><p>Un volumen <code>emptyDir</code>es creado primero cuando se asigna un Pod a un nodo, y existe mientras el Pod está corriendo en el nodo.
Como su nombre lo indica un volumen <code>emptydir</code>está inicialmente vacío. Todos los contenedores en el Pod pueden leer y escribir
los archivos en el volumen <code>emptyDir</code>, aunque ese volumen se puede montar en la misma o diferente ruta en cada contenedor. Cuando un Pod es removido del nodo por alguna razón, los datos en <code>emptydir</code> se borran permanentemente.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Un contenedor que colapsa <em>no</em> remueve el Pod del nodo. Los datos en un volumen <code>emptyDir</code> están seguros en caso de
colapso del contenedor.</div><p>Algunos usos para un <code>emptyDir</code> son:</p><ul><li>Espacio temporal, como para una clasificación de combinación basada en disco</li><li>Marcar un largo cálculo para la recuperación de fallos</li><li>Contener archivos que un contenedor de administrador de contenido recupera mientras un contenedor de servidor web
sirve los datos</li></ul><p>Dependiendo de tu entorno, los volúmenes <code>emptydir</code> se almacenan en cualquier medio que respalde el nodo tales como disco SSD, o almacenamiento de red. Sin embargo, si se establece el campo <code>emptydir.medium</code> a <code>Memory</code>, Kubernetes monta en su lugar un tmpfs (sistema de ficheros respaldado por la RAM). Mientras que tmpfs es muy rápido, ten en cuenta que a diferencia de los discos, tmpfs se limpia cuando el nodo reinicia y cualquier archivo que escribas cuenta con el límite de memoria del contenedor.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Si el <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> <code>SizeMemoryBackedVolumes</code> está habilitado,
puedes especificar un tamaño para los volúmenes respaldados en memoria. Si no se especifica ningún tamaño, los volúmenes respaldados en memoria tienen un tamaño del 50% de la memoria en un host Linux.</div><h4 id=ejemplo-de-configuración-de-emptydir>Ejemplo de configuración de emptyDir</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/cache<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cache-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cache-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=fc>fc (canal de fibra)</h3><p>Un tipo de volumen <code>fc</code> permite que un volumen de almacenamiento de bloque de canal de fibra existente se monte en un Pod.
Puede especificar nombres mundiales de destino únicos o múltiples (WWN) utilizando el parámetro <code>targetWWNs</code> en su configuración de volumen.
Si se especifican varios WWN, targettWWNs esperan que esos WWN sean de conexiones de múltiples rutas.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes configurar FC SAN zoning para asignar y enmascarar esos (volúmenes) LUNs para apuntar a los WWNs de destino de antemano
para que los hosts Kubernetes pueda acceder a ellos.</div><p>Revisa el <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel>ejemplo de canal de fibra</a> para más detalles.</p><h3 id=flocker>flocker (deprecado)</h3><p><a href=https://github.com/ClusterHQ/flocker>Flocker</a> es un administrador open-source de volúmenes de contenedor agrupado por clúster.
Flocker proporciona administración y orquestación de volúmenes de datos respaldados por una variedad de backends de almacenamiento.</p><p>Un volumen <code>flocker</code> permite montar un conjunto de datos Flocker en un Pod. Si el conjunto de datos no existe en Flocker, necesita ser creado primero con el CLI de Flocker o usando la API de Flocker. Si el conjunto de datos existe será adjuntado
de nuevo por Flocker al nodo donde el Pod está programado. Esto significa que los datos pueden ser compartidos entre pods como sea necesario.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes tener una instalación propia de Flocker ejecutándose antes de poder usarla.</div><p>Mira el <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/flocker>ejemplo de Flocker</a> para más detalles.</p><h3 id=gcepersistentdisk>gcePersistentDisk</h3><p>Un volumen <code>gcePersistentDisk</code> monta un volumen de Google Compute Engine (GCE)
de <a href=https://cloud.google.com/compute/docs/disks>disco persistente</a> (DP) en tu Pod.
A diferencia de <code>emptyDir</code>, que se borra cuando el Pod es removido, el contenido de un disco persistente es preservado
y el volumen solamente se desmonta. Esto significa que un disco persistente puede ser pre-poblado con datos,
y que esos datos se pueden compartir entre pods.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes crear un disco persistente usando <code>gcloud</code>, la API de GCE o la UI antes de poder usarlo.</div><p>Existen algunas restricciones cuando usas <code>gcePersistentDisk</code>:</p><ul><li>Los nodos en los que se ejecutan los pods deben ser máquinas virtuales GCE.</li><li>Esas máquinas virtuales deben estar en el mismo proyecto GCE y zona que el disco persistente se encuentra.</li></ul><p>Una de las características del disco persistente CGE es acceso concurrente de solo lectura al disco persistente.
Un volumen <code>gcePersistentDisk</code> permite montar simultáneamente un disco de solo lectura a múltiples consumidores.
Esto significa que puedes pre-poblar un DP con tu conjunto de datos y luego servirlo en paralelo desde tantos pods como necesites. Desafortunadamente, los DPs solo se pueden montar por un único consumidor en modo lectura-escritura.
No están permitidos escritores simultáneos.</p><p>Usar un disco persistente GCE con un Pod controlado por un ReplicaSet fallará a manos que el DP sea de solo lectura o el número de réplicas sea 0 o 1.</p><h4 id=gce-create-persistent-disk>Creando un disco persistente GCE</h4><p>Antes de poder usar un disco persistente GCE en un Pod, necesitas crearlo.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>gcloud compute disks create --size<span style=color:#666>=</span>500GB --zone<span style=color:#666>=</span>us-central1-a my-data-disk
</span></span></code></pre></div><h4 id=ejemplo-de-configuración-de-un-disco-persistente-gce>Ejemplo de configuración de un disco persistente GCE</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Este PD GCE debe existir con anterioridad.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>gcePersistentDisk</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pdName</span>:<span style=color:#bbb> </span>my-data-disk<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=discos-regionales-persistentes>Discos regionales persistentes</h4><p>La función de <a href=https://cloud.google.com/compute/docs/disks/#repds>discos regionales persistentes</a>
permite la creación de discos persistentes que están disponibles en dos zonas dentro de la misma región.
Para usar esta función, el volumen debe ser provisto como un PersistentVolumen; referenciar el volumen directamente desde un Pod no está soportado.</p><h4 id=aprovisionamiento-manual-de-un-pd-persistentvolume-regional>Aprovisionamiento manual de un PD PersistentVolume Regional</h4><p>El aprovisionamiento dinámico es posible usando un <a href=/docs/concepts/storage/storage-classes/#gce-pd>StorageClass para el DP GCE</a>.
Antes de crear un PersistentVolume, debes crear el disco persistente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>gcloud compute disks create --size<span style=color:#666>=</span>500GB my-data-disk
</span></span><span style=display:flex><span>  --region us-central1
</span></span><span style=display:flex><span>  --replica-zones us-central1-a,us-central1-b
</span></span></code></pre></div><h4 id=ejemplo-de-configuración-de-un-disco-persistente-regional>Ejemplo de configuración de un disco persistente regional</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>capacity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>400Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>gcePersistentDisk</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pdName</span>:<span style=color:#bbb> </span>my-data-disk<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>required</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>failure-domain.beta.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- us-central1-a<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- us-central1-b<span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=migración-csi-gce>Migración CSI GCE</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [beta]</code></div><p>La función <code>CSIMigration</code> para el DP GCE, cuando se habilita, redirige todas las operaciones
de complemento desde el complemento existente dentro del árbol existente al controlador de Interfaz de Almacenamiento del Contenedor (CSI) de <code>pd.csi.storage.gke.io</code>. Para poder usar esta función, el <a href=https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver>controlador PD GCE</a> debe ser instalado en el clúster y habilitar las funciones beta
<code>CSIMigration</code> y <code>CSIMigrationGCE</code>.</p><h3 id=gitrepo>gitRepo (deprecado)</h3><div class="alert alert-danger warning callout" role=alert><strong>Advertencia:</strong> El volumen <code>gitRepo</code> está deprecado. Para aprovisionar un contenedor con un repositorio git, monta un <a href=#emptydir>EmptyDir</a> en un InitContainer que clona un repositorio usando git, luego monta el <a href=#emptydir>EmptyDir</a> en el contenedor del Pod.</div><p>Un volumen <code>gitRepo</code> es un ejemplo de un complemento de volumen. Este complemento monta un directorio vacío y clona un repositorio git en este directorio para que tu Pod pueda usarlo.</p><p>Aquí un ejemplo de un volumen <code>gitrepo</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/mypath<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>git-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>git-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>gitRepo</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>repository</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;git@somewhere:me/my-git-repository.git&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>revision</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;22f1d8406d464b0c0874075539c1f2e96c253775&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=glusterfs>glusterfs</h3><p>Un volumen <code>glusterfs</code> permite montar un volumen <a href=https://www.gluster.org>Glusterfs</a> en tu Pod.
A diferencia de <code>emptyDir</code>, que se borra cuando se remueve un Pod, el contenido de un volumen <code>glusterfs</code> es preservado
y el volumen solamente se desmonta. Esto significa que un volumen glusterfs puede ser pre-poblado con datos,
y que los datos pueden ser compartidos entre pods. GlusterFS puede ser montado por múltiples escritores simultáneamente.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes tener tu propia instalación de GlusterFS ejecutándose antes de poder usarla.</div><p>Mira el <a href=https://github.com/kubernetes/examples/tree/master/volumes/glusterfs>ejemplo de GlusterFS</a> para más detalles.</p><h3 id=hostpath>hostPath</h3><p>Un volumen <code>hostPath</code> monta un archivo o un directorio del sistema de archivos del nodo host a tu Pod.
Esto no es algo de muchos Pods necesiten, pero ofrece una trampa de escape poderosa para algunas aplicaciones.</p><p>Por ejemplo, algunos usos de un <code>hostPath</code> son:</p><ul><li>ejecutar un contenedor que necesita acceso a los directorios internos de Docker, usa un <code>hostPath</code> de <code>/var/lib/docker</code></li><li>ejecutar un cAdvisor en un contenedor; usa un <code>hostPath</code> de <code>/sys</code></li><li>permitir a un Pod especificar si un <code>hostPath</code> dado debería existir ante de correr el Pod, si debe crearse, cómo debe existir</li></ul><p>Además de la propiedad requerida <code>path</code>, puedes especificar opcionalmente un <code>tipo</code>para un volumen <code>hostPath</code>.</p><p>Los valores soportados para el campo <code>tipo</code> son:</p><table><thead><tr><th style=text-align:left>Valor</th><th style=text-align:left>Comportamiento</th></tr></thead><tbody><tr><td style=text-align:left></td><td style=text-align:left>Una cadena vacía (por defecto) es para compatibilidad con versiones anteriores, lo que significa que no se harán revisiones antes de montar el volumen hostPath.</td></tr><tr><td style=text-align:left><code>DirectoryOrCreate</code></td><td style=text-align:left>Si no hay nada en la ruta dada, se creará un directorio vacío como es requerido con los permisos a 0755, teniendo el mismo grupo y propiedad que el Kubelet.</td></tr><tr><td style=text-align:left><code>Directory</code></td><td style=text-align:left>Un directorio debe existir en la ruta dada</td></tr><tr><td style=text-align:left><code>FileOrCreate</code></td><td style=text-align:left>Si no hay nada en la ruta dada, se creará un archivo vacío como es requerido con los permisos a 0644, teniendo el mismo grupo y propiedad que el Kubelet.</td></tr><tr><td style=text-align:left><code>File</code></td><td style=text-align:left>Un archivo debe existir en la ruta dada</td></tr><tr><td style=text-align:left><code>Socket</code></td><td style=text-align:left>Un socket de UNIX debe existir en la ruta dada</td></tr><tr><td style=text-align:left><code>CharDevice</code></td><td style=text-align:left>Un dispositivo de caracteres debe existir en la ruta data</td></tr><tr><td style=text-align:left><code>BlockDevice</code></td><td style=text-align:left>Un dispositivo de bloques dbe existir en la ruta dada</td></tr></tbody></table><p>Ten cuidado cuando uses este tipo de volumen, porque:</p><ul><li>Los Pods con configuración idéntica (tales como los creados por un PodTemplate) pueden comportarse de forma distinta
en nodos distintos debido a diferentes ficheros en los nodos.</li><li>Los ficheros o directorios creados en los hosts subyacentes son modificables solo por root. Debes ejecutar tu proceso como root en un <a href=/docs/tasks/configure-pod-container/security-context/>Contenedor privilegiado</a> o modificar los permisos de archivo en el host para escribir a un volumen <code>hostPath</code></li></ul><h4 id=ejemplo-de-configuración-hostpath>Ejemplo de configuración hostPath</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># localización del directorio en el host</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># este campo es opcional</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Directory<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-warning caution callout" role=alert><strong>Precaución:</strong> El modo <code>FileOrCreate</code> no crea el directorio padre del archivo. Si el directorio padre del archivo montado no existe,
el Pod falla en iniciar. Para asegurar que este modo funciona, puedes intentar montar directorios y ficheros de forma separada,
tal como se muestra en la <a href=#hostpath-fileorcreate-example>confugiración <code>FileOrCreate</code></a></div><h4 id=hostpath-fileorcreate-example>ejemplo de configuración hostPath FileOrCreate</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/test-webserver:latest<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/local/aaa<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydir<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/local/aaa/1.txt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myfile<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydir<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Asegúrate que el directorio del archivo es creado.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/local/aaa<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>DirectoryOrCreate<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myfile<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/local/aaa/1.txt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>FileOrCreate<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=iscsi>iscsi</h3><p>Un volumen <code>iscsi</code> permite que se monte un volumen ISCSI (SCSI sobre IP) existente
en tu Pod. A diferencia de <code>emptydir</code>, que es removido cuando se remueve un Pod,
el contenido de un volumen <code>iscsi</code> es preservado y el volumen solamente se desmonta.
Esto significa que un volumen iscsi puede ser pre-poblado con datos, y que estos datos se pueden compartir entre pods.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes tener tu propio servidor ISCSI corriendo con el volumen creado antes de poder usarlo.</div><p>Una función de SCSI es que puede ser montado como de solo lectura por múltiples consumidores simultáneamente.
Esto significa que puedes pre-poblar un volumen con tu conjunto de datos y servirlo en paralelo para tantos Pods como necesites.
Desafortunadamente, los volúmenes ISCSI solo se pueden montar por un único consumidor en modo lectura-escritura.
Escritores simultáneos no está permitido.</p><p>Mira el <a href=https://github.com/kubernetes/examples/tree/master/volumes/iscsi>ejemplo iSCSI</a> para más detalles.</p><h3 id=local>local</h3><p>Un volumen <code>local</code> representa un dispositivo de almacenamiento local como un disco, una partición o un directorio.</p><p>Los volúmenes locales solo se pueden usar como un PersistenVolume creado estáticamente.
El aprovisionamiento dinámico no está soportado.</p><p>Comparados con volúmenes <code>hostPath</code>, los volúmenes <code>local</code> se usan de manera duradera y portátil sin programar pods manualmente
a los nodos. El sistema está consciente de las limitaciones del nodo del volumen al mirar la afinidad del nodo en el PersistenVolumen.</p><p>Sin embargo, los volúmenes <code>local</code>están sujetos a la disponibilidad del nodo subyacente y no son compatibles para todas las aplicaciones.</p><p>Si un nodo deja de estar sano, entonces el volumen <code>local</code> se vuelve inaccesible al Pod.
El Pod que utiliza este volumen no se puede ejecutar.
Las aplicaciones que usan volúmenes <code>local</code> deben ser capaces de tolerar esta disponibilidad reducida,
así como la pérdida potencial de datos, dependiendo de las características de durabilidad del disco subyacente.</p><p>El siguiente ejemplo muestra un PersistentVolume usando un volumen <code>local</code>y <code>nodeAffinity</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-pv<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>capacity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>100Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Filesystem<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>persistentVolumeReclaimPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>local-storage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>local</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/mnt/disks/ssd1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>required</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>kubernetes.io/hostname<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- example-node<span style=color:#bbb>
</span></span></span></code></pre></div><p>Debes establecer un valor de <code>nodeAffinity</code> del PersistenVolume cuando uses volúmenes <code>local</code>.
El Scheduler de Kubernetes usa <code>nodeaffinity</code> del PersistenVolume para programar estos Pods al nodo correcto.</p><p>El <code>volumeMode</code> del PersistentVolume se puede establecer en "Block" (en lugar del valor por defecto, "Filesystem")
para exponer el volumen local como un dispositivo de bloque sin formato.</p><p>Cuando usas volúmenes locales, se recomienda crear un StorageClass con <code>volumeBindingMode</code> en <code>WaitForFirstConsumer</code>.
Para más detalles, mira el ejemplo de <a href=/docs/concepts/storage/storage-classes/#local>StorageClass</a>. Retrasar el enlace con el volumen asegura que la decisión del PersistenVolumeClaim sea evaluada con otras limitaciones que el Pod pueda tener,
tales como requisitos de recursos del nodo, selectores de nodo, afinidad del Pod, y anti-afinidad del Pod.</p><p>Se puede ejecutar un aprovisionador estático externo para un manejo mejorado del ciclo de vida del volumen local.
Ten en cuenta que este aprovisionador no soporta aprovisionamiento dinámico todavía. Para un ejemplo de un aprovisionador local externo, mira la <a href=https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner>guía de usuario de aprovisionador de volumen local</a></p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El PersistentVolume local requiere limpieza y borrado manual por el usuario si no se utiliza el aprovisionador estático externo para manejar el ciclo de vida del volumen.</div><h3 id=nfs>nfs</h3><p>Un volumen <code>nfs</code> permite montar un NFS (Sistema de Ficheros de Red) compartido en tu Pod.
A diferencia de <code>emptyDir</code> que se borra cuando el Pod es removido, el contenido de un volumen <code>nfs</code> solamente se desmonta.
Esto significa que un volumen NFS puede ser pre-poblado con datos, y que estos datos puedes ser compartidos entre pods.
NFS puede ser montado por múltiples escritores simultáneamente.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes tener tu propio servidor NFS en ejecución con el recurso compartido exportado antes de poder usarlo.</div><p>Mira el <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs>ejemplo NFS</a> para más información.</p><h3 id=persistentvolumeclaim>persistentVolumeClaim</h3><p>Un volumen <code>persistenceVolumeClain</code> se utiliza para montar un <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolume</a> en tu Pod. PersistentVolumeClaims son una forma en que el usuario "reclama" almacenamiento duradero (como un PersistentDisk GCE o un volumen ISCSI) sin conocer los detalles del entorno de la nube en particular.</p><p>Mira la información spbre <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a> para más detalles.</p><h3 id=portworxvolume>portworxVolume</h3><p>Un <code>portworxVolume</code> es un almacenamiento de bloque elástico que corre hiperconvergido con Kubernetes.
Almacenamiento de huellas de <a href=https://portworx.com/use-case/kubernetes-storage/>Portworx</a> en un servidor, niveles basados en capacidades y capacidad agregada en múltiples servidores.
Portworx se ejecuta como invitado en máquinas virtuales o en nodos Linux nativos.</p><p>Un <code>portworxVolume</code> puede ser creado dinámicamente a través de Kubernetes o puede ser pre-aprovisionado y referido dentro de un Pod. Aquí un Pod de ejemplo refiriendo a un volumen Portworx pre-aprovisionado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-portworx-volume-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/mnt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pxvol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pxvol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Este volumen portworx debe sxistir con anterioridad.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>portworxVolume</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeID</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;pxvol&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&lt;fs-type&gt;&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Asegúrate de tener un PortworxVolume con el nombre <code>pxvol</code> antes de usarlo en el Pod.</div><p>Para más detalles, mira los ejemplos de <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md>volumen Portworx</a>.</p><h3 id=projected>projected</h3><p>Un volumen <code>projected</code> mapea distintas fuentes de volúmenes existentes en un mismo directorio.</p><p>Actualmente, se pueden los siguientes tipos de volúmenes:</p><ul><li><a href=#secret><code>secret</code></a></li><li><a href=#downwardapi><code>downwardAPI</code></a></li><li><a href=#configmap><code>configMap</code></a></li><li><code>serviceAccountToken</code></li></ul><p>Se requiere que todas las fuentes estén en el mismo namespace que el Pod. Para más detalles mira el <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/all-in-one-volume.md>all-in-one volume design document</a>.</p><h4 id=example-configuration-secret-downwardapi-configmap>Configuración de ejemplo con un secret, un downwardAPI, y un configMap</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>volume-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>container-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>all-in-one<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/projected-volume&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>all-in-one<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>projected</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>sources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>downwardAPI</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;labels&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>fieldRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>fieldPath</span>:<span style=color:#bbb> </span>metadata.labels<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;cpu_limit&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>resourceFieldRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>containerName</span>:<span style=color:#bbb> </span>container-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                    </span><span style=color:green;font-weight:700>resource</span>:<span style=color:#bbb> </span>limits.cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myconfigmap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-config<span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=example-configuration-secrets-nondefault-permission-mode>Configuración de ejemplo: secrets con un modo de permisos no predeterminados</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>volume-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>container-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>all-in-one<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/projected-volume&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>all-in-one<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>projected</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>sources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>password<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-password<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>mode</span>:<span style=color:#bbb> </span><span style=color:#666>511</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Cada volumen proyectado está listado en spec bajo <code>sources</code>. Los parámetros son casi los mismos salvo dos excepciones:</p><ul><li>Para los secrets, el campo <code>secretName</code> ha sido cambiado a <code>name</code> para ser consistente con el nombre del configMap.</li><li>El <code>defaultMode</code> solo se puede especificar en el nivel proyectado y no para cada fuente de volumen. Sin, como se muestra arriba, puedes establecer explícitamente el <code>mode</code> para cada proyección individual.</li></ul><p>Cuando la función <code>TokenRequestProjection</code> está habilitada, puedes inyectar el token para el <a href=/docs/reference/access-authn-authz/authentication/#service-account-tokens>service account</a> actual en un Pod en la ruta especificada.
Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>sa-token-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>container-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>token-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/service-account&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>token-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>projected</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>sources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>serviceAccountToken</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>audience</span>:<span style=color:#bbb> </span>api<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>expirationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>3600</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>token<span style=color:#bbb>
</span></span></span></code></pre></div><p>El Pod de ejemplo tiene un volumen proyectado que contiene el token del serviceAccount inyectado.
Este token se puede usar por el contenedor de un Pod para acceder a la API del servidor de Kubernetes.
El <code>audience</code> contiene la audiencia dirigida del token. Un recipiente del token debe identificarse a sí mismo con
un identificador especificado en la audiencia del token, de lo contrario debería rechazar el token. Este campo es opcional y por defecto tiene el valor del identificador del servidor API.</p><p>EL campo <code>expirationSeconds</code> es la duración esperada de la validez del token del serviceAccount.
Su valor por defecto es 1 hora y debe ser al menos 10 minutos (600 segundos). Un administrador puede limitar
su valor máximo al especificar la opción <code>--service-account-max-token-expiration</code> para el servidor API. El campo <code>path</code> especifica una ruta relativa al punto de montaje del volumen proyectado.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Un contenedor que usa una fuente de volumen proyectado como un volumen <a href=#using-subpath><code>subPath</code></a> no recibirá actualizaciones de estas fuentes de volumen.</div><h3 id=quobyte>quobyte</h3><p>Un volumen <code>quobyte</code> permite montar un volumen <a href=https://www.quobyte.com>Quobyte</a> en tu Pod.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes tener tu propia configuración Quobyte ejecutándose con los volúmenes creados antes de usarlo.</div><p>Quobyte soporta el <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label='Container Storage Interface'>Container Storage Interface</a>.
CSI es el complemento recomendado para usar Quobyte dentro de Kubernetes. El proyecto Github de Quobyte tiene <a href=https://github.com/quobyte/quobyte-csi#quobyte-csi>instrucciones</a> para desplegar usando CSI, junto con ejemplos.</p><h3 id=rbd>rbd</h3><p>Un volumen <code>rbd</code> permite montar un volumen <a href=https://docs.ceph.com/en/latest/rbd/>Rados Block Device</a> (RBD) en tu Pod.
A diferencia de <code>emptyDir</code>, que se borra cuando el Pod es removido, el contenido de un volumen <code>rbd</code> es preservado y el volumen se desmonta. Esto significa que un volumen RBD puede ser pre-poblado con datos, y que estos datos pueden ser compartidos entre pods.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes tener una instalación de Ceph ejecutándose antes de usar RBD.</div><p>Una función de RBD es que solo se puede montar como de solo lectura por múltiples consumidores simultáneamente.
Esto significa que puedes pre-poblar un volumen con tu conjunto de datos y luego servirlo en paralelo desde tantos pods como necesites. Desafortunadamente, los volúmenes RBD solo se pueden montar por un único consumidor en modo lectura-escritura. No se permiten escritores simultáneos.</p><p>Mira el <a href=https://github.com/kubernetes/examples/tree/master/volumes/rbd>ejemplo RBD</a> para más detalles.</p><h3 id=scaleio>scaleIO (deprecado)</h3><p>ScaleIO es una plataforma de almacenamiento basada en software que usa el hardware existente para crear clústeres de almacenamiento en red de bloques compartidos escalables. El complemento de volumen <code>scaleIO</code> permite a los pods desplegados acceder a volúmenes existentes ScaleIO. Para información acerca de aprovisionamiento dinámico de nuevos persistence volumen claims, mira <a href=/docs/concepts/storage/persistent-volumes/#scaleio>ScaleIO persistent volumes</a></p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes tener un Cluster ScaleIO ya configurado y ejecutándose con los volúmenes creados antes de poder usarlos.</div><p>El siguiente ejemplo es una configuración de un Pod con ScaleIO:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod-0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod-0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>vol-0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>vol-0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>scaleIO</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>gateway</span>:<span style=color:#bbb> </span>https://localhost:443/api<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>system</span>:<span style=color:#bbb> </span>scaleio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>protectionDomain</span>:<span style=color:#bbb> </span>sd0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>storagePool</span>:<span style=color:#bbb> </span>sp1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeName</span>:<span style=color:#bbb> </span>vol-0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>secretRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>sio-secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>xfs<span style=color:#bbb>
</span></span></span></code></pre></div><p>Para más detalles, mira los ejemplos de <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/scaleio>ScaleIO</a></p><h3 id=secret>secret</h3><p>Un volumen <code>seret</code> se utiliza para pasar información sensible, como contraseñas, a los Pods.
Puedes guardar secrets en la API de Kubernetes y montarlos como ficheros para usarlos con los pods sin acoplarlos con Kubernetes directamente. Los volúmenes <code>secret</code> son respaldados por tmpfs (un sistema de ficheros respaldado por la RAM) así que nunca se escriben en un almacenamiento no volátil.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes crear un secreto en la API de Kubernetes antes de poder usarlo.</div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Un contenedor que usa un Secret como un volumen <a href=#using-subpath><code>subPath</code></a> no recibirá las actualizaciones del Secret.</div><p>Para más detalles, mira <a href=/docs/concepts/configuration/secret/>Configurando Secrets</a>.</p><h3 id=storageos>storageOS</h3><p>Un volumen <code>storageos</code> permite montar un volumen existente <a href=https://www.storageos.com>StorageOS</a> en tu Pod.</p><p>StorageOS corre como un contenedor dentro de tu contenedor Kubernetes, haciendo accesible el almacenamiento local o adjunto desde cualquier node dentro del cluster de Kubernetes.
Los datos pueden ser replicados para protegerlos contra fallos del nodo. Este aprovisionamiento y compresión pueden mejorar el uso y reducir costes.</p><p>El contenedor StorageOs requiere Linux de 64 bits y no tiene dependencias adicionales.
Una licencia gratuita para desarrolladores está disponible.</p><div class="alert alert-warning caution callout" role=alert><strong>Precaución:</strong> Debes correr un contenedor StorageOS en cada nodo que quiera acceder a los volúmenes StorageOS o que contribuyan a la capacidad de almacenamiento al grupo.
Para instrucciones de instalación, consulta la <a href=https://docs.storageos.com>documentación StorageOS</a></div><p>El siguiente ejemplo es una configuración de un Pod con Storage OS:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>master<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-storageos-redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>master<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>kubernetes/redis:v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>MASTER<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>6379</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/redis-master-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storageos</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># El volumen `redis-vol01` debe existir dentro de StorageOS en el namespace `default`.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeName</span>:<span style=color:#bbb> </span>redis-vol01<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span></code></pre></div><p>Para más información sobre StorageOS, aprovisionamiento dinámico, y PersistentVolumeClaims, mira los
<a href=https://github.com/kubernetes/examples/blob/master/volumes/storageos>ejemplos de StorageOS examples</a>.</p><h3 id=vspherevolume>vsphereVolume</h3><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes configurar el proveedor en la nube de vSphere de Kubernetes.
Para configuración de proveedores en la nube, mira la <a href=https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/>guía de inicio de vSphere </a>.</div><p>Un volumen <code>vsphereVolume</code> se usa para montar un volumen VMDK vSphere en tu Pod. El contenido de un volumen es preservado cuando se desmonta. Tiene soporte para almacén de datos VMFS y VSAN.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes crear un volumen vSphere VMDK usando uno de los siguientes métodos antes de usarlo con un Pod.</div><h4 id=creating-vmdk-volume>Creando un volumen VMDK</h4><p>Elige uno de los siguientes métodos para crear un VMDK.</p><ul class="nav nav-tabs" id=tabs-volumes role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tabs-volumes-0 role=tab aria-controls=tabs-volumes-0 aria-selected=true>Create using vmkfstools</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-volumes-1 role=tab aria-controls=tabs-volumes-1>Create using vmware-vdiskmanager</a></li></ul><div class=tab-content id=tabs-volumes><div id=tabs-volumes-0 class="tab-pane show active" role=tabpanel aria-labelledby=tabs-volumes-0><p><p>Primero entra mediante ssh en ESX, luego usa uno de los siguientes comandos para crear un VMDK:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>vmkfstools -c 2G /vmfs/volumes/DatastoreName/volumes/myDisk.vmdk
</span></span></code></pre></div></div><div id=tabs-volumes-1 class=tab-pane role=tabpanel aria-labelledby=tabs-volumes-1><p><p>Usa el siguiente comando para crear un VMDK:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>vmware-vdiskmanager -c -t <span style=color:#666>0</span> -s 40GB -a lsilogic myDisk.vmdk
</span></span></code></pre></div></div></div><h4 id=vsphere-vmdk-configuration>Ejemplo de configuración vSphere VMDK</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-vmdk<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-vmdk<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Este volumen VMDK ya debe existir.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>vsphereVolume</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumePath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;[DatastoreName] volumes/myDisk&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span></code></pre></div><p>Para mayor información, mira el ejemplo de <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere>vSphere volume</a>.</p><h4 id=vsphere-csi-migration>Migración CSI vSphere</h4><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code></div>Cuando la función <code>CSIMigration</code> está habilitada, redirige todas las operaciones de complemento desde el complemento existente en el árbol al controlador <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> <code>csi.vsphere.vmware.com</code>. Para
usar esta función, el <a href=https://github.com/kubernetes-sigs/vsphere-csi-driver>controlador vSphere CSI</a> debe estar instalado en el clúster y las <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gates</a> <code>CSIMigration</code> y <code>CSIMigrationvSphere</code> deben estar habilitadas.</p><p>Esto también requiere que la versión de vSphere vCenter/ESXi sea la 7.0u1 y la versión mínima de HW version sea VM versión 15.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Los siguientes parámetros de Storageclass desde el complemento incorporado <code>vsphereVolume</code> no están soportados por el controlador vSphere CSI:</p><ul><li><code>diskformat</code></li><li><code>hostfailurestotolerate</code></li><li><code>forceprovisioning</code></li><li><code>cachereservation</code></li><li><code>diskstripes</code></li><li><code>objectspacereservation</code></li><li><code>iopslimit</code></li></ul><p>Los volúmenes existentes creados usando estos parámetros serán migrados al controlador vSphere CSI, pero los volúmenes nuevos creados por el controlador vSphere CSI no respetarán estos parámetros</p></div><h4 id=vsphere-csi-migration-complete>migración completa de vSphere CSI</h4><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code></div>Para apagar el complemento <code>vsphereVolume</code> y no cargarlo por el administrador del controlador y el kubelet, necesitas establecer eta función a <code>true</code>. Debes instalar un controlador de tipo <code>csi.vsphere.vmware.com</code> en todos los nodos worker.</p><h2 id=using-subpath>Uso de subPath</h2><p>Algunas veces es útil compartir un volumen para múltiples usos en un único Pod.
La propiedad <code>volumeMounts.subPath</code> especifica una sub-ruta dentro del volumen referenciado en lugar de su raíz.</p><p>El siguiente ejemplo muestra cómo configurar un Pod con la pila LAMP (Linux Apache MySQL PHP) usando un único volumen compartido. Esta configuración de ejemplo usando <code>subPath</code> no se recomienda para su uso en producción.</p><p>El código de la aplicación PHP y los recursos apuntan al directorio <code>html</code> del volumen y la base de datos MySQL se almacena en el directorio <code>mysql</code>. Por ejemplo:
The PHP application's code and assets map to the volume's <code>html</code> folder and
the MySQL database is stored in the volume's <code>mysql</code> folder. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-lamp-site<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>MYSQL_ROOT_PASSWORD<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;rootpasswd&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>site-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>subPath</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>php<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>php:7.0-apache<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/www/html<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>site-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>subPath</span>:<span style=color:#bbb> </span>html<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>site-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>my-lamp-site-data<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=using-subpath-expanded-environment>Uso de subPath con variables de entorno expandidas</h3><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code></div>Usa el campo <code>subPathExpr</code> para construir un nombre de directorio <code>subPath</code> desde variables de entorno de la API.
Las propiedades <code>subPath</code> y <code>subPathExpr</code> son mutuamente exclusivas.</p><p>En este ejemplo, un <code>Pod</code> usa <code>subPathExpr</code> para crear un directorio <code>pod1</code> dentro del volumen <code>hostPath</code> <code>var/logs/pods</code>.
El volumen <code>hostPath</code> toma el nombre del <code>Pod</code> desde la <code>downwardAPI</code>.
El directorio anfitrión <code>var/log/pods/pod1</code> se monta en <code>/logs</code> en el contenedor.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>container1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>POD_NAME<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>fieldRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>fieldPath</span>:<span style=color:#bbb> </span>metadata.name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>[<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:#b44>&#34;sh&#34;</span>,<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:#b44>&#34;while [ true ]; do echo &#39;Hello&#39;; sleep 10; done | tee -a /logs/hello.txt&#34;</span>,<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>workdir1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/logs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>subPathExpr</span>:<span style=color:#bbb> </span>$(POD_NAME)<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>workdir1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/log/pods<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=recursos>Recursos</h2><p>El medio de almacenamiento (como un disco o un SSD) de un volumen <code>emptyDir</code>
se determina por el medio del sistema de archivos que contiene el directorio raíz
del kubelet (típicamente <code>/var/lib/kubelet</code>).
No hay límite de cuánto espacio puede consumir un volumen <code>emptydir</code> o <code>hostPath</code>,
y no hay aislamiento entre contenedores o entre pods.</p><p>Para aprender más sobre requerir espacio usando una espacificación de recurso, mira <a href=/docs/concepts/configuration/manage-resources-containers/>cómo administrar recursos</a>.</p><h2 id=complementos-de-volúmenes-fuera-del-árbol>Complementos de volúmenes fuera del árbol</h2><p>Los complementos de volumen fuera del árbol incluyen <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label='Container Storage Interface'>Container Storage Interface</a> (CSI) y FlexVolume. Estos complementos permiten a los proveedores de almacenamiento crear complementos de almacenamiento personalizados sin añadir su código fuente al repositorio de Kubernetes.</p><p>Anteriormente, todos los complementos de volumen estaban "en el árbol". Los complementos "en el árbol" se construían, enlazaban, compilaban y enviaban con los binarios del núcleo de Kubernetes. Esto significaba que agregar un nuevo sistema de almacenamiento a Kubernetes (un complemento de volumen) requería verificar el código en el repositorio de código del núcleo de Kubernetes.</p><p>Tanto CSI como FlexVolume permiten que se desarrollen complementos de volúmenes independientemente del código base de Kubernetes, y se desplieguen (instalen) en los clústeres de Kubernetes como extensiones.</p><p>Para los proveedores de almacenamiento que buscan crear un complemento de volumen fuera del árbol, por favor refiéranse a <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md>Preguntas frecuentees de complementos de volumen</a>.</p><h3 id=csi>csi</h3><p>La <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>interfaz de almacenamiento del contenedor</a> (CSI) define una interfaz estándar para sistemas de orquestación del contenedor (como Kubernetes) para exponer sistemas de almacenamiento arbitrario a sus cargas de trabajo del contenedor.</p><p>Por favor, lee la <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/container-storage-interface.md>propuesta de diseño CSI</a> para más información.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El soporte para las especificaciones de las versiones CSI 0.2 y 0.3 están deprecadas en Kubernetes v1.13 y serán removidos en una versión futura.</div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los controladores CSI podrían no ser compatibles con todas las versiones de Kubernetes.
Por favor, revisa la documentación específica del controlador CSI para los pasos de despliegue soportados para cada versión de Kubernetes y una matriz de compatibilidad.</div><p>Una vez que se despliega un controlador de volumen CSI compatible, los usuarios pueden usar el tipo de volumen <code>csi</code> para adjuntar o montar los volúmenes expuestos por el controlador CSI.</p><p>Un volumen <code>csi</code> puede ser usado en un Pod en tres maneras distintas:</p><ul><li>a través de una referencia a <a href=#persistentvolumeclaim>PersistentVolumeClaim</a></li><li>con un <a href=/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes>volumen general efímero</a>
(característica alpha)</li><li>con un <a href=/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes>volumen efímero CSI</a> si el controlador permite esta (característica beta)</li></ul><p>Los siguientes campos están disponibles para que los administradores de almacenamiento configuren el volumen persistente CSI</p><ul><li><code>driver</code>: Un valor de cadena de caracteres que especifica el nombre del controlador de volumen a usar. Este valor debe corresponder al valor de respuesta en el <code>GetPluginInfoResponse</code> por el controlador CSI tal como se define en la <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo>especificación CSI</a>. Es usado por Kubernetes para identificar cuál controlador llamar, y por los componentes del controlador CSI para identificar cuáles objetos PV pertenecen al controlador CSI.</li><li><code>volumenHandle</code>: Un valor de cadena de caracteres que identifica el volumen unívocamente. Este valor debe corresponder al valor en el campo <code>volumen.id</code> del <code>CreateVolumeResponse</code> por el controlador CSI como se define en la <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume>especificación CSI spec</a>. El valor es pasado como <code>volume.id</code> en todas las llamadas al controlador de volumen CSI cuando referencias el volumen.</li><li><code>readOnly</code>: Un valor booleano opcional que indica si el volumen es para ser "ControllerPublished" (adjuntado) como solo lectura. Por defecto es falso. Este valor es pasado el controlador CSI en el campo <code>readOnly</code> en el <code>ControllerPublishVolumeRequest</code>.</li><li><code>fsType</code>: Si el <code>VolumeMode</code>del PV es <code>Filesystem</code> entonces este campo se puede usar para especificar el sistema de archivos que debería usarse para montar el volumen. Si el volumen no ha sido formateado y soportado, este valor se utilizará para formatear el volumen. Este valor se para al controlador CSI con el campo <code>VolumeCapability</code> de <code>ControllerPublishVolumeRequest</code>, <code>NodeStageVolumeRequest</code>, y <code>NodePublishVolumeRequest</code>.</li><li><code>volumeAttributes</code>: Un mapa de cadena de caracteres que especifica las propiedades estáticas de un volumen. Este mapa debe corresponder al map devuelto por el campo <code>volume.attributes</code> del <code>CreateVolumeResponse</code> por el controlador CSI tal como se define en la <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume>especificación CSI spec</a>. El mapa es pasado al controlador CSI con el campo <code>volume.context</code> en el <code>ControllerPublishVolumeRequest</code>, <code>NodeStageVolumeRequest</code>, y <code>NodePublishVolumeRequest</code>.</li><li><code>controllerPublishSecretRef</code>: Una referencia al objeto secret que contiene información sensible para pasar al controlador CSI para completar las llamadas CSI <code>ControllerPublishVolume</code> y <code>ControllerUnpublishVolume</code>. Este campo es opcional, y puede estar vacío si no se requiere un secret. Si el Secret contiene más de un secret, se pasan todos los secrets.</li><li><code>nodeStageSecretRef</code>: Una referencia al objeto secret que contiene información sensible a pasar al controlador CSI para completar la llamada CSI <code>NodeStageVolume</code>. Este ampo es opcional, y puede estar vacío si no se requiere un secret. Si el Secret contiene más de un secret, todos los secrets son pasados.</li><li><code>nodePublishSecretRef</code>: Una referencia al objeto que contiene información sensible a pasar al controlador CSI para completar la llamada CSI <code>NodePublishVolume</code>. Este ampo es opcional, y puede estar vacío si no se requiere un secret. Si el Secret contiene más de un secret, todos los secrets son pasados.</li></ul><h4 id=soporte-de-volumen-csi-de-fila-de-bloques>Soporte de volumen CSI de fila de bloques</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code></div><p>Los proveedores con controladores CSI externos pueden implementar soporte de volumen de bloques sin procesar en cargas de trabajo de Kubernetes.</p><p>Puedes configurar tu
You can set up your <a href=/docs/concepts/storage/persistent-volumes/#raw-block-volume-support>PersistentVolume/PersistentVolumeClaim with raw block volume support</a> como de costumbre, sin ningún cambio específico CSI.</p><h4 id=volúmenes-efímeros-csi>Volúmenes efímeros CSI</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code></div><p>Puedes configurar directamente volúmenes CSI dentro de la especificación del Pod.
Los volúmenes especificados de esta manera son efímeros y no se persisten entre reinicios del Pod. Mira <a href=/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes>Volúmenes efímeros</a> para más información.</p><p>Para más información de cómo desarrollador un controlador CSI, mira la <a href=https://kubernetes-csi.github.io/docs/>documentación kubernetes-csi</a></p><h4 id=migrando-a-controladores-csi-desde-complementos-en-el-árbol>Migrando a controladores CSI desde complementos en el árbol.</h4><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [beta]</code></div>La función <code>CSIMigration</code>, cuando está habilitada, dirige todas las operaciones hacia complementos existentes en el árbol a complementos CSI correspondientes (que se espera que estén instalados y configurados). Como resultado, los operadores no tienen que hacer ningún cambio de configuración a las clases de almacenamiento, PersistentVolumes o PersistentVolumeClaims (refiriéndose a complementos en el árbol) cuando haces la transición a un controlador CSI que un reemplaza complemento en el árbol.</p><p>Las operaciones y funciones que están soportadas incluye:
aprovisionamiento/borrado, adjuntar/separar, montar/desmontar y redimensionar volúmenes.</p><p>In-tree plugins that support <code>CSIMigration</code> and have a corresponding CSI driver implemented
are listed in <a href=#volume-types>Types of Volumes</a>.</p><h3 id=flexvolume>flexVolume</h3><p>FlexVolume is an out-of-tree plugin interface that has existed in Kubernetes
since version 1.2 (before CSI). It uses an exec-based model to interface with
drivers. The FlexVolume driver binaries must be installed in a pre-defined volume
plugin path on each node and in some cases the control plane nodes as well.</p><p>Pods interact with FlexVolume drivers through the <code>flexvolume</code> in-tree volume plugin.
For more details, see the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md>FlexVolume</a> examples.</p><h2 id=propagación-del-montaje>Propagación del montaje</h2><p>La propagación del montaje permite compartir volúmenes montados por un contenedor para otros contenedores en el mismo Pod, o aun para otros pods en el mismo nodo.</p><p>La propagación del montaje de un volumen es controlada por el campo <code>mountPropagation</code> en <code>Container.volumeMounts</code>. Sus valores son:</p><ul><li><p><code>None</code> - Este montaje de volumen no recibirá ningún montaje posterior que el host haya montado en este volumen o en cualquiera de sus subdirectorios. De manera similar, los montajes creados por el contenedor no serán visibles en el host. Este es el modo por defecto.</p><p>Este modo es igual la propagación del montaje <code>private</code> tal como se describe en la <a href=https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt>documentación Linux kernel documentation</a></p></li><li><p><code>HostToContainer</code> - Este montaje de volumen recibirá todos los montajes subsecuentes que son montados a este volumen o cualquiera de sus subdirectorios.</p><p>En otras palabras, si el host monta algo dentro del montaje del volumen, el contenedor lo verá montado allí.</p><p>De manera similar, si cualquier Pod con propagación de montaje <code>Bidirectional</code> al mismo volumen monta algo allí, el contenedor con propagación de montaje<code> HostToContainer</code> lo verá.</p><p>Este modo es igual a la propagación de montaje <code>rslave</code> tal como se describe en la <a href=https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt>documentación del kernel de Linux</a></p></li><li><p><code>Bidirectional</code>- Este montaje de volumen se comporta de la misma manera de el montaje<code>HostToContainer</code>. Adicionalmente, todos los montajes de volúmenes creados por el contenedor serán propagados de vuelta al host y a todos los contenedores de todos los pods que usan el mismo volumen.</p><p>Un uso típico para este modo es un Pod con un FlexVolumen o un controlador CSI o un Pod que necesita montar al en el host usando un volumen <code>hostPath</code>.</p><p>Este modo es igual a la propagación de montaje <code>rshared</code> tal como se describe en la <a href=https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt>documentación del kernel de Linux documentación</a></p><div class="alert alert-danger warning callout" role=alert><strong>Advertencia:</strong> La propagación por montaje <code>Bidirectional</code> puede ser peligrosa. Puede dañar el sistema operativo host y, por lo tanto, solo se permite en contenedores privilegiados. Se recomienda encarecidamente estar familiarizado con el comportamiento del kernel de Linux.
Además, cualquier montaje de volumen creado por contenedores en pods debe destruirse (desmontado) por los contenedores en el momento de la terminación.</div></li></ul><h3 id=configuration>Configuration</h3><p>Before mount propagation can work properly on some deployments (CoreOS,
RedHat/Centos, Ubuntu) mount share must be configured correctly in
Docker as shown below.</p><p>Edit your Docker's <code>systemd</code> service file. Set <code>MountFlags</code> as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>MountFlags</span><span style=color:#666>=</span>shared
</span></span></code></pre></div><p>Or, remove <code>MountFlags=slave</code> if present. Then restart the Docker daemon:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo systemctl daemon-reload
</span></span><span style=display:flex><span>sudo systemctl restart docker
</span></span></code></pre></div><h2 id=siguientes-pasos>Siguientes pasos</h2><p>Sigue un ejemplo de <a href=/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/>desplegar WordPrss y MySQL con volúmenes persistentes</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c262af210c6828dec445d2f55a1d877a>6.2 - Snapshots de Volúmenes</h1><p>En Kubernetes, un <em>VolumeSnapshot</em> representa un Snapshot de un volumen en un sistema de almacenamiento. Este documento asume que está familiarizado con <a href=/docs/concepts/storage/persistent-volumes/>volúmenes persistentes</a> de Kubernetes.</p><h2 id=introducción>Introducción</h2><p>Al igual que los recursos de API <code>PersistentVolume</code> y <code>PersistentVolumeClaim</code> se utilizan para aprovisionar volúmenes para usuarios y administradores, <code>VolumeSnapshotContent</code> y <code>VolumeSnapshot</code> se proporcionan para crear Snapshots de volumen para usuarios y administradores.</p><p>Un <code>VolumeSnapshotContent</code> es un Snapshot tomado de un volumen en el clúster que ha sido aprovisionado por un administrador. Es un recurso en el clúster al igual que un PersistentVolume es un recurso de clúster.</p><p>Un <code>VolumeSnapshot</code> es una solicitud de Snapshot de un volumen por parte del usuario. Es similar a un PersistentVolumeClaim.</p><p><code>VolumeSnapshotClass</code> permite especificar diferentes atributos que pertenecen a un <code>VolumeSnapshot</code>. Estos atributos pueden diferir entre Snapshots tomados del mismo volumen en el sistema de almacenamiento y, por lo tanto, no se pueden expresar mediante el mismo <code>StorageClass</code> de un <code>PersistentVolumeClaim</code>.</p><p>Los Snapshots de volumen brindan a los usuarios de Kubernetes una forma estandarizada de copiar el contenido de un volumen en un momento determinado, sin crear uno completamente nuevo. Esta funcionalidad permite, por ejemplo, a los administradores de bases de datos realizar copias de seguridad de las bases de datos antes de realizar una edición o eliminar modificaciones.</p><p>Cuando utilicen esta función los usuarios deben tener en cuenta lo siguiente:</p><ul><li>Los objetos de API <code>VolumeSnapshot</code>, <code>VolumeSnapshotContent</code>, y <code>VolumeSnapshotClass</code> son <a class=glossary-tooltip title='Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.' data-toggle=tooltip data-placement=top href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ target=_blank aria-label=CRDs>CRDs</a>, y no forman parte de la API principal.</li><li>La compatibilidad con <code>VolumeSnapshot</code> solo está disponible para controladores CSI.</li><li>Como parte del proceso de implementación de <code>VolumeSnapshot</code>, el equipo de Kubernetes proporciona un controlador de Snapshot para implementar en el plano de control y un sidecar auxiliar llamado csi-snapshotter para implementar junto con el controlador CSI. El controlador de Snapshot observa los objetos <code>VolumeSnapshot</code> y <code>VolumeSnapshotContent</code> y es responsable de la creación y eliminación del objeto <code>VolumeSnapshotContent</code>. El sidecar csi-snapshotter observa los objetos <code>VolumeSnapshotContent</code> y activa las operaciones <code>CreateSnapshot</code> y <code>DeleteSnapshot</code> en un punto final CSI.</li><li>También hay un servidor webhook de validación que proporciona una validación más estricta en los objetos Snapshot. Esto debe ser instalado por las distribuciones de Kubernetes junto con el controlador de Snapshots y los CRDs, no los controladores CSI. Debe instalarse en todos los clústeres de Kubernetes que tengan habilitada la función de Snapshot.</li><li>Los controladores CSI pueden haber implementado o no la funcionalidad de Snapshot de volumen. Los controladores CSI que han proporcionado soporte para Snapshot de volumen probablemente usarán csi-snapshotter. Consulte <a href=https://kubernetes-csi.github.io/docs/>CSI Driver documentation</a> para obtener más detalles.</li><li>Los CRDs y las instalaciones del controlador de Snapshot son responsabilidad de la distribución de Kubernetes.</li></ul><h2 id=ciclo-de-vida-de-un-snapshot-de-volumen-y-el-contenido-de-un-snapshot-de-volumen>Ciclo de vida de un Snapshot de volumen y el contenido de un Snapshot de volumen.</h2><p><code>VolumeSnapshotContents</code> son recursos en el clúster. <code>VolumeSnapshots</code> son solicitudes de esos recursos. La interacción entre <code>VolumeSnapshotContents</code> y <code>VolumeSnapshots</code> sigue este ciclo de vida:</p><h3 id=snapshot-del-volumen-de-aprovisionamiento>Snapshot del volumen de aprovisionamiento</h3><p>Hay dos formas de aprovisionar los Snapshots: aprovisionadas previamente o aprovisionadas dinámicamente.</p><h4 id=static>Pre-aprovisionado</h4><p>Un administrador de clúster crea una serie de <code>VolumeSnapshotContents</code>. Llevan los detalles del Snapshot del volumen real en el sistema de almacenamiento que está disponible para que lo utilicen los usuarios del clúster. Existen en la API de Kubernetes y están disponibles para su consumo.</p><h4 id=dinámica>Dinámica</h4><p>En lugar de utilizar un Snapshot preexistente, puede solicitar que se tome una Snapshot dinámicamente de un PersistentVolumeClaim. El <a href=/docs/concepts/storage/volume-snapshot-classes/>VolumeSnapshotClass</a> especifica los parámetros específicos del proveedor de almacenamiento para usar al tomar una Snapshot.</p><h3 id=vinculante>Vinculante</h3><p>El controlador de Snapshots maneja el enlace de un objeto <code>VolumeSnapshot</code> con un objeto <code>VolumeSnapshotContent</code> apropiado, tanto en escenarios de aprovisionamiento previo como de aprovisionamiento dinámico. El enlace es un mapeo uno a uno.</p><p>En el caso de un enlace aprovisionado previamente, el VolumeSnapshot permanecerá sin enlazar hasta que se cree el objeto VolumeSnapshotContent solicitado.</p><h3 id=persistent-volume-claim-como-snapshot-source-protection>Persistent Volume Claim como Snapshot Source Protection</h3><p>El propósito de esta protección es garantizar que los objetos de la API
<a class=glossary-tooltip title='Reserva el recurso de almacenamiento definido en un PersistentVolume para poderlo montar como un volúmen en un contenedor.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PersistentVolumeClaim>PersistentVolumeClaim</a>
en uso, no se eliminen del sistema mientras se toma un Snapshot (ya que esto puede resultar en la pérdida de datos).</p><p>Mientras se toma un Snapshot de un PersistentVolumeClaim, ese PersistentVolumeClaim está en uso. Si elimina un objeto de la API PersistentVolumeClaim en uso activo como fuente de Snapshot, el objeto PersistentVolumeClaim no se elimina de inmediato. En cambio, la eliminación del objeto PersistentVolumeClaim se pospone hasta que el Snapshot esté readyToUse o se cancele.</p><h3 id=borrar>Borrar</h3><p>La eliminación se activa al eliminar el objeto <code>VolumeSnapshot</code>, y se seguirá la <code>DeletionPolicy</code>. Sí <code>DeletionPolicy</code> es <code>Delete</code>, entonces el Snapshot de almacenamiento subyacente se eliminará junto con el objeto <code>VolumeSnapshotContent</code>. Sí <code>DeletionPolicy</code> es <code>Retain</code>, tanto el Snapshot subyacente como el <code>VolumeSnapshotContent</code> permanecen.</p><h2 id=volumesnapshots>VolumeSnapshots</h2><p>Cada VolumeSnapshot contiene una especificación y un estado.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotClassName</span>:<span style=color:#bbb> </span>csi-hostpath-snapclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>persistentVolumeClaimName</span>:<span style=color:#bbb> </span>pvc-test<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>persistentVolumeClaimName</code> es el nombre de la fuente de datos PersistentVolumeClaim para el Snapshot. Este campo es obligatorio para aprovisionar dinámicamente un Snapshot.</p><p>Un Snapshot de volumen puede solicitar una clase particular especificando el nombre de un <a href=/docs/concepts/storage/volume-snapshot-classes/>VolumeSnapshotClass</a>
utilizando el atributo <code>volumeSnapshotClassName</code>. Si no se establece nada, se usa la clase predeterminada si está disponible.</p><p>Para los Snapshots aprovisionadas previamente, debe especificar un <code>volumeSnapshotContentName</code> como el origen del Snapshot, como se muestra en el siguiente ejemplo. El campo de origen <code>volumeSnapshotContentName</code> es obligatorio para los Snapshots aprovisionados previamente.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapshot<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeSnapshotContentName</span>:<span style=color:#bbb> </span>test-content<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=contenido-del-snapshot-de-volumen>Contenido del Snapshot de volumen</h2><p>Cada VolumeSnapshotContent contiene una especificación y un estado. En el aprovisionamiento dinámico, el controlador común de Snapshots crea objetos <code>VolumeSnapshotContent</code>. Aquí hay un ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotContent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>snapcontent-72d9a349-aacd-42d2-a240-d775650d2455<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>hostpath.csi.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeHandle</span>:<span style=color:#bbb> </span>ee0cfb94-f8d4-11e9-b2d8-0242ac110002<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotClassName</span>:<span style=color:#bbb> </span>csi-hostpath-snapclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>uid</span>:<span style=color:#bbb> </span>72d9a349-aacd-42d2-a240-d775650d2455<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>volumeHandle</code> es el identificador único del volumen creado en el backend de almacenamiento y devuelto por el controlador CSI durante la creación del volumen. Este campo es obligatorio para aprovisionar dinámicamente un Snapshot. Especifica el origen del volumen del Snapshot.</p><p>Para los Snapshots aprovisionados previamente, usted (como administrador del clúster) es responsable de crear el objeto <code>VolumeSnapshotContent</code> de la siguiente manera.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotContent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-content-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>hostpath.csi.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>snapshotHandle</span>:<span style=color:#bbb> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>snapshotHandle</code> es el identificador único del Snapshot de volumen creado en el backend de almacenamiento. Este campo es obligatorio para las Snapshots aprovisionadas previamente. Especifica el ID del Snapshot CSI en el sistema de almacenamiento que representa el <code>VolumeSnapshotContent</code>.</p><h2 id=aprovisionamiento-de-volúmenes-a-partir-de-snapshots>Aprovisionamiento de Volúmenes a partir de Snapshots</h2><p>Puede aprovisionar un nuevo volumen, rellenado previamente con datos de una Snapshot, mediante el campo <em>dataSource</em> en el objeto <code>PersistentVolumeClaim</code>.</p><p>Para obtener más detalles, consulte
<a href=/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support>Volume Snapshot and Restore Volume from Snapshot</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-707ca81a34eb1ca202f34692e9917d1e>6.3 - Clonación de volumen CSI</h1><p>Este documento describe el concepto para clonar volúmenes CSI existentes en Kubernetes. Se sugiere estar familiarizado con <a href=/docs/concepts/storage/volumes>Volúmenes</a>.</p><h2 id=introducción>Introducción</h2><p>La función de clonación de volumen <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> agrega soporte para especificar <a class=glossary-tooltip title='Reserva el recurso de almacenamiento definido en un PersistentVolume para poderlo montar como un volúmen en un contenedor.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PVC>PVC</a>s existentes en el campo <code>dataSource</code> para indicar que un usuario desea clonar un <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volume>Volume</a>.</p><p>Un Clon se define como un duplicado de un volumen de Kubernetes existente que se puede consumir como lo sería cualquier volumen estándar. La única diferencia es que al aprovisionar, en lugar de crear un "nuevo" Volumen vacío, el dispositivo de backend crea un duplicado exacto del Volumen especificado.</p><p>La implementación de la clonación, desde la perspectiva de la API de Kubernetes, agrega la capacidad de especificar un PVC existente como dataSource durante la creación de un nuevo PVC. El PVC de origen debe estar vinculado y disponible (no en uso).</p><p>Los usuarios deben tener en cuenta lo siguiente cuando utilicen esta función:</p><ul><li>El soporte de clonación (<code>VolumePVCDataSource</code>) sólo está disponible para controladores CSI.</li><li>El soporte de clonación sólo está disponible para aprovisionadores dinámicos.</li><li>Los controladores CSI pueden haber implementado o no la funcionalidad de clonación de volúmenes.</li><li>Sólo puede clonar un PVC cuando existe en el mismo Namespace que el PVC de destino (el origen y el destino deben estar en el mismo Namespace).</li><li>La clonación sólo se admite dentro de la misma Clase de Almacenamiento.<ul><li>El volumen de destino debe ser de la misma clase de almacenamiento que el origen</li><li>Se puede utilizar la clase de almacenamiento predeterminada y se puede omitir storageClassName en la especificación</li></ul></li><li>La clonación sólo se puede realizar entre dos volúmenes que usan la misma configuración de VolumeMode (si solicita un volumen en modo de bloqueo, la fuente DEBE también ser en modo de bloqueo)</li></ul><h2 id=aprovisionamiento>Aprovisionamiento</h2><p>Los clones se aprovisionan como cualquier otro PVC con la excepción de agregar un origen de datos que hace referencia a un PVC existente en el mismo Namespace.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>clone-of-pvc-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>myns<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>cloning<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>5Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dataSource</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pvc-1<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debe especificar un valor de capacidad para <code>spec.resources.requests.storage</code> y el valor que especifique debe ser igual o mayor que la capacidad del volumen de origen.</div><p>El resultado es un nuevo PVC con el nombre <code>clone-of-pvc-1</code> que tiene exactamente el mismo contenido que la fuente especificada <code>pvc-1</code>.</p><h2 id=uso>Uso</h2><p>Una vez disponible el nuevo PVC, el PVC clonado se consume igual que el resto de PVC. También se espera en este punto que el PVC recién creado sea un objeto independiente. Se puede consumir, clonar, tomar snapshots, o eliminar de forma independiente y sin tener en cuenta sus datos originales. Esto también implica que la fuente no está vinculada de ninguna manera al clon recién creado, también puede modificarse o eliminarse sin afectar al clon recién creado.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4d00116c86dade62bdd5be7dc2afa1ca>6.4 - Volume Snapshot Classes</h1><p>Este documento describe el concepto de VolumeSnapshotClass en Kubernetes. Se sugiere estar familiarizado
con <a href=/docs/concepts/storage/volume-snapshots/>Volume Snapshots</a> y
<a href=/docs/concepts/storage/storage-classes>Storage Classes</a>.</p><h2 id=introducción>Introducción</h2><p>Al igual que StorageClass proporciona a los administradores una forma de describir las “clases”
de almacenamiento que ofrecen al aprovisionar un volumen, VolumeSnapshotClass proporciona una
forma de describir las “clases” de almacenamiento al aprovisionar un Snapshot de volumen.</p><h2 id=el-recurso-volumesnapshotclass>El Recurso VolumeSnapshotClass</h2><p>Cada VolumeSnapshotClass contiene los campos <code>driver</code>, <code>deletionPolicy</code>, y <code>parameters</code>,
que se utilizan cuando un VolumeSnapshot que pertenece a la clase, necesita aprovisionarse dinámicamente.</p><p>El nombre de un objeto VolumeSnapshotClass es significativo y es la forma en que los usuarios pueden solicitar una clase en particular. Los administradores establecen el nombre y parámetros de una clase cuando crean por primera vez objetos VolumeSnapshotClass; una vez creados los objetos no pueden ser actualizados.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-hostpath-snapclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>hostpath.csi.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span></code></pre></div><p>Los administradores pueden especificar un VolumeSnapshotClass predeterminado para VolumeSnapshots que no solicitan ninguna clase en particular. Para definir la clase predeterminada agregue la anotación: <code>snapshot.storage.kubernetes.io/is-default-class: "true"</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-hostpath-snapclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>snapshot.storage.kubernetes.io/is-default-class</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>hostpath.csi.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=driver>Driver</h3><p>Las clases de Snapshot de volumen tienen un controlador que determina que complemento de volumen CSI se utiliza para aprovisionar VolumeSnapshots. Este campo debe especificarse.</p><h3 id=deletionpolicy>DeletionPolicy</h3><p>Las clases de Snapshot de volumen tienen un deletionPolicy. Permite configurar lo que sucede con un VolumeSnapshotContent cuando se va a eliminar el objeto VolumeSnapshot al que está vinculado. La deletionPolicy de una clase de Snapshot de volumen puede <code>Retain</code> o <code>Delete</code>. Este campo debe ser especificado.</p><p>Si la deletionPolicy es <code>Delete</code>, el Snapshot de almacenamiento subyacente se eliminará junto con el objeto VolumeSnapshotContent. Si deletionPolicy es <code>Retain</code>, tanto el Snapshot subyacente como VolumeSnapshotContent permanecerán.</p><h3 id=parameters>Parameters</h3><p>Las clases de Snapshot de volumen tienen parámetros que describen los Snapshots de volumen que pertenecen a la clase de Snapshot de volumen. Se pueden aceptar diferentes parámetros dependiendo del <code>driver</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-018f0a7fc6e2f6d16da37702fc39b4f3>6.5 - Aprovisionamiento Dinámico de volumen</h1><p>El aprovisionamiento dinámico de volúmenes permite crear volúmenes de almacenamiento bajo demanda. Sin el aprovisionamiento dinámico, los administradores de clústeres tienen que realizar llamadas manualmente a su proveedor de almacenamiento o nube para crear nuevos volúmenes de almacenamiento y luego crear <a href=/docs/concepts/storage/persistent-volumes/>objetos de <code>PersistentVolume</code></a>
para representarlos en Kubernetes. La función de aprovisionamiento dinámico elimina la necesidad de que los administradores del clúster aprovisionen previamente el almacenamiento. En cambio, el aprovisionamiento ocurre automáticamente cuando los usuarios lo solicitan.</p><h2 id=antecedentes>Antecedentes</h2><p>La implementación del aprovisionamiento dinámico de volúmenes se basa en el objeto API <code>StorageClass</code>
del grupo API <code>storage.k8s.io</code>. Un administrador de clúster puede definir tantos objetos
<code>StorageClass</code> como sea necesario, cada uno especificando un <em>volume plugin</em> (aka
<em>provisioner</em>) que aprovisiona un volumen y el conjunto de parámetros para pasar a ese aprovisionador. Un administrador de clúster puede definir y exponer varios tipos de almacenamiento (del mismo o de diferentes sistemas de almacenamiento) dentro de un clúster, cada uno con un conjunto personalizado de parámetros. Este diseño también garantiza que los usuarios finales no tengan que preocuparse por la complejidad y los matices de cómo se aprovisiona el almacenamiento, pero que aún tengan la capacidad de seleccionar entre múltiples opciones de almacenamiento.</p><p>Puede encontrar más información sobre las clases de almacenamiento
<a href=/docs/concepts/storage/storage-classes/>aqui</a>.</p><h2 id=habilitación-del-aprovisionamiento-dinámico>Habilitación del aprovisionamiento dinámico</h2><p>Para habilitar el aprovisionamiento dinámico, un administrador de clúster debe crear previamente uno o más objetos StorageClass para los usuarios. Los objetos StorageClass definen qué aprovisionador se debe usar y qué parámetros se deben pasar a ese aprovisionador cuando se invoca el aprovisionamiento dinámico.
El nombre de un objeto StorageClass debe ser un
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nombre de subdominio de DNS</a> válido.</p><p>El siguiente manifiesto crea una clase de almacenamiento llamada "slow" que aprovisiona discos persistentes estándar similares a discos.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/gce-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>pd-standard<span style=color:#bbb>
</span></span></span></code></pre></div><p>El siguiente manifiesto crea una clase de almacenamiento llamada "fast" que aprovisiona discos persistentes similares a SSD.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fast<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/gce-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>pd-ssd<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=usar-aprovisionamiento-dinámico>Usar Aprovisionamiento Dinámico</h2><p>Los usuarios solicitan almacenamiento aprovisionado dinámicamente al incluir una clase de almacenamiento en su <code>PersistentVolumeClaim</code>. Antes de Kubernetes v1.6, esto se hacía a través del la anotación
<code>volume.beta.kubernetes.io/storage-class</code>. Sin embargo, esta anotación está obsoleta desde v1.9. Los usuarios ahora pueden y deben usar el campo
<code>storageClassName</code> del objeto <code>PersistentVolumeClaim</code>. El valor de este campo debe coincidir con el nombre de un <code>StorageClass</code> configurada por el administrador
(ver <a href=#habilitaci%C3%B3n-del-aprovisionamiento-din%C3%A1mico>documentación</a>).</p><p>Para seleccionar la clase de almacenamiento llamada "fast", por ejemplo, un usuario crearía el siguiente PersistentVolumeClaim:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>claim1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>fast<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>30Gi<span style=color:#bbb>
</span></span></span></code></pre></div><p>Esta afirmación da como resultado que se aprovisione automáticamente un disco persistente similar a SSD. Cuando se elimina la petición, se destruye el volumen.</p><h2 id=comportamiento-predeterminado>Comportamiento Predeterminado</h2><p>El aprovisionamiento dinámico se puede habilitar en un clúster de modo que todas las peticiones se aprovisionen dinámicamente si no se especifica una clase de almacenamiento. Un administrador de clúster puede habilitar este comportamiento al:</p><ul><li>Marcar un objeto <code>StorageClass</code> como <em>default</em>;</li><li>Asegúrese de que el <a href=/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass>controlador de admisión <code>DefaultStorageClass</code></a> esté habilitado en el servidor de API.</li></ul><p>Un administrador puede marcar un <code>StorageClass</code> específico como predeterminada agregando la anotación
<code>storageclass.kubernetes.io/is-default-class</code>.
Cuando existe un <code>StorageClass</code> predeterminado en un clúster y un usuario crea un
<code>PersistentVolumeClaim</code> con <code>storageClassName</code> sin especificar, el controlador de admisión
<code>DefaultStorageClass</code> agrega automáticamente el campo
<code>storageClassName</code> que apunta a la clase de almacenamiento predeterminada.</p><p>Tenga en cuenta que puede haber como máximo una clase de almacenamiento <em>default</em>, o un <code>PersistentVolumeClaim</code> sin <code>storageClassName</code> especificado explícitamente.</p><h2 id=conocimiento-de-la-topología>Conocimiento de la Topología</h2><p>En los clústeres <a href=/docs/setup/multiple-zones>Multi-Zone</a>, los Pods se pueden distribuir en zonas de una región. Los backends de almacenamiento de zona única deben aprovisionarse en las zonas donde se programan los Pods. Esto se puede lograr configurando el <a href=/docs/concepts/storage/storage-classes/#volume-binding-mode>Volume Binding
Mode</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-00cd24f4570b7acaac75c2551c948bc7>6.6 - Capacidad de Almacenamiento</h1><p>La capacidad de almacenamiento es limitada y puede variar según el nodo en el que un Pod se ejecuta: es posible que no todos los nodos puedan acceder al almacenamiento conectado a la red o que, para empezar, el almacenamiento sea local en un nodo.</p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code></div><p>Esta página describe cómo Kubernetes realiza un seguimiento de la capacidad de almacenamiento y cómo el planificador usa esa información para programar Pods en nodos que tienen acceso a suficiente capacidad de almacenamiento para los volúmenes restantes que faltan. Sin el seguimiento de la capacidad de almacenamiento, el planificador puede elegir un nodo que no tenga suficiente capacidad para aprovisionar un volumen y se necesitarán varios reintentos de planificación.</p><p>El seguimiento de la capacidad de almacenamiento es compatible con los controladores de la <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label='Interfaz de Almacenamiento de Contenedores'>Interfaz de Almacenamiento de Contenedores</a> (CSI) y
<a href=#enabling-storage-capacity-tracking>necesita estar habilitado</a> al instalar un controlador CSI.</p><h2 id=api>API</h2><p>Hay dos extensiones de API para esta función:</p><ul><li>Los objetos CSIStorageCapacity:
son producidos por un controlador CSI en el Namespace donde está instalado el controlador. Cada objeto contiene información de capacidad para una clase de almacenamiento y define qué nodos tienen acceso a ese almacenamiento.</li><li><a href=/docs/reference/generated/kubernetes-api/v1.25/#csidriverspec-v1-storage-k8s-io>El campo <code>CSIDriverSpec.StorageCapacity</code></a>:
cuando se establece en <code>true</code>, el <a href=/docs/concepts/scheduling-eviction/kube-scheduler/>Planificador de Kubernetes</a> considerará la capacidad de almacenamiento para los volúmenes que usan el controlador CSI.</li></ul><h2 id=planificación>Planificación</h2><p>El planificador de Kubernetes utiliza la información sobre la capacidad de almacenamiento si:</p><ul><li>la Feature gate de <code>CSIStorageCapacity</code> es <code>true</code>,</li><li>un Pod usa un volumen que aún no se ha creado,</li><li>ese volumen usa un <a class=glossary-tooltip title='A StorageClass provides a way for administrators to describe different available storage types.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/storage-classes target=_blank aria-label=StorageClass>StorageClass</a> que hace referencia a un controlador CSI y usa el [modo de enlace de volumen] (/docs/concepts/storage/storage-classes/#volume-binding-mode)<code>WaitForFirstConsumer</code>,
y</li><li>el objeto <code>CSIDriver</code> para el controlador tiene <code>StorageCapacity</code> establecido en <code>true</code>.</li></ul><p>En ese caso, el planificador sólo considera los nodos para el Pod que tienen suficiente almacenamiento disponible. Esta verificación es muy simplista y solo compara el tamaño del volumen con la capacidad indicada en los objetos <code>CSIStorageCapacity</code> con una topología que incluye el nodo.</p><p>Para los volúmenes con el modo de enlace de volumen <code>Immediate</code>, el controlador de almacenamiento decide dónde crear el volumen, independientemente de los pods que usarán el volumen.
Luego, el planificador programa los pods en los nodos donde el volumen está disponible después de que se haya creado.</p><p>Para los <a href=/docs/concepts/storage/volumes/#csi>volúmenes efímeros de CSI</a>,
la planificación siempre ocurre sin considerar la capacidad de almacenamiento. Esto se basa en la suposición de que este tipo de volumen solo lo utilizan controladores CSI especiales que son locales a un nodo y no necesitan allí recursos importantes.</p><h2 id=replanificación>Replanificación</h2><p>Cuando se selecciona un nodo para un Pod con volúmenes <code>WaitForFirstConsumer</code>, esa decisión sigue siendo tentativa. El siguiente paso es que se le pide al controlador de almacenamiento CSI que cree el volumen con una pista de que el volumen está disponible en el nodo seleccionado.</p><p>Debido a que Kubernetes pudo haber elegido un nodo basándose en información de capacidad desactualizada, es posible que el volumen no se pueda crear realmente. Luego, la selección de nodo se restablece y el planificador de Kubernetes intenta nuevamente encontrar un nodo para el Pod.</p><h2 id=limitaciones>Limitaciones</h2><p>El seguimiento de la capacidad de almacenamiento aumenta las posibilidades de que la planificación funcione en el primer intento, pero no puede garantizarlo porque el planificador tiene que decidir basándose en información potencialmente desactualizada. Por lo general, el mismo mecanismo de reintento que para la planificación sin información de capacidad de almacenamiento es manejado por los errores de planificación.</p><p>Una situación en la que la planificación puede fallar de forma permanente es cuando un pod usa varios volúmenes: es posible que un volumen ya se haya creado en un segmento de topología que luego no tenga suficiente capacidad para otro volumen. La intervención manual es necesaria para recuperarse de esto, por ejemplo, aumentando la capacidad o eliminando el volumen que ya se creó. <a href=https://github.com/kubernetes/enhancements/pull/1703>Trabajo adicional</a> para manejar esto automáticamente.</p><h2 id=habilitación-del-seguimiento-de-la-capacidad-de-almacenamiento>Habilitación del seguimiento de la capacidad de almacenamiento</h2><p>El seguimiento de la capacidad de almacenamiento es una función beta y está habilitada de forma predeterminada en un clúster de Kubernetes desde Kubernetes 1.21. Además de tener la función habilitada en el clúster, un controlador CSI también tiene que admitirlo. Consulte la documentación del controlador para obtener más detalles.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Para obtener más información sobre el diseño, consulte las
<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md>Restricciones de Capacidad de Almacenamiento para la Planificación de Pods KEP</a>.</li><li>Para obtener más información sobre un mayor desarrollo de esta función, consulte <a href=https://github.com/kubernetes/enhancements/issues/1472>problema de seguimiento de mejoras #1472</a>.</li><li>Aprender sobre <a href=/docs/concepts/scheduling-eviction/kube-scheduler/>Planificador de Kubernetes</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b2e4b16ac37988c678a3312a4a6639f8>6.7 - Límites de Volumen específicos del Nodo</h1><p>Esta página describe la cantidad máxima de Volúmenes que se pueden adjuntar a un Nodo para varios proveedores de nube.</p><p>Los proveedores de la nube como Google, Amazon y Microsoft suelen tener un límite en la cantidad de Volúmenes que se pueden adjuntar a un Nodo. Es importante que Kubernetes respete esos límites. De lo contrario, los Pods planificados en un Nodo podrían quedarse atascados esperando que los Volúmenes se conecten.</p><h2 id=límites-predeterminados-de-kubernetes>Límites predeterminados de Kubernetes</h2><p>El Planificador de Kubernetes tiene límites predeterminados en la cantidad de Volúmenes que se pueden adjuntar a un Nodo:</p><table><tr><th>Servicio de almacenamiento en la nube</th><th>Volúmenes máximos por Nodo</th></tr><tr><td><a href=https://aws.amazon.com/ebs/>Amazon Elastic Block Store (EBS)</a></td><td>39</td></tr><tr><td><a href=https://cloud.google.com/persistent-disk/>Google Persistent Disk</a></td><td>16</td></tr><tr><td><a href=https://azure.microsoft.com/en-us/services/storage/main-disks/>Microsoft Azure Disk Storage</a></td><td>16</td></tr></table><h2 id=límites-personalizados>Límites personalizados</h2><p>Puede cambiar estos límites configurando el valor de la variable de entorno KUBE_MAX_PD_VOLS y luego iniciando el Planificador. Los controladores CSI pueden tener un procedimiento diferente, consulte su documentación sobre cómo personalizar sus límites.</p><p>Tenga cuidado si establece un límite superior al límite predeterminado. Consulte la documentación del proveedor de la nube para asegurarse de que los Nodos realmente puedan admitir el límite que establezca.</p><p>El límite se aplica a todo el clúster, por lo que afecta a todos los Nodos.</p><h2 id=límites-de-volumen-dinámico>Límites de Volumen dinámico</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code></div><p>Los límites de Volumen dinámico son compatibles con los siguientes tipos de Volumen.</p><ul><li>Amazon EBS</li><li>Google Persistent Disk</li><li>Azure Disk</li><li>CSI</li></ul><p>Para los Volúmenes administrados por in-tree plugins de Volumen, Kubernetes determina automáticamente el tipo de Nodo y aplica la cantidad máxima adecuada de Volúmenes para el Nodo. Por ejemplo:</p><ul><li><p>En
<a href=https://cloud.google.com/compute/>Google Compute Engine</a>,
se pueden adjuntar hasta 127 Volúmenes a un Nodo, <a href=https://cloud.google.com/compute/docs/disks/#pdnumberlimits>según el tipo de Nodo</a>.</p></li><li><p>Para los discos de Amazon EBS en los tipos de instancias M5,C5,R5,T3 y Z1D, Kubernetes permite que solo se adjunten 25 Volúmenes a un Nodo. Para otros tipos de instancias en
<a href=https://aws.amazon.com/ec2/>Amazon Elastic Compute Cloud (EC2)</a>,
Kubernetes permite adjuntar 39 Volúmenes a un Nodo.</p></li><li><p>En Azure, se pueden conectar hasta 64 discos a un Nodo, según el tipo de Nodo. Para obtener más detalles, consulte <a href=https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes>Sizes for virtual machines in Azure</a>.</p></li><li><p>Si un controlador de almacenamiento CSI anuncia una cantidad máxima de Volúmenes para un Nodo (usando <code>NodeGetInfo</code>), el <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a> respeta ese límite.
Consulte las <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo>especificaciones de CSI</a> para obtener más información.</p></li><li><p>Para los Volúmenes administrados por in-tree plugins que han sido migrados a un controlador CSI, la cantidad máxima de Volúmenes será la que informe el controlador CSI.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4f40cb95a671e51b4f0156a409d95c6d>6.8 - Supervisión del Estado del Volumen</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code></div><p>La supervisión del estado del volumen de <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> permite que los controladores de CSI detecten condiciones de volumen anómalas de los sistemas de almacenamiento subyacentes y las notifiquen como eventos en <a class=glossary-tooltip title='Reserva el recurso de almacenamiento definido en un PersistentVolume para poderlo montar como un volúmen en un contenedor.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PVCs>PVCs</a> o <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>.</p><h2 id=supervisión-del-estado-del-volumen>Supervisión del Estado del Volumen</h2><p>El <em>monitoreo del estado del volumen</em> de Kubernetes es parte de cómo Kubernetes implementa la Interfaz de Almacenamiento de Contenedores (CSI). La función de supervisión del estado del volumen se implementa en dos componentes: un controlador de supervisión del estado externo y <a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a>.</p><p>Si un controlador CSI admite la función supervisión del estado del volumen desde el lado del controlador, se informará un evento en el <a class=glossary-tooltip title='Reserva el recurso de almacenamiento definido en un PersistentVolume para poderlo montar como un volúmen en un contenedor.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PersistentVolumeClaim>PersistentVolumeClaim</a> (PVC) relacionado cuando se detecte una condición de volumen anormal en un volumen CSI.</p><p>El <a class=glossary-tooltip title='Los controladores son bucles de control que observan el estado del clúster, y ejecutan o solicitan los cambios que sean necesarios para alcanzar el estado deseado.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controlador>controlador</a> de estado externo también observa los eventos de falla del nodo. Se puede habilitar la supervisión de fallas de nodos configurando el indicador <code>enable-node-watcher</code> en verdadero. Cuando el monitor de estado externo detecta un evento de falla de nodo, el controlador reporta que se informará un evento en el PVC para indicar que los Pods que usan este PVC están en un nodo fallido.</p><p>Si un controlador CSI es compatible con la función monitoreo del estado del volumen desde el lado del nodo, se informará un evento en cada Pod que use el PVC cuando se detecte una condición de volumen anormal en un volumen CSI.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Se necesita habilitar el <code>CSIVolumeHealth</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> para usar esta función desde el lado del nodo.</div><h2 id=siguientes-pasos>Siguientes pasos</h2><p>Ver la <a href=https://kubernetes-csi.github.io/docs/drivers.html>documentación del controlador CSI</a> para averiguar qué controladores CSI han implementado esta característica.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-275bea454e1cf4c5adeca4058b5af988>7 - Configuración</h1></div><div class=td-content><h1 id=pg-6b5ccadd699df0904e8e9917c5450c4b>7.1 - ConfigMaps</h1><p><p>Un configmap es un objeto de la API utilizado para almacenar datos no confidenciales en el formato clave-valor. Los <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> pueden utilizar los ConfigMaps como variables de entorno, argumentos de la linea de comandos o como ficheros de configuración en un <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volumen>Volumen</a>.</p></p><p>Un ConfigMap te permite desacoplar la configuración de un entorno específico de una imagen de contenedor, así las aplicaciones son fácilmente portables.</p><div class="alert alert-warning caution callout" role=alert><strong>Precaución:</strong> ConfigMap no proporciona encriptación.
Si los datos que quieres almacenar son confidenciales, utiliza un
<a class=glossary-tooltip title='Almacena información sensible, como contraseñas, tokens OAuth o claves ssh.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a> en lugar de un ConfigMap,
o utiliza otras herramientas externas para mantener los datos seguros.</div><h2 id=motivo>Motivo</h2><p>Utiliza un ConfigMap para crear una configuración separada del código de la aplicación.</p><p>Por ejemplo, imagina que estás desarrollando una aplicación que puedes correr en
tu propio equipo (para desarrollo) y en el cloud (para mantener tráfico real).
Escribes el código para configurar una variable llamada <code>DATABASE_HOST</code>.
En tu equipo configuras la variable con el valor <code>localhost</code>.
En el cloud, la configuras con referencia a un kubernetes
<a class=glossary-tooltip title='Un Service, servicio en castellano, es el objeto de la API de Kubernetes que describe cómo se accede a las aplicaciones, tal como un conjunto de Pods, y que puede describir puertos y balanceadores de carga.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> que expone el componente
de la base de datos en tu cluster.</p><p>Esto permite tener una imagen corriendo en un cloud y
tener el mismo código localmente para checkearlo si es necesario.</p><h2 id=objeto-configmap>Objeto ConfigMap</h2><p>Un ConfigMap es un <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/>objeto</a> de la API
que permite almacenar la configuración de otros objetos utilizados. Aunque muchos
objetos de kubernetes que tienen un <code>spec</code>, un ConfigMap tiene una sección <code>data</code> para
almacenar items, identificados por una clave, y sus valores.</p><p>El nombre del ConfigMap debe ser un
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nombre de subdominio DNS</a> válido.</p><h2 id=configmaps-y-pods>ConfigMaps y Pods</h2><p>Puedes escribir un Pod <code>spec</code> y referenciarlo a un ConfigMap y configurar el contenedor(es)
de ese <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> en base a los datos del ConfigMap. El <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> y el ConfigMap deben estar en
el mismo <a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=Namespace>Namespace</a>.</p><p>Este es un ejemplo de ConfigMap que tiene algunas claves con un valor simple,
y otras claves donde el valor tiene un formato de un fragmento de configuración.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># property-like keys; each key maps to a simple value</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>player_initial_lives</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;3&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ui_properties_file_name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user-interface.properties&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic>#</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># file-like keys</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>game.properties</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    enemy.types=aliens,monsters
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    player.maximum-lives=5</span><span style=color:#bbb>    
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>user-interface.properties</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    color.good=purple
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    color.bad=yellow
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    allow.textmode=true</span><span style=color:#bbb>    
</span></span></span></code></pre></div><p>Hay cuatro maneras diferentes de usar un ConfigMap para configurar
un contenedor dentro de un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>:</p><ol><li>Argumento en la linea de comandos como entrypoint de un contenedor</li><li>Variable de entorno de un contenedor</li><li>Como fichero en un volumen de solo lectura, para que lo lea la aplicación</li><li>Escribir el código para ejecutar dentro de un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> que utiliza la API para leer el ConfigMap</li></ol><p>Estos diferentes mecanismos permiten utilizar diferentes métodos para modelar
los datos que se van a usar.
Para los primeros tres mecanismos, el
<a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> utiliza la información
del ConfigMap cuando lanza un contenedor (o varios) en un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>.</p><p>Para el cuarto método, tienes que escribir el código para leer el ConfigMap y sus datos.
Sin embargo, como estás utilizando la API de kubernetes directamente, la aplicación puede
suscribirse para obtener actualizaciones cuando el ConfigMap cambie, y reaccionar
cuando esto ocurra. Accediendo directamente a la API de kubernetes, esta
técnica también permite acceder al ConfigMap en diferentes namespaces.</p><p>En el siguiente ejemplo el Pod utiliza los valores de <code>game-demo</code> para configurar el contenedor:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>configmap-demo-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>game.example/demo-game<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Define the environment variable</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PLAYER_INITIAL_LIVES<span style=color:#bbb> </span><span style=color:#080;font-style:italic># Notice that the case is different here</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                                     </span><span style=color:#080;font-style:italic># from the key name in the ConfigMap.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>configMapKeyRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo          <span style=color:#bbb> </span><span style=color:#080;font-style:italic># The ConfigMap this value comes from.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>player_initial_lives<span style=color:#bbb> </span><span style=color:#080;font-style:italic># The key to fetch.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>UI_PROPERTIES_FILE_NAME<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>configMapKeyRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>ui_properties_file_name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/config&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># You set volumes at the Pod level, then mount them into containers inside that Pod</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Provide the name of the ConfigMap you want to mount.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># An array of keys from the ConfigMap to create as files</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;game.properties&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;game.properties&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user-interface.properties&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user-interface.properties&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Un ConfigMap no diferencia entre las propiedades de una linea individual y
un fichero con múltiples lineas y valores.
Lo importante es como los <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> y otros objetos consumen estos valores.</p><p>Para este ejemplo, definimos un <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volumen>Volumen</a> y lo montamos dentro del contenedor
<code>demo</code> como <code>/config</code> creando dos ficheros,
<code>/config/game.properties</code> y <code>/config/user-interface.properties</code>,
aunque haya cuatro claves en el ConfigMap. Esto es debido a que enla definición
del <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> se especifica el array <code>items</code> en la sección <code>volumes</code>.
Si quieres omitir el array <code>items</code> entero, cada clave del ConfigMap se convierte en
un fichero con el mismo nombre que la clave, y tienes 4 ficheros.</p><h2 id=usando-configmaps>Usando ConfigMaps</h2><p>Los ConfigMaps pueden montarse como volúmenes. También pueden ser utilizados por otras
partes del sistema, sin ser expuestos directamente al <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>. Por ejemplo,
los ConfigMaps pueden contener información para que otros elementos del sistema utilicen
para su configuración.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>La manera más común de usar los Configmaps es para configurar
los contenedores que están corriendo en un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> en el mismo <a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=Namespace>Namespace</a>.
También se pueden usar por separado.</p><p>Por ejemplo,
quizá encuentres <a class=glossary-tooltip title='Resources that extend the functionality of Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/cluster-administration/addons/ target=_blank aria-label=AddOns>AddOns</a>
u <a class=glossary-tooltip title='A specialized controller used to manage a custom resource' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/operator/ target=_blank aria-label=Operadores>Operadores</a> que
ajustan su comportamiento en base a un ConfigMap.</p></div><h3 id=usando-configmaps-como-ficheros-en-un-pod>Usando ConfigMaps como ficheros en un Pod</h3><p>Para usar un ConfigMap en un volumen en un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>:</p><ol><li>Crear un ConfigMap o usar uno que exista. Múltiples <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> pueden utilizar el mismo ConfigMap.</li><li>Modifica la configuración del <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> para añadir el volumen en <code>.spec.volumes[]</code>. Pon cualquier nombre al <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volumen>Volumen</a>, y tienes un campo <code>.spec.volumes[].configMap.name</code> configurado con referencia al objeto ConfigMap.</li><li>Añade un <code>.spec.containers[].volumeMounts[]</code> a cada contenedor que necesite el ConfigMap. Especifica <code>.spec.containers[].volumeMounts[].readOnly = true</code> y <code>.spec.containers[].volumeMounts[].mountPath</code> en un directorio sin uso donde quieras que aparezca el ConfigMap.</li><li>Modifica la imagen o el comando utilizado para que el programa busque los ficheros en el directorio. Cada clave del ConfigMap <code>data</code> se convierte en un un fichero en el <code>mountPath</code>.</li></ol><p>En este ejemplo, el <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> monta un ConfigMap como un <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volumen>volumen</a>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myconfigmap<span style=color:#bbb>
</span></span></span></code></pre></div><p>Cada ConfigMap que quieras utilizar debe estar referenciado en <code>.spec.volumes</code>.</p><p>Si hay múltiples contenedores en el <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, cada contenedor tiene su propio
bloque <code>volumeMounts</code>, pero solo un <code>.spec.volumes</code> es necesario por cada ConfigMap.</p><h4 id=configmaps-montados-son-actualizados-automáticamente>ConfigMaps montados son actualizados automáticamente</h4><p>Cuando un ConfigMap está siendo utilizado en un <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volumen>volumen</a> y es actualizado, las claves son actualizadas también.
El <a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> comprueba si el ConfigMap montado está actualizado cada periodo de sincronización.
Sin embargo, el <a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> utiliza su caché local para obtener el valor actual del ConfigMap.
El tipo de caché es configurable usando el campo <code>ConfigMapAndSecretChangeDetectionStrategy</code> en el
<a href=https://github.com/kubernetes/kubernetes/blob/main/staging/src/k8s.io/kubelet/config/v1beta1/types.go>KubeletConfiguration struct</a>.
Un ConfigMap puede ser propagado por vista (default), ttl-based, o simplemente redirigiendo
todas las consultas directamente a la API.
Como resultado, el retraso total desde el momento que el ConfigMap es actualizado hasta el momento
que las nuevas claves son proyectadas en el <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> puede ser tan largo como la sincronización del <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a></p><ul><li>el retraso de propagación de la caché, donde la propagación de la caché depende del tipo de
caché elegido (es igual al retraso de propagación, ttl de la caché, o cero correspondientemente).</li></ul><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [alpha]</code></div><p>La característica alpha de kubernetes <em>Immutable Secrets and ConfigMaps</em> provee una opción para configurar
<a class=glossary-tooltip title='Almacena información sensible, como contraseñas, tokens OAuth o claves ssh.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secrets>Secrets</a> individuales y ConfigMaps como inmutables. Para los <a class=glossary-tooltip title='Un conjunto de máquinas, llamadas nodos, que ejecutan aplicaciones en contenedores administradas por Kubernetes.' data-toggle=tooltip data-placement=top href='/es/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=Clústeres>Clústeres</a> que usan ConfigMaps como extensión
(al menos decenas o cientos de un único ConfigMap montado en <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>), previene cambios en sus
datos con las siguientes ventajas:</p><ul><li>protección de actualizaciones accidentales (o no deseadas) que pueden causar caídas de aplicaciones</li><li>mejora el rendimiento del <a class=glossary-tooltip title='Un conjunto de máquinas, llamadas nodos, que ejecutan aplicaciones en contenedores administradas por Kubernetes.' data-toggle=tooltip data-placement=top href='/es/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=Clúster>Clúster</a> significativamente reduciendo la carga del <a class=glossary-tooltip title='Componente del plano de control que expone la API de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-apiserver/ target=_blank aria-label=kube-apiserver>kube-apiserver</a>,
cerrando las vistas para el ConfigMap marcado como inmutable.</li></ul><p>Para usar esta característica, habilita el <code>ImmutableEmphemeralVolumes</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> y configura
el campo del <a class=glossary-tooltip title='Almacena información sensible, como contraseñas, tokens OAuth o claves ssh.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a> o ConfigMap <code>immutable</code> como <code>true</code>. Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>immutable</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Una vez que un ConfigMap o un <a class=glossary-tooltip title='Almacena información sensible, como contraseñas, tokens OAuth o claves ssh.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a> es marcado como inmutable, <em>no</em> es posible revertir el cambio
ni cambiar el contenido del campo <code>data</code>. Solo se puede eliminar y recrear el ConfigMap.
Los <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> existentes mantiene un punto de montaje del ConfigMap eliminado - es recomendable
recrear los <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>.</div><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Leer sobre <a href=/docs/concepts/configuration/secret/>Secrets</a>.</li><li>Leer <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>Configure a Pod to Use a ConfigMap</a>.</li><li>Leer <a href=https://12factor.net/>The Twelve-Factor App</a> para entender el motivo de separar
el código de la configuración.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6bc4711b1e133ac278036e963f4ce696>7.2 - Sobrecarga de Pod</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.16 [alpha]</code></div><p>Cuando se está ejecutando un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> en un <a class=glossary-tooltip title='Un Node, nodo en castellano, es una de las máquinas del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodo>nodo</a>, el Pod por sí mismo utiliza una cantidad de recursos del sistema. Estos recursos son adicionales a los recursos necesarios para hacer funcionar el/los contenedor(es) dentro del Pod.
La <em>Sobrecarga de Pod</em> es una característica para contabilizar los recursos consumidos por la infraestructura de Pods que están por encima de los valores de <em>Requests</em> y <em>Limits</em> del/los contenedor(es).</p><h2 id=sobrecarga-de-pod>Sobrecarga de Pod</h2><p>En Kubernetes, la sobrecarga de <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> se configura en el tiempo de <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks>admisión</a> con respecto a la sobrecarga asociada con el <a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a> del Pod.</p><p>Cuando se habilita la opción de sobrecarga de <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, se considera tanto la propia sobrecarga como la suma de solicitudes de recursos del contenedor al programar el <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>. Del mismo modo, <a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a> incluirá la sobrecarga de <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> cuando se dimensione el cgroup del <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, y cuando se realice la clasificación de la expulsión de <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>.</p><h3 id=configuración>Configuración</h3><p>Debe asegurarse de que el <a href=/docs/reference/command-line-tools-reference/feature-gates/>Feature Gate</a> <code>PodOverhead</code> esté activado (su valor está desactivado de manera predeterminada) en todo el <a class=glossary-tooltip title='Un conjunto de máquinas, llamadas nodos, que ejecutan aplicaciones en contenedores administradas por Kubernetes.' data-toggle=tooltip data-placement=top href='/es/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=clúster>clúster</a>. Esto significa:</p><ul><li>en el <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li><li>en el <a class=glossary-tooltip title='Componente del plano de control que expone la API de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-apiserver/ target=_blank aria-label=kube-apiserver>kube-apiserver</a></li><li>en el <a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> de cada <a class=glossary-tooltip title='Un Node, nodo en castellano, es una de las máquinas del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodo>nodo</a></li><li>en cualquier servidor de API personalizado que necesite <a href=/docs/reference/command-line-tools-reference/feature-gates/>Feature Gates</a>.</li></ul><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los usuarios que pueden escribir recursos del tipo RuntimeClass podrían impactar y poner en riesgo el rendimiento de la carga de trabajo en todo el <a class=glossary-tooltip title='Un conjunto de máquinas, llamadas nodos, que ejecutan aplicaciones en contenedores administradas por Kubernetes.' data-toggle=tooltip data-placement=top href='/es/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=clúster>clúster</a>. Por ello, se puede limitar el acceso a esta característica usando los controles de acceso de Kubernetes.
Para obtener más detalles vea la <a href=/docs/reference/access-authn-authz/authorization/>documentación sobre autorización</a>.</div><ul><li><a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a></li><li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>Diseño de capacidad de PodOverhead</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-436057b96151ecb8a4a9a9f456b5d0fc>7.3 - Administrando los recursos de los contenedores</h1><p>Cuando especificas un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, opcionalmente puedes especificar
los recursos que necesita un <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=Contenedor>Contenedor</a>.
Los recursos que normalmente se definen son CPU y memoria (RAM); pero hay otros.</p><p>Cuando especificas el recurso <em>request</em> para Contenedores en un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>,
el <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label='Scheduler de Kubernetes '>Scheduler de Kubernetes</a> usa esta información para decidir en qué nodo colocar el <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>.
Cuando especificas el recurso <em>limit</em> para un Contenedor, Kubelet impone estos límites, así que el contenedor no
puede utilizar más recursos que el límite que le definimos. Kubelet también reserva al menos la cantidad
especificada en <em>request</em> para el contenedor.</p><h2 id=peticiones-y-límites>Peticiones y límites</h2><p>Si el nodo donde está corriendo un pod tiene suficientes recursos disponibles, es posible
(y válido) que el <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=contenedor>contenedor</a> utilice más recursos de los especificados en <code>request</code>.
Sin embargo, un <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=contenedor>contenedor</a> no está autorizado a utilizar más de lo especificado en <code>limit</code>.</p><p>Por ejemplo, si configuras una petición de <code>memory</code> de 256 MiB para un <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=contenedor>contenedor</a>, y ese contenedor está
en un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> colocado en un nodo con 8GiB de memoria y no hay otros <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, entonces el contenedor puede intentar usar
más RAM.</p><p>Si configuras un límite de <code>memory</code> de 4GiB para el contenedor, <a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>)
(y
<a class=glossary-tooltip title='El Container Runtime, entorno de ejecución de un contenedor, es el software responsable de ejecutar contenedores.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='motor de ejecución del contenedor'>motor de ejecución del contenedor</a>) impone el límite.
El Runtime evita que el <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=contenedor>contenedor</a> use más recursos de los configurados en el límite. Por ejemplo:
cuando un proceso en el <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=contenedor>contenedor</a> intenta consumir más cantidad de memoria de la permitida,
el Kernel del sistema termina el proceso que intentó la utilización de la memoria, con un error de out of memory (OOM).</p><p>Los límites se pueden implementar de forma reactiva (el sistema interviene cuando ve la violación)
o por imposición (el sistema previene al contenedor de exceder el límite). Diferentes Runtimes pueden tener distintas
implementaciones a las mismas restricciones.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Si un contenedor especifica su propio límite de memoria, pero no especifica la petición de memoria, Kubernetes
automáticamente asigna una petición de memoria igual a la del límite. De igual manera, si un contenedor especifica su propio límite de CPU, pero no especifica una petición de CPU, Kubernetes automáticamente asigna una petición de CPU igual a la especificada en el límite.</div><h2 id=tipos-de-recursos>Tipos de recursos</h2><p><em>CPU</em> y <em>memoria</em> son cada uno un <em>tipo de recurso</em>. Un tipo de recurso tiene una unidad base.
CPU representa procesos de computación y es especificada en unidades de <a href=#meaning-of-cpu>Kubernetes CPUs</a>.
Memoria es especificada en unidades de bytes.
Si estás usando Kubernetes v1.14 o posterior, puedes especificar recursos <em>huge page</em>.
Huge pages son una característica de Linux específica donde el kernel del nodo asigna bloques
de memoria que son más grandes que el tamaño de paginación por defecto.</p><p>Por ejemplo, en un sistema donde el tamaño de paginación por defecto es de 4KiB, podrías
especificar un límite, <code>hugepages-2Mi: 80Mi</code>. Si el contenedor intenta asignar
más de 40 2MiB huge pages (un total de 80 MiB), la asignación fallará.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> No se pueden sobreasignar recursos <code>hugepages-*</code>.
A diferencia de los recursos de <code>memoria</code> y <code>cpu</code>.</div><p>CPU y memoria son colectivamente conocidos como <em>recursos de computación</em>, o simplemente como
<em>recursos</em>. Los recursos de computación son cantidades medibles que pueden ser solicitadas, asignadas
y consumidas. Son distintas a los <a href=/docs/concepts/overview/kubernetes-api/>Recursos API</a>. Los recursos API , como <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> y
<a href=/docs/concepts/services-networking/service/>Services</a> son objetos que pueden ser leídos y modificados
a través de la API de Kubernetes.</p><h2 id=peticiones-y-límites-de-recursos-de-pods-y-contenedores>Peticiones y límites de recursos de Pods y Contenedores</h2><p>Cada contenedor de un Pod puede especificar uno o más de los siguientes:</p><ul><li><code>spec.containers[].resources.limits.cpu</code></li><li><code>spec.containers[].resources.limits.memory</code></li><li><code>spec.containers[].resources.limits.hugepages-&lt;size></code></li><li><code>spec.containers[].resources.requests.cpu</code></li><li><code>spec.containers[].resources.requests.memory</code></li><li><code>spec.containers[].resources.requests.hugepages-&lt;size></code></li></ul><p>Aunque las peticiones y límites pueden ser especificadas solo en contenedores individuales, es conveniente hablar
sobre los recursos de peticiones y límites del Pod. Un <em>limite/petición
de recursos de un Pod</em> para un tipo de recurso particular es la suma de
peticiones/límites de cada tipo para cada contenedor del Pod.</p><h2 id=unidades-de-recursos-en-kubernetes>Unidades de recursos en Kubernetes</h2><h3 id=significado-de-cpu>Significado de CPU</h3><p>Límites y peticiones para recursos de CPU son medidos en unidades de <em>cpu</em>.
Una cpu, en Kubernetes, es equivalente a <strong>1 vCPU/Core</strong> para proveedores de cloud y <strong>1 hyperthread</strong> en procesadores bare-metal Intel.</p><p>Las peticiones fraccionadas están permitidas. Un contenedor con <code>spec.containers[].resources.requests.cpu</code> de <code>0.5</code> tiene garantizada la mitad, tanto
CPU como otro que requiere 1 CPU. La expresión <code>0.1</code> es equivalente a la expresión <code>100m</code>, que puede ser leída como "cien millicpus". Algunas personas dicen
"cienmilicores", y se entiende que quiere decir lo mismo. Una solicitud con un punto decimal, como <code>0.1</code>, es convertido a <code>100m</code> por la API, y no se permite
una precisión mayor que <code>1m</code>. Por esta razón, la forma <code>100m</code> es la preferente.
CPU es siempre solicitada como una cantidad absoluta, nunca como una cantidad relativa;
0.1 es la misma cantidad de cpu que un core-simple, dual-core, o máquina de 48-core.</p><h3 id=significado-de-memoria>Significado de memoria</h3><p>Los límites y peticiones de <code>memoria</code> son medidos en bytes. Puedes expresar la memoria como
un número entero o como un número decimal usando alguno de estos sufijos:
E, P, T, G, M, k, m (millis). También puedes usar los equivalentes en potencia de dos: Ei, Pi, Ti, Gi,
Mi, Ki. Por ejemplo, los siguientes valores representan lo mismo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>128974848, 129e6, 129M, 128974848000m, 123Mi
</span></span></code></pre></div><p>Aquí un ejemplo.
El siguiente <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> tiene dos contenedores. Cada contenedor tiene una petición de 0.25 cpu
y 64MiB (2<sup>26</sup> bytes) de memoria. Cada contenedor tiene un límite de 0.5 cpu
y 128MiB de memoria. Puedes decirle al Pod que solicite 0.5 cpu y 128MiB de memoria
y un límite de 1 cpu y 256MiB de memoria.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/app:v4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;64Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;128Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>log-aggregator<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/log-aggregator:v6<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;64Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;128Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=cómo-son-programados-los-pods-con-solicitudes-de-recursos>Cómo son programados los Pods con solicitudes de recursos</h2><p>Cuando creas un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, el <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label='planificador de Kubernetes '>planificador de Kubernetes</a> determina el nodo para correr dicho <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>.
Cada nodo tiene una capacidad máxima para cada tipo de recurso:
la cantidad de CPU y memoria que dispone para los Pods. El <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label='planificador de Kubernetes'>planificador de Kubernetes</a> se asegura de que,
para cada tipo de recurso, la suma de los recursos solicitados de los contenedores programados sea menor a la capacidad del nodo. Cabe mencionar que aunque la memoria actual o CPU
en uso de los nodos sea muy baja, el <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=planificador>planificador</a> todavía rechaza programar un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> en un nodo si
la comprobación de capacidad falla. Esto protege contra escasez de recursos en un nodo
cuando el uso de recursos posterior crece, por ejemplo, durante un pico diario de
solicitud de recursos.</p><h2 id=cómo-corren-los-pods-con-límites-de-recursos>Cómo corren los Pods con límites de recursos</h2><p>Cuando el <a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> inicia un <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=contenedor>contenedor</a> de un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, este pasa los límites de CPU y
memoria al <a class=glossary-tooltip title='El Container Runtime, entorno de ejecución de un contenedor, es el software responsable de ejecutar contenedores.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='runtime del contenedor'>runtime del contenedor</a>.</p><p>Cuando usas Docker:</p><ul><li><p>El <code>spec.containers[].resources.requests.cpu</code> es convertido a su valor interno,
el cuál es fraccional, y multiplicado por 1024. El mayor valor de este número o
2 es usado por el valor de
<a href=https://docs.docker.com/engine/reference/run/#cpu-share-constraint><code>--cpu-shares</code></a>
en el comando <code>docker run</code>.</p></li><li><p>El <code>spec.containers[].resources.limits.cpu</code> se convierte a su valor en milicore y
multiplicado por 100. El resultado es el tiempo total de CPU que un contenedor puede usar
cada 100ms. Un contenedor no puede usar más tiempo de CPU que del solicitado durante este intervalo.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El período por defecto es de 100ms. La resolución mínima de cuota mínima es 1ms.</div></li><li><p>El <code>spec.containers[].resources.limits.memory</code> se convierte a entero, y
se usa como valor de
<a href=https://docs.docker.com/engine/reference/run/#/user-memory-constraints><code>--memory</code></a>
del comando <code>docker run</code>.</p></li></ul><p>Si el <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=contenedor>contenedor</a> excede su límite de memoria, este quizá se detenga. Si es reiniciable,
el <a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> lo reiniciará, así como cualquier otro error.</p><p>Si un Contenedor excede su petición de memoria, es probable que ese Pod sea
desalojado en cualquier momento que el nodo se quede sin memoria.</p><p>Un Contenedor puede o no tener permitido exceder el límite de CPU por
algunos períodos de tiempo. Sin embargo, esto no lo destruirá por uso excesivo de CPU.</p><p>Para conocer cuando un Contenedor no puede ser programado o será destruido debido a
límite de recursos, revisa la sección de <a href=#troubleshooting>Troubleshooting</a>.</p><h3 id=monitorización-del-uso-de-recursos-de-computación-y-memoria>Monitorización del uso de recursos de computación y memoria.</h3><p>El uso de recursos de un Pod es reportado como parte del estado del Pod.</p><p>Si <a href=/docs/tasks/debug-application-cluster/resource-usage-monitoring/>herramientas opcionales para monitorización</a>
están disponibles en tu cluster, entonces el uso de recursos del Pod puede extraerse directamente de
<a href=/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#the-metrics-api>Métricas API</a>
o desde tus herramientas de monitorización.</p><h2 id=almacenamiento-local-efímero>Almacenamiento local efímero</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.10 [beta]</code></div><p>Los nodos tienen almacenamiento local efímero, respaldado por
dispositivos de escritura agregados o, a veces, por RAM.
"Efímero" significa que no se garantiza la durabilidad a largo plazo.
.
Los Pods usan el almacenamiento local efímero para añadir espacio, caché, y para logs.
Kubelet puede proveer espacio añadido a los Pods usando almacenamiento local efímero para
montar <a href=/docs/concepts/storage/volumes/#emptydir><code>emptyDir</code></a>
<a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volumes>volumes</a> en los contenedores.</p><p>Kubelet también usa este tipo de almacenamiento para guardar
<a href=/docs/concepts/cluster-administration/logging/#logging-at-the-node-level>logs de contenedores a nivel de nodo</a>,
imágenes de contenedores, y la capa de escritura de los contenedores.</p><div class="alert alert-warning caution callout" role=alert><strong>Precaución:</strong> Si un nodo falla, los datos en el almacenamiento efímero se pueden perder.
Tus aplicaciones no pueden esperar ningun SLA (IOPS de disco, por ejemplo)
del almacenamiento local efímero.</div><p>Como característica beta, Kubernetes te deja probar, reservar y limitar la cantidad
de almacenamiento local efímero que un Pod puede consumir.</p><h3 id=configuraciones-para-almacenamiento-local-efímero>Configuraciones para almacenamiento local efímero</h3><p>Kubernetes soporta 2 maneras de configurar el almacenamiento local efímero en un nodo:<ul class="nav nav-tabs" id=local-storage-configurations role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#local-storage-configurations-0 role=tab aria-controls=local-storage-configurations-0 aria-selected=true>Single filesystem</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#local-storage-configurations-1 role=tab aria-controls=local-storage-configurations-1>Two filesystems</a></li></ul><div class=tab-content id=local-storage-configurations><div id=local-storage-configurations-0 class="tab-pane show active" role=tabpanel aria-labelledby=local-storage-configurations-0><p><p>En esta configuración, colocas todos los tipos de datos (<code>emptyDir</code> volúmenes, capa de escritura,
imágenes de contenedores, logs) en un solo sistema de ficheros.
La manera más efectiva de configurar Kubelet es dedicando este sistema de archivos para los datos de Kubernetes (kubelet).</p><p>Kubelet también escribe
<a href=/docs/concepts/cluster-administration/logging/#logging-at-the-node-level>logs de contenedores a nivel de nodo</a>
y trata estos de manera similar al almacenamiento efímero.</p><p>Kubelet escribe logs en ficheros dentro del directorio de logs (por defecto <code>/var/log</code>
); y tiene un directorio base para otros datos almacenados localmente
(<code>/var/lib/kubelet</code> por defecto).</p><p>Por lo general, <code>/var/lib/kubelet</code> y <code>/var/log</code> están en el sistema de archivos de root,
y Kubelet es diseñado con ese objetivo en mente.</p><p>Tu nodo puede tener tantos otros sistema de archivos, no usados por Kubernetes,
como quieras.</p></div><div id=local-storage-configurations-1 class=tab-pane role=tabpanel aria-labelledby=local-storage-configurations-1><p><p>Tienes un sistema de archivos en el nodo que estás usando para datos efímeros que
provienen de los Pods corriendo: logs, y volúmenes <code>emptyDir</code>.
Puedes usar este sistema de archivos para otros datos (por ejemplo: logs del sistema no relacionados
con Kubernetes); estos pueden ser incluso del sistema de archivos root.</p><p>Kubelet también escribe
<a href=/docs/concepts/cluster-administration/logging/#logging-at-the-node-level>logs de contenedores a nivel de nodo</a>
en el primer sistema de archivos, y trata estos de manera similar al almacenamiento efímero.</p><p>También usas un sistema de archivos distinto, respaldado por un dispositivo de almacenamiento lógico diferente.
En esta configuración, el directorio donde le dices a Kubelet que coloque
las capas de imágenes de los contenedores y capas de escritura es este segundo sistema de archivos.</p><p>El primer sistema de archivos no guarda ninguna capa de imágenes o de escritura.</p><p>Tu nodo puede tener tantos sistemas de archivos, no usados por Kubernetes, como quieras.</p></div></div></p><p>Kubelet puede medir la cantidad de almacenamiento local que se está usando. Esto es posible por:</p><ul><li>el <code>LocalStorageCapacityIsolation</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
está habilitado (esta caracterísitca está habilitada por defecto), y</li><li>has configurado el nodo usando una de las configuraciones soportadas
para almacenamiento local efímero..</li></ul><p>Si tienes una configuración diferente, entonces Kubelet no aplica límites de recursos
para almacenamiento local efímero.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Kubelet rastrea <code>tmpfs</code> volúmenes emptyDir como uso de memoria de contenedor, en lugar de
almacenamiento local efímero.</div><h3 id=configurando-solicitudes-y-límites-para-almacenamiento-local-efímero>Configurando solicitudes y límites para almacenamiento local efímero</h3><p>Puedes usar <em>ephemeral-storage</em> para manejar almacenamiento local efímero. Cada contenedor de un Pod puede especificar
uno o más de los siguientes:</p><ul><li><code>spec.containers[].resources.limits.ephemeral-storage</code></li><li><code>spec.containers[].resources.requests.ephemeral-storage</code></li></ul><p>Los límites y solicitudes para <code>almacenamiento-efímero</code> son medidos en bytes. Puedes expresar el almacenamiento
como un numero entero o flotante usando los siguientes sufijos:
E, P, T, G, M, K. También puedes usar las siguientes equivalencias: Ei, Pi, Ti, Gi,
Mi, Ki. Por ejemplo, los siguientes representan el mismo valor:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>128974848, 129e6, 129M, 123Mi
</span></span></code></pre></div><p>En el siguiente ejemplo, el Pod tiene dos contenedores. Cada contenedor tiene una petición de 2GiB de almacenamiento local efímero. Cada
contenedor tiene un límite de 4GiB de almacenamiento local efímero. Sin embargo, el Pod tiene una petición de 4GiB de almacenamiento efímero
, y un límite de 8GiB de almacenamiento local efímero.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/app:v4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ephemeral<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/tmp&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>log-aggregator<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/log-aggregator:v6<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ephemeral<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/tmp&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ephemeral<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=como-son-programados-los-pods-con-solicitudes-de-almacenamiento-efímero>Como son programados los Pods con solicitudes de almacenamiento efímero</h3><p>Cuando creas un Pod, el planificador de Kubernetes selecciona un nodo para el Pod donde sera creado.
Cada nodo tiene una cantidad máxima de almacenamiento local efímero que puede proveer a los Pods. Para
más información, mira <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>Node Allocatable</a>.</p><p>El planificador se asegura de que el total de los recursos solicitados para los contenedores sea menor que la capacidad del nodo.</p><h3 id=resource-emphemeralstorage-consumption>Manejo del consumo de almacenamiento efímero</h3><p>Si Kubelet está manejando el almacenamiento efímero local como un recurso, entonces
Kubelet mide el uso de almacenamiento en:</p><ul><li>volúmenes <code>emptyDir</code>, excepto <em>tmpfs</em> volúmenes<code>emptyDir</code></li><li>directorios que guardan logs de nivel de nodo</li><li>capas de escritura de contenedores</li></ul><p>Si un Pod está usando más almacenamiento efímero que el permitido, Kubelet
establece una señal de desalojo que desencadena el desalojo del Pod.</p><p>Para aislamiento a nivel de contenedor, si una capa de escritura del contenedor y
logs excede el límite de uso del almacenamiento, Kubelet marca el Pod para desalojo.</p><p>Para aislamiento a nivel de Pod, Kubelet calcula un límite de almacenamiento
general para el Pod sumando los límites de los contenedores de ese Pod.
En este caso, si la suma del uso de almacenamiento local efímero para todos los contenedores
y los volúmenes <code>emptyDir</code> de los Pods excede el límite de almacenamiento general del
Pod, Kubelet marca el Pod para desalojo.</p><div class="alert alert-warning caution callout" role=alert><strong>Precaución:</strong><p>Si Kubelet no está midiendo el almacenamiento local efímero, entonces el Pod
que excede este límite de almacenamiento, no será desalojado para liberar
el límite del recurso de almacenamiento.</p><p>Sin embargo, si el espacio del sistema de archivos para la capa de escritura del contenedor,
logs a nivel de nodo o volúmenes <code>emptyDir</code> decae, el
<a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=taints>taints</a> del nodo lanza la desalojo para
cualquier Pod que no tolere dicho taint.</p><p>Mira las <a href=#configurations-for-local-ephemeral-storage>configuraciones soportadas</a>
para almacenamiento local efímero.</p></div><p>Kubelet soporta diferentes maneras de medir el uso de almacenamiento del Pod:</p><ul class="nav nav-tabs" id=resource-emphemeralstorage-measurement role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#resource-emphemeralstorage-measurement-0 role=tab aria-controls=resource-emphemeralstorage-measurement-0 aria-selected=true>Periodic scanning</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#resource-emphemeralstorage-measurement-1 role=tab aria-controls=resource-emphemeralstorage-measurement-1>Filesystem project quota</a></li></ul><div class=tab-content id=resource-emphemeralstorage-measurement><div id=resource-emphemeralstorage-measurement-0 class="tab-pane show active" role=tabpanel aria-labelledby=resource-emphemeralstorage-measurement-0><p><p>Kubelet realiza frecuentemente, verificaciones programadas que revisan cada
volumen <code>emptyDir</code>, directorio de logs del contenedor, y capa de escritura
del contenedor.</p><p>El escáner mide cuanto espacio está en uso.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>En este modo, Kubelet no rastrea descriptores de archivos abiertos
para archivos eliminados.</p><p>Si tú (o un contenedor) creas un archivo dentro de un volumen <code>emptyDir</code>,
y algo mas abre ese archivo, y tú lo borras mientras este está abierto,
entonces el inodo para este archivo borrado se mantiene hasta que cierras
el archivo, pero Kubelet no cataloga este espacio como en uso.</p></div></div><div id=resource-emphemeralstorage-measurement-1 class=tab-pane role=tabpanel aria-labelledby=resource-emphemeralstorage-measurement-1><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.15 [alpha]</code></div><p>Las cuotas de proyecto están definidas a nivel de sistema operativo
para el manejo de uso de almacenamiento en uso de sistema de archivos.
Con Kubernetes, puedes habilitar las cuotas de proyecto para el uso
de la monitorización del almacenamiento. Asegúrate que el respaldo del
Sistema de archivos de los volúmenes <code>emptyDir</code> , en el nodo, provee soporte de
cuotas de proyecto.
Por ejemplo, XFS y ext4fs ofrecen cuotas de proyecto.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Las cuotas de proyecto te permiten monitorear el uso del almacenamiento; no
fuerzan los límites.</div><p>Kubernetes usa IDs de proyecto empezando por <code>1048576</code>. Los IDs en uso
son registrados en <code>/etc/projects</code> y <code>/etc/projid</code>. Si los IDs de proyecto
en este rango son usados para otros propósitos en el sistema, esos IDs
de proyecto deben ser registrados en <code>/etc/projects</code> y <code>/etc/projid</code> para
que Kubernetes no los use.</p><p>Las cuotas son más rápidas y más precisas que el escáner de directorios.
Cuando un directorio es asignado a un proyecto, todos los ficheros creados
bajo un directorio son creados en ese proyecto, y el kernel simplemente
tiene que mantener rastreados cuántos bloques están en uso por ficheros
en ese proyecto. Si un fichero es creado y borrado, pero tiene un fichero abierto,
continúa consumiendo espacio. El seguimiento de cuotas registra ese espacio
con precisión mientras que los escaneos de directorios pasan por alto
el almacenamiento utilizado por los archivos eliminados</p><p>Si quieres usar cuotas de proyecto, debes:</p><ul><li><p>Habilitar el <code>LocalStorageCapacityIsolationFSQuotaMonitoring=true</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
en la configuración del kubelet.</p></li><li><p>Asegúrese de que el sistema de archivos raíz (o el sistema de archivos en tiempo de ejecución opcional)
tiene las cuotas de proyectos habilitadas. Todos los sistemas de archivos XFS admiten cuotas de proyectos.
Para los sistemas de archivos ext4, debe habilitar la función de seguimiento de cuotas del proyecto
mientras el sistema de archivos no está montado.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># For ext4, with /dev/block-device not mounted</span>
</span></span><span style=display:flex><span>sudo tune2fs -O project -Q prjquota /dev/block-device
</span></span></code></pre></div></li><li><p>Asegúrese de que el sistema de archivos raíz (o el sistema de archivos de tiempo de ejecución opcional) esté
montado con cuotas de proyecto habilitadas. Tanto para XFS como para ext4fs, la opción de montaje
se llama <code>prjquota</code>.</p></li></ul></div></div><h2 id=recursos-extendidos>Recursos extendidos</h2><p>Los recursos extendidos son nombres de recursos calificados fuera del
dominio <code>kubernetes.io</code>. Permiten que los operadores de clústers publiciten y los usuarios
consuman los recursos no integrados de Kubernetes.</p><p>Hay dos pasos necesarios para utilizar los recursos extendidos. Primero, el operador del clúster
debe anunciar un Recurso Extendido. En segundo lugar, los usuarios deben solicitar
el Recurso Extendido en los Pods.</p><h3 id=manejando-recursos-extendidos>Manejando recursos extendidos</h3><h4 id=recursos-extendido-a-nivel-de-nodo>Recursos extendido a nivel de nodo</h4><p>Los recursos extendidos a nivel de nodo están vinculados a los nodos</p><h5 id=device-plugin-managed-resources>Device plugin managed resources</h5><p>Mira <a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>Plugins de
Dispositivos</a>
para percibir como los plugins de dispositivos manejan los recursos
en cada nodo.</p><h5 id=otros-recursos>Otros recursos</h5><p>Para anunciar un nuevo recurso extendido a nivel de nodo, el operador del clúster puede
enviar una solicitud HTTP <code>PATCH</code> al servidor API para especificar la cantidad
disponible en el <code>status.capacity</code> para un nodo en el clúster. Después de esta
operación, el <code>status.capacity</code> del nodo incluirá un nuevo recurso. El campo
<code>status.allocatable</code> se actualiza automáticamente con el nuevo recurso
de forma asíncrona por el <a class=glossary-tooltip title='Agente que se ejecuta en cada nodo de un clúster. Se asegura de que los contenedores estén corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>. Tenga en cuenta que debido a que el <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=planificador>planificador</a>
utiliza el valor de <code>status.allocatable</code> del nodo cuando evalúa la aptitud del <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, puede haber un breve
retraso entre parchear la capacidad del nodo con un nuevo recurso y el primer Pod
que solicita el recurso en ese nodo.</p><p><strong>Ejemplo:</strong></p><p>Aquí hay un ejemplo que muestra cómo usar <code>curl</code> para formar una solicitud HTTP que
anuncia cinco recursos "example.com/foo" en el nodo <code>k8s-node-1</code> cuyo nodo master
es <code>k8s-master</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl --header <span style=color:#b44>&#34;Content-Type: application/json-patch+json&#34;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--request PATCH <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--data <span style=color:#b44>&#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1foo&#34;, &#34;value&#34;: &#34;5&#34;}]&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> En la solicitud anterior, <code>~ 1</code> es la codificación del carácter<code>/</code>
en la ruta del parche. El valor de la ruta de operación en JSON-Patch se interpreta como un
puntero JSON. Para obtener más detalles, consulte
<a href=https://tools.ietf.org/html/rfc6901#section-3>IETF RFC 6901, sección 3</a>.</div><h4 id=recursos-extendidos-a-nivel-de-clúster>Recursos extendidos a nivel de Clúster</h4><p>Los recursos extendidos a nivel de clúster no están vinculados a los nodos. Suelen estar gestionados
por extensores del scheduler, que manejan el consumo de recursos y la cuota de recursos.</p><p>Puedes especificar los recursos extendidos que son mantenidos por los extensores del scheduler en
<a href=https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/scheduler/api/v1/types.go#L31>configuración de políticas del scheduler</a>.</p><p><strong>Ejemplo:</strong></p><p>La siguiente configuración para una política del scheduler indica que el
recurso extendido a nivel de clúster "example.com/foo" es mantenido
por el extensor del scheduler.</p><ul><li>El scheduler envía un Pod al extensor del scheduler solo si la solicitud del Pod "example.com/foo".</li><li>El campo <code>ignoredByScheduler</code> especifica que el schduler no compruba el recurso
"example.com/foo" en su predicado <code>PodFitsResources</code>.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Policy&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;extenders&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;urlPrefix&#34;</span>:<span style=color:#b44>&#34;&lt;extender-endpoint&gt;&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;bindVerb&#34;</span>: <span style=color:#b44>&#34;bind&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;managedResources&#34;</span>: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>          <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;example.com/foo&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:green;font-weight:700>&#34;ignoredByScheduler&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=consumiendo-recursos-extendidos>Consumiendo recursos extendidos</h3><p>Los usuarios pueden consumir recursos extendidos en las especificaciones del Pod, como la CPU y la memoria.
El <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=planificador>planificador</a> se encarga de la contabilidad de recursos para que no más de
la cantidad disponible sea asignada simultáneamente a los Pods.</p><p>El servidor de API restringe las cantidades de recursos extendidos a números enteros.
Ejemplos de cantidades <em>validas</em> son <code>3</code>, <code>3000m</code> y <code>3Ki</code>. Ejemplos de
<em>cantidades no válidas</em> son <code>0.5</code> y <code>1500m</code>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los recursos extendidos reemplazan los Recursos Integrales Opacos.
Los usuarios pueden usar cualquier otro prefijo de dominio que <code>kubernetes.io</code>
tenga reservado.</div><p>Para consumir un recurso extendido en un Pod, incluye un nombre de recurso
como clave en <code>spec.containers[].resources.limits</code> en las especificaciones del contenedor.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los Recursos Extendidos no pueden ser sobreescritos, así que solicitudes y límites
deben ser iguales si ambos están presentes en las especificaciones de un contenedor.</div><p>Un pod se programa solo si se satisfacen todas las solicitudes de recursos, incluidas
CPU, memoria y cualquier recurso extendido. El <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> permanece en estado <code>PENDING</code>
siempre que no se pueda satisfacer la solicitud de recursos.</p><p><strong>Ejemplo:</strong></p><p>El siguiente <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> solicita 2CPUs y 1 "example.com/foo" (un recurso extendido).</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>myimage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/foo</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/foo</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=solución-de-problemas>Solución de problemas</h2><h3 id=mis-pods-están-en-estado-pendiente-con-un-mensaje-de-failedscheduling>Mis Pods están en estado pendiente con un mensaje de failedScheduling</h3><p>Si el <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=planificador>planificador</a> no puede encontrar ningún nodo donde pueda colocar un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, el <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> permanece
no programado hasta que se pueda encontrar un lugar. Se produce un evento cada vez que
el <a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=planificador>planificador</a> no encuentra un lugar para el <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>, como este:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pod frontend | grep -A <span style=color:#666>3</span> Events
</span></span></code></pre></div><pre tabindex=0><code>Events:
  FirstSeen LastSeen   Count  From          Subobject   PathReason      Message
  36s   5s     6      {scheduler }              FailedScheduling  Failed for reason PodExceedsFreeCPU and possibly others
</code></pre><p>En el ejemplo anterior, el Pod llamado "frontend" no se puede programar debido a
recursos de CPU insuficientes en el nodo. Mensajes de error similares también pueden sugerir
fallo debido a memoria insuficiente (PodExceedsFreeMemory). En general, si un Pod
está pendiente con un mensaje de este tipo, hay varias cosas para probar:</p><ul><li>Añadir más nodos al clúster.</li><li>Terminar Pods innecesarios para hacer hueco a los Pods en estado pendiente.</li><li>Compruebe que el Pod no sea más grande que todos los nodos. Por ejemplo, si todos los
los nodos tienen una capacidad de <code>cpu: 1</code>, entonces un Pod con una solicitud de <code>cpu: 1.1</code>
nunca se programará.</li></ul><p>Puedes comprobar las capacidades del nodo y cantidad utilizada con el comando
<code>kubectl describe nodes</code>. Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe nodes e2e-test-node-pool-4lw4
</span></span></code></pre></div><pre tabindex=0><code>Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
</code></pre><p>EN la salida anterior, puedes ver si una solicitud de Pod mayor que 1120m
CPUs o 6.23Gi de memoria, no cabrán en el nodo.</p><p>Echando un vistazo a la sección <code>Pods</code>, puedes ver qué Pods están ocupando espacio
en el nodo.</p><p>La cantidad de recursos disponibles para los pods es menor que la capacidad del nodo, porque
los demonios del sistema utilizan una parte de los recursos disponibles. El campo <code>allocatable</code>
<a href=/docs/reference/generated/kubernetes-api/v1.25/#nodestatus-v1-core>NodeStatus</a>
indica la cantidad de recursos que están disponibles para los Pods. Para más información, mira
<a href=https://git.k8s.io/design-proposals-archive/node/node-allocatable.md>Node Allocatable Resources</a>.</p><p>La característica <a href=/docs/concepts/policy/resource-quotas/>resource quota</a> se puede configurar
para limitar la cantidad total de recursos que se pueden consumir. Si se usa en conjunto
con espacios de nombres, puede evitar que un equipo acapare todos los recursos.</p><h3 id=mi-contenedor-está-terminado>Mi contenedor está terminado</h3><p>Es posible que su contenedor se cancele porque carece de recursos. Para verificar
si un contenedor está siendo eliminado porque está alcanzando un límite de recursos, ejecute
<code>kubectl describe pod</code> en el Pod de interés:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pod simmemleak-hra99
</span></span></code></pre></div><pre tabindex=0><code>Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Replication Controllers:        simmemleak (1/1 replicas created)
Containers:
  simmemleak:
    Image:  saadali/simmemleak
    Limits:
      cpu:                      100m
      memory:                   50Mi
    State:                      Running
      Started:                  Tue, 07 Jul 2015 12:54:41 -0700
    Last Termination State:     Terminated
      Exit Code:                1
      Started:                  Fri, 07 Jul 2015 12:54:30 -0700
      Finished:                 Fri, 07 Jul 2015 12:54:33 -0700
    Ready:                      False
    Restart Count:              5
Conditions:
  Type      Status
  Ready     False
Events:
  FirstSeen                         LastSeen                         Count  From                              SubobjectPath                       Reason      Message
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {scheduler }                                                          scheduled   Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   pulled      Pod container image &#34;k8s.gcr.io/pause:0.8.0&#34; already present on machine
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   created     Created with docker id 6a41280f516d
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    implicitly required container POD   started     Started with docker id 6a41280f516d
  Tue, 07 Jul 2015 12:53:51 -0700   Tue, 07 Jul 2015 12:53:51 -0700  1      {kubelet kubernetes-node-tf0f}    spec.containers{simmemleak}         created     Created with docker id 87348f12526a
</code></pre><p>En el ejemplo anterior, <code>Restart Count: 5</code> indica que el contenedor <code>simmemleak</code>
del Pod se reinició cinco veces.</p><p>Puedes ejecutar <code>kubectl get pod</code> con la opción <code>-o go-template=...</code> para extraer el estado
previos de los Contenedores terminados:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pod -o go-template<span style=color:#666>=</span><span style=color:#b44>&#39;{{range.status.containerStatuses}}{{&#34;Container Name: &#34;}}{{.name}}{{&#34;\r\nLastState: &#34;}}{{.lastState}}{{end}}&#39;</span>  simmemleak-hra99
</span></span></code></pre></div><pre tabindex=0><code>Container Name: simmemleak
LastState: map[terminated:map[exitCode:137 reason:OOM Killed startedAt:2015-07-07T20:58:43Z finishedAt:2015-07-07T20:58:43Z containerID:docker://0e4095bba1feccdfe7ef9fb6ebffe972b4b14285d5acdec6f0d3ae8a22fad8b2]]
</code></pre><p>Puedes ver que el Contenedor fué terminado a causa de <code>reason:OOM Killed</code>, donde <code>OOM</code> indica una falta de memoria.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li><p>Obtén experiencia práctica <a href=/docs/tasks/configure-pod-container/assign-memory-resource/>assigning Memory resources to Containers and Pods</a>.</p></li><li><p>Obtén experiencia práctica <a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>assigning CPU resources to Containers and Pods</a>.</p></li><li><p>Para más detalles sobre la diferencia entre solicitudes y límites, mira
<a href=https://git.k8s.io/design-proposals-archive/node/resource-qos.md>Resource QoS</a>.</p></li><li><p>Lee <a href=/docs/reference/generated/kubernetes-api/v1.25/#container-v1-core>Container</a> referencia de API</p></li><li><p>Lee <a href=/docs/reference/generated/kubernetes-api/v1.25/#resourcerequirements-v1-core>ResourceRequirements</a> referencia de API</p></li><li><p>Lee sobre <a href=https://xfs.org/index.php/XFS_FAQ#Q:_Quota:_Do_quotas_work_on_XFS.3F>cuotas de proyecto</a> en XFS</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e511ed821ada65d0053341dbd8ad2bb5>7.4 - Secrets</h1><p>Los objetos de tipo <a class=glossary-tooltip title='Almacena información sensible, como contraseñas, tokens OAuth o claves ssh.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a> en Kubernetes te permiten almacenar y administrar información confidencial, como
contraseñas, tokens OAuth y llaves ssh. Poniendo esta información en un Secret
es más seguro y más flexible que ponerlo en la definición de un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> o en un <a class=glossary-tooltip title='Instantánea de un contenedor que contiene un conjunto de librerías necesarias para ejecutar la aplicación.' data-toggle=tooltip data-placement=top href='/es/docs/reference/glossary/?all=true#term-image' target=_blank aria-label='container image'>container image</a>. Ver <a href=https://git.k8s.io/design-proposals-archive/auth/secrets.md>Secrets design document</a> para más información.</p><h2 id=introducción-a-secrets>Introducción a Secrets</h2><p>Un Secret es un objeto que contiene una pequeña cantidad de datos confidenciales como contraseñas, un token, o una llave. Tal información podría ser puesta en la especificación de un Pod o en una imagen; poniendolo en un objeto de tipo Secret permite mayor control sobre como se usa, y reduce el riesgo de exposicición accidental.</p><p>Los usuarios pueden crear Secrets, y el sistema también puede crearlos.</p><p>Para usar un Secret, un Pod debe hacer referencia a este. Un Secret puede ser usado con un Pod de dos formas: como archivos en un <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a> montado en uno o más de sus contenedores, o utilizados por el kubelet al extraer imágenes del pod.</p><h3 id=secrets-incorporados>Secrets incorporados</h3><h4 id=las-cuentas-de-servicio-crean-y-adjuntan-secrets-con-las-credenciales-de-la-api>Las Cuentas de Servicio Crean y Adjuntan Secrets con las Credenciales de la API</h4><p>Kubernetes crea automaticamente Secrets que contienen credenciales para acceder a la API y modifica automáticamente sus pods para usar este tipo de Secret.</p><p>La creación y el uso automático de las credenciales de la API, pueden desabilitarse o anularse si se desea. Sin embargo, si todo lo que necesita hacer es acceder de forma segura al apiserver, este es el flujo de trabajo recomendado.</p><p>Ver la documentación de <a href=/docs/tasks/configure-pod-container/configure-service-account/>Service Account</a> para más información sobre cómo funcionan las Cuentas de Servicio.</p><h3 id=creando-tu-propio-secret>Creando tu propio Secret</h3><h4 id=creando-un-secret-usando-kubectl-create-secret>Creando un Secret Usando kubectl create Secret</h4><p>Pongamos como ejemplo el caso de una grupo de pods que necesitan acceder a una base de datos. El nombre y contraseña que los pods deberían usar están en los archivos:
<code>./username.txt</code> y <code>./password.txt</code> en tu máquina local.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># Crear archivos necesarios para el resto del ejemplo.</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> -n <span style=color:#b44>&#39;admin&#39;</span> &gt; ./username.txt
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> -n <span style=color:#b44>&#39;1f2d1e2e67df&#39;</span> &gt; ./password.txt
</span></span></code></pre></div><p>El comando <code>kubectl create secret</code>
empaqueta esos archivos en un Secret y crea el objeto en el Apiserver.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic db-user-pass --from-file<span style=color:#666>=</span>./username.txt --from-file<span style=color:#666>=</span>./password.txt
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>Secret &#34;db-user-pass&#34; created
</code></pre><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Si la contraseña que está utilizando tiene caracteres especiales como por ejemplo <code>$</code>, <code>\</code>, <code>*</code>, o <code>!</code>, es posible que sean interpretados por tu intérprete de comandos y es necesario escapar cada carácter utilizando <code>\</code> o introduciéndolos entre comillas simples <code>'</code>.
Por ejemplo, si tú password actual es <code>S!B\*d$zDsb</code>, deberías ejecutar el comando de esta manera:</p><p><code>kubectl create Secret generic dev-db-secret --from-literal=username=devuser --from-literal=password=' S\!B*d$zDsb'</code></p><p>No necesita escapar de caracteres especiales en contraseñas de archivos (<code>--from-file</code>).</p></div><p>Puedes comprobar que el Secret se haya creado, así:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get secrets
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>NAME                  TYPE                                  DATA      AGE
db-user-pass          Opaque                                2         51s
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe secrets/db-user-pass
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>Name:            db-user-pass
Namespace:       default
Labels:          &lt;none&gt;
Annotations:     &lt;none&gt;

Type:            Opaque

Data
====
password.txt:    12 bytes
username.txt:    5 bytes
</code></pre><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> <code>kubectl get</code> y <code>kubectl describe</code> evita mostrar el contenido de un Secret por defecto.
Esto es para proteger el Secret de ser expuesto accidentalmente a un espectador, o de ser almacenado en un registro de terminal.</div><p>Ver <a href=#decoding-a-secret>Decodificando un Secret</a> para ver el contenido de un Secret.</p><h4 id=creando-un-secret-manualmente>Creando un Secret Manualmente</h4><p>Puedes crear también un Secret primero en un archivo, en formato json o en yaml, y luego crear ese objeto. El
<a href=/docs/reference/generated/kubernetes-api/v1.12/#secret-v1-core>Secret</a> contiene dos mapas:
data y stringData. El campo de data es usado para almacenar datos arbitrarios, codificado usando base64. El campo stringData se proporciona para su conveniencia, y le permite proporcionar datos secretos como cadenas no codificadas.</p><p>Por ejemplo, para almacenar dos cadenas en un Secret usando el campo data, conviértalos a base64 de la siguiente manera:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> -n <span style=color:#b44>&#39;admin&#39;</span> | base64
</span></span><span style=display:flex><span><span style=color:#b8860b>YWRtaW4</span><span style=color:#666>=</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> -n <span style=color:#b44>&#39;1f2d1e2e67df&#39;</span> | base64
</span></span><span style=display:flex><span>MWYyZDFlMmU2N2Rm
</span></span></code></pre></div><p>Escribe un secret que se vea así:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Opaque<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span>YWRtaW4=<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>password</span>:<span style=color:#bbb> </span>MWYyZDFlMmU2N2Rm<span style=color:#bbb>
</span></span></span></code></pre></div><p>Ahora escribe un Secret usando <a href=/docs/reference/generated/kubectl/kubectl-commands#apply><code>kubectl apply</code></a>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f ./secret.yaml
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>secret &#34;mysecret&#34; created
</code></pre><p>Para ciertos escenarios, es posible que desee utilizar el campo de stringData field en su lugar.
Este campo le permite poner una cadena codificada que no sea base64 directamente en el Secret,
y la cadena será codificada para ti cuando el Secret es creado o actualizado.</p><p>Un ejemplo práctico de esto podría ser donde está implementando una aplicación
que usa un Secret para almacenar un archivo de configuración, y desea completar partes de ese archivo de configuración durante su proceso de implementación.</p><p>Si su aplicación usa el siguiente archivo de configuración:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiUrl</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;https://my.api.com/api/v1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>password</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;password&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Podrías almacenarlo en un Secret usando lo siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Opaque<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>stringData</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>config.yaml</span>:<span style=color:#bbb> </span>|-<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    apiUrl: &#34;https://my.api.com/api/v1&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    username: {{username}}
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    password: {{password}}</span><span style=color:#bbb>    
</span></span></span></code></pre></div><p>Su herramienta de despliegue podría entonces reemplazar el <code>{{username}}</code> y <code>{{password}}</code>
variables de plantilla antes de ejecutar <code>kubectl apply</code>.</p><p>stringData es un campo de conveniencia de solo lectura. Nunca se muestran cuando se recuperan Secrets. Por ejemplo, si ejecuta el siguiente comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get secret mysecret -o yaml
</span></span></code></pre></div><p>La salida será similar a:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>creationTimestamp</span>:<span style=color:#bbb> </span>2018-11-15T20:40:59Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resourceVersion</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;7225&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selfLink</span>:<span style=color:#bbb> </span>/api/v1/namespaces/default/secrets/mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>uid</span>:<span style=color:#bbb> </span>c280ad2e-e916-11e8-98f2-025000000001<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Opaque<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>config.yaml</span>:<span style=color:#bbb> </span>YXBpVXJsOiAiaHR0cHM6Ly9teS5hcGkuY29tL2FwaS92MSIKdXNlcm5hbWU6IHt7dXNlcm5hbWV9fQpwYXNzd29yZDoge3twYXNzd29yZH19<span style=color:#bbb>
</span></span></span></code></pre></div><p>Si se especifica un campo tanto de data y stringData, el valor de StringData
es usado. Por ejemplo, la siguiente definición de Secret:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Opaque<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span>YWRtaW4=<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>stringData</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span>administrator<span style=color:#bbb>
</span></span></span></code></pre></div><p>Los resultado en el siguiente Secret:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>creationTimestamp</span>:<span style=color:#bbb> </span>2018-11-15T20:46:46Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resourceVersion</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;7579&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selfLink</span>:<span style=color:#bbb> </span>/api/v1/namespaces/default/secrets/mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>uid</span>:<span style=color:#bbb> </span>91460ecb-e917-11e8-98f2-025000000001<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Opaque<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span>YWRtaW5pc3RyYXRvcg==<span style=color:#bbb>
</span></span></span></code></pre></div><p>Donde <code>YWRtaW5pc3RyYXRvcg==</code> decodifica a <code>administrator</code>.</p><p>Las llaves de data y stringData deben consistir en caracteres alfanuméricos,
'-', '_' or '.'.</p><p><strong>Nota de codificación:</strong> Los valores serializados JSON y YAML de los datos secretos estan codificadas como cadenas base64. Las nuevas lineas no son válidas dentro de esa cadena y debe ser omitido. Al usar <code>base64</code> en Darwin/macOS, los usuarios deben evitar el uso de la opción <code>-b</code> para dividir líneas largas. Por lo contratio los usuarios de Linux <em>deben</em> añadir la opción <code>-w 0</code> a los comandos <code>base64</code> o al pipeline <code>base64 | tr -d '\n'</code> si la opción <code>-w</code> no esta disponible.</p><h4 id=creando-un-secret-a-partir-de-generador>Creando un Secret a partir de Generador</h4><p>Kubectl soporta <a href=/docs/tasks/manage-kubernetes-objects/kustomization/>managing objects using Kustomize</a>
desde 1.14. Con esta nueva característica,
puedes tambien crear un Secret a partir de un generador y luego aplicarlo para crear el objeto en el Apiserver. Los generadores deben ser especificados en un <code>kustomization.yaml</code> dentro de un directorio.</p><p>Por ejemplo, para generar un Secret a partir de los archivos <code>./username.txt</code> y <code>./password.txt</code></p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># Crear un fichero llamado kustomization.yaml con SecretGenerator</span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>secretGenerator:
</span></span></span><span style=display:flex><span><span style=color:#b44>- name: db-user-pass
</span></span></span><span style=display:flex><span><span style=color:#b44>  files:
</span></span></span><span style=display:flex><span><span style=color:#b44>  - username.txt
</span></span></span><span style=display:flex><span><span style=color:#b44>  - password.txt
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><p>Aplica el directorio kustomization para crear el objeto Secret.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ kubectl apply -k .
</span></span><span style=display:flex><span>secret/db-user-pass-96mffmfh4k created
</span></span></code></pre></div><p>Puedes verificar que el secret fue creado de la siguiente manera:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ kubectl get secrets
</span></span><span style=display:flex><span>NAME                             TYPE                                  DATA      AGE
</span></span><span style=display:flex><span>db-user-pass-96mffmfh4k          Opaque                                <span style=color:#666>2</span>         51s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>$ kubectl describe secrets/db-user-pass-96mffmfh4k
</span></span><span style=display:flex><span>Name:            db-user-pass
</span></span><span style=display:flex><span>Namespace:       default
</span></span><span style=display:flex><span>Labels:          &lt;none&gt;
</span></span><span style=display:flex><span>Annotations:     &lt;none&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Type:            Opaque
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#b8860b>Data</span>
</span></span><span style=display:flex><span><span style=color:#666>====</span>
</span></span><span style=display:flex><span>password.txt:    <span style=color:#666>12</span> bytes
</span></span><span style=display:flex><span>username.txt:    <span style=color:#666>5</span> bytes
</span></span></code></pre></div><p>Por ejemplo, para generar un Secret a partir de literales <code>username=admin</code> y <code>password=secret</code>,
puedes especificar el generador del Secret en <code>kustomization.yaml</code> como:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># Crea un fichero kustomization.yaml con SecretGenerator</span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt;./kustomization.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>secretGenerator:
</span></span></span><span style=display:flex><span><span style=color:#b44>- name: db-user-pass
</span></span></span><span style=display:flex><span><span style=color:#b44>  literals:
</span></span></span><span style=display:flex><span><span style=color:#b44>  - username=admin
</span></span></span><span style=display:flex><span><span style=color:#b44>  - password=secret
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><p>Aplica el directorio kustomization para crear el objeto Secret.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -k .
</span></span><span style=display:flex><span>secret/db-user-pass-dddghtt9b5 created
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El nombre generado del Secret tiene un sufijo agregado al hashing de los contenidos. Esto asegura que se genera un nuevo Secret cada vez que el contenido es modificado.</div><h4 id=decodificando-un-secret>Decodificando un Secret</h4><p>Los Secrets se pueden recuperar a través del comando <code>kubectl get secret</code> . Por ejemplo, para recuperar el Secret creado en la sección anterior:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get secret mysecret -o yaml
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>apiVersion: v1
kind: Secret
metadata:
  creationTimestamp: 2016-01-22T18:41:56Z
  name: mysecret
  namespace: default
  resourceVersion: &#34;164619&#34;
  selfLink: /api/v1/namespaces/default/secrets/mysecret
  uid: cfee02d6-c137-11e5-8d73-42010af00002
type: Opaque
data:
  username: YWRtaW4=
  password: MWYyZDFlMmU2N2Rm
</code></pre><p>Decodifica el campo de contraseña:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b44>&#39;MWYyZDFlMmU2N2Rm&#39;</span> | base64 --decode
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>1f2d1e2e67df
</code></pre><h2 id=usando-secrets>Usando Secrets</h2><p>Los Secrets se pueden montar como volúmenes de datos o ser expuestos como
<a class=glossary-tooltip title='Container environment variables are name=value pairs that provide useful information into containers running in a Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/containers/container-environment/ target=_blank aria-label='variables de entorno'>variables de entorno</a>
para ser usados por un contenedor en un pod. También pueden ser utilizados por otras partes del sistema,
sin estar directamente expuesto en el pod. Por ejemplo, pueden tener credenciales que otras partes del sistema usan para interactuar con sistemas externos en su nombre.</p><h3 id=usando-secrets-como-archivos-de-un-pod>Usando Secrets como Archivos de un Pod</h3><p>Para consumir un Secret en un volumen en un Pod:</p><ol><li>Crear un Secret o usar uno existente. Múltiples pods pueden referenciar el mismo Secret.</li><li>Modifique la definición del Pod para agregar un volumen debajo de <code>.spec.volumes[]</code>. Asigne un nombre al volumen y tenga un campo <code>.spec.volumes[].secret.secretName</code> igual al nombre del objeto del Secret.</li><li>Agrega un <code>.spec.containers[].volumeMounts[]</code> a cada contenedor que necesite un Secret. Especifica <code>.spec.containers[].volumeMounts[].readOnly = true</code> y <code>.spec.containers[].volumeMounts[].mountPath</code> a un nombre de directorio no utilizado donde desea que aparezca los Secrets.</li><li>Modifique la imagen y/o linea de comando para que el programa busque archivos en ese directorio. Cada llave en el <code>data</code> map del los Secrets se convierte en el nombre del archivo bajo <code>mountPath</code>.</li></ol><p>Este es un ejemplo de un pod que monta un Secret en un volumen:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span></code></pre></div><p>Cada Secret que desea usar debe mencionarse en <code>.spec.volumes</code>.</p><p>Si hay múltiples contenedores en un Pod, entonces cada contenedor necesita su propio bloque <code>volumeMounts</code> , pero solo un <code>.spec.volumes</code> se necesita por Secret.</p><p>Puede empaquetar muchos archivos en un Secret, o usar muchos Secrets, lo que sea conveniente.</p><p><strong>Proyección de llaves Secret a rutas específicas</strong></p><p>También podemos controlar las rutas dentro del volumen donde se proyectan las llaves Secrets.
Puede usar el campo <code>.spec.volumes[].secret.items</code> para cambiar la ruta de destino de cada clave:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-username<span style=color:#bbb>
</span></span></span></code></pre></div><p>Lo que sucederá:</p><ul><li>El Secret <code>username</code> se almacena bajo el archivo <code>/etc/foo/my-group/my-username</code> en lugar de <code>/etc/foo/username</code>.</li><li>El Secret <code>password</code> no se proyecta</li></ul><p>Si se utiliza <code>.spec.volumes[].secret.items</code> , solo se proyectan las llaves específicadas en los <code>items</code>.
Para consumir todas las llaves del Secret, Todas deben ser enumeradas en el campo <code>items</code>.
Todas las llaves enumeradas deben existir en el Secret correspondiente. De lo contrario, el volumen no se crea.</p><p><strong>Permisos de archivos Secrets</strong></p><p>Tambien puede especificar el modo de permiso de los archivos de bits que tendrá una parte de un Secret.
Si no especifica ninguno, <code>0644</code> es usado por defecto. Puede especificar un modo predeterminado para todo el volumen del Secret y sobreescribir por llave si es necesario.</p><p>Por ejemplo, puede especificar un modo predeterminado como este:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>defaultMode</span>:<span style=color:#bbb> </span><span style=color:#666>256</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Entonces, el Secret será montado en <code>/etc/foo</code> y todos los archivos creados por el
montaje del volumen del Secret tendrán permiso <code>0400</code>.</p><p>Tenga en cuenta que la especificación JSON no soporta la notación octal, entonces use el valor 256 para
permisos 0400. Si usa yaml en lugar de json para el pod, puede usar notación octal para especificar permisos de una manera más natural.</p><p>También puede usar el mapeo, como en el ejemplo anterior, y especificar diferentes permisos para diferentes archivos como:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>mode</span>:<span style=color:#bbb> </span><span style=color:#666>511</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>En este caso, el archivo resultante en <code>/etc/foo/my-group/my-username</code> tendrá
un valor de permiso <code>0777</code>. Debido a las limitaciones de JSON, debe especificar el modo en notación decimal.</p><p>Tenga en cuenta que este valor de permiso puede mostrarse en notación decimal si lo lee después.</p><p><strong>Consumir Valores Secrets de Volúmenes</strong></p><p>Dentro del contenedor que monta un volumen del Secret, las llaves del Secret aparece como archivos y los valores del Secret son decodificados en base-64 y almacenados dentro de estos archivos.
Este es el resultado de comandos ejecutados dentro del contenedor del ejemplo anterior:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ls /etc/foo/
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>username
password
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat /etc/foo/username
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>admin
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat /etc/foo/password
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>1f2d1e2e67df
</code></pre><p>El programa en un contenedor es responsable de leer los Secrets de los archivos.</p><p><strong>Los Secrets Montados se actualizan automáticamente</strong></p><p>Cuando se actualiza un Secret que ya se está consumiendo en un volumen, las claves proyectadas también se actualizan eventualmente.
Kubelet está verificando si el Secret montado esta actualizado en cada sincronización periódica.
Sin embargo, está usando su caché local para obtener el valor actual del Secret.
El tipo de caché es configurable usando el (campo <code>ConfigMapAndSecretChangeDetectionStrategy</code> en
<a href=https://github.com/kubernetes/kubernetes/blob/main/staging/src/k8s.io/kubelet/config/v1beta1/types.go>KubeletConfiguration struct</a>).
Puede ser propagado por el reloj (default), ttl-based, o simplemente redirigiendo
todas las solicitudes a kube-apiserver directamente.
Como resultado, el retraso total desde el momento en que se actualiza el Secret hasta el momento en que se proyectan las nuevas claves en el Pod puede ser tan largo como el periodo de sincronización de kubelet + retraso de
propagación de caché, donde el retraso de propagación de caché depende del tipo de caché elegido.
(Es igual a mirar el retraso de propagación, ttl of cache, o cero correspondientemente).</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Un contenedor que usa un Secret como
<a href=/docs/concepts/storage/volumes#using-subpath>subPath</a> el montaje del volumen no recibirá actualizaciones de Secret.</div><h3 id=usando-secrets-como-variables-de-entorno>Usando Secrets como Variables de Entorno</h3><p>Para usar un Secret en una <a class=glossary-tooltip title='Container environment variables are name=value pairs that provide useful information into containers running in a Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/containers/container-environment/ target=_blank aria-label='variable de entorno'>variable de entorno</a>
en un pod:</p><ol><li>Crea un Secret o usa uno existente. Múltiples pods pueden hacer referencia a un mismo Secret.</li><li>Modifique la definición de su Pod en cada contenedor que desee consumir el valor de una llave Secret para agregar una variable de entorno para cada llave Secret que deseas consumir. La variable de entorno que consume la llave Secret debe completar el nombre y la llave del Secret en <code>env[].valueFrom.secretKeyRef</code>.</li><li>Modifique su imagen y/o linea de comandos para que el programa busque valores en las variables de entorno especificadas.</li></ol><p>Esto es un ejemplo de un pod que usa Secrets de variables de entorno:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-env-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mycontainer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>SECRET_USERNAME<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secretKeyRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>SECRET_PASSWORD<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secretKeyRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>password<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><p><strong>Consumiendo Valores Secrets a partir de Variables de Entorno</strong></p><p>Dentro de un contenedor que consume un Secret en una variable de entorno, las claves Secrets aparecen como variables de entorno normal que contienen valores decodificados de base-64 de los datos del Secret.
Este es el resultado de comandos ejecutados dentro del contenedor del ejemplo anterior.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$SECRET_USERNAME</span>
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>admin
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$SECRET_PASSWORD</span>
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>1f2d1e2e67df
</code></pre><h3 id=usando-imagepullsecrets>Usando imagePullSecrets</h3><p>Una imagePullSecret es una forma de pasar a kubelet un Secret que contiene las credenciales para un registro de imagenes de Docker (u otro) para que pueda obtener una imagen privada en nombre de su pod.</p><p><strong>Especificar manualmente una imagePullSecret</strong></p><p>El uso de imagePullSecrets se desccriben en la documentación de las imágenes <a href=/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod>images documentation</a></p><h3 id=organización-de-imagepullsecrets-para-que-se-adjunte-automáticamente>Organización de imagePullSecrets para que se Adjunte Automáticamente</h3><p>Puede crear manualmente un imagePullSecret, y hacer referencia a él desde un serviceAccount. Cualquier pod creado con ese serviceAccount
o ese valor predeterminado para usar serviceAccount, obtendrá su campo imagePullSecret
establecido en el service account.
Ver <a href=/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account>Agregar ImagePullSecrets a una cuenta de servicio</a>
para una explicación detallada de ese proceso.</p><h3 id=montaje-automatico-de-secrets-creados-manualmente>Montaje Automatico de Secrets Creados Manualmente</h3><p>Secrets creados Manualmente, (por ejemplo uno que contiene un token para acceder a una cuenta github) se puede conectar automáticamente a los pods según su cuenta de servicio. Vea <a href=/docs/tasks/inject-data-application/podpreset/>Inyección de infromación en pods usando un a PodPreset</a> para una explicación detallada de este proceso.</p><h2 id=detalles>Detalles</h2><h3 id=restricciones>Restricciones</h3><p>Las fuentes del volumen del Secret se validan para garantizar que la referencia del objeto especificado apunte a un objeto de tipo <code>Secret</code>. Por lo tanto, se debe crear un <code>Secret</code> antes de que cualquier pod dependa de él.</p><p>Los objetos API Secret residen en <a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=namespace>namespace</a>.
Solo pueden ser referenciados por pods en el mismo namespace.</p><p>Los <code>Secrets</code> individuales estan limitados a 1MiB de tamaño. Esto es para desalentar la creación de <code>Secrets</code> muy grandes que agotarían la memoria del apiserver y de kubelet.
Sin embargo la creación de muchos Secrets más pequeños también podría agotar la memoria. Límites más completos en el uso de memoria debido a <code>Secret</code> es una característica planificada.</p><p>Kubelet solo admite el uso de <code>Secret</code> para Pods que obtiene del API server. Esto incluye cualquier pods creado usando kubectl, o indirectamente a través de un contralador de replicación.
No incluye pods creados a través de los kubelets
<code>--manifest-url</code> flag, its <code>--config</code> flag, o su REST API (estas no son formas comunes de crear pods.)</p><p>Los Secrets deben crearse antes de que se consuman en pod como variables de entono a menos que estén marcados como optional. Referencias a Secrets que no existen evitarán que el pod inicie.
Las referencias a través de <code>secretKeyRef</code> a claves que no existen en un Secret con nombre evitarán que el pod se inicie.</p><p>Los Secrets que se utilizan para poblar variables de entorno a través de <code>envFrom</code> que tienen claves que se consideran nombres de variables de entorno no validos, tendran esas claves omitidas. El Pod se permitira reiniciar. Habrá un evento cuyo motivo es <code>InvalidVariableNames</code> y el mensaje contendrá la lista de claves no validas que se omitieron. El ejemplo muestra un pod que se refiere al default/mysecret que contiene 2 claves no validas, 1 badkey y 2 alsobad.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get events
</span></span></code></pre></div><pre tabindex=0><code>LASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         TYPE      REASON
0s         0s          1         dapi-test-pod   Pod                                         Warning   InvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.
</code></pre><h3 id=interacción-del-secret-y-pod-de-por-vida>Interacción del Secret y Pod de por vida</h3><p>Cuando se crea un Pod a través de la API, no se verifica que exista un recreto referenciado.
Una vez que se programa el Pod, kubelet intentará obtener el valor del Secret.
Si el Secret no se puede recuperar será por que no existe o por una falla
temporal de conexión al servidor API, kubelet volverá a intentarlo periodicamente.
Enviará un evento sobre el pod explicando las razones por la que aún no se inició.
Una vez que el Secret es encontrado, kubelet creará y montará el volumen que lo contiene.
Ninguno de los contenedorees del pod se iniciará hasta que se monten todos los volúmes del pod.</p><h2 id=casos-de-uso>Casos de uso</h2><h3 id=caso-de-uso-pod-con-llaves-ssh>Caso de Uso: Pod con llaves ssh</h3><p>Cree un fichero kustomization.yaml con SecretGenerator conteniendo algunas llaves ssh:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic ssh-key-secret --from-file<span style=color:#666>=</span>ssh-privatekey<span style=color:#666>=</span>/path/to/.ssh/id_rsa --from-file<span style=color:#666>=</span>ssh-publickey<span style=color:#666>=</span>/path/to/.ssh/id_rsa.pub
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>secret &#34;ssh-key-secret&#34; created
</code></pre><div class="alert alert-warning caution callout" role=alert><strong>Precaución:</strong> Piense detenidamente antes de enviar tus propias llaves ssh: otros usuarios del cluster pueden tener acceso al Secret. Utilice una cuenta de servicio a la que desee que estén accesibles todos los usuarios con los que comparte el cluster de Kubernetes, y pueda revocarlas si se ven comprometidas.</div><p>Ahora podemos crear un pod que haga referencia al Secret con la llave ssh key y lo consuma en un volumen:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-test-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>ssh-key-secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ssh-test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mySshImage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/secret-volume&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Cuando se ejecuta el comando del contenedor, las partes de la llave estarán disponible en:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>/etc/secret-volume/ssh-publickey
</span></span><span style=display:flex><span>/etc/secret-volume/ssh-privatekey
</span></span></code></pre></div><p>El contenedor es libre de usar los datos del Secret para establecer conexión ssh.</p><h3 id=caso-de-uso-pods-con-credenciales-prod-test>Caso de uso: Pods con credenciales prod / test</h3><p>Este ejemplo ilustra un pod que consume un Secret que contiene credenciales de prod y otro pod que consume un Secret con credenciales de entorno de prueba.</p><p>Crear un fichero kustomization.yaml con SecretGenerator</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic prod-db-secret --from-literal<span style=color:#666>=</span><span style=color:#b8860b>username</span><span style=color:#666>=</span>produser --from-literal<span style=color:#666>=</span><span style=color:#b8860b>password</span><span style=color:#666>=</span>Y4nys7f11
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>secret &#34;prod-db-secret&#34; created
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic test-db-secret --from-literal<span style=color:#666>=</span><span style=color:#b8860b>username</span><span style=color:#666>=</span>testuser --from-literal<span style=color:#666>=</span><span style=color:#b8860b>password</span><span style=color:#666>=</span>iluvtests
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>secret &#34;test-db-secret&#34; created
</code></pre><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Caracteres especiales como <code>$</code>, <code>\*</code>, y <code>!</code> requieren ser escapados.
Si el password que estas usando tiene caracteres especiales, necesitas escaparlos usando el caracter <code>\\</code> . Por ejemplo, si tu password actual es <code>S!B\*d$zDsb</code>, deberías ejecutar el comando de esta forma:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic dev-db-secret --from-literal<span style=color:#666>=</span><span style=color:#b8860b>username</span><span style=color:#666>=</span>devuser --from-literal<span style=color:#666>=</span><span style=color:#b8860b>password</span><span style=color:#666>=</span>S<span style=color:#b62;font-weight:700>\\</span>!B<span style=color:#b62;font-weight:700>\\\*</span>d<span style=color:#b62;font-weight:700>\\</span><span style=color:#b8860b>$zDsb</span>
</span></span></code></pre></div><p>No necesitas escapar caracteres especiales en contraseñas de los archivos (<code>--from-file</code>).</p></div><p>Ahora haz los pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: List
</span></span></span><span style=display:flex><span><span style=color:#b44>items:
</span></span></span><span style=display:flex><span><span style=color:#b44>- kind: Pod
</span></span></span><span style=display:flex><span><span style=color:#b44>  apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>  metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>    name: prod-db-client-pod
</span></span></span><span style=display:flex><span><span style=color:#b44>    labels:
</span></span></span><span style=display:flex><span><span style=color:#b44>      name: prod-db-client
</span></span></span><span style=display:flex><span><span style=color:#b44>  spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>    volumes:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: secret-volume
</span></span></span><span style=display:flex><span><span style=color:#b44>      secret:
</span></span></span><span style=display:flex><span><span style=color:#b44>        secretName: prod-db-secret
</span></span></span><span style=display:flex><span><span style=color:#b44>    containers:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: db-client-container
</span></span></span><span style=display:flex><span><span style=color:#b44>      image: myClientImage
</span></span></span><span style=display:flex><span><span style=color:#b44>      volumeMounts:
</span></span></span><span style=display:flex><span><span style=color:#b44>      - name: secret-volume
</span></span></span><span style=display:flex><span><span style=color:#b44>        readOnly: true
</span></span></span><span style=display:flex><span><span style=color:#b44>        mountPath: &#34;/etc/secret-volume&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>- kind: Pod
</span></span></span><span style=display:flex><span><span style=color:#b44>  apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>  metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>    name: test-db-client-pod
</span></span></span><span style=display:flex><span><span style=color:#b44>    labels:
</span></span></span><span style=display:flex><span><span style=color:#b44>      name: test-db-client
</span></span></span><span style=display:flex><span><span style=color:#b44>  spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>    volumes:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: secret-volume
</span></span></span><span style=display:flex><span><span style=color:#b44>      secret:
</span></span></span><span style=display:flex><span><span style=color:#b44>        secretName: test-db-secret
</span></span></span><span style=display:flex><span><span style=color:#b44>    containers:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: db-client-container
</span></span></span><span style=display:flex><span><span style=color:#b44>      image: myClientImage
</span></span></span><span style=display:flex><span><span style=color:#b44>      volumeMounts:
</span></span></span><span style=display:flex><span><span style=color:#b44>      - name: secret-volume
</span></span></span><span style=display:flex><span><span style=color:#b44>        readOnly: true
</span></span></span><span style=display:flex><span><span style=color:#b44>        mountPath: &#34;/etc/secret-volume&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><p>Añade los pods a el mismo fichero kustomization.yaml</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt;&gt; kustomization.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>resources:
</span></span></span><span style=display:flex><span><span style=color:#b44>- pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><p>Aplique todos estos objetos en el Apiserver por</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply --k .
</span></span></code></pre></div><p>Ambos contenedores tendrán los siguientes archivos presentes en sus sistemas de archivos con valores para el entorno de cada contenedor:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>/etc/secret-volume/username
</span></span><span style=display:flex><span>/etc/secret-volume/password
</span></span></code></pre></div><p>observe cómo las especificaciones para los dos pods difieren solo en un campo; esto facilita la creación de pods con diferentes capacidades de una plantilla de configuración de pod común.</p><p>Deberías simplificar aún más la especificación del pod base utilizando dos Cuentas de Servicio:
uno llamado, <code>prod-user</code> con el <code>prod-db-secret</code>, y otro llamado,
<code>test-user</code> con el <code>test-db-secret</code>. Luego, la especificación del pod se puede acortar a, por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prod-db-client-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prod-db-client<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceAccount</span>:<span style=color:#bbb> </span>prod-db-client<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>db-client-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>myClientImage<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=caso-de-uso-dotfiles-en-el-volume-del-secret>Caso de uso: Dotfiles en el volume del Secret</h3><p>Para hacer que los datos esten 'ocultos' (es decir, en un file dónde el nombre comienza con un caracter de punto), simplemente haga que esa clave comience con un punto. Por ejemplo, cuando el siguiente Secret es montado en un volumen:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dotfile-secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>.secret-file</span>:<span style=color:#bbb> </span>dmFsdWUtMg0KDQo=<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-dotfiles-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>dotfile-secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dotfile-test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ls<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;-l&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;/etc/secret-volume&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/secret-volume&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>El <code>secret-volume</code> contendrá un solo archivo, llamado <code>.secret-file</code>, y
el <code>dotfile-test-container</code> tendrá este fichero presente en el path
<code>/etc/secret-volume/.secret-file</code>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los archivos que comiences con caracterers de punto estan ocultos de la salida de <code>ls -l</code>;
tu debes usar <code>ls -la</code> para verlos al enumerar los contenidos del directorio.</div><h3 id=caso-de-uso-secret-visible-para-un-contenedor-en-un-pod>Caso de uso: Secret visible para un contenedor en un pod</h3><p>Considere un programa que necesita manejar solicitudes HTTP, hacer una lógica empresarial compleja y luego firmar algunos mensajes con un HMAC. Debido a que tiene una lógica de aplicación compleja, puede haber una vulnerabilidad de lectura remota de archivos inadvertida en el servidor, lo que podría exponer la clave privada a un atacante.</p><p>Esto podría dividirse en dos procesos en dos contenedores: un contenedor de frontend que maneja la interacción del usuario y la lógica empresarial. pero que no puede ver la clave privada; y un contenedor de firmante que puede ver la clave privada, y responde a solicitudes de firma simples del frontend (ejemplo, a través de redes de localhost).</p><p>Con este enfoque particionado, un atacante ahora tiene que engañar a un servidor de aplicaciones para que haga algo bastante arbitrario, lo que puede ser más difícil que hacer que lea un archivo.</p><h2 id=mejores-prácticas>Mejores prácticas</h2><h3 id=clientes-que-usan-la-api-de-secrets>Clientes que usan la API de Secrets</h3><p>Al implementar aplicaciones que interactuan con los API Secrets, el acceso debe limitarse utilizando <a href=/docs/reference/access-authn-authz/authorization/>authorization policies</a> como <a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>.</p><p>Los Secrets a menudo contienen valores que abarcan un espectro de importancia, muchos de los cuales pueden causar escalamientos dentro de Kubernetes (ejememplo, tokens de cuentas de servicio) y a sistemas externos. Incluso si una aplicación individual puede razonar sobre el poder de los Secrets con los que espera interactuar, otras aplicaciones dentro dle mismo namespace pueden invalidar esos supuestos.</p><p>Por esas razones las solicitudes de <code>watch</code> y <code>list</code> dentro de un espacio de nombres son extremadamente poderosos y deben evitarse, dado que listar Secrets permiten a los clientes inspecionar los valores de todos los Secrets que estan en el namespace. La capacidad para <code>watch</code> and <code>list</code> todos los Secrets en un cluster deben reservarse solo para los componentes de nivel de sistema más privilegiados.</p><p>Las aplicaciones que necesitan acceder a la API de Secrets deben realizar solicitudes de <code>get</code> de los Secrets que necesitan. Esto permite a los administradores restringir el acceso a todos los Secrets mientras <a href=/docs/reference/access-authn-authz/rbac/#referring-to-resources>white-listing access to individual instances</a> que necesita la aplicación.</p><p>Para un mejor rendimiento sobre un bucle <code>get</code>, los clientes pueden diseñar recursos que hacen referencia a un Secret y luego un Secret <code>watch</code> el recurso, al volver a solicitar el Secret cuando cambie la referencia. Además,, un <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/bulk_watch.md>"bulk watch" API</a>
para que los clientes puedan <code>watch</code> recursos individuales, y probablemente estará disponible en futuras versiones de Kubernetes.</p><h2 id=propiedades-de-seguridad>Propiedades de seguridad</h2><h3 id=protecciones>Protecciones</h3><p>Debido a que los objetos <code>Secret</code> se pueden crear independientemente de los <code>Pods</code> que los usan, hay menos riesgo de que el Secret expuesto durante el flujo de trabajo de la creación, visualización, y edición de pods. El sistema también puede tomar precausiones con los objetos<code>Secret</code>, tal como eviar escribirlos en el disco siempre que sea posible.</p><p>Un Secret solo se envía a un nodo si un pod en ese nodo lo requiere. Kubelet almacena el Secret en un <code>tmpfs</code> para que el Secret no se escriba en el almacenamiento de disco. Una vez que se elimina el pod que depende del Secret, kubelet eliminará su copia local de los datos de Secrets.</p><p>Puede haber Secrets para varios Pods en el mismo nodo. Sin embargo, solo los Secrets que solicita un Pod son potencialmente visibles dentro de sus contenedores. Por lo tanto, un Pod no tiene acceso a los Secrets de otro Pod.</p><p>Puede haver varios contenedores en un Pod. Sin embargo, cada contenedor en un pod tiene que solicitar el volumen del Secret en su
<code>volumeMounts</code> para que sea visible dentro del contenedor. Esto se puede usar para construir particiones de seguridad útiles en el Pod level](#use-case-secret-visible-to-one-container-in-a-pod).</p><p>En la mayoría de las distribuciones Kubernetes-project-maintained, la comunicación entre usuario a el apiserver, y del apiserver a kubelets, ista protegido por SSL/TLS.
Los Secrets estan protegidos cuando se transmiten por estos canales.</p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.13 [beta]</code></div><p>Puedes habilitar <a href=/docs/tasks/administer-cluster/encrypt-data/>encryption at rest</a>
para datos secretos, para que los Secrets no se almacenen en claro en <a class=glossary-tooltip title='Almacén de datos persistente, consistente y distribuido de clave-valor utilizado para almacenar toda a la información del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>.</p><h3 id=riesgos>Riesgos</h3><ul><li>En el servidor API, los datos de los Secrets se almacenan en <a class=glossary-tooltip title='Almacén de datos persistente, consistente y distribuido de clave-valor utilizado para almacenar toda a la información del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>; por lo tanto:<ul><li>Los adminsitradores deben habilitar el cifrado en reposo para los datos del cluster (requiere v1.13 o posterior)</li><li>Los administradores deben limitar el acceso a etcd a los usuarios administradores</li><li>Los administradores pueden querer borrar/destruir discos usados por etcd cuando ya no estén en uso</li><li>Si ejecuta etcd en un clúster, los administradores deben asegurarse de usar SSL/TSL para la comunicación entre pares etcd.</li></ul></li><li>Si configura el Secret a través de un archivo de (JSON o YAML) que tiene los datos del Secret codificados como base64, compartir este archivo o registrarlo en un repositorio de origen significa que el Secret está comprometido. La codificación Base64 no es un método de cifrado y se considera igual que un texto plano.</li><li>Las aplicaciones aún necesitan proteger el valor del Secret después de leerlo del volumen, como no registrarlo accidentalmente o transmitirlo a una parte no confiable.</li><li>Un usuario que puede crear un pod que usa un Secret también puede ver el valor del Secret. Incluso si una política del apiserver no permite que ese usuario lea el objeto Secret, el usuario puede ejecutar el pod que expone el Secret.</li><li>Actualmente, cualquier persona con root en cualquier nodo puede leer <em>cualquier</em> secret del apiserver, haciéndose pasar por el kubelet. Es una característica planificada enviar Secrets a los nodos que realmente lo requieran, para restringir el impacto de una explosión de root en un single node.</li></ul><h2 id=siguientes-pasos>Siguientes pasos</h2></div><div class=td-content style=page-break-before:always><h1 id=pg-ab6d20f33ad930a67ee7ef57bff6c75e>7.5 - Organizar el acceso a los clústeres utilizando archivos kubeconfig</h1><p>Utilice los archivos kubeconfig para organizar la información acerca de los clústeres, los
usuarios, los Namespaces y los mecanismos de autenticación. La herramienta de
línea de comandos <code>kubectl</code> utiliza los archivos kubeconfig para hallar la información que
necesita para escoger un clúster y comunicarse con el servidor API de un clúster.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Un archivo utilizado para configurar el acceso a los clústeres se denomina
<em>archivo kubeconfig</em>. Esta es una forma genérica de referirse a los archivos de
configuración. Esto no significa que exista un archivo llamado <code>kubeconfig</code>.</div><p>Por defecto, <code>kubectl</code> busca un archivo llamado <code>config</code> en el directorio <code>$HOME/.kube</code>.
Puedes especificar otros archivos kubeconfig mediante la configuración de la variable
de entorno <code>KUBECONFIG</code> o mediante la configuracion del flag
<a href=/docs/reference/generated/kubectl/kubectl/><code>--kubeconfig</code></a>.</p><p>Para obtener instrucciones paso a paso acerca de cómo crear y especificar los archivos kubeconfig,
consulte el recurso
<a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters>Configurar El Acceso A Múltiples Clústeres</a>.</p><h2 id=compatibilidad-con-múltiples-clústeres-usuarios-y-mecanismos-de-autenticación>Compatibilidad con múltiples clústeres, usuarios y mecanismos de autenticación</h2><p>Suponga que tiene diversos clústeres y que sus usuarios y componentes se autentican
de diversas maneras. Por ejemplo:</p><ul><li>Un kubelet en ejecución se podría autenticar usando certificados.</li><li>Un usuario se podría autenticar utilizando tokens.</li><li>Los administradores podrían tener un conjunto de certificados que sean suministrados a los usuarios individualmente.</li></ul><p>Con los archivos kubeconfig puedes organizar tus clústeres, usuarios y Namespaces.
También puedes definir diferentes contextos para realizar de forma rápida y
fácil cambios entre clústeres y Namespaces.</p><h2 id=contexto>Contexto</h2><p>Un elemento <em>context</em> en un archivo kubeconfig se utiliza para agrupar los parámetros de
acceso bajo un nombre apropiado. Cada contexto tiene tres parámetros: clúster, Namespace
y usuario.
Por defecto, la herramienta de línea de comandos <code>kubectl</code> utiliza los parámetros del
<em>contexto actual</em> para comunicarse con el clúster.</p><p>Para seleccionar el contexto actual:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl config use-context
</span></span></code></pre></div><h2 id=variable-de-entorno-kubeconfig>Variable de entorno KUBECONFIG</h2><p>La variable de entorno <code>KUBECONFIG</code> contiene una lista de archivos kubeconfig.
En el caso de Linux y Mac, la lista está delimitada por dos puntos. Si se trata
de Windows, la lista está delimitada por punto y coma. La variable de entorno
<code>KUBECONFIG</code> no es indispensable. Si la variable de entorno <code>KUBECONFIG</code> no existe,
<code>kubectl</code> utiliza el archivo kubeconfig por defecto <code>$HOME/.kube/config</code>.</p><p>Si la variable de entorno <code>KUBECONFIG</code> existe, <code>kubectl</code> utiliza una
configuración eficiente que es el resultado de la fusión de los archivos
listados en la variable de entorno <code>KUBECONFIG</code>.</p><h2 id=fusionando-archivos-kubeconfig>Fusionando archivos kubeconfig</h2><p>Para poder ver su configuración, escriba el siguiente comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl config view
</span></span></code></pre></div><p>Como se ha descrito anteriormente, la respuesta de este comando podría resultar a partir de un solo
archivo kubeconfig, o podría ser el resultado de la fusión de varios archivos kubeconfig.</p><p>A continuación se muestran las reglas que usa <code>kubectl</code> cuando fusiona archivos kubeconfig:</p><ol><li><p>Si el flag <code>--kubeconfig</code> está activado, usa solamente el archivo especificado. Sin fusionar.
Sólo se permite una instancia con este flag.</p><p>En caso contrario, si la variable de entorno <code>KUBECONFIG</code> está activada, sera usada
como un listado de los archivos a ser fusionados.
Fusionar los archivos listados en la variable de entorno <code>KUBECONFIG</code> de acuerdo
con estas reglas:</p><ul><li>Ignorar nombres de archivo vacíos.</li><li>Producir errores para archivos con contenido que no pueden ser deserializados.</li><li>El primer archivo que establezca un valor particular o una clave se impone.</li><li>Nunca cambie el valor o la clave.
Ejemplo: Conserva el contexto del primer archivo para configurar el <code>contexto actual</code>.
Ejemplo: Si dos archivos especifican un <code>red-user</code>, utilice sólo los valores del primer archivo.
Incluso desechar el segundo archivo aunque tenga registros que no tengan conflictos.</li></ul><p>Para obtener un ejemplo de configuración de la variable de entorno <code>KUBECONFIG</code>, consulte la sección
<a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable>Configuración de la variable de entorno KUBECONFIG</a>.</p><p>En caso contrario, utilice el archivo kubeconfig predeterminado <code>$HOME/.kube/config</code>, sin fusionar.</p></li><li><p>Determinar el contexto a utilizar con base en el primer acierto en esta secuencia:</p><ol><li>Si es que existe, utilice el flag <code>---contexto</code> de la línea de comandos.</li><li>Utilice el <code>contexto actual</code> procedente de los archivos kubeconfig fusionados.</li></ol><p>En este punto se permite un contexto vacío.</p></li><li><p>Determinar el clúster y el usuario. En este caso, puede o no haber un contexto.
Determine el clúster y el usuario con base en el primer acierto que se ejecute dos veces en
esta secuencia: una para el usuario y otra para el clúster:</p><ol><li>Si es que existen, utilice el flag <code>--user</code> o <code>--cluster</code> de la línea de comandos.</li><li>Si el contexto no está vacío, tome el usuario o clúster del contexto.</li></ol><p>En este caso el usuario y el clúster pueden estar vacíos.</p></li><li><p>Determinar la información del clúster a utilizar. En este caso, puede o no haber información del clúster.
Se construye cada pieza de la información del clúster con base en esta secuencia, el primer acierto se impone:</p><ol><li>Si es que existen, use el flag <code>--server</code>, <code>--certificate-authority</code>, <code>--insecure-skip-tls-verify</code> en la línea de comandos.</li><li>Si existen atributos de información de clúster procedentes de los archivos kubeconfig fusionados, utilícelos.</li><li>Falla si no existe la ubicación del servidor.</li></ol></li><li><p>Determinar la información del usuario a utilizar. Cree información de usuario utilizando las mismas reglas que
la información de clúster, con la excepción de permitir sólo un mecanismo de autenticación por usuario:</p><ol><li>Si es que existen, utilice el flag <code>--client-certificate</code>, <code>--client-key</code>, <code>--username</code>, <code>--password</code>, <code>--token</code> de la línea de comandos.</li><li>Utilice los campos <code>user</code> de los archivos kubeconfig fusionados.</li><li>Falla si hay dos mecanismos de autenticación contradictorios.</li></ol></li><li><p>Si todavía falta información, utilice los valores predeterminados y solicite
información de autenticación.</p></li></ol><h2 id=referencias-de-archivos>Referencias de archivos</h2><p>Las referencias, así también como, las rutas de un archivo kubeconfig son relativas a la ubicación del archivo kubeconfig.
Las referencias de un archivo en la línea de comandos son relativas al directorio actual de trabajo.
Dentro de <code>$HOME/.kube/config</code>, las rutas relativas se almacenan de manera relativa a la ubicación del archivo kubeconfig , al igual que las rutas absolutas
se almacenan absolutamente.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li><a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters/>Configurar el acceso a multiples Clústeres</a></li><li><a href=/docs/reference/generated/kubectl/kubectl-commands#config><code>kubectl config</code></a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-712cb3c03ff14a39e5a83a6d9b71d203>8 - Seguridad</h1></div><div class=td-content><h1 id=pg-04eeb110d75afc8acb2cf7a3db743985>8.1 - Vista General de Seguridad Cloud Native</h1><p>Esta descripción general define un modelo para la seguridad de Kubernetes en el contexto de Seguridad en Cloud Native.</p><div class="alert alert-danger warning callout" role=alert><strong>Advertencia:</strong> Este modelo de seguridad en el contenedor brinda sugerencias, no es una prueba de políticas de seguridad de la información.</div><h2 id=las-4c-de-seguridad-en-cloud-native>Las 4C de Seguridad en Cloud Native</h2><p>Puede pensar en seguridad por capas. Las 4C de la seguridad en Cloud Native son la nube (Cloud),
<a class=glossary-tooltip title='Un conjunto de máquinas, llamadas nodos, que ejecutan aplicaciones en contenedores administradas por Kubernetes.' data-toggle=tooltip data-placement=top href='/es/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=Clústeres>Clústeres</a>, <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=Contenedores>Contenedores</a> y Código.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Este enfoque en capas aumenta la <a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defensa en profundidad</a>
de la seguridad, es considerada una buena práctica en seguridad para el software de sistemas.</div><figure><img src=/images/docs/4c.png><figcaption><h4>Las 4C de Seguridad en Cloud Native</h4></figcaption></figure><p>Cada capa del modelo de seguridad Cloud Native es basada en la siguiente capa más externa.
La capa de código se beneficia de una base sólida (nube, clúster, contenedor) de capas seguras.
No podemos garantizar la seguridad aplicando solo seguridad a nivel del código, y usar estándares de seguridad deficientes en las otras capas.</p><h2 id=nube-cloud>Nube (Cloud)</h2><p>En muchos sentidos, la nube (o los servidores o el centro de datos corporativo) es la
<a href=https://es.wikipedia.org/wiki/Base_de_computador_confiable>base de computador confiable</a>
de un clúster de Kubernetes. Si la capa de la nube es vulnerable (o
configurado de alguna manera vulnerable), por consecuencia no hay garantía de que los componentes construidos
encima de la base sean seguros. Cada proveedor de la nube tiene recomendaciones de seguridad
para ejecutar las cargas de trabajo de forma segura en sus entornos.</p><h3 id=seguridad-del-proveedor-de-la-nube>Seguridad del proveedor de la nube</h3><p>Si está ejecutando un clúster de Kubernetes en su propio hardware o en un proveedor de nube diferente,
consulte la documentación para conocer las mejores prácticas de seguridad.
A continuación, algunos enlaces a la documentación de seguridad de los proveedores de nube más populares:</p><table><caption style=display:none>Cloud provider security</caption><thead><tr><th>Proveedor IaaS</th><th>Link</th></tr></thead><tbody><tr><td>Alibaba Cloud</td><td><a href=https://www.alibabacloud.com/trust-center>https://www.alibabacloud.com/trust-center</a></td></tr><tr><td>Amazon Web Services</td><td><a href=https://aws.amazon.com/security/>https://aws.amazon.com/security/</a></td></tr><tr><td>Google Cloud Platform</td><td><a href=https://cloud.google.com/security/>https://cloud.google.com/security/</a></td></tr><tr><td>IBM Cloud</td><td><a href=https://www.ibm.com/cloud/security>https://www.ibm.com/cloud/security</a></td></tr><tr><td>Microsoft Azure</td><td><a href=https://docs.microsoft.com/en-us/azure/security/azure-security>https://docs.microsoft.com/en-us/azure/security/azure-security</a></td></tr><tr><td>Oracle Cloud Infrastructure</td><td><a href=https://www.oracle.com/security/>https://www.oracle.com/security/</a></td></tr><tr><td>VMWare VSphere</td><td><a href=https://www.vmware.com/security/hardening-guides.html>https://www.vmware.com/security/hardening-guides.html</a></td></tr></tbody></table><h3 id=infrastructure-security>Seguridad de la Infraestructura</h3><p>Sugerencias para proteger su infraestructura en un clúster de Kubernetes:</p><table><caption style=display:none>Infrastructure security</caption><thead><tr><th>Área de Interés para la Infraestructura de Kubernetes</th><th>Recomendación</th></tr></thead><tbody><tr><td>Acceso de red al Plano de Control</td><td>Todo acceso público al <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/es/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='plano de control'>plano de control</a> del Kubernetes en Internet no está permitido y es controlado por listas de control de acceso a la red estrictas a un conjunto de direcciones IP necesarias para administrar el clúster.</td></tr><tr><td>Acceso a la red de los Nodos</td><td>Los <a class=glossary-tooltip title='Un Node, nodo en castellano, es una de las máquinas del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodos>nodos</a> deben ser configurados para <em>solo</em> aceptar conexiones (por medio de listas de control de acceso a la red) desde el plano de control en los puertos especificados y aceptar conexiones para servicios en Kubernetes del tipo NodePort y LoadBalancer. Si es posible, estos nodos no deben exponerse públicamente en Internet.</td></tr><tr><td>Acceso a la API de Kubernetes del proveedor de la nube</td><td>Cada proveedor de la nube debe dar un conjunto de permisos al plano de control y nodos del Kubernetes. Es mejor otorgar al clúster el permiso de acceso al proveedor de nube siguiendo el <a href=https://es.wikipedia.org/wiki/Principio_de_m%C3%ADnimo_privilegio>principio de mínimo privilegio</a> para los recursos que necesite administrar. La <a href=https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#iam-roles>documentación del Kops</a> ofrece información sobre las políticas y roles de IAM.</td></tr><tr><td>Acceso a etcd</td><td>El acceso a <a class=glossary-tooltip title='Almacén de datos persistente, consistente y distribuido de clave-valor utilizado para almacenar toda a la información del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a> (banco de datos de Kubernetes) debe ser limitado apenas al plano de control. Dependiendo de su configuración, debería intentar usar etcd sobre TLS. Puede encontrar mas información en la <a href=https://github.com/etcd-io/etcd/tree/master/Documentation>documentación de etcd</a>.</td></tr><tr><td>Encriptación etcd</td><td>Siempre que sea posible, es una buena práctica encriptar todas las unidades de almacenamiento. Etcd mantiene el estado de todo el clúster (incluidos los Secretos), por lo que su disco debe estar encriptado.</td></tr></tbody></table><h2 id=clúster>Clúster</h2><p>Existen dos áreas de preocupación para proteger Kubernetes:</p><ul><li>Protección de las configuraciones de los componentes del clúster.</li><li>Protección de las aplicaciones que se ejecutan en el clúster.</li></ul><h3 id=cluster-components>Componentes del Clúster</h3><p>Si desea proteger su clúster de accesos accidentales o maliciosos y adoptar
buenas prácticas de seguridad, a continuación sigue estos consejos sobre
<a href=/docs/tasks/administer-cluster/securing-a-cluster/>como proteger el clúster</a>.</p><h3 id=cluster-applications>Componentes del clúster (su aplicación)</h3><p>Dependiendo de la superficie de ataque de su aplicación, es posible que desee concentrarse en
temas de seguridad específicos. Por ejemplo: si está ejecutando un servicio (Servicio A) que es crítico
en una cadena de otros recursos y otra carga de trabajo separada (Servicio B) que es
vulnerable a un ataque de sobrecarga de recursos, el riesgo de comprometer el Servicio A
es alto si no limita las funciones del Servicio B. La siguiente tabla enumera
áreas de atención de seguridad y recomendaciones para proteger las cargas de trabajo que se ejecutan en Kubernetes:</p><table><thead><tr><th>Áreas para la seguridad de la carga del trabajo</th><th>Recomendación</th></tr></thead><tbody><tr><td>Autorización RBAC (acceso a la API Kubernetes)</td><td><a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/>https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a></td></tr><tr><td>Autenticación</td><td><a href=https://kubernetes.io/docs/concepts/security/controlling-access/>https://kubernetes.io/docs/concepts/security/controlling-access/</a></td></tr><tr><td>Administrar secretos en la aplicación (encriptar el etcd - dato en reposo)</td><td><a href=https://kubernetes.io/docs/concepts/configuration/secret/>https://kubernetes.io/docs/concepts/configuration/secret/</a><br><a href=https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/</a></td></tr><tr><td>Políticas de seguridad de Pod</td><td><a href=https://kubernetes.io/docs/concepts/policy/pod-security-policy/>https://kubernetes.io/docs/concepts/policy/pod-security-policy/</a></td></tr><tr><td>Calidad de servicio (y gestión de recursos del clúster)</td><td><a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/>https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/</a></td></tr><tr><td>Políticas de Red</td><td><a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>https://kubernetes.io/docs/concepts/services-networking/network-policies/</a></td></tr><tr><td>TLS para Kubernetes Ingress</td><td><a href=https://kubernetes.io/docs/concepts/services-networking/ingress/#tls>https://kubernetes.io/docs/concepts/services-networking/ingress/#tls</a></td></tr></tbody></table><h2 id=contenedor>Contenedor</h2><p>La seguridad de los contenedores está fuera del alcance de la guía. Aquí hay recomendaciones generales y
enlaces para explorar este tema:</p><table><thead><tr><th>Área de Interés para Contenedores</th><th>Recomendación</th></tr></thead><tbody><tr><td>Escáneres de vulnerabilidad de contenedores y seguridad de dependencia del sistema operativo</td><td>Como parte del paso de la creación de la imagen, se debe utilizar un escáner de contenedores para detectar vulnerabilidades.</td></tr><tr><td>Firma de Imágenes y Aplicación</td><td>Firma de imágenes de contenedores para mantener un sistema confiable para el contenido de sus contenedores.</td></tr><tr><td>Prohibir Usuarios Privilegiados</td><td>Al crear contenedores, consulte la documentación para crear usuarios dentro de los contenedores con el menor privilegio necesario para cumplir con el propósito del contenedor en el sistema operativo.</td></tr><tr><td>Utilice el contenedor de tiempo de ejecución con el aislamiento más fuerte</td><td>Seleccione <a href=/docs/concepts/containers/runtime-class/>clases del contenedor runtime</a> con el proveedor de aislamiento más fuerte.</td></tr></tbody></table><h2 id=código>Código</h2><p>El código de la aplicación es una de las principales superficies de ataque sobre las que tenemos más control.
Aunque la protección del código de la aplicación está fuera del tema de seguridad de Kubernetes, aquí algunas
recomendaciones para proteger el código de su aplicación:</p><h3 id=seguridad-del-código>Seguridad del código</h3><table><caption style=display:none>Code security</caption><thead><tr><th>Áreas de Atención para el Código</th><th>Recomendación</th></tr></thead><tbody><tr><td>Acceso solo a través de TLS</td><td>Si su código necesita comunicarse a través de TCP, ejecute un handshake TLS con el cliente anticipadamente. Con la excepción de algunos casos, encripte todo lo que está en tránsito. Yendo un paso más allá, es una buena idea cifrar el tráfico de red entre los servicios. Esto se puede hacer a través del proceso de autenticación mutua o <a href=https://en.wikipedia.org/wiki/Mutual_authentication>mTLS</a>, que realiza una verificación bilateral de la comunicación a través de los certificados en los servicios.</td></tr><tr><td>Limitación de rangos de puertos de comunicación</td><td>Esta recomendación puede ser un poco evidente, pero siempre que sea posible, solo debe exponer los puertos de su servicio que son absolutamente esenciales para la comunicación o la recopilación de métricas.</td></tr><tr><td>Seguridad en dependencia de terceros</td><td>Es una buena práctica comprobar periódicamente las bibliotecas de terceros de su aplicación en busca de vulnerabilidades de seguridad. Cada lenguaje de programación tiene una herramienta para realizar esta verificación de forma automática.</td></tr><tr><td>Análisis de código estático</td><td>La mayoría de los lenguajes proporcionan una forma de analizar el código en busca de prácticas de codificación potencialmente inseguras. Siempre que sea posible, debe automatizar los escaneos utilizando herramientas que puedan escanear las bases del código en busca de errores de seguridad comunes. Algunas de las herramientas se pueden encontrar en <a href=https://owasp.org/www-community/Source_Code_Analysis_Tools>OWASP Source Code Analysis Tools</a>.</td></tr><tr><td>Ataques de sondeo dinámico</td><td>Existen algunas herramientas automatizadas que puede ejecutar en su servicio para explorar algunos de los ataques más conocidos. Esto incluye la inyección de SQL, CSRF y XSS. Una de las herramientas de análisis dinámico más populares es la <a href=https://owasp.org/www-project-zap/>OWASP Zed Attack proxy</a>.</td></tr></tbody></table><h2 id=siguientes-pasos>Siguientes pasos</h2><p>Obtenga más información sobre los temas de seguridad de Kubernetes:</p><ul><li><a href=/docs/concepts/security/pod-security-standards/>Estándares de seguridad del pod</a></li><li><a href=/docs/concepts/services-networking/network-policies/>Políticas de red para pods</a></li><li><a href=/docs/concepts/security/controlling-access>Control de acceso a la API de Kubernetes</a></li><li><a href=/docs/tasks/administer-cluster/securing-a-cluster/>Protegiendo su clúster</a></li><li><a href=/docs/tasks/tls/managing-tls-in-a-cluster/>Criptografía de datos en tránsito</a></li><li><a href=/docs/tasks/administer-cluster/encrypt-data/>Criptografía de datos en reposo</a></li><li><a href=/docs/concepts/configuration/secret/>Secretos en Kubernetes</a></li><li><a href=/docs/concepts/containers/runtime-class>Runtime class</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4d77d1ae4c06aa14f54b385191627881>8.2 - Controlando el Acceso a la API de Kubernetes</h1><p>Esta página proporciona información sobre cómo controlar el acceso a la API de Kubernetes.</p><p>Los usuarios acceden a la <a href=/docs/concepts/overview/kubernetes-api/>API de Kubernetes</a> usando <code>kubectl</code>,
bibliotecas de cliente, o haciendo peticiones REST. Usuarios y
<a href=/docs/tasks/configure-pod-container/configure-service-account/>Kubernetes service accounts</a> pueden ser
autorizados para acceder a la API.
Cuando una petición llega a la API, pasa por varias etapas, están ilustradas en el
siguiente diagrama:</p><p><img src=/images/docs/admin/access-control-overview.svg alt="Diagrama de pasos para una petición a la API de Kubernetes"></p><h2 id=seguridad-en-la-capa-de-transporte>Seguridad en la capa de transporte</h2><p>En un <a class=glossary-tooltip title='Un conjunto de máquinas, llamadas nodos, que ejecutan aplicaciones en contenedores administradas por Kubernetes.' data-toggle=tooltip data-placement=top href='/es/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=cluster>cluster</a> típico de Kubernetes, la API sirve peticiones en el puerto 443, protegida por TLS.
El <a class=glossary-tooltip title='Componente del plano de control que expone la API de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-apiserver/ target=_blank aria-label='API Server'>API Server</a> presenta un certificado. Este certificado puede ser firmando usando
un certificado de autoridad privada (CA) o basado en una llave pública relacionada
generalmente a un CA reconocido.</p><p>Si el cluster usa un certificado de autoridad privado, se necesita copiar este certificado
CA configurado dentro de su <code>~/.kube/config</code> en el cliente, entonces se podrá
confiar en la conexión y estar seguro que no será comprometida.</p><p>El cliente puede presentar un certificado TLS de cliente en esta etapa.</p><h2 id=autenticación>Autenticación</h2><p>Una vez que se estableció la conexión TLS, las peticiones HTTP avanzan a la etapa de autenticación.
Esto se muestra en el paso 1 del diagrama.
El script de creación del cluster o el administrador del cluster puede configurar el <a class=glossary-tooltip title='Componente del plano de control que expone la API de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-apiserver/ target=_blank aria-label='API Server'>API Server</a> para ejecutar
uno o mas módulos de autenticación.
Los Autenticadores están descritos con más detalle en
<a href=/docs/reference/access-authn-authz/authentication/>Authentication</a>.</p><p>La entrada al paso de autenticación es la petición HTTP completa, aun así, esta tipicamente
examina las cabeceras y/o el certificado del cliente.</p><p>Los modulos de autenticación incluyen certificado de cliente, contraseña, tokens planos,
tokens de inicio y JSON Web Tokens (usados para los service accounts).</p><p>Múltiples módulos de autenticación puede ser especificados, en este caso cada uno es probado secuencialmente,
hasta que uno de ellos tiene éxito.</p><p>Si la petición no puede ser autenticada, la misma es rechazada con un código HTTP 401.
Si la autenticación tiene éxito, el usuario es validado con el <code>username</code> específico, y el nombre de usuario
esta disponible para los pasos siguientes. Algunos autenticadores
también proporcionan membresías de grupo al usuario, mientras que otros
no lo hacen.</p><p>Aunque Kubernetes utiliza los nombres de usuario para tomar decisiones durante el control de acceso y para registrar las peticiones de entrada, no tiene un objeto <code>User</code> ni tampoco almacena información sobre los usuarios en la API.</p><h2 id=autorización>Autorización</h2><p>Después de autenticar la petición como proveniente de un usuario específico, la petición debe ser autorizada. Esto se muestra en el paso 2 del diagrama.</p><p>Una petición debe incluir el nombre de usuario solicitante, la acción solicitada y el objeto afectado por la acción. La petición es autorizada si hay una política existente que declare que el usuario tiene permisos para la realizar la acción.</p><p>Por ejemplo, si el usuario Bob tiene la siguiente política, entonces puede leer pods solamente en el namespace <code>projectCaribou</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;abac.authorization.kubernetes.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Policy&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;spec&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;user&#34;</span>: <span style=color:#b44>&#34;bob&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;projectCaribou&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;pods&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;readonly&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Si Bob hace la siguiente petición, será autorizada dado que tiene permitido leer los objetos en el namespace <code>projectCaribou</code> :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;authorization.k8s.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;SubjectAccessReview&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;spec&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;resourceAttributes&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;projectCaribou&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;verb&#34;</span>: <span style=color:#b44>&#34;get&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;group&#34;</span>: <span style=color:#b44>&#34;unicorn.example.org&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;pods&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>En cambio, si Bob en su petición intenta escribir (<code>create</code> o <code>update</code>) en los objetos del namespace <code>projectCaribou</code>, la petición será denegada. Del mismo modo, si Bob hace una petición para leer (<code>get</code>) objetos en otro namespace como <code>projectFish</code>, la autorización también será denegada.</p><p>Las autorizaciones en Kubernetes requieren que se usen atributos REST comunes para interactuar con el existente sistema de control de toda la organización o del proveedor cloud. Es importante usar formatos REST porque esos sistemas de control pueden interactuar con otras APIs además de la API de Kubernetes.</p><p>Kubernetes soporta múltiples módulos de autorización, como el modo ABAC, el modo RBAC y el modo Webhook. Cuando un administrador crea un cluster, se realiza la configuración de los módulos de autorización que deben ser usados con la API del server. Si más de uno módulo de autorización es configurado, Kubernetes verificada cada uno y si alguno de ellos autoriza la petición entonces la misma se ejecuta. Si todos los modules deniegan la petición, entonces la misma es denegada (Con un error HTTP con código 403).</p><p>Para leer más acerca de las autorizaciones en Kubernetes, incluyendo detalles sobre cómo crear politicas usando los módulos de autorización soportados, vea <a href=/docs/reference/access-authn-authz/authorization/>Authorization</a>.</p><h2 id=control-de-admisión>Control de Admisión</h2><p>Los módulos de Control de Admisión son módulos de software que solo pueden modificar o rechazar peticiones.
Adicionalmente a los atributos disponibles en los módulos de Autorización, los de
Control de Admisión pueden acceder al contenido del objeto que esta siendo creado o modificado.</p><p>Los Controles de Admisión actúan en las peticiones que crean, modifican, borran o se conectan (proxy) a un objeto.
Cuando múltiples módulos de control de admisión son configurados, son llamados en orden.</p><p>Esto se muestra en el paso 3 del diagrama.</p><p>A diferencia de los módulos de Autorización y Autenticación, si uno de los módulos de control de admisión
rechaza la petición, entonces es inmediatamente rechazada.</p><p>Adicionalmente a rechazar objetos, los controles de admisión también permiten establecer
valores predeterminados complejos.</p><p>Los módulos de Control de Admisión disponibles están descritos en <a href=/docs/reference/access-authn-authz/admission-controllers/>Admission Controllers</a>.</p><p>Cuando una petición pasa todos los controles de admisión, esta es validada usando la rutinas de validación
para el objeto API correspondiente y luego es escrita en el objeto.</p><h2 id=puertos-e-ips-del-api-server>Puertos e IPs del API server</h2><p>La discusión previa aplica a peticiones enviadas a un puerto seguro del servidor API
(el caso típico). El servidor API puede en realidad servir en 2 puertos:</p><p>Por defecto, la API de Kubernetes entrega HTTP en 2 puertos:</p><ol><li><p>puerto <code>localhost</code>:</p><ul><li>debe usarse para testeo e iniciar el sistema y para otros componentes del nodo maestro
(scheduler, controller-manager) para hablar con la API</li><li>no se usa TLS</li><li>el puerto predeterminado es el <code>8080</code></li><li>la IP por defecto es localhost, la puede cambiar con el flag <code>--insecure-bind-address</code>.</li><li>la petición no pasa por los mecanismos de autenticación ni autorización</li><li>peticiones controladas por los modulos de control de admisión.</li><li>protegidas por necesidad para tener acceso al host</li></ul></li><li><p>“Puerto seguro”:</p><ul><li>usar siempre que sea posible</li><li>usa TLS. Se configura el certificado con el flag <code>--tls-cert-file</code> y la clave con <code>--tls-private-key-file</code>.</li><li>el puerto predeterminado es <code>6443</code>, se cambia con el flag <code>--secure-port</code>.</li><li>la IP por defecto es la primer interface que no es la localhost. se cambia con el flag <code>--bind-address</code>.</li><li>peticiones controladas por los módulos de autenticación y autorización.</li><li>peticiones controladas por los módulos de control de admisión.</li></ul></li></ol><h2 id=siguientes-pasos>Siguientes pasos</h2><p>En los siguientes enlaces, encontrará mucha más documentación sobre autenticación, autorización y el control de acceso a la API:</p><ul><li><p><a href=/docs/reference/access-authn-authz/authentication/>Authenticating</a></p><ul><li><a href=/docs/reference/access-authn-authz/bootstrap-tokens/>Authenticating with Bootstrap Tokens</a></li></ul></li><li><p><a href=/docs/reference/access-authn-authz/admission-controllers/>Admission Controllers</a></p><ul><li><a href=/docs/reference/access-authn-authz/extensible-admission-controllers/>Dynamic Admission Control</a></li></ul></li><li><p><a href=/docs/reference/access-authn-authz/authorization/>Authorization</a></p><ul><li><a href=/docs/reference/access-authn-authz/rbac/>Role Based Access Control</a></li><li><a href=/docs/reference/access-authn-authz/abac/>Attribute Based Access Control</a></li><li><a href=/docs/reference/access-authn-authz/node/>Node Authorization</a></li><li><a href=/docs/reference/access-authn-authz/webhook/>Webhook Authorization</a></li></ul></li><li><p><a href=/docs/reference/access-authn-authz/certificate-signing-requests/>Certificate Signing Requests</a></p><ul><li>including <a href=/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection>CSR approval</a>
and <a href=/docs/reference/access-authn-authz/certificate-signing-requests/#signing>certificate signing</a></li></ul></li><li><p>Service accounts</p><ul><li><a href=/docs/tasks/configure-pod-container/configure-service-account/>Developer guide</a></li><li><a href=/docs/reference/access-authn-authz/service-accounts-admin/>Administration</a></li></ul></li><li><p>Como los pods pueden usar
<a href=/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials>Secrets</a>
para obtener credenciales para la API.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ac9161c6d952925b083ad9602b4e8e7f>9 - Políticas</h1><div class=lead>Políticas configurables que se aplican a grupos de recursos.</div><p>La sección de Políticas describe las diferentes políticas configurables que se aplican a grupos de recursos:</p></div><div class=td-content><h1 id=pg-a935ff8c59eb116b43494255cc67f69a>9.1 - Rangos de límites (Limit Ranges)</h1><div class=lead>Aplica límites de recursos a un Namespace para restringir y garantizar la asignación y consumo de recursos informáticos.</div><h3 id=contexto>Contexto</h3><p>Por defecto, los contenedores se ejecutan sin restricciones sobre los <a href=/docs/concepts/configuration/manage-resources-containers/>recursos informáticos disponibles en un clúster de Kubernetes</a>.
Si el <a class=glossary-tooltip title='Un Node, nodo en castellano, es una de las máquinas del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodo>Nodo</a> dispone de los recursos informáticos, un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> o sus <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=Contenedores>Contenedores</a> tienen permitido consumir por encima de la cuota solicitada si no superan el límite establecido en su especificación.
Existe la preocupación de que un Pod o Contenedor pueda monopolizar todos los recursos disponibles.</p><h3 id=utilidad>Utilidad</h3><p>Aplicando restricciones de asignación de recursos, los administradores de clústeres se aseguran del cumplimiento del consumo de recursos por espacio de nombre (<a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=Namespace>Namespace</a>).</p><p>Un <strong><a class=glossary-tooltip title='Proporciona restricciones para limitar el consumo de recursos por Contenedores o Pods en un espacio de nombres' data-toggle=tooltip data-placement=top href=/docs/concepts/policy/limit-range/ target=_blank aria-label=LimitRange>LimitRange</a></strong> es la política que permite:</p><ul><li>Imponer restricciones de requisitos de recursos a <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> o <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=Contenedores>Contenedores</a> por Namespace.</li><li>Imponer las limitaciones de recursos mínimas/máximas para Pods o Contenedores dentro de un Namespace.</li><li>Especificar requisitos y límites de recursos predeterminados para Pods o Contenedores de un Namespace.</li><li>Imponer una relación de proporción entre los requisitos y el límite de un recurso.</li><li>Imponer el cumplimiento de las demandas de almacenamiento mínimo/máximo para <a class=glossary-tooltip title='Reserva el recurso de almacenamiento definido en un PersistentVolume para poderlo montar como un volúmen en un contenedor.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label='Solicitudes de Volúmenes Persistentes'>Solicitudes de Volúmenes Persistentes</a>.</li></ul><h3 id=habilitar-el-limitrange>Habilitar el LimitRange</h3><p>La compatibilidad con LimitRange está habilitada por defecto en Kubernetes desde la versión 1.10.</p><p>Para que un LimitRange se active en un <a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=Namespace>Namespace</a> en particular, el LimitRange debe definirse con el Namespace, o aplicarse a éste.</p><p>El nombre de recurso de un objeto LimitRange debe ser un
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nombre de subdominio DNS</a> válido.</p><h3 id=aplicando-limitranges>Aplicando LimitRanges</h3><ul><li>El administrador crea un LimitRange en un <a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=Namespace>Namespace</a>.</li><li>Los usuarios crean recursos como <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>, <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=Contenedores>Contenedores</a> o <a class=glossary-tooltip title='Reserva el recurso de almacenamiento definido en un PersistentVolume para poderlo montar como un volúmen en un contenedor.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label='Solicitudes de Volúmenes Persistentes'>Solicitudes de Volúmenes Persistentes</a> en el Namespace.</li><li>El controlador de admisión <code>LimitRanger</code> aplicará valores predeterminados y límites, para todos los Pods o Contenedores que no establezcan requisitos de recursos informáticos. Y realizará un seguimiento del uso para garantizar que no excedan el mínimo, el máximo, y la proporción de ningún LimitRange definido en el Namespace.</li><li>Si al crear o actualizar un recurso del ejemplo (Pods, Contenedores, <a class=glossary-tooltip title='Reserva el recurso de almacenamiento definido en un PersistentVolume para poderlo montar como un volúmen en un contenedor.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label='Solicitudes de Volúmenes Persistentes'>Solicitudes de Volúmenes Persistentes</a>) se viola una restricción al LimitRange, la solicitud al servidor API fallará con un código de estado HTTP "403 FORBIDDEN" y un mensaje que explica la restricción que se ha violado.</li><li>En caso de que en se active un LimitRange para recursos de cómputos como <code>cpu</code> y <code>memory</code>, los usuarios deberán especificar los requisitos y/o límites de recursos a dichos valores. De lo contrario, el sistema puede rechazar la creación del Pod.</li><li>Las validaciones de LimitRange ocurren solo en la etapa de Admisión de Pod, no en Pods que ya se han iniciado (Running <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>).</li></ul><p>Algunos ejemplos de políticas que se pueden crear utilizando rangos de límites son:</p><ul><li>En un clúster de 2 nodos con una capacidad de 8 GiB de RAM y 16 núcleos, podría restringirse los <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> en un <a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=Namespace>Namespace</a> a requerir <code>100m</code> de CPU con un límite máximo de <code>500m</code> para CPU y requerir <code>200Mi</code> de memoria con un límite máximo de <code>600Mi</code> de memoria.</li><li>Definir el valor por defecto de límite y requisitos de CPU a <code>150m</code> y el valor por defecto de requisito de memoria a <code>300Mi</code> <a class=glossary-tooltip title='Una imagen ligera y portátil que contiene un software y todas sus dependencias.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=Contenedores>Contenedores</a> que se iniciaron sin requisitos de CPU y memoria en sus especificaciones.</li></ul><p>En el caso de que los límites totales del <a class=glossary-tooltip title='Abstracción utilizada por Kubernetes para soportar múltiples clústeres virtuales en el mismo clúster físico.' data-toggle=tooltip data-placement=top href=/es/docs/concepts/overview/working-with-objects/namespaces/ target=_blank aria-label=Namespace>Namespace</a> sean menores que la suma de los límites de los <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>,
puede haber contienda por los recursos. En este caso, los contenedores o pods no seran creados.</p><p>Ni la contención ni los cambios en un LimitRange afectarán a los recursos ya creados.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><p>Consulte el <a href=https://git.k8s.io/design-proposals-archive/resource-management/admission_control_limit_range.md>documento de diseño del LimitRanger</a> para más información.</p><p>Los siguientes ejemplos utilizan límites y están pendientes de su traducción:</p><ul><li><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>how to configure minimum and maximum CPU constraints per namespace</a>.</li><li><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>how to configure minimum and maximum Memory constraints per namespace</a>.</li><li><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>how to configure default CPU Requests and Limits per namespace</a>.</li><li><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>how to configure default Memory Requests and Limits per namespace</a>.</li><li><a href=/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage>how to configure minimum and maximum Storage consumption per namespace</a>.</li><li><a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>a detailed example on configuring quota per namespace</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-285a3785fd3d20f437c28d87ca4dadca>10 - Administración del Clúster</h1></div><div class=td-content style=page-break-before:always><h1 id=pg-7e0d97616b15e2c383c6a0a96ec442cb>11 - Extendiendo Kubernetes</h1></div><div class=td-content><h1 id=pg-0af41d3bd7c785621b58b7564793396a>11.1 - Extendiendo la API de Kubernetes</h1></div><div class=td-content style=page-break-before:always><h1 id=pg-c8937cdc9df96f3328becf04f8211292>11.2 - Extensiones de computación, almacenamiento y redes</h1></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/es/docs/home/>Home</a>
<a class=text-white href=/es/blog/>Blog</a>
<a class=text-white href=/es/partners/>Partners</a>
<a class=text-white href=/es/community/>Comunidad</a>
<a class=text-white href=/es/case-studies/>Casos de éxito</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 Los autores de Kubernetes | Documentación distribuida bajo <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. Todos los derechos reservados. The Linux Foundation tiene marcas registradas y utiliza marcas registradas. Para obtener una lista de marcas registradas por The Linux Foundation, visita <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>