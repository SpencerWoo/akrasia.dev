<!doctype html><html lang=es class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/workloads/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/workloads/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/workloads/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/workloads/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/workloads/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/workloads/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/workloads/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/concepts/workloads/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/es/docs/concepts/workloads/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Cargas de trabajo | Kubernetes</title><meta property="og:title" content="Cargas de trabajo"><meta property="og:description" content="Orquestación de contenedores para producción"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/es/docs/concepts/workloads/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Cargas de trabajo"><meta itemprop=description content="Orquestación de contenedores para producción"><meta name=twitter:card content="summary"><meta name=twitter:title content="Cargas de trabajo"><meta name=twitter:description content="Orquestación de contenedores para producción"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/es/docs/concepts/workloads/"><meta property="og:title" content="Cargas de trabajo"><meta name=twitter:title content="Cargas de trabajo"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/es/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/es/docs/>Documentación</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/es/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/es/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/es/community/>Comunidad</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/es/case-studies/>Casos de éxito</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/es/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/es/docs/concepts/workloads/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/es/docs/concepts/workloads/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/es/docs/concepts/workloads/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/es/docs/concepts/workloads/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/es/docs/concepts/workloads/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Español (Spanish)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/workloads/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/workloads/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/workloads/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/workloads/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/workloads/>Français (French)</a>
<a class=dropdown-item href=/de/docs/concepts/workloads/>Deutsch (German)</a>
<a class=dropdown-item href=/id/docs/concepts/workloads/>Bahasa Indonesia</a>
<a class=dropdown-item href=/uk/docs/concepts/workloads/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Versión imprimible multipagina.
<a href=# onclick="return print(),!1">Haga click aquí para imprimir</a>.</p><p><a href=/es/docs/concepts/workloads/>Volver a la vista normal de esta página</a>.</p></div><h1 class=title>Cargas de trabajo</h1><ul><li>1: <a href=#pg-4d68b0ccf9c683e6368ffdcc40c838d4>Pods</a></li><ul><li>1.1: <a href=#pg-99cce294fe789317ee684a6e1f07f20f>Pods</a></li><li>1.2: <a href=#pg-1ccbd4eeded6ab138d98b59175bd557e>Contenedores de Inicialización</a></li><li>1.3: <a href=#pg-4e9b9cbc9776b12e7335c53da377c9c8>Pod Preset</a></li><li>1.4: <a href=#pg-4aaf43c715cd764bc8ed4436f3537e68>Interrupciones</a></li><li>1.5: <a href=#pg-53a1005011e1bda2ce81819aad7c8b32>Containers Efímeros</a></li></ul><li>2: <a href=#pg-89637410cacae45a36ab1cc278c482eb>Controladores</a></li><ul><li>2.1: <a href=#pg-d459b930218774655fa7fd1620625539>ReplicaSet</a></li><li>2.2: <a href=#pg-27f1331d515d95f76aa1156088b4ad91>ReplicationController</a></li><li>2.3: <a href=#pg-a2dc0393e0c4079e1c504b6429844e86>Deployment</a></li><li>2.4: <a href=#pg-6d72299952c37ca8cc61b416e5bdbcd4>StatefulSets</a></li><li>2.5: <a href=#pg-41600eb8b6631c88848156f381e9d588>DaemonSet</a></li><li>2.6: <a href=#pg-9add0d2120634b63073ad08dc8683bd6>Recolección de Basura</a></li><li>2.7: <a href=#pg-4de50a37ebb6f2340484192126cb7a04>Controlador TTL para Recursos Finalizados</a></li><li>2.8: <a href=#pg-230b370ed7a1dedf04163f02fa701802>Jobs - Ejecución hasta el final</a></li><li>2.9: <a href=#pg-2e4cec01c525b45eccd6010e21cc76d9>CronJob</a></li></ul></ul><div class=content></div></div><div class=td-content><h1 id=pg-4d68b0ccf9c683e6368ffdcc40c838d4>1 - Pods</h1></div><div class=td-content><h1 id=pg-99cce294fe789317ee684a6e1f07f20f>1.1 - Pods</h1><p>Los <em>Pods</em> son las unidades de computación desplegables más pequeñas que se pueden crear y gestionar en Kubernetes.</p><h2 id=qué-és-un-pod>¿Qué és un Pod?</h2><p>Un <em>Pod</em> (como en una vaina de ballenas o vaina de guisantes) es un grupo de uno o más contenedores (como contenedores Docker), con almacenamiento/red compartidos, y unas especificaciones de cómo ejecutar los contenedores. Los contenidos de un Pod son siempre coubicados, coprogramados y ejecutados en un contexto compartido. Un Pod modela un "host lógico" específico de la aplicación: contiene uno o más contenedores de aplicaciones relativamente entrelazados. Antes de la llegada de los contenedores, ejecutarse en la misma máquina física o virtual significaba ser ejecutado en el mismo host lógico.</p><p>Mientras que Kubernetes soporta más <a class=glossary-tooltip title='El Container Runtime, entorno de ejecución de un contenedor, es el software responsable de ejecutar contenedores.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='runtimes de contenedores'>runtimes de contenedores</a> a parte de Docker, este último es el más conocido y ayuda a describir Pods en términos de Docker.</p><p>El contexto compartido de un Pod es un conjunto de namespaces de Linux, cgroups y, potencialmente, otras facetas de aislamiento, las mismas cosas que aíslan un contenedor Docker. Dentro del contexto de un Pod, las aplicaciones individuales pueden tener más subaislamientos aplicados.</p><p>Los contenedores dentro de un Pod comparten dirección IP y puerto, y pueden encontrarse a través de <code>localhost</code>. También pueden comunicarse entre sí mediante comunicaciones estándar entre procesos, como semáforos de SystemV o la memoria compartida POSIX. Los contenedores en diferentes Pods tienen direcciones IP distintas y no pueden comunicarse por IPC sin <a href=/docs/concepts/policy/pod-security-policy/>configuración especial</a>.
Estos contenedores normalmente se comunican entre sí a través de las direcciones IP del Pod.</p><p>Las aplicaciones dentro de un Pod también tienen acceso a <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volúmenes>volúmenes</a> compartidos, que se definen como parte de un Pod y están disponibles para ser montados en el sistema de archivos de cada aplicación.</p><p>En términos de <a href=https://www.docker.com/>Docker</a>, un Pod se modela como un grupo de contenedores de Docker con namespaces y volúmenes de sistemas de archivos compartidos.</p><p>Al igual que los contenedores de aplicaciones individuales, los Pods se consideran entidades relativamente efímeras (en lugar de duraderas). Como se explica en <a href=/docs/concepts/workloads/pods/pod-lifecycle/>ciclo de vida del pod</a>, los Pods se crean, se les asigna un identificador único (UID) y se planifican en nodos donde permanecen hasta su finalización (según la política de reinicio) o supresión. Si un <a class=glossary-tooltip title='Un Node, nodo en castellano, es una de las máquinas del clúster de Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodo>nodo</a> muere, los Pods programados para ese nodo se programan para su eliminación después de un período de tiempo de espera. Un Pod dado (definido por su UID) no se "replanifica" a un nuevo nodo; en su lugar, puede reemplazarse por un Pod idéntico, con incluso el mismo nombre si lo desea, pero con un nuevo UID (consulte <a href=/es/docs/concepts/workloads/controllers/replicationcontroller/>controlador de replicación</a> para obtener más detalles).</p><p>Cuando se dice que algo tiene la misma vida útil que un Pod, como un volumen, significa que existe mientras exista ese Pod (con ese UID). Si ese Pod se elimina por cualquier motivo, incluso si se crea un reemplazo idéntico, el recurso relacionado (por ejemplo, el volumen) también se destruye y se crea de nuevo.<figure><img src=/images/docs/pod.svg width=50%><figcaption><h4>diagrama de Pod</h4></figcaption></figure></p><p><em>Un Pod de múltiples contenedores que contiene un extractor de archivos y un servidor web que utiliza un volumen persistente para el almacenamiento compartido entre los contenedores.</em></p><h2 id=motivación-para-los-pods>Motivación para los Pods</h2><h3 id=gestión>Gestión</h3><p>Los Pods son un modelo del patrón de múltiples procesos de cooperación que forman una unidad de servicio cohesiva. Simplifican la implementación y la administración de las aplicaciones proporcionando una abstracción de mayor nivel que el conjunto de las aplicaciones que lo constituyen. Los Pods sirven como unidad de despliegue, escalado horizontal y replicación. La colocación (coprogramación), el destino compartido (por ejemplo, la finalización), la replicación coordinada, el uso compartido de recursos y la gestión de dependencias se controlan automáticamente para los contenedores en un Pod.</p><h3 id=recursos-compartidos-y-comunicación>Recursos compartidos y comunicación</h3><p>Los Pods permiten el intercambio de datos y la comunicación entre los contenedores que lo constituyen.</p><p>Todas las aplicaciones en un Pod utilizan el mismo namespace de red (la misma IP y puerto) y, por lo tanto, pueden "encontrarse" entre sí y comunicarse utilizando <code>localhost</code>.
Debido a esto, las aplicaciones en un Pod deben coordinar su uso de puertos. Cada Pod tiene una dirección IP en un espacio de red compartido que tiene comunicación completa con otros servidores físicos y Pods a través de la red.</p><p>Los contenedores dentro del Pod ven que el hostname del sistema es el mismo que el <code>nombre</code> configurado para el Pod. Hay más información sobre esto en la sección <a href=/docs/concepts/cluster-administration/networking/>networking</a>.</p><p>Además de definir los contenedores de aplicaciones que se ejecutan en el Pod, el Pod especifica un conjunto de volúmenes de almacenamiento compartido. Los volúmenes permiten que los datos sobrevivan a reinicios de contenedores y se compartan entre las aplicaciones dentro del Pod.</p><h2 id=usos-de-pods>Usos de Pods</h2><p>Los Pods pueden ser usados para alojar pilas de aplicaciones integradas (por ejemplo, LAMP), pero su objetivo principal es apoyar los programas de ayuda coubicados y coadministrados, como:</p><ul><li>sistemas de gestión de contenido, loaders de datos y archivos, gestores de caché locales, etc.</li><li>copia de seguridad de registro y punto de control, compresión, rotación, captura de imágenes, etc.</li><li>observadores de cambio de datos, adaptadores de registro y monitoreo, publicadores de eventos, etc.</li><li>proxies, bridges y adaptadores.</li><li>controladores, configuradores y actualizadores.</li></ul><p>Los Pods individuales no están diseñados para ejecutar varias instancias de la misma aplicación, en general.</p><p>Para una explicación más detallada, ver <a href=https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns>El sistema distribuido ToolKit: Patrones para Contenedores multiaplicación</a>.</p><h2 id=alternativas>Alternativas</h2><p><em>¿Por qué simplemente no ejecutar múltiples programas en un solo contenedor de Docker?</em></p><ol><li>Transparencia. Hacer visibles los contenedores dentro del Pod
a la infraestructura permite que esta brinde servicios, como gestión de procesos
y monitoreo de recursos, a los contenedores, facilitando una
serie de comodidades a los usuarios.</li><li>Desacople de dependencias de software. Los contenedores individuales pueden ser
versionados, reconstruidos y redistribuidos independientemente. Kubernetes podría incluso apoyar
actualizaciones en vivo de contenedores individuales en un futuro.</li><li>Facilidad de uso. Los usuarios no necesitan ejecutar sus propios administradores de procesos,
para propagación de señales, códigos de salida, etc.</li><li>Eficiencia. Debido a que la infraestructura asume más responsabilidad,
los contenedores pueden ser más livianos.</li></ol><p><em>¿Por qué no admitir la planificación conjunta de contenedores por afinidad?</em></p><p>Ese enfoque proporcionaría la ubicación conjunta, pero no la mayor parte de
beneficios de los Pods, como compartir recursos, IPC, compartir el destino garantizado y
gestión simplificada.</p><h2 id=durabilidad-de-pods-o-su-ausencia>Durabilidad de pods (o su ausencia)</h2><p>Los Pods no están destinados a ser tratados como entidades duraderas. No sobrevivirán a errores de planificación, caídas de nodo u otros desalojos, ya sea por falta de recursos o en el caso de mantenimiento de nodos.</p><p>En general, los usuarios no deberían necesitar crear Pods directamente, deberían
usar siempre controladores incluso para Pods individuales, como por ejemplo, los
<a href=/es/docs/concepts/workloads/controllers/deployment/>Deployments</a>.
Los controladores proporcionan autorecuperación con un alcance de clúster, así como replicación
y gestión de despliegue.
Otros controladores como los <a href=/es/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>
pueden tambien proporcionar soporte para Pods que necesiten persistir el estado.</p><p>El uso de API colectivas como la principal primitiva de cara al usuario es relativamente común entre los sistemas de planificación de clúster, incluyendo <a href=https://research.google.com/pubs/pub43438.html>Borg</a>, <a href=https://mesosphere.github.io/marathon/docs/rest-api.html>Marathon</a>, <a href=http://aurora.apache.org/documentation/latest/reference/configuration/#job-schema>Aurora</a>, y <a href=http://www.slideshare.net/Docker/aravindnarayanan-facebook140613153626phpapp02-37588997>Tupperware</a>.</p><p>El Pod se expone como primitiva para facilitar:</p><ul><li>planificación y capacidad de conexión del controlador</li><li>soporte para operaciones a nivel de Pod sin la necesidad de "proxy" a través de las API del controlador</li><li>desacople de la vida útil del Pod de la vida útil del controlador, como para el arranque</li><li>desacople de controladores y servicios, el endpoint del controlador solo mira Pods</li><li>composición limpia de funcionalidad a nivel de Kubelet con funcionalidad a nivel de clúster, Kubelet es efectivamente el "controlador de Pod"</li><li>aplicaciones en alta disponibilidad, que esperan que los Pods sean reemplazados antes de su finalización y ciertamente antes de su eliminación, como en el caso de desalojos planificados o descarga previa de imágenes.</li></ul><h2 id=finalización-de-pods>Finalización de Pods</h2><p>Debido a que los Pods representan procesos en ejecución en los nodos del clúster, es importante permitir que esos procesos finalicen de forma correcta cuando ya no se necesiten (en lugar de ser detenidos bruscamente con una señal de KILL). Los usuarios deben poder solicitar la eliminación y saber cuándo finalizan los procesos, pero también deben poder asegurarse de que las eliminaciones finalmente se completen. Cuando un usuario solicita la eliminación de un Pod, el sistema registra el período de gracia previsto antes de que el Pod pueda ser eliminado de forma forzada, y se envía una señal TERM al proceso principal en cada contenedor. Una vez que el período de gracia ha expirado, la señal KILL se envía a esos procesos y el Pod se elimina del servidor API. Si se reinicia Kubelet o el administrador de contenedores mientras se espera que finalicen los procesos, la terminación se volverá a intentar con el período de gracia completo.</p><p>Un ejemplo del ciclo de terminación de un Pod:</p><ol><li>El usuario envía un comando para eliminar Pod, con un período de gracia predeterminado (30s)</li><li>El Pod en el servidor API se actualiza con el tiempo a partir del cual el Pod se considera "muerto" junto con el período de gracia.</li><li>El Pod aparece como "Terminando" cuando aparece en los comandos del cliente</li><li>(simultáneo con 3) Cuando el Kubelet ve que un Pod se ha marcado como terminado porque se ha configurado el tiempo en 2, comienza el proceso de apagado del Pod.<ol><li>Si uno de los contenedores del Pod ha definido un <a href=/es/docs/concepts/containers/container-lifecycle-hooks/#hook-details>preStop hook</a>, se invoca dentro del contenedor. Si el hook <code>preStop</code> todavía se está ejecutando después de que expire el período de gracia, el paso 2 se invoca con un pequeño período de gracia extendido (2s).</li><li>El contenedor recibe la señal TERM. Tenga en cuenta que no todos los contenedores en el Pod recibirán la señal TERM al mismo tiempo y cada uno puede requerir un hook <code>preStop</code> si el orden en el que se cierra es importante.</li></ol></li><li>(simultáneo con 3) Pod se elimina de la lista de endponts del servicio, y ya no se considera parte del conjunto de Pods en ejecución para controladores de replicación. Los Pods que se apagan lentamente no pueden continuar sirviendo el tráfico ya que los balanceadores de carga (como el proxy de servicio) los eliminan de sus rotaciones.</li><li>Cuando expira el período de gracia, todos los procesos que todavía se ejecutan en el Pod se eliminan con SIGKILL.</li><li>El Kubelet terminará de eliminar el Pod en el servidor API configurando el período de gracia 0 (eliminación inmediata). El Pod desaparece de la API y ya no es visible desde el cliente.</li></ol><p>Por defecto, todas las eliminaciones se realizan correctamente en 30 segundos. El comando <code>kubectl delete</code> admite la opción<code>--grace-period = &lt;seconds></code>que permite al usuario anular el valor predeterminado y especificar su propio valor. El valor <code>0</code> <a href=/es/docs/concepts/workloads/pods/pod/#forzar-destrucci%C3%B3n-de-pods>forzar eliminación</a> del Pod.
Debe especificar un indicador adicional <code>--force</code> junto con <code>--grace-period = 0</code> para realizar eliminaciones forzadas.</p><h3 id=forzar-destrucción-de-pods>Forzar destrucción de Pods</h3><p>La eliminación forzada de un Pod se define como la eliminación de un Pod del estado del clúster y etcd inmediatamente. Cuando se realiza una eliminación forzada, el apiserver no espera la confirmación del kubelet de que el Pod ha finalizado en el nodo en el que se estaba ejecutando. Elimina el Pod en la API inmediatamente para que se pueda crear un nuevo Pod con el mismo nombre. En el nodo, los Pods que están configurados para terminar de inmediato recibirán un pequeño período de gracia antes de ser forzadas a matar.</p><p>Estas eliminaciones pueden ser potencialmente peligrosas para algunos Pods y deben realizarse con precaución. En el caso de Pods de StatefulSets, consulte la documentación de la tarea para <a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>eliminando Pods de un StatefulSet</a>.</p><h2 id=modo-privilegiado-para-pods>Modo privilegiado para Pods</h2><p>Cualquier contenedor en un Pod puede habilitar el modo privilegiado, utilizando el indicador <code>privilegiado</code> en el <a href=/docs/tasks/configure-pod-container/security-context/>contexto de seguridad</a> de la especificación del contenedor. Esto es útil para contenedores que desean usar capacidades de Linux como manipular la pila de red y acceder a dispositivos. Los procesos dentro del contenedor obtienen casi los mismos privilegios que están disponibles para los procesos fuera de un contenedor. Con el modo privilegiado, debería ser más fácil escribir complementos de red y volumen como Pods separados que no necesitan compilarse en el kubelet.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El <a class=glossary-tooltip title='El Container Runtime, entorno de ejecución de un contenedor, es el software responsable de ejecutar contenedores.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='runtime de contenedores'>runtime de contenedores</a> debe admitir el concepto de un contenedor privilegiado para que esta configuración sea relevante.</div><h2 id=api>API</h2><p>Pod es un recurso de nivel superior en la API REST de Kubernetes.
La definición de <a href=/docs/reference/generated/kubernetes-api/v1.25/#pod-v1-core>objeto de API Pod</a>
describe el objeto en detalle.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1ccbd4eeded6ab138d98b59175bd557e>1.2 - Contenedores de Inicialización</h1><p>Esta página proporciona una descripción general de los contenedores de inicialización (init containers): contenedores especializados que se ejecutan
antes de los contenedores de aplicación en un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>.
Los contenedores de inicialización pueden contener utilidades o scripts de instalación no presentes en una imagen de aplicación.</p><p>Tú puedes especificar contenedores de inicialización en la especificación del Pod junto con el arreglo de <code>containers</code>
(el cual describe los contenedores de aplicación).</p><h2 id=entendiendo-los-contenedores-de-inicialización>Entendiendo los contenedores de inicialización</h2><p>Un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> puede tener múltiples contenedores
ejecutando aplicaciones dentro de él, pero también puede tener uno o más contenedores de inicialización
que se ejecutan antes de que se inicien los contenedores de aplicación.</p><p>Los contenedores de inicialización son exactamente iguales a los contenedores regulares excepto por:</p><ul><li>Los contenedores de inicialización siempre se ejecutan hasta su finalización.</li><li>Cada contenedor de inicialiación debe completarse correctamente antes de que comience el siguiente.</li></ul><p>Si el contenedor de inicialización de un Pod falla, kubelet reinicia repetidamente ese contenedor de inicialización hasta que tenga éxito.
Sin embargo, si el Pod tiene una <code>restartPolicy</code> de <code>Never</code> y un contenedor de inicialización falla durante el inicio de ese Pod, Kubernetes trata al Pod en general como fallido.</p><p>Para especificar un contenedor de inicialización para un Pod, agrega el campo <code>initContainers</code> en
la <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec>especificación del Pod</a>,
como un arreglo de elementos <code>container</code> (similar al campo <code>containers</code> de aplicación y su contenido).
Consulta <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container>Container</a> en la
referencia de API para más detalles.</p><p>El estado de los contenedores de inicialización se devuelve en el campo <code>.status.initContainerStatuses</code>
como un arreglo de los estados del contenedor (similar al campo <code>.status.containerStatuses</code>).</p><h3 id=diferencias-con-los-contenedores-regulares>Diferencias con los contenedores regulares</h3><p>Los contenedores de inicialización admiten todos los campos y características de los contenedores de aplicaciones,
incluidos los límites de recursos, los volúmenes y la configuración de seguridad. Sin embargo, las
solicitudes de recursos y los límites para un contenedor de inicialización se manejan de manera diferente,
como se documenta en <a href=#resources>Recursos</a>.</p><p>Además, los contenedores de inicialización no admiten <code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code> o
<code>startupProbe</code> porque deben de ejecutarse hasta su finalización antes de que el Pod pueda estar listo.</p><p>Si especificas varios contenedores de inicialización para un Pod, kubelet ejecuta cada contenedor
de inicialización secuencialmente. Cada contenedor de inicialización debe tener éxito antes de que se pueda ejecutar el siguiente.
Cuando todos los contenedores de inicialización se hayan ejecutado hasta su finalización, kubelet inicializa
los contenedores de aplicación para el Pod y los ejecuta como de costumbre.</p><h3 id=usando-contenedores-de-inicialización>Usando contenedores de inicialización</h3><p>Dado que los contenedores de inicialización tienen imágenes separadas de los contenedores de aplicaciones, estos
tienen algunas ventajas sobre el código relacionado de inicio:</p><ul><li>Los contenedores de inicialización pueden contener utilidades o código personalizado para la configuración que no están presentes en una
imagen de aplicación. Por ejemplo, no hay necesidad de hacer una imagen <code>FROM</code> de otra imagen solo para usar una herramienta como
<code>sed</code>, <code>awk</code>, <code>python</code> o <code>dig</code> durante la instalación.</li><li>Los roles de constructor e implementador de imágenes de aplicación pueden funcionar de forma independiente sin
la necesidad de construir conjuntamente una sola imagen de aplicación.</li><li>Los contenedores de inicialización pueden ejecutarse con una vista diferente al sistema de archivos que los contenedores de aplicaciones en
el mismo Pod. En consecuencia, se les puede dar acceso a
<a class=glossary-tooltip title='Almacena información sensible, como contraseñas, tokens OAuth o claves ssh.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secrets>Secrets</a> a los que los contenedores de aplicaciones no pueden acceder.</li><li>Debido a que los contenedores de inicialización se ejecutan hasta su finalización antes de que se inicien los contenedores de aplicaciones, los contenedores de inicialización ofrecen
un mecanismo para bloquear o retrasar el inicio del contenedor de aplicación hasta que se cumplan una serie de condiciones previas. Una vez
que las condiciones previas se cumplen, todos los contenedores de aplicaciones de un Pod pueden iniciarse en paralelo.</li><li>Los contenedores de inicialización pueden ejecutar de forma segura utilidades o código personalizado que de otro modo harían a una imagen de aplicación
de contenedor menos segura. Si mantiene separadas herramientas innecesarias, puede limitar la superficie de ataque
a la imagen del contenedor de aplicación.</li></ul><h3 id=ejemplos>Ejemplos</h3><p>A continuación, se muestran algunas ideas sobre cómo utilizar los contenedores de inicialización:</p><ul><li><p>Esperar a que se cree un <a class=glossary-tooltip title='Un Service, servicio en castellano, es el objeto de la API de Kubernetes que describe cómo se accede a las aplicaciones, tal como un conjunto de Pods, y que puede describir puertos y balanceadores de carga.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>
usando una sola linea de comando de shell:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#666>{</span>1..100<span style=color:#666>}</span>; <span style=color:#a2f;font-weight:700>do</span> sleep 1; <span style=color:#a2f;font-weight:700>if</span> dig myservice; <span style=color:#a2f;font-weight:700>then</span> <span style=color:#a2f>exit</span> 0; <span style=color:#a2f;font-weight:700>fi</span>; <span style=color:#a2f;font-weight:700>done</span>; <span style=color:#a2f>exit</span> <span style=color:#666>1</span>
</span></span></code></pre></div></li><li><p>Registrar este Pod con un servidor remoto desde la downward API con un comando como:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -X POST http://<span style=color:#b8860b>$MANAGEMENT_SERVICE_HOST</span>:<span style=color:#b8860b>$MANAGEMENT_SERVICE_PORT</span>/register -d <span style=color:#b44>&#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;</span>
</span></span></code></pre></div></li><li><p>Esperar algo de tiempo antes de iniciar el contenedor de aplicación con un comando como:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sleep <span style=color:#666>60</span>
</span></span></code></pre></div></li><li><p>Clonar un repositorio de Git en un <a class=glossary-tooltip title='Un directorio que contiene datos y que es accesible desde los contenedores corriendo en un pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volume>Volume</a></p></li><li><p>Colocar valores en un archivo de configuración y ejecutar una herramienta de plantilla para generar
dinámicamente un archivo de configuración para el contenedor de aplicación principal. Por ejemplo,
colocar el valor <code>POD_IP</code> en una configuración y generar el archivo de configuración
de la aplicación principal usando Jinja.</p></li></ul><h4 id=contenedores-de-inicialización-en-uso>Contenedores de inicialización en uso</h4><p>Este ejemplo define un simple Pod que tiene dos contenedores de inicialización.
El primero espera por <code>myservice</code> y el segundo espera por <code>mydb</code>. Una vez que ambos
contenedores de inicialización se completen, el Pod ejecuta el contenedor de aplicación desde su sección <code>spec</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;echo ¡La aplicación se está ejecutando! &amp;&amp; sleep 3600&#39;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>initContainers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo esperando a myservice; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-mydb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo esperando a mydb; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div><p>Puedes iniciar este Pod ejecutando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f myapp.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pod/myapp-pod created
</span></span></code></pre></div><p>Y verificar su estado con:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME        READY     STATUS     RESTARTS   AGE
</span></span><span style=display:flex><span>myapp-pod   0/1       Init:0/2   <span style=color:#666>0</span>          6m
</span></span></code></pre></div><p>o para más detalles:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe -f myapp.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Name:          myapp-pod
</span></span><span style=display:flex><span>Namespace:     default
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>Labels:        app.kubernetes.io/name<span style=color:#666>=</span>MyApp
</span></span><span style=display:flex><span>Status:        Pending
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>Init Containers:
</span></span><span style=display:flex><span>  init-myservice:
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>    State:         Running
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>  init-mydb:
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>    State:         Waiting
</span></span><span style=display:flex><span>      Reason:      PodInitializing
</span></span><span style=display:flex><span>    Ready:         False
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>Containers:
</span></span><span style=display:flex><span>  myapp-container:
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>    State:         Waiting
</span></span><span style=display:flex><span>      Reason:      PodInitializing
</span></span><span style=display:flex><span>    Ready:         False
</span></span><span style=display:flex><span><span style=color:#666>[</span>...<span style=color:#666>]</span>
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
</span></span><span style=display:flex><span>  ---------    --------    -----    ----                      -------------                           --------      ------        -------
</span></span><span style=display:flex><span>  16s          16s         <span style=color:#666>1</span>        <span style=color:#666>{</span>default-scheduler <span style=color:#666>}</span>                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
</span></span><span style=display:flex><span>  16s          16s         <span style=color:#666>1</span>        <span style=color:#666>{</span>kubelet 172.17.4.201<span style=color:#666>}</span>    spec.initContainers<span style=color:#666>{</span>init-myservice<span style=color:#666>}</span>     Normal        Pulling       pulling image <span style=color:#b44>&#34;busybox&#34;</span>
</span></span><span style=display:flex><span>  13s          13s         <span style=color:#666>1</span>        <span style=color:#666>{</span>kubelet 172.17.4.201<span style=color:#666>}</span>    spec.initContainers<span style=color:#666>{</span>init-myservice<span style=color:#666>}</span>     Normal        Pulled        Successfully pulled image <span style=color:#b44>&#34;busybox&#34;</span>
</span></span><span style=display:flex><span>  13s          13s         <span style=color:#666>1</span>        <span style=color:#666>{</span>kubelet 172.17.4.201<span style=color:#666>}</span>    spec.initContainers<span style=color:#666>{</span>init-myservice<span style=color:#666>}</span>     Normal        Created       Created container with docker id 5ced34a04634; Security:<span style=color:#666>[</span><span style=color:#b8860b>seccomp</span><span style=color:#666>=</span>unconfined<span style=color:#666>]</span>
</span></span><span style=display:flex><span>  13s          13s         <span style=color:#666>1</span>        <span style=color:#666>{</span>kubelet 172.17.4.201<span style=color:#666>}</span>    spec.initContainers<span style=color:#666>{</span>init-myservice<span style=color:#666>}</span>     Normal        Started       Started container with docker id 5ced34a04634
</span></span></code></pre></div><p>Para ver los logs de los contenedores de inicialización en este Pod ejecuta:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs myapp-pod -c init-myservice <span style=color:#080;font-style:italic># Inspecciona el primer contenedor de inicialización</span>
</span></span><span style=display:flex><span>kubectl logs myapp-pod -c init-mydb      <span style=color:#080;font-style:italic># Inspecciona el segundo contenedor de inicialización</span>
</span></span></code></pre></div><p>En este punto, estos contenedores de inicialización estarán esperando para descubrir los Servicios denominados
<code>mydb</code> y <code>myservice</code>.</p><p>Aquí hay una configuración que puedes usar para que aparezcan esos Servicios:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9377</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Para crear los servicios de <code>mydb</code> y <code>myservice</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f services.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>service/myservice created
</span></span><span style=display:flex><span>service/mydb created
</span></span></code></pre></div><p>Luego verás que esos contenedores de inicialización se completan y que el Pod <code>myapp-pod</code>
pasa al estado <code>Running</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>El resultado es similar a esto:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME        READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>myapp-pod   1/1       Running   <span style=color:#666>0</span>          9m
</span></span></code></pre></div><p>Este sencillo ejemplo debería servirte de inspiración para crear tus propios
contenedores de inicialización. <a href=#what-s-next>¿Qué es lo que sigue?</a> contiene un enlace a un ejemplo más detallado.</p><h2 id=comportamiento-detallado>Comportamiento detallado</h2><p>Durante el inicio del Pod, kubelet retrasa la ejecución de contenedores de inicialización hasta que la red
y el almacenamiento estén listos. Después, kubelet ejecuta los contenedores de inicialización del Pod en el orden que
aparecen en la especificación del Pod.</p><p>Cada contenedor de inicialización debe salir correctamente antes de que
comience el siguiente contenedor. Si un contenedor falla en iniciar debido al tiempo de ejecución o
sale con una falla, se vuelve a intentar de acuerdo con el <code>restartPolicy</code> del Pod. Sin embargo,
si el <code>restartPolicy</code> del Pod se establece en <code>Always</code>, los contenedores de inicialización usan
el <code>restartPolicy</code> como <code>OnFailure</code>.</p><p>Un Pod no puede estar <code>Ready</code> sino hasta que todos los contenedores de inicialización hayan tenido éxito. Los puertos en un
contenedor de inicialización no se agregan a un Servicio. Un Pod que se está inicializando,
está en el estado de <code>Pending</code>, pero debe tener una condición <code>Initialized</code> configurada como falsa.</p><p>Si el Pod <a href=#pod-restart-reasons>se reinicia</a> o es reiniciado, todos los contenedores de inicialización
deben ejecutarse de nuevo.</p><p>Los cambios en la especificación del contenedor de inicialización se limitan al campo de la imagen del contenedor.
Alterar un campo de la imagen del contenedor de inicialización equivale a reiniciar el Pod.</p><p>Debido a que los contenedores de inicialización se pueden reiniciar, reintentar o volverse a ejecutar, el código del contenedor de inicialización
debe ser idempotente. En particular, el código que escribe en archivos en <code>EmptyDirs</code>
debe estar preparado para la posibilidad de que ya exista un archivo de salida.</p><p>Los contenedores de inicialización tienen todos los campos de un contenedor de aplicaciones. Sin embargo, Kubernetes
prohíbe el uso de <code>readinessProbe</code> porque los contenedores de inicialización no pueden
definir el <code>readiness</code> distinto de la finalización. Esto se aplica durante la validación.</p><p>Usa <code>activeDeadlineSeconds</code> en el Pod para prevenir que los contenedores de inicialización fallen por siempre.
La fecha límite incluye contenedores de inicialización.
Sin embargo, se recomienda utilizar <code>activeDeadlineSeconds</code> si el usuario implementa su aplicación
como un <code>Job</code> porque <code>activeDeadlineSeconds</code> tiene un efecto incluso después de que <code>initContainer</code> finaliza.
El Pod que ya se está ejecutando correctamente sería eliminado por <code>activeDeadlineSeconds</code> si lo estableces.</p><p>El nombre de cada aplicación y contenedor de inicialización en un Pod debe ser único; un
error de validación es arrojado para cualquier contenedor que comparta un nombre con otro.</p><h3 id=recursos>Recursos</h3><p>Dado el orden y la ejecución de los contenedores de inicialización, las siguientes reglas
para el uso de recursos se aplican:</p><ul><li>La solicitud más alta de cualquier recurso o límite particular definido en todos los contenedores
de inicialización es la <em>solicitud/límite de inicialización efectiva</em>. Si algún recurso no tiene un
límite de recursos especificado éste se considera como el límite más alto.</li><li>La <em>solicitud/límite efectiva</em> para un recurso es la más alta entre:<ul><li>la suma de todas las solicitudes/límites de los contenedores de aplicación, y</li><li>la solicitud/límite de inicialización efectiva para un recurso</li></ul></li><li>La planificación es hecha con base en las solicitudes/límites efectivos, lo que significa
que los contenedores de inicialización pueden reservar recursos para la inicialización que no se utilizan
durante la vida del Pod.</li><li>El nivel de <code>QoS</code> (calidad de servicio) del <em>nivel de <code>QoS</code> efectivo</em> del Pod es el
nivel de <code>QoS</code> tanto para los contenedores de inicialización como para los contenedores de aplicación.</li></ul><p>La cuota y los límites son aplicados con base en la solicitud y límite efectivos de Pod.</p><p>Los grupos de control de nivel de Pod (cgroups) se basan en la solicitud y el límite de Pod efectivos, al igual que el planificador de Kubernetes (<a class=glossary-tooltip title='Componente del plano de control que está pendiente de los pods que no tienen ningún nodo asignado y seleciona uno dónde ejecutarlo.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a>).</p><h3 id=razones-de-reinicio-del-pod>Razones de reinicio del Pod</h3><p>Un Pod puede reiniciarse, provocando la re-ejecución de los contenedores de inicialización por las siguientes razones:</p><ul><li>Se reinicia el contenedor de infraestructura del Pod. Esto es poco común y debería hacerlo alguien con acceso de root a los nodos.</li><li>Todos los contenedores en un Pod son terminados mientras <code>restartPolicy</code> esté configurado en <code>Always</code>,
forzando un reinicio y el registro de finalización del contenedor de inicialización se ha perdido debido a
la recolección de basura.</li></ul><p>El Pod no se reiniciará cuando se cambie la imagen del contenedor de inicialización o cuando
se pierda el registro de finalización del contenedor de inicialización debido a la recolección de basura. Esto
se aplica a Kubernetes v1.20 y posteriores. Si estás utilizando una versión anterior de
Kubernetes, consulta la documentación de la versión que estás utilizando.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Lee acerca de <a href=/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container>creando un Pod que tiene un contenedor de inicialización</a></li><li>Aprende cómo <a href=/docs/tasks/debug/debug-application/debug-init-containers/>depurar contenedores de inicialización</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4e9b9cbc9776b12e7335c53da377c9c8>1.3 - Pod Preset</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.6 [alpha]</code></div><p>Esta página provee una descripción general de los PodPresets, los cuales son
los objetos que se utilizan para inyectar cierta información en los Pods en
el momento de la creación. Esta información puede incluir secretos, volúmenes,
montajes de volúmenes y variables de entorno.</p><h2 id=entendiendo-los-pod-presets>Entendiendo los Pod Presets</h2><p>Un PodPreset es un recurso de la API utilizado para poder inyectar requerimientos
adicionales de tiempo de ejecución en un Pod en el momento de la creación.
Se utilizan los <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>selectores de etiquetas</a>
para especificar los Pods a los que se aplica un PodPreset determinado.</p><p>El uso de un PodPreset permite a los autores de plantillas de Pods no tener que proporcionar
explícitamente toda la información de cada Pod. De esta manera, los autores de plantillas de
Pods que consuman un determinado servicio no tendrán que conocer todos los detalles de ese servicio.</p><h2 id=habilitando-un-podpreset-en-su-clúster>Habilitando un PodPreset en su clúster</h2><p>Con el fin de utilizar los Pod Presets en un clúster debe asegurarse de lo siguiente:</p><ol><li><p>Que se ha configurado el tipo de API <code>settings.k8s.io/v1alpha1/podpreset</code>. Esto se puede hacer,
por ejemplo, incluyendo <code>settings.k8s.io/v1alpha1=true</code> como valor de la opción <code>--runtime-config</code>
en el servidor API. En minikube se debe añadir el flag
<code>--extra-config=apiserver.runtime-config=settings.k8s.io/v1alpha1=true</code> cuando el clúster
se está iniciando.</p></li><li><p>Que se ha habilitado el controlador de admisión <code>PodPreset</code>. Una forma de hacer esto es incluir
<code>PodPreset</code> como valor de la opción <code>--enable-admission-plugins</code> especificada
para el servidor API. En minikube se debe añadir el flag</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>--extra-config<span style=color:#666>=</span>apiserver.enable-admission-plugins<span style=color:#666>=</span>NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,PodPreset
</span></span></code></pre></div><p>cuando el clúster se está iniciando.</p></li></ol><h2 id=cómo-funciona>Cómo funciona</h2><p>Kubernetes provee un controlador de admisión (<code>PodPreset</code>) que, cuando está habilitado,
aplica los Pod Presets a las peticiones de creación de Pods entrantes.
Cuando se realiza una solicitud de creación de Pods, el sistema hace lo siguiente:</p><ol><li>Obtiene todos los <code>PodPresets</code> disponibles para usar.</li><li>Verifica si los selectores de etiquetas de cualquier <code>PodPreset</code> correspondan
con las etiquetas del Pod que se está creando.</li><li>Intenta fusionar los diversos recursos definidos por el <code>PodPreset</code> dentro del Pod
que se está creando.</li><li>Si se llegase a producir un error al intentar fusionar los recursos dentro del Pod,
lanza un evento que documente este error, luego crea el Pod <em>sin</em> ningún recurso que se
inyecte desde el <code>PodPreset</code>.</li><li>Escribe una nota descriptiva de la especificación de Pod modificada resultante para
indicar que ha sido modificada por un <code>PodPreset</code>. La nota descriptiva presenta la forma
<code>podpreset.admission.kubernetes.io/podpreset-&lt;pod-preset name>: "&lt;resource version>"</code>.</li></ol><p>Cada Pod puede ser correspondido por cero o más Pod Presets; y cada <code>PodPreset</code> puede ser
aplicado a cero o más Pods. Cuando se aplica un <code>PodPreset</code> a una o más Pods, Kubernetes
modifica la especificación del Pod. Para los cambios a <code>env</code>, <code>envFrom</code>, y <code>volumeMounts</code>,
Kubernetes modifica la especificación del Container para todos los Containers en el Pod;
para los cambios a <code>volumes</code>, Kubernetes modifica la especificación del Pod.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>Un Pod Preset es capaz de modificar los siguientes campos en las especificaciones de un Pod
en caso de ser necesario:</p><ul><li>El campo <code>.spec.containers</code>.</li><li>El campo <code>.spec.initContainers</code></li></ul></div><h3 id=deshabilitar-un-pod-preset-para-un-pod-específico>Deshabilitar un Pod Preset para un Pod específico</h3><p>Puede haber casos en los que se desee que un Pod no se vea alterado por ninguna posible
modificación del Pod Preset. En estos casos, se puede añadir una observación en el Pod
<code>.spec</code> de la siguiente forma: <code>podpreset.admission.kubernetes.io/exclude: "true"</code>.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><p>Ver <a href=/docs/tasks/inject-data-application/podpreset/>Inyectando datos en un Pod usando PodPreset</a></p><p>Para más información sobre los detalles de los trasfondos, consulte la <a href=https://git.k8s.io/design-proposals-archive/service-catalog/pod-preset.md>propuesta de diseño de PodPreset</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4aaf43c715cd764bc8ed4436f3537e68>1.4 - Interrupciones</h1><p>Esta guía es para los dueños de aplicaciones que quieren crear
aplicaciones con alta disponibilidad y que necesitan entender
que tipos de interrupciones pueden suceder en los Pods.</p><p>También es para los administradores de clústers que quieren aplicar acciones
automatizadas en sus clústers, cómo actualizar o autoescalar los clústers.</p><h2 id=interrupciones-voluntarias-e-involuntarias>Interrupciones voluntarias e involuntarias</h2><p>Los Pods no desaparecen hasta que algo (una persona o un controlador) los destruye
ó hay problemas de hardware ó software que son inevitables.</p><p>Nosotros llamamos a esos casos inevitables <em>interrupciones involuntarias</em> de
una aplicación. Algunos ejemplos:</p><ul><li>Una falla en hardware de la máquina física del nodo</li><li>Un administrador del clúster borra una VM (instancia) por error</li><li>El proveedor de la nube o el hipervisor falla y hace desaparecer la VM</li><li>Un kernel panic</li><li>El nodo desaparece del clúster por un problema de red que lo separa del clúster</li><li>Una remoción del Pod porque el nodo <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>no tiene recursos suficientes</a>.</li></ul><p>A excepción de la condición sin recursos suficientes, todas estas condiciones
deben ser familiares para la mayoría de los usuarios, no son específicas
de Kubernetes</p><p>Nosotros llamamos a los otros casos <em>interrupciones voluntarias</em>. Estas incluyen
las acciones iniciadas por el dueño de la aplicación y aquellas iniciadas por el Administrador
del Clúster. Las acciones típicas de los dueños de la aplicación incluye:</p><ul><li>borrar el Deployment u otro controlador que maneja el Pod</li><li>actualizar el Deployment del Pod que causa un reinicio</li><li>borrar un Pod (por ejemplo, por accidente)</li></ul><p>Las acciones del administrador del clúster incluyen:</p><ul><li><a href=/docs/tasks/administer-cluster/safely-drain-node/>Drenar un nodo</a> para reparar o actualizar.</li><li>Drenar un nodo del clúster para reducir el clúster (aprenda acerca de <a href=https://github.com/kubernetes/autoscaler/#readme>Autoescalamiento de Clúster</a>
).</li><li>Remover un Pod de un nodo para permitir que otra cosa pueda ingresar a ese nodo.</li></ul><p>Estas acciones pueden ser realizadas directamente por el administrador del clúster, por
tareas automatizadas del administrador del clúster ó por el proveedor del clúster.</p><p>Consulte al administrador de su clúster, a su proveedor de la nube ó a la documentación de su distribución
para determinar si alguna de estas interrupciones voluntarias están habilitadas en su clúster.
Si ninguna se encuentra habilitada, puede omitir la creación del presupuesto de Interrupción de Pods.</p><div class="alert alert-warning caution callout" role=alert><strong>Precaución:</strong> No todas las interrupciones voluntarias son consideradas por el presupuesto de interrupción de Pods. Por ejemplo,
borrar un Deployment o Pods que evitan el uso del presupuesto.</div><h2 id=tratando-con-las-interrupciones>Tratando con las interrupciones</h2><p>Estas son algunas de las maneras para mitigar las interrupciones involuntarias:</p><ul><li>Asegurarse que el Pod <a href=/docs/tasks/configure-pod-container/assign-memory-resource>solicite los recursos</a> que necesita.</li><li>Replique su aplicación si usted necesita alta disponibilidad. (Aprenda sobre correr aplicaciones replicadas
<a href=/docs/tasks/run-application/run-stateless-application-deployment/>stateless</a>
y <a href=/docs/tasks/run-application/run-replicated-stateful-application/>stateful</a></li><li>Incluso, para una alta disponibilidad mayor cuando se corren aplicaciones replicadas,
propague las aplicaciones por varios racks (usando
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>anti-affinity</a>)
o usando zonas (si usa un <a href=/docs/setup/multiple-zones>clúster multi-zona</a>.)</li></ul><p>La frecuencia de las interrupciones voluntarias varía. En un clúster basico de Kubernetes, no hay
interrupciones voluntarias automáticas (solo el usuario las genera). Sin embargo, su administrador del clúster o proveedor de alojamiento
puede correr algun servicio adicional que pueda causar estas interrupciones voluntarias. Por ejemplo,
desplegando una actualización de software en los nodos puede causar interrupciones. También, algunas implementaciones
de clústers con autoescalamiento de nodos puede causar interrupciones para defragmentar o compactar los nodos.
Su administrador de clúster o proveedor de alojamiento debe tener documentado cuál es el nivel de interrupciones
voluntarias esperadas, sí es que las hay. Ciertas opciones de configuración, como ser
<a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>usar PriorityClasses</a>
en las especificaciones de su Pod pueden también causar interrupciones voluntarias (o involuntarias).</p><h2 id=presupuesto-de-interrupción-de-pods>Presupuesto de Interrupción de Pods</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>Kubernetes ofrece carácteristicas para ayudar a ejecutar aplicaciones con alta disponibliidad, incluso cuando usted
introduce interrupciones voluntarias frecuentes.</p><p>Como dueño de la aplicación, usted puede crear un presupuesto de interrupción de Pods (PDB por sus siglas en inglés) para cada aplicación.
Un PDB limita el numero de Pods de una aplicación replicada, que estan caídos de manera simultánea por
interrupciones voluntarias. Por ejemplo, una aplicación basada en quórum puede
asegurarse que el número de réplicas corriendo nunca es menor al
número necesitado para obtener el quórum. Una web de tipo front end puede querer
asegurarse que el número de réplicas atendiendo al tráfico nunca puede caer bajo un cierto
porcentaje del total.</p><p>Los administradores del clúster y proveedores de hosting pueden usar herramientas que
respeten el presupuesto de interrupción de Pods utilizando la <a href=/docs/tasks/administer-cl%C3%BAster/safely-drain-node/#eviction-api>API de Desalojo</a>
en vez de directamente borrar Pods o Deployments.</p><p>Por ejemplo, el subcomando <code>kubectl drain</code> le permite marcar un nodo a un modo fuera de
servicio. Cuando se ejecuta <code>kubectl drain</code>, la herramienta trata de quitar a todos los Pods en
el nodo que se esta dejando fuera de servicio. La petición de desalojo que <code>kubectl</code> solicita en
su nombre puede ser temporalmente denegado, entonces la herramienta periodicamente reintenta todas las
peticiones fallidas hasta que todos los Pods en el nodo afectado son terminados ó hasta que el tiempo de espera,
que puede ser configurado, es alcanzado.</p><p>Un PDB especifica el número de réplicas que una aplicación puede tolerar, relativo a cuantas
se pretende tener. Por ejemplo, un Deployment que tiene un <code>.spec.replicas: 5</code> se
supone que tiene 5 Pods en cualquier momento. Si su PDB permite tener 4 a la vez,
entonces la API de Desalojo va a permitir interrupciones voluntarias de un (pero no a dos) Pod a la vez.</p><p>El grupo de Pods que comprende a la aplicación esta especificada usando una etiqueta selectora, la misma
que es usada por el controlador de aplicación (deployment, stateful-set, etc).</p><p>El numero de Pods "deseado" es calculado a partir de <code>.spec.replicas</code> de el recurso de Workload
que es manejado para esos Pods. El plano de control descubre el recurso Workload perteneciente a el
examinando las <code>.metadata.ownerReferences</code> del Pod.</p><p>Las <a href=#voluntary-and-involuntary-disruptions>Interrupciones Involuntarias</a> no pueden ser prevenidas por los PDB; pero si
son contabilizadas a partir este presupuesto.</p><p>Los Pods que son borrados o no estan disponibles debido a una actualización continua de una aplicación forman parte del presupuesto de interrupciones, pero los recursos Workload (como los Deployments y StatefulSet)
no están limitados por los PDBs cuando se hacen actualizaciones continuas. En cambio, la administración de fallas
durante la actualización de la aplicación es configurada en la especificación para este recurso Workload específico.</p><p>Cuando un Pod es quitado usando la API de desalojo, este es
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>terminado</a> correctamente, haciendo honor al
<code>terminationGracePeriodSeconds</code> configurado en su <a href=/docs/reference/generated/kubernetes-api/v1.25/#podspec-v1-core>PodSpec</a>.</p><h2 id=pdb-example>Ejemplo de Presupuesto de Interrupción de POD</h2><p>Considere un clúster con 3 nodos, <code>nodo-1</code> hasta <code>nodo-3</code>.
El clúster esta corriendo varias aplicaciones. Uno de ellos tiene 3 replicas, que llamaremos
<code>pod-a</code>, <code>pod-b</code>, y <code>pod-c</code>. Otro Pod no relacionado y sin PDB, llamado <code>pod-x</code>, también se muestra.</p><p>Inicialmente los pods estan distribuidos de esta manera:</p><table><thead><tr><th style=text-align:center>nodo-1</th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center>pod-a <em>available</em></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center>pod-x <em>available</em></td><td style=text-align:center></td><td style=text-align:center></td></tr></tbody></table><p>Los 3 Pods son parte de un Deployment, ellos colectivamente tienen un PDB que requiere
que por lo menos 2 de los 3 Pods esten disponibles todo el tiempo.</p><p>Por ejemplo, supongamos que el administrador del clúster quiere reiniciar para actualizar el kernel y arreglar un bug.
El administrador del clúster primero intenta desocupar el <code>nodo-1</code> usando el comando <code>kubectl drain</code>.
La herramienta intenta desalojar a los pods <code>pod-a</code> y <code>pod-x</code>. Esto tiene éxito inmediatamente.
Ambos Pods van al estado <code>terminating</code> al mismo tiempo.
Pone al clúster en el siguiente estado:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>draining</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center>pod-a <em>terminating</em></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center>pod-x <em>terminating</em></td><td style=text-align:center></td><td style=text-align:center></td></tr></tbody></table><p>El Deployment detecta que uno de los Pods esta terminando, entonces crea un reemplazo
llamado <code>pod-d</code>. Como el <code>nodo-1</code> esta bloqueado, el pod termina en otro nodo. Algo más, adicionalmente
a creado el pod <code>pod-y</code> como un reemplazo del <code>pod-x</code> .</p><p>(Nota: para un StatefulSet, <code>pod-a</code>, el cual debería ser llamado algo como <code>pod-0</code>, necesitaría ser terminado completamente antes de su remplazo, el cual también es llamado <code>pod-0</code> pero tiene un UID diferente, podría ser creado. De lo contrario, el ejemplo también aplica a un StatefulSet.)</p><p>Ahora el clúster esta en este estado:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>draining</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center>pod-a <em>terminating</em></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center>pod-x <em>terminating</em></td><td style=text-align:center>pod-d <em>starting</em></td><td style=text-align:center>pod-y</td></tr></tbody></table><p>En algún punto, los Pods finalizan y el clúster se ve de esta forma:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>drained</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center></td><td style=text-align:center>pod-d <em>starting</em></td><td style=text-align:center>pod-y</td></tr></tbody></table><p>En este estado, si un administrador del clúster impaciente intenta desalojar el <code>nodo-2</code> ó el
<code>nodo-3</code>, el comando drain va a ser bloqueado, porque hay solamente 2 Pods disponibles para
el Deployment y el PDB requiere por lo menos 2. Después de pasado un tiempo el <code>pod-d</code> esta disponible.</p><p>El estado del clúster ahora se ve así:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>drained</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center></td><td style=text-align:center>pod-d <em>available</em></td><td style=text-align:center>pod-y</td></tr></tbody></table><p>Ahora, el administrador del clúster desaloja el <code>nodo-2</code>.
El comando drain tratará de desalojar a los 2 Pods con algún orden, digamos
primero el <code>pod-b</code> y después el <code>pod-d</code>. Va a tener éxito en quitar el <code>pod-b</code>.
Pero cuando intente desalojar al <code>pod-d</code>, va a ser rechazado porque esto va a dejar
un Pod solamente disponible para el Deployment.</p><p>El Deployment crea un reemplazo para el <code>pod-b</code> llamado <code>pod-e</code>.
Porque no hay recursos suficientes disponibles en el clúster para programar
el <code>pod-e</code> el desalojo será bloqueado nuevamente. El clúster va a terminar en este
estado:</p><table><thead><tr><th style=text-align:center>nodo-1 <em>drained</em></th><th style=text-align:center>nodo-2</th><th style=text-align:center>nodo-3</th><th style=text-align:center><em>no node</em></th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>pod-b <em>terminating</em></td><td style=text-align:center>pod-c <em>available</em></td><td style=text-align:center>pod-e <em>pending</em></td></tr><tr><td style=text-align:center></td><td style=text-align:center>pod-d <em>available</em></td><td style=text-align:center>pod-y</td><td style=text-align:center></td></tr></tbody></table><p>Ahora, el administrador del clúster necesita
agregar un nuevo nodo en el clúster para continuar con la actualización.</p><p>Usted puede ver como Kubernetes varia la tasa a la que las interrupciones
pueden suceder, en función de:</p><ul><li>cuantas réplicas una aplicación necesita</li><li>cuanto toma apagar una instancia de manera correcta</li><li>cuanto tiempo toma que una nueva instancia inicie</li><li>el tipo de controlador</li><li>la capacidad de recursos del clúster</li></ul><h2 id=separando-al-dueño-del-clúster-y-los-roles-de-dueños-de-la-aplicación>Separando al dueño del Clúster y los roles de dueños de la Aplicación</h2><p>Muchas veces es útil pensar en el Administrador del Clúster
y al dueño de la aplicación como roles separados con conocimiento limitado
el uno del otro. Esta separación de responsabilidades
puede tener sentido en estos escenarios:</p><ul><li>Cuando hay muchas equipos con aplicaciones compartiendo un clúster de Kubernetes y
hay una especialización natural de roles</li><li>Cuando una herramienta de terceros o servicio es usado para automatizar el control del clúster</li></ul><p>El presupuesto de interrupción de Pods soporta esta separación de roles, ofreciendo
una interfaz entre los roles.</p><p>Si no se tiene tal separación de responsabilidades en la organización,
posiblemente no se necesite el Presupuesto de Interrupción de Pods.</p><h2 id=como-realizar-acciones-disruptivas-en-el-clúster>Como realizar Acciones Disruptivas en el Clúster</h2><p>Si usted es el Administrador del Clúster y necesita realizar una acción disruptiva en todos
los nodos en el clúster, como ser una actualización de nodo o de software, estas son algunas de las opciones:</p><ul><li>Aceptar el tiempo sin funcionar mientras dura la actualización.</li><li>Conmutar a otra replica completa del clúster.<ul><li>No hay tiempo sin funcionar, pero puede ser costoso tener duplicados los nodos
y tambien un esfuerzo humano para orquestar dicho cambio.</li></ul></li><li>Escribir la toleracia a la falla de la aplicación y usar PDBs.<ul><li>No hay tiempo sin funcionar.</li><li>Duplicación de recursos mínimo.</li><li>Permite mucha más automatización de la administración del clúster.</li><li>Escribir aplicaciones que tengan tolerancia a fallas es complicado, pero el trabajo para tolerar interrupciones
involuntarias, largamente se sobrepone con el trabajo que es dar soporte a autoescalamientos y tolerar
interrupciones involuntarias.</li></ul></li></ul><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li><p>Siga los pasos para proteger su aplicación con <a href=/docs/tasks/run-application/configure-pdb/>configurar el Presupuesto de Interrupciones de Pods</a>.</p></li><li><p>Aprenda más sobre <a href=/docs/tasks/administer-cl%C3%BAster/safely-drain-node/>desalojar nodos</a></p></li><li><p>Aprenda sobre <a href=/docs/concepts/workloads/controllers/deployment/#updating-a-deployment>actualizar un Deployment</a>
incluyendo los pasos para mantener su disponibilidad mientras dura la actualización.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-53a1005011e1bda2ce81819aad7c8b32>1.5 - Containers Efímeros</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><p>Esta página proporciona una descripción general de los Containers efímeros: un tipo especial de Container
que se ejecuta temporalmente en un <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> ya existente para cumplir las
acciones iniciadas por el usuario, como por ejemplo, la solución de problemas. En vez de ser utilizadas para
crear aplicaciones, los Containers efímeros se utilizan para examinar los servicios.</p><div class="alert alert-danger warning callout" role=alert><strong>Advertencia:</strong> Los Containers efímeros se encuentran en una fase alfa inicial y no son aptos para clústers
de producción. Es de esperar que esta característica no funcione en algunas situaciones, por
ejemplo, al seleccionar los Namespaces de un Container. De acuerdo con la <a href=/docs/reference/using-api/deprecation-policy/>Política de
Deprecación de Kubernetes</a>, esta característica
alfa puede variar significativamente en el futuro o ser eliminada por completo.</div><h2 id=entendiendo-los-containers-efímeros>Entendiendo los Containers efímeros</h2><p><a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> son el componente fundamental de las
aplicaciones de Kubernetes. Puesto que los Pods están previstos para ser desechables
y reemplazables, no se puede añadir un Container a un Pod una vez creado. Sin embargo, por lo
general se eliminan y se reemplazan los Pods de manera controlada utilizando
<a class=glossary-tooltip title='Un objeto API que gestiona una aplicación replicada.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployments>Deployments</a>.</p><p>En ocasiones es necesario examinar el estado de un Pod existente, como por ejemplo,
para poder solucionar un error difícil de reproducir. Puede ejecutar en estos casos
un Container efímero en un Pod ya existente para examinar su estado y para ejecutar
comandos de manera arbitraria.</p><h3 id=qué-es-un-container-efímero>Qué es un Container efímero?</h3><p>Los Containers efímeros se diferencian de otros Containers en que no garantizan ni los
recursos ni la ejecución, y en que nunca se reiniciarán automáticamente, de modo que no
son aptos para la construcción de aplicaciones. Los Containers efímeros se describen
usando la misma <a href=/docs/reference/generated/kubernetes-api/v1.25/#container-v1-core>ContainerSpec</a> que los Containers regulares, aunque muchos campos son
incompatibles y no están habilitados para los Containers efímeros.</p><ul><li>Los Containers efímeros no pueden tener puertos, por lo que campos como <code>ports</code>,
<code>livenessProbe</code>, <code>readinessProbe</code> no están habilitados.</li><li>Las asignaciones de recursos del Pod son inmutables, por lo que no esta habilitado
configurar "resources".</li><li>Para obtener una lista completa de los campos habilitados, consulte la documentación
de referencia [EphemeralContainer] (/docs/reference/generated/kubernetes-api/v1.25/#ephemeralcontainer-v1-core).</li></ul><p>En vez de añadirlos de forma directa al <code>pod.spec</code>, los Containers efímeros se crean usando un
controlador especial de la API, <code>ephemeralcontainers</code>, por lo tanto no es posible añadir un
Container efímero utilizando <code>kubectl edit</code>.</p><p>Al igual en el caso de los Containers regulares, no se puede modificar o remover un Container
efímero después de haberlo agregado a un Pod.</p><h2 id=casos-de-uso-para-los-containers-efímeros>Casos de uso para los Containers efímeros</h2><p>Los Containers efímeros resultan útiles para la solución interactiva de incidencias cuando
<code>kubectl exec</code> es insuficiente tanto porque un container se ha caído, como porque la imagen de un
Container no incluye las utilidades de depuración.</p><p>En particular, las <a href=https://github.com/GoogleContainerTools/distroless>imágenes distroless</a>
le permiten desplegar imágenes de Containers mínimos que disminuyen la superficie de ataque
y la exposición a errores y vulnerabilidades. Ya que las imágenes distroless no contienen un
shell ni ninguna utilidad de depuración, resulta difícil solucionar los problemas de las imágenes
distroless usando solamente <code>kubectl exec</code>.</p><p>Cuando utilice Containers efímeros, es conveniente habilitar el <a href=/docs/tasks/configure-pod-container/share-process-namespace/>proceso Namespace de uso
compartido</a> para poder ver los
procesos en otros containers.</p><h3 id=ejemplos>Ejemplos</h3><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los ejemplos de esta sección requieren que los <code>EphemeralContainers</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature
gate</a> estén habilitados
y que tanto el cliente como el servidor de Kubernetes tengan la version v1.16 o posterior.</div><p>En los ejemplos de esta sección muestran la forma en que los Containers efímeros se
presentan en la API. Los usuarios normalmente usarían un plugin <code>kubectl</code> para la solución
de problemas que automatizaría estos pasos.</p><p>Los Containers efímeros son creados utilizando el subrecurso <code>ephemeralcontainers</code> del Pod,
que puede ser visto utilizando <code>kubectl --raw</code>. En primer lugar describa el Container
efímero a añadir como una lista de <code>EphemeralContainers</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;EphemeralContainers&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;example-pod&#34;</span>
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;ephemeralContainers&#34;</span>: [{
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;command&#34;</span>: [
</span></span><span style=display:flex><span>            <span style=color:#b44>&#34;sh&#34;</span>
</span></span><span style=display:flex><span>        ],
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;image&#34;</span>: <span style=color:#b44>&#34;busybox&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;imagePullPolicy&#34;</span>: <span style=color:#b44>&#34;IfNotPresent&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;debugger&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;stdin&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;tty&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;terminationMessagePolicy&#34;</span>: <span style=color:#b44>&#34;File&#34;</span>
</span></span><span style=display:flex><span>    }]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Para actualizar los Containers efímeros de los <code>example-pod</code> en ejecución:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl replace --raw /api/v1/namespaces/default/pods/example-pod/ephemeralcontainers  -f ec.json
</span></span></code></pre></div><p>Esto devolverá una nueva lista de Containers efímeros:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;kind&#34;</span>:<span style=color:#b44>&#34;EphemeralContainers&#34;</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>:<span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;metadata&#34;</span>:{
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>:<span style=color:#b44>&#34;example-pod&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>:<span style=color:#b44>&#34;default&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;selfLink&#34;</span>:<span style=color:#b44>&#34;/api/v1/namespaces/default/pods/example-pod/ephemeralcontainers&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;uid&#34;</span>:<span style=color:#b44>&#34;a14a6d9b-62f2-4119-9d8e-e2ed6bc3a47c&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;resourceVersion&#34;</span>:<span style=color:#b44>&#34;15886&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;creationTimestamp&#34;</span>:<span style=color:#b44>&#34;2019-08-29T06:41:42Z&#34;</span>
</span></span><span style=display:flex><span>   },
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;ephemeralContainers&#34;</span>:[
</span></span><span style=display:flex><span>      {
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;name&#34;</span>:<span style=color:#b44>&#34;debugger&#34;</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;image&#34;</span>:<span style=color:#b44>&#34;busybox&#34;</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;command&#34;</span>:[
</span></span><span style=display:flex><span>            <span style=color:#b44>&#34;sh&#34;</span>
</span></span><span style=display:flex><span>         ],
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;resources&#34;</span>:{
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>         },
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;terminationMessagePolicy&#34;</span>:<span style=color:#b44>&#34;File&#34;</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;imagePullPolicy&#34;</span>:<span style=color:#b44>&#34;IfNotPresent&#34;</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;stdin&#34;</span>:<span style=color:#a2f;font-weight:700>true</span>,
</span></span><span style=display:flex><span>         <span style=color:green;font-weight:700>&#34;tty&#34;</span>:<span style=color:#a2f;font-weight:700>true</span>
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>   ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Se puede ver el estado del Container efímero creado usando <code>kubectl describe</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pod example-pod
</span></span></code></pre></div><pre tabindex=0><code>...
Ephemeral Containers:
  debugger:
    Container ID:  docker://cf81908f149e7e9213d3c3644eda55c72efaff67652a2685c1146f0ce151e80f
    Image:         busybox
    Image ID:      docker-pullable://busybox@sha256:9f1003c480699be56815db0f8146ad2e22efea85129b5b5983d0e0fb52d9ab70
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      sh
    State:          Running
      Started:      Thu, 29 Aug 2019 06:42:21 +0000
    Ready:          False
    Restart Count:  0
    Environment:    &lt;none&gt;
    Mounts:         &lt;none&gt;
...
</code></pre><p>Se puede conectar al nuevo Container efímero usando <code>kubectl attach</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl attach -it example-pod -c debugger
</span></span></code></pre></div><p>Si el proceso Namespace de uso compartido está habilitado, se pueden visualizar los procesos de todos los Containers de ese Pod.
Por ejemplo, después de haber conectado, ejecute <code>ps</code> en el debugger del container:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ps auxww
</span></span></code></pre></div><p>La respuesta es semejante a:</p><pre tabindex=0><code>PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    6 root      0:00 nginx: master process nginx -g daemon off;
   11 101       0:00 nginx: worker process
   12 101       0:00 nginx: worker process
   13 101       0:00 nginx: worker process
   14 101       0:00 nginx: worker process
   15 101       0:00 nginx: worker process
   16 101       0:00 nginx: worker process
   17 101       0:00 nginx: worker process
   18 101       0:00 nginx: worker process
   19 root      0:00 /pause
   24 root      0:00 sh
   29 root      0:00 ps auxww
</code></pre></div><div class=td-content style=page-break-before:always><h1 id=pg-89637410cacae45a36ab1cc278c482eb>2 - Controladores</h1></div><div class=td-content><h1 id=pg-d459b930218774655fa7fd1620625539>2.1 - ReplicaSet</h1><p>El objeto de un ReplicaSet es el de mantener un conjunto estable de réplicas de Pods ejecutándose
en todo momento. Así, se usa en numerosas ocasiones para garantizar la disponibilidad de un
número específico de Pods idénticos.</p><h2 id=cómo-funciona-un-replicaset>Cómo funciona un ReplicaSet</h2><p>Un ReplicaSet se define con campos, incluyendo un selector que indica cómo identificar a los Pods que puede adquirir,
un número de réplicas indicando cuántos Pods debería gestionar, y una plantilla pod especificando los datos de los nuevos Pods
que debería crear para conseguir el número de réplicas esperado. Un ReplicaSet alcanza entonces su propósito
mediante la creación y eliminación de los Pods que sea necesario para alcanzar el número esperado.
Cuando un ReplicaSet necesita crear nuevos Pods, utiliza su plantilla Pod.</p><p>El enlace que un ReplicaSet tiene hacia sus Pods es a través del campo del Pod denominado <a href=/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents>metadata.ownerReferences</a>,
el cual indica qué recurso es el propietario del objeto actual. Todos los Pods adquiridos por un ReplicaSet tienen su propia
información de identificación del ReplicaSet en su campo ownerReferences. Y es a través de este enlace
cómo el ReplicaSet conoce el estado de los Pods que está gestionando y actúa en consecuencia.</p><p>Un ReplicaSet identifica los nuevos Pods a adquirir usando su selector. Si hay un Pod que no tiene OwnerReference
o donde OwnerReference no es un controlador, pero coincide con el selector del ReplicaSet,
este será inmediatamente adquirido por dicho ReplicaSet.</p><h2 id=cuándo-usar-un-replicaset>Cuándo usar un ReplicaSet</h2><p>Un ReplicaSet garantiza que un número específico de réplicas de un pod se está ejecutando en todo momento.
Sin embargo, un Deployment es un concepto de más alto nivel que gestiona ReplicaSets y
proporciona actualizaciones de forma declarativa de los Pods junto con muchas otras características útiles.
Por lo tanto, se recomienda el uso de Deployments en vez del uso directo de ReplicaSets, a no ser
que se necesite una orquestración personalizada de actualización o no se necesite las actualizaciones en absoluto.</p><p>En realidad, esto quiere decir que puede que nunca necesites manipular los objetos ReplicaSet:
en vez de ello, usa un Deployment, y define tu aplicación en la sección spec.</p><h2 id=ejemplo>Ejemplo</h2><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/frontend.yaml download=controllers/frontend.yaml><code>controllers/frontend.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-frontend-yaml")' title="Copy controllers/frontend.yaml to clipboard"></img></div><div class=includecode id=controllers-frontend-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># modifica las réplicas según tu caso de uso</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>php-redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google_samples/gb-frontend:v3<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Si guardas este manifiesto en un archivo llamado <code>frontend.yaml</code> y lo lanzas en un clúster de Kubernetes,
se creará el ReplicaSet definido y los Pods que maneja.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f http://k8s.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>Puedes ver los ReplicaSets actuales desplegados:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>Y ver el frontend que has creado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME       DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>frontend   <span style=color:#666>3</span>         <span style=color:#666>3</span>         <span style=color:#666>3</span>       6s
</span></span></code></pre></div><p>También puedes comprobar el estado del replicaset:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe rs/frontend
</span></span></code></pre></div><p>Y verás una salida parecida a la siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Name:		frontend
</span></span><span style=display:flex><span>Namespace:	default
</span></span><span style=display:flex><span>Selector:	<span style=color:#b8860b>tier</span><span style=color:#666>=</span>frontend,tier in <span style=color:#666>(</span>frontend<span style=color:#666>)</span>
</span></span><span style=display:flex><span>Labels:		<span style=color:#b8860b>app</span><span style=color:#666>=</span>guestbook
</span></span><span style=display:flex><span>		<span style=color:#b8860b>tier</span><span style=color:#666>=</span>frontend
</span></span><span style=display:flex><span>Annotations:	&lt;none&gt;
</span></span><span style=display:flex><span>Replicas:	<span style=color:#666>3</span> current / <span style=color:#666>3</span> desired
</span></span><span style=display:flex><span>Pods Status:	<span style=color:#666>3</span> Running / <span style=color:#666>0</span> Waiting / <span style=color:#666>0</span> Succeeded / <span style=color:#666>0</span> Failed
</span></span><span style=display:flex><span>Pod Template:
</span></span><span style=display:flex><span>  Labels:       <span style=color:#b8860b>app</span><span style=color:#666>=</span>guestbook
</span></span><span style=display:flex><span>                <span style=color:#b8860b>tier</span><span style=color:#666>=</span>frontend
</span></span><span style=display:flex><span>  Containers:
</span></span><span style=display:flex><span>   php-redis:
</span></span><span style=display:flex><span>    Image:      gcr.io/google_samples/gb-frontend:v3
</span></span><span style=display:flex><span>    Port:       80/TCP
</span></span><span style=display:flex><span>    Requests:
</span></span><span style=display:flex><span>      cpu:      100m
</span></span><span style=display:flex><span>      memory:   100Mi
</span></span><span style=display:flex><span>    Environment:
</span></span><span style=display:flex><span>      GET_HOSTS_FROM:   dns
</span></span><span style=display:flex><span>    Mounts:             &lt;none&gt;
</span></span><span style=display:flex><span>  Volumes:              &lt;none&gt;
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
</span></span><span style=display:flex><span>  ---------    --------    -----    ----                -------------    --------    ------            -------
</span></span><span style=display:flex><span>  1m           1m          <span style=color:#666>1</span>        <span style=color:#666>{</span>replicaset-controller <span style=color:#666>}</span>             Normal      SuccessfulCreate  Created pod: frontend-qhloh
</span></span><span style=display:flex><span>  1m           1m          <span style=color:#666>1</span>        <span style=color:#666>{</span>replicaset-controller <span style=color:#666>}</span>             Normal      SuccessfulCreate  Created pod: frontend-dnjpy
</span></span><span style=display:flex><span>  1m           1m          <span style=color:#666>1</span>        <span style=color:#666>{</span>replicaset-controller <span style=color:#666>}</span>             Normal      SuccessfulCreate  Created pod: frontend-9si5l
</span></span></code></pre></div><p>Y por último, puedes comprobar los Pods que ha arrancado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get Pods
</span></span></code></pre></div><p>Deberías ver la información de cada Pod similar a:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-9si5l   1/1       Running   <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-dnjpy   1/1       Running   <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-qhloh   1/1       Running   <span style=color:#666>0</span>          1m
</span></span></code></pre></div><p>También puedes verificar que la referencia de propietario de dichos pods está puesta al ReplicaSet frontend.
Para ello, obtén el yaml de uno de los Pods ejecutándose:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods frontend-9si5l -o yaml
</span></span></code></pre></div><p>La salida será parecida a esta, donde la información sobre el ReplicaSet aparece en el campo ownerReferences de los metadatos:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  creationTimestamp: 2019-01-31T17:20:41Z
</span></span><span style=display:flex><span>  generateName: frontend-
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    tier: frontend
</span></span><span style=display:flex><span>  name: frontend-9si5l
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: extensions/v1beta1
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    controller: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    kind: ReplicaSet
</span></span><span style=display:flex><span>    name: frontend
</span></span><span style=display:flex><span>    uid: 892a2330-257c-11e9-aecd-025000000001
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=adquisiciones-de-pods-fuera-de-la-plantilla>Adquisiciones de Pods fuera de la plantilla</h2><p>Aunque puedes crear Pods simples sin problemas, se recomienda encarecidamente asegurarse de que dichos Pods no tienen
etiquetas que puedan coincidir con el selector de alguno de tus ReplicaSets.
La razón de esta recomendación es que un ReplicaSet no se limita a poseer los Pods
especificados en su plantilla -- sino que puede adquirir otros Pods como se explicó en secciones anteriores.</p><p>Toma el ejemplo anterior del ReplicaSet frontend, y los Pods especificados en el siguiente manifiesto:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/pods/pod-rs.yaml download=pods/pod-rs.yaml><code>pods/pod-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-rs-yaml")' title="Copy pods/pod-rs.yaml to clipboard"></img></div><div class=includecode id=pods-pod-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:2.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:1.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Como estos Pods no tienen un Controlador (o cualquier otro objeto) como referencia de propietario
y como además su selector coincide con el del ReplicaSet frontend, este último los terminará adquiriendo de forma inmediata.</p><p>Supón que creas los Pods después de que el ReplicaSet frontend haya desplegado los suyos
para satisfacer su requisito de cuenta de réplicas:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f http://k8s.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>Los nuevos Pods serán adquiridos por el ReplicaSet, e inmediatamente terminados ya que
el ReplicaSet estaría por encima del número deseado.</p><p>Obtener los Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get Pods
</span></span></code></pre></div><p>La salida muestra que los nuevos Pods se han terminado, o están en el proceso de terminarse:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY   STATUS        RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-9si5l   1/1     Running       <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-dnjpy   1/1     Running       <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-qhloh   1/1     Running       <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>pod2             0/1     Terminating   <span style=color:#666>0</span>          4s
</span></span></code></pre></div><p>Si creas primero los Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f http://k8s.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>Y entonces creas el ReplicaSet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f http://k8s.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>Verás que el ReplicaSet ha adquirido dichos Pods y simplemente ha creado tantos nuevos
como necesarios para cumplir con su especificación hasta que el número de
sus nuevos Pods y los originales coincidan con la cuenta deseado. Al obtener los Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get Pods
</span></span></code></pre></div><p>Veremos su salida:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-pxj4r   1/1     Running   <span style=color:#666>0</span>          5s
</span></span><span style=display:flex><span>pod1             1/1     Running   <span style=color:#666>0</span>          13s
</span></span><span style=display:flex><span>pod2             1/1     Running   <span style=color:#666>0</span>          13s
</span></span></code></pre></div><p>De esta forma, un ReplicaSet puede poseer un conjunto no homogéneo de Pods</p><h2 id=escribir-un-manifiesto-de-replicaset>Escribir un manifiesto de ReplicaSet</h2><p>Al igual que con el esto de los objeto de la API de Kubernetes, un ReplicaSet necesita los campos
<code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>. Para los ReplicaSets, el tipo es siempre ReplicaSet.
En la versión 1.9 de Kubernetes, la versión <code>apps/v1</code> de la API en un tipo ReplicaSet es la versión actual y está habilitada por defecto.
La versión <code>apps/v1beta2</code> de la API se ha desaprobado.
Consulta las primeras líneas del ejemplo <code>frontend.yaml</code> como guía.</p><p>Un ReplicaSet también necesita una <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>sección <code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/Pods/pod-overview/#pod-templates>plantilla pod</a> que es
también necesita obligatoriamente tener etiquetas definidas. En nuestro ejemplo <code>frontend.yaml</code> teníamos una etiqueta: <code>tier: frontend</code>.
Lleva cuidado de que no se entremezcle con los selectores de otros controladores, no sea que traten de adquirir este Pod.</p><p>Para el campo de <a href=/docs/concepts/workloads/Pods/pod-lifecycle/#restart-policy>regla de reinicio</a> de la plantilla,
<code>.spec.template.spec.restartPolicy</code>, el único valor permitido es <code>Always</code>, que es el valor predeterminado.</p><h3 id=selector-de-pod>Selector de Pod</h3><p>El campo <code>.spec.selector</code> es un <a href=/docs/concepts/overview/working-with-objects/labels/>selector de etiqueta</a>.
Como se explicó <a href=#how-a-replicaset-works>anteriormente</a>, estas son las etiquetas que se usan para
identificar los Pods potenciales a adquirir. En nuestro ejemplo <code>frontend.yaml</code>, el selector era:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>matchLabels:
</span></span><span style=display:flex><span>	tier: frontend
</span></span></code></pre></div><p>El el ReplicaSet, <code>.spec.template.metadata.labels</code> debe coincidir con <code>spec.selector</code>, o será
rechazado por la API.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Cuando 2 ReplicaSets especifican el mismo campo <code>.spec.selector</code>, pero los campos
<code>.spec.template.metadata.labels</code> y <code>.spec.template.spec</code> diferentes, cada ReplicaSet
ignora los Pods creados por el otro ReplicaSet.</div><h3 id=réplicas>Réplicas</h3><p>Puedes configurar cuántos Pods deberían ejecutarse de forma concurrente indicando el campo <code>.spec.replicas</code>.
El ReplicaSet creará/eliminará sus Pods para alcanzar este número.</p><p>Si no indicas el valor del campo <code>.spec.replicas</code>, entonces por defecto se inicializa a 1.</p><h2 id=trabajar-con-replicasets>Trabajar con ReplicaSets</h2><h3 id=eliminar-un-replicaset-y-sus-pods>Eliminar un ReplicaSet y sus Pods</h3><p>Para eliminar un ReplicaSet y todos sus Pods, utiliza el comando <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>.
El <a href=/docs/concepts/workloads/controllers/garbage-collection/>Recolector de basura</a> eliminará automáticamente
todos los Pods subordinados por defecto.</p><p>Cuando se usa la API REST o la librería <code>client-go</code>, se debe poner el valor de <code>propagationPolicy</code> a <code>Background</code> o
<code>Foreground</code> en la opción -d.
Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><h3 id=eliminar-sólo-un-replicaset>Eliminar sólo un ReplicaSet</h3><p>Se puede eliminar un ReplicaSet sin afectar a ninguno de sus Pods usando el comando <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a> con la opción <code>--cascade=false</code>.
Cuando se usa la API REST o la librería <code>client-go</code>, se debe poner <code>propagationPolicy</code> a <code>Orphan</code>.
Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>Una vez que se ha eliminado el original, se puede crear un nuevo ReplicaSet para sustituirlo.
Mientras el viejo y el nuevo <code>.spec.selector</code> sean el mismo, el nuevo adoptará a los viejos Pods.
Sin embargo, no se esforzará en conseguir que los Pods existentes coincidan con una plantilla pod nueva, diferente.
Para actualizar dichos Pods a la nueva especificación de forma controlada,
usa una <a href=#rolling-updates>actualización en línea</a>.</p><h3 id=aislar-pods-de-un-replicaset>Aislar Pods de un ReplicaSet</h3><p>Es posible aislar Pods de un ReplicaSet cambiando sus etiquetas. Esta técnica puede usarse
para eliminar Pods de un servicio para poder depurar, recuperar datos, etc. Los Pods
que se eliminar de esta forma serán sustituidos de forma automática (siempre que el
número de réplicas no haya cambiado).</p><h3 id=escalar-un-replicaset>Escalar un ReplicaSet</h3><p>Se puede aumentar o reducir fácilmente un ReplicaSet simplemente actualizando el campo <code>.spec.replicas</code>.
El controlador del ReplicaSet se asegura de que el número deseado de Pods con un selector
de etiquetas coincidente está disponible y operacional.</p><h3 id=replicaset-como-blanco-de-un-horizontal-pod-autoscaler>ReplicaSet como blanco de un Horizontal Pod Autoscaler</h3><p>Un ReplicaSet puede también ser el blanco de un
<a href=/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscalers (HPA)</a>. Esto es,
un ReplicaSet puede auto-escalarse mediante un HPA. Aquí se muestra un ejemplo de HPA dirigido
al ReplicaSet que creamos en el ejemplo anterior.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/hpa-rs.yaml download=controllers/hpa-rs.yaml><code>controllers/hpa-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-hpa-rs-yaml")' title="Copy controllers/hpa-rs.yaml to clipboard"></img></div><div class=includecode id=controllers-hpa-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>autoscaling/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>HorizontalPodAutoscaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-scaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scaleTargetRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>minReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>maxReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>targetCPUUtilizationPercentage</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Si guardas este manifiesto en un archivo <code>hpa-rs.yaml</code> y lo lanzas contra el clúster de Kubernetes,
debería crear el HPA definido que auto-escala el ReplicaSet destino dependiendo del uso
de CPU de los Pods replicados.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
</span></span></code></pre></div><p>Alternativamente, puedes usar el comando <code>kubectl autoscale</code> para conseguir el mismo objetivo
(¡y mucho más fácil!)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale rs frontend --max<span style=color:#666>=</span><span style=color:#666>10</span>
</span></span></code></pre></div><h2 id=alternativas-al-replicaset>Alternativas al ReplicaSet</h2><h3 id=deployment-recomendado>Deployment (recomendado)</h3><p>Un<a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> es un objeto que puede poseer ReplicaSets
y actualizar a estos y a sus Pods mediante actualizaciones en línea declarativas en el servidor.
Aunque que los ReplicaSets puede usarse independientemente, hoy en día se usan principalmente a través de los Deployments
como el mecanismo para orquestrar la creación, eliminación y actualización de los Pods.
Cuando usas Deployments no tienes que preocuparte de gestionar los ReplicaSets que crean.
Los Deployments poseen y gestionan sus ReplicaSets.
Por tanto, se recomienda que se use Deployments cuando se quiera ReplicaSets.</p><h3 id=pods-simples>Pods simples</h3><p>A diferencia del caso en que un usuario creaba Pods de forma directa, un ReplicaSet sustituye los Pods que se eliminan
o se terminan por la razón que sea, como en el caso de un fallo de un nodo o
una intervención disruptiva de mantenimiento, como una actualización de kernel.
Por esta razón, se recomienda que se use un ReplicaSet incluso cuando la aplicación
sólo necesita un único Pod. Entiéndelo de forma similar a un proceso supervisor,
donde se supervisa múltiples Pods entre múltiples nodos en vez de procesos individuales
en un único nodo. Un ReplicaSet delega los reinicios del contenedor local a algún agente
del nodo (por ejemplo, Kubelet o Docker).</p><h3 id=job>Job</h3><p>Usa un <a href=/docs/concepts/jobs/run-to-completion-finite-workloads/><code>Job</code></a> en vez de un ReplicaSet para
aquellos Pods que se esperan que terminen por ellos mismos (esto es, trabajos por lotes).</p><h3 id=daemonset>DaemonSet</h3><p>Usa un <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> en vez de un ReplicaSet para aquellos
Pods que proporcionan funcionalidad a nivel de servidor, como monitorización de servidor o
logging de servidor. Estos Pods tienen un ciclo de vida asociado al del servidor mismo:
el Pod necesita ejecutarse en el servidor antes de que los otros Pods comiencen, y es seguro
que terminen cuando el servidor esté listo para ser reiniciado/apagado.</p><h3 id=replicationcontroller>ReplicationController</h3><p>Los ReplicaSets son los sucesores de los <a href=/docs/concepts/workloads/controllers/replicationcontroller/><em>ReplicationControllers</em></a>.
Los dos sirven al mismo propósito, y se comportan de forma similar, excepto porque un ReplicationController
no soporta los requisitos del selector basado en conjunto, como se describe en la <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>guía de usuario de etiquetas</a>.
Por ello, se prefiere los ReplicaSets a los ReplicationControllers.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-27f1331d515d95f76aa1156088b4ad91>2.2 - ReplicationController</h1><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> hoy en día la forma recomendada de configurar la replicación es con un <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> que configura un <a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a>.</div><p>Un <em>ReplicationController</em> garantiza que un número determinado de réplicas se estén ejecutando
en todo momento. En otras palabras, un ReplicationController se asegura que un pod o un conjunto homogéneo de pods
siempre esté arriba y disponible.</p><h2 id=cómo-funciona-un-replicationcontroller>Cómo Funciona un ReplicationController</h2><p>Si hay muchos pods, el ReplicationController termina los pods extra. Si hay muy pocos, el
ReplicationController arranca más pods. A difrencia de los pods creados manualmente, los pods mantenidos por un
ReplicationController se sustituyen de forma automática si fallan, se borran, o se terminan.
Por ejemplo, tus pods se re-crean en un nodo durante una intervención disruptiva de mantenimiento como una actualización del kernel.
Por esta razón, deberías usar un ReplicationController incluso cuando tu aplicación sólo necesita
un único pod. Un ReplicationController es parecido a un supervisor de procesos,
pero en vez de supervisar procesos individuales en un único nodo,
el ReplicationController supervisa múltiples pods entre múltiples nodos.</p><p>A menudo nos referimos a un ReplicationController de forma abreviada como "rc" o "rcs", así como
atajo en los comandos de kubectl.</p><p>Un caso simple es crear un objeto ReplicationController para ejecutar de manera fiable una instancia
de un Pod indefinidamente. Un caso de uso más complejo es ejecutar varias réplicas idénticas
de un servicio replicado, como los servidores web.</p><h2 id=ejecutar-un-ejemplo-de-replicationcontroller>Ejecutar un ejemplo de ReplicationController</h2><p>Esta configuración de un ReplicationController de ejemplo ejecuta tres copias del servidor web nginx.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/replication.yaml download=controllers/replication.yaml><code>controllers/replication.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-replication-yaml")' title="Copy controllers/replication.yaml to clipboard"></img></div><div class=includecode id=controllers-replication-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicationController<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Ejecuta el ejemplo descargando el archivo de ejemplo y ejecutando este comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
</span></span></code></pre></div><pre tabindex=0><code>replicationcontroller/nginx created
</code></pre><p>Comprueba el estado del ReplicationController con este comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe replicationcontrollers/nginx
</span></span></code></pre></div><pre tabindex=0><code>Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
</code></pre><p>Como se puede observar, se han creado tres pods, pero ninguno se está ejecutándose todavía,
puede que porque la imagen todavía se está descargando.
Unos momentos después, el mismo comando puede que muestre:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Pods Status:    <span style=color:#666>3</span> Running / <span style=color:#666>0</span> Waiting / <span style=color:#666>0</span> Succeeded / <span style=color:#666>0</span> Failed
</span></span></code></pre></div><p>Para listar todos los pods que pertenecen al ReplicationController de forma legible,
puedes usar un comando como el siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>pods</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl get pods --selector<span style=color:#666>=</span><span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx --output<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>={</span>.items..metadata.name<span style=color:#666>}</span><span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><pre tabindex=0><code>nginx-3ntk0 nginx-4ok8v nginx-qrm3m
</code></pre><p>Como se puede ver, el selector es el mismo que el selector del ReplicationController (mostrado en la salida de
<code>kubectl describe</code>), y con una forma diferente a lo definido en el archivo <code>replication.yaml</code>.
La opción <code>--output=jsonpath</code> especifica una expresión que simplemente muestra el nombre
de cada pod en la lista devuelta.</p><h2 id=escribir-una-especificación-de-replicationcontroller>Escribir una especificación de ReplicationController</h2><p>Al igual que con el resto de configuraciones de Kubernetes, un ReplicationController necesita los campos <code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>.
Para información general acerca del trabajo con archivos de configuración, ver la <a href=/docs/concepts/overview/object-management-kubectl/overview/>gestión de objetos</a>.</p><p>Un ReplicationController también necesita un <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>sección <code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>El campo <code>.spec.template</code> es el único campo requerido de <code>.spec</code>.</p><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/pods/pod-overview/#pod-templates>plantilla pod</a>.
Tiene exactamente el mismo esquema que un <a href=/docs/concepts/workloads/pods/pod/>pod</a>, excepto por el hecho de que está anidado y no tiene los campos <code>apiVersion</code> ni <code>kind</code>.</p><p>Además de los campos obligatorios de un Pod, una plantilla pod de un ReplicationController debe especificar las etiquetas apropiadas
y la regla de reinicio apropiada. En el caso de las etiquetas, asegúrate que no se entremezclan con otros controladores. Ver el <a href=#pod-selector>selector de pod</a>.</p><p>Sólo se permite el valor <code>Always</code> para el campo <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>.spec.template.spec.restartPolicy</code></a>,
que es el valor predeterminado si no se indica.</p><p>Para los reinicios locales de los contenedores, los ReplicationControllers delegan en los agentes del nodo,
por ejmplo el <a href=/docs/admin/kubelet/>Kubelet</a> o Docker.</p><h3 id=etiquetas-en-los-replicationcontroller>Etiquetas en los ReplicationController</h3><p>los ReplicationController puede tener sus propias (<code>.metadata.labels</code>). Normalmente, se indicaría dichas etiquetas
con los mismos valores que el campo <code>.spec.template.metadata.labels</code>; si el campo <code>.metadata.labels</code> no se indica,
entonces se predetermina al valor de <code>.spec.template.metadata.labels</code>. Sin embargo, se permite que sean diferentes,
y el valor de <code>.metadata.labels</code> no afecta al comportamiento del ReplicationController.</p><h3 id=selector-de-pod>Selector de Pod</h3><p>El campo <code>.spec.selector</code> es un <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>selector de etiqueta</a>. Un ReplicationController
gestiona todos los pods con etiquetas que coinciden con el selector. No distingue entre
pods que creó o eliminó, y pods que otra persona o proceso creó o eliminó. Esto permite sustituir al ReplicationController sin impactar a ninguno de sus pods que se esté ejecutando.</p><p>Si se indica, el valor de <code>.spec.template.metadata.labels</code> debe ser igual al de <code>.spec.selector</code>, o será rechazado por la API.
Si no se indica el valor de <code>.spec.selector</code>, se tomará como predeterminado el de <code>.spec.template.metadata.labels</code>.</p><p>Tampoco deberías crear ningún pod cuyas etiquetas coincidan con las de este selector, ni directamente con
otro ReplicationController, ni con otro controlador como un Job. Si lo haces, el
ReplicationController piensa que el creó también los otros pods. Kubernetes no te impide hacerlo.</p><p>Si al final terminas con múltiples controladores que tienen selectores que se entremezclan,
tendrás que gestionar la eliminación tú mismo (ver <a href=#working-with-replicationcontrollers>abajo</a>).</p><h3 id=múltiples-réplicas>Múltiples Réplicas</h3><p>Puedes configurar cuántos pods deberían ejecutarse de forma concurrente poniendo el valor de <code>.spec.replicas</code> al número
de pods que te gustaría tener ejecutándose a la vez. El número de ejecuciones en cualquier momento puede que sea superior
o inferior, dependiendo de si las réplicas se han incrementado o decrementado, o si un pod se ha apagado de forma controlada,
y su sustituto arranca más pronto.</p><p>Si no se indica el valor de <code>.spec.replicas</code>, entonces se predetermina a 1.</p><h2 id=trabajar-con-replicationcontrollers>Trabajar con ReplicationControllers</h2><h3 id=eliminar-un-replicationcontroller-y-sus-pods>Eliminar un ReplicationController y sus Pods</h3><p>Para eliminar un ReplicationController y todos sus pods, usa el comando <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>. Kubectl reducirá el ReplicationController a cero y esperará
que elimine cada pod antes de eliminar al ReplicationController mismo. Si este comando kubectl
se interrumpe, puede ser reiniciado.</p><p>Cuando uses la API REST o la librería Go, necesitas realizar los pasos de forma explícita (reducir las réplicas a cero,
esperar a que se eliminen los pods, y entonces eliminar el ReplicationController).</p><h3 id=eliminar-sólo-el-replicationcontroller>Eliminar sólo el ReplicationController</h3><p>Puedes eliminar un ReplicationController sin impactar a ninguno de sus Pods.</p><p>Usando kubectl, indica la opción <code>--cascade=false</code> en el comando <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>.</p><p>Cuando uses la API REST o la librería Go, simplemente elimina objeto ReplicationController.</p><p>Una vez que el objeto original se ha eliminado, puedes crear un nuevo ReplicationController para sustituirlo.
Mientras el viejo y el nuevo valor del <code>.spec.selector</code> sea el mismo, el nuevo adoptará a los viejos pods.
Sin embargo, no se molestará en hacer que los pods actuales coincidan con una plantilla pod nueva, diferente.
Para actualizar los pods con una nueva especificación de forma controlada, utiliza la <a href=#rolling-updates>actualización en línea</a>.</p><h3 id=aislar-pods-de-un-replicationcontroller>Aislar pods de un ReplicationController</h3><p>Se puede aislar Pods del conjunto destino de un ReplicationController cambiando sus etiquetas.
Esta técnica puede usarse para eliminar pods de un servicio para poder depurarlos, recuperar datos, etc.
Los Pods que se eliminan de esta forma serán sustituidos de forma automática (asumiendo que el número de réplicas no ha cambiado tampoco).</p><h2 id=patrones-comunes-de-uso>Patrones comunes de uso</h2><h3 id=reprogramación>Reprogramación</h3><p>Como se comentó arriba, cuando tienes 1 pod que quieres mantener ejecutándose, o 1000, un ReplicationController se asegura de que el número indicado de pods exista,
incluso si falla un nodo o se termina algún pod (por ejemplo, debido a alguna acción de otro agente de control).</p><h3 id=escalado>Escalado</h3><p>El ReplicationController facilita el escalado del número de réplicas tanto para su aumento como para su disminución,
bien manualmente o mediante un agente de auto-escalado, simplemente actualizando el campo <code>replicas</code>.</p><h3 id=actualizaciones-en-línea>Actualizaciones en línea</h3><p>El ReplicationController se ha diseñado para facilitar las actualizaciones en línea de un servicio mediante la sustitución de sus pods uno por uno.</p><p>Cómo se explicó en <a href=http://issue.k8s.io/1353>#1353</a>, la estrategia recomendada es crear un nuevo ReplicationController con 1 réplica,
escalar el nuevo (+1) y el viejo (-1) controlador uno por uno, y entonces eliminar el viejo controlador una vez que alcanza las 0 réplicas.
Esto actualiza de forma predecible el conjunto de pods independientemente de que se produzcan fallos inesperados.</p><p>De forma ideal, el controlador de actualización en línea tendrá en cuenta si la aplicación está lista, y
se asegurará de que un número suficiente de pods está en servicio en todo momento.</p><p>Los dos ReplicationControllers necesitarán crear pods con al menos una etiqueta diferenciadora, como la etiqueta de imagen del contenedor primario del pod,
ya que las actualizaciones de imagen son las que normalmente desencadenan las actualizaciones en línea.</p><p>La actualización en línea se implementa a través de la herramienta cliente mediante
<a href=/docs/reference/generated/kubectl/kubectl-commands#rolling-update><code>kubectl rolling-update</code></a>.
Echa un vistazo a la <a href=/docs/tasks/run-application/rolling-update-replication-controller/>tarea <code>kubectl rolling-update</code></a> para más ejemplos concretos.</p><h3 id=múltiples-operaciones-de-despliegue>Múltiples operaciones de despliegue</h3><p>Además de llevar a cabo múltiples despliegues de una aplicación cuando una actualización en línea está en progreso,
es común ejecutar varios despliegues durante un período extendido de tiempo, o incluso de forma contínua, usando múltiples operaciones de despliegue. Dichas operaciones se diferenciarían por etiquetas.</p><p>Por ejemplo, un servicio puede que exponga todos los pods con etiquetas <code>tier in (frontend), environment in (prod)</code>. Ahora digamos que tenemos 10 pods replicados que forman este grupo.
Pero queremos poder desplegar una nueva versión 'canary' de este component. Se podría configurar un ReplicationController con el valor de <code>replicas</code> puesto a 9 para la mayor parte de las réplicas,
con etiquetas <code>tier=frontend, environment=prod, track=stable</code>, y otro ReplicationController con el valor de <code>replicas</code> puesto a 1 para el 'canary',
con las etiquetas <code>tier=frontend, environment=prod, track=canary</code>. Así el servicio cubriría tanto los pods canary como el resto.
Pero también es posible trastear con los ReplicationControllers de forma separada para probar cosas, monitorizar los resultados, etc.</p><h3 id=usar-replicationcontrollers-con-servicios>Usar ReplicationControllers con servicios</h3><p>Un único servicio puede exponer múltiples ReplicationControllers, de forma que, por ejemplo, algo de tráfico
vaya a la versión vieja, y otro tanto vaya a la versión nueva.</p><p>Un ReplicationController nunca se terminará por sí mismo, pero tampoco se espera que se ejecute permanentemente como los servicios.
Los servicios puede que estén compuestos de pods controlados por múltiples ReplicationControllers,
y se espera que muchos ReplicationControllers se creen y se destruyan durante el ciclo de vida de un servicio (por ejemplo,
para realizar una actualización de los pods que ejecutan el servicio). Ambos servicios mismos y sus clientes deberían permanecer
ajenos a los ReplicationControllers que mantienen los pods que proporcionan los servicios.</p><h2 id=escribir-aplicaciones-que-se-repliquen>Escribir aplicaciones que se repliquen</h2><p>Los Pods creados por un ReplicationController están pensados para que sean intercambiables y semánticamente idénticos,
aunque sus configuraciones puede que sean heterogéneas a lo largo del tiempo. Este es un ajuste obvio para los servidores sin estado replicados,
pero los ReplicationControllers también pueden utilizarse para mantener la disponibilidad de aplicaciones que se elijen por un maestro, las particionadas, y las de grupos de trabajadores.
Dichas aplicaciones deberían usar los mecanismos de asignación dinámica de trabajo, como las <a href=https://www.rabbitmq.com/tutorials/tutorial-two-python.html>colas de trabajo RabbitMQ</a>,
en vez de la personalización estática/de una sola vez en la configuración de cada pod,
ya que se considera un anti-patrón. Cualquier personalización de pod que se haga, como el calibrado vertical automático de recursos (por ejemplo, cpu o memoria),
debería realizarse a través de otro proceso de controlador en línea, no con el mismo ReplicationController.</p><h2 id=responsabilidades-de-un-replicationcontroller>Responsabilidades de un ReplicationController</h2><p>El ReplicationController simplemente garantiza que el número deseado de pods coincide con su selector de etiqueta y que son operacionales.
Actualmente, sólo los pods que han terminado se excluyen de la cuenta. En el futuro, la <a href=http://issue.k8s.io/620>disponibilidad</a> y otra información disponible en el sistema
se tendrá en cuenta, se añadirá más controles sobre la regla de sussitución, y se está planificando
emitir eventos que podrían ser aprovechados por clientes externos para implementar reglas complejas de sustitución y escalado de forma arbitraria.</p><p>El ReplicationController está siempre condicionado a esta reducida responsabilidad.
Él mismo no llevará a cabo ni pruebas de estar listo ni vivo. En vez de aplicar el auto-escalado,
se pretende que este sea realizado por un auto-escalador externo (como se vio en <a href=http://issue.k8s.io/492>#492</a>), que sería el encargado de cambiar su campo <code>replicas</code>.
No se añadirá reglas de programación (por ejemplo, <a href=http://issue.k8s.io/367#issuecomment-48428019>propagación</a>) al ReplicationController.
Ni se debería validar que los pods controlados coincidan con la plantilla actual especificada, ya que eso obstruiría el auto-calibrado y otros procesos automáticos.
De forma similar, los vencimientos de término, las dependencias de orden, la extensión de la configuración, y otras características se aplican en otro lado.
Incluso se plantea excluir el mecanismo de creación de pods a granel (<a href=http://issue.k8s.io/170>#170</a>).</p><p>El ReplicationController está pensado para ser una primitiva de bloques is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The "macro" operations currently supported by kubectl (run, scale, rolling-update) are proof-of-concept examples of this. For instance, we could imagine something like <a href=http://techblog.netflix.com/2012/06/asgard-web-based-cloud-management-and.html>Asgard</a> managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.</p><h2 id=objeto-api>Objeto API</h2><p>El ReplicationController es un recurso de alto nivel en la API REST de Kubernetes. Más detalles acerca del
objeto API se pueden encontrar aquí:
<a href=/docs/reference/generated/kubernetes-api/v1.25/#replicationcontroller-v1-core>Objeto API ReplicationController</a>.</p><h2 id=alternativas-al-replicationcontroller>Alternativas al ReplicationController</h2><h3 id=replicaset>ReplicaSet</h3><p>El <a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a> es el ReplicationController de nueva generación que soporta el nuevo <a href=/docs/concepts/overview/working-with-objects/labels/#set-based-requirement>selector de etiqueta basado en conjunto</a>.
Se usa principalmente por el <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> como un mecanismo para orquestrar la creación de pods, la eliminación y las actualizaciones.
Nótese que se recomienda usar Deployments en vez de directamente usar los ReplicaSets, a menos que necesites una orquestración personalizada de actualizaciones o no quieras actualizaciones en absoluto.</p><h3 id=deployment-recomendado>Deployment (Recomendado)</h3><p>El <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> es un objeto de alto nivel de la API que actualiza sus ReplicaSets subyacenetes y sus Pods
de forma similar a cómo lo hace el comando <code>kubectl rolling-update</code>. Se recomienda el uso de Deployments si se quiere esta functionalidad de actualización en línea,
porque a diferencia del comando <code>kubectl rolling-update</code>, son declarativos, se ejecutan del lado del servidor, y tienen características adicionales.</p><h3 id=pods-simples>Pods simples</h3><p>A diferencia del caso en que un usuario ha creado directamente pods, un ReplicationController sustituye los pods que han sido eliminador o terminados por cualquier motivo,
como en el caso de un fallo de un nodo o una intervención disruptiva de mantenimiento, como la actualización del kernel.
Por esta razón, se recomienda que se usa un ReplicationController incluso si tu aplicación sólo necesita un único pod.
Piensa que es similar a un supervisor de proceso, sólo que supervisa múltiples pods entre múltiples nodos en vez de
procesos individuales en un único nodo. Un ReplicationController delega los reinicios locales de
los contenedores a algún agente del nodo (por ejemplo, Kubelet o Docker).</p><h3 id=job>Job</h3><p>Usa un <a href=/docs/concepts/jobs/run-to-completion-finite-workloads/><code>Job</code></a> en vez de un ReplicationController para aquellos pods que se espera que terminen por sí mismos
(esto es, trabajos por lotes).</p><h3 id=daemonset>DaemonSet</h3><p>Usa un <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> en vez de un ReplicationController para aquellos pods que proporcionan
una función a nivel de servidor, como la monitorización o el loggin de servidor. Estos pods tienen un ciclo de vida que está asociado
al del servidor: el pod necesita ejecutarse en el servidor antes que los otros pods arranquen, y es seguro
terminarlo cuando el servidor está listo para reiniciarse/apagarse.</p><h2 id=para-más-información>Para más información</h2><p>Lee <a href=/docs/tutorials/stateless-application/run-stateless-ap-replication-controller/>Ejecutar Aplicaciones sin Estado con un ReplicationController</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a2dc0393e0c4079e1c504b6429844e86>2.3 - Deployment</h1><p>Un controlador de <em>Deployment</em> proporciona actualizaciones declarativas para los <a href=/docs/concepts/workloads/pods/pod/>Pods</a> y los
<a href=/docs/concepts/workloads/controllers/replicaset/>ReplicaSets</a>.</p><p>Cuando describes el <em>estado deseado</em> en un objeto Deployment, el controlador del Deployment se encarga de cambiar el estado actual al estado deseado de forma controlada.
Puedes definir Deployments para crear nuevos ReplicaSets, o eliminar Deployments existentes y adoptar todos sus recursos con nuevos Deployments.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> No deberías gestionar directamente los ReplicaSets que pertenecen a un Deployment.
Todos los casos de uso deberían cubrirse manipulando el objeto Deployment.
Considera la posibilidad de abrir un incidente en el repositorio principal de Kubernetes si tu caso de uso no está soportado por el motivo que sea.</div><h2 id=casos-de-uso>Casos de uso</h2><p>A continuación se presentan los casos de uso típicos de los Deployments:</p><ul><li><a href=#creating-a-deployment>Crear un Deployment para desplegar un ReplicaSet</a>. El ReplicaSet crea los Pods en segundo plano. Comprueba el estado del despliegue para comprobar si es satisfactorio o no.</li><li><a href=#updating-a-deployment>Declarar el nuevo estado de los Pods</a> actualizando el PodTemplateSpec del Deployment. Ello crea un nuevo ReplicaSet y el Deployment gestiona el cambio de los Pods del viejo ReplicaSet al nuevo de forma controlada. Cada nuevo ReplicaSet actualiza la revisión del Deployment.</li><li><a href=#rolling-back-a-deployment>Retroceder a una revisión anterior del Deployment</a> si el estado actual de un Deployment no es estable. Cada retroceso actualiza la revisión del Deployment.</li><li><a href=#scaling-a-deployment>Escalar horizontalmente el Deployment para soportar más carga</a>.</li><li><a href=#pausing-and-resuming-a-deployment>Pausar el Deployment</a> para aplicar múltiples arreglos a su PodTemplateSpec y, a continuación, reanúdalo para que comience un nuevo despliegue.</li><li><a href=#deployment-status>Usar el estado del Deployment</a> como un indicador de que el despliegue se ha atascado.</li><li><a href=#clean-up-policy>Limpiar los viejos ReplicaSets</a> que no necesites más.</li></ul><h2 id=crear-un-deployment>Crear un Deployment</h2><p>El siguiente ejemplo de un Deployment crea un ReplicaSet para arrancar tres Pods con <code>nginx</code>:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/nginx-deployment.yaml download=controllers/nginx-deployment.yaml><code>controllers/nginx-deployment.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-nginx-deployment-yaml")' title="Copy controllers/nginx-deployment.yaml to clipboard"></img></div><div class=includecode id=controllers-nginx-deployment-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.7.9<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>En este ejemplo:</p><ul><li><p>Se crea un Deployment denominado <code>nginx-deployment</code>, indicado a través del campo <code>.metadata.name</code>.</p></li><li><p>El Deployment crea tres Pods replicados, indicado a través del campo <code>replicas</code>.</p></li><li><p>El campo <code>selector</code> define cómo el Deployment identifica los Pods que debe gestionar.
En este caso, simplemente seleccionas una etiqueta que se define en la plantilla Pod (<code>app: nginx</code>).
Sin embargo, es posible definir reglas de selección más sofisticadas,
siempre que la plantilla Pod misma satisfaga la regla.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> <code>matchLabels</code> es un mapa de entradas {clave,valor}. Una entrada simple {clave,valor} en el mapa <code>matchLabels</code>
es equivalente a un elemento de <code>matchExpressions</code> cuyo campo sea la "clave", el operador sea "In",
y la matriz de valores contenga únicamente un "valor". Todos los requisitos se concatenan con AND.</div></li><li><p>El campo <code>template</code> contiene los siguientes sub-campos:</p><ul><li>Los Pods se etiquetan como <code>app: nginx</code> usando el campo <code>labels</code>.</li><li>La especificación de la plantilla Pod, o el campo <code>.template.spec</code>, indica
que los Pods ejecutan un contenedor, <code>nginx</code>, que utiliza la versión 1.7.9 de la imagen de <code>nginx</code> de
<a href=https://hub.docker.com/>Docker Hub</a>.</li><li>Crea un contenedor y lo llamar <code>nginx</code> usando el campo <code>name</code>.</li><li>Ejecuta la imagen <code>nginx</code> en su versión <code>1.7.9</code>.</li><li>Abre el puerto <code>80</code> para que el contenedor pueda enviar y recibir tráfico.</li></ul></li></ul><p>Para crear este Deployment, ejecuta el siguiente comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Debes indicar el parámetro <code>--record</code> para registrar el comando ejecutado en la anotación de recurso <code>kubernetes.io/change-cause</code>.
Esto es útil para futuras introspecciones, por ejemplo para comprobar qué comando se ha ejecutado en cada revisión del Deployment.</div><p>A continuación, ejecuta el comando <code>kubectl get deployments</code>. La salida debe ser parecida a la siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
</span></span><span style=display:flex><span>nginx-deployment   3/3     <span style=color:#666>3</span>            <span style=color:#666>3</span>           1s  
</span></span></code></pre></div><p>Cuando inspeccionas los Deployments de tu clúster, se muestran los siguientes campos:</p><ul><li><code>NAME</code> enumera los nombre de los Deployments del clúster.</li><li><code>READY</code> muestra cuántas réplicas de la aplicación están disponibles para sus usuarios. Sigue el patrón número de réplicas <code>listas/deseadas</code>.</li><li><code>UP-TO-DATE</code> muestra el número de réplicas que se ha actualizado para alcanzar el estado deseado.</li><li><code>AVAILABLE</code> muestra cuántas réplicas de la aplicación están disponibles para los usuarios.</li><li><code>AGE</code> muestra la cantidad de tiempo que la aplicación lleva ejecutándose.</li></ul><p>Nótese cómo los valores de cada campo corresponden a los valores de la especificación del Deployment:</p><ul><li>El número de réplicas deseadas es 3 de acuerdo con el campo <code>.spec.replicas</code>.</li><li>El número de réplicas actuales es 0 de acuerdo con el campo <code>.status.replicas</code>.</li><li>El número de réplicas actualizadas es 0 de acuerdo con el campo <code>.status.updatedReplicas</code>.</li><li>El número de réplicas disponibles es 0 de acuerdo con el campo <code>.status.availableReplicas</code>.</li></ul><p>Si deseamos obtener más información del Deployment utilice el parámetro '-o wide', ejecutando el comando 'kubectl get deployments -o wide'. La salida será parecida a la siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         SELECTOR
</span></span><span style=display:flex><span>nginx-deployment   3/3     <span style=color:#666>3</span>            <span style=color:#666>3</span>           10s   nginx        nginx:1.7.9   <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</span></span></code></pre></div><p>Ejecutando el comando anterior se muestran los siguientes campos adicionales:</p><ul><li><code>CONTAINERS</code> muestra los nombres de los contenedores declarados en <code>.spec.template.spec.containers.[name]</code>.</li><li><code>IMAGES</code> muestra los nombres de las imágenes declaradas en <code>.spec.template.spec.containers.[image]</code>.</li><li>'SELECTOR' muestra el Label selector que se declaró en matchLabels o matchExpressions.</li></ul><p>Para ver el estado del Deployment, ejecuta el comando <code>kubectl rollout status deployment.v1.apps/nginx-deployment</code>. Este comando devuelve el siguiente resultado:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Waiting <span style=color:#a2f;font-weight:700>for</span> rollout to finish: <span style=color:#666>2</span> out of <span style=color:#666>3</span> new replicas have been updated...
</span></span><span style=display:flex><span>deployment <span style=color:#b44>&#34;nginx-deployment&#34;</span> successfully rolled out
</span></span></code></pre></div><p>Ejecuta de nuevo el comando <code>kubectl get deployments</code> unos segundos más tarde:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
</span></span><span style=display:flex><span>nginx-deployment   3/3     <span style=color:#666>3</span>            <span style=color:#666>3</span>           18s  
</span></span></code></pre></div><p>Fíjate que el Deployment ha creado todas las tres réplicas, y que todas las réplicas están actualizadas (contienen
la última plantilla Pod) y están disponibles (el estado del Pod tiene el valor Ready al menos para el campo <code>.spec.minReadySeconds</code> del Deployment).</p><p>Para ver el ReplicaSet (<code>rs</code>) creado por el Deployment, ejecuta el comando <code>kubectl get rs</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                          DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>nginx-deployment-75675f5897   <span style=color:#666>3</span>         <span style=color:#666>3</span>         <span style=color:#666>3</span>       18s
</span></span></code></pre></div><p>Fíjate que el nombre del ReplicaSet siempre se formatea con el patrón <code>[DEPLOYMENT-NAME]-[RANDOM-STRING]</code>. La cadena aleatoria se
genera de forma aleatoria y usa el pod-template-hash como semilla.</p><p>Para ver las etiquetas generadas automáticamente en cada pod, ejecuta el comando <code>kubectl get pods --show-labels</code>. Se devuelve la siguiente salida:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE       LABELS
</span></span><span style=display:flex><span>nginx-deployment-75675f5897-7ci7o   1/1       Running   <span style=color:#666>0</span>          18s       <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx,pod-template-hash<span style=color:#666>=</span><span style=color:#666>3123191453</span>
</span></span><span style=display:flex><span>nginx-deployment-75675f5897-kzszj   1/1       Running   <span style=color:#666>0</span>          18s       <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx,pod-template-hash<span style=color:#666>=</span><span style=color:#666>3123191453</span>
</span></span><span style=display:flex><span>nginx-deployment-75675f5897-qqcnn   1/1       Running   <span style=color:#666>0</span>          18s       <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx,pod-template-hash<span style=color:#666>=</span><span style=color:#666>3123191453</span>
</span></span></code></pre></div><p>El ReplicaSet creado garantiza que hay tres Pods de <code>nginx</code> ejecutándose en todo momento.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> En un Deployment, debes especificar un selector apropiado y etiquetas de plantilla Pod (en este caso,
<code>app: nginx</code>). No entremezcles etiquetas o selectores con otros controladores (incluyendo otros Deployments y StatefulSets).
Kubernetes no te impide que lo hagas, pero en el caso de que múltiples controladores tengan selectores mezclados, dichos controladores pueden entrar en conflicto y provocar resultados inesperados.</div><h3 id=etiqueta-pod-template-hash>Etiqueta pod-template-hash</h3><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> No cambies esta etiqueta.</div><p>La etiqueta <code>pod-template-hash</code> es añadida por el controlador del Deployment a cada ReplicaSet que el Deployment crea o adopta.</p><p>Esta etiqueta garantiza que todos los hijos ReplicaSets de un Deployment no se entremezclan. Se genera mediante una función hash aplicada al <code>PodTemplate</code> del ReplicaSet
y usando el resultado de la función hash como el valor de la etiqueta que se añade al selector del ReplicaSet, en las etiquetas de la plantilla Pod,
y en cualquier Pod existente que el ReplicaSet tenga.</p><h2 id=actualizar-un-deployment>Actualizar un Deployment</h2><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El lanzamiento de un Deployment se activa si y sólo si la plantilla Pod del Deployment (esto es, <code>.spec.template</code>)
se cambia, por ejemplo si se actualiza las etiquetas o las imágenes de contenedor de la plantilla.
Otras actualizaciones, como el escalado del Deployment, no conllevan un lanzamiento de despliegue.</div><p>Asumiendo que ahora quieres actualizar los Pods nginx para que usen la imagen <code>nginx:1.9.1</code>
en vez de la imagen <code>nginx:1.7.9</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl --record deployment.apps/nginx-deployment <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.9.1
</span></span></code></pre></div><pre tabindex=0><code>image updated
</code></pre><p>De forma alternativa, puedes <code>editar</code> el Deployment y cambiar el valor del campo <code>.spec.template.spec.containers[0].image</code> de <code>nginx:1.7.9</code> a <code>nginx:1.9.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment edited
</code></pre><p>Para ver el estado del despliegue, ejecuta:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment &#34;nginx-deployment&#34; successfully rolled out
</code></pre><p>Cuando el despliegue funciona, puede que quieras <code>obtener</code> el Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployments
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   3/3     3            3           36s 
</code></pre><p>El número de réplicas actualizadas indica que el Deployment ha actualizado las réplicas según la última configuración.
Las réplicas actuales indican el total de réplicas que gestiona este Deployment, y las réplicas disponibles indican
el número de réplicas actuales que están disponibles.</p><p>Puedes ejecutar el comando <code>kubectl get rs</code> para ver que el Deployment actualizó los Pods creando un nuevo ReplicaSet y escalándolo
hasta las 3 réplicas, así como escalando el viejo ReplicaSet a 0 réplicas.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
</code></pre><p>Si ejecutas el comando <code>get pods</code> deberías ver los nuevos Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><pre tabindex=0><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</code></pre><p>La próxima vez que quieras actualizar estos Pods, sólo necesitas actualizar la plantilla Pod del Deployment otra vez.</p><p>El Deployment permite garantizar que sólo un número determinado de Pods puede eliminarse mientras se están actualizando.
Por defecto, garantiza que al menos el 25% menos del número deseado de Pods se está ejecutando (máx. 25% no disponible).</p><p>El Deployment también permite garantizar que sólo un número determinado de Pods puede crearse por encima del número deseado de
Pods. Por defecto, garantiza que al menos el 25% más del número deseado de Pods se está ejecutando (máx. 25% de aumento).</p><p>Por ejemplo, si miras detenidamente el Deployment de arriba, verás que primero creó un Pod,
luego eliminó algunos viejos Pods y creó otros nuevos. No elimina los viejos Pods hasta que un número suficiente de
nuevos Pods han arrancado, y no crea nuevos Pods hasta que un número suficiente de viejos Pods se han eliminado.
De esta forma, asegura que el número de Pods disponibles siempre es al menos 2, y el número de Pods totales es cómo máximo 4.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployments
</span></span></code></pre></div><pre tabindex=0><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
  Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
</code></pre><p>Aquí puedes ver que cuando creaste por primera vez el Deployment, este creó un ReplicaSet (nginx-deployment-2035384211)
y lo escaló a 3 réplicas directamente. Cuando actualizaste el Deployment, creó un nuevo ReplicaSet
(nginx-deployment-1564180365) y lo escaló a 1 y entonces escaló el viejo ReplicaSet a 2, de forma que al menos
hubiera 2 Pods disponibles y como mucho 4 Pods en total en todo momento. Entonces, continuó escalando
el nuevo y el viejo ReplicaSet con la misma estrategia de actualización continua. Finalmente, el nuevo ReplicaSet acaba con 3 réplicas
disponibles, y el viejo ReplicaSet se escala a 0.</p><h3 id=sobrescritura-o-sea-múltiples-actualizaciones-a-la-vez>Sobrescritura (o sea, múltiples actualizaciones a la vez)</h3><p>Cada vez que el controlador del Deployment observa un nuevo objeto de despliegue, se crea un ReplicaSet para arrancar
los Pods deseados si es que no existe otro ReplicaSet haciéndolo. Los ReplicaSet existentes que controlan los Pods cuyas etiquetas
coinciden con el valor del campo <code>.spec.selector</code>, pero cuya plantilla no coincide con el valor del campo <code>.spec.template</code> se reducen. Al final,
el nuevo ReplicaSet se escala hasta el valor del campo <code>.spec.replicas</code> y todos los viejos ReplicaSets se escalan a 0.</p><p>Si actualizas un Deployment mientras otro despliegue está en curso, el Deployment creará un nuevo ReplicaSet
como consecuencia de la actualización y comenzará a escalarlo, y sobrescribirá al ReplicaSet que estaba escalando anteriormente
-- lo añadirá a su lista de viejos ReplicaSets y comenzará a reducirlos.</p><p>Por ejemplo, supongamos que creamos un Deployment para crear 5 réplicas de <code>nginx:1.7.9</code>,
pero entonces actualizamos el Deployment para crear 5 réplicas de <code>nginx:1.9.1</code> cuando sólo se ha creado 3
réplicas de <code>nginx:1.7.9</code>. En este caso, el Deployment comenzará automáticamente a matar los 3 Pods de <code>nginx:1.7.9</code>
que había creado, y empezará a crear los Pods de <code>nginx:1.9.1</code>. Es decir, no esperará a que se creen las 5 réplicas de <code>nginx:1.7.9</code>
antes de aplicar la nueva configuración.</p><h3 id=actualizaciones-del-selector-de-etiquetas>Actualizaciones del selector de etiquetas</h3><p>No se recomienda hacer cambios al selector del etiquetas y, por ello, se aconseja encarecidamente planificar el valor de dichos selectores por adelantado.
En cualquier caso, si necesitas cambiar un selector de etiquetas, hazlo con mucho cuidado y asegúrate que entiendes todas sus implicaciones.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> En la versión <code>apps/v1</code> de la API, el selector de etiquetas del Deployment es inmutable una vez se ha creado.</div><ul><li>Las adiciones posteriores al selector obligan también a actualizar las etiquetas de la plantilla Pod en la especificación del Deployment con los nuevos valores,
ya que de lo contrario se devolvería un error. Este cambio no es de superposición, es decir, que el nuevo selector
no selecciona los ReplicaSets y Pods creados con el viejo selector, lo que provoca que todos los viejos ReplicaSets se marquen como huérfanos y
la creación de un nuevo ReplicaSet.</li><li>Las actualizaciones de selector -- esto es, cambiar el valor actual en una clave de selector -- provocan el mismo comportamiento que las adiciones.</li><li>Las eliminaciones de selector -- esto es, eliminar una clave actual del selector del Deployment -- no necesitan de cambios en las etiquetas de la plantilla Pod.
No se marca ningún ReplicaSet existente como huérfano, y no se crea ningún ReplicaSet nuevo, pero debe tenerse en cuenta que
la etiqueta eliminada todavía existe en los Pods y ReplicaSets que se están ejecutando.</li></ul><h2 id=revertir-un-deployment>Revertir un Deployment</h2><p>En ocasiones necesitas revertir un Deployment; por ejemplo, cuando el Deployment no es estable, como cuando no para de reiniciarse.
Por defecto, toda la historia de despliegue del Deployment se mantiene en el sistema de forma que puedes revertir en cualquier momento
(se puede modificar este comportamiento cambiando el límite de la historia de revisiones de modificaciones).</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Cuando se lanza el despligue de un Deployment, se crea una nueva revisión. Esto quiere decir que
la nueva revisión se crea si y sólo si la plantilla Pod del Deployment (<code>.spec.template</code>) se cambia;
por ejemplo, si cambias las etiquetas o la imagen del contenedor de la plantilla.
Otras actualizaciones, como escalar el Deployment,
no generan una nueva revisión del Deployment, para poder facilitar el escalado manual simultáneo - o auto-escalado.
Esto significa que cuando reviertes a una versión anterior, sólo la parte de la plantilla Pod del Deployment se revierte.</div><p>Vamos a suponer que hemos cometido un error al actualizar el Deployment, poniendo como nombre de imagen <code>nginx:1.91</code> en vez de <code>nginx:1.9.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.91 --record<span style=color:#666>=</span><span style=color:#a2f>true</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre><p>El despliegue se atasca y no progresa.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
</code></pre><p>Presiona Ctrl-C para detener la monitorización del despliegue de arriba. Para obtener más información sobre despliegues atascados,
<a href=#deployment-status>lee más aquí</a>.</p><p>Verás que el número de réplicas viejas (nginx-deployment-1564180365 y nginx-deployment-2035384211) es 2, y el número de nuevas réplicas (nginx-deployment-3066724191) es 1.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s
</code></pre><p>Echando un vistazo a los Pods creados, verás que uno de los Pods creados por el nuevo ReplicaSet está atascado en un bucle intentando bajar la imagen:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><pre tabindex=0><code>NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
</code></pre><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El controlador del Deployment parará el despliegue erróneo de forma automática, y detendrá el escalado del nuevo
ReplicaSet. Esto depende de los parámetros del rollingUpdate (<code>maxUnavailable</code> específicamente) que hayas configurado.
Kubernetes por defecto establece el valor en el 25%.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment
</span></span></code></pre></div><pre tabindex=0><code>Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.91
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubobjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
</code></pre><p>Para arreglar este problema, necesitas volver a una revisión previa del Deployment que sea estable.</p><h3 id=comprobar-la-historia-de-despliegues-de-un-deployment>Comprobar la Historia de Despliegues de un Deployment</h3><p>Primero, comprobemos las revisiones de este despliegue:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployments &#34;nginx-deployment&#34;
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true
</code></pre><p>En el momento de la creación, el mensaje en <code>CHANGE-CAUSE</code> se copia de la anotación <code>kubernetes.io/change-cause</code> del Deployment a sus revisiones. Podrías indicar el mensaje <code>CHANGE-CAUSE</code>:</p><ul><li>Anotando el Deployment con el comando <code>kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause="image updated to 1.9.1"</code></li><li>Añadiendo el parámetro <code>--record</code> para registrar el comando <code>kubectl</code> que está haciendo cambios en el recurso.</li><li>Manualmente editando el manifiesto del recursos.</li></ul><p>Para ver más detalles de cada revisión, ejecuta:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment.v1.apps/nginx-deployment --revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><pre tabindex=0><code>deployments &#34;nginx-deployment&#34; revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
  Containers:
   nginx:
    Image:      nginx:1.9.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &lt;none&gt;
  No volumes.
</code></pre><h3 id=retroceder-a-una-revisión-previa>Retroceder a una Revisión Previa</h3><p>Ahora has decidido que quieres deshacer el despliegue actual y retrocederlo a la revisión previa:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment
</code></pre><p>Alternativamente, puedes retroceder a una revisión específica con el parámetro <code>--to-revision</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment
</code></pre><p>Para más detalles acerca de los comandos relacionados con las revisiones de un Deployment, echa un vistazo a <a href=/docs/reference/generated/kubectl/kubectl-commands#rollout><code>kubectl rollout</code></a>.</p><p>El Deployment se ha revertido ahora a una revisión previa estable. Como se puede comprobar, el controlador del Deployment genera un evento <code>DeploymentRollback</code>
al retroceder a la revisión 2.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   3/3     3            3           30m 
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.9.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &#34;nginx-deployment&#34; to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
</code></pre><h2 id=escalar-un-deployment>Escalar un Deployment</h2><p>Puedes escalar un Deployment usando el siguiente comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment.v1.apps/nginx-deployment --replicas<span style=color:#666>=</span><span style=color:#666>10</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment scaled
</code></pre><p>Asumiendo que se ha habilitado el <a href=/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/>escalado horizontal de pod</a>
en tu clúster, puedes configurar un auto-escalado para tu Deployment y elegir el mínimo y máximo número de Pods
que quieres ejecutar en base al uso de CPU de tus Pods actuales.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale deployment.v1.apps/nginx-deployment --min<span style=color:#666>=</span><span style=color:#666>10</span> --max<span style=color:#666>=</span><span style=color:#666>15</span> --cpu-percent<span style=color:#666>=</span><span style=color:#666>80</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment scaled
</code></pre><h3 id=escalado-proporcional>Escalado proporcional</h3><p>La actualización continua de los Deployments permite la ejecución de múltiples versiones de una aplicación al mismo tiempo.
Cuando tú o un auto-escalado escala un Deployment con actualización continua que está en medio de otro despliegue (bien en curso o pausado),
entonces el controlador del Deployment balanceará las réplicas adicionales de los ReplicaSets activos (ReplicaSets con Pods)
para así poder mitigar el riesgo. Esto se conoce como <em>escalado proporcional</em>.</p><p>Por ejemplo, imagina que estás ejecutando un Deployment con 10 réplicas, donde <a href=#max-surge>maxSurge</a>=3, y <a href=#max-unavailable>maxUnavailable</a>=2.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   10/10   10           10          50s 
</code></pre><p>Si actualizas a una nueva imagen que no puede descargarse desde el clúster:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:sometag
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre><p>La actualización de la imagen arranca un nuevo despliegue con el ReplicaSet nginx-deployment-1989198191,
pero se bloquea debido al requisito <code>maxUnavailable</code> indicado arriba:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
</code></pre><p>Y entonces se origina una nueva petición de escalado para el Deployment. El auto-escalado incrementa las réplicas del Deployment
a 15. El controlador del Deployment necesita ahora decidir dónde añadir esas nuevas 5 réplicas.
Si no estuvieras usando el escalado proporcional, las 5 se añadirían al nuevo ReplicaSet. Pero con el escalado proporcional,
las réplicas adicionales se distribuyen entre todos los ReplicaSets. Las partes más grandes van a los ReplicaSets
con el mayor número de réplicas y las partes más pequeñas van a los ReplicaSets con menos réplicas. Cualquier resto sobrante se añade
al ReplicaSet con mayor número de réplicas. Aquellos ReplicaSets con 0 réplicas no se escalan.</p><p>En nuestro ejemplo anterior, se añadirán 3 réplicas al viejo ReplicaSet y 2 réplicas al nuevo ReplicaSet.
EL proceso de despliegue debería al final mover todas las réplicas al nuevo ReplicaSet, siempre que las nuevas
réplicas arranquen positivamente.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   18/15   7            8           7m 
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
</code></pre><h2 id=pausar-y-reanudar-un-deployment>Pausar y Reanudar un Deployment</h2><p>Puedes pausar un Deployment antes de arrancar una o más modificaciones y luego reanudarlo. Esto te permite aplicar múltiples arreglos
entre la pausa y la reanudación sin necesidad de arrancar despliegues innecesarios.</p><p>Por ejemplo, con un Deployment que acaba de crearse:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE 
nginx-deployment   3/3     3            3           1m 
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
</code></pre><p>Lo pausamos ejecutando el siguiente comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout pause deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment paused
</code></pre><p>Y luego actualizamos la imagen del Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.9.1
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre><p>Nótese que no se arranca ningún despliegue nuevo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployments &#34;nginx&#34;
REVISION  CHANGE-CAUSE
1   &lt;none&gt;
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m
</code></pre><p>Puedes realizar tantas modificaciones como quieras, por ejemplo, para actualizar los recursos a utilizar:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> resources deployment.v1.apps/nginx-deployment -c<span style=color:#666>=</span>nginx --limits<span style=color:#666>=</span><span style=color:#b8860b>cpu</span><span style=color:#666>=</span>200m,memory<span style=color:#666>=</span>512Mi
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment resource requirements updated
</code></pre><p>El estado inicial del Deployment anterior a la pausa continuará su función, pero las nuevas modificaciones
del Deployment no tendrán efecto ya que el Deployment está pausado.</p><p>Al final, reanuda el Deployment y observa cómo se genera un nuevo ReplicaSet con todos los cambios:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout resume deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment resumed
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs -w
</span></span></code></pre></div><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
</code></pre><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> No se puede revertir un Deployment pausado hasta que se vuelve a reanudar.</div><h2 id=estado-del-deployment>Estado del Deployment</h2><p>Un Deployment pasa por varios estados a lo largo de su ciclo de vida. Así, puede estar <a href=#progressing-deployment>progresando</a> mientras
se despliega un nuevo ReplicaSet, puede estar <a href=#complete-deployment>completo</a>, o puede quedar en estado <a href=#failed-deployment>fallido</a>.</p><h3 id=progresar-un-deployment>Progresar un Deployment</h3><p>Kubernetes marca un Deployment como <em>progresando</em> cuando se realiza cualquiera de las siguientes tareas:</p><ul><li>El Deployment crea un nuevo ReplicaSet.</li><li>El Deployment está escalando su ReplicaSet más nuevo.</li><li>El Deployment está reduciendo su(s) ReplicaSet(s) más antiguo(s).</li><li>Hay nuevos Pods disponibles y listos (listo por lo menos <a href=#min-ready-seconds>MinReadySeconds</a>).</li></ul><p>Puedes monitorizar el progreso de un Deployment usando el comando <code>kubectl rollout status</code>.</p><h3 id=completar-un-deployment>Completar un Deployment</h3><p>Kubernetes marca un Deployment como <em>completado</em> cuando presenta las siguientes características:</p><ul><li>Todas las réplicas asociadas con el Deployment han sido actualizadas a la última versión indicada, lo cual quiere decir
que todas las actualizaciones se han completado.</li><li>Todas las réplicas asociadas con el Deployment están disponibles.</li><li>No están ejecutándose viejas réplicas del Deployment.</li></ul><p>Puedes comprobar si un Deployment se ha completado usando el comando <code>kubectl rollout status</code>. Si el despliegue se ha completado
de forma satisfactoria, el comando <code>kubectl rollout status</code> devuelve un código 0 de salida.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment &#34;nginx-deployment&#34; successfully rolled out
$ echo $?
0
</code></pre><h3 id=deployment-fallido>Deployment fallido</h3><p>Tu Deployment puede quedarse bloqueado intentando desplegar su nuevo ReplicaSet sin nunca completarse. Esto puede ocurrir
debido a algunos de los factores siguientes:</p><ul><li>Cuota insuficiente</li><li>Fallos en la prueba de estar listo</li><li>Errores en la descarga de imágenes</li><li>Permisos insuficientes</li><li>Rangos de límites de recursos</li><li>Mala configuración del motor de ejecución de la aplicación</li></ul><p>Una forma de detectar este tipo de situación es especificar un parámetro de vencimiento en la especificación de tu Deployment:
(<a href=#progress-deadline-seconds><code>.spec.progressDeadlineSeconds</code></a>). <code>.spec.progressDeadlineSeconds</code> denota el número
de segundos que el controlador del Deployment debe esperar antes de indicar (en el estado del Deployment) que el
Deployment no avanza.</p><p>El siguiente comando <code>kubectl</code> configura el campo <code>progressDeadlineSeconds</code> para forzar al controlador a
informar de la falta de avance de un Deployment después de 10 minutos:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl patch deployment.v1.apps/nginx-deployment -p <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;progressDeadlineSeconds&#34;:600}}&#39;</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/nginx-deployment patched
</code></pre><p>Una vez que se ha excedido el vencimiento, el controlador del Deployment añade una DeploymentCondition
con los siguientes atributos al campo <code>.status.conditions</code> del Deployment:</p><ul><li>Type=Progressing</li><li>Status=False</li><li>Reason=ProgressDeadlineExceeded</li></ul><p>Ver las <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties>convenciones de la API de Kubernetes</a> para más información acerca de las condiciones de estado.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Kubernetes no emprenderá ninguna acción ante un Deployment parado que no sea la de reportar el estado mediante
<code>Reason=ProgressDeadlineExceeded</code>. Los orquestradores de alto nivel pueden aprovecharse y actuar consecuentemente, por ejemplo,
retrocediendo el Deployment a su versión previa.</div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Si pausas un Deployment, Kubernetes no comprueba el avance en base al vencimiento indicado. Así, es posible pausar
de forma segura un Deployment en medio de un despliegue y reanudarlo sin que se arranque el estado de exceso de vencimiento.</div><p>Puede que notes errores transitorios en tus Deployments, bien debido a un tiempo de vencimiento muy pequeño que hayas configurado
o bien a cualquier otro tipo de error que puede considerarse como transitorio. Por ejemplo,
supongamos que no tienes suficiente cuota. Si describes el Deployment, te darás cuenta de la sección siguiente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
</code></pre><p>Si ejecutas el comando <code>kubectl get deployment nginx-deployment -o yaml</code>, el estado del Deployment puede parecerse a:</p><pre tabindex=0><code>status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &#34;nginx-deployment-4262182780&#34; is progressing.
    reason: ReplicaSetUpdated
    status: &#34;True&#34;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &#34;True&#34;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: &#39;Error creating: pods &#34;nginx-deployment-4262182780-&#34; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2&#39;
    reason: FailedCreate
    status: &#34;True&#34;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
</code></pre><p>Al final, una vez que se supera el vencimiento del progreso del Deployment, Kubernetes actualiza el estado
y la razón de el estado de progreso:</p><pre tabindex=0><code>Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
</code></pre><p>Puedes solucionar un problema de cuota insuficiente simplemente reduciendo el número de réplicas de tu Deployment, reduciendo
otros controladores que puedas estar ejecutando, o incrementando la cuota en tu espacio de nombres. Si una vez satisfechas las condiciones de tu cuota,
el controlador del Deployment completa el despliegue, entonces verás que el estado del Deployment se actualiza al estado satisfactorio (<code>Status=True</code> y <code>Reason=NewReplicaSetAvailable</code>).</p><pre tabindex=0><code>Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
</code></pre><p><code>Type=Available</code> con <code>Status=True</code> significa que tu Deployment tiene disponibilidad mínima. La disponibilidad mínima se prescribe
mediante los parámetros indicados en la estrategia de despligue. <code>Type=Progressing</code> con <code>Status=True</code> significa que tu Deployment
está bien en medio de un despliegue y está progresando o bien que se ha completado de forma satisfactoria y el número mínimo
requerido de nuevas réplicas ya está disponible (ver la Razón del estado para cada caso particular - en nuestro caso
<code>Reason=NewReplicaSetAvailable</code> significa que el Deployment se ha completado).</p><p>Puedes comprobar si un Deployment ha fallado en su progreso usando el comando <code>kubectl rollout status</code>. <code>kubectl rollout status</code>
devuelve un código de salida distinto de 0 si el Deployment ha excedido su tiempo de vencimiento.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &#34;nginx&#34; exceeded its progress deadline
$ echo $?
1
</code></pre><h3 id=actuar-ante-un-despliegue-fallido>Actuar ante un despliegue fallido</h3><p>Todas las acciones que aplican a un Deployment completado también aplican a un Deployment fallido. Puedes escalarlo/reducirlo, retrocederlo
a una revisión previa, o incluso pausarlo si necesitas realizar múltiples cambios a la plantilla Pod del Deployment.</p><h2 id=regla-de-limpieza>Regla de Limpieza</h2><p>Puedes configurar el campo <code>.spec.revisionHistoryLimit</code> de un Deployment para especificar cuántos ReplicaSets viejos quieres conservar
para este Deployment. El resto será eliminado en segundo plano. Por defecto, es 10.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Poner este campo de forma explícita a 0 resulta en la limpieza de toda la historia de tu Deployment,
por lo que tu Deployment no podrá retroceder a revisiones previas.</div><h2 id=casos-de-uso-1>Casos de Uso</h2><h3 id=despligue-canary>Despligue Canary</h3><p>Si quieres desplegar nuevas versiones a un sub-conjunto de usuarios o servidores usando el Deployment,
puedes hacerlo creando múltiples Deployments, uno para cada versión nueva, siguiendo el patrón canary descrito en
<a href=/docs/concepts/cluster-administration/manage-deployment/#canary-deployments>gestionar recursos</a>.</p><h2 id=escribir-una-especificación-de-deployment>Escribir una especificación de Deployment</h2><p>Al igual que con el resto de configuraciones de Kubernetes, un Deployment requiere los campos <code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>.
Para información general acerca de cómo trabajar con ficheros de configuración, ver los documentos acerca de <a href=/docs/tutorials/stateless-application/run-stateless-application-deployment/>desplegar aplicaciones</a>,
configurar contenedores, y <a href=/docs/concepts/overview/object-management-kubectl/overview/>usar kubectl para gestionar recursos</a>.</p><p>Un Deployment también necesita una <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>sección <code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>Tanto <code>.spec.template</code> como <code>.spec.selector</code> sin campos obligatorios dentro de <code>.spec</code>.</p><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/pods/pod-overview/#pod-templates>plantilla Pod</a>. Tiene exactamente el mismo esquema que un <a href=/docs/concepts/workloads/pods/pod/>Pod</a>,
excepto por el hecho de que está anidado y no tiene <code>apiVersion</code> ni <code>kind</code>.</p><p>Junto con los campos obligatorios de un Pod, una plantilla Pod de un Deployment debe indicar las etiquetas
y las reglas de reinicio apropiadas. Para el caso de las etiquetas, asegúrate que no se entremezclan con otros controladores. Ver <a href=#selector>selector</a>).</p><p>Únicamente se permite una <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>.spec.template.spec.restartPolicy</code></a> igual a <code>Always</code>,
que es el valor por defecto si no se indica.</p><h3 id=réplicas>Réplicas</h3><p><code>.spec.replicas</code> es un campo opcional que indica el número de Pods deseados. Su valor por defecto es 1.</p><h3 id=selector>Selector</h3><p><code>.spec.selector</code> es un campo opcional que indica un <a href=/docs/concepts/overview/working-with-objects/labels/>selector de etiquetas</a>
para los Pods objetivo del deployment.</p><p><code>.spec.selector</code> debe coincidir con <code>.spec.template.metadata.labels</code>, o será descartado por la API.</p><p>A partir de la versión <code>apps/v1</code> de la API, <code>.spec.selector</code> y <code>.metadata.labels</code> no toman como valor por defecto el valor de <code>.spec.template.metadata.labels</code> si no se indica.
Por ello, debe especificarse de forma explícita. Además hay que mencionar que <code>.spec.selector</code> es inmutable tras la creación del Deployment en <code>apps/v1</code>.</p><p>Un Deployment puede finalizar aquellos Pods cuyas etiquetas coincidan con el selector si su plantilla es diferente
de <code>.spec.template</code> o si el número total de dichos Pods excede <code>.spec.replicas</code>. Arranca nuevos
Pods con <code>.spec.template</code> si el número de Pods es menor que el número deseado.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> No deberías crear otros Pods cuyas etiquetas coincidan con este selector, ni directamente creando
otro Deployment, ni creando otro controlador como un ReplicaSet o un ReplicationController. Si lo haces,
el primer Deployment pensará que también creó esos otros Pods. Kubernetes no te impide hacerlo.</div><p>Si tienes múltiples controladores que entremezclan sus selectores, dichos controladores competirán entre ellos
y no se comportarán de forma correcta.</p><h3 id=estrategia>Estrategia</h3><p><code>.spec.strategy</code> especifica la estrategia usada para remplazar los Pods viejos con los nuevos.
<code>.spec.strategy.type</code> puede tener el valor "Recreate" o "RollingUpdate". "RollingUpdate" el valor predeterminado.</p><h4 id=despliegue-mediante-recreación>Despliegue mediante recreación</h4><p>Todos los Pods actuales se eliminan antes de que los nuevos se creen cuando <code>.spec.strategy.type==Recreate</code>.</p><h4 id=despliegue-mediante-actualización-continua>Despliegue mediante actualización continua</h4><p>El Deployment actualiza los Pods en modo de <a href=/docs/tasks/run-application/rolling-update-replication-controller/>actualización continua</a>
cuando <code>.spec.strategy.type==RollingUpdate</code>. Puedes configurar los valores de <code>maxUnavailable</code> y <code>maxSurge</code>
para controlar el proceso de actualización continua.</p><h5 id=número-máximo-de-pods-no-disponibles>Número máximo de pods no disponibles</h5><p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> es un campo opcional que indica el número máximo
de Pods que pueden no estar disponibles durante el proceso de actualización. El valor puede ser un número absoluto (por ejemplo, 5)
o un porcentaje de los Pods deseados (por ejemplo, 10%). El número absoluto se calcula a partir del porcentaje
con redondeo a la baja. El valor no puede ser 0 si <code>.spec.strategy.rollingUpdate.maxSurge</code> es 0. El valor predeterminado es 25%.</p><p>Por ejemplo, cuando este valor es 30%, el ReplicaSet viejo puede escalarse al 70% de los
Pods deseados de forma inmediata tras comenzar el proceso de actualización. Una vez que los Pods están listos,
el ReplicaSet viejo puede reducirse aún mas, seguido de un escalado del nuevo ReplicaSet,
asegurándose que el número total de Pods disponibles en todo momento durante la actualización
es de al menos el 70% de los Pods deseados.</p><h5 id=número-máximo-de-pods-por-encima-del-número-deseado>Número máximo de pods por encima del número deseado</h5><p><code>.spec.strategy.rollingUpdate.maxSurge</code> es un campo opcional que indica el número máximo de Pods
que puede crearse por encima del número deseado de Pods. El valor puede ser un número absoluto (por ejemplo, 5)
o un porcentaje de los Pods deseados (por ejemplo, 10%). El valor no puede ser 0 si <code>MaxUnavailable</code> es 0.
El número absoluto se calcula a partir del porcentaje con redondeo al alza. El valor predeterminado es 25%.</p><p>Por ejemplo, cuando este valor es 30%, el nuevo ReplicaSet puede escalarse inmediatamente cuando
comienza la actualización continua, de forma que el número total de Pods viejos y nuevos no
excede el 130% de los Pods deseados. Una vez que los viejos Pods se han eliminado, el nuevo ReplicaSet
puede seguir escalándose, asegurándose que el número total de Pods ejecutándose en todo momento
durante la actualización es como mucho del 130% de los Pods deseados.</p><h3 id=segundos-para-vencimiento-del-progreso>Segundos para vencimiento del progreso</h3><p><code>.spec.progressDeadlineSeconds</code> es un campo opcional que indica el número de segundos que quieres
esperar a que tu Deployment avance antes de que el sistema reporte que dicho Deployment
<a href=#failed-deployment>ha fallado en su avance</a> - expresado como un estado con <code>Type=Progressing</code>, <code>Status=False</code>.
y <code>Reason=ProgressDeadlineExceeded</code> en el recurso. El controlador del Deployment seguirá intentando
el despliegue. En el futuro, una vez que se implemente el retroceso automático, el controlador del Deployment
retrocederá el despliegue en cuanto detecte ese estado.</p><p>Si se especifica, este campo debe ser mayor que <code>.spec.minReadySeconds</code>.</p><h3 id=tiempo-mínimo-para-considerar-el-pod-disponible>Tiempo mínimo para considerar el Pod disponible</h3><p><code>.spec.minReadySeconds</code> es un campo opcional que indica el número mínimo de segundos en que
un Pod recién creado debería estar listo sin que falle ninguno de sus contenedores, para que se considere disponible.
Por defecto su valor es 0 (el Pod se considera disponible en el momento que está listo). Para aprender más acerca de
cuándo un Pod se considera que está listo, ver las <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>pruebas de contenedor</a>.</p><h3 id=vuelta-atrás>Vuelta atrás</h3><p>El campo <code>.spec.rollbackTo</code> se ha quitado de las versiones <code>extensions/v1beta1</code> y <code>apps/v1beta1</code> de la API, y ya no se permite en las versiones de la API a partir de <code>apps/v1beta2</code>.
En su caso, se debería usar <code>kubectl rollout undo</code>, tal y como se explicó en <a href=#rolling-back-to-a-previous-revision>Retroceder a una Revisión Previa</a>.</p><h3 id=límite-del-histórico-de-revisiones>Límite del histórico de revisiones</h3><p>La historia de revisiones de un Deployment se almacena en los ReplicaSets que este controla.</p><p><code>.spec.revisionHistoryLimit</code> es un campo opcional que indica el número de ReplicaSets viejos a retener
para permitir los retrocesos. Estos ReplicaSets viejos consumen recursos en <code>etcd</code> y rebosan la salida de <code>kubectl get rs</code>.
La configuración de cada revisión de Deployment se almacena en sus ReplicaSets;
por lo tanto, una vez que se elimina el ReplicaSet viejo, se pierde la posibilidad de retroceder a dicha revisión del Deployment.
Por defecto, se retienen hasta 10 ReplicaSets viejos; pero su valor ideal depende de la frecuencia y la estabilidad de los nuevos Deployments.</p><p>De forma más específica, si ponemos este campo a cero quiere decir que todos los ReplicaSets viejos con 0 réplicas se limpiarán.
En este caso, el nuevo despliegue del Deployment no se puede deshacer, ya que su historia de revisiones se habrá limpiado.</p><h3 id=pausa>Pausa</h3><p><code>.spec.paused</code> es un campo booleano opcional para pausar y reanudar un Deployment. La única diferencia entre
un Deployment pausado y otro que no lo está es que cualquier cambio al PodTemplateSpec del Deployment pausado
no generará nuevos despliegues mientras esté pausado. Un Deployment se pausa de forma predeterminada cuando se crea.</p><h2 id=alternativa-a-los-deployments>Alternativa a los Deployments</h2><h3 id=kubectl-rolling-update>kubectl rolling update</h3><p><a href=/docs/reference/generated/kubectl/kubectl-commands#rolling-update><code>kubectl rolling update</code></a> actualiza los Pods y los ReplicationControllers
de forma similar. Pero se recomienda el uso de Deployments porque se declaran del lado del servidor, y proporcionan características adicionales
como la posibilidad de retroceder a revisiones anteriores incluso después de haber terminado una actualización continua.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6d72299952c37ca8cc61b416e5bdbcd4>2.4 - StatefulSets</h1><p>Un StatefulSet es el objeto de la API workload que se usa para gestionar aplicaciones con estado.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Los StatefulSets son estables (GA) en la versión 1.9.</div><p>Gestiona el despliegue y escalado de un conjunto de <a class=glossary-tooltip title='El objeto más pequeño y simple de Kubernetes. Un Pod es la unidad mínima de computación en Kubernetes y representa uno o más contenedores ejecutándose en el clúster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>,
<em>y garantiza el orden y unicidad</em> de dichos Pods.</p><p>Al igual que un <a class=glossary-tooltip title='Un objeto API que gestiona una aplicación replicada.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>, un StatefulSet gestiona Pods
que se basan en una especificación idéntica de contenedor. A diferencia de un Deployment, un
StatefulSet mantiene una identidad asociada a sus Pods. Estos pods se crean a partir de la
misma especificación, pero no pueden intercambiarse; cada uno tiene su propio identificador persistente
que mantiene a lo largo de cualquier re-programación.</p><p>Un StatefulSet opera bajo el mismo patrón que cualquier otro controlador.
Se define el estado deseado en un <em>objeto</em> StatefulSet, y el <em>controlador</em> del StatefulSet efectúa
las actualizaciones que sean necesarias para alcanzarlo a partir del estado actual.</p><h2 id=usar-statefulsets>Usar StatefulSets</h2><p>Los StatefulSets son valiosos para aquellas aplicaciones que necesitan uno o más de los siguientes:</p><ul><li>Identificadores de red estables, únicos.</li><li>Almacenamiento estable, persistente.</li><li>Despliegue y escalado ordenado, controlado.</li><li>Actualizaciones en línea ordenadas, automatizadas.</li></ul><p>De los de arriba, estable es sinónimo de persistencia entre (re)programaciones de Pods.
Si una aplicación no necesita ningún identificador estable o despliegue,
eliminación, o escalado ordenado, deberías desplegar tu aplicación con un controlador que
proporcione un conjunto de réplicas sin estado, como un
<a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a> o un
<a href=/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a>, ya que están mejor preparados
para tus necesidades sin estado.</p><h2 id=limitaciones>Limitaciones</h2><ul><li>El almacenamiento de un determinado Pod debe provisionarse por un <a href=https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md>Provisionador de PersistentVolume</a> basado en la <code>storage class</code> requerida, o pre-provisionarse por un administrador.</li><li>Eliminar y/o reducir un StatefulSet <em>no</em> eliminará los volúmenes asociados con el StatefulSet. Este comportamiento es intencional y sirve para garantizar la seguridad de los datos, que da más valor que la purga automática de los recursos relacionados del StatefulSet.</li><li>Los StatefulSets actualmente necesitan un <a href=/docs/concepts/services-networking/service/#headless-services>Servicio Headless</a> como responsable de la identidad de red de los Pods. Es tu responsabilidad crear este Service.</li><li>Los StatefulSets no proporcionan ninguna garantía de la terminación de los pods cuando se elimina un StatefulSet. Para conseguir un término de los pods ordenado y controlado en el StatefulSet, es posible reducir el StatefulSet a 0 réplicas justo antes de eliminarlo.</li><li>Cuando se usan las <a href=#rolling-updates>Actualizaciones en línea</a> con la
<a href=#pod-management-policies>Regla de Gestión de Pod</a> (<code>OrderedReady</code>) por defecto,
es posible entrar en un estado inconsistente que requiere de una
<a href=#retroceso-forzado>intervención manual para su reparación</a>.</li></ul><h2 id=componentes>Componentes</h2><p>El ejemplo de abajo demuestra los componentes de un StatefulSet:</p><ul><li>Un servicio Headless, llamado nginx, se usa para controlar el dominio de red.</li><li>Un StatefulSet, llamado web, que tiene una especificación que indica que se lanzarán 3 réplicas del contenedor nginx en Pods únicos.</li><li>Un volumeClaimTemplate que proporciona almacenamiento estable por medio de <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a> provisionados por un provisionador de tipo PersistentVolume.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span>None<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># tiene que coincidir con .spec.template.metadata.labels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;nginx&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># por defecto es 1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># tiene que coincidir con .spec.selector.matchLabels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/nginx-slim:0.8<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/usr/share/nginx/html<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeClaimTemplates</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;ReadWriteOnce&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;my-storage-class&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=selector-de-pod>Selector de Pod</h2><p>Debes poner el valor del campo <code>.spec.selector</code> de un StatefulSet para que coincida con las etiquetas de su campo <code>.spec.template.metadata.labels</code>. Antes de Kubernetes 1.8,
el campo <code>.spec.selector</code> se predeterminaba cuando se omitía. A partir de la versión 1.8, si no se especifica un selector de coincidencia de Pods, se produce un error de validación
durante la creación del StatefulSet.</p><h2 id=identidad-de-pod>Identidad de Pod</h2><p>Los Pods de un StatefulSet tienen una identidad única que está formada por un ordinal,
una identidad estable de red, y almacenamiento estable. La identidad se asocia al Pod,
independientemente del nodo en que haya sido (re)programado.</p><h3 id=índice-ordinal>Índice Ordinal</h3><p>Para un StatefulSet con N réplicas, a cada Pod del StatefulSet se le asignará
un número entero ordinal, desde 0 hasta N-1, y que es único para el conjunto.</p><h3 id=id-estable-de-red>ID estable de Red</h3><p>El nombre de anfitrión (hostname) de cada Pod de un StatefulSet se deriva del nombre del StatefulSet
y del número ordinal del Pod. El patrón para construir dicho hostname
es <code>$(statefulset name)-$(ordinal)</code>. Así, el ejemplo de arriba creará tres Pods
denominados <code>web-0,web-1,web-2</code>.
Un StatefulSet puede usar un <a href=/docs/concepts/services-networking/service/#headless-services>Servicio Headless</a>
para controlar el nombre de dominio de sus Pods. El nombre de dominio gestionado por este Service tiene la forma:
<code>$(service name).$(namespace).svc.cluster.local</code>, donde "cluster.local" es el nombre de dominio del clúster.
Conforme se crea cada Pod, se le asigna un nombre DNS correspondiente de subdominio, que tiene la forma:
<code>$(podname).$(governing service domain)</code>, donde el servicio en funciones se define por el campo
<code>serviceName</code> del StatefulSet.</p><p>Como se indicó en la sección <a href=#limitaciones>limitaciones</a>, la creación del
<a href=/docs/concepts/services-networking/service/#headless-services>Servicio Headless</a>
encargado de la identidad de red de los pods es enteramente tu responsabilidad.</p><p>Aquí se muestran algunos ejemplos de elecciones de nombres de Cluster Domain, nombres de Service,
nombres de StatefulSet, y cómo impactan en los nombres DNS de los Pods del StatefulSet:</p><table><thead><tr><th>Cluster Domain</th><th>Service (ns/nombre)</th><th>StatefulSet (ns/nombre)</th><th>StatefulSet Domain</th><th>Pod DNS</th><th>Pod Hostname</th></tr></thead><tbody><tr><td>cluster.local</td><td>default/nginx</td><td>default/web</td><td>nginx.default.svc.cluster.local</td><td>web-{0..N-1}.nginx.default.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>cluster.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.cluster.local</td><td>web-{0..N-1}.nginx.foo.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>kube.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.kube.local</td><td>web-{0..N-1}.nginx.foo.svc.kube.local</td><td>web-{0..N-1}</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El valor de Cluster Domain se pondrá a <code>cluster.local</code> a menos que
<a href=/docs/concepts/services-networking/dns-pod-service/>se configure de otra forma</a>.</div><h3 id=almacenamiento-estable>Almacenamiento estable</h3><p>Kubernetes crea un <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolume</a> para cada
VolumeClaimTemplate. En el ejemplo de nginx de arriba, cada Pod recibirá un único PersistentVolume
con una StorageClass igual a <code>my-storage-class</code> y 1 Gib de almacenamiento provisionado. Si no se indica ninguna StorageClass,
entonces se usa la StorageClass por defecto. Cuando un Pod se (re)programa
en un nodo, sus <code>volumeMounts</code> montan los PersistentVolumes asociados con sus
PersistentVolume Claims. Nótese que los PersistentVolumes asociados con los
PersistentVolume Claims de los Pods no se eliminan cuando los Pods, o los StatefulSet se eliminan.
Esto debe realizarse manualmente.</p><h3 id=etiqueta-de-nombre-de-pod>Etiqueta de Nombre de Pod</h3><p>Cuando el controlador del StatefulSet crea un Pod, añade una etiqueta, <code>statefulset.kubernetes.io/pod-name</code>,
que toma el valor del nombre del Pod. Esta etiqueta te permite enlazar un Service a un Pod específico
en el StatefulSet.</p><h2 id=garantías-de-despliegue-y-escalado>Garantías de Despliegue y Escalado</h2><ul><li>Para un StatefulSet con N réplicas, cuando los Pods se despliegan, se crean secuencialmente, en orden de {0..N-1}.</li><li>Cuando se eliminan los Pods, se terminan en orden opuesto, de {N-1..0}.</li><li>Antes de que una operación de escalado se aplique a un Pod, todos sus predecesores deben estar Running y Ready.</li><li>Antes de que se termine un Pod, todos sus sucesores deben haberse apagado completamente.</li></ul><p>El StatefulSet no debería tener que indicar un valor 0 para el campo <code>pod.Spec.TerminationGracePeriodSeconds</code>.
Esta práctica no es segura y se aconseja no hacerlo. Para una explicación más detallada, por favor echa un vistazo a cómo <a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>forzar la eliminación de Pods de un StatefulSet</a>.</p><p>Cuando el ejemplo nginx de arriba se crea, se despliegan tres Pods en el orden
web-0, web-1, web-2. web-1 no se desplegará hasta que web-0 no esté
<a href=/docs/user-guide/pod-states/>Running y Ready</a>, y web-2 no se desplegará hasta que
web-1 esté Running y Ready. En caso de que web-0 fallase, después de que web-1 estuviera Running y Ready, pero antes
de que se desplegara web-2, web-2 no se desplegaría hasta que web-0 se redesplegase con éxito y estuviera
Running y Ready.</p><p>Si un usuario fuera a escalar el ejemplo desplegado parcheando el StatefulSet de forma que
<code>replicas=1</code>, web-2 se terminaría primero. web-1 no se terminaría hasta que web-2
no se hubiera apagado y eliminado por completo. Si web-0 fallase después de que web-2 se hubiera terminado y
apagado completamente, pero antes del término de web-1, entonces web-1 no se terminaría hasta
que web-0 estuviera Running y Ready.</p><h3 id=reglas-de-gestión-de-pods>Reglas de Gestión de Pods</h3><p>En Kubernetes 1.7 y versiones posteriores, el StatefulSet permite flexibilizar sus garantías de ordenación
al mismo tiempo que preservar su garantía de singularidad e identidad a través del campo <code>.spec.podManagementPolicy</code>.</p><h4 id=gestión-de-tipo-orderedready-de-pods>Gestión de tipo OrderedReady de Pods</h4><p>La gestión de tipo <code>OrderedReady</code> de pods es la predeterminada para los StatefulSets. Implementa el comportamiento
descrito <a href=#deployment-and-scaling-guarantees>arriba</a>.</p><h4 id=gestión-de-tipo-parallel-de-pods>Gestión de tipo Parallel de Pods</h4><p>La gestión de tipo <code>Parallel</code> de pods le dice al controlador del StatefulSet que lance y termine
todos los Pods en paralelo, y que no espere a que los Pods estén Running
y Ready o completamente terminados antes de lanzar o terminar otro Pod.</p><h2 id=estrategias-de-actualización>Estrategias de Actualización</h2><p>En Kubernetes 1.7 y a posteriori, el campo <code>.spec.updateStrategy</code> del StatefulSet permite configurar
y deshabilitar las actualizaciones automátizadas en línea para los contenedores, etiquetas, peticiones/límites de recursos,
y anotaciones de los Pods del StatefulSet.</p><h3 id=on-delete>On Delete</h3><p>La estrategia de actualización <code>OnDelete</code> implementa el funcionamiento tradicional (1.6 y previo). Cuando el campo
<code>.spec.updateStrategy.type</code> de un StatefulSet se pone al valor <code>OnDelete</code>, el controlador del StatefulSet no actualizará automáticamente
los Pods del StatefulSet. Los usuarios deben eliminar manualmente los Pods para forzar al controlador a crear
nuevos Pods que reflejen las modificaciones hechas al campo <code>.spec.template</code> del StatefulSet.</p><h3 id=rolling-updates>Rolling Updates</h3><p>La estrategia de actualización <code>RollingUpdate</code> implementa una actualización automatizada en línea de los Pods del
StatefulSet. Es la estrategia por defecto cuando el campo <code>.spec.updateStrategy</code> se deja sin valor. Cuando el campo <code>.spec.updateStrategy.type</code> de un StatefulSet
se pone al valor <code>RollingUpdate</code>, el controlador del StatefulSet lo eliminará y recreará cada Pod en el StatefulSet. Procederá
en el mismo orden en que ha terminado los Pod (del número ordinal más grande al más pequeño), actualizando
cada Pod uno por uno. Esperará a que el Pod actualizado esté Running y Ready antes de
actualizar su predecesor.</p><h4 id=particiones>Particiones</h4><p>La estrategia de actualización <code>RollingUpdate</code> puede particionarse, indicando el valor del campo
<code>.spec.updateStrategy.rollingUpdate.partition</code>. Si se indica una partición, todos los Pods con un
número ordinal mayor o igual que el de la partición serán actualizados cuando el campo <code>.spec.template</code>
del StatefulSet se actualice. Todos los Pods con un número ordinal que sea menor que el de la partición
no serán actualizados, e incluso si son eliminados, serán recreados con la versión anterior. Si el campo
<code>.spec.updateStrategy.rollingUpdate.partition</code> de un StatefulSet es mayor que el valor del campo <code>.spec.replicas</code>,
las modificaciones al campo <code>.spec.template</code> no se propagarán a sus Pods.
En la mayoría de ocasiones, no necesitarás usar una partición, pero pueden resultar útiles si quieres preparar una actualización,
realizar un despliegue tipo canary, o llevar a cabo un despliegue en fases.</p><h4 id=retroceso-forzado>Retroceso Forzado</h4><p>Cuando se usa <a href=#rolling-updates>Actualizaciones en línea</a> con el valor de la
<a href=#pod-management-policies>Regla de Gestión de Pod</a> (<code>OrderedReady</code>) por defecto,
es posible acabar en un estado inconsistente que requiera de una intervención manual para arreglarlo.</p><p>Si actualizas la plantilla Pod a una configuración que nunca llega a Running y
Ready (por ejemplo, debido a un binario incorrecto o un error de configuración a nivel de aplicación),
el StatefulSet detendrá el despliegue y esperará.</p><p>En este estado, no es suficiente con revertir la plantilla Pod a la configuración buena.
Debido a un <a href=https://github.com/kubernetes/kubernetes/issues/67250>problema conocido</a>,
el StatefulSet seguirá esperando a que los Pod estropeados se pongan en Ready
(lo que nunca ocurre) antes de intentar revertirla a la configuración que funcionaba.</p><p>Antes de revertir la plantilla, debes también eliminar cualquier Pod que el StatefulSet haya
intentando ejecutar con la configuración incorrecta.
El StatefulSet comenzará entonces a recrear los Pods usando la plantilla revertida.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><ul><li>Sigue el ejemplo de cómo <a href=/docs/tutorials/stateful-application/basic-stateful-set/>desplegar un aplicación con estado</a>.</li><li>Sigue el ejemplo de cómo <a href=/docs/tutorials/stateful-application/cassandra/>desplegar Cassandra con StatefulSets</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-41600eb8b6631c88848156f381e9d588>2.5 - DaemonSet</h1><p>Un <em>DaemonSet</em> garantiza que todos (o algunos) de los nodos ejecuten una copia de un Pod. Conforme se añade más nodos
al clúster, nuevos Pods son añadidos a los mismos. Conforme se elimina nodos del clúster, dichos Pods se destruyen.
Al eliminar un DaemonSet se limpian todos los Pods que han sido creados.</p><p>Algunos casos de uso típicos de un DaemonSet son:</p><ul><li>Ejecutar un proceso de almacenamiento en el clúster.</li><li>Ejecutar un proceso de recolección de logs en cada nodo.</li><li>Ejecutar un proceso de monitorización de nodos en cada nodo.</li></ul><p>De forma básica, se debería usar un DaemonSet, cubriendo todos los nodos, por cada tipo de proceso.
En configuraciones más complejas se podría usar múltiples DaemonSets para un único tipo de proceso,
pero con diferentes parámetros y/o diferentes peticiones de CPU y memoria según el tipo de hardware.</p><h2 id=escribir-una-especificación-de-daemonset>Escribir una especificación de DaemonSet</h2><h3 id=crear-un-daemonset>Crear un DaemonSet</h3><p>Un DaemonSet se describe por medio de un archivo YAML. Por ejemplo, el archivo <code>daemonset.yaml</code> de abajo describe un DaemonSet que ejecuta la imagen Docker de fluentd-elasticsearch:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/daemonset.yaml download=controllers/daemonset.yaml><code>controllers/daemonset.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-daemonset-yaml")' title="Copy controllers/daemonset.yaml to clipboard"></img></div><div class=includecode id=controllers-daemonset-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DaemonSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>fluentd-logging<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node-role.kubernetes.io/master<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Exists<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/fluentd-elasticsearch/fluentd:v2.5.1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlibdockercontainers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/docker/containers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>30</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlibdockercontainers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/lib/docker/containers<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><ul><li>Crear un DaemonSet basado en el archivo YAML:</li></ul><pre tabindex=0><code>kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
</code></pre><h3 id=campos-requeridos>Campos requeridos</h3><p>Como con cualquier otra configuración de Kubernetes, un DaemonSet requiere los campos <code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>.
Para información general acerca de cómo trabajar con ficheros de configuración, ver los documentos <a href=/docs/user-guide/deploying-applications/>desplegar aplicaciones</a>,
<a href=/docs/tasks/>configurar contenedores</a>, y <a href=/docs/concepts/overview/object-management-kubectl/overview/>gestión de objetos usando kubectl</a>.</p><p>Un DaemonSet también necesita un sección <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>El campo <code>.spec.template</code> es uno de los campos obligatorios de la sección <code>.spec</code>.</p><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/pods/pod-overview/#pod-templates>plantilla Pod</a>. Tiene exactamente el mismo esquema que un <a href=/docs/concepts/workloads/pods/pod/>Pod</a>,
excepto por el hecho de que está anidado y no tiene los campos <code>apiVersion</code> o <code>kind</code>.</p><p>Además de los campos obligatorios de un Pod, la plantilla Pod para un DaemonSet debe especificar
las etiquetas apropiadas (ver <a href=#pod-selector>selector de pod</a>).</p><p>Una plantilla Pod para un DaemonSet debe tener una <a href=/docs/user-guide/pod-states><code>RestartPolicy</code></a>
igual a <code>Always</code>, o no indicarse, lo cual asume por defecto el valor <code>Always</code>.</p><h3 id=selector-de-pod>Selector de Pod</h3><p>El campo <code>.spec.selector</code> es un selector de pod. Funciona igual que el campo <code>.spec.selector</code>
de un <a href=/docs/concepts/jobs/run-to-completion-finite-workloads/>Job</a>.</p><p>A partir de Kubernetes 1.8, se debe configurar un selector de pod que coincida con las
etiquetas definidas en el <code>.spec.template</code>. Así, el selector de pod ya no asume valores por defecto cuando no se indica.
Dichos valores por defecto no eran compatibles con <code>kubectl apply</code>. Además, una vez que se ha creado el DaemonSet,
su campo <code>.spec.selector</code> no puede alterarse porque, si fuera el caso, ello podría resultar
en Pods huérfanos, lo cual confundiría a los usuarios.</p><p>El campo <code>.spec.selector</code> es un objeto que, a su vez, consiste en dos campos:</p><ul><li><code>matchLabels</code> - funciona igual que el campo <code>.spec.selector</code> de un <a href=/docs/concepts/workloads/controllers/replicationcontroller/>ReplicationController</a>.</li><li><code>matchExpressions</code> - permite construir selectores más sofisticados indicando la clave,
la lista de valores y un operador para relacionar la clave y los valores.</li></ul><p>Cuando se configura ambos campos, el resultado es conjuntivo (AND).</p><p>Si se especifica el campo <code>.spec.selector</code>, entonces debe coincidir con el campo <code>.spec.template.metadata.labels</code>. Aquellas configuraciones que no coinciden, son rechazadas por la API.</p><p>Además, normalmente no se debería crear ningún Pod con etiquetas que coincidan con el selector, bien sea de forma directa, via otro
DaemonSet, o via otro controlador como un ReplicaSet. De ser así, el controlador del DaemonSet
pensará que dichos Pods fueron en realidad creados por él mismo. Kubernetes, en cualquier caso, no te impide realizar esta
operación. Un caso donde puede que necesites hacer esto es cuando quieres crear manualmente un Pod con un valor diferente en un nodo para pruebas.</p><h3 id=ejecutar-pods-sólo-en-nodos-seleccionados>Ejecutar Pods sólo en Nodos seleccionados</h3><p>Si se configura un <code>.spec.template.spec.nodeSelector</code>, entonces el controlador del DaemonSet
creará los Pods en aquellos nodos que coincidan con el <a href=/docs/concepts/configuration/assign-pod-node/>selector de nodo</a> indicado.
De forma similar, si se configura una <code>.spec.template.spec.affinity</code>,
entonces el controlador del DaemonSet creará los Pods en aquellos nodos que coincidan con la <a href=/docs/concepts/configuration/assign-pod-node/>afinidad de nodo</a> indicada.
Si no se configura ninguno de los dos, entonces el controlador del DaemonSet creará los Pods en todos los nodos.</p><h2 id=cómo-se-planifican-los-pods-procesos>Cómo se planifican los Pods procesos</h2><h3 id=planificados-por-el-controlador-del-daemonset-deshabilitado-por-defecto-a-partir-de-1-12>Planificados por el controlador del DaemonSet (deshabilitado por defecto a partir de 1.12)</h3><p>Normalmente, el planificador de Kubernetes determina la máquina donde se ejecuta un Pod. Sin embargo, los Pods
creados por el controlador del DaemonSet ya tienen la máquina seleccionada (puesto que cuando se crea el Pod,
se indica el campo <code>.spec.nodeName</code>, y por ello el planificador los ignora). Por lo tanto:</p><ul><li>El controlador del DaemonSet no tiene en cuenta el campo <a href=/docs/admin/node/#manual-node-administration><code>unschedulable</code></a> de un nodo.</li><li>El controlador del DaemonSet puede crear Pods incluso cuando el planificador no ha arrancado, lo cual puede ayudar en el arranque del propio clúster.</li></ul><h3 id=planificados-por-el-planificador-por-defecto-de-kubernetes-habilitado-por-defecto-desde-1-12>Planificados por el planificador por defecto de Kubernetes (habilitado por defecto desde 1.12)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [beta]</code></div><p>Un DaemonSet garantiza que todos los nodos elegibles ejecuten una copia de un Pod.
Normalmente, es el planificador de Kubernetes quien determina el nodo donde se ejecuta un Pod. Sin embargo,
los pods del DaemonSet son creados y planificados por el mismo controlador del DaemonSet.
Esto introduce los siguientes inconvenientes:</p><ul><li>Comportamiento inconsistente de los Pods: Los Pods normales que están esperando
a ser creados, se encuentran en estado <code>Pending</code>, pero los pods del DaemonSet no pasan por el estado <code>Pending</code>.
Esto confunde a los usuarios.</li><li>La <a href=/docs/concepts/configuration/pod-priority-preemption/>prioridad y el comportamiento de apropiación de Pods</a>
se maneja por el planificador por defecto. Cuando se habilita la contaminación, el controlador del DaemonSet
tomará la decisiones de planificación sin considerar ni la prioridad ni la contaminación del pod.</li></ul><p><code>ScheduleDaemonSetPods</code> permite planificar DaemonSets usando el planificador por defecto
en vez del controlador del DaemonSet, añadiendo la condición <code>NodeAffinity</code>
a los pods del DaemonSet, en vez de la condición <code>.spec.nodeName</code>. El planificador por defecto
se usa entonces para asociar el pod a su servidor destino. Si la afinidad de nodo del
pod del DaemonSet ya existe, se sustituye. El controlador del DaemonSet sólo realiza
estas operaciones cuando crea o modifica los pods del DaemonSet, y no se realizan cambios
al <code>spec.template</code> del DaemonSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>matchFields</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>metadata.name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- target-host-name<span style=color:#bbb>
</span></span></span></code></pre></div><p>Adicionalmente, se añade de forma automática la tolerancia <code>node.kubernetes.io/unschedulable:NoSchedule</code>
a los Pods del DaemonSet. Así, el planificador por defecto ignora los nodos
<code>unschedulable</code> cuando planifica los Pods del DaemonSet.</p><h3 id=contaminaciones-taints-y-tolerancias-tolerations>Contaminaciones (taints) y Tolerancias (tolerations)</h3><p>A pesar de que los Pods de proceso respetan las
<a href=/docs/concepts/configuration/taint-and-toleration>contaminaciones y tolerancias</a>,
la siguientes tolerancias son añadidas a los Pods del DaemonSet de forma automática
según las siguientes características:</p><table><thead><tr><th>Clave de tolerancia</th><th>Efecto</th><th>Versión</th><th>Descripción</th></tr></thead><tbody><tr><td><code>node.kubernetes.io/not-ready</code></td><td>NoExecute</td><td>1.13+</td><td>Los pods del DaemonSet no son expulsados cuando hay problemas de nodo como una partición de red.</td></tr><tr><td><code>node.kubernetes.io/unreachable</code></td><td>NoExecute</td><td>1.13+</td><td>Los pods del DaemonSet no son expulsados cuando hay problemas de nodo como una partición de red.</td></tr><tr><td><code>node.kubernetes.io/disk-pressure</code></td><td>NoSchedule</td><td>1.8+</td><td>Los pods del DaemonSet no son expulsados cuando hay problemas de nodo como la falta de espacio en disco.</td></tr><tr><td><code>node.kubernetes.io/memory-pressure</code></td><td>NoSchedule</td><td>1.8+</td><td>Los pods del DaemonSet no son expulsados cuando hay problemas de nodo como la falta de memoria.</td></tr><tr><td><code>node.kubernetes.io/unschedulable</code></td><td>NoSchedule</td><td>1.12+</td><td>Los pods del DaemonSet toleran los atributos unschedulable del planificador por defecto.</td></tr><tr><td><code>node.kubernetes.io/network-unavailable</code></td><td>NoSchedule</td><td>1.12+</td><td>Los pods del DaemonSet, que usan la red del servidor anfitrión, toleran los atributos network-unavailable del planificador por defecto.</td></tr></tbody></table><h2 id=comunicarse-con-los-pods-de-los-daemonsets>Comunicarse con los Pods de los DaemonSets</h2><p>Algunos patrones posibles para la comunicación con los Pods de un DaemonSet son:</p><ul><li><strong>Push</strong>: Los Pods del DaemonSet se configuran para enviar actualizaciones a otro servicio,
como una base de datos de estadísticas. No tienen clientes.</li><li><strong>NodeIP y Known Port</strong>: Los Pods del DaemonSet pueden usar un <code>hostPort</code>, de forma que se les puede alcanzar via las IPs del nodo. Los clientes conocen la lista de IPs del nodo de algún modo,
y conocen el puerto acordado.</li><li><strong>DNS</strong>: Se crea un <a href=/docs/concepts/services-networking/service/#headless-services>servicio headless</a> con el mismo selector de pod,
y entonces se descubre a los DaemonSets usando los recursos <code>endpoints</code> o mediante múltiples registros de tipo A en el DNS.</li><li><strong>Service</strong>: Se crea un servicio con el mismo selector de Pod, y se usa el servicio para llegar al proceso de uno de los nodos. (No hay forma de determinar el nodo exacto.)</li></ul><h2 id=actualizar-un-daemonset>Actualizar un DaemonSet</h2><p>Si se cambian las etiquetas de nodo, el DaemonSet comenzará de forma inmediata a añadir Pods a los nuevos nodos que coincidan y a eliminar
los Pods de aquellos nuevos nodos donde no coincidan.</p><p>Puedes modificar los Pods que crea un DaemonSet. Sin embargo, no se permite actualizar todos los campos de los Pods.
Además, el controlador del DaemonSet utilizará la plantilla original la próxima vez que se cree un nodo (incluso con el mismo nombre).</p><p>Puedes eliminar un DaemonSet. Si indicas el parámetro <code>--cascade=false</code> al usar <code>kubectl</code>,
entonces los Pods continuarán ejecutándose en los nodos. Así, puedes crear entonces un nuevo DaemonSet con una plantilla diferente.
El nuevo DaemonSet con la plantilla diferente reconocerá a todos los Pods existentes que tengan etiquetas coincidentes y
no modificará o eliminará ningún Pod aunque la plantilla no coincida con los Pods desplegados.
Entonces, deberás forzar la creación del nuevo Pod eliminando el Pod mismo o el nodo.</p><p>A partir de las versión 1.6 de Kubernetes, puedes <a href=/docs/tasks/manage-daemon/update-daemon-set/>llevar a cabo una actualización continua</a> en un DaemonSet.</p><h2 id=alternativas-al-daemonset>Alternativas al DaemonSet</h2><h3 id=secuencias-de-comandos-de-inicialización>Secuencias de comandos de inicialización</h3><p>Aunque es perfectamente posible ejecutar procesos arrancándolos directamente en un nodo (ej. usando
<code>init</code>, <code>upstartd</code>, o <code>systemd</code>), existen numerosas ventajas si se realiza via un DaemonSet:</p><ul><li>Capacidad de monitorizar y gestionar los logs de los procesos del mismo modo que para las aplicaciones.</li><li>Mismo lenguaje y herramientas de configuración (ej. plantillas de Pod, <code>kubectl</code>) tanto para los procesos como para las aplicaciones.</li><li>Los procesos que se ejecutan en contenedores con límitaciones de recursos aumentan el aislamiento entre dichos procesos y el resto de contenedores de aplicaciones.
Sin embargo, esto también se podría conseguir ejecutando los procesos en un contenedor en vez de un Pod
(ej. arrancarlos directamente via Docker).</li></ul><h3 id=pods-individuales>Pods individuales</h3><p>Es posible crear Pods directamente sin indicar el nodo donde ejecutarse. Sin embargo,
la ventaja del DaemonSet es que sustituye los Pods que se eliminan o terminan por cualquier razón, como en el caso
de un fallo del nodo o una intervención disruptiva de mantenimiento del nodo, como la actualización del kernel.
Por esta razón, deberías siempre utilizar un DaemonSet en vez de crear Pods individuales.</p><h3 id=pods-estáticos>Pods estáticos</h3><p>Es posible crear Pods a partir de archivos en el directorio donde está escuchando el proceso Kubelet.
Este tipo de Pods se denomina <a href=/docs/concepts/cluster-administration/static-pod/>pods estáticos</a>.
A diferencia del DaemonSet, los Pods estáticos no se pueden gestionar con kubectl
o cualquier otro cliente de la API de Kubernetes. Los Pods estáticos no dependen del apiserver, lo cual los hace
convenientes para el arranque inicial del clúster. Además, puede que los Pods estáticos se deprecien en el futuro.</p><h3 id=deployments>Deployments</h3><p>Los DaemonSets son similares a los <a href=/docs/concepts/workloads/controllers/deployment/>Deployments</a> en el sentido que
ambos crean Pods, y que dichos Pods tienen procesos que no se espera que terminen (ej. servidores web,
servidores de almacenamiento).</p><p>Utiliza un Deployment para definir servicios sin estado, como las interfaces de usuario, donde el escalado vertical y horizontal
del número de réplicas y las actualizaciones continuas son mucho más importantes que el control exacto del servidor donde se ejecuta el Pod.
Utiliza un DaemonSet cuando es importante que una copia de un Pod siempre se ejecute en cada uno de los nodos,
y cuando se necesite que arranque antes que el resto de Pods.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9add0d2120634b63073ad08dc8683bd6>2.6 - Recolección de Basura</h1><p>El papel del recolector de basura de Kubernetes es el de eliminar determinados objetos
que en algún momento tuvieron un propietario, pero que ahora ya no.</p><h2 id=propietarios-y-subordinados>Propietarios y subordinados</h2><p>Algunos objetos de Kubernetes son propietarios de otros objetos. Por ejemplo, un ReplicaSet
es el propietario de un conjunto de Pods. Los objetos que se poseen se denominan <em>subordinados</em> del
objeto propietario. Cada objeto subordinado tiene un campo <code>metadata.ownerReferences</code>
que apunta al objeto propietario.</p><p>En ocasiones, Kubernetes pone el valor del campo <code>ownerReference</code> automáticamente.
Por ejemplo, cuando creas un ReplicaSet, Kubernetes automáticamente pone el valor del campo
<code>ownerReference</code> de cada Pod en el ReplicaSet. A partir de la versión 1.8, Kubernetes
automáticamente pone el valor de <code>ownerReference</code> para los objetos creados o adoptados
por un ReplicationController, ReplicaSet, StatefulSet, DaemonSet, Deployment, Job
y CronJob.</p><p>También puedes configurar las relaciones entre los propietarios y sus subordinados
de forma manual indicando el valor del campo <code>ownerReference</code>.</p><p>Aquí se muestra un archivo de configuración para un ReplicaSet que tiene tres Pods:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/replicaset.yaml download=controllers/replicaset.yaml><code>controllers/replicaset.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-replicaset-yaml")' title="Copy controllers/replicaset.yaml to clipboard"></img></div><div class=includecode id=controllers-replicaset-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-repset<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pod-is-for</span>:<span style=color:#bbb> </span>garbage-collection-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pod-is-for</span>:<span style=color:#bbb> </span>garbage-collection-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Si se crea el ReplicaSet y entonces se muestra los metadatos del Pod, se puede
observar el campo OwnerReferences:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/replicaset.yaml
</span></span><span style=display:flex><span>kubectl get pods --output<span style=color:#666>=</span>yaml
</span></span></code></pre></div><p>La salida muestra que el propietario del Pod es el ReplicaSet denominado <code>my-repset</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  ...
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: apps/v1
</span></span><span style=display:flex><span>    controller: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    kind: ReplicaSet
</span></span><span style=display:flex><span>    name: my-repset
</span></span><span style=display:flex><span>    uid: d9607e19-f88f-11e6-a518-42010a800195
</span></span><span style=display:flex><span>  ...
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Nota:</strong><p>No se recomienda el uso de OwnerReferences entre Namespaces por diseño. Esto quiere decir que:</p><ol><li>Los subordinados dentro del ámbito de Namespaces sólo pueden definir propietarios en ese mismo Namespace,
y propietarios dentro del ámbito de clúster.</li><li>Los subordinados dentro del ámbito del clúster sólo pueden definir propietarios dentro del ámbito del clúster, pero no
propietarios dentro del ámbito de Namespaces.</li></ol></div><h2 id=controlar-cómo-el-recolector-de-basura-elimina-los-subordinados>Controlar cómo el recolector de basura elimina los subordinados</h2><p>Cuando eliminas un objeto, puedes indicar si sus subordinados deben eliminarse también
de forma automática. Eliminar los subordinados automáticamente se denomina <em>borrado en cascada</em>.<br>Hay dos modos de <em>borrado en cascada</em>: <em>en segundo plano</em> y <em>en primer plano</em>.</p><p>Si eliminas un objeto sin borrar sus subordinados de forma automática,
dichos subordinados se convierten en <em>huérfanos</em>.</p><h3 id=borrado-en-cascada-en-primer-plano>Borrado en cascada en primer plano</h3><p>En el <em>borrado en cascada en primer plano</em>, el objeto raíz primero entra en un estado
llamado "deletion in progress". En este estado "deletion in progress",
se cumplen las siguientes premisas:</p><ul><li>El objeto todavía es visible a través de la API REST</li><li>Se pone el valor del campo <code>deletionTimestamp</code> del objeto</li><li>El campo <code>metadata.finalizers</code> del objeto contiene el valor "foregroundDeletion".</li></ul><p>Una vez que se pone el estado "deletion in progress", el recolector de basura elimina
los subordinados del objeto. Una vez que el recolector de basura ha eliminado todos
los subordinados "bloqueantes" (los objetos con <code>ownerReference.blockOwnerDeletion=true</code>), elimina
el objeto propietario.</p><p>Cabe mencionar que usando "foregroundDeletion", sólo los subordinados con valor en
<code>ownerReference.blockOwnerDeletion</code> bloquean la eliminación del objeto propietario.
A partir de la versión 1.7, Kubernetes añadió un <a href=/docs/reference/access-authn-authz/admission-controllers/#ownerreferencespermissionenforcement>controlador de admisión</a>
que controla el acceso de usuario cuando se intenta poner el campo <code>blockOwnerDeletion</code> a true
con base a los permisos de borrado del objeto propietario, de forma que aquellos subordinados no autorizados
no puedan retrasar la eliminación del objeto propietario.</p><p>Si un controlador (como un Deployment o un ReplicaSet) establece el valor del campo <code>ownerReferences</code> de un objeto,
se pone blockOwnerDeletion automáticamente y no se necesita modificar de forma manual este campo.</p><h3 id=borrado-en-cascada-en-segundo-plano>Borrado en cascada en segundo plano</h3><p>En el <em>borrado en cascada en segundo plano</em>, Kubernetes elimina el objeto propietario
inmediatamente y es el recolector de basura quien se encarga de eliminar los subordinados en segundo plano.</p><h3 id=configurar-la-regla-de-borrado-en-cascada>Configurar la regla de borrado en cascada</h3><p>Para controlar la regla de borrado en cascada, configura el campo <code>propagationPolicy</code>
del parámetro <code>deleteOptions</code> cuando elimines un objeto. Los valores posibles incluyen "Orphan",
"Foreground", o "Background".</p><p>Antes de la versión 1.9 de Kubernetes, la regla predeterminada del recolector de basura para la mayoría de controladores era <code>orphan</code>.
Esto incluía al ReplicationController, ReplicaSet, StatefulSet, DaemonSet, y al Deployment.
Para los tipos dentro de las versiones de grupo <code>extensions/v1beta1</code>, <code>apps/v1beta1</code>, y <code>apps/v1beta2</code>, a menos que
se indique de otra manera, los objetos subordinados se quedan huérfanos por defecto.
En Kubernetes 1.9, para todos los tipos de la versión de grupo <code>apps/v1</code>, los objetos subordinados se eliminan por defecto.</p><p>Aquí se muestra un ejemplo que elimina los subordinados en segundo plano:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Background&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>Aquí se muestra un ejemplo que elimina los subordinados en primer plano:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>Aquí se muestra un ejemplo de subordinados huérfanos:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE localhost:8080/apis/apps/v1/namespaces/default/replicasets/my-repset <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>kubectl también permite el borrado en cascada.
Para eliminar los subordinados automáticamente, utiliza el parámetro <code>--cascade</code> a true.
Usa false para subordinados huérfanos. Por defecto, el valor de <code>--cascade</code>
es true.</p><p>Aquí se muestra un ejemplo de huérfanos de subordinados de un ReplicaSet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete replicaset my-repset --cascade<span style=color:#666>=</span><span style=color:#a2f>false</span>
</span></span></code></pre></div><h3 id=nota-adicional-sobre-los-deployments>Nota adicional sobre los Deployments</h3><p>Antes de la versión 1.7, cuando se usaba el borrado en cascada con Deployments se <em>debía</em> usar <code>propagationPolicy: Foreground</code>
para eliminar no sólo los ReplicaSets creados, sino también sus Pods correspondientes. Si este tipo de <em>propagationPolicy</em>
no se usa, solo se elimina los ReplicaSets, y los Pods se quedan huérfanos.
Ver <a href=https://github.com/kubernetes/kubeadm/issues/149#issuecomment-284766613>kubeadm/#149</a> para más información.</p><h2 id=problemas-conocidos>Problemas conocidos</h2><p>Seguimiento en <a href=https://github.com/kubernetes/kubernetes/issues/26120>#26120</a></p><h2 id=siguientes-pasos>Siguientes pasos</h2><p><a href=https://git.k8s.io/design-proposals-archive/api-machinery/garbage-collection.md>Documento de Diseño 1</a></p><p><a href=https://git.k8s.io/design-proposals-archive/api-machinery/synchronous-garbage-collection.md>Documento de Diseño 2</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-4de50a37ebb6f2340484192126cb7a04>2.7 - Controlador TTL para Recursos Finalizados</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.12 [alpha]</code></div><p>El controlador TTL proporciona un mecanismo TTL para limitar el tiempo de vida de los objetos
de recurso que ya han terminado su ejecución. El controlador TTL sólo se ocupa de los
<a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/>Jobs</a> por el momento,
y puede que se extienda para gestionar otros recursos que terminen su ejecución,
como los Pods y los recursos personalizados.</p><p>Descargo de responsabilidad Alpha: esta característica está actualmente en versión alpha, y puede habilitarse mediante el
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
<code>TTLAfterFinished</code>.</p><h2 id=controlador-ttl>Controlador TTL</h2><p>El controlador TTL sólo soporta los Jobs por ahora. Un operador del clúster puede usar esta funcionalidad para limpiar
los Jobs terminados (bien <code>Complete</code> o <code>Failed</code>) automáticamente especificando el valor del campo
<code>.spec.ttlSecondsAfterFinished</code> del Job, como en este
<a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/#clean-up-finished-jobs-automatically>ejemplo</a>.
El controlador TTL asumirá que un recurso es candidato a ser limpiado
TTL segundos después de que el recurso haya terminado; dicho de otra forma, cuando el TTL haya expirado.
Cuando el controlador TTL limpia un recursos, lo elimina en cascada, esto es, borra
sus objetos subordinados juntos. Nótese que cuando se elimina un recurso,
se respetan las garantías de su ciclo de vida, como con los finalizadores.</p><p>Los segundos TTL pueden ser configurados en cualquier momento. Aquí se muestran algunos ejemplos para poner valores al campo
<code>.spec.ttlSecondsAfterFinished</code> de un Job:</p><ul><li>Indicando este campo en el manifiesto de los recursos, de forma que se pueda limpiar un Job
automáticamente un tiempo después de que haya finalizado.</li><li>Haciendo que el campo de los recursos existentes, ya finalizados, adopte esta nueva característica.</li><li>Usando un <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>mutating admission webhook</a>
para poner el valor de este campo dinámicamente en el momento de la creación del recursos. Los administradores del clúster pueden
usar este enfoque para forzar una regla TTL para los recursos terminados.</li><li>Usando un
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>mutating admission webhook</a>
para poner el valor de este campo dinámicamente después de que el recurso haya terminado,
y eligiendo diferentes valores TTL basados en los estados de los recursos, etiquetas, etc.</li></ul><h2 id=advertencia>Advertencia</h2><h3 id=actualizar-los-segundos-ttl>Actualizar los segundos TTL</h3><p>Cabe señalar que el período TTL , ej. campo <code>.spec.ttlSecondsAfterFinished</code> de los Jobs,
puede modificarse después de que el recurso haya sido creado o terminado. Sin embargo, una vez
que el Job se convierte en candidato para ser eliminado (cuando el TTL ha expirado), el sistema
no garantiza que se mantendrán los Jobs, incluso si una modificación para extender el TTL
devuelve una respuesta API satisfactoria.</p><h3 id=diferencia-horaria>Diferencia horaria</h3><p>Como el controlador TTL usa marcas de fecha/hora almacenadas en los recursos de Kubernetes
para determinar si el TTL ha expirado o no, esta funcionalidad es sensible a las
diferencias horarias del clúster, lo que puede provocar que el controlador TTL limpie recursos
en momentos equivocados.</p><p>En Kubernetes, se necesita ejecutar NTP en todos los nodos
(ver <a href=https://github.com/kubernetes/kubernetes/issues/6159#issuecomment-93844058>#6159</a>)
para evitar este problema. Los relojes no siempre son correctos, pero la diferencia debería ser muy pequeña.
Ten presente este riesgo cuando pongas un valor distinto de cero para el TTL.</p><h2 id=siguientes-pasos>Siguientes pasos</h2><p><a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/#clean-up-finished-jobs-automatically>Limpiar Jobs automáticamente</a></p><p><a href=https://github.com/kubernetes/community/blob/master/keps/sig-apps/0026-ttl-after-finish.md>Documento de diseño</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-230b370ed7a1dedf04163f02fa701802>2.8 - Jobs - Ejecución hasta el final</h1><p>Un Job crea uno o más Pods y se asegura de que un número específico de ellos termina de forma satisfactoria.
Conforme los pods terminan satisfactoriamente, el Job realiza el seguimiento de las ejecuciones satisfactorias.
Cuando se alcanza un número específico de ejecuciones satisfactorias, la tarea (esto es, el Job) se completa.
Al eliminar un Job se eliminan los Pods que haya creado.</p><p>Un caso simple de uso es crear un objeto Job para que se ejecute un Pod de manera fiable hasta el final.
El objeto Job arrancará un nuevo Pod si el primer Pod falla o se elimina (por ejemplo
como consecuencia de un fallo de hardware o un reinicio en un nodo).</p><p>También se puede usar un Job para ejecutar múltiples Pods en paralelo.</p><h2 id=ejecutar-un-job-de-ejemplo>Ejecutar un Job de ejemplo</h2><p>Aquí se muestra un ejemplo de configuración de Job. Este ejemplo calcula los primeros 2000 decimales de π y los imprime por pantalla.
Tarda unos 10s en completarse.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/es/examples/controllers/job.yaml download=controllers/job.yaml><code>controllers/job.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-job-yaml")' title="Copy controllers/job.yaml to clipboard"></img></div><div class=includecode id=controllers-job-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>4</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Puedes ejecutar el ejemplo con este comando:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/job.yaml
</span></span></code></pre></div><pre tabindex=0><code>job &#34;pi&#34; created
</code></pre><p>Comprueba el estado del Job con <code>kubectl</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe jobs/pi
</span></span></code></pre></div><pre tabindex=0><code>Name:             pi
Namespace:        default
Selector:         controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495
Labels:           controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495
                  job-name=pi
Annotations:      &lt;none&gt;
Parallelism:      1
Completions:      1
Start Time:       Tue, 07 Jun 2016 10:56:16 +0200
Pods Statuses:    0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:       controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495
                job-name=pi
  Containers:
   pi:
    Image:      perl
    Port:
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen    LastSeen    Count    From            SubobjectPath    Type        Reason            Message
  ---------    --------    -----    ----            -------------    --------    ------            -------
  1m           1m          1        {job-controller }                Normal      SuccessfulCreate  Created pod: pi-dtn4q
</code></pre><p>Para ver los Pods de un Job que se han completado, usa <code>kubectl get pods</code>.</p><p>Para listar todos los Pods que pertenecen a un Job de forma que sea legible, puedes usar un comando como:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>pods</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl get pods --selector<span style=color:#666>=</span>job-name<span style=color:#666>=</span>pi --output<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.items[*].metadata.name}&#39;</span><span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><pre tabindex=0><code>pi-aiw0a
</code></pre><p>En este caso, el selector es el mismo que el selector del Job. La opción <code>--output=jsonpath</code> indica un expresión
que simplemente obtiene el nombre de cada Pod en la lista devuelta.</p><p>Mira la salida estándar de uno de los Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ kubectl logs <span style=color:#b8860b>$pods</span>
</span></span><span style=display:flex><span>3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
</span></span></code></pre></div><h2 id=escribir-una-especificación-de-job>Escribir una especificación de Job</h2><p>Como con el resto de configuraciones de Kubernetes, un Job necesita los campos <code>apiVersion</code>, <code>kind</code>, y <code>metadata</code>.</p><p>Un Job también necesita la <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>sección <code>.spec</code></a>.</p><h3 id=plantilla-pod>Plantilla Pod</h3><p>El campo <code>.spec.template</code> es el único campo obligatorio de <code>.spec</code>.</p><p>El campo <code>.spec.template</code> es una <a href=/docs/concepts/workloads/pods/pod-overview/#pod-templates>plantilla Pod</a>. Tiene exactamente el mismo esquema que un <a href=/docs/user-guide/pods>pod</a>,
excepto por el hecho de que está anidado y no tiene el campo <code>apiVersion</code> o <code>kind</code>.</p><p>Además de los campos olbigatorios de un Pod, una plantilla Pod de un Job debe indicar las etiquetas apropiadas
(ver <a href=#pod-selector>selector de pod</a>) y una regla de reinicio apropiada.</p><p>Sólo se permite los valores <code>Never</code> o <code>OnFailure</code> para <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>RestartPolicy</code></a>.</p><h3 id=selector-de-pod>Selector de Pod</h3><p>El campo <code>.spec.selector</code> es opcional. En la práctica mayoría de los casos no deberías configurarlo.
Mira la sección sobre <a href=#specifying-your-own-pod-selector>configurar tu propio selector de pod</a>.</p><h3 id=jobs-en-paralelo>Jobs en paralelo</h3><p>Hay tres tipos principales de tarea aptos para ejecutarse como un Job:</p><ol><li>Jobs no paralelos</li></ol><ul><li>normalmente, sólo se arranca un Pod, a menos que el Pod falle.</li><li>el Job se completa tan pronto como su Pod termine de forma satisfactoria.</li></ul><ol><li>Jobs en paralelo con un <em>cupo fijo de terminación</em>:</li></ol><ul><li>se configura un valor positivo distinto de cero para el campo <code>.spec.completions</code>.</li><li>el Job representa la tarea en general, y se completa cuando hay una ejecución satisfactoria de un Pod por cada valor dentro del rango de 1 a <code>.spec.completions</code>.</li><li><strong>no implementado todavía:</strong> A cada Pod se le pasa un índice diferenente dentro del rango de 1 a <code>.spec.completions</code>.</li></ul><ol><li>Jobs en paralelo con una <em>cola de trabajo</em>:</li></ol><ul><li>no se especifica el campo <code>.spec.completions</code>, por defecto <code>.spec.parallelism</code>.</li><li>los Pods deben coordinarse entre ellos mismos o a través de un servicio externo que determine quién debe trabajar en qué.
Por ejemplo, un Pod podría ir a buscar un lote de hasta N ítems de una cola de trabajo.</li><li>cada Pod es capaz de forma independiente de determinar si sus compañeros han terminado o no, y como consecuencia el Job entero ha terminado.</li><li>cuando <em>cualquier</em> Pod del Job termina con éxito, no se crean nuevos Pods.</li><li>una vez que al menos uno de los Pods ha terminado con éxito y todos los Pods han terminado, entonces el Job termina con éxito.</li><li>una vez que cualquier Pod ha terminado con éxito, ningún otro Pod debería continuar trabajando en la misma tarea o escribiendo ningún resultado. Todos ellos deberían estar en proceso de terminarse.</li></ul><p>En un Job <em>no paralelo</em>, no debes indicar el valor de <code>.spec.completions</code> ni <code>.spec.parallelism</code>. Cuando ambos se dejan
sin valor, ambos se predeterminan a 1.</p><p>En un Job con <em>cupo fijo de terminación</em>, deberías poner el valor de <code>.spec.completions</code> al número de terminaciones que se necesiten.
Puedes dar un valor a <code>.spec.parallelism</code>, o dejarlo sin valor, en cuyo caso se predetermina a 1.</p><p>En un Job con <em>cola de trabajo</em>, no debes indicar el valor de <code>.spec.completions</code>, y poner el valor de <code>.spec.parallelism</code> a
un entero no negativo.</p><p>Para más información acerca de cómo usar los distintos tipos de Job, ver la sección de <a href=#job-patterns>patrones de job</a>.</p><h4 id=controlar-el-paralelismo>Controlar el paralelismo</h4><p>El paralelismo solicitado (<code>.spec.parallelism</code>) puede usar cualquier valor no negativo.
Si no se indica, se predeterminad a 1.
Si se indica como 0, entonces el Job se pausa de forma efectiva hasta que se incremente.</p><p>El paralelismo actual (número de pods ejecutándose en cada momento) puede que sea mayor o menor que el solicitado,
por los siguientes motivos:</p><ul><li>Para los Jobs con <em>cupo fijo de terminaciones</em>, el número actual de pods ejecutándose en paralelo no excede el número de terminaciones pendientes.
Los valores superiores de <code>.spec.parallelism</code> se ignoran.</li><li>Para los Jobs con <em>cola de trabajo</em>, no se arranca nuevos Pods después de que cualquier Pod se haya completado -- sin embargo, se permite que se completen los Pods pendientes.</li><li>Cuando el controlador no ha tenido tiempo para reaccionar.</li><li>Cuando el controlador no pudo crear los Pods por el motivo que fuera (falta de <code>ResourceQuota</code>, falta de permisos, etc.),
entonces puede que haya menos pods que los solicitados.</li><li>El controlador puede que regule la creación de nuevos Pods debido al excesivo número de fallos anteriores en el mismo Job.</li><li>Cuando un Pod se para de forma controlada, lleva tiempo pararlo.</li></ul><h2 id=gestionar-fallos-de-pod-y-contenedor>Gestionar Fallos de Pod y Contenedor</h2><p>Un contenedor de un Pod puede fallar por cualquier motivo, como porque el proceso que se estaba ejecutando termina con un código de salida distinto de cero,
o porque se mató el contenedor por exceder un límite de memoria, etc. Si esto ocurre, y se tiene
<code>.spec.template.spec.restartPolicy = "OnFailure"</code>, entonces el Pod permance en el nodo,
pero el contenedor se vuelve a ejecutar. Por lo tanto, tu aplicación debe poder gestionar el caso en que se reinicia de forma local,
o bien especificar <code>.spec.template.spec.restartPolicy = "Never"</code>.
Ver el <a href=/docs/concepts/workloads/pods/pod-lifecycle/#example-states>ciclo de vida de un pod</a> para más información sobre <code>restartPolicy</code>.</p><p>Un Pod entero puede también fallar por cualquier motivo, como cuando se expulsa al Pod del nodo
(porque el nodo se actualiza, reinicia, elimina, etc.), o si un contenedor del Pod falla
cuando <code>.spec.template.spec.restartPolicy = "Never"</code>. Cuando un Pod falla, entonces el controlador del Job
arranca un nuevo Pod. Esto quiere decir que tu aplicación debe ser capaz de gestionar el caso en que se reinicia en un nuevo pod.
En particular, debe ser capaz de gestionar los ficheros temporales, los bloqueos, los resultados incompletos, y cualquier otra dependencia
de ejecuciones previas.</p><p>Nótese que incluso si se configura <code>.spec.parallelism = 1</code> y <code>.spec.completions = 1</code> y
<code>.spec.template.spec.restartPolicy = "Never"</code>, el mismo programa puede arrancarse dos veces.</p><p>Si se especifica <code>.spec.parallelism</code> y <code>.spec.completions</code> con valores mayores que 1,
entonces puede que haya múltiples pods ejecutándose a la vez. Por ello, tus pods deben tolerar la concurrencia.</p><h3 id=regla-de-retroceso-de-pod-por-fallo>Regla de retroceso de Pod por fallo</h3><p>Hay situaciones en que quieres que el Job falle después de intentar ejecutarlo unas cuantas veces debido
a un error lógico en la configuración, etc.
Para hacerlo, pon el valor de <code>.spec.backoffLimit</code> al número de reintentos que quieres
antes de considerar el Job como fallido. El límite de retroceso se predetermina a 6.
Los Pods fallidos asociados al Job son recreados por el controlador del Job con un
retroceso exponencial (10s, 20s, 40s ...) limitado a seis minutos. El contador
de retroceso se resetea si no aparecen Pods fallidos antes del siguiente chequeo de estado del Job.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> El problema <a href=https://github.com/kubernetes/kubernetes/issues/54870>#54870</a> todavía existe en las versiones de Kubernetes anteriores a la versión 1.12</div><h2 id=terminación-y-limpieza-de-un-job>Terminación y Limpieza de un Job</h2><p>Cuando un Job se completa, ya no se crea ningún Pod, pero tampoco se elimina los Pods. Guardarlos permite
ver todavía los logs de los pods acabados para comprobar errores, avisos, o cualquier otro resultado de diagnóstico.
El objeto job también se conserva una vez que se ha completado para que se pueda ver su estado. Es decisión del usuario si elimina
los viejos jobs después de comprobar su estado. Eliminar el job con el comando <code>kubectl</code> (ej. <code>kubectl delete jobs/pi</code> o <code>kubectl delete -f ./job.yaml</code>).
Cuando eliminas un job usando el comando <code>kubectl</code>, todos los pods que creó se eliminan también.</p><p>Por defecto, un Job se ejecutará de forma ininterrumpida a menos que uno de los Pods falle, en cuyo caso el Job se fija en el valor de
<code>.spec.backoffLimit</code> descrito arriba. Otra forma de acabar un Job es poniéndole un vencimiento activo.
Haz esto poniendo el valor del campo <code>.spec.activeDeadlineSeconds</code> del Job a un número de segundos.</p><p>El campo <code>activeDeadlineSeconds</code> se aplica a la duración del job, independientemente de cuántos Pods se hayan creado.
Una vez que el Job alcanza <code>activeDeadlineSeconds</code>, se terminan todos sus Pods y el estado del Job se pone como <code>type: Failed</code> con <code>reason: DeadlineExceeded</code>.</p><p>Fíjate que el campo <code>.spec.activeDeadlineSeconds</code> de un Job tiene precedencia sobre el campo <code>.spec.backoffLimit</code>.
Por lo tanto, un Job que está reintentando uno o más Pods fallidos no desplegará nuevos Pods una vez que alcance el límite de tiempo especificado por <code>activeDeadlineSeconds</code>,
incluso si todavía no se ha alcanzado el <code>backoffLimit</code>.</p><p>Ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi-with-timeout<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>activeDeadlineSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><p>Fíjate que tanto la especificación del Job como la <a href=/docs/concepts/workloads/pods/init-containers/#detailed-behavior>especificación de la plantilla Pod</a>
dentro del Job tienen un campo <code>activeDeadlineSeconds</code>. Asegúrate que pones el valor de este campo de forma adecuada.</p><h2 id=limpiar-los-jobs-terminados-automáticamente>Limpiar los Jobs terminados automáticamente</h2><p>Normalmente, los Jobs que han terminado ya no se necesitan en el sistema. Conservarlos sólo añade
más presión al servidor API. Si dichos Jobs no se gestionan de forma directa por un controlador de más alto nivel,
como los <a href=/docs/concepts/workloads/controllers/cron-jobs/>CronJobs</a>, los Jobs pueden
limpiarse por medio de CronJobs en base a la regla de limpieza basada en capacidad que se haya especificado.</p><h3 id=mecanismo-ttl-para-jobs-terminados>Mecanismo TTL para Jobs terminados</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.12 [alpha]</code></div><p>Otra forma de limpiar los Jobs terminados (bien <code>Complete</code> o <code>Failed</code>)
de forma automática es usando un mecanismo TTL proporcionado por un
<a href=/docs/concepts/workloads/controllers/ttlafterfinished/>controlador TTL</a> de recursos finalizados,
indicando el valor <code>.spec.ttlSecondsAfterFinished</code> del Job.</p><p>Cuando el controlador TTL limpia el Job, lo eliminará en cascada,
esto es, eliminará sus objetos subordinados, como Pods, junto con el Job. Nótese
que cuando se elimina el Job, sus garantías de ciclo de vida, como los finalizadores,
se tendrán en cuenta.</p><p>Por ejemplo:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi-with-ttl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ttlSecondsAfterFinished</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><p>Aquí el Job <code>pi-with-ttl</code> será candidato a ser automáticamente eliminado, <code>100</code>
segundos después de que termine.</p><p>Si el campo se pone a <code>0</code>, el Job será candidato a ser automáticamente eliminado
inmediatamente después de haber terminado. Si no se pone valor al campo, este Job no será eliminado
por el controlador TTL una vez concluya.</p><p>Nótese que este mecanismo TTL está todavía en alpha, a través de la característica denominada <code>TTLAfterFinished</code>.
Para más información, ver la documentación del <a href=/docs/concepts/workloads/controllers/ttlafterfinished/>controlador TTL</a> para
recursos terminados.</p><h2 id=patrones-de-job>Patrones de Job</h2><p>El objeto Job puede usarse para dar soporte a la ejecución fiable de Pods en paralelo. El objeto Job
no se diseñó para dar soporte a procesos paralelos estrechamente comunicados, como los que comúnmente
se encuentran en la computación científica. Eso sí, permite el proceso paralelo de un conjunto de <em>ítems de trabajo</em> independientes, pero relacionados entre sí.
Estos pueden ser correos a enviar, marcos a renderizar, archivos a codificar, rangos de claves en una base de datos NoSQL a escanear, y demás.</p><p>En un sistema complejo, puede haber múltiples diferentes conjuntos de ítems de trabajo. Aquí sólo se está
considerando un conjunto de ítems de trabajo que el usuario quiere gestionar de forma conjunta — un <em>proceso por lotes</em>.</p><p>Hay varios patrones diferentes para computación en paralelo, cada uno con sus fortalezas y sus debilidades.
Los sacrificios a tener en cuenta son:</p><ul><li>Un objeto Job para cada ítem de trabajo vs. un objeto Job simple para todos los ítems de trabajo. El último es mejor
para grandes números de ítems de trabajo. El primero añade sobrecarga para el usuario y para el sistema
al tener que gestionar grandes números de objetos Job.</li><li>El número de pods creados es igual al número de ítems de trabajo vs. cada Pod puede procesar múltiplese ítems de trabajo.
El primero típicamente requiere menos modificaciones al código existente y a los contenedores.
El último es mejor cuanto mayor sea el número de ítems de trabajo, por las mismas razones que antes..</li><li>Varios enfoques usan una cola de trabajo. Ello requiere ejecutar un servicio de colas,
y modificaciones a las aplicaciones o contenedores existentes para que hagan uso de la cola de trabajo.
Otras estrategias son más fáciles de adaptar a una aplicación ya usando contenedores.</li></ul><p>Los sacrificios a tener en cuenta se indican a continuación, donde las columnas 2 a 4 representan los sacrificios de arriba.
Los nombres de los patrones son también enlaces a ejemplos e información más detallada.</p><table><thead><tr><th>Patrón</th><th style=text-align:center>Objeto Job simple</th><th style=text-align:center>¿Menos pods que ítems de trabajo?</th><th style=text-align:center>¿No modificar la aplicación?</th><th style=text-align:center>¿Funciona en Kube 1.1?</th></tr></thead><tbody><tr><td><a href=/docs/tasks/job/parallel-processing-expansion/>Extensión de la Plantilla Job</a></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>✓</td><td style=text-align:center>✓</td></tr><tr><td><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Cola con Pod por Ítem de Trabajo</a></td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>a veces</td><td style=text-align:center>✓</td></tr><tr><td><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Cola con Cuenta Variable de Pods</a></td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>✓</td></tr><tr><td>Job simple con Asignación Estática de Trabajo</td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>✓</td><td style=text-align:center></td></tr></tbody></table><p>Cuando se especifican terminaciones con <code>.spec.completions</code>, cada Pod creado por el controlado del Job
tiene un <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>spec</code></a>idéntico.
Esto significa que todos los pods de una tarea tendrán la misma línea de comandos y la
misma imagne, los mismo volúmenes, y (casi) las mismas variables de entorno.
Estos patrones otorgan diferentes formas de organizar los pods para que trabajen en cosas distintas.</p><p>Esta tabla muestra la configuración necesaria para <code>.spec.parallelism</code> y <code>.spec.completions</code> para cada uno de los patrones.
Aquí, <code>T</code> es el número de ítems de trabajo.</p><table><thead><tr><th>Patrón</th><th style=text-align:center><code>.spec.completions</code></th><th style=text-align:center><code>.spec.parallelism</code></th></tr></thead><tbody><tr><td><a href=/docs/tasks/job/parallel-processing-expansion/>Extensión de la Plantilla Job</a></td><td style=text-align:center>1</td><td style=text-align:center>debería ser 1</td></tr><tr><td><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Cola con Pod por Ítem de Trabajo</a></td><td style=text-align:center>T</td><td style=text-align:center>cualquiera</td></tr><tr><td><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Cola con Cuenta Variable de Pods</a></td><td style=text-align:center>1</td><td style=text-align:center>cualquiera</td></tr><tr><td>Job simple con Asignación Estática de Trabajo</td><td style=text-align:center>T</td><td style=text-align:center>cualquiera</td></tr></tbody></table><h2 id=uso-avanzado>Uso Avanzado</h2><h3 id=especificar-tu-propio-selector-de-pod>Especificar tu propio selector de pod</h3><p>Normalmente, cuando creas un objeto Job, no especificas el campo <code>.spec.selector</code>.
La lógica por defecto del sistema añade este campo cuando se crea el Job.
Se elige un valor de selector que no se entremezcle con otras tareas.</p><p>Sin embargo, en algunos casos, puede que necesites sobreescribir este selector que se configura de forma automática.
Para ello, puedes indicar el valor de <code>.spec.selector</code> en el Job.</p><p>Pero ten mucho cuidado cuando lo hagas. Si configuras un selector de etiquta que no
es único para los pods de ese Job, y que selecciona Pods que no tienen que ver,
entonces estos últimos pueden ser eliminados, o este Job puede contar los otros
Pods para terminarse, o uno o ambos Jobs pueden negarse a crear Pods o ejecutarse hasta el final.
Si se elige un selector que no es único, entonces otros controladores (ej. ReplicationController)
y sus Pods puede comportarse de forma impredecibles también. Kubernetes no te impide cometer un error
especificando el <code>.spec.selector</code>.</p><p>Aquí se muestra un ejemplo de un caso en que puede que necesites usar esta característica.</p><p>Digamos que el Job <code>viejo</code> todavía está ejeuctándose. Quieres que los Pods existentes
sigan corriendo, pero quieres que el resto de los Pods que se creen
usen una plantilla pod diferente y que el Job tenga un nombre nuevo.
Como no puedes modificar el Job porque esos campos no son modificables, eliminas el Job <code>old</code>,
pero <em>dejas sus pods ejecutándose</em> mediante el comando <code>kubectl delete jobs/old --cascade=false</code>.
Antes de eliminarlo, apúntate el selector actual que está usando:</p><pre tabindex=0><code>kind: Job
metadata:
  name: viejo
  ...
spec:
  selector:
    matchLabels:
      job-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
</code></pre><p>Entonces, creas un nuevo Job con el nombre <code>nuevo</code> y le configuras explícitamente el mismo selector.
Puesto que los Pods existentes tienen la etiqueta <code>job-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002</code>,
son controlados por el Job <code>nuevo</code> igualmente.</p><p>Necesitas configurar <code>manualSelector: true</code> en el nuevo Job, ya qye no estás usando
el selector que normalmente se genera de forma automática por el sistema.</p><pre tabindex=0><code>kind: Job
metadata:
  name: nuevo
  ...
spec:
  manualSelector: true
  selector:
    matchLabels:
      job-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
</code></pre><p>El mismo Job nuevo tendrá un uid distinto a <code>a8f3d00d-c6d2-11e5-9f87-42010af00002</code>.
Poniendo <code>manualSelector: true</code> le dice al sistema que sabes lo que estás haciendo
y que te permita hacer este desajuste.</p><h2 id=alternativas>Alternativas</h2><h3 id=pods-simples>Pods simples</h3><p>Cuando el nodo donde un Pod simple se estaba ejecutando se reinicia o falla, dicho pod se termina
y no será reinicado. Sin embargo, un Job creará nuevos Pods para sustituir a los que se han terminando.
Por esta razón, se recomienda que se use un Job en vez de un Pod simple, incluso si tu aplicación
sólo necesita un único Pod.</p><h3 id=replication-controller>Replication Controller</h3><p>Los Jobs son complementarios a los <a href=/docs/user-guide/replication-controller>Replication Controllers</a>.
Un Replication Controller gestiona aquellos Pods que se espera que no terminen (ej. servidores web), y un Job
gestiona aquellos Pods que se espera que terminen (ej. tareas por lotes).</p><p>Como se discutió en el <a href=/docs/concepts/workloads/pods/pod-lifecycle/>Ciclo de vida de un Pod</a>, un <code>Job</code> <em>sólo</em> es apropiado
para aquellos pods con <code>RestartPolicy</code> igual a <code>OnFailure</code> o <code>Never</code>.
(Nota: Si <code>RestartPolicy</code> no se pone, el valor predeterminado es <code>Always</code>.)</p><h3 id=job-simple-arranca-que-arranca-un-controlador-de-pod>Job simple arranca que arranca un controlador de Pod</h3><p>Otro patrón es aquel donde un Job simple crea un Pod que, a su vez, crea otros Pods, actuando como una especie
de controlador personalizado para esos Pods. Esto da la máxima flexibilidad, pero puede que
cueste un poco más de entender y ofrece menos integración con Kubernetes.</p><p>Un ejemplo de este patrón sería un Job que arranca un Pod que ejecuta una secuencia de comandos que, a su vez,
arranca un controlador maestro de Spark (ver el <a href=https://github.com/kubernetes/examples/tree/master/staging/spark/README.md>ejemplo de spark</a>),
ejecuta un manejador de spark, y a continuación lo limpia todo.</p><p>Una ventaja de este enfoque es que el proceso general obtiene la garantía del objeto Job,
además del control completo de los Pods que se crean y cómo se les asigna trabajo.</p><h2 id=cron-jobs>Cron Jobs</h2><p>Puedes utilizar un <a href=/docs/concepts/workloads/controllers/cron-jobs/><code>CronJob</code></a> para crear un Job que se ejecute en una hora/fecha determinadas, de forma similar
a la herramienta <code>cron</code> de Unix.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2e4cec01c525b45eccd6010e21cc76d9>2.9 - CronJob</h1><p>Un <em>Cron Job</em> ejecuta tareas, <a href=/docs/concepts/workloads/controllers/jobs-run-to-completion/>Jobs</a>, a intervalos regulares.</p><p>Un objeto CronJob es como una línea de un archivo <em>crontab</em> (tabla cron). Ejecuta un trabajo de forma periódica
según un horario programado escrito en formato <a href=https://en.wikipedia.org/wiki/Cron>Cron</a>.</p><div class="alert alert-info note callout" role=alert><strong>Nota:</strong> Todos los <code>horarios</code> <strong>CronJob</strong> se basan en la zona horaria del máster donde se inicia el trabajo.</div><p>Para instrucciones sobre cómo crear y trabajar con trabajos programados,
incluyendo definiciones de ejemplo,
puedes consultar <a href=/docs/tasks/job/automated-tasks-with-cron-jobs>Ejecutar tareas automatizadas con trabajos programados</a>.</p><h2 id=limitaciones-de-las-tareas-programados>Limitaciones de las tareas programados</h2><p>Un trabajo programado crea un objeto job <em>como mínimo</em> una vez por cada ejecución de su programación. Decimos "como mínimo" porque
hay determinadas circunstancias bajo las cuales dos trabajos pueden crearse, o ninguno de ellos se crea. Se intenta que estos casos sean residuales,
pero no pueden evitarse completamente. Por lo tanto, los trabajos deberían ser <em>idempotentes</em>, es decir, que se pueden ejecutar más de una vez con el mismo resultado.</p><p>Si el valor de <code>startingDeadlineSeconds</code> se establece a un valor grande o se deja sin especificar (por defecto)
y si el valor de <code>concurrencyPolicy</code> se establece a <code>Allow</code>, los trabajos siempre se ejecutarán por lo menos una vez.</p><p>Para cada CronJob, el controlador de CronJob verifica cuántas programaciones se han perdido desde la última programación hasta el momento actual.
Si hay más de 100 programaciones perdidas, entonces ya no vuelve a ejecutar el trabajo y registra el error:</p><pre tabindex=0><code>Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
</code></pre><p>Es importante destacar que si el campo <code>startingDeadlineSeconds</code> está configurado, es decir, no es nulo (<code>nil</code>), el controlador cuenta cuántos trabajos perdidos se produjeron desde el valor de <code>startingDeadlineSeconds</code>
hasta el momento actual, en vez de la última programación. Por ejemplo, si <code>startingDeadlineSeconds</code> es <code>200</code>, el controlador cuenta cuántos trabajos perdidos se produjeron en los últimos 200 segundos.</p><p>Se cuenta un CronJob como perdido si no se ha podido crear a la hora programada. Por ejemplo, si establecemos el valor de <code>concurrencyPolicy</code> a <code>Forbid</code> y se intentó programar
un CronJob cuando otro previamente programado estaba todavía ejecutándose, entonces contará como perdido.</p><p>Por ejemplo, imagina que un CronJob se configura para programar un nuevo Job cada minuto a partir de las <code>08:30:00</code>, y su campo
<code>startingDeadlineSeconds</code> no se configura. Si el controlador del CronJob no estuviera disponible de <code>08:29:00</code> a <code>10:21:00</code>,
el trabajo no comenzaría porque el número de trabajos perdidos que se habría perdido en su programación sería superior a 100.</p><p>Para ilustrar este concepto mejor, vamos a suponer que programamos un CronJob para que ejecute un nuevo Job cada minuto comenzando a las <code>08:30:00</code>, y establecemos el valor del campo
<code>startingDeadlineSeconds</code> a 200 segundos. Si el controlador del CronJob no se encuentra disponible
durante el mismo período que en el ejemplo anterior (<code>08:29:00</code> a <code>10:21:00</code>,) aún así el Job comenzará a las 10:22:00.
Esto ocurre porque el controlador en este caso comprueba cuántas programaciones perdidas ha habido en los últimos 200 segundos (esto es, 3 programaciones que no se han ejecutado), en vez de comprobarlo a partir de la última programación hasta el momento actual.</p><p>El CronJob es únicamente responsable de crear los Jobs que coinciden con su programación, y
el Job por otro lado es el responsable de gestionar los Pods que representa.</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/es/docs/home/>Home</a>
<a class=text-white href=/es/blog/>Blog</a>
<a class=text-white href=/es/partners/>Partners</a>
<a class=text-white href=/es/community/>Comunidad</a>
<a class=text-white href=/es/case-studies/>Casos de éxito</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 Los autores de Kubernetes | Documentación distribuida bajo <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. Todos los derechos reservados. The Linux Foundation tiene marcas registradas y utiliza marcas registradas. Para obtener una lista de marcas registradas por The Linux Foundation, visita <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>