<!doctype html><html lang=fr class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/architecture/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/architecture/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/architecture/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/architecture/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/architecture/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/architecture/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/architecture/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/architecture/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/architecture/><link rel=alternate hreflang=vi href=https://kubernetes.io/vi/docs/concepts/architecture/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/architecture/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/fr/docs/concepts/architecture/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Architecture de Kubernetes | Kubernetes</title><meta property="og:title" content="Architecture de Kubernetes"><meta property="og:description" content="Architecture Kubernetes"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/fr/docs/concepts/architecture/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Architecture de Kubernetes"><meta itemprop=description content="Architecture Kubernetes"><meta name=twitter:card content="summary"><meta name=twitter:title content="Architecture de Kubernetes"><meta name=twitter:description content="Architecture Kubernetes"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Architecture Kubernetes"><meta property="og:description" content="Architecture Kubernetes"><meta name=twitter:description content="Architecture Kubernetes"><meta property="og:url" content="https://kubernetes.io/fr/docs/concepts/architecture/"><meta property="og:title" content="Architecture de Kubernetes"><meta name=twitter:title content="Architecture de Kubernetes"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/fr/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/fr/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/blog/>Blog de Kubernetes</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/partners/>Partenaires</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/community/>Communauté</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/case-studies/>Études de cas</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/fr/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/fr/docs/concepts/architecture/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/fr/docs/concepts/architecture/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/fr/docs/concepts/architecture/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/fr/docs/concepts/architecture/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/fr/docs/concepts/architecture/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Français (French)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/architecture/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/architecture/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/architecture/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/architecture/>日本語 (Japanese)</a>
<a class=dropdown-item href=/it/docs/concepts/architecture/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/architecture/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/architecture/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/architecture/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/architecture/>Bahasa Indonesia</a>
<a class=dropdown-item href=/vi/docs/concepts/architecture/>Tiếng Việt (Vietnamese)</a>
<a class=dropdown-item href=/ru/docs/concepts/architecture/>Русский (Russian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Version imprimable multipages.
<a href=# onclick="return print(),!1">Cliquer ici pour imprimer</a>.</p><p><a href=/fr/docs/concepts/architecture/>Retour à la version par défaut</a>.</p></div><h1 class=title>Architecture de Kubernetes</h1><div class=lead>Architecture Kubernetes</div><ul><li>1: <a href=#pg-9ef2890698e773b6c0d24fd2c20146f5>Noeuds</a></li><li>2: <a href=#pg-63e7fdf87ba61eb2586bb8c625c23506>Communication Master-Node</a></li><li>3: <a href=#pg-bc804b02614d67025b4c788f1ca87fbc>Concepts sous-jacents au Cloud Controller Manager</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-9ef2890698e773b6c0d24fd2c20146f5>1 - Noeuds</h1><div class=lead>Concept Noeud Kubernetes</div><p>Un nœud est une machine de travail dans Kubernetes, connue auparavant sous le nom de <code>minion</code>.
Un nœud peut être une machine virtuelle ou une machine physique, selon le cluster.
Chaque nœud contient les services nécessaires à l'exécution de <a href=/docs/concepts/workloads/pods/pod/>pods</a> et est géré par les composants du master.
Les services sur un nœud incluent le <a href=/docs/concepts/overview/components/#node-components>container runtime</a>, kubelet et kube-proxy.
Consultez la section <a href=https://git.k8s.io/community/contributors/design-proposals/architecture/architecture.md#the-kubernetes-node>Le Nœud Kubernetes</a> dans le document de conception de l'architecture pour plus de détails.</p><h2 id=statut-du-nœud>Statut du nœud</h2><p>Le statut d'un nœud contient les informations suivantes:</p><ul><li><a href=#addresses>Addresses</a></li><li><a href=#condition>Condition</a></li><li><a href=#capacity>Capacity</a></li><li><a href=#info>Info</a></li></ul><p>Chaque section est décrite en détail ci-dessous.</p><h3 id=adresses>Adresses</h3><p>L'utilisation de ces champs varie en fonction de votre fournisseur de cloud ou de votre configuration physique.</p><ul><li>HostName: Le nom d'hôte tel que rapporté par le noyau du nœud. Peut être remplacé via le paramètre kubelet <code>--hostname-override</code>.</li><li>ExternalIP: En règle générale, l'adresse IP du nœud pouvant être routé en externe (disponible de l'extérieur du cluster).</li><li>InternalIP: En règle générale, l'adresse IP du nœud pouvant être routé uniquement dans le cluster.</li></ul><h3 id=condition>Condition</h3><p>Le champ <code>conditions</code> décrit le statut de tous les nœuds <code>Running</code>.</p><table><thead><tr><th>Node Condition</th><th>Description</th></tr></thead><tbody><tr><td><code>OutOfDisk</code></td><td><code>True</code> si l'espace disponible sur le nœud est insuffisant pour l'ajout de nouveaux pods, sinon <code>False</code></td></tr><tr><td><code>Ready</code></td><td><code>True</code> si le noeud est sain et prêt à accepter des pods, <code>False</code> si le noeud n'est pas sain et n'accepte pas de pods, et <code>Unknown</code> si le contrôleur de noeud n'a pas reçu d'information du noeud depuis <code>node-monitor-grace-period</code> (la valeur par défaut est de 40 secondes)</td></tr><tr><td><code>MemoryPressure</code></td><td><code>True</code> s'il existe une pression sur la mémoire du noeud, c'est-à-dire si la mémoire du noeud est faible; autrement <code>False</code></td></tr><tr><td><code>PIDPressure</code></td><td><code>True</code> s'il existe une pression sur le nombre de processus, c'est-à-dire s'il y a trop de processus sur le nœud; autrement <code>False</code></td></tr><tr><td><code>DiskPressure</code></td><td><code>True</code> s'il existe une pression sur la taille du disque, c'est-à-dire si la capacité du disque est faible; autrement <code>False</code></td></tr><tr><td><code>NetworkUnavailable</code></td><td><code>True</code> si le réseau pour le noeud n'est pas correctement configuré, sinon <code>False</code></td></tr></tbody></table><p>La condition de noeud est représentée sous la forme d'un objet JSON.
Par exemple, la réponse suivante décrit un nœud sain.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;conditions&#34;</span><span>:</span> [
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;Ready&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;True&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>Si le statut de l'état Ready reste <code>Unknown</code> ou <code>False</code> plus longtemps que <code>pod-eviction-timeout</code>, un argument est passé au <a href=/docs/admin/kube-controller-manager/>kube-controller-manager</a> et les pods sur le nœud sont programmés pour être supprimés par le contrôleur du nœud.
Le délai d’expulsion par défaut est de <strong>cinq minutes</strong>..
Dans certains cas, lorsque le nœud est inaccessible, l'apiserver est incapable de communiquer avec le kubelet sur le nœud.
La décision de supprimer les pods ne peut pas être communiquée au kublet tant que la communication avec l'apiserver n'est pas rétablie.
Entre-temps, les pods dont la suppression est planifiée peuvent continuer à s'exécuter sur le nœud inaccessible.</p><p>Dans les versions de Kubernetes antérieures à 1.5, le contrôleur de noeud <a href=/docs/concepts/workloads/pods/pod/#force-deletion-of-pods>forcait la suppression</a> de ces pods inaccessibles de l'apiserver.
Toutefois, dans la version 1.5 et ultérieure, le contrôleur de noeud ne force pas la suppression des pods tant qu'il n'est pas confirmé qu'ils ont cessé de fonctionner dans le cluster.
Vous pouvez voir que les pods en cours d'exécution sur un nœud inaccessible sont dans l'état <code>Terminating</code> ou<code> Unknown</code>.
Dans les cas où Kubernetes ne peut pas déduire de l'infrastructure sous-jacente si un nœud a définitivement quitté un cluster, l'administrateur du cluster peut avoir besoin de supprimer l'objet nœud à la main.
La suppression de l'objet nœud de Kubernetes entraîne la suppression de tous les objets Pod exécutés sur le nœud de l'apiserver et libère leurs noms.</p><p>Dans la version 1.12, la fonctionnalité <code>TaintNodesByCondition</code> est promue en version bêta, ce qui permet au contrôleur de cycle de vie du nœud de créer automatiquement des <a href=/docs/concepts/configuration/taint-and-toleration/>marquages</a> (taints en anglais) qui représentent des conditions.
De même, l'ordonnanceur ignore les conditions lors de la prise en compte d'un nœud; au lieu de cela, il regarde les taints du nœud et les tolérances d'un pod.</p><p>Les utilisateurs peuvent désormais choisir entre l'ancien modèle de planification et un nouveau modèle de planification plus flexible.
Un pod qui n’a aucune tolérance est programmé selon l’ancien modèle.
Mais un pod qui tolère les taints d'un nœud particulier peut être programmé sur ce nœud.</p><div class="alert alert-warning caution callout" role=alert><strong>Avertissement:</strong> L'activation de cette fonctionnalité crée un léger délai entre le moment où une condition est observée et le moment où une taint est créée.
Ce délai est généralement inférieur à une seconde, mais il peut augmenter le nombre de pods programmés avec succès mais rejetés par le kubelet.</div><h3 id=capacité>Capacité</h3><p>Décrit les ressources disponibles sur le nœud: CPU, mémoire et nombre maximal de pods pouvant être planifiés sur le nœud.</p><h3 id=info>Info</h3><p>Informations générales sur le noeud, telles que la version du noyau, la version de Kubernetes (versions de kubelet et kube-proxy), la version de Docker (si utilisée), le nom du système d'exploitation.
Les informations sont collectées par Kubelet à partir du noeud.</p><h2 id=gestion>Gestion</h2><p>Contrairement aux <a href=/docs/concepts/workloads/pods/>pods</a> et aux [services] (/docs/concepts/services-networking/service/), un nœud n'est pas créé de manière inhérente par Kubernetes: il est créé de manière externe par un cloud tel que Google Compute Engine, ou bien il existe dans votre pool de machines physiques ou virtuelles.
Ainsi, lorsque Kubernetes crée un nœud, il crée un objet qui représente le nœud.
Après la création, Kubernetes vérifie si le nœud est valide ou non.
Par exemple, si vous essayez de créer un nœud à partir du contenu suivant:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Node&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;10.240.79.157&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;my-first-k8s-node&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Kubernetes crée un objet noeud en interne (la représentation) et valide le noeud en vérifiant son intégrité en fonction du champ <code>metadata.name</code>.
Si le nœud est valide, c'est-à-dire si tous les services nécessaires sont en cours d'exécution, il est éligible pour exécuter un pod.
Sinon, il est ignoré pour toute activité de cluster jusqu'à ce qu'il devienne valide.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes conserve l'objet pour le nœud non valide et vérifie s'il devient valide.
Vous devez explicitement supprimer l'objet Node pour arrêter ce processus.</div><p>Actuellement, trois composants interagissent avec l'interface de noeud Kubernetes: le contrôleur de noeud, kubelet et kubectl.</p><h3 id=contrôleur-de-nœud>Contrôleur de nœud</h3><p>Le contrôleur de noeud (node controller en anglais) est un composant du master Kubernetes qui gère divers aspects des noeuds.</p><p>Le contrôleur de nœud a plusieurs rôles dans la vie d'un nœud.
La première consiste à affecter un bloc CIDR au nœud lorsqu’il est enregistré (si l’affectation CIDR est activée).</p><p>La seconde consiste à tenir à jour la liste interne des nœuds du contrôleur de nœud avec la liste des machines disponibles du fournisseur de cloud.
Lorsqu'il s'exécute dans un environnement de cloud, chaque fois qu'un nœud est en mauvaise santé, le contrôleur de nœud demande au fournisseur de cloud si la machine virtuelle de ce nœud est toujours disponible.
Sinon, le contrôleur de nœud supprime le nœud de sa liste de nœuds.</p><p>La troisième est la surveillance de la santé des nœuds.
Le contrôleur de noeud est responsable de la mise à jour de la condition NodeReady de NodeStatus vers ConditionUnknown lorsqu'un noeud devient inaccessible (le contrôleur de noeud cesse de recevoir des heartbeats pour une raison quelconque, par exemple en raison d'une panne du noeud), puis de l'éviction ultérieure de tous les pods du noeud. (en utilisant une terminaison propre) si le nœud continue d’être inaccessible.
(Les délais d'attente par défaut sont de 40 secondes pour commencer à signaler ConditionUnknown et de 5 minutes après cela pour commencer à expulser les pods.)</p><p>Le contrôleur de nœud vérifie l'état de chaque nœud toutes les <code>--node-monitor-period</code> secondes.</p><p>Dans les versions de Kubernetes antérieures à 1.13, NodeStatus correspond au heartbeat du nœud.
À partir de Kubernetes 1.13, la fonctionnalité de bail de nœud (node lease en anglais) est introduite en tant que fonctionnalité alpha (feature gate <code>NodeLease</code>, <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/0009-node-heartbeat.md>KEP-0009</a>).
Lorsque la fonction de node lease est activée, chaque noeud a un objet <code>Lease</code> associé dans le namespace <code>kube-node-lease</code> qui est renouvelé périodiquement par le noeud, et NodeStatus et le node lease sont traités comme des heartbeat du noeud.
Les node leases sont renouvelés fréquemment lorsque NodeStatus est signalé de nœud à master uniquement lorsque des modifications ont été apportées ou que suffisamment de temps s'est écoulé (la valeur par défaut est 1 minute, ce qui est plus long que le délai par défaut de 40 secondes pour les nœuds inaccessibles).
Étant donné qu'un node lease est beaucoup plus léger qu'un NodeStatus, cette fonctionnalité rends le heartbeat d'un nœud nettement moins coûteux, tant du point de vue de l'évolutivité que des performances.</p><p>Dans Kubernetes 1.4, nous avons mis à jour la logique du contrôleur de noeud afin de mieux gérer les cas où un grand nombre de noeuds rencontrent des difficultés pour atteindre le master (par exemple parce que le master a un problème de réseau).
À partir de la version 1.4, le contrôleur de noeud examine l’état de tous les noeuds du cluster lorsqu’il prend une décision concernant l’éviction des pods.</p><p>Dans la plupart des cas, le contrôleur de noeud limite le taux d’expulsion à <code>--node-eviction-rate</code> (0,1 par défaut) par seconde, ce qui signifie qu’il n’expulsera pas les pods de plus d’un nœud toutes les 10 secondes.</p><p>Le comportement d'éviction de noeud change lorsqu'un noeud d'une zone de disponibilité donnée devient défaillant.
Le contrôleur de nœud vérifie quel pourcentage de nœuds de la zone est défaillant (la condition NodeReady est ConditionUnknown ou ConditionFalse) en même temps.
Si la fraction de nœuds défaillant est au moins <code>--unhealthy-zone-threshold</code> (valeur par défaut de 0,55), le taux d'expulsion est réduit: si le cluster est petit (c'est-à-dire inférieur ou égal à <code>--large-cluster-size-threshold</code> noeuds - valeur par défaut 50) puis les expulsions sont arrêtées, sinon le taux d'expulsion est réduit à <code>--secondary-node-eviction-rate</code> (valeur par défaut de 0,01) par seconde.</p><p>Ces stratégies sont implémentées par zone de disponibilité car une zone de disponibilité peut être partitionnée à partir du master, tandis que les autres restent connectées.
Si votre cluster ne s'étend pas sur plusieurs zones de disponibilité de fournisseur de cloud, il n'existe qu'une seule zone de disponibilité (la totalité du cluster).</p><p>L'une des principales raisons de la répartition de vos nœuds entre les zones de disponibilité est de pouvoir déplacer la charge de travail vers des zones saines lorsqu'une zone entière tombe en panne.
Par conséquent, si tous les nœuds d’une zone sont défaillants, le contrôleur de nœud expulse à la vitesse normale <code>--node-eviction-rate</code>.
Le cas pathologique se produit lorsque toutes les zones sont complètement défaillantes (c'est-à-dire qu'il n'y a pas de nœuds sains dans le cluster).
Dans ce cas, le contrôleur de noeud suppose qu'il existe un problème de connectivité au master et arrête toutes les expulsions jusqu'à ce que la connectivité soit restaurée.</p><p>À partir de Kubernetes 1.6, NodeController est également responsable de l'expulsion des pods s'exécutant sur des noeuds avec des marques <code>NoExecute</code>, lorsque les pods ne tolèrent pas ces marques.
De plus, en tant que fonctionnalité alpha désactivée par défaut, NodeController est responsable de l'ajout de marques correspondant aux problèmes de noeud tels que les noeuds inaccessibles ou non prêts.
Voir <a href=/docs/concepts/configuration/taint-and-toleration/>cette documentation</a> pour plus de détails sur les marques <code>NoExecute</code> et cette fonctionnalité alpha.</p><p>À partir de la version 1.8, le contrôleur de noeud peut être chargé de créer des tâches représentant les conditions de noeud.
Ceci est une fonctionnalité alpha de la version 1.8.</p><h3 id=auto-enregistrement-des-nœuds>Auto-enregistrement des nœuds</h3><p>Lorsque l'indicateur de kubelet <code>--register-node</code> est à true (valeur par défaut), le kubelet tente de s'enregistrer auprès du serveur d'API.
C'est le modèle préféré, utilisé par la plupart des distributions Linux.</p><p>Pour l'auto-enregistrement (self-registration en anglais), le kubelet est lancé avec les options suivantes:</p><ul><li><code>--kubeconfig</code> - Chemin d'accès aux informations d'identification pour s'authentifier auprès de l'apiserver.</li><li><code>--cloud-provider</code> - Comment lire les métadonnées d'un fournisseur de cloud sur lui-même.</li><li><code>--register-node</code> - Enregistrement automatique avec le serveur API.</li><li><code>--register-with-taints</code> - Enregistrez le noeud avec la liste donnée de marques (séparés par des virgules <code>&lt;key>=&lt;value>:&lt;effect></code>). Sans effet si <code>register-node</code> est à false.</li><li><code>--node-ip</code> - Adresse IP du noeud.</li><li><code>--node-labels</code> - Labels à ajouter lors de l’enregistrement du noeud dans le cluster (voir Restrictions des labels appliquées par le <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>plugin NodeRestriction admission</a> dans les versions 1.13+).</li><li><code>--node-status-update-frequency</code> - Spécifie la fréquence à laquelle kubelet publie le statut de nœud sur master.</li></ul><p>Quand le mode <a href=/docs/reference/access-authn-authz/node/>autorisation de nœud</a> et <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>plugin NodeRestriction admission</a> sont activés, les kubelets sont uniquement autorisés à créer / modifier leur propre ressource de noeud.</p><h4 id=administration-manuelle-de-noeuds>Administration manuelle de noeuds</h4><p>Un administrateur de cluster peut créer et modifier des objets de nœud.</p><p>Si l'administrateur souhaite créer des objets de noeud manuellement, définissez l'argument de kubelet: <code>--register-node=false</code>.</p><p>L'administrateur peut modifier les ressources du nœud (quel que soit le réglage de <code>--register-node</code>).
Les modifications comprennent la définition de labels sur le nœud et son marquage comme non programmable.</p><p>Les étiquettes sur les nœuds peuvent être utilisées avec les sélecteurs de nœuds sur les pods pour contrôler la planification. Par exemple, pour contraindre un pod à ne pouvoir s'exécuter que sur un sous-ensemble de nœuds.</p><p>Marquer un nœud comme non planifiable empêche la planification de nouveaux pods sur ce nœud, mais n'affecte pas les pods existants sur le nœud.
Ceci est utile comme étape préparatoire avant le redémarrage d'un nœud, etc. Par exemple, pour marquer un nœud comme non programmable, exécutez la commande suivante:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl cordon <span style=color:#b8860b>$NODENAME</span>
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Les pods créés par un contrôleur DaemonSet contournent le planificateur Kubernetes et ne respectent pas l'attribut unschedulable sur un nœud.
Cela suppose que les démons appartiennent à la machine même si celle-ci est en cours de vidage des applications pendant qu'elle se prépare au redémarrage.</div><h3 id=capacité-de-nœud>Capacité de nœud</h3><p>La capacité du nœud (nombre de CPU et quantité de mémoire) fait partie de l’objet Node.
Normalement, les nœuds s'enregistrent et indiquent leur capacité lors de la création de l'objet Node.
Si vous faites une <a href=#manual-node-administration>administration manuelle de nœud</a>, alors vous devez définir la capacité du nœud lors de l'ajout d'un nœud.</p><p>Le scheduler Kubernetes veille à ce qu'il y ait suffisamment de ressources pour tous les pods d'un noeud.
Il vérifie que la somme des demandes des conteneurs sur le nœud n'est pas supérieure à la capacité du nœud.
Cela inclut tous les conteneurs lancés par le kubelet, mais pas les conteneurs lancés directement par le <a href=/docs/concepts/overview/components/#noeud-composants>conteneur runtime</a>, ni aucun processus exécuté en dehors des conteneurs.</p><p>Si vous souhaitez réserver explicitement des ressources pour des processus autres que Pod, suivez ce tutoriel pour: <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved>réserver des ressources pour les démons système</a>.</p><h2 id=api-object>API Object</h2><p>L'objet Node est une ressource de niveau supérieur dans l'API REST de Kubernetes.
Plus de détails sur l'objet API peuvent être trouvés à l'adresse suivante: <a href=/docs/reference/generated/kubernetes-api/v1.25/#node-v1-core>Node API object</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-63e7fdf87ba61eb2586bb8c625c23506>2 - Communication Master-Node</h1><div class=lead>Communication Master-Node Kubernetes</div><p>Ce document répertorie les canaux de communication entre l'API du noeud maître (apiserver of master node en anglais) et le reste du cluster Kubernetes.
L'objectif est de permettre aux utilisateurs de personnaliser leur installation afin de sécuriser la configuration réseau, de sorte que le cluster puisse être exécuté sur un réseau non approuvé (ou sur des adresses IP entièrement publiques d'un fournisseur de cloud).</p><h2 id=communication-du-cluster-vers-le-master>Communication du Cluster vers le Master</h2><p>Tous les canaux de communication du cluster au master se terminent à l'apiserver (aucun des autres composants principaux n'est conçu pour exposer des services distants).
Dans un déploiement typique, l'apiserver est configuré pour écouter les connexions distantes sur un port HTTPS sécurisé (443) avec un ou plusieurs types d'<a href=/docs/reference/access-authn-authz/authentication/>authentification</a> client.
Une ou plusieurs formes d'<a href=/docs/reference/access-authn-authz/authorization/>autorisation</a> devraient être activées, notamment si les <a href=/docs/reference/access-authn-authz/authentication/#anonymous-requests>requêtes anonymes</a> ou <a href=/docs/reference/access-authn-authz/authentication/#service-account-tokens>jeton de compte de service</a> sont autorisés.</p><p>Le certificat racine public du cluster doit être configuré pour que les nœuds puissent se connecter en toute sécurité à l'apiserver avec des informations d'identification client valides.
Par exemple, dans un déploiement GKE par défaut, les informations d'identification client fournies au kubelet sont sous la forme d'un certificat client.
Consultez <a href=/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/>amorçage TLS de kubelet</a> pour le provisioning automatisé des certificats de client Kubelet.</p><p>Les pods qui souhaitent se connecter à l'apiserver peuvent le faire de manière sécurisée en utilisant un compte de service afin que Kubernetes injecte automatiquement le certificat racine public et un jeton de support valide dans le pod lorsqu'il est instancié.
Le service <code>kubernetes</code> (dans tous les namespaces) est configuré avec une adresse IP virtuelle redirigée (via kube-proxy) vers le point de terminaison HTTPS sur l'apiserver.</p><p>Les composants du master communiquent également avec l'apiserver du cluster via le port sécurisé.</p><p>Par conséquent, le mode de fonctionnement par défaut pour les connexions du cluster (nœuds et pods s'exécutant sur les nœuds) au master est sécurisé par défaut et peut s'exécuter sur des réseaux non sécurisés et/ou publics.</p><h2 id=communication-du-master-vers-le-cluster>Communication du Master vers le Cluster</h2><p>Il existe deux voies de communication principales du master (apiserver) au cluster.
La première est du processus apiserver au processus kubelet qui s'exécute sur chaque nœud du cluster.
La seconde part de l'apiserver vers n'importe quel nœud, pod ou service via la fonctionnalité proxy de l'apiserver.</p><h3 id=communication-de-l-apiserver-vers-le-kubelet>Communication de l'apiserver vers le kubelet</h3><p>Les connexions de l'apiserver au kubelet sont utilisées pour:</p><ul><li>Récupérer les logs des pods.</li><li>S'attacher (via kubectl) à des pods en cours d'exécution.</li><li>Fournir la fonctionnalité de transfert de port du kubelet.</li></ul><p>Ces connexions se terminent au point de terminaison HTTPS du kubelet.
Par défaut, l'apiserver ne vérifie pas le certificat du kubelet, ce qui rend la connexion sujette aux attaques de type "man-in-the-middle", et <strong>non sûre</strong> sur des réseaux non approuvés et/ou publics.</p><p>Pour vérifier cette connexion, utilisez l'argument <code>--kubelet-certificate-authority</code> pour fournir à l'apiserver un ensemble de certificats racine à utiliser pour vérifier le certificat du kubelet.</p><p>Si ce n'est pas possible, utilisez <a href=/docs/tasks/access-application-cluster/port-forward-access-application-cluster/>SSH tunneling</a> entre l'apiserver et le kubelet si nécessaire pour éviter la connexion sur un réseau non sécurisé ou public.</p><p>Finalement, l'<a href=/docs/admin/kubelet-authentication-authorization/>authentification et/ou autorisation du Kubelet</a> devrait être activée pour sécuriser l'API kubelet.</p><h3 id=apiserver-vers-nodes-pods-et-services>apiserver vers nodes, pods et services</h3><p>Les connexions de l'apiserver à un nœud, à un pod ou à un service sont définies par défaut en connexions HTTP.
Elles ne sont donc ni authentifiées ni chiffrées.
Elles peuvent être exécutées sur une connexion HTTPS sécurisée en préfixant <code>https:</code> au nom du nœud, du pod ou du service dans l'URL de l'API.
Cependant ils ne valideront pas le certificat fourni par le point de terminaison HTTPS ni ne fourniront les informations d'identification du client.
De plus, aucune garantie d'intégrité n'est fournie.
Ces connexions <strong>ne sont actuellement pas sûres</strong> pour fonctionner sur des réseaux non sécurisés et/ou publics.</p><h3 id=ssh-tunnels>SSH Tunnels</h3><p>Kubernetes prend en charge les tunnels SSH pour protéger les communications master -> cluster.
Dans cette configuration, l'apiserver initie un tunnel SSH vers chaque nœud du cluster (en se connectant au serveur ssh sur le port 22) et transmet tout le trafic destiné à un kubelet, un nœud, un pod ou un service via un tunnel.
Ce tunnel garantit que le trafic n'est pas exposé en dehors du réseau dans lequel les nœuds sont en cours d'exécution.</p><p>Les tunnels SSH étant actuellement obsolètes, vous ne devriez pas choisir de les utiliser à moins de savoir ce que vous faites.
Un remplacement pour ce canal de communication est en cours de conception.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bc804b02614d67025b4c788f1ca87fbc>3 - Concepts sous-jacents au Cloud Controller Manager</h1><p>Le concept de cloud controller manager (CCM) (ne pas confondre avec le binaire) a été créé à l'origine pour permettre au code de fournisseur spécifique de cloud et au noyau Kubernetes d'évoluer indépendamment les uns des autres.
Le gestionnaire de contrôleur de cloud fonctionne aux côtés d'autres composants principaux, tels que le gestionnaire de contrôleur Kubernetes, le serveur d'API et le planificateur.
Il peut également être démarré en tant qu’addon Kubernetes, auquel cas il s’exécute sur Kubernetes.</p><p>La conception du gestionnaire de contrôleur de cloud repose sur un mécanisme de plugin qui permet aux nouveaux fournisseurs de cloud de s'intégrer facilement à Kubernetes à l'aide de plugins.
Des plans sont en place pour intégrer de nouveaux fournisseurs de cloud sur Kubernetes et pour migrer les fournisseurs de cloud de l'ancien modèle vers le nouveau modèle CCM.</p><p>Ce document discute des concepts derrière le cloud controller manager et donne des détails sur ses fonctions associées.</p><p>Voici l'architecture d'un cluster Kubernetes sans le cloud controller manager:</p><p><img src=/images/docs/pre-ccm-arch.png alt="Pre CCM Kube Arch"></p><h2 id=conception>Conception</h2><p>Dans le diagramme précédent, Kubernetes et le fournisseur de cloud sont intégrés via plusieurs composants différents:</p><ul><li>Kubelet</li><li>Kubernetes controller manager</li><li>Kubernetes API server</li></ul><p>Le CCM consolide toute la logique dépendante du cloud des trois composants précédents pour créer un point d’intégration unique avec le cloud.
La nouvelle architecture avec le CCM se présente comme suit:</p><p><img src=/images/docs/post-ccm-arch.png alt="CCM Kube Arch"></p><h2 id=composants-du-ccm>Composants du CCM</h2><p>Le CCM rompt certaines fonctionnalités du Kubernetes Controller Manager (KCM) et les exécute en tant que processus séparé.
Plus précisément, il sépare les contrôleurs du KCM qui dépendent du cloud.
Le KCM comporte les boucles de contrôle dépendant du cloud suivantes:</p><ul><li>Contrôleur de nœud</li><li>Contrôleur de volume</li><li>Contrôleur de route</li><li>Contrôleur de service</li></ul><p>Dans la version 1.9, le CCM exécute les contrôleurs suivants de la liste précédente:</p><ul><li>Contrôleur de nœud</li><li>Contrôleur de route</li><li>Contrôleur de service</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Le contrôleur de volume a été délibérément choisi pour ne pas faire partie de CCM.
En raison de la complexité du processus et des efforts déployés pour supprimer la logique de volume spécifique au fournisseur, il a été décidé que le contrôleur de volume ne serait pas déplacé vers CCM.</div><p>Le plan initial de prise en charge des volumes à l'aide de CCM consistait à utiliser des volumes Flex pour prendre en charge des volumes pouvant être connectés.
Cependant, un effort concurrentiel appelé CSI est prévu pour remplacer Flex.</p><p>Compte tenu de cette dynamique, nous avons décidé d'avoir une mesure de transition intermédiaire jusqu'à ce que le CSI soit prêt.</p><h2 id=fonctions-du-ccm>Fonctions du CCM</h2><p>Le CCM hérite ses fonctions des composants de Kubernetes qui dépendent d'un fournisseur de cloud.
Cette section est structurée en fonction de ces composants.</p><h3 id=1-kubernetes-controller-manager>1. Kubernetes controller manager</h3><p>La majorité des fonctions du CCM sont dérivées du KCM.
Comme indiqué dans la section précédente, le CCM exécute les boucles de contrôle suivantes:</p><ul><li>Contrôleur de nœud</li><li>Contrôleur de route</li><li>Contrôleur de service</li></ul><h4 id=contrôleur-de-nœud>Contrôleur de nœud</h4><p>Le contrôleur de noeud est responsable de l'initialisation d'un noeud en obtenant des informations sur les noeuds s'exécutant dans le cluster auprès du fournisseur de cloud.
Le contrôleur de noeud exécute les fonctions suivantes:</p><ol><li>Il initialise le nœud avec des labels de zone/région spécifiques au cloud.</li><li>Il initialise le nœud avec des détails d'instance spécifiques au cloud, tels que le type et la taille de l'instance.</li><li>Il obtient les adresses réseau et le nom d'hôte du nœud.</li><li>Si un nœud ne répond plus, le controlleur vérifie avec le cloud pour voir s'il a été supprimé du cloud.
Si le nœud a été supprimé du cloud, le controlleur supprime l'objet Kubernetes Node.</li></ol><h4 id=contrôleur-de-route>Contrôleur de route</h4><p>Le contrôleur de route est responsable de la configuration appropriée des itinéraires dans le cloud afin que les conteneurs situés sur différents nœuds du cluster Kubernetes puissent communiquer entre eux.
Le contrôleur de route ne s'applique qu'aux clusters Google Compute Engine.</p><h4 id=contrôleur-de-service>Contrôleur de service</h4><p>Le contrôleur de service est chargé d'écouter les événements de création, de mise à jour et de suppression de service.
En fonction de l'état actuel des services dans Kubernetes, il configure les équilibreurs de charge dans le cloud (tels que ELB, Google LB ou Oracle Cloud Infrastructure LB) pour refléter l'état des services dans Kubernetes.
De plus, cela garantit que les services de base des services pour les load balancers dans le cloud sont à jour.</p><h3 id=2-kubelet>2. Kubelet</h3><p>Le contrôleur de noeud contient les fonctionnalités du kubelet dépendant du cloud.
Avant l'introduction du CCM, la sous-unité était responsable de l'initialisation d'un nœud avec des détails spécifiques au cloud, tels que les adresses IP, les étiquettes de région / zone et les informations de type d'instance.
L’introduction du CCM a déplacé cette opération d’initialisation du kubelet vers le CCM.</p><p>Dans ce nouveau modèle, le kubelet initialise un nœud sans informations spécifiques au cloud.
Cependant, il ajoute un marquage au nœud nouvellement créé, qui rend le nœud non planifiable jusqu'à ce que le CCM initialise le nœud avec des informations spécifiques au cloud.
Il supprime ensuite ce marquage.</p><h2 id=mécanisme-de-plugin>Mécanisme de plugin</h2><p>Le cloud controller manager utilise des interfaces Go pour autoriser la mise en œuvre d'implémentations depuis n'importe quel cloud.
Plus précisément, il utilise l'interface CloudProvider définie <a href=https://github.com/kubernetes/cloud-provider/blob/9b77dc1c384685cb732b3025ed5689dd597a5971/cloud.go#L42-L62>ici</a>.</p><p>La mise en œuvre des quatre contrôleurs partagés soulignés ci-dessus, ainsi que certaines configurations ainsi que l'interface partagée du fournisseur de cloud, resteront dans le noyau Kubernetes.
Les implémentations spécifiques aux fournisseurs de cloud seront construites en dehors du noyau et implémenteront les interfaces définies dans le noyau.</p><p>Pour plus d’informations sur le développement de plugins, consultez <a href=/docs/tasks/administer-cluster/developing-cloud-controller-manager/>Developing Cloud Controller Manager</a>.</p><h2 id=autorisation>Autorisation</h2><p>Cette section détaille les accès requis par le CCM sur divers objets API pour effectuer ses opérations.</p><h3 id=contrôleur-de-nœud-1>Contrôleur de nœud</h3><p>Le contrôleur de noeud ne fonctionne qu'avec les objets de noeud.
Il nécessite un accès complet aux objets Node via get, list, create, update, patch, watch et delete.</p><p>v1/Node:</p><ul><li>Get</li><li>List</li><li>Create</li><li>Update</li><li>Patch</li><li>Watch</li><li>Delete</li></ul><h3 id=contrôleur-de-route-1>Contrôleur de route</h3><p>Le contrôleur de route écoute les évenements de création d'objet Node et configure les routes de manière appropriée.
Cela nécessite un accès get aux objets Node.</p><p>v1/Node:</p><ul><li>Get</li></ul><h3 id=contrôleur-de-service-1>Contrôleur de Service</h3><p>Le contrôleur de service écoute les évenements de création, suppression et mises à jour des objets Service et configure les endpoints pour ces Services de manière appropriée.</p><p>Pour accéder aux Services, il faut les accès list et watch.
Pour mettre à jour les Services, il faut les accès patch et update.</p><p>Pour configurer des points de terminaison pour les services, vous devez avoir accès au create, list, get, watch, et update.</p><p>v1/Service:</p><ul><li>List</li><li>Get</li><li>Watch</li><li>Patch</li><li>Update</li></ul><h3 id=autres>Autres</h3><p>La mise en œuvre du CCM nécessite un accès pour créer des événements, et pour garantir un fonctionnement sécurisé, un accès est nécessaire pour créer ServiceAccounts.</p><p>v1/Event:</p><ul><li>Create</li><li>Patch</li><li>Update</li></ul><p>v1/ServiceAccount:</p><ul><li>Create</li></ul><p>Le ClusterRole RBAC pour le CCM ressemble à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- events<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#39;*&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes/status<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- services<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- serviceaccounts<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- persistentvolumes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- endpoints<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=implémentations-des-fournisseurs-de-cloud>Implémentations des fournisseurs de cloud</h2><p>Les fournisseurs de cloud suivants ont implémenté leur CCM:</p><ul><li><a href=https://github.com/digitalocean/digitalocean-cloud-controller-manager>Digital Ocean</a></li><li><a href=https://github.com/oracle/oci-cloud-controller-manager>Oracle</a></li><li><a href=https://github.com/kubernetes/cloud-provider-azure>Azure</a></li><li><a href=https://github.com/kubernetes/cloud-provider-gcp>GCP</a></li><li><a href=https://github.com/kubernetes/cloud-provider-aws>AWS</a></li><li><a href=https://github.com/baidu/cloud-provider-baiducloud>BaiduCloud</a></li><li><a href=https://github.com/linode/linode-cloud-controller-manager>Linode</a></li><li><a href=https://github.com/scaleway/scaleway-cloud-controller-manager>Scaleway</a></li></ul><h2 id=administration-de-cluster>Administration de cluster</h2><p>Des instructions complètes pour la configuration et l'exécution du CCM sont fournies <a href=/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager>ici</a>.</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/fr/docs/home/>Accueil</a>
<a class=text-white href=/fr/blog/>Blog</a>
<a class=text-white href=/fr/partners/>Partenaires</a>
<a class=text-white href=/fr/community/>Communauté</a>
<a class=text-white href=/fr/case-studies/>Études de cas</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>