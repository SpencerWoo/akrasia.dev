<!doctype html><html lang=fr class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/workloads/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/workloads/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/workloads/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/workloads/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/workloads/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/workloads/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/workloads/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/concepts/workloads/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/fr/docs/concepts/workloads/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Workloads | Kubernetes</title><meta property="og:title" content="Workloads"><meta property="og:description" content="Comprendre les Pods, le plus petit objet déployable sur Kubernetes, et les abstractions de haut niveaux vous permettant de les lancer.
"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/fr/docs/concepts/workloads/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Workloads"><meta itemprop=description content="Comprendre les Pods, le plus petit objet déployable sur Kubernetes, et les abstractions de haut niveaux vous permettant de les lancer.
"><meta name=twitter:card content="summary"><meta name=twitter:title content="Workloads"><meta name=twitter:description content="Comprendre les Pods, le plus petit objet déployable sur Kubernetes, et les abstractions de haut niveaux vous permettant de les lancer.
"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Comprendre les Pods, le plus petit objet déployable sur Kubernetes, et les abstractions de haut niveaux vous permettant de les lancer.
"><meta property="og:description" content="Comprendre les Pods, le plus petit objet déployable sur Kubernetes, et les abstractions de haut niveaux vous permettant de les lancer.
"><meta name=twitter:description content="Comprendre les Pods, le plus petit objet déployable sur Kubernetes, et les abstractions de haut niveaux vous permettant de les lancer.
"><meta property="og:url" content="https://kubernetes.io/fr/docs/concepts/workloads/"><meta property="og:title" content="Workloads"><meta name=twitter:title content="Workloads"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/fr/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/fr/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/blog/>Blog de Kubernetes</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/partners/>Partenaires</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/community/>Communauté</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/case-studies/>Études de cas</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/fr/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/fr/docs/concepts/workloads/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/fr/docs/concepts/workloads/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/fr/docs/concepts/workloads/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/fr/docs/concepts/workloads/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/fr/docs/concepts/workloads/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Français (French)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/workloads/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/workloads/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/workloads/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/workloads/>日本語 (Japanese)</a>
<a class=dropdown-item href=/de/docs/concepts/workloads/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/workloads/>Español (Spanish)</a>
<a class=dropdown-item href=/id/docs/concepts/workloads/>Bahasa Indonesia</a>
<a class=dropdown-item href=/uk/docs/concepts/workloads/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Version imprimable multipages.
<a href=# onclick="return print(),!1">Cliquer ici pour imprimer</a>.</p><p><a href=/fr/docs/concepts/workloads/>Retour à la version par défaut</a>.</p></div><h1 class=title>Workloads</h1><div class=lead>Comprendre les Pods, le plus petit objet déployable sur Kubernetes, et les abstractions de haut niveaux vous permettant de les lancer.</div><ul><li>1: <a href=#pg-4d68b0ccf9c683e6368ffdcc40c838d4>Pods</a></li><ul><li>1.1: <a href=#pg-37afa6c66c74400d1579f10faf55e5b6>Aperçu du Pod</a></li><li>1.2: <a href=#pg-99cce294fe789317ee684a6e1f07f20f>Pods</a></li><li>1.3: <a href=#pg-c3c2b9cf30915ec9d46c147201da3332>Cycle de vie d'un Pod</a></li><li>1.4: <a href=#pg-c8d62295ca703fdcef1aaf89fb4c916a>Contraintes de propagation de topologie pour les Pods</a></li><li>1.5: <a href=#pg-1ccbd4eeded6ab138d98b59175bd557e>Init Containers</a></li></ul><li>2: <a href=#pg-89637410cacae45a36ab1cc278c482eb>Contrôleurs</a></li><ul><li>2.1: <a href=#pg-d459b930218774655fa7fd1620625539>ReplicaSet</a></li><li>2.2: <a href=#pg-a2dc0393e0c4079e1c504b6429844e86>Déploiements</a></li><li>2.3: <a href=#pg-6d72299952c37ca8cc61b416e5bdbcd4>StatefulSets</a></li></ul></ul><div class=content><p>Un workload (charge de travail) est une application fonctionnant sur Kubernetes. Que votre workload soit un composant unique ou un agrégat de composants, sur Kubernetes celui-ci fonctionnera dans une série de pods. Dans Kubernetes, un Pod represente un ensemble de conteneur (containers) en fonctionnement sur votre cluster.</p><p>Les pods Kubernetes ont un cycle de vie définit (defined lifecycle). Par exemple, quand un pod est en fonction sur votre cluster et qu’une panne critique survient sur le noeud (node) où se situe ce pod, tous les pods du noeud seront en échec. Kubernetes traite ce niveau d’échec comme un état final :
Vous devez créer un nouveau Pod pour retrouver l’état initial même si le noeud redevient sain.</p><p>Cependant, pour vous simplifier la vie, vous n’avez pas a gérer chaque Pod directement. Vous pouvez utiliser une ressource workload qui gère votre groupe de pods à votre place. Ces ressources configurent des controleurs (controllers) qui s’assurent que le bon nombre et le bon type de pod soit en fonction pour égaler l’état que vous avez spécifié.</p><p>Kubernetes fournit plusieurs ressources workload pré-faites :</p><ul><li><a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> et <a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a>
(qui remplacent l’ancienne ressource <a class=glossary-tooltip title='A (deprecated) API object that manages a replicated application.' data-toggle=tooltip data-placement=top href='/fr/docs/reference/glossary/?all=true#term-replication-controller' target=_blank aria-label=ReplicationController>ReplicationController</a>)).
Le <code>Deployment</code> (déploiement) est une bonne approche pour manager une application stateless sur votre cluster, tous les <code>Pods</code> d’un <code>Deployment</code> sont interchangeables et peuvent être remplacés si besoin.</li><li>Le <a href=/docs/concepts/workloads/controllers/statefulset/><code>StatefulSet</code></a> vous permet de lancer un ou plusieurs Pods en relation qui garde plus ou moins la trace de leurs état.
Par exemple si votre workload enregistre des données de façon persistente, vous pouvez lancer un <code>StatefulSet</code> qui fera le lien entre les <code>Pods</code> et un volume persistent (<a href=/docs/concepts/storage/persistent-volumes/><code>PersistentVolume</code></a>).
Votre code, présent dans les <code>Pods</code> du <code>StatefulSet</code>, peut répliquer des données dans les autres <code>Pods</code> qui sont dans le même <code>StatefulSet</code>,
pour améliorer la résilience global.</li><li>Le <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> permet de définir les <code>Pods</code> qui effectuent des actions sur le noeud local.
Ceux-ci peuvent être fondamental aux opérations de votre cluster, comme un outil d’aide réseau, ou peuvent faire part d’un module complémentaire (add-on).
Pour chaque nouveau noeud ajouté au cluster, le controle plane organise l'ajout d'un <code>Pod</code> pour ce <code>DaemonSet</code> sur le nouveau noeud.</li><li>Les <a href=/docs/concepts/workloads/controllers/job/><code>Job</code></a> et <a href=/docs/concepts/workloads/controllers/cron-jobs/><code>CronJob</code></a> sont des taches lancées jusqu’à accomplissement puis s’arrêtent. Les <code>Jobs</code> réprésentent une tâche ponctuelle, les <code>CronJob</code> sont des tâches récurrentes planifiés.</li></ul><p>Dans l’écosystème étendu de Kubernetes, vous pouvez trouver des ressources workload de fournisseurs tiers qui offrent des fonctionnalités supplémentaires.
L’utilisation d’un <a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/><code>CustomResourceDefinition</code></a> permet d’ajouter une ressource workload d’un fournisseur tiers si vous souhaitez rajouter une fonctionnalité ou un comportement spécifique qui ne fait pas partie du noyau de Kubernetes.
Par exemple, si vous voulez lancer un groupe de <code>Pods</code> pour votre application mais que vous devez arrêter leurs fonctionnement tant qu’ils ne sont pas tous disponibles, alors vous pouvez implémenter ou installer une extension qui permet cette fonctionnalité.</p><h2 id=a-suivre>A suivre</h2><p>Vous pouvez continuer la lecture des ressources, vous pouvez aussi apprendre à connaitre les taches qui leurs sont liées :</p><ul><li>Lancer une <a href=/docs/tasks/run-application/run-stateless-application-deployment/>application stateless en utilisant un <code>Deployment</code></a>.</li><li>Lancer une application statefull, soit comme <a href=/docs/tasks/run-application/run-single-instance-stateful-application/>instance unique</a>
ou alors comme un <a href=/docs/tasks/run-application/run-replicated-stateful-application/>ensemble répliqué</a>.</li><li>Lancer une <a href=/docs/tasks/job/automated-tasks-with-cron-jobs/>tâche automatisée avec un <code>CronJob</code></a>.</li></ul><p>Pour en apprendre plus sur les méchanismes de Kubernetes, de séparation du code et de la configuration,
allez voir <a href=/docs/concepts/configuration/>Configuration</a>.</p><p>Il y a deux concepts supportés qui fournissent un contexte sur le sujet : comment Kubernetes gère les pods pour les applications :</p><ul><li>Le <a href=/docs/concepts/workloads/controllers/garbage-collection/>ramasse-miettes</a>, fait le ménage dans votre cluster après qu’une de <em>vos ressource</em> soit supprimé.</li><li>Le <a href=/docs/concepts/workloads/controllers/ttlafterfinished/>temps de vie d’un controlleur éteint</a> supprime les Jobs une fois qu’un temps définit soit passé après son accomplissement.</li></ul><p>Une fois que votre application est lancée, vous souhaitez peut etre la rendre disponible sur internet comme un <a href=/docs/concepts/services-networking/service/>Service</a> ou comme une application web uniquement en utilsant un <a href=/docs/concepts/services-networking/ingress>Ingress</a>.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-4d68b0ccf9c683e6368ffdcc40c838d4>1 - Pods</h1></div><div class=td-content><h1 id=pg-37afa6c66c74400d1579f10faf55e5b6>1.1 - Aperçu du Pod</h1><div class=lead>Pod Concept Kubernetes</div><p>Cette page fournit un aperçu du <code>Pod</code>, l'objet déployable le plus petit dans le modèle d'objets Kubernetes.</p><h2 id=comprendre-les-pods>Comprendre les Pods</h2><p>Un <em>Pod</em> est l'unité d'exécution de base d'une application Kubernetes--l'unité la plus petite et la plus simple dans le modèle d'objets de Kubernetes--que vous créez ou déployez. Un Pod représente des process en cours d'exécution dans votre <a class=glossary-tooltip title='Un ensemble de machines, appelées des "nœuds", qui exécutent des applications conteneurisées gérées par Kubernetes.' data-toggle=tooltip data-placement=top href='/fr/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=cluster>cluster</a>.</p><p>Un Pod encapsule un conteneur applicatif (ou, dans certains cas, plusieurs conteneurs), des ressources de stockage, une identité réseau (adresse IP) unique, ainsi que des options qui contrôlent comment le ou les conteneurs doivent s'exécuter. Un Pod représente une unité de déploiement : <em>une instance unique d'une application dans Kubernetes</em>, qui peut consister soit en un unique <a class=glossary-tooltip title='Une image exécutable légère et portable qui contient le logiciel et toutes ses dépendances.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/what-is-kubernetes/#why-containers target=_blank aria-label=container>container</a> soit en un petit nombre de conteneurs qui sont étroitement liés et qui partagent des ressources.</p><blockquote><p><a href=https://www.docker.com>Docker</a> est le runtime de conteneurs le plus courant utilisé dans un Pod Kubernetes, mais les Pods prennent également en charge d'autres <a href=/docs/setup/production-environment/container-runtimes/>runtimes de conteneurs</a>.</p></blockquote><p>Les Pods dans un cluster Kubernetes peuvent être utilisés de deux manières différentes :</p><ul><li><strong>les Pods exécutant un conteneur unique</strong>. Le modèle "un-conteneur-par-Pod" est le cas d'utilisation Kubernetes le plus courant ; dans ce cas, vous pouvez voir un Pod comme un wrapper autour d'un conteneur unique, et Kubernetes gère les Pods plutôt que directement les conteneurs.</li><li><strong>les Pods exécutant plusieurs conteneurs devant travailler ensemble</strong>. Un Pod peut encapsuler une application composée de plusieurs conteneurs co-localisés qui sont étroitement liés et qui doivent partager des ressources. Ces conteneurs co-localisés pourraient former une unique unité de service cohésive--un conteneur servant des fichiers d'un volume partagé au public, alors qu'un conteneur "sidecar" séparé rafraîchit ou met à jour ces fichiers. Le Pod enveloppe ensemble ces conteneurs et ressources de stockage en une entité maniable de base.</li></ul><p>Chaque Pod est destiné à exécuter une instance unique d'une application donnée. Si vous désirez mettre à l'échelle votre application horizontalement, (pour fournir plus de ressources au global en exécutant plus d'instances), vous devez utiliser plusieurs Pods, un pour chaque instance. Dans Kubernetes, on parle typiquement de <em>réplication</em>. Des Pods répliqués sont en général créés et gérés en tant que groupe par une ressource de charge de travail et son <a class=glossary-tooltip title="Boucle de contrôle surveillant l'état partagé du cluster à travers l'apiserver et effectuant des changements en essayant de déplacer l'état actuel vers l'état désiré." data-toggle=tooltip data-placement=top href=/docs/admin/kube-controller-manager/ target=_blank aria-label=_contrôleur_>_contrôleur_</a>. Voir <a href=#pods-et-controleurs>Pods et contrôleurs</a> pour plus d'informations.</p><h3 id=comment-les-pods-gèrent-plusieurs-conteneurs>Comment les Pods gèrent plusieurs conteneurs</h3><p>Les Pods sont conçus pour supporter plusieurs process coopérants (sous forme de conteneurs) qui forment une unité de service cohésive. Les conteneurs d'un même Pod sont automatiquement co-localisés et co-programmés sur la même machine physique ou virtuelle dans le cluster. Ces conteneurs peuvent partager des ressources et dépendances, communiquer entre eux, et coordonner quand et comment ils sont arrêtés.</p><p>Notez que grouper plusieurs conteneurs co-localisés et co-gérés dans un unique Pod est un cas d'utilisation relativement avancé. Vous devez utiliser ce pattern seulement dans des instances spécifiques dans lesquelles vos conteneurs sont étroitement liés. Par exemple, vous pourriez avoir un conteneur qui agit comme un serveur web pour des fichiers contenus dans un volume partagé, et un conteneur "sidecar" séparé qui met à jour ces fichiers depuis une source externe, comme dans le diagramme suivant :</p><figure><img src=/images/docs/pod.svg alt="example pod diagram" width=50%></figure><p>Certains Pods ont des <a class=glossary-tooltip title="Un ou plusieurs conteneurs d'initialisation qui doivent être exécutés jusqu'à la fin, avant l'exécution de tout conteneur d'application." data-toggle=tooltip data-placement=top href='/fr/docs/reference/glossary/?all=true#term-init-container' target=_blank aria-label='init containers'>init containers</a> en plus d'<a class=glossary-tooltip title="Un conteneur utilisé pour exécuter une partie d'une charge de travail, comparable à un init conteneur." data-toggle=tooltip data-placement=top href='/fr/docs/reference/glossary/?all=true#term-app-container' target=_blank aria-label='app containers'>app containers</a>. Les Init containers s'exécutent et terminent avant que les conteneurs d'application soient démarrés.</p><p>Les Pods fournissent deux types de ressources partagées pour leurs conteneurs : <em>réseau</em> et <em>stockage</em>.</p><h4 id=réseau>Réseau</h4><p>Chaque Pod se voit assigner une adresse IP unique pour chaque famille d'adresses. Tous les conteneurs d'un Pod partagent le même namespace réseau, y compris l'adresse IP et les ports réseau. Les conteneurs <em>à l'intérieur d'un Pod</em> peuvent communiquer entre eux en utilisant <code>localhost</code>. Lorsque les conteneurs dans un Pod communiquent avec des entités <em>en dehors du Pod</em>, ils doivent coordonner comment ils utilisent les ressources réseau partagées (comme les ports).</p><h4 id=stockage>Stockage</h4><p>Un Pod peut spécifier un jeu de <a class=glossary-tooltip title="Un répertoire contenant des données, accessible aux conteneurs d'un pod." data-toggle=tooltip data-placement=top href=/fr/docs/concepts/storage/volumes/ target=_blank aria-label=volumes>volumes</a> de stockage partagés. Tous les conteneurs dans le Pod peuvent accéder aux volumes partagés, permettant à ces conteneurs de partager des données. Les volumes permettent aussi les données persistantes d'un Pod de survivre au cas où un des conteneurs doit être redémarré. Voir <a href=/docs/concepts/storage/volumes/>Volumes</a> pour plus d'informations sur la façon dont Kubernetes implémente le stockage partagé dans un Pod.</p><h2 id=travailler-avec-des-pods>Travailler avec des Pods</h2><p>Vous aurez rarement à créer directement des Pods individuels dans Kubernetes--même des Pods à un seul conteneur. Ceci est dû au fait que les Pods sont conçus comme des entités relativement éphémères et jetables. Lorsqu'un Pod est créé (directement par vous ou indirectement par un <a class=glossary-tooltip title="Boucle de contrôle surveillant l'état partagé du cluster à travers l'apiserver et effectuant des changements en essayant de déplacer l'état actuel vers l'état désiré." data-toggle=tooltip data-placement=top href=/docs/admin/kube-controller-manager/ target=_blank aria-label=_contrôleur_>_contrôleur_</a>), il est programmé pour s'exécuter sur un <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Node>Node</a> dans votre cluster. Le Pod reste sur ce nœud jusqu'à ce que le process se termine, l'objet pod soit supprimé, le pod soit <em>expulsé</em> par manque de ressources, ou le nœud soit en échec.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Redémarrer un conteneur dans un Pod ne doit pas être confondu avec redémarrer un Pod. Un Pod n'est pas un process, mais un environnement pour exécuter un conteneur. Un Pod persiste jusqu'à ce qu'il soit supprimé.</div><p>Les Pods ne se guérissent pas par eux-mêmes. Si un Pod est programmé sur un Nœud qui échoue, ou si l'opération de programmation elle-même échoue, le Pod est supprimé ; de plus, un Pod ne survivra pas à une expulsion due à un manque de ressources ou une mise en maintenance du Nœud. Kubernetes utilise une abstraction de plus haut niveau, appelée un <em>contrôleur</em>, qui s'occupe de gérer les instances de Pods relativement jetables. Ainsi, même s'il est possible d'utiliser des Pods directement, il est beaucoup plus courant dans Kubernetes de gérer vos Pods en utilisant un contrôleur.</p><h3 id=pods-et-contrôleurs>Pods et contrôleurs</h3><p>Vous pouvez utiliser des ressources de charges de travail pour créer et gérer plusieurs Pods pour vous. Un contrôleur pour la ressource gère la réplication,
le plan de déploiement et la guérison automatique en cas de problèmes du Pod. Par exemple, si un noeud est en échec, un contrôleur note que les Pods de ce noeud
ont arrêté de fonctionner et créent des Pods pour les remplacer. L'ordonnanceur place le Pod de remplacement sur un noeud en fonctionnement.</p><p>Voici quelques exemples de ressources de charges de travail qui gèrent un ou plusieurs Pods :</p><ul><li><a class=glossary-tooltip title='Objet API gérant une application répliquée.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a></li><li><a class=glossary-tooltip title="Gère le déploiement et la mise à l'échelle d'un ensemble de Pods, avec un stockage durable et des identifiants persistants pour chaque Pod." data-toggle=tooltip data-placement=top href=/fr/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a></li><li><a class=glossary-tooltip title="S'assure qu'une copie d'un Pod s'exécute sur un ensemble de nœuds d'un cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a></li></ul><h2 id=templates-de-pod>Templates de Pod</h2><p>Les Templates de Pod sont des spécifications pour créer des Pods, et sont inclus dans les ressources de charges de travail comme
les <a href=/fr/docs/concepts/workloads/controllers/deployment/>Deployments</a>, les <a href=/docs/concepts/jobs/run-to-completion-finite-workloads/>Jobs</a> et
les <a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSets</a>.</p><p>Chaque contrôleur pour une ressource de charges de travail utilise le template de pod à l'intérieur de l'objet pour créer les Pods. Le template de pod fait partie de l'état désiré de la ressource de charges de travail que vous avez utilisé pour exécuter votre application.</p><p>L'exemple ci-dessous est un manifest pour un Job simple avec un <code>template</code> qui démarre un conteneur. Le conteneur dans ce Pod affiche un message puis se met en pause.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Ceci est un template de pod</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;echo &#34;Hello, Kubernetes!&#34; &amp;&amp; sleep 3600&#39;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>OnFailure<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Le template de pod se termine ici</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Modifier le template de pod ou changer pour un nouvau template de pod n'a pas d'effet sur les pods déjà existants. Les Pods ne reçoivent pas une mise à jour
du template directement ; au lieu de cela, un nouveau Pod est créé pour correspondre au nouveau template de pod.</p><p>Par exemple, un contrôleur de Deployment s'assure que les Pods en cours d'exécution correspondent au template de pod en cours. Si le template est mis à jour,
le contrôleur doit supprimer les pods existants et créer de nouveaux Pods avec le nouveau template. Chaque contrôleur de charges de travail implémente ses propres
règles pour gérer les changements du template de Pod.</p><p>Sur les noeuds, le <a class=glossary-tooltip title="Un agent qui s'exécute sur chaque nœud du cluster. Il s'assure que les conteneurs fonctionnent dans un pod." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> n'observe ou ne gère pas directement les détails concernant les templates de pods et leurs mises à jours ; ces détails sont abstraits. Cette abstraction et cette séparation des préoccupations simplifie la sémantique du système, et rend possible l'extension du comportement du cluster sans changer le code existant.</p><h2 id=a-suivre>A suivre</h2><ul><li>En savoir plus sur les <a href=/docs/concepts/workloads/pods/pod/>Pods</a></li><li><a href=https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns>The Distributed System Toolkit: Patterns for Composite Containers</a> explique les dispositions courantes pour des Pods avec plusieurs conteneurs</li><li>En savoir plus sur le comportement des Pods :<ul><li><a href=/docs/concepts/workloads/pods/pod/#termination-of-pods>Terminaison d'un Pod</a></li><li><a href=/docs/concepts/workloads/pods/pod-lifecycle/>Cycle de vie d'un Pod</a></li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-99cce294fe789317ee684a6e1f07f20f>1.2 - Pods</h1><p>Les <em>Pods</em> sont les plus petites unités informatiques déployables
qui peuvent être créées et gérées dans Kubernetes.</p><h2 id=qu-est-ce-qu-un-pod>Qu'est-ce qu'un pod ?</h2><p>Un <em>pod</em> (terme anglo-saxon décrivant un groupe de baleines ou une gousse de pois) est un groupe d'un ou plusieurs conteneurs
(comme des conteneurs Docker), ayant du stockage/réseau partagé, et une spécification
sur la manière d'exécuter ces conteneurs. Les éléments d'un pod sont toujours co-localisés
et co-ordonnancés, et s'exécutent dans un contexte partagé. Un pod modélise un
"hôte logique" spécifique à une application - il contient un ou plusieurs conteneurs applicatifs
qui sont étroitement liés — dans un monde pré-conteneurs, être exécuté sur la même machine
physique ou virtuelle signifierait être exécuté sur le même hôte logique.</p><p>Bien que Kubernetes prenne en charge d'autres runtimes de conteneurs que Docker, Docker est le runtime
le plus connu, et cela aide à décrire des pods en termes Docker.</p><p>Le contexte partagé d'un pod est un ensemble de namespaces Linux, cgroups, et
potentiellement d'autres facettes d'isolation - les mêmes choses qui isolent un conteneur Docker.
Dans le contexte d'un pod, les applications individuelles peuvent se voir appliquer d'autres sous-isolations.</p><p>Les conteneurs d'un pod partagent une adresse IP et un espace de ports, et peuvent communiquer via <code>localhost</code>.
Ils peuvent aussi communiquer entre eux en utilisant des communications inter-process standard comme
les sémaphores SystemV ou la mémoire partagée POSIX. Les conteneurs appartenant à des pods distincts ont des adresses IP
distinctes et ne peuvent pas communiquer par IPC sans <a href=/docs/concepts/policy/pod-security-policy/>configuration spécifique</a>. Ces conteneurs communiquent en général entre eux via les adresses IP de leurs pods.</p><p>Les applications à l'intérieur d'un pod ont aussi accès à des volumes partagés,
qui sont définis dans le cadre d'un pod et sont mis à disposition pour être montés
dans le système de fichiers de chaque application.</p><p>En terme de concepts <a href=https://www.docker.com/>Docker</a>, un pod est modélisé par un groupe de conteneurs Docker
ayant des namespaces et des <a href=/docs/concepts/storage/volumes/>volumes</a> partagés.</p><p>Tout comme des conteneurs applicatifs individuels, les pods sont considérés comme des entités relativement éphémères (plutôt que durables).
Comme discuté dans <a href=/docs/concepts/workloads/pods/pod-lifecycle/>Cycle de vie d'un pod</a>, les pods sont créés, des ID uniques (UID) leurs sont assignés,
et ils sont ordonnancés sur des nœuds où il restent jusqu'à leur arrêt (selon la politique de redémarrage) ou suppression.
Si un nœud meurt, les pods ordonnancés sur ce nœud sont programmés pour être terminés, après un délai d'attente. Un pod donné (défini par un UID)
n'est pas "re-ordonnancé" sur un nouveau nœud ; par contre, il peut être remplacé par un pod identique,
ayant le même nom si désiré, mais avec un nouvel UID (voir <a href=/docs/concepts/workloads/controllers/replicationcontroller/>replication
controller</a> pour plus de détails).</p><p>Lorsque quelque chose, comme un volume, a le même cycle de vie qu'un pod, il existe aussi longtemps
que le pod (avec l'UID donné) existe. Si ce pod est supprimé pour une quelconque raison, même si un remplaçant
identique est recréé, la chose liée (par ex. le volume) est aussi détruite et créée à nouveau.</p><figure><img src=/images/docs/pod.svg width=50%><figcaption><h4>pod diagram</h4></figcaption></figure><p><em>Un pod multi-conteneurs contenant un extracteur de fichiers et un serveur web
utilisant un volume persistant comme espace de stockage partagé entre les conteneurs.</em></p><h2 id=intérêts-des-pods>Intérêts des pods</h2><h3 id=gestion>Gestion</h3><p>Les pods fournissent une unité de service cohérente afin d'avoir un modèle coopératif entre plusieurs processus.
Ils simplifient le déploiement et la gestion d'applications
en fournissant une abstraction de plus haut niveau que l'ensemble des applications les constituant.
Les pods servent d'unité de déploiement, de mise à l'échelle horizontale, et de réplication.
La co-localisation (co-ordonnancement), la fin partagée (par ex. l'arrêt),
la réplication coordonnée, le partage de ressources et la gestion des dépendances sont
traités automatiquement pour les conteneurs dans un pod.</p><h3 id=partage-de-ressources-et-communication>Partage de ressources et communication</h3><p>Les pods permettent le partage de ressources et la communication entre ses constituants.</p><p>Les applications dans un pod utilisent toutes le même réseau (même adresse IP et espace de ports)
et peuvent donc "se trouver" entre elles et communiquer en utilisant <code>localhost</code>.
À cause de cela, les applications dans un pod doivent coordonner leurs usages de ports.
Chaque pod a une adresse IP dans un réseau plat partagé ayant un accès complet
aux autres hôtes et pods à travers le réseau.</p><p>Le nom d'hôte est défini avec le nom du pod pour les conteneurs applicatifs à l'intérieur du pod.
<a href=/docs/concepts/cluster-administration/networking/>Plus de détails sur le réseau</a>.</p><p>En plus de définir les conteneurs applicatifs s'exécutant dans le pod, le pod spécifie
un ensemble de volumes de stockage partagés. Les volumes permettent aux données de survivre
aux redémarrages de conteneurs et d'être partagés entre les applications d'un même pod.</p><h2 id=cas-d-utilisation-de-pods>Cas d'utilisation de pods</h2><p>Des pods peuvent être utilisés pour héberger verticalement des piles applicatives intégrées (par ex. LAMP),
mais leur principal intérêt est la mise en place de programmes auxiliaires co-localisés et co-gérés, comme :</p><ul><li>systèmes de gestion de contenu, chargeurs de fichiers et de données, gestionnaires de cache local, etc.</li><li>sauvegarde de log et checkpoint, compression, rotation, prise d'instantanés, etc.</li><li>data change watchers, log tailers, adaptateurs de logs et monitoring, éditeurs d'événements, etc.</li><li>proxies, bridges et adaptateurs</li><li>contrôleurs, gestionnaires, configurateurs et gestionnaires de mise à jour</li></ul><p>Des pods individuels ne sont pas destinés à exécuter plusieurs instances de la même application, en général.</p><p>Pour une explication plus détaillée, voir <a href=https://kubernetes.io/blog/2015/06/the-distributed-system-toolkit-patterns>The Distributed System ToolKit: Patterns for
Composite
Containers</a>.</p><h2 id=alternatives-envisagées>Alternatives envisagées</h2><p><em>Pourquoi ne pas simplement exécuter plusieurs programmes dans un unique conteneur (Docker) ?</em></p><ol><li>Transparence. Rendre les conteneurs à l'intérieur du pod visibles par l'infrastucture
permet à l'infrastucture de fournir des services à ces conteneurs,
comme la gestion des processus et le monitoring des ressources. Ceci
apporte un certain nombre de facilités aux utilisateurs.</li><li>Découpler les dépendances logicielles. Les conteneurs individuels peuvent être
versionnés, reconstruits et redéployés de manière indépendante. Kubernetes pourrait
même un jour prendre en charge la mise à jour à chaud de conteneurs individuels.</li><li>Facilité d'utilisation. Les utilisateurs n'ont pas besoin d'exécuter leur propre gestionnaire
de processus, de se soucier de la propagation de signaux et de codes de sortie, etc.</li><li>Efficacité. L'infrastructure prenant plus de responsabilités, les conteneurs peuvent être plus légers.</li></ol><p><em>Pourquoi ne pas prendre en charge le co-ordonnancement de conteneurs basé sur les affinités ?</em></p><p>Cette approche pourrait fournir la co-localisation, mais ne fournirait pas la plupart
des bénéfices des pods, comme le partage de ressources, IPC, la garantie d'une fin partagée et une gestion simplifiée.</p><h2 id=durabilité-des-pods-ou-manque-de>Durabilité des pods (ou manque de)</h2><p>Les pods ne doivent pas être considérés comme des entités durables. Ils ne survivent pas à des erreurs d'ordonnancement, à un nœud en échec
ou à d'autres expulsions, suite à un manque de ressources ou une mise en maintenance d'un nœud.</p><p>En général, les utilisateurs n'ont pas à créer directement des pods. Ils doivent presque toujours
utiliser des contrôleurs, même pour des singletons, comme par exemple des <a href=/docs/concepts/workloads/controllers/deployment/>Deployments</a>.
Les contrôleurs fournissent l'auto-guérison à l'échelle du cluster, ainsi que la réplication et la gestion des déploiements (rollout).
Les contrôleurs comme <a href=/docs/concepts/workloads/controllers/statefulset.md>StatefulSet</a>
peuvent aussi prendre en charge des pods avec état (stateful).</p><p>L'utilisation d'APIs collectives comme principale primitive exposée à l'utilisateur est courante dans les systèmes d'ordonnancement de clusters, comme <a href=https://research.google.com/pubs/pub43438.html>Borg</a>, <a href=https://mesosphere.github.io/marathon/docs/rest-api.html>Marathon</a>, <a href=http://aurora.apache.org/documentation/latest/reference/configuration/#job-schema>Aurora</a>, et <a href=http://www.slideshare.net/Docker/aravindnarayanan-facebook140613153626phpapp02-37588997>Tupperware</a>.</p><p>Un Pod est exposé en tant que primitive afin de faciliter :</p><ul><li>la connexion du scheduler et du contrôleur</li><li>la possibilité d'opérations au niveau du pod sans besoin de passer par des APIs au niveau du contrôleur</li><li>le découplage du cycle de fin d'un pod de celui d'un contrôleur, comme pour l'amorçage (bootstrapping)</li><li>le découplage des contrôleurs et des services — le contrôleur d'endpoints examine uniquement des pods</li><li>la composition claire des fonctionnalités niveau Kubelet et des fonctionnalités niveau cluster — concrètement, Kubelet est le "contrôleur de pods"</li><li>les applications hautement disponibles, qui attendront que les pods soient remplacés avant leur arrêt et au moins avant leur suppression, comme dans les cas d'éviction programmée ou de pré-chargement d'image.</li></ul><h2 id=arrêt-de-pods>Arrêt de pods</h2><p>Les pods représentant des processus s'exécutant sur des nœuds d'un cluster, il est important de permettre à ces processus de se terminer proprement
lorsqu'ils ne sont plus nécessaires (plutôt que d'être violemment tués avec un signal KILL et n'avoir aucune chance de libérer ses ressources). Les
utilisateurs doivent pouvoir demander une suppression et savoir quand les processus se terminent, mais aussi être capable de s'assurer que la suppression
est réellement effective. Lorsqu'un utilisateur demande la suppression d'un pod, le système enregistre le délai de grâce prévu avant que le pod puisse
être tué de force, et qu'un signal TERM soit envoyé au processus principal de chaque conteneur. Une fois la période de grâce expirée, le signal KILL
est envoyé à ces processus, et le pod est alors supprimé de l'API server. Si Kubelet ou le gestionnaire de conteneurs est redémarré lors de l'attente de l'arrêt des processus, l'arrêt sera réessayé avec la période de grâce complète.</p><p>Un exemple de déroulement :</p><ol><li>Un utilisateur envoie une commande pour supprimer un Pod, avec une période de grâce par défaut (30s)</li><li>Le Pod dans l'API server est mis à jour avec le temps au delà duquel le Pod est considéré "mort" ainsi que la période de grâce.</li><li>Le Pod est affiché comme "Terminating" dans les listes des commandes client</li><li>(en même temps que 3) Lorsque Kubelet voit qu'un Pod a été marqué "Terminating", le temps ayant été mis en 2, il commence le processus de suppression du pod.<ol><li>Si un des conteneurs du Pod a défini un <a href=/fr/docs/concepts/containers/container-lifecycle-hooks/#hook-details>preStop hook</a>, il est exécuté à l'intérieur du conteneur. Si le <code>preStop</code> hook est toujours en cours d'exécution à la fin de la période de grâce, l'étape 2 est invoquée avec une courte (2 secondes) période de grâce supplémentaire une seule fois. Vous devez modifier <code>terminationGracePeriodSeconds</code> si le hook <code>preStop</code> a besoin de plus de temps pour se terminer.</li><li>Le signal TERM est envoyé aux conteneurs. Notez que tous les conteneurs du Pod ne recevront pas le signal TERM en même temps et il peut être nécessaire de définir des <code>preStop</code> hook si l'ordre d'arrêt est important.</li></ol></li><li>(en même temps que 3) Le Pod est supprimé des listes d'endpoints des services, et n'est plus considéré comme faisant partie des pods en cours d'exécution pour les contrôleurs de réplication. Les Pods s'arrêtant lentement ne peuvent pas continuer à servir du trafic, les load balancers (comme le service proxy) les supprimant de leurs rotations.</li><li>Lorsque la période de grâce expire, les processus s'exécutant toujours dans le Pod sont tués avec SIGKILL.</li><li>Kubelet va supprimer le Pod dans l'API server en indiquant une période de grâce de 0 (suppression immédiate). Le Pod disparaît de l'API et n'est plus visible par le client.</li></ol><p>Par défaut, toutes les suppressions ont une période de grâce de 30 secondes. La commande <code>kubectl delete</code> prend en charge l'option <code>--grace-period=&lt;secondes></code> permettant à l'utilisateur de spécifier sa propre valeur. La valeur <code>0</code> <a href=/docs/concepts/workloads/pods/pod/#force-deletion-of-pods>force la suppression</a> du pod. Avec kubectl version >= 1.5, vous devez spécifier un flag supplémentaire <code>--force</code> avec <code>--grace-period=0</code> pour pouvoir forcer la suppression.</p><h3 id=suppression-forcée-de-pods>Suppression forcée de pods</h3><p>La suppression forcée d'un pod est définie comme la suppression immédiate d'un pod de l'état du cluster et d'etcd. Lorqu'une suppression forcée est effectuée, l'apiserver n'attend pas la confirmation de kubelet que le pod a été terminé sur le nœud sur lequel il s'exécutait. Il supprime le pod de l'API immédiatement pour qu'un nouveau pod puisse être créé avec le même nom. Sur le nœud, les pods devant se terminer immédiatement se verront donner une courte période de grâce avant d'être tués de force.</p><p>Les suppressions forcées peuvent être potentiellement dangereuses pour certains pods et doivent être effectuées avec précaution. Dans le cas de pods d'un StatefulSet, veuillez vous référer à la documentation pour <a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>supprimer des Pods d'un StatefulSet</a>.</p><h2 id=mode-privilégié-pour-les-conteneurs-d-un-pod>Mode privilégié pour les conteneurs d'un pod</h2><p>Depuis Kubernetes v1.1, tout conteneur d'un pod peut activer le mode privilégié, en utilisant le flag <code>privileged</code> du <code>SecurityContext</code> de la spec du conteneur. Ceci est utile pour les conteneurs voulant utiliser les capacités de Linux comme manipuler la pile réseau ou accéder aux périphériques. Les processus dans un tel conteneur ont pratiquement les mêmes privilèges que les processus en dehors d'un conteneur. En mode privilégié, il doit être plus facile d'écrire des plugins réseau et volume en tant que pods séparés ne devant pas être compilés dans kubelet.</p><p>Si le master exécute Kubernetes v1.1 ou supérieur, et les nœuds exécutent une version antérieure à v1.1, les nouveaux pods privilégiés seront acceptés par l'api-server, mais ne seront pas lancés. Il resteront en état "pending".
Si l'utilisateur appelle <code>kubectl describe pod FooPodName</code>, l'utilisateur peut voir la raison pour laquelle le pod est en état "pending". La table d'événements dans la sortie de la commande "describe" indiquera :
<code>Error validating pod "FooPodName"."FooPodNamespace" from api, ignoring: spec.containers[0].securityContext.privileged: forbidden '&lt;*>(0xc2089d3248)true'</code></p><p>Si le master exécute une version antérieure à v1.1, les pods privilégiés ne peuvent alors pas être créés. Si l'utilisateur tente de créer un pod ayant un conteneur privilégié, l'utilisateur obtiendra l'erreur suivante :
<code>The Pod "FooPodName" is invalid. spec.containers[0].securityContext.privileged: forbidden '&lt;*>(0xc20b222db0)true'</code></p><h2 id=objet-de-l-api>Objet de l'API</h2><p>Le Pod est une ressource au plus haut niveau dans l'API REST Kubernetes. Plus de détails sur l'objet de l'API peuvent être trouvés à :
<a href=/docs/reference/generated/kubernetes-api/v1.25/#pod-v1-core>Objet de l'API Pod</a>.</p><p>Lorsque vous créez un manifest pour un objet Pod, soyez certain que le nom spécifié est un <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nom de sous-domaine DNS</a> valide.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c3c2b9cf30915ec9d46c147201da3332>1.3 - Cycle de vie d'un Pod</h1><p>Cette page décrit le cycle de vie d'un Pod.</p><h2 id=phase-du-pod>Phase du Pod</h2><p>Le champ <code>status</code> d'un Pod est un objet
<a href=/docs/reference/generated/kubernetes-api/v1.25/#podstatus-v1-core>PodStatus</a>,
contenant un champ <code>phase</code>.</p><p>La phase d'un Pod est un résumé simple et de haut niveau de l'étape à laquelle le Pod se trouve
dans son cycle de vie.
La phase n'est pas faite pour être un cumul complet d'observations de l'état
du conteneur ou du Pod, ni pour être une machine à état compréhensible.</p><p>Le nombre et la signification des valeurs de phase d'un pod sont soigneusement gardés.
Hormis ce qui est documenté ici, rien ne doit être supposé sur des Pods
ayant une valeur de <code>phase</code> donnée.</p><p>Voici les valeurs possibles pour <code>phase</code> :</p><table><thead><tr><th style=text-align:left>Valeur</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>Pending</code></td><td style=text-align:left>Le Pod a été accepté par Kubernetes, mais une ou plusieurs images de conteneurs n'ont pas encore été créées. Ceci inclut le temps avant d'être affecté ainsi que le temps à télécharger les images à travers le réseau, ce qui peut prendre un certain temps.</td></tr><tr><td style=text-align:left><code>Running</code></td><td style=text-align:left>Le pod a été affecté à un nœud et tous les conteneurs ont été créés. Au moins un conteneur est toujours en cours d'exécution, ou est en train de démarrer ou redémarrer.</td></tr><tr><td style=text-align:left><code>Succeeded</code></td><td style=text-align:left>Tous les conteneurs du pod ont terminé avec succès et ne seront pas redémarrés.</td></tr><tr><td style=text-align:left><code>Failed</code></td><td style=text-align:left>Tous les conteneurs d'un pod ont terminé, et au moins un conteneur a terminé en échec : soit le conteneur a terminé avec un status non zéro, soit il a été arrêté par le système.</td></tr><tr><td style=text-align:left><code>Unknown</code></td><td style=text-align:left>Pour quelque raison l'état du pod ne peut pas être obtenu, en général en cas d'erreur de communication avec l'hôte du Pod.</td></tr></tbody></table><h2 id=conditions-du-pod>Conditions du Pod</h2><p>Un Pod a un PodStatus, qui contient un tableau de
<a href=/docs/reference/generated/kubernetes-api/v1.25/#podcondition-v1-core>PodConditions</a>
à travers lesquelles le Pod est ou non passé. Chaque élément
du tableau de PodCondition a six champs possibles :</p><ul><li><p>Le champ <code>lastProbeTime</code> fournit un timestamp auquel la condition du Pod
a été sondée pour la dernière fois.</p></li><li><p>Le champ <code>lastTransitionTime</code> fournit un timestamp auquel le Pod a changé de statut
pour la dernière fois.</p></li><li><p>Le champ <code>message</code> est un message lisible indiquant les détails de la transition.</p></li><li><p>Le champ <code>reason</code> est une raison unique, en un seul mot et en CamelCase de la transition
vers la dernière condition.</p></li><li><p>Le champ <code>status</code> est une chaîne de caractères avec les valeurs possibles "<code>True</code>", "<code>False</code>", et "<code>Unknown</code>".</p></li><li><p>Le champ <code>type</code> est une chaîne de caractères ayant une des valeurs suivantes :</p><ul><li><code>PodScheduled</code> : le Pod a été affecté à un nœud ;</li><li><code>Ready</code> : le Pod est prêt à servir des requêtes et doit être rajouté aux équilibreurs
de charge de tous les Services correspondants ;</li><li><code>Initialized</code> : tous les <a href=/fr/docs/concepts/workloads/pods/init-containers>init containers</a>
ont démarré correctement ;</li><li><code>ContainersReady</code> : tous les conteneurs du Pod sont prêts.</li></ul></li></ul><h2 id=sondes-du-conteneur>Sondes du Conteneur</h2><p>Une <a href=/docs/reference/generated/kubernetes-api/v1.25/#probe-v1-core>Sonde</a> (Probe) est un diagnostic
exécuté périodiquement par <a href=/docs/admin/kubelet/>kubelet</a>
sur un Conteneur. Pour exécuter un diagnostic, kubelet appelle un
<a href=https://godoc.org/k8s.io/kubernetes/pkg/api/v1#Handler>Handler</a> implémenté par
le Conteneur. Il existe trois types de handlers :</p><ul><li><p><a href=/docs/reference/generated/kubernetes-api/v1.25/#execaction-v1-core>ExecAction</a>:
Exécute la commande spécifiée à l'intérieur du Conteneur. Le diagnostic
est considéré réussi si la commande se termine avec un code de retour de 0.</p></li><li><p><a href=/docs/reference/generated/kubernetes-api/v1.25/#tcpsocketaction-v1-core>TCPSocketAction</a>:
Exécute un contrôle TCP sur l'adresse IP du Conteneur et sur un port spécifié.
Le diagnostic est considéré réussi si le port est ouvert.</p></li><li><p><a href=/docs/reference/generated/kubernetes-api/v1.25/#httpgetaction-v1-core>HTTPGetAction</a>:
Exécute une requête HTTP Get sur l'adresse IP du Conteneur et sur un port et
un chemin spécifiés. Le diagnostic est considéré réussi si la réponse a un code
de retour supérieur ou égal à 200 et inférieur à 400.</p></li></ul><p>Chaque sonde a un résultat parmi ces trois :</p><ul><li>Success: Le Conteneur a réussi le diagnostic.</li><li>Failure: Le Conteneur a échoué au diagnostic.</li><li>Unknown: L'exécution du diagnostic a échoué, et donc aucune action ne peut être prise.</li></ul><p>kubelet peut optionnellement exécuter et réagir à trois types de sondes sur des conteneurs
en cours d'exécution :</p><ul><li><p><code>livenessProbe</code> : Indique si le Conteneur est en cours d'exécution. Si
la liveness probe échoue, kubelet tue le Conteneur et le Conteneur
est soumis à sa <a href=#politique-de-redemarrage>politique de redémarrage</a> (restart policy).
Si un Conteneur ne fournit pas de liveness probe, l'état par défaut est <code>Success</code>.</p></li><li><p><code>readinessProbe</code> : Indique si le Conteneur est prêt à servir des requêtes.
Si la readiness probe échoue, le contrôleur de points de terminaison (Endpoints)
retire l'adresse IP du Pod des points de terminaison de tous les Services
correspodant au Pod. L'état par défaut avant le délai initial est
<code>Failure</code>. Si le Conteneur ne fournit pas de readiness probe, l'état par
défaut est <code>Success</code>.</p></li><li><p><code>startupProbe</code>: Indique si l'application à l'intérieur du conteneur a démarré.
Toutes les autres probes sont désactivées si une starup probe est fournie,
jusqu'à ce qu'elle réponde avec succès. Si la startup probe échoue, le kubelet
tue le conteneur, et le conteneur est assujetti à sa <a href=#politique-de-redemarrage>politique de redémarrage</a>.
Si un conteneur ne fournit pas de startup probe, l'état par défaut est <code>Success</code>.</p></li></ul><h3 id=quand-devez-vous-utiliser-une-liveness-probe>Quand devez-vous utiliser une liveness probe ?</h3><p>Si le process de votre Conteneur est capable de crasher de lui-même lorsqu'il
rencontre un problème ou devient inopérant, vous n'avez pas forcément besoin
d'une liveness probe ; kubelet va automatiquement exécuter l'action correcte
en accord avec la politique de redémarrage (<code>restartPolicy</code>) du Pod.</p><p>Si vous désirez que votre Conteneur soit tué et redémarré si une sonde échoue, alors
spécifiez une liveness probe et indiquez une valeur pour <code>restartPolicy</code> à Always
ou OnFailure.</p><h3 id=quand-devez-vous-utiliser-une-readiness-probe>Quand devez-vous utiliser une readiness probe ?</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.0 [stable]</code></div><p>Si vous voulez commencer à envoyer du trafic à un Pod seulement lorsqu'une sonde
réussit, spécifiez une readiness probe. Dans ce cas, la readiness probe peut être
la même que la liveness probe, mais l'existence de la readiness probe dans la spec
veut dire que le Pod va démarrer sans recevoir aucun trafic et va commencer
à recevoir du trafic après que la sonde réussisse.
Si votre Conteneur doit charger une grande quantité de données, des fichiers de
configuration ou exécuter des migrations au démarrage, spécifiez une readiness probe.</p><p>Si vous désirez que le Conteneur soit capable de se mettre en maintenance tout seul, vous
pouvez spécifier une readiness probe qui vérifie un point de terminaison spécifique au
readiness et différent de la liveness probe.</p><p>Notez que si vous voulez uniquement être capable de dérouter les requêtes lorsque
le Pod est supprimé, vous n'avez pas forcément besoin d'une readiness probe; lors
de sa suppression, le Pod se met automatiquement dans un état non prêt, que la
readiness probe existe ou non.
Le Pod reste dans le statut non prêt le temps que les Conteneurs du Pod s'arrêtent.</p><h3 id=quand-devez-vous-utiliser-une-startup-probe>Quand devez-vous utiliser une startup probe ?</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.16 [alpha]</code></div><p>Si votre conteneur démarre habituellement en plus de <code>initialDelaySeconds + failureThreshold × periodSeconds</code>,
vous devriez spécifier une startup probe qui vérifie le même point de terminaison que la liveness probe. La valeur par défaut pour <code>periodSeconds</code> est 30s.
Vous devriez alors mettre sa valeur <code>failureThreshold</code> suffisamment haute pour permettre au conteneur de démarrer, sans changer les valeurs par défaut de la liveness probe. Ceci aide à se protéger de deadlocks.</p><p>Pour plus d'informations sur la manière de mettre en place une liveness, readiness ou startup probe,
voir <a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/>Configurer des Liveness, Readiness et Startup Probes</a>.</p><h2 id=statut-d-un-pod-et-d-un-conteneur>Statut d'un Pod et d'un Conteneur</h2><p>Pour des informations détaillées sur le statut d'un Pod et d'un Conteneur, voir
<a href=/docs/reference/generated/kubernetes-api/v1.25/#podstatus-v1-core>PodStatus</a>
et
<a href=/docs/reference/generated/kubernetes-api/v1.25/#containerstatus-v1-core>ContainerStatus</a>.
Notez que l'information rapportée comme statut d'un Pod dépend du
<a href=/docs/reference/generated/kubernetes-api/v1.25/#containerstatus-v1-core>ContainerState</a> actuel.</p><h2 id=états-d-un-conteneur>États d'un Conteneur</h2><p>Une fois que le Pod est assigné à un nœud par le scheduler, kubelet commence
à créer les conteneurs en utilisant le runtime de conteneurs. Il existe trois états possibles
pour les conteneurs : en attente (Waiting), en cours d'exécution (Running) et terminé (Terminated). Pour vérifier l'état d'un conteneur, vous pouvez utiliser <code>kubectl describe pod [POD_NAME]</code>. L'état est affiché pour chaque conteneur du Pod.</p><ul><li><p><code>Waiting</code> : état du conteneur par défaut. Si le conteneur n'est pas dans un état Running ou Terminated, il est dans l'état Waiting. Un conteneur dans l'état Waiting exécute
les opérations nécessaires, comme télécharger les images, appliquer des Secrets, etc. À côté
de cet état, un message et une raison sur l'état sont affichés pour vous fournir plus
d'informations.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>State</span>:<span style=color:#bbb>          </span>Waiting<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>Reason</span>:<span style=color:#bbb>       </span>ErrImagePull<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div></li><li><p><code>Running</code> : Indique que le conteneur s'exécute sans problème. Le hook <code>postStart</code> (s'il existe) est exécuté avant que le conteneur entre dans l'état Running. Cet état affiche aussi le moment auquel le conteneur est entré dans l'état Running.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>State</span>:<span style=color:#bbb>          </span>Running<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>Started</span>:<span style=color:#bbb>      </span>Wed, 30 Jan 2019 16:46:38 +0530<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div></li><li><p><code>Terminated</code>: Indique que le conteneur a terminé son exécution et s'est arrêté.
Un conteneur entre dans cet état lorsqu'il s'est exécuté avec succès ou lorsqu'il a
échoué pour une raison quelconque. De plus, une raison et un code de retour sont affichés,
ainsi que les moments de démarrage et d'arrêt du conteneur. Avant qu'un conteneur entre
dans l'état Terminated, le hook <code>preStop</code> est exécuté (s'il existe).</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>State</span>:<span style=color:#bbb>          </span>Terminated<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>Reason</span>:<span style=color:#bbb>       </span>Completed<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>Exit Code</span>:<span style=color:#bbb>    </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>Started</span>:<span style=color:#bbb>      </span>Wed, 30 Jan 2019 11:45:26 +0530<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>Finished</span>:<span style=color:#bbb>     </span>Wed, 30 Jan 2019 11:45:26 +0530<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span>...<span style=color:#bbb>
</span></span></span></code></pre></div></li></ul><h2 id=pod-readiness-gate>Pod readiness</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><p>Votre application peut injecter des données dans <code>PodStatus</code>.</p><p><em>Pod readiness</em>. Pour utiliser cette fonctionnalité, remplissez <code>readinessGates</code> dans le PodSpec avec
une liste de conditions supplémentaires que le kubelet évalue pour la disponibilité du Pod.</p><p>Les Readiness gates sont déterminées par l'état courant des champs <code>status.condition</code> du Pod.
Si Kubernetes ne peut pas trouver une telle condition dans le champs <code>status.conditions</code> d'un Pod, the statut de la condition
est mise par défaut à "<code>False</code>".</p><p>Voici un exemple :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>readinessGates</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>conditionType</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;www.example.com/feature-1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>conditions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Ready <span style=color:#bbb> </span><span style=color:#080;font-style:italic># une PodCondition intégrée</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;False&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastProbeTime</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>null</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span>2018-01-01T00:00:00Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;www.example.com/feature-1&#34;</span><span style=color:#bbb>   </span><span style=color:#080;font-style:italic># une PodCondition supplémentaire</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;False&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastProbeTime</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>null</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span>2018-01-01T00:00:00Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containerStatuses</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>containerID</span>:<span style=color:#bbb> </span>docker://abcd...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ready</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Les conditions du Pod que vous ajoutez doivent avoir des noms qui sont conformes au <a href=/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set>format des étiquettes</a> de Kubernetes.</p><h3 id=statut-pod-disponibilité>Statut de la disponibilité d'un Pod</h3><p>La commande <code>kubectl patch</code> ne peut pas patcher le statut d'un objet.
Pour renseigner ces <code>status.conditions</code> pour le pod, les applications et
<a class=glossary-tooltip title='A specialized controller used to manage a custom resource' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/operator/ target=_blank aria-label=operators>operators</a> doivent utiliser l'action <code>PATCH</code>.
Vous pouvez utiliser une <a href=/docs/reference/using-api/client-libraries/>bibliothèque client Kubernetes</a> pour
écrire du code qui renseigne les conditions particulières pour la disponibilité dun Pod.</p><p>Pour un Pod utilisant des conditions particulières, ce Pod est considéré prêt <strong>seulement</strong>
lorsque les deux déclarations ci-dessous sont vraies :</p><ul><li>Tous les conteneurs du Pod sont prêts.</li><li>Toutes les conditions spécifiées dans <code>ReadinessGates</code> sont <code>True</code>.</li></ul><p>Lorsque les conteneurs d'un Pod sont prêts mais qu'au moins une condition particulière
est manquante ou <code>False</code>, le kubelet renseigne la condition du Pod à <code>ContainersReady</code>.</p><h2 id=politique-de-redémarrage>Politique de redémarrage</h2><p>La structure PodSpec a un champ <code>restartPolicy</code> avec comme valeur possible
Always, OnFailure et Never. La valeur par défaut est Always.
<code>restartPolicy</code> s'applique à tous les Conteneurs du Pod. <code>restartPolicy</code> s'applique
seulement aux redémarrages des Conteneurs par kubelet sur le même nœud. Des conteneurs
terminés qui sont redémarrés par kubelet sont redémarrés avec un délai exponentiel
(10s, 20s, 40s ...) plafonné à cinq minutes, qui est réinitialisé après dix minutes
d'exécution normale. Comme discuté dans le
<a href=/docs/user-guide/pods/#durability-of-pods-or-lack-thereof>document sur les Pods</a>,
une fois attaché à un nœud, un Pod ne sera jamais rattaché à un autre nœud.</p><h2 id=durée-de-vie-d-un-pod>Durée de vie d'un Pod</h2><p>En général, les Pods restent jusqu'à ce qu'un humain ou un process de
<a class=glossary-tooltip title="Boucle de contrôle surveillant l'état partagé du cluster à travers l'apiserver et effectuant des changements en essayant de déplacer l'état actuel vers l'état désiré." data-toggle=tooltip data-placement=top href=/docs/admin/kube-controller-manager/ target=_blank aria-label=contrôleur>contrôleur</a> les supprime explicitement.</p><p>Le plan de contrôle nettoie les Pods terminés (avec une phase à <code>Succeeded</code> ou
<code>Failed</code>), lorsque le nombre de Pods excède le seuil configuré
(determiné par <code>terminated-pod-gc-threshold</code> dans le kube-controller-manager).
Ceci empêche une fuite de ressources lorsque les Pods sont créés et supprimés au fil du temps.</p><p>Il y a différents types de ressources pour créer des Pods :</p><ul><li><p>Utilisez un <a class=glossary-tooltip title='Objet API gérant une application répliquée.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Déploiement>Déploiement</a>,
<a class=glossary-tooltip title='ReplicaSet ensures that a specified number of Pod replicas are running at one time' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/replicaset/ target=_blank aria-label=ReplicaSet>ReplicaSet</a> ou <a class=glossary-tooltip title="Gère le déploiement et la mise à l'échelle d'un ensemble de Pods, avec un stockage durable et des identifiants persistants pour chaque Pod." data-toggle=tooltip data-placement=top href=/fr/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>
pour les Pods qui ne sont pas censés terminer, par exemple des serveurs web.</p></li><li><p>Utilisez un <a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Job>Job</a>
pour les Pods qui sont censés se terminer une fois leur tâche accomplie. Les Jobs sont appropriés
seulement pour des Pods ayant <code>restartPolicy</code> égal à OnFailure ou Never.</p></li><li><p>Utilisez un <a class=glossary-tooltip title="S'assure qu'une copie d'un Pod s'exécute sur un ensemble de nœuds d'un cluster." data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a>
pour les Pods qui doivent s'exécuter sur chaque noeud éligible.</p></li></ul><p>Toutes les ressources de charges de travail contiennent une PodSpec. Il est recommandé de créer
la ressource de charges de travail appropriée et laisser le contrôleur de la ressource créer les Pods
pour vous, plutôt que de créer directement les Pods vous-même.</p><p>Si un nœud meurt ou est déconnecté du reste du cluster, Kubernetes applique
une politique pour mettre la <code>phase</code> de tous les Pods du nœud perdu à Failed.</p><h2 id=exemples>Exemples</h2><h3 id=exemple-avancé-de-liveness-probe>Exemple avancé de liveness probe</h3><p>Les Liveness probes sont exécutées par kubelet, toutes les requêtes sont donc faites
dans l'espace réseau de kubelet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>test</span>:<span style=color:#bbb> </span>liveness<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>liveness-http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/liveness<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>livenessProbe</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>httpGet</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># lorsque &#34;host&#34; n&#39;est pas défini, &#34;PodIP&#34; sera utilisé</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># host: my-host</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># lorsque &#34;scheme&#34; n&#39;est pas défini, &#34;HTTP&#34; sera utilisé. &#34;HTTP&#34; et &#34;HTTPS&#34; sont les seules valeurs possibles</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># scheme: HTTPS</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/healthz<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>8080</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>httpHeaders</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>X-Custom-Header<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>Awesome<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>initialDelaySeconds</span>:<span style=color:#bbb> </span><span style=color:#666>15</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>timeoutSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>liveness<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=exemples-d-états>Exemples d'états</h3><ul><li><p>Un Pod est en cours d'exécution et a un Conteneur. Le conteneur se termine avec succès.</p><ul><li>Écriture d'un événement de complétion.</li><li>Si <code>restartPolicy</code> est :<ul><li>Always : Redémarrage du Conteneur ; la <code>phase</code> du Pod reste à Running.</li><li>OnFailure : la <code>phase</code> du Pod passe à Succeeded.</li><li>Never : la <code>phase</code> du Pod passe à Succeeded.</li></ul></li></ul></li><li><p>Un Pod est en cours d'exécution et a un Conteneur. Le conteneur se termine en erreur.</p><ul><li>Écriture d'un événement d'échec.</li><li>Si <code>restartPolicy</code> est :<ul><li>Always : Redémarrage du Conteneur ; la <code>phase</code> du Pod reste à Running.</li><li>OnFailure : Redémarrage du Conteneur ; la <code>phase</code> du Pod reste à Running.</li><li>Never : la <code>phase</code> du Pod passe à Failed.</li></ul></li></ul></li><li><p>Un Pod est en cours d'exécution et a deux Conteneurs. Le conteneur 1 termine en erreur.</p><ul><li>Écriture d'un événement d'échec.</li><li>Si <code>restartPolicy</code> est :<ul><li>Always : Redémarrage du Conteneur ; la <code>phase</code> du Pod reste à Running.</li><li>OnFailure : Redémarrage du Conteneur ; la <code>phase</code> du Pod reste à Running.</li><li>Never : Le Conteneur n'est pas redémarré ; la <code>phase</code> du Pod reste à Running.</li></ul></li><li>Si Container 1 est arrêté, et Conteneur 2 se termine :<ul><li>Écriture d'un événement d'échec.</li><li>Si <code>restartPolicy</code> est :<ul><li>Always : Redémarrage du Conteneur ; la <code>phase</code> du Pod reste à Running.</li><li>OnFailure : Redémarrage du Conteneur ; la <code>phase</code> du Pod reste à Running.</li><li>Never : la <code>phase</code> du Pod passe à Failed.</li></ul></li></ul></li></ul></li><li><p>Un Pod est en cours d'exécution et a un Conteneur. Le Conteneur n'a plus assez de mémoire.</p><ul><li>Le Conteneur se termine en erreur.</li><li>Écriture d'un événement OOM.</li><li>Si <code>restartPolicy</code> est :<ul><li>Always : Redémarrage du Conteneur ; la <code>phase</code> du Pod reste à Running.</li><li>OnFailure : Redémarrage du Conteneur ; la <code>phase</code> du Pod reste à Running.</li><li>Never : Écriture d'un événement d'erreur ; la <code>phase</code> du Pod passe à Failed.</li></ul></li></ul></li><li><p>Le Pod est en cours d'exécution, et un disque meurt.</p><ul><li>Tous les conteneurs sont tués.</li><li>Écriture d'un événement approprié.</li><li>La <code>phase</code> du Pod devient Failed.</li><li>Si le Pod s'exécute sous un contrôleur, le Pod est recréé ailleurs.</li></ul></li><li><p>Le Pod est en cours d'exécution et son nœud est segmenté.</p><ul><li>Le contrôleur de Nœud attend un certain temps.</li><li>Le contrôleur de Nœud passe la <code>phase</code> du Pod à Failed.</li><li>Si le Pod s'exécute sous un contrôleur, le Pod est recréé ailleurs.</li></ul></li></ul><h2 id=a-suivre>A suivre</h2><ul><li><p>Apprenez par la pratique
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>attacher des handlers à des événements de cycle de vie d'un conteneur</a>.</p></li><li><p>Apprenez par la pratique
<a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/>configurer des liveness, readiness et startup probes</a>.</p></li><li><p>En apprendre plus sur les <a href=/docs/concepts/containers/container-lifecycle-hooks/>hooks de cycle de vie d'un Conteneur</a>.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c8d62295ca703fdcef1aaf89fb4c916a>1.4 - Contraintes de propagation de topologie pour les Pods</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code></div><p>Vous pouvez utiliser des <em>contraintes de propagation de topologie</em> pour contrôler comment les <a class=glossary-tooltip title='Le plus petit et le plus simple des objets Kubernetes. Un Pod est un ensemble de conteneurs fonctionnant sur votre cluster.' data-toggle=tooltip data-placement=top href=/fr/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a> sont propagés à travers votre cluster parmi les domaines de défaillance comme les régions, zones, noeuds et autres domaines de topologie définis par l'utilisateur. Ceci peut aider à mettre en place de la haute disponibilité et à utiliser efficacement les ressources.</p><h2 id=conditions-préalables>Conditions préalables</h2><h3 id=autoriser-la-feature-gate>Autoriser la Feature Gate</h3><p>La <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> <code>EvenPodsSpread</code> doit être autorisée pour
<a class=glossary-tooltip title="Composant sur le master qui expose l'API Kubernetes. Il s'agit du front-end pour le plan de contrôle Kubernetes." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-apiserver/ target=_blank aria-label="l'API Server">l'API Server</a> <strong>et</strong> le
<a class=glossary-tooltip title="Composant sur le master qui surveille les pods nouvellement créés qui ne sont pas assignés à un nœud et sélectionne un nœud sur lequel ils vont s'exécuter." data-toggle=tooltip data-placement=top href=/docs/reference/generated/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a>.</p><h3 id=labels-de-noeuds>Labels de noeuds</h3><p>Les contraintes de propagation de topologie reposent sur les labels de noeuds pour identifier le ou les domaines de topologie dans lesquels se trouve chacun des noeuds. Par exemple, un noeud pourrait avoir les labels: <code>node=node1,zone=us-east-1a,region=us-east-1</code></p><p>Supposons que vous ayez un cluster de 4 noeuds ayant les labels suivants:</p><pre tabindex=0><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre><p>Une vue logique du cluster est celle-ci :</p><pre tabindex=0><code>+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
</code></pre><p>Plutôt que d'appliquer des labels manuellement, vous pouvez aussi réutiliser les <a href=/docs/reference/kubernetes-api/labels-annotations-taints/>labels réputés</a> qui sont créés et renseignés automatiquement dans la plupart des clusters.</p><h2 id=contraintes-de-propagation-pour-les-pods>Contraintes de propagation pour les Pods</h2><h3 id=api>API</h3><p>Le champ <code>pod.spec.topologySpreadConstraints</code> est introduit dans 1.16 comme suit :</p><pre tabindex=0><code>apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  topologySpreadConstraints:
    - maxSkew: &lt;integer&gt;
      minDomains: &lt;integer&gt;
      topologyKey: &lt;string&gt;
      whenUnsatisfiable: &lt;string&gt;
      labelSelector: &lt;object&gt;
</code></pre><p>Vous pouvez définir une ou plusieurs <code>topologySpreadConstraint</code> pour indiquer au kube-scheduler comment placer chaque nouveau Pod par rapport aux Pods déjà existants dans votre cluster. Les champs sont :</p><ul><li><strong>maxSkew</strong> décrit le degré avec lequel les Pods peuvent être inégalement distribués. C'est la différence maximale permise entre le nombre de Pods correspondants entre deux quelconques domaines de topologie d'un type donné. Il doit être supérieur à zéro.</li><li><strong>topologyKey</strong> est la clé des labels de noeuds. Si deux noeuds sont étiquettés avec cette clé et ont des valeurs égales pour ce label, le scheduler considère les deux noeuds dans la même topologie. Le scheduler essaie de placer un nombre équilibré de Pods dans chaque domaine de topologie.</li><li><strong>whenUnsatisfiable</strong> indique comment traiter un Pod qui ne satisfait pas les contraintes de propagation :<ul><li><code>DoNotSchedule</code> (défaut) indique au scheduler de ne pas le programmer.</li><li><code>ScheduleAnyway</code> indique au scheduler de le programmer, tout en priorisant les noeuds minimisant le biais (<em>skew</em>).</li></ul></li><li><strong>labelSelector</strong> est utilisé pour touver les Pods correspondants. Les Pods correspondants à ce sélecteur de labels sont comptés pour déterminer le nombre de Pods dans leurs domaines de topologie correspodants. Voir <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>Sélecteurs de labels</a> pour plus de détails.</li></ul><p>Vous pouvez en savoir plus sur ces champ en exécutant <code>kubectl explain Pod.spec.topologySpreadConstraints</code>.</p><h3 id=exemple-une-topologyspreadconstraint>Exemple : Une TopologySpreadConstraint</h3><p>Supposons que vous ayez un cluster de 4 noeuds où 3 Pods étiquettés <code>foo:bar</code> sont placés sur node1, node2 et node3 respectivement (<code>P</code> représente un Pod) :</p><pre tabindex=0><code>+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
</code></pre><p>Si nous voulons qu'un nouveau Pod soit uniformément réparti avec les Pods existants à travers les zones, la spec peut être :</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/fr/examples/pods/topology-spread-constraints/one-constraint.yaml download=pods/topology-spread-constraints/one-constraint.yaml><code>pods/topology-spread-constraints/one-constraint.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-one-constraint-yaml")' title="Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-one-constraint-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:3.1<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p><code>topologyKey: zone</code> implique que la distribution uniforme sera uniquement appliquée pour les noeuds ayant le label "zone:&lt;any value>" présent. <code>whenUnsatisfiable: DoNotSchedule</code> indique au scheduler de laisser le Pod dans l'état Pending si le Pod entrant ne peut pas satisfaire la contrainte.</p><p>Si le scheduler plaçait ce Pod entrant dans "zoneA", la distribution des Pods deviendrait [3, 1], et le biais serait de 2 (3 - 1) - ce qui va à l'encontre de <code>maxSkew: 1</code>. Dans cet exemple, le Pod entrant peut uniquement être placé dans "zoneB":</p><pre tabindex=0><code>+---------------+---------------+      +---------------+---------------+
|     zoneA     |     zoneB     |      |     zoneA     |     zoneB     |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |  OR  | node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
|   P   |   P   |   P   |   P   |      |   P   |   P   |  P P  |       |
+-------+-------+-------+-------+      +-------+-------+-------+-------+
</code></pre><p>Vous pouvez ajuster la spec du Pod pour pour répondre à divers types d'exigences :</p><ul><li>Changez <code>maxSkew</code> pour une valeur plus grande comme "2" pour que le Pod entrant puisse aussi être placé dans la "zoneA".</li><li>Changez <code>topologyKey</code> pour "node" pour distribuer les Pods uniformément à travers les noeuds et non plus les zones. Dans l'exemple ci-dessus, si <code>maxSkew</code> reste à "1", le Pod entrant peut être uniquement placé dans "node4".</li><li>Changez <code>whenUnsatisfiable: DoNotSchedule</code> en <code>whenUnsatisfiable: ScheduleAnyway</code> pour s'assurer que le Pod est toujours programmable (en supposant que les autres APIs de scheduling soient satisfaites). Cependant, il sera de préférence placé dans la topologie de domaine ayant le moins de Pods correspondants. (Prenez note que cette préférence est normalisée conjointement avec d'autres priorités de scheduling interne comme le ratio d'usage de ressources, etc.)</li></ul><h3 id=example-plusieurs-topologyspreadconstraints>Example: Plusieurs TopologySpreadConstraints</h3><p>Cela s'appuie sur l'exemple précédent. Supposons que vous ayez un cluster de 4 noeuds où 3 Pods étiquetés <code>foo:bar</code> sont placés sur node1, node2 et node3 respectivement (<code>P</code> représente un Pod):</p><pre tabindex=0><code>+---------------+---------------+
|     zoneA     |     zoneB     |
+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 |
+-------+-------+-------+-------+
|   P   |   P   |   P   |       |
+-------+-------+-------+-------+
</code></pre><p>Vous pouvez utiliser 2 TopologySpreadConstraints pour contrôler la répartition des Pods aussi bien dans les zones que dans les noeuds :</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/fr/examples/pods/topology-spread-constraints/two-constraints.yaml download=pods/topology-spread-constraints/two-constraints.yaml><code>pods/topology-spread-constraints/two-constraints.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-two-constraints-yaml")' title="Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-two-constraints-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>node<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:3.1<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Dans ce cas, pour satisfaire la première contrainte, le Pod entrant peut uniquement être placé dans "zoneB" ; alors que pour satisfaire la seconde contrainte, le Pod entrant peut uniquement être placé dans "node4". Le résultat étant l'intersection des résultats des 2 contraintes, l'unique option possible est de placer le Pod entrant dans "node4".</p><p>Plusieurs contraintes peuvent entraîner des conflits. Supposons que vous ayez un cluster de 3 noeuds couvrant 2 zones :</p><pre tabindex=0><code>+---------------+-------+
|     zoneA     | zoneB |
+-------+-------+-------+
| node1 | node2 | node3 |
+-------+-------+-------+
|  P P  |   P   |  P P  |
+-------+-------+-------+
</code></pre><p>Si vous appliquez "two-constraints.yaml" à ce cluster, vous noterez que "mypod" reste dans l'état <code>Pending</code>. Cela parce que : pour satisfaire la première contrainte, "mypod" peut uniquement être placé dans "zoneB"; alors que pour satisfaire la seconde contrainte, "mypod" peut uniquement être placé sur "node2". Ainsi, le résultat de l'intersection entre "zoneB" et "node2" ne retourne rien.</p><p>Pour surmonter cette situation, vous pouvez soit augmenter <code>maxSkew</code>, soit modifier une des contraintes pour qu'elle utilise <code>whenUnsatisfiable: ScheduleAnyway</code>.</p><h3 id=conventions>Conventions</h3><p>Il existe quelques conventions implicites qu'il est intéressant de noter ici :</p><ul><li><p>Seuls le Pods du même espace de noms que le Pod entrant peuvent être des candidats pour la correspondance.</p></li><li><p>Les noeuds sans label <code>topologySpreadConstraints[*].topologyKey</code> seront ignorés. Cela induit que :</p><ol><li>les Pods localisés sur ces noeuds n'impactent pas le calcul de <code>maxSkew</code> - dans l'exemple ci-dessus, supposons que "node1" n'a pas de label "zone", alors les 2 Pods ne seront pas comptés, et le Pod entrant sera placé dans "zoneA".</li><li>le Pod entrant n'a aucune chance d'être programmé sur ce type de noeuds - dans l'exemple ci-dessus, supposons qu'un "node5" portant un label <code>{zone-typo: zoneC}</code> joigne le cluster ; il sera ignoré, en raison de l'absence de label "zone".</li></ol></li><li><p>Faites attention à ce qui arrive lorsque le <code>topologySpreadConstraints[*].labelSelector</code> du Pod entrant ne correspond pas à ses propres labels. Dans l'exemple ci-dessus, si nous supprimons les labels du Pod entrant, il sera toujours placé dans "zoneB" car les contraintes sont toujours satisfaites. Cependant, après le placement, le degré de déséquilibre du cluster reste inchangé - zoneA contient toujours 2 Pods ayant le label {foo:bar}, et zoneB contient 1 Pod cayant le label {foo:bar}. Si ce n'est pas ce que vous attendez, nous recommandons que <code>topologySpreadConstraints[*].labelSelector</code> du workload corresponde à ses propres labels.</p></li><li><p>Si le Pod entrant a défini <code>spec.nodeSelector</code> ou <code>spec.affinity.nodeAffinity</code>, les noeuds non correspondants seront ignorés.</p><p>Supposons que vous ayez un cluster de 5 noeuds allant de zoneA à zoneC :</p><pre tabindex=0><code>+---------------+---------------+-------+
|     zoneA     |     zoneB     | zoneC |
+-------+-------+-------+-------+-------+
| node1 | node2 | node3 | node4 | node5 |
+-------+-------+-------+-------+-------+
|   P   |   P   |   P   |       |       |
+-------+-------+-------+-------+-------+
</code></pre><p>et vous savez que "zoneC" doit être exclue. Dans ce cas, vous pouvez écrire le yaml ci-dessous, pour que "mypod" soit placé dans "zoneB" plutôt que dans "zoneC". <code>spec.nodeSelector</code> est pris en compte de la même manière.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/fr/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml download=pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml")' title="Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>NotIn<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- zoneC<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
    </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/pause:3.1<span style=color:#bbb>
    </span></span></span></code></pre></div></div></div></li></ul><h3 id=contraintes-par-défaut-au-niveau-du-cluster>Contraintes par défaut au niveau du cluster</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [alpha]</code></div><p>Il est possible de définir des contraintes de propagation de topologie par défaut pour un cluster. Les contraintes de propagation de topologie sont appliquées à un Pod si et seulement si :</p><ul><li>Il ne définit aucune contrainte dans son <code>.spec.topologySpreadConstraints</code>.</li><li>Il appartient à un service, replication controller, replica set ou stateful set.</li></ul><p>Les contraintes par défaut peuvent être définies comme arguments du plugin <code>PodTopologySpread</code>
dans un <a href=/docs/reference/scheduling/profiles>profil de scheduling</a>.
Les contraintes sont spécifiées avec la même <a href=#api>API ci-dessus</a>, à l'exception que
<code>labelSelector</code> doit être vide. Les sélecteurs sont calculés à partir des services,
replication controllers, replica sets ou stateful sets auxquels le Pod appartient.</p><p>Un exemple de configuration pourrait ressembler à :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1alpha2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PodTopologySpread<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Le score produit par les contraintes de scheduling par défaut peuvent rentrer en conflit avec le score
produit par le <a href=/docs/reference/scheduling/profiles/#scheduling-plugins>plugin <code>DefaultPodTopologySpread</code></a>.
Il est recommandé de désactiver ce plugin dans le profil de scheduling lorsque vous utilisez des contraintes
par défaut pour <code>PodTopologySpread</code>.</div><h2 id=comparaison-avec-podaffinity-podantiaffinity>Comparaison avec PodAffinity/PodAntiAffinity</h2><p>Dans Kubernetes, les directives relatives aux "Affinités" contrôlent comment les Pods sont
programmés - plus regroupés ou plus dispersés.</p><ul><li>Pour <code>PodAffinity</code>, vous pouvez essayer de regrouper un certain nombre de Pods dans des domaines de topologie qualifiés,</li><li>Pour <code>PodAntiAffinity</code>, seulement un Pod peut être programmé dans un domaine de topologie unique.</li></ul><p>La fonctionnalité "EvenPodsSpread" fournit des options flexibles pour distribuer des Pods uniformément sur différents domaines de topologie - pour mettre en place de la haute disponibilité ou réduire les coûts. Cela peut aussi aider
au rolling update des charges de travail et à la mise à l'échelle de réplicas. Voir <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation>Motivations</a> pour plus de détails.</p><h2 id=limitations-connues>Limitations connues</h2><p>En version 1.18, pour laquelle cette fonctionnalité est en Beta, il y a quelques limitations connues :</p><ul><li>Réduire un Déploiement peut résulter en une distrubution désiquilibrée des Pods.</li><li>Les Pods correspondants sur des noeuds taintés sont respectés. Voir <a href=https://github.com/kubernetes/kubernetes/issues/80921>Issue 80921</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1ccbd4eeded6ab138d98b59175bd557e>1.5 - Init Containers</h1><p>Cette page fournit une vue d'ensemble des <em>conteneurs d'initialisation</em> (init containers) : des conteneurs spécialisés qui s'exécutent avant les conteneurs d'application dans un <a class=glossary-tooltip title='Le plus petit et le plus simple des objets Kubernetes. Un Pod est un ensemble de conteneurs fonctionnant sur votre cluster.' data-toggle=tooltip data-placement=top href=/fr/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a>.
Les init containers peuvent contenir des utilitaires ou des scripts d'installation qui ne sont pas présents dans une image d'application.</p><p>Vous pouvez spécifier des init containers dans la spécification du Pod à côté du tableau <code>containers</code> (qui décrit les conteneurs d'application)</p><h2 id=comprendre-les-init-containers>Comprendre les init containers</h2><p>Un <a class=glossary-tooltip title='Le plus petit et le plus simple des objets Kubernetes. Un Pod est un ensemble de conteneurs fonctionnant sur votre cluster.' data-toggle=tooltip data-placement=top href=/fr/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pod>Pod</a> peut avoir plusieurs conteneurs exécutant des applications mais peut aussi avoir un ou plusieurs init containers, qui sont exécutés avant que les conteneurs d'application ne démarrent.</p><p>Les init containers se comportent comme les conteneurs réguliers, avec quelques différences :</p><ul><li>Les init containers s'exécutent toujours jusqu'à la complétion.</li><li>Chaque init container doit se terminer avec succès avant que le prochain ne démarre.</li></ul><p>Si le init container d'un Pod échoue, Kubernetes redémarre le Pod à répétition jusqu'à ce que le init container se termine avec succès.
Cependant, si le Pod a une <code>restartPolicy</code> à "Never", Kubernetes ne redémarre pas le Pod.</p><p>Afin de spécifier un init container pour un Pod, il faut ajouter le champ <code>initContainers</code> dans la spécification du Pod, comme un
tableau d'objets de type <a href=/docs/reference/generated/kubernetes-api/v1.25/#container-v1-core>Container</a>, au même niveau que le tableau d'applications <code>containers</code>.
Le statut des init containers est retourné dans le champ <code>.status.initContainerStatuses</code>
comme un tableau des statuts du conteneur (comparable au champ <code>.status.containerStatuses</code>).</p><h3 id=différences-avec-les-conteneurs-réguliers>Différences avec les conteneurs réguliers</h3><p>Les init containers supportent tous les champs et fonctionnalités des conteneurs d'application
incluant les limites de ressources, les volumes et les paramètres de sécurité.
Cependant, les demandes de ressources pour un init container sont gérées différemment des
limites de ressources, tel que documenté dans <a href=#ressources>Ressources</a>.</p><p>De plus, les init containers ne supportent pas les readiness probes parce que ces conteneurs
s'exécutent jusqu'au bout avant que le Pod soit prêt.</p><p>Si l'on spécifie plusieurs init containers pour un Pod, Kubelet exécute chaque
init container de manière séquentielle.
Chaque init container doit se terminer avec succès avant que le prochain ne puisse s'exécuter.
Lorsque tous les init containers se sont exécutés jusqu'au bout, Kubelet initialise
les conteneurs d'application pour le Pod et les exécute comme d'habitude.</p><h2 id=utiliser-les-init-containers>Utiliser les init containers</h2><p>Puisque les init containers ont des images séparées des conteneurs d'application,
ils apportent certains avantages pour du code de mise en route :</p><ul><li>Les init containers peuvent contenir des utilitaires ou du code de configuration personnalisé
qui ne sont pas présents dans une image d'application.
Par exemple, il n'y a pas besoin de faire hériter une image d'une autre (<code>FROM</code>) seulement pour utiliser
un outil comme <code>sed</code>, <code>awk</code>, <code>python</code>, ou <code>dig</code> pendant l'installation.</li><li>Les init containers peuvent exécuter en toute sécurité des utilitaires qui rendraient moins sécurisée une image de conteneur d'application.</li><li>Les rôles "builder" et "deployer" d'une image d'application peuvent travailler indépendamment sans qu'il n'y ait besoin
de créer conjointement une seule image d'application.</li><li>Les init containers peuvent s'exécuter avec une vue du système de fichiers différente de celle des conteneurs d'application dans le même Pod. Par conséquent, on peut leur donner accès aux <a class=glossary-tooltip title='Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secrets>Secrets</a>, auxquels les conteneurs d'application n'ont pas accès.</li><li>Puisque les init containers s'exécutent jusqu'à la complétion avant qu'un conteneur d'application ne démarre, les init containers
offrent un mécanisme pour bloquer ou retarder le démarrage d'un conteneur d'application tant qu'un ensemble de préconditions n'est pas respecté. Une fois que les préconditions sont respectées, tous les conteneurs d'application dans un Pod peuvent démarrer en parallèle.</li></ul><h3 id=exemples>Exemples</h3><p>Voici plusieurs idées pour utiliser les init containers :</p><ul><li><p>Attendre qu'un <a class=glossary-tooltip title="Un moyen d'exposer une application s'exécutant sur un ensemble de pods en tant que service réseau." data-toggle=tooltip data-placement=top href=/fr/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> soit créé,
en utilisant une commande shell d'une ligne telle que :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#666>{</span>1..100<span style=color:#666>}</span>; <span style=color:#a2f;font-weight:700>do</span> sleep 1; <span style=color:#a2f;font-weight:700>if</span> dig myservice; <span style=color:#a2f;font-weight:700>then</span> <span style=color:#a2f>exit</span> 0; <span style=color:#a2f;font-weight:700>fi</span>; <span style=color:#a2f;font-weight:700>done</span>; <span style=color:#a2f>exit</span> <span style=color:#666>1</span>
</span></span></code></pre></div></li><li><p>Enregistrer ce Pod à un serveur distant depuis l'API downward avec une commande telle que :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -X POST http://<span style=color:#b8860b>$MANAGEMENT_SERVICE_HOST</span>:<span style=color:#b8860b>$MANAGEMENT_SERVICE_PORT</span>/register -d <span style=color:#b44>&#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;</span>
</span></span></code></pre></div></li><li><p>Attendre un certain temps avant de démarrer le conteneur d'application avec une commande telle que :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sleep <span style=color:#666>60</span>
</span></span></code></pre></div></li><li><p>Cloner un dépôt Git dans un <a class=glossary-tooltip title="Un répertoire contenant des données, accessible aux conteneurs d'un pod." data-toggle=tooltip data-placement=top href=/fr/docs/concepts/storage/volumes/ target=_blank aria-label=Volume>Volume</a></p></li><li><p>Placer des valeurs dans un fichier de configuration et exécuter un outil de templating pour générer
dynamiquement un fichier de configuration pour le conteneur d'application principal.
Par exemple, placer la valeur <code>POD_IP</code> dans une configuration et générer le fichier de configuration de l'application principale
en utilisant Jinja.</p></li></ul><h4 id=les-init-containers-en-utilisation>Les init containers en utilisation</h4><p>Cet exemple définit un simple Pod possédant deux init containers.
Le premier attend <code>myservice</code> et le second attend <code>mydb</code>. Une fois que les deux
init containers terminent leur exécution, le Pod exécute le conteneur d'application décrit dans sa section <code>spec</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;echo &#34;L&#39;&#39;app s&#39;&#39;exécute!&#34; &amp;&amp; sleep 3600&#39;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>initContainers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo en attente de myservice; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-mydb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo en attente de mydb; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div><p>Les fichiers YAML suivants résument les services <code>mydb</code> et <code>myservice</code> :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9377</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Vous pouvez démarrer ce Pod en exécutant :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f myapp.yaml
</span></span></code></pre></div><pre tabindex=0><code>pod/myapp-pod created
</code></pre><p>Et vérifier son statut avec :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><pre tabindex=0><code>NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
</code></pre><p>ou pour plus de détails :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe -f myapp.yaml
</span></span></code></pre></div><pre tabindex=0><code>Name:          myapp-pod
Namespace:     default
[...]
Labels:        app.kubernetes.io/name=MyApp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container with docker id 5ced34a04634; Security:[seccomp=unconfined]
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container with docker id 5ced34a04634
</code></pre><p>Pour voir les logs des init containers dans ce Pod, exécuter :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs myapp-pod -c init-myservice <span style=color:#080;font-style:italic># Inspecter le premier init container</span>
</span></span><span style=display:flex><span>kubectl logs myapp-pod -c init-mydb      <span style=color:#080;font-style:italic># Inspecter le second init container</span>
</span></span></code></pre></div><p>À ce stade, ces init containers attendent de découvrir les services nommés
<code>mydb</code> et <code>myservice</code>.</p><p>Voici une configuration que vous pouvez utiliser pour faire apparaître ces Services :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9377</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Pour créer les services <code>mydb</code> et <code>myservice</code> :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f services.yaml
</span></span></code></pre></div><pre tabindex=0><code>service/myservice created
service/mydb created
</code></pre><p>Vous verrez ensuite que ces init containers se terminent et que le Pod <code>myapp-pod</code> évolue vers l'état "Running" (en exécution) :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><pre tabindex=0><code>NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
</code></pre><p>Cet exemple simple devrait suffire à vous inspirer pour créer vos propres init containers.
<a href=#a-suivre>A suivre</a> contient un lien vers un exemple plus détaillé.</p><h2 id=comportement-détaillé>Comportement détaillé</h2><p>Pendant le démarrage d'un Pod, chaque init container démarre en ordre, après que le réseau
et les volumes ont été initialisés. Chaque conteneur doit se terminer avec succès avant que le prochain
ne démarre. Si un conteneur n'arrive pas à démarrer à cause d'un problème d'exécution ou
se termine avec un échec, il est redémarré selon la <code>restartPolicy</code> du Pod.
Toutefois, si la <code>restartPolicy</code> du Pod est configurée à "Always", les init containers utilisent la <code>restartPolicy</code> "OnFailure".</p><p>Un Pod ne peut pas être <code>Ready</code> tant que tous les init containers ne se sont pas exécutés avec succès.
Les ports d'un init container ne sont pas agrégés sous un Service. Un Pod qui s'initialise
est dans l'état <code>Pending</code> mais devrait avoir une condition <code>Initialized</code> configurée à "true".</p><p>Si le Pod <a href=#raisons-du-red%C3%A9marrage-d-un-pod>redémarre</a> ou est redémarré, tous les init containers
doivent s'exécuter à nouveau.</p><p>Les changements aux spec d'un init containers sont limités au champ image du conteneur.
Changer le champ image d'un init container équivaut à redémarrer le Pod.</p><p>Puisque les init containers peuvent être redémarrés, réessayés ou ré-exécutés,
leur code doit être idempotent. En particulier, le code qui écrit dans des fichiers sur <code>EmptyDirs</code>
devrait être préparé à la possibilité qu'un fichier de sortie existe déjà.</p><p>Les init containers ont tous les champs d'un conteneur d'application.
Cependant, Kubernetes interdit l'utilisation de <code>readinessProbe</code> parce que les init containers
ne peuvent pas définir une "readiness" distincte de la complétion. Ceci est appliqué lors de la validation.</p><p>L'utilisation de <code>activeDeadlineSeconds</code> sur le Pod et <code>livenessProbe</code> sur le conteneur
permet d'empêcher les init containers d'échouer tout le temps.
La deadline active inclut les init containers.</p><p>Le nom de chaque application et init container dans un Pod doit être unique; une erreur de validation
est générée pour tout conteneur partageant un nom avec un autre.</p><h3 id=ressources>Ressources</h3><p>Étant donné l'ordonnancement et l'exécution des init containers, les règles suivantes s'appliquent pour l'utilisation des ressources :</p><ul><li>La plus haute requête ou limite particulière de ressource définie pour tous les init containers
est la <em>limite/requête d'initialisation effective</em></li><li>La <em>limite/requête effective</em> d'un Pod pour une ressource est la plus haute parmis :<ul><li>la somme de toutes les requêtes/limites des conteneurs d'application pour une ressource</li><li>la limite/requête d'initialisation effective pour une ressource</li></ul></li><li>Le Scheduling est effectué sur la base des requêtes/limites effectives, ce qui signifie
que les init containers peuvent réserver des ressources pour l'initialisation qui ne sont pas utilisées durant le
cycle de vie du Pod.</li><li>La QoS (qualité de service) tierce de la <em>QoS tierce effective</em> d'un Pod est la QoS tierce aussi bien pour les init containers
que pour les conteneurs d'application.</li></ul><p>Les quotas et limites sont appliqués sur la base de la requête/limite effective d'un Pod.</p><p>Les groupes de contrôle au niveau du Pod (<a class=glossary-tooltip title="Un groupe de processus Linux avec des options d'isolation, de suivi, et de limites des ressources." data-toggle=tooltip data-placement=top href='/fr/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label=cgroups>cgroups</a>) sont basés sur la requête/limite effective de Pod, la même que
celle du scheduler.</p><h3 id=raisons-du-redémarrage-d-un-pod>Raisons du redémarrage d'un Pod</h3><p>Un Pod peut redémarrer, ce qui cause la ré-exécution des init containers, pour les raisons suivantes :</p><ul><li>Un utilisateur met à jour les spécifications du Pod, ce qui cause le changement de l'image de l'init container.
Tout changement à l'image du init container redémarre le Pod. Les changements au conteneur d'application entraînent seulement le
redémarrage du conteneur d'application.</li><li>Le conteneur d'infrastructure Pod est redémarré. Ceci est peu commun et serait effectué par une personne ayant un accès root aux nœuds.</li><li>Tous les conteneurs dans un Pod sont terminés tandis que <code>restartPolicy</code> est configurée à "Always", ce qui force le redémarrage, et l'enregistrement de complétion du init container a été perdu à cause d'une opération de garbage collection (récupération de mémoire).</li></ul><h2 id=a-suivre>A suivre</h2><ul><li>Lire à propos de la <a href=/docs/tasks/configure-pod-container/configure-pod-initialization/#creating-a-pod-that-has-an-init-container>création d'un Pod ayant un init container</a></li><li>Apprendre à <a href=/docs/tasks/debug/debug-application/debug-init-containers/>debugger les init containers</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-89637410cacae45a36ab1cc278c482eb>2 - Contrôleurs</h1></div><div class=td-content><h1 id=pg-d459b930218774655fa7fd1620625539>2.1 - ReplicaSet</h1><p>Un ReplicaSet (ensemble de réplicas en français) a pour but de maintenir un ensemble stable de Pods à un moment donné.
Cet objet est souvent utilisé pour garantir la disponibilité d'un certain nombre identique de Pods.</p><h2 id=comment-un-replicaset-fonctionne>Comment un ReplicaSet fonctionne</h2><p>Un ReplicaSet est défini avec des champs, incluant un selecteur qui spécifie comment identifier les Pods qu'il peut posséder,
un nombre de replicas indiquant le nombre de Pods qu'il doit maintenir et un modèle de Pod spécifiant les données que les
nouveaux Pods que le replicatSet va créer jusqu'au nombre de replicas demandé.</p><p>Un ReplicaSet va atteindre son objectif en créant et supprimant des Pods pour atteindre le nombre de réplicas désirés.
Quand un ReplicaSet a besoin de créer de nouveaux Pods, il utilise alors son Pod template.</p><p>Le lien d'un ReplicaSet à ses Pods est fait par le champ <a href=/docs/concepts/workloads/controllers/garbage-collection/#owners-and-dependents>metadata.ownerReferences</a>,
qui spécifie la ressource de l'objet par lequel il est détenu. Tous les Pods acquis par un ReplicaSet ont leurs propres informations d'identification de leur Replicaset, avec leur propre champ ownerReferences. C'est par ce lien que le ReplicaSet connait l'état des Pods qu'il maintient et agit en fonction de ces derniers.</p><p>Un ReplicaSet identifie des nouveaux Pods à acquérir en utilisant son selecteur.
Si il y a un Pod qui n'a pas de OwnerReference ou que OwnerReference n'est pas un controller et qu'il correspond à un sélecteur de ReplicaSet, il va immédiatement être acquis par ce ReplicaSet.</p><h2 id=quand-utiliser-un-replicaset>Quand utiliser un ReplicaSet ?</h2><p>Un ReplicaSet garantit qu’un nombre spécifié de réplicas de Pod soient exécutés à un moment donné.
Cependant, un Deployment est un concept de plus haut niveau qui gère les ReplicaSets et
fournit des mises à jour déclaratives aux Pods ainsi que de nombreuses autres fonctionnalités utiles.
Par conséquent, nous vous recommandons d’utiliser des Deployments au lieu d’utiliser directement des ReplicaSets, sauf si
vous avez besoin d'une orchestration personnalisée des mises à jour ou si vous n'avez pas besoin de mises à jour.</p><p>Cela signifie qu'il est possible que vous n'ayez jamais besoin de manipuler des objets ReplicaSet :
utilisez plutôt un déploiement et définissez votre application dans la section spec.</p><h2 id=exemple>Exemple</h2><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/fr/examples/controllers/frontend.yaml download=controllers/frontend.yaml><code>controllers/frontend.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-frontend-yaml")' title="Copy controllers/frontend.yaml to clipboard"></img></div><div class=includecode id=controllers-frontend-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># modify replicas according to your case</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>php-redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google_samples/gb-frontend:v3<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Enregistrer ce manifeste dans <code>frontend.yaml</code> et le soumettre à un cluster Kubernetes va créer le ReplicaSet défini et les pods qu’il gère.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>Vous pouvez ensuite récupérer les ReplicaSets actuellement déployés :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>Et voir le frontend que vous avez créé :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME       DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>frontend   <span style=color:#666>3</span>         <span style=color:#666>3</span>         <span style=color:#666>3</span>       6s
</span></span></code></pre></div><p>Vous pouvez également vérifier l'état du ReplicaSet :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe rs/frontend
</span></span></code></pre></div><p>Et vous verrez une sortie similaire à :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Name:		frontend
</span></span><span style=display:flex><span>Namespace:	default
</span></span><span style=display:flex><span>Selector:	<span style=color:#b8860b>tier</span><span style=color:#666>=</span>frontend,tier in <span style=color:#666>(</span>frontend<span style=color:#666>)</span>
</span></span><span style=display:flex><span>Labels:		<span style=color:#b8860b>app</span><span style=color:#666>=</span>guestbook
</span></span><span style=display:flex><span>		<span style=color:#b8860b>tier</span><span style=color:#666>=</span>frontend
</span></span><span style=display:flex><span>Annotations:	&lt;none&gt;
</span></span><span style=display:flex><span>Replicas:	<span style=color:#666>3</span> current / <span style=color:#666>3</span> desired
</span></span><span style=display:flex><span>Pods Status:	<span style=color:#666>3</span> Running / <span style=color:#666>0</span> Waiting / <span style=color:#666>0</span> Succeeded / <span style=color:#666>0</span> Failed
</span></span><span style=display:flex><span>Pod Template:
</span></span><span style=display:flex><span>  Labels:       <span style=color:#b8860b>app</span><span style=color:#666>=</span>guestbook
</span></span><span style=display:flex><span>                <span style=color:#b8860b>tier</span><span style=color:#666>=</span>frontend
</span></span><span style=display:flex><span>  Containers:
</span></span><span style=display:flex><span>   php-redis:
</span></span><span style=display:flex><span>    Image:      gcr.io/google_samples/gb-frontend:v3
</span></span><span style=display:flex><span>    Port:       80/TCP
</span></span><span style=display:flex><span>    Requests:
</span></span><span style=display:flex><span>      cpu:      100m
</span></span><span style=display:flex><span>      memory:   100Mi
</span></span><span style=display:flex><span>    Environment:
</span></span><span style=display:flex><span>      GET_HOSTS_FROM:   dns
</span></span><span style=display:flex><span>    Mounts:             &lt;none&gt;
</span></span><span style=display:flex><span>  Volumes:              &lt;none&gt;
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
</span></span><span style=display:flex><span>  ---------    --------    -----    ----                -------------    --------    ------            -------
</span></span><span style=display:flex><span>  1m           1m          <span style=color:#666>1</span>        <span style=color:#666>{</span>replicaset-controller <span style=color:#666>}</span>             Normal      SuccessfulCreate  Created pod: frontend-qhloh
</span></span><span style=display:flex><span>  1m           1m          <span style=color:#666>1</span>        <span style=color:#666>{</span>replicaset-controller <span style=color:#666>}</span>             Normal      SuccessfulCreate  Created pod: frontend-dnjpy
</span></span><span style=display:flex><span>  1m           1m          <span style=color:#666>1</span>        <span style=color:#666>{</span>replicaset-controller <span style=color:#666>}</span>             Normal      SuccessfulCreate  Created pod: frontend-9si5l
</span></span></code></pre></div><p>Et enfin, vous pourrez afficher les Pods déployés :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get Pods
</span></span></code></pre></div><p>Vous devriez voir des informations sur les Pods avec une sortie similaire à :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-9si5l   1/1       Running   <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-dnjpy   1/1       Running   <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-qhloh   1/1       Running   <span style=color:#666>0</span>          1m
</span></span></code></pre></div><p>Vous pouvez également vérifier que la OwnerReference de ces pods est définie sur le frontend ReplicaSet.
Pour ce faire, récupérez le yaml de l’un des pods :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods frontend-9si5l -o yaml
</span></span></code></pre></div><p>La sortie sera similaire à celle-ci, avec les informations de l'interface ReplicaSet frontend définies dans le champ ownerReferences des métadonnées:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  creationTimestamp: 2019-01-31T17:20:41Z
</span></span><span style=display:flex><span>  generateName: frontend-
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    tier: frontend
</span></span><span style=display:flex><span>  name: frontend-9si5l
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  ownerReferences:
</span></span><span style=display:flex><span>  - apiVersion: extensions/v1beta1
</span></span><span style=display:flex><span>    blockOwnerDeletion: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    controller: <span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>    kind: ReplicaSet
</span></span><span style=display:flex><span>    name: frontend
</span></span><span style=display:flex><span>    uid: 892a2330-257c-11e9-aecd-025000000001
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h2 id=acquisitions-de-pod-en-dehors-du-template>Acquisitions de Pod en dehors du template</h2><p>Bien que vous puissiez créer des pods manuellement sans problème, il est fortement recommandé de s’assurer que ces pods n'ont pas de
labels correspondant au sélecteur de l’un de vos ReplicaSets. Car un ReplicaSet n’est pas limité
à posséder les pods spécifiés par son modèle - il peut acquérir d’autres pods de la manière spécifiée dans les sections précédentes.</p><p>Prenez l'exemple précédent de ReplicaSet, ainsi que les pods spécifiés dans le manifeste suivant :</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/fr/examples/pods/pod-rs.yaml download=pods/pod-rs.yaml><code>pods/pod-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-rs-yaml")' title="Copy pods/pod-rs.yaml to clipboard"></img></div><div class=includecode id=pods-pod-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:2.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:1.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Ces pods n’ayant pas de contrôleur (ni d’objet) en tant que référence propriétaire, ils correspondent au sélecteur de du ReplicaSet frontend, ils seront donc immédiatement acquis par ce ReplicaSet.</p><p>Supposons que vous créiez les pods une fois le ReplicaSet frontend déployé et qui a déjà déployé ses replicas de Pods initiaux afin de
remplir son exigence de nombre de replicas :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>Les nouveaux pods seront acquis par le ReplicaSet, puis immédiatement terminés car le ReplicaSet dépasserait alors le compte désiré.</p><p>En récupérant les pods :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get Pods
</span></span></code></pre></div><p>La sortie montre que les nouveaux pods sont soit déjà terminés, soit en voie de l'être :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY   STATUS        RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-9si5l   1/1     Running       <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-dnjpy   1/1     Running       <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>frontend-qhloh   1/1     Running       <span style=color:#666>0</span>          1m
</span></span><span style=display:flex><span>pod2             0/1     Terminating   <span style=color:#666>0</span>          4s
</span></span></code></pre></div><p>Cependant, si vous créez d'abord les pods :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>Et puis créez le ReplicaSet :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>Vous verrez que le ReplicaSet a acquis les pods et n'a créé que les nouveaux Pods manquants, conformément à ses spécifications,
jusqu'au nombre souhaité de Pods. En récupérant les Pods :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get Pods
</span></span></code></pre></div><p>La sortie va donner :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-pxj4r   1/1     Running   <span style=color:#666>0</span>          5s
</span></span><span style=display:flex><span>pod1             1/1     Running   <span style=color:#666>0</span>          13s
</span></span><span style=display:flex><span>pod2             1/1     Running   <span style=color:#666>0</span>          13s
</span></span></code></pre></div><p>De cette manière, un ReplicaSet peut avoir un ensemble de Pods hétérogène.</p><h2 id=écrire-un-manifest-de-replicaset>Écrire un manifest de ReplicaSet</h2><p>Comme avec tous les autres objets API Kubernetes, un ReplicaSet a besoin des champs <code>apiVersion</code>, <code>kind</code> et <code>metadata</code>.
Pour ReplicaSets, l'attribut <code>kind</code> est toujours ReplicaSet.</p><p>Dans Kubernetes 1.9, la version de l'API <code>apps/v1</code> pour le type ReplicaSet est la version actuelle et activée par défaut. La version de l'API <code>apps/v1beta2</code> est obsolète.</p><p>Reportez-vous aux premières lignes de l'exemple <code>frontend.yaml</code> pour obtenir des conseils.</p><p>Un ReplicaSet a également besoin de <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>L'attribut <code>.spec.template</code> est un <a href=/docs/concepts/workloads/Pods/pod-overview/#pod-templates>modèle de pod</a> qui requiert d'avoir des labels. Dans notre exemple <code>frontend.yaml</code>, nous avons un label : <code>tier: frontend</code>.
Il faut faire attention à ne pas avoir des selecteurs que d'autres controllers utilisent, afin d'éviter que le ReplicaSet n'adopte ce pod.</p><p>Pour le champ <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy>restart policy</a>,
<code>.spec.template.spec.restartPolicy</code>, la seule valeur autorisée est <code>Always</code>, qui est la valeur par défaut.</p><h3 id=sélecteur-de-pod>Sélecteur de Pod</h3><p>Le champ <code>.spec.selector</code> est un <a href=/docs/concepts/overview/working-with-objects/labels/>label selector</a>. Tel que discuté
<a href=#how-a-replicaset-works>précédemment</a>, ce sont les labels utilisés pour identifier les Pods potentiels à acquérir. Dans notre
exemple avec <code>frontend.yaml</code>, le sélecteur était :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>matchLabels:
</span></span><span style=display:flex><span>	tier: frontend
</span></span></code></pre></div><p>Dans le ReplicaSet, <code>.spec.template.metadata.labels</code> doit correspondre à <code>spec.selector</code>, ou sinon il sera rejeté par l'API.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Pour 2 ReplicaSets spécifiant le même <code>.spec.selector</code> mais différents <code>.spec.template.metadata.labels</code> et <code>.spec.template.spec</code>, chaque ReplicaSet ignore les pods créés par l'autre ReplicaSet.</div><h3 id=replicas>Replicas</h3><p>Vous pouvez spécifier le nombre de pods à exécuter simultanément en définissant <code>.spec.replicas</code>. Le ReplicaSet va créer/supprimer
ses pods pour correspondre à ce nombre.</p><p>Si vous ne spécifiez pas <code>.spec.replicas</code>, la valeur par défaut est 1.</p><h2 id=travailler-avec-des-replicasets>Travailler avec des ReplicaSets</h2><h3 id=suppression-d-un-replicaset-et-de-ses-pods>Suppression d'un ReplicaSet et de ses pods</h3><p>Pour supprimer un ReplicaSet et tous ses pods, utilisez <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>. The <a href=/docs/concepts/workloads/controllers/garbage-collection/>Garbage collector</a> supprime automatiquement tous les pods associés par défaut.</p><p>Lors de l’utilisation de l’API REST ou de la bibliothèque <code>client-go</code>, vous devez définir <code>propagationPolicy</code> sur <code>Background</code> ou <code>Foreground</code> dans
l'option -d.
Par exemple :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><h3 id=supprimer-juste-un-replicaset>Supprimer juste un ReplicaSet</h3><p>Vous pouvez supprimer un ReplicaSet sans affecter ses pods à l’aide de <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a> avec l'option <code>--cascade=false</code>.
Lorsque vous utilisez l'API REST ou la bibliothèque <code>client-go</code>, vous devez définir <code>propagationPolicy</code> sur <code>Orphan</code>.
Par exemple :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/extensions/v1beta1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>Une fois l’original supprimé, vous pouvez créer un nouveau ReplicaSet pour le remplacer. Tant que l'ancien et le nouveau <code>.spec.selector</code> sont identiques, le nouveau adoptera les anciens Pods.
Cependant, le ReplicaSet ne fera aucun effort pour que les pods existants correspondent à un nouveau Pod template.
Pour mettre à jour les Pods à une nouvelle spec de manière contrôlée, utilisez un
<a href=/docs/concepts/workloads/controllers/deployment/#creating-a-deployment>Deployment</a>, car les ReplicaSets ne supportent pas de rolling update directement.</p><h3 id=isoler-les-pods-d-un-replicaset>Isoler les pods d'un ReplicaSet</h3><p>Vous pouvez supprimer les pods d'un ReplicaSet en modifiant leurs labels. Cette technique peut être utilisée pour enlever les pods
pour le débogage, récupération de données, etc. Les pods ainsi supprimés seront automatiquement remplacés
(en supposant que le nombre de réplicas n’est pas également modifié).</p><h3 id=scaling-d-un-replicaset>Scaling d'un ReplicaSet</h3><p>Un ReplicaSet peut facilement être scalé en mettant simplement à jour le champ <code>.spec.replicas</code>. Le contrôleur ReplicaSet
garantit que le nombre souhaité de pods avec un sélecteur de label correspondant soient disponibles et opérationnels.</p><h3 id=replicaset-en-tant-que-horizontal-pod-autoscaler-target>ReplicaSet en tant que Horizontal Pod Autoscaler Target</h3><p>Un ReplicaSet peut également être une cible pour
<a href=/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscalers (HPA)</a>.
Un ReplicaSet peut être mis à l'échelle automatiquement par un HPA. Voici un exemple HPA qui cible
le ReplicaSet que nous avons créé dans l'exemple précédent.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/fr/examples/controllers/hpa-rs.yaml download=controllers/hpa-rs.yaml><code>controllers/hpa-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-hpa-rs-yaml")' title="Copy controllers/hpa-rs.yaml to clipboard"></img></div><div class=includecode id=controllers-hpa-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>autoscaling/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>HorizontalPodAutoscaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-scaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scaleTargetRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>minReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>maxReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>targetCPUUtilizationPercentage</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Enregistrer ce manifeste dans <code>hpa-rs.yaml</code> et le soumettre à un cluster Kubernetes devrait
créer le HPA défini qui scale automatiquement le ReplicaSet cible en fonction de l'utilisation du processeur
des pods répliqués.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
</span></span></code></pre></div><p>Vous pouvez aussi utiliser la commande <code>kubectl autoscale</code> pour accomplir la même chose.
(et c'est plus facile !)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale rs frontend --max<span style=color:#666>=</span><span style=color:#666>10</span>
</span></span></code></pre></div><h2 id=alternatives-au-replicaset>Alternatives au ReplicaSet</h2><h3 id=deployment-recommandé>Deployment (recommandé)</h3><p>Le <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> est un object qui peut posséder les ReplicaSets et les mettres à jour ainsi que leurs Pods de façon déclarative, côté serveur et avec des rolling updates.</p><p>Alors que les ReplicaSets peuvent être utilisés indépendamment, ils sont principalement utilisés aujourd'hui par Deployments comme mécanisme pour orchestrer la création, suppresion et mises à jour des Pods.
Lorsque vous utilisez des Deployments, vous n’aurez plus à vous soucier de la gestion des ReplicaSets ainsi créés.
Les déploiements possèdent et gèrent leurs ReplicaSets.
C'est pourquoi il est recommandé d’utiliser les déploiements lorsque vous voulez des ReplicaSets.</p><h3 id=pods-nus>Pods nus</h3><p>Contrairement au cas où un utilisateur a créé directement des pods, un ReplicaSet remplace les pods supprimés ou terminés pour quelque raison que ce soit, par exemple en cas de défaillance d'un nœud ou de maintenance de nœud perturbateur, telle qu'une mise à jour kernel. Pour cette raison, nous vous recommandons d'utiliser un ReplicaSet même si votre application ne nécessite qu'un seul pod. Pensez-y de la même manière qu’un superviseur de processus, mais il supervise plusieurs pods sur plusieurs nœuds au lieu de processus individuels sur un seul nœud. Un ReplicaSet délègue les redémarrages de conteneurs locaux à un agent du nœud (par exemple, Kubelet ou Docker).</p><h3 id=job>Job</h3><p>Utilisez un <a href=/docs/concepts/jobs/run-to-completion-finite-workloads/><code>Job</code></a> au lieu d'un ReplicaSet pour les pods qui doivent se terminer seuls
(c'est à dire des batch jobs).</p><h3 id=daemonset>DaemonSet</h3><p>Utilisez un <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> au lieu d’un ReplicaSet pour les pods qui fournissent une
fonction au niveau du noeud, comme le monitoring ou la gestion des logs de ce noeud. Ces pods ont une durée de vie qui est liée
durée de vie d’une machine : le pod doit être en cours d’exécution sur la machine avant le démarrage des autres Pods et sont
sûrs de se terminer lorsque la machine est prête à être redémarrée/arrêtée.</p><h3 id=replicationcontroller>ReplicationController</h3><p>Les ReplicaSets sont les successeurs de <a href=/docs/concepts/workloads/controllers/replicationcontroller/><em>ReplicationControllers</em></a>.
Les deux servent le même objectif et se comportent de la même manière, à la différence près que ReplicationController ne prend pas en charge les
les exigences de sélecteur décrites dans le <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>labels user guide</a>.
En tant que tels, les ReplicaSets sont préférés aux ReplicationControllers.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a2dc0393e0c4079e1c504b6429844e86>2.2 - Déploiements</h1><p>Un <em>Deployment</em> (déploiement en français) fournit des mises à jour déclaratives pour <a href=/fr/docs/concepts/workloads/pods/pod/>Pods</a> et <a href=/fr/docs/concepts/workloads/controllers/replicaset/>ReplicaSets</a>.</p><p>Vous décrivez un <em>état désiré</em> dans un déploiement et le <a class=glossary-tooltip title="Boucle de contrôle surveillant l'état partagé du cluster à travers l'apiserver et effectuant des changements en essayant de déplacer l'état actuel vers l'état désiré." data-toggle=tooltip data-placement=top href=/docs/admin/kube-controller-manager/ target=_blank aria-label=controlleur>controlleur</a> déploiement change l'état réel à l'état souhaité à un rythme contrôlé.
Vous pouvez définir des Deployments pour créer de nouveaux ReplicaSets, ou pour supprimer des déploiements existants et adopter toutes leurs ressources avec de nouveaux déploiements.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Ne gérez pas les ReplicaSets appartenant à un Deployment.
Pensez à ouvrir un ticket dans le dépot Kubernetes principal si votre cas d'utilisation n'est pas traité ci-dessous.</div><h2 id=cas-d-utilisation>Cas d'utilisation</h2><p>Voici des cas d'utilisation typiques pour les déploiements:</p><ul><li><a href=#cr%C3%A9ation-dun-d%C3%A9ploiement>Créer un déploiement pour déployer un ReplicaSet</a>.
Le ReplicaSet crée des pods en arrière-plan.
Vérifiez l'état du déploiement pour voir s'il réussit ou non.</li><li><a href=#mise-%C3%A0-jour-dun-d%C3%A9ploiement>Déclarez le nouvel état des Pods</a> en mettant à jour le PodTemplateSpec du déploiement.
Un nouveau ReplicaSet est créé et le déploiement gère le déplacement des pods de l'ancien ReplicaSet vers le nouveau à un rythme contrôlé.
Chaque nouveau ReplicaSet met à jour la révision du déploiement.</li><li><a href=#annulation-dun-d%C3%A9ploiement>Revenir à une révision de déploiement antérieure</a> si l'état actuel du déploiement n'est pas stable.
Chaque restauration met à jour la révision du déploiement.</li><li><a href=#mise-%C3%A0-l%C3%A9chelle-dun-d%C3%A9ploiement>Augmentez le déploiement pour traiter plus de charge</a>.</li><li><a href=#pause-et-reprise-dun-d%C3%A9ploiement>Suspendre le déploiement</a> d'appliquer plusieurs correctifs à son PodTemplateSpec, puis de le reprendre pour démarrer un nouveau déploiement.</li><li><a href=#statut-de-d%C3%A9ploiement>Utiliser l'état du déploiement</a> comme indicateur qu'un déploiement est bloqué.</li><li><a href=#politique-de-nettoyage>Nettoyer les anciens ReplicaSets</a> dont vous n'avez plus besoin.</li></ul><h2 id=création-d-un-déploiement>Création d'un déploiement</h2><p>Voici un exemple de déploiement.
Il crée un ReplicaSet pour faire apparaître trois pods <code>nginx</code>:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/fr/examples/controllers/nginx-deployment.yaml download=controllers/nginx-deployment.yaml><code>controllers/nginx-deployment.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-nginx-deployment-yaml")' title="Copy controllers/nginx-deployment.yaml to clipboard"></img></div><div class=includecode id=controllers-nginx-deployment-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.7.9<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Dans cet exemple:</p><ul><li><p>Un déploiement nommé <code>nginx-deployment</code> est créé, indiqué par le champ <code>.metadata.name</code>.</p></li><li><p>Le déploiement crée trois pods répliqués, indiqués par le champ <code>replicas</code>.</p></li><li><p>Le champ <code>selector</code> définit comment le déploiement trouve les pods à gérer.
Dans ce cas, vous sélectionnez simplement un label définie dans le template de pod (<code>app:nginx</code>).
Cependant, des règles de sélection plus sophistiquées sont possibles, tant que le modèle de pod satisfait lui-même la règle.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Le champ <code>matchLabels</code> est une table de hash {clé, valeur}.
Une seule {clé, valeur} dans la table <code>matchLabels</code> est équivalente à un élément de <code>matchExpressions</code>, dont le champ clé est "clé", l'opérateur est "In" et le tableau de valeurs contient uniquement "valeur".
Toutes les exigences, à la fois de <code>matchLabels</code> et de <code>matchExpressions</code>, doivent être satisfaites pour correspondre.</div></li><li><p>Le champ <code>template</code> contient les sous-champs suivants:</p><ul><li>Les Pods reçoivent le label <code>app:nginx</code> dans le champ <code>labels</code>.</li><li>La spécification du template de pod dans le champ <code>.template.spec</code>, indique que les pods exécutent un conteneur, <code>nginx</code>, qui utilise l'image <code>nginx</code> <a href=https://hub.docker.com/>Docker Hub</a> à la version 1.7.9.</li><li>Créez un conteneur et nommez-le <code>nginx</code> en utilisant le champ <code>name</code>.</li></ul></li></ul><p>Suivez les étapes ci-dessous pour créer le déploiement ci-dessus:</p><p>Avant de commencer, assurez-vous que votre cluster Kubernetes est opérationnel.</p><ol><li><p>Créez le déploiement en exécutant la commande suivante:</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Vous pouvez spécifier l'indicateur <code>--record</code> pour écrire la commande exécutée dans l'annotation de ressource <code>kubernetes.io/change-cause</code>.
C'est utile pour une future introspection.
Par exemple, pour voir les commandes exécutées dans chaque révision de déploiement.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
</span></span></code></pre></div></li><li><p>Exécutez <code>kubectl get deployments</code> pour vérifier si le déploiement a été créé.
Si le déploiement est toujours en cours de création, la sortie est similaire à:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx-deployment   0/3     <span style=color:#666>0</span>            <span style=color:#666>0</span>           1s
</span></span></code></pre></div><p>Lorsque vous inspectez les déploiements de votre cluster, les champs suivants s'affichent:</p><ul><li><code>NAME</code> répertorie les noms des déploiements dans le cluster.</li><li><code>DESIRED</code> affiche le nombre souhaité de <em>répliques</em> de l'application, que vous définissez lorsque vous créez le déploiement.
C'est l'<em>état désiré</em>.</li><li><code>CURRENT</code> affiche le nombre de réplicas en cours d'exécution.</li><li><code>UP-TO-DATE</code> affiche le nombre de réplicas qui ont été mises à jour pour atteindre l'état souhaité.</li><li><code>AVAILABLE</code> affiche le nombre de réplicas de l'application disponibles pour vos utilisateurs.</li><li><code>AGE</code> affiche la durée d'exécution de l'application.</li></ul><p>Notez que le nombre de réplicas souhaitées est de 3 selon le champ <code>.spec.replicas</code>.</p></li><li><p>Pour voir l'état du déploiement, exécutez:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Waiting <span style=color:#a2f;font-weight:700>for</span> rollout to finish: <span style=color:#666>2</span> out of <span style=color:#666>3</span> new replicas have been updated...
</span></span><span style=display:flex><span>deployment <span style=color:#b44>&#34;nginx-deployment&#34;</span> successfully rolled out
</span></span></code></pre></div></li><li><p>Exécutez à nouveau <code>kubectl get deployments</code> quelques secondes plus tard.
La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx-deployment   3/3     3            3           18s
</span></span></code></pre></div><p>Notez que le déploiement a créé les trois répliques et que toutes les répliques sont à jour (elles contiennent le dernier modèle de pod) et disponibles.</p></li><li><p>Pour voir le ReplicaSet (<code>rs</code>) créé par le déploiement, exécutez <code>kubectl get rs</code>.
La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                          DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>nginx-deployment-75675f5897   3         3         3       18s
</span></span></code></pre></div><p>Notez que le nom du ReplicaSet est toujours formaté comme: <code>[DEPLOYMENT-NAME]-[RANDOM-STRING]</code>.
La chaîne aléatoire est générée aléatoirement et utilise le pod-template-hash comme graine.</p></li><li><p>Pour voir les labels générées automatiquement pour chaque Pod, exécutez <code>kubectl get pods --show-labels</code>.
La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE       LABELS
</span></span><span style=display:flex><span>nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
</span></span><span style=display:flex><span>nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
</span></span><span style=display:flex><span>nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
</span></span></code></pre></div></li></ol><p>Le ReplicaSet créé garantit qu'il y a trois pods <code>nginx</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Vous devez spécifier un sélecteur approprié et des labels de template de pod dans un déploiement (dans ce cas, <code>app: nginx</code>).
Ne superposez pas les étiquettes ou les sélecteurs avec d'autres contrôleurs (y compris d'autres déploiements et StatefulSets).
Kubernetes n'empêche pas les chevauchements de noms, et si plusieurs contrôleurs ont des sélecteurs qui se chevauchent, ces contrôleurs peuvent entrer en conflit et se comporter de façon inattendue.</div><h3 id=étiquette-pod-template-hash>Étiquette pod-template-hash</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Ne modifiez pas ce label.</div><p>Le label <code>pod-template-hash</code> est ajoutée par le contrôleur de déploiement à chaque ReplicaSet créé ou adopté par un déploiement.</p><p>Ce label garantit que les ReplicaSets enfants d'un déploiement ne se chevauchent pas.
Il est généré en hachant le <code>PodTemplate</code> du ReplicaSet et en utilisant le hachage résultant comme valeur de label qui est ajoutée au sélecteur ReplicaSet, aux labels de template de pod et dans tous les pods existants que le ReplicaSet peut avoir.</p><h2 id=mise-à-jour-d-un-déploiement>Mise à jour d'un déploiement</h2><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Le re-déploiement d'un déploiement est déclenché si et seulement si le modèle de pod du déploiement (c'est-à-dire <code>.spec.template</code>) est modifié, par exemple si les labels ou les images de conteneur du template sont mis à jour.
D'autres mises à jour, telles que la mise à l'échelle du déploiement, ne déclenchent pas de rollout.</div><p>Suivez les étapes ci-dessous pour mettre à jour votre déploiement:</p><ol><li><p>Mettons à jour les pods nginx pour utiliser l'image <code>nginx: 1.9.1</code> au lieu de l'image <code>nginx: 1.7.9</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl --record deployment.apps/nginx-deployment <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.9.1
</span></span></code></pre></div><p>ou utilisez la commande suivante:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.9.1 --record
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment image updated
</span></span></code></pre></div><p>Alternativement, vous pouvez <code>éditer</code> le déploiement et changer <code>.spec.template.spec.containers[0].image</code> de <code>nginx: 1.7.9</code> à <code>nginx: 1.9.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment edited
</span></span></code></pre></div></li><li><p>Pour voir l'état du déploiement, exécutez:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
</span></span></code></pre></div><p>ou</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment &#34;nginx-deployment&#34; successfully rolled out
</span></span></code></pre></div></li></ol><p>Obtenez plus de détails sur votre déploiement mis à jour:</p><ul><li><p>Une fois le déploiement réussi, vous pouvez afficher le déploiement en exécutant <code>kubectl get deployments</code>.
La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx-deployment   3/3     3            3           36s
</span></span></code></pre></div></li><li><p>Exécutez <code>kubectl get rs</code> pour voir que le déploiement a mis à jour les pods en créant un nouveau ReplicaSet et en le redimensionnant jusqu'à 3 replicas, ainsi qu'en réduisant l'ancien ReplicaSet à 0 réplicas.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                          DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>nginx-deployment-1564180365   3         3         3       6s
</span></span><span style=display:flex><span>nginx-deployment-2035384211   0         0         0       36s
</span></span></code></pre></div></li><li><p>L'exécution de <code>kubectl get pods</code> ne devrait désormais afficher que les nouveaux pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                                READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-deployment-1564180365-khku8   1/1       Running   0          14s
</span></span><span style=display:flex><span>nginx-deployment-1564180365-nacti   1/1       Running   0          14s
</span></span><span style=display:flex><span>nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</span></span></code></pre></div><p>La prochaine fois que vous souhaitez mettre à jour ces pods, il vous suffit de mettre à jour le modèle de pod de déploiement à nouveau.</p><p>Le déploiement garantit que seul un certain nombre de pods sont en panne pendant leur mise à jour.
Par défaut, il garantit qu'au moins 75% du nombre souhaité de pods sont en place (25% max indisponible).</p><p>Le déploiement garantit également que seul un certain nombre de pods sont créés au-dessus du nombre souhaité de pods.
Par défaut, il garantit qu'au plus 125% du nombre de pods souhaité sont en hausse (surtension maximale de 25%).</p><p>Par exemple, si vous regardez attentivement le déploiement ci-dessus, vous verrez qu'il a d'abord créé un nouveau pod, puis supprimé certains anciens pods et en a créé de nouveaux.
Il ne tue pas les anciens Pods tant qu'un nombre suffisant de nouveaux Pods n'est pas apparu, et ne crée pas de nouveaux Pods tant qu'un nombre suffisant de Pods anciens n'a pas été tué.
Il s'assure qu'au moins 2 pods sont disponibles et qu'au maximum 4 pods au total sont disponibles.</p></li><li><p>Obtenez les détails de votre déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployments
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Name:                   nginx-deployment
</span></span><span style=display:flex><span>Namespace:              default
</span></span><span style=display:flex><span>CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
</span></span><span style=display:flex><span>Labels:                 app=nginx
</span></span><span style=display:flex><span>Annotations:            deployment.kubernetes.io/revision=2
</span></span><span style=display:flex><span>Selector:               app=nginx
</span></span><span style=display:flex><span>Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
</span></span><span style=display:flex><span>StrategyType:           RollingUpdate
</span></span><span style=display:flex><span>MinReadySeconds:        0
</span></span><span style=display:flex><span>RollingUpdateStrategy:  25% max unavailable, 25% max surge
</span></span><span style=display:flex><span>Pod Template:
</span></span><span style=display:flex><span>    Labels:  app=nginx
</span></span><span style=display:flex><span>    Containers:
</span></span><span style=display:flex><span>    nginx:
</span></span><span style=display:flex><span>        Image:        nginx:1.9.1
</span></span><span style=display:flex><span>        Port:         80/TCP
</span></span><span style=display:flex><span>        Environment:  &lt;none&gt;
</span></span><span style=display:flex><span>        Mounts:       &lt;none&gt;
</span></span><span style=display:flex><span>    Volumes:        &lt;none&gt;
</span></span><span style=display:flex><span>    Conditions:
</span></span><span style=display:flex><span>    Type           Status  Reason
</span></span><span style=display:flex><span>    ----           ------  ------
</span></span><span style=display:flex><span>    Available      True    MinimumReplicasAvailable
</span></span><span style=display:flex><span>    Progressing    True    NewReplicaSetAvailable
</span></span><span style=display:flex><span>    OldReplicaSets:  &lt;none&gt;
</span></span><span style=display:flex><span>    NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
</span></span><span style=display:flex><span>    Events:
</span></span><span style=display:flex><span>    Type    Reason             Age   From                   Message
</span></span><span style=display:flex><span>    ----    ------             ----  ----                   -------
</span></span><span style=display:flex><span>    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
</span></span><span style=display:flex><span>    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
</span></span><span style=display:flex><span>    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
</span></span><span style=display:flex><span>    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
</span></span><span style=display:flex><span>    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
</span></span><span style=display:flex><span>    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
</span></span><span style=display:flex><span>    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
</span></span></code></pre></div><p>Ici, vous voyez que lorsque vous avez créé le déploiement pour la première fois, il a créé un ReplicaSet (nginx-deployment-2035384211) et l'a mis à l'échelle directement jusqu'à 3 réplicas.
Lorsque vous avez mis à jour le déploiement, il a créé un nouveau ReplicaSet (nginx-deployment-1564180365) et l'a mis à l'échelle jusqu'à 1, puis a réduit l'ancien ReplicaSet à 2, de sorte qu'au moins 2 pods étaient disponibles et au plus 4 pods ont été créés à chaque fois.
Il a ensuite poursuivi la montée en puissance du nouveau et de l'ancien ReplicaSet, avec la même stratégie de mise à jour continue.
Enfin, vous aurez 3 réplicas disponibles dans le nouveau ReplicaSet, et l'ancien ReplicaSet est réduit à 0.</p></li></ul><h3 id=rollover>Rollover (alias plusieurs mises à jour en vol)</h3><p>Chaque fois qu'un nouveau déploiement est observé par le contrôleur de déploiement, un ReplicaSet est créé pour afficher les pods souhaités.
Si le déploiement est mis à jour, le ReplicaSet existant qui contrôle les pods dont les étiquettes correspondent à <code>.spec.selector</code> mais dont le modèle ne correspond pas à <code>.spec.template</code> est réduit.
Finalement, le nouveau ReplicaSet est mis à l'échelle à <code>.spec.replicas</code> et tous les anciens ReplicaSets sont mis à l'échelle à 0.</p><p>Si vous mettez à jour un déploiement alors qu'un déploiement existant est en cours, le déploiement crée un nouveau ReplicaSet conformément à la mise à jour et commence à le mettre à l'échelle, et arrête de mettre à jour le ReplicaSet qu'il augmentait précédemment - il l'ajoutera à sa liste de anciens ReplicaSets et commencera à le réduire.</p><p>Par exemple, supposons que vous créez un déploiement pour créer 5 répliques de <code>nginx: 1.7.9</code>, puis mettez à jour le déploiement pour créer 5 répliques de <code>nginx: 1.9.1</code>, alors que seulement 3 répliques de <code>nginx:1.7.9</code> avait été créés.
Dans ce cas, le déploiement commence immédiatement à tuer les 3 pods <code>nginx: 1.7.9</code> qu'il avait créés et commence à créer des pods <code>nginx: 1.9.1</code>.
Il n'attend pas que les 5 répliques de <code>nginx: 1.7.9</code> soient créées avant de changer de cap.</p><h3 id=mises-à-jour-du-sélecteur-de-labels>Mises à jour du sélecteur de labels</h3><p>Il est généralement déconseillé de mettre à jour le sélecteur de labels et il est suggéré de planifier vos sélecteurs à l'avance.
Dans tous les cas, si vous devez effectuer une mise à jour du sélecteur de labels, soyez très prudent et assurez-vous d'avoir saisi toutes les implications.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Dans la version d'API <code>apps/v1</code>, le sélecteur de label d'un déploiement est immuable après sa création.</div><ul><li>Les ajouts de sélecteur nécessitent que les labels de template de pod dans la spécification de déploiement soient également mises à jour avec les nouveaux labels, sinon une erreur de validation est renvoyée.
Cette modification ne se chevauche pas, ce qui signifie que le nouveau sélecteur ne sélectionne pas les ReplicaSets et les pods créés avec l'ancien sélecteur, ce qui entraîne la perte de tous les anciens ReplicaSets et la création d'un nouveau ReplicaSet.</li><li>Les mises à jour du sélecteur modifient la valeur existante dans une clé de sélection - entraînent le même comportement que les ajouts.</li><li>La suppression de sélecteur supprime une clé existante du sélecteur de déploiement - ne nécessite aucune modification dans les labels du template de pod.
Les ReplicaSets existants ne sont pas orphelins et aucun nouveau ReplicaSet n'est créé, mais notez que le label supprimé existe toujours dans tous les Pods et ReplicaSets existants.</li></ul><h2 id=annulation-d-un-déploiement>Annulation d'un déploiement</h2><p>Parfois, vous souhaiterez peut-être annuler un déploiement; par exemple, lorsque le déploiement n'est pas stable, comme en cas d'échecs à répétition (CrashLoopBackOff).
Par défaut, tout l'historique des déploiements d'un déploiement est conservé dans le système afin que vous puissiez le restaurer à tout moment (vous pouvez le modifier en modifiant la limite de l'historique des révisions).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> La révision d'un déploiement est créée lorsque le déploiement d'un déploiement est déclenché.
Cela signifie qu'une nouvelle révision est créée si et seulement si le template de pod de déploiement (<code>.spec.template</code>) est modifié, par exemple si vous mettez à jour les labels ou les images de conteneur du template.
D'autres mises à jour, telles que la mise à l'échelle du déploiement, ne créent pas de révision de déploiement, de sorte que vous puissiez faciliter la mise à l'échelle manuelle ou automatique simultanée.
Cela signifie que lorsque vous revenez à une révision antérieure, seule la partie du template de pod de déploiement est annulée.</div><ul><li><p>Supposons que vous ayez fait une faute de frappe lors de la mise à jour du déploiement, en mettant le nom de l'image sous la forme <code>nginx:1.91</code> au lieu de <code>nginx: 1.9.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.91 --record<span style=color:#666>=</span><span style=color:#a2f>true</span>
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment image updated
</span></span></code></pre></div></li><li><p>Le déploiement est bloqué.
Vous pouvez le vérifier en vérifiant l'état du déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
</span></span></code></pre></div></li><li><p>Appuyez sur Ctrl-C pour arrêter la surveillance d'état de déploiement ci-dessus.
Pour plus d'informations sur les déploiements bloqués, <a href=#deployment-status>en savoir plus ici</a>.</p></li><li><p>Vous voyez que le nombre d'anciens réplicas (<code>nginx-deployment-1564180365</code> et <code>nginx-deployment-2035384211</code>) est 2, et les nouveaux réplicas (<code>nginx-deployment-3066724191</code>) est 1.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                          DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>nginx-deployment-1564180365   3         3         3       25s
</span></span><span style=display:flex><span>nginx-deployment-2035384211   0         0         0       36s
</span></span><span style=display:flex><span>nginx-deployment-3066724191   1         1         0       6s
</span></span></code></pre></div></li><li><p>En regardant les pods créés, vous voyez que 1 pod créé par le nouveau ReplicaSet est coincé dans une boucle pour récupérer son image:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                                READY     STATUS             RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-deployment-1564180365-70iae   1/1       Running            0          25s
</span></span><span style=display:flex><span>nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
</span></span><span style=display:flex><span>nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
</span></span><span style=display:flex><span>nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Le contrôleur de déploiement arrête automatiquement le mauvais déploiement et arrête la mise à l'échelle du nouveau ReplicaSet.
Cela dépend des paramètres rollingUpdate (<code>maxUnavailable</code> spécifiquement) que vous avez spécifiés.
Kubernetes définit par défaut la valeur à 25%.</div></li><li><p>Obtenez la description du déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Name:           nginx-deployment
</span></span><span style=display:flex><span>Namespace:      default
</span></span><span style=display:flex><span>CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
</span></span><span style=display:flex><span>Labels:         app=nginx
</span></span><span style=display:flex><span>Selector:       app=nginx
</span></span><span style=display:flex><span>Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
</span></span><span style=display:flex><span>StrategyType:       RollingUpdate
</span></span><span style=display:flex><span>MinReadySeconds:    0
</span></span><span style=display:flex><span>RollingUpdateStrategy:  25% max unavailable, 25% max surge
</span></span><span style=display:flex><span>Pod Template:
</span></span><span style=display:flex><span>  Labels:  app=nginx
</span></span><span style=display:flex><span>  Containers:
</span></span><span style=display:flex><span>   nginx:
</span></span><span style=display:flex><span>    Image:        nginx:1.91
</span></span><span style=display:flex><span>    Port:         80/TCP
</span></span><span style=display:flex><span>    Host Port:    0/TCP
</span></span><span style=display:flex><span>    Environment:  &lt;none&gt;
</span></span><span style=display:flex><span>    Mounts:       &lt;none&gt;
</span></span><span style=display:flex><span>  Volumes:        &lt;none&gt;
</span></span><span style=display:flex><span>Conditions:
</span></span><span style=display:flex><span>  Type           Status  Reason
</span></span><span style=display:flex><span>  ----           ------  ------
</span></span><span style=display:flex><span>  Available      True    MinimumReplicasAvailable
</span></span><span style=display:flex><span>  Progressing    True    ReplicaSetUpdated
</span></span><span style=display:flex><span>OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
</span></span><span style=display:flex><span>NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
</span></span><span style=display:flex><span>  --------- --------    -----   ----                    -------------   --------    ------              -------
</span></span><span style=display:flex><span>  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
</span></span><span style=display:flex><span>  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
</span></span><span style=display:flex><span>  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
</span></span><span style=display:flex><span>  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
</span></span><span style=display:flex><span>  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
</span></span><span style=display:flex><span>  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
</span></span><span style=display:flex><span>  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
</span></span><span style=display:flex><span>  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
</span></span></code></pre></div><p>Pour résoudre ce problème, vous devez revenir à une version précédente de Deployment qui est stable.</p></li></ul><h3 id=vérification-de-l-historique-de-déploiement-d-un-déploiement>Vérification de l'historique de déploiement d'un déploiement</h3><p>Suivez les étapes ci-dessous pour vérifier l'historique de déploiement:</p><ol><li><p>Tout d'abord, vérifiez les révisions de ce déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployments &#34;nginx-deployment&#34;
</span></span><span style=display:flex><span>REVISION    CHANGE-CAUSE
</span></span><span style=display:flex><span>1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml --record=true
</span></span><span style=display:flex><span>2           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
</span></span><span style=display:flex><span>3           kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.91 --record=true
</span></span></code></pre></div><p><code>CHANGE-CAUSE</code> est copié de l'annotation de déploiement <code>kubernetes.io/change-cause</code> dans ses révisions lors de la création.
Vous pouvez spécifier le message<code>CHANGE-CAUSE</code> en:</p><ul><li>Annoter le déploiement avec <code>kubectl annotate deployment.v1.apps/nginx-deployment kubernetes.io/change-cause="image mis à jour en 1.9.1"</code></li><li>Ajoutez le drapeau <code>--record</code> pour enregistrer la commande <code>kubectl</code> qui apporte des modifications à la ressource.</li><li>Modification manuelle du manifeste de la ressource.</li></ul></li><li><p>Pour voir les détails de chaque révision, exécutez:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment.v1.apps/nginx-deployment --revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployments &#34;nginx-deployment&#34; revision 2
</span></span><span style=display:flex><span>  Labels:       app=nginx
</span></span><span style=display:flex><span>          pod-template-hash=1159050644
</span></span><span style=display:flex><span>  Annotations:  kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
</span></span><span style=display:flex><span>  Containers:
</span></span><span style=display:flex><span>   nginx:
</span></span><span style=display:flex><span>    Image:      nginx:1.9.1
</span></span><span style=display:flex><span>    Port:       80/TCP
</span></span><span style=display:flex><span>     QoS Tier:
</span></span><span style=display:flex><span>        cpu:      BestEffort
</span></span><span style=display:flex><span>        memory:   BestEffort
</span></span><span style=display:flex><span>    Environment Variables:      &lt;none&gt;
</span></span><span style=display:flex><span>  No volumes.
</span></span></code></pre></div></li></ol><h3 id=revenir-à-une-révision-précédente>Revenir à une révision précédente</h3><p>Suivez les étapes ci-dessous pour restaurer le déploiement de la version actuelle à la version précédente, qui est la version 2.</p><ol><li><p>Vous avez maintenant décidé d'annuler le déploiement actuel et le retour à la révision précédente:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment
</span></span></code></pre></div><p>Alternativement, vous pouvez revenir à une révision spécifique en la spécifiant avec <code>--to-revision</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment
</span></span></code></pre></div><p>Pour plus de détails sur les commandes liées au déploiement, lisez <a href=/docs/reference/generated/kubectl/kubectl-commands#rollout><code>kubectl rollout</code></a>.</p><p>Le déploiement est maintenant rétabli à une précédente révision stable.
Comme vous pouvez le voir, un événement <code>DeploymentRollback</code> pour revenir à la révision 2 est généré à partir du contrôleur de déploiement.</p></li><li><p>Vérifiez si la restauration a réussi et que le déploiement s'exécute comme prévu, exécutez:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx-deployment   3/3     3            3           30m
</span></span></code></pre></div></li><li><p>Obtenez la description du déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Name:                   nginx-deployment
</span></span><span style=display:flex><span>Namespace:              default
</span></span><span style=display:flex><span>CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
</span></span><span style=display:flex><span>Labels:                 app=nginx
</span></span><span style=display:flex><span>Annotations:            deployment.kubernetes.io/revision=4
</span></span><span style=display:flex><span>                        kubernetes.io/change-cause=kubectl set image deployment.v1.apps/nginx-deployment nginx=nginx:1.9.1 --record=true
</span></span><span style=display:flex><span>Selector:               app=nginx
</span></span><span style=display:flex><span>Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
</span></span><span style=display:flex><span>StrategyType:           RollingUpdate
</span></span><span style=display:flex><span>MinReadySeconds:        0
</span></span><span style=display:flex><span>RollingUpdateStrategy:  25% max unavailable, 25% max surge
</span></span><span style=display:flex><span>Pod Template:
</span></span><span style=display:flex><span>  Labels:  app=nginx
</span></span><span style=display:flex><span>  Containers:
</span></span><span style=display:flex><span>   nginx:
</span></span><span style=display:flex><span>    Image:        nginx:1.9.1
</span></span><span style=display:flex><span>    Port:         80/TCP
</span></span><span style=display:flex><span>    Host Port:    0/TCP
</span></span><span style=display:flex><span>    Environment:  &lt;none&gt;
</span></span><span style=display:flex><span>    Mounts:       &lt;none&gt;
</span></span><span style=display:flex><span>  Volumes:        &lt;none&gt;
</span></span><span style=display:flex><span>Conditions:
</span></span><span style=display:flex><span>  Type           Status  Reason
</span></span><span style=display:flex><span>  ----           ------  ------
</span></span><span style=display:flex><span>  Available      True    MinimumReplicasAvailable
</span></span><span style=display:flex><span>  Progressing    True    NewReplicaSetAvailable
</span></span><span style=display:flex><span>OldReplicaSets:  &lt;none&gt;
</span></span><span style=display:flex><span>NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type    Reason              Age   From                   Message
</span></span><span style=display:flex><span>  ----    ------              ----  ----                   -------
</span></span><span style=display:flex><span>  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
</span></span><span style=display:flex><span>  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
</span></span><span style=display:flex><span>  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
</span></span><span style=display:flex><span>  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
</span></span><span style=display:flex><span>  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
</span></span><span style=display:flex><span>  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
</span></span><span style=display:flex><span>  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
</span></span><span style=display:flex><span>  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
</span></span><span style=display:flex><span>  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &#34;nginx-deployment&#34; to revision 2
</span></span><span style=display:flex><span>  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
</span></span></code></pre></div></li></ol><h2 id=mise-à-l-échelle-d-un-déploiement>Mise à l'échelle d'un déploiement</h2><p>Vous pouvez mettre à l'échelle un déploiement à l'aide de la commande suivante:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment.v1.apps/nginx-deployment --replicas<span style=color:#666>=</span><span style=color:#666>10</span>
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment scaled
</span></span></code></pre></div><p>En supposant que l'<a href=/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/>horizontal Pod autoscaling</a> est activé dans votre cluster, vous pouvez configurer une mise à l'échelle automatique pour votre déploiement et choisir le nombre minimum et maximum de pods que vous souhaitez exécuter en fonction de l'utilisation du processeur de vos pods existants.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale deployment.v1.apps/nginx-deployment --min<span style=color:#666>=</span><span style=color:#666>10</span> --max<span style=color:#666>=</span><span style=color:#666>15</span> --cpu-percent<span style=color:#666>=</span><span style=color:#666>80</span>
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment scaled
</span></span></code></pre></div><h3 id=mise-à-l-échelle-proportionnelle>Mise à l'échelle proportionnelle</h3><p>Les déploiements RollingUpdate prennent en charge l'exécution simultanée de plusieurs versions d'une application.
Lorsque vous ou un autoscaler mettez à l'échelle un déploiement RollingUpdate qui se trouve au milieu d'un déploiement (en cours ou en pause), le contrôleur de déploiement équilibre les réplicas supplémentaires dans les ReplicaSets actifs existants (ReplicaSets avec pods) afin d'atténuer le risque.
Ceci est appelé <em>mise à l'échelle proportionnelle</em>.</p><p>Par exemple, vous exécutez un déploiement avec 10 réplicas, <a href=#max-surge>maxSurge</a>=3, et <a href=#max-unavailable>maxUnavailable</a>=2.</p><ul><li><p>Assurez-vous que les 10 réplicas de votre déploiement sont en cours d'exécution.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx-deployment     10        10        10           10          50s
</span></span></code></pre></div></li><li><p>Vous effectuez une mise à jour vers une nouvelle image qui s'avère impossible à résoudre depuis l'intérieur du cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:sometag
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment image updated
</span></span></code></pre></div></li><li><p>La mise à jour de l'image démarre un nouveau déploiement avec ReplicaSet <code>nginx-deployment-1989198191</code>, mais elle est bloquée en raison de l'exigence <code>maxUnavailable</code> que vous avez mentionnée ci-dessus.
Découvrez l'état du déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                          DESIRED   CURRENT   READY     AGE
</span></span><span style=display:flex><span>nginx-deployment-1989198191   5         5         0         9s
</span></span><span style=display:flex><span>nginx-deployment-618515232    8         8         8         1m
</span></span></code></pre></div></li><li><p>Ensuite, une nouvelle demande de mise à l'échelle pour le déploiement arrive.
La mise à l'échelle automatique incrémente les réplicas de déploiement à 15.
Le contrôleur de déploiement doit décider où ajouter ces 5 nouvelles répliques.
Si vous n'utilisiez pas la mise à l'échelle proportionnelle, les 5 seraient ajoutés dans le nouveau ReplicaSet.
Avec une mise à l'échelle proportionnelle, vous répartissez les répliques supplémentaires sur tous les ReplicaSets.
Des proportions plus importantes vont aux ReplicaSets avec le plus de répliques et des proportions plus faibles vont aux ReplicaSets avec moins de replicas.
Tous les restes sont ajoutés au ReplicaSet avec le plus de répliques.
Les ReplicaSets avec zéro réplicas ne sont pas mis à l'échelle.</p></li></ul><p>Dans notre exemple ci-dessus, 3 répliques sont ajoutées à l'ancien ReplicaSet et 2 répliques sont ajoutées au nouveau ReplicaSet.
Le processus de déploiement devrait éventuellement déplacer toutes les répliques vers le nouveau ReplicaSet, en supposant que les nouvelles répliques deviennent saines.
Pour confirmer cela, exécutez:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx-deployment     15        18        7            8           7m
</span></span></code></pre></div><p>Le statut de déploiement confirme la façon dont les réplicas ont été ajoutés à chaque ReplicaSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME                          DESIRED   CURRENT   READY     AGE
</span></span><span style=display:flex><span>nginx-deployment-1989198191   7         7         0         7m
</span></span><span style=display:flex><span>nginx-deployment-618515232    11        11        11        7m
</span></span></code></pre></div><h2 id=pause-et-reprise-d-un-déploiement>Pause et reprise d'un déploiement</h2><p>Vous pouvez suspendre un déploiement avant de déclencher une ou plusieurs mises à jour, puis le reprendre.
Cela vous permet d'appliquer plusieurs correctifs entre la pause et la reprise sans déclencher de déploiements inutiles.</p><ul><li><p>Par exemple, avec un déploiement qui vient d'être créé:
Obtenez les détails du déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx     3         3         3            3           1m
</span></span></code></pre></div><p>Obtenez le statut de déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME               DESIRED   CURRENT   READY     AGE
</span></span><span style=display:flex><span>nginx-2142116321   3         3         3         1m
</span></span></code></pre></div></li><li><p>Mettez le déploiement en pause en exécutant la commande suivante:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout pause deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment paused
</span></span></code></pre></div></li><li><p>Mettez ensuite à jour l'image du déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.9.1
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment image updated
</span></span></code></pre></div></li><li><p>Notez qu'aucun nouveau déploiement n'a commencé:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployments &#34;nginx&#34;
</span></span><span style=display:flex><span>REVISION  CHANGE-CAUSE
</span></span><span style=display:flex><span>1   &lt;none&gt;
</span></span></code></pre></div></li><li><p>Obtenez l'état de déploiement pour vous assurer que le déploiement est correctement mis à jour:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME               DESIRED   CURRENT   READY     AGE
</span></span><span style=display:flex><span>nginx-2142116321   3         3         3         2m
</span></span></code></pre></div></li><li><p>Vous pouvez effectuer autant de mises à jour que vous le souhaitez, par exemple, mettre à jour les ressources qui seront utilisées:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> resources deployment.v1.apps/nginx-deployment -c<span style=color:#666>=</span>nginx --limits<span style=color:#666>=</span><span style=color:#b8860b>cpu</span><span style=color:#666>=</span>200m,memory<span style=color:#666>=</span>512Mi
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment resource requirements updated
</span></span></code></pre></div><p>L'état initial du déploiement avant de le suspendre continuera de fonctionner, mais les nouvelles mises à jour du déploiement n'auront aucun effet tant que le déploiement sera suspendu.</p></li><li><p>Finalement, reprenez le déploiement et observez un nouveau ReplicaSet à venir avec toutes les nouvelles mises à jour:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout resume deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment resumed
</span></span></code></pre></div></li><li><p>Regardez l'état du déploiement jusqu'à ce qu'il soit terminé.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs -w
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME               DESIRED   CURRENT   READY     AGE
</span></span><span style=display:flex><span>nginx-2142116321   2         2         2         2m
</span></span><span style=display:flex><span>nginx-3926361531   2         2         0         6s
</span></span><span style=display:flex><span>nginx-3926361531   2         2         1         18s
</span></span><span style=display:flex><span>nginx-2142116321   1         2         2         2m
</span></span><span style=display:flex><span>nginx-2142116321   1         2         2         2m
</span></span><span style=display:flex><span>nginx-3926361531   3         2         1         18s
</span></span><span style=display:flex><span>nginx-3926361531   3         2         1         18s
</span></span><span style=display:flex><span>nginx-2142116321   1         1         1         2m
</span></span><span style=display:flex><span>nginx-3926361531   3         3         1         18s
</span></span><span style=display:flex><span>nginx-3926361531   3         3         2         19s
</span></span><span style=display:flex><span>nginx-2142116321   0         1         1         2m
</span></span><span style=display:flex><span>nginx-2142116321   0         1         1         2m
</span></span><span style=display:flex><span>nginx-2142116321   0         0         0         2m
</span></span><span style=display:flex><span>nginx-3926361531   3         3         3         20s
</span></span></code></pre></div></li><li><p>Obtenez le statut du dernier déploiement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>NAME               DESIRED   CURRENT   READY     AGE
</span></span><span style=display:flex><span>nginx-2142116321   0         0         0         2m
</span></span><span style=display:flex><span>nginx-3926361531   3         3         3         28s
</span></span></code></pre></div></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Vous ne pouvez pas annuler un déploiement suspendu avant de le reprendre.</div><h2 id=statut-de-déploiement>Statut de déploiement</h2><p>Un déploiement entre dans différents états au cours de son cycle de vie.
Il peut être <a href=#progressing-deployment>progressant</a> lors du déploiement d'un nouveau ReplicaSet, il peut être <a href=#complete-deployment>effectué</a>, ou il peut <a href=#failed-deployment>ne pas progresser</a>.</p><h3 id=progression-du-déploiement>Progression du déploiement</h3><p>Kubernetes marque un déploiement comme <em>progressing</em> lorsqu'une des tâches suivantes est effectuée:</p><ul><li>Le déploiement crée un nouveau ReplicaSet.</li><li>Le déploiement augmente son nouveau ReplicaSet.</li><li>Le déploiement réduit ses anciens ReplicaSet.</li><li>De nouveaux pods deviennent prêts ou disponibles (prêt pour au moins <a href=#min-ready-seconds>MinReadySeconds</a>).</li></ul><p>Vous pouvez surveiller la progression d'un déploiement à l'aide de <code>kubectl rollout status</code>.</p><h3 id=déploiement-effectué>Déploiement effectué</h3><p>Kubernetes marque un déploiement comme <em>effectué</em> lorsqu'il présente les caractéristiques suivantes:</p><ul><li>Toutes les répliques associées au déploiement ont été mises à jour vers la dernière version que vous avez spécifiée, ce qui signifie que toutes les mises à jour que vous avez demandées ont été effectuées.</li><li>Toutes les répliques associées au déploiement sont disponibles.</li><li>Aucune ancienne réplique pour le déploiement n'est en cours d'exécution.</li></ul><p>Vous pouvez vérifier si un déploiement est terminé en utilisant <code>kubectl rollout status</code>.
Si le déploiement s'est terminé avec succès, <code>kubectl rollout status</code> renvoie un code de sortie de 0.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Waiting for rollout to finish: 2 of 3 updated replicas are available...
</span></span><span style=display:flex><span>deployment &#34;nginx-deployment&#34; successfully rolled out
</span></span><span style=display:flex><span>$ echo $?
</span></span><span style=display:flex><span>0
</span></span></code></pre></div><h3 id=déploiement-échoué>Déploiement échoué</h3><p>Votre déploiement peut rester bloqué en essayant de déployer son nouveau ReplicaSet sans jamais terminer.
Cela peut se produire en raison de certains des facteurs suivants:</p><ul><li>Quota insuffisant</li><li>Échecs de la sonde de préparation</li><li>Erreurs d'extraction d'image</li><li>Permissions insuffisantes</li><li>Plages limites</li><li>Mauvaise configuration de l'exécution de l'application</li></ul><p>Vous pouvez détecter cette condition en spécifiant un paramètre d'échéance dans votre spécification de déploiement:
(<a href=#progress-deadline-seconds><code>.spec.progressDeadlineSeconds</code></a>).
<code>.spec.progressDeadlineSeconds</code> indique le nombre de secondes pendant lesquelles le contrôleur de déploiement attend avant d'indiquer (dans l'état de déploiement) que la progression du déploiement est au point mort.</p><p>La commande <code>kubectl</code> suivante définit la spécification avec <code>progressDeadlineSeconds</code> pour que le contrôleur signale l'absence de progression pour un déploiement après 10 minutes:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl patch deployment.v1.apps/nginx-deployment -p <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;progressDeadlineSeconds&#34;:600}}&#39;</span>
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>deployment.apps/nginx-deployment patched
</span></span></code></pre></div><p>Une fois le délai dépassé, le contrôleur de déploiement ajoute un <code>DeploymentCondition</code> avec les attributs suivants aux <code>.status.conditions</code> du déploiement:</p><ul><li>Type=Progressing</li><li>Status=False</li><li>Reason=ProgressDeadlineExceeded</li></ul><p>Voir les <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties>conventions Kubernetes API</a> pour plus d'informations sur les conditions d'état.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes ne prend aucune mesure sur un déploiement bloqué, sauf pour signaler une condition d'état avec <code>Reason=ProgressDeadlineExceeded</code>.
Les orchestrateurs de niveau supérieur peuvent en tirer parti et agir en conséquence, par exemple, restaurer le déploiement vers sa version précédente.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Si vous suspendez un déploiement, Kubernetes ne vérifie pas la progression par rapport à votre échéance spécifiée.
Vous pouvez suspendre un déploiement en toute sécurité au milieu d'un déploiement et reprendre sans déclencher la condition de dépassement du délai.</div><p>Vous pouvez rencontrer des erreurs transitoires avec vos déploiements, soit en raison d'un délai d'attente bas que vous avez défini, soit en raison de tout autre type d'erreur pouvant être traité comme transitoire.
Par exemple, supposons que votre quota soit insuffisant.
Si vous décrivez le déploiement, vous remarquerez la section suivante:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&lt;...&gt;
</span></span><span style=display:flex><span>Conditions:
</span></span><span style=display:flex><span>  Type            Status  Reason
</span></span><span style=display:flex><span>  ----            ------  ------
</span></span><span style=display:flex><span>  Available       True    MinimumReplicasAvailable
</span></span><span style=display:flex><span>  Progressing     True    ReplicaSetUpdated
</span></span><span style=display:flex><span>  ReplicaFailure  True    FailedCreate
</span></span><span style=display:flex><span>&lt;...&gt;
</span></span></code></pre></div><p>Si vous exécutez <code>kubectl get deployment nginx-deployment -o yaml</code>, l'état de déploiement est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>availableReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>conditions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span>2016-10-04T12:25:39Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>lastUpdateTime</span>:<span style=color:#bbb> </span>2016-10-04T12:25:39Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>message</span>:<span style=color:#bbb> </span>Replica set &#34;nginx-deployment-4262182780&#34; is progressing.<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>reason</span>:<span style=color:#bbb> </span>ReplicaSetUpdated<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;True&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Progressing<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span>2016-10-04T12:25:42Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>lastUpdateTime</span>:<span style=color:#bbb> </span>2016-10-04T12:25:42Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>message</span>:<span style=color:#bbb> </span>Deployment has minimum availability.<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>reason</span>:<span style=color:#bbb> </span>MinimumReplicasAvailable<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;True&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Available<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span>2016-10-04T12:25:39Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>lastUpdateTime</span>:<span style=color:#bbb> </span>2016-10-04T12:25:39Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>message: &#39;Error creating</span>:<span style=color:#bbb> </span>pods &#34;nginx-deployment-4262182780-&#34; is forbidden: exceeded quota:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>object-counts, requested: pods=1, used: pods=3, limited</span>:<span style=color:#bbb> </span>pods=2&#39;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>reason</span>:<span style=color:#bbb> </span>FailedCreate<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;True&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>ReplicaFailure<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>observedGeneration</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>unavailableReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Finalement, une fois la date limite de progression du déploiement dépassée, Kubernetes met à jour le statut et la raison de la condition de progression:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Conditions:
</span></span><span style=display:flex><span>  Type            Status  Reason
</span></span><span style=display:flex><span>  ----            ------  ------
</span></span><span style=display:flex><span>  Available       True    MinimumReplicasAvailable
</span></span><span style=display:flex><span>  Progressing     False   ProgressDeadlineExceeded
</span></span><span style=display:flex><span>  ReplicaFailure  True    FailedCreate
</span></span></code></pre></div><p>Vous pouvez résoudre un problème de quota insuffisant en réduisant votre déploiement, en réduisant d'autres contrôleurs que vous exécutez ou en augmentant le quota de votre namespace.
Si vous remplissez les conditions de quota et que le contrôleur de déploiement termine ensuite le déploiement de déploiement, vous verrez la mise à jour de l'état du déploiement avec une condition réussie (<code>Status=True</code> et <code>Reason=NewReplicaSetAvailable</code>).</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Conditions:
</span></span><span style=display:flex><span>  Type          Status  Reason
</span></span><span style=display:flex><span>  ----          ------  ------
</span></span><span style=display:flex><span>  Available     True    MinimumReplicasAvailable
</span></span><span style=display:flex><span>  Progressing   True    NewReplicaSetAvailable
</span></span></code></pre></div><p><code>Type=Available</code> avec <code>Status=True</code> signifie que votre déploiement a une disponibilité minimale.
La disponibilité minimale est dictée par les paramètres spécifiés dans la stratégie de déploiement.
<code>Type=Progressing</code> avec <code>Status=True</code> signifie que votre déploiement est soit au milieu d'un déploiement et qu'il progresse ou qu'il a terminé avec succès sa progression et que les nouvelles répliques minimales requises sont disponibles (voir la raison de la condition pour les détails - dans notre cas, <code>Reason=NewReplicaSetAvailable</code> signifie que le déploiement est terminé).</p><p>Vous pouvez vérifier si un déploiement n'a pas pu progresser en utilisant <code>kubectl rollout status</code>.
<code>kubectl rollout status</code> renvoie un code de sortie différent de zéro si le déploiement a dépassé le délai de progression.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment.v1.apps/nginx-deployment
</span></span></code></pre></div><p>La sortie est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
</span></span><span style=display:flex><span>error: deployment &#34;nginx&#34; exceeded its progress deadline
</span></span><span style=display:flex><span>$ echo $?
</span></span><span style=display:flex><span>1
</span></span></code></pre></div><h3 id=agir-sur-un-déploiement-échoué>Agir sur un déploiement échoué</h3><p>Toutes les actions qui s'appliquent à un déploiement complet s'appliquent également à un déploiement ayant échoué.
Vous pouvez le mettre à l'échelle à la hausse/baisse, revenir à une révision précédente ou même la suspendre si vous devez appliquer plusieurs réglages dans le modèle de pod de déploiement.</p><h2 id=politique-de-nettoyage>Politique de nettoyage</h2><p>Vous pouvez définir le champ <code>.spec.revisionHistoryLimit</code> dans un déploiement pour spécifier le nombre d'anciens ReplicaSets pour ce déploiement que vous souhaitez conserver.
Le reste sera effacé en arrière-plan.
Par défaut, c'est 10.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> La définition explicite de ce champ sur 0 entraînera le nettoyage de tout l'historique de votre déploiement, de sorte que le déploiement ne pourra pas revenir en arrière.</div><h2 id=déploiement-des-canaries>Déploiement des Canaries</h2><p>Si vous souhaitez déployer des versions sur un sous-ensemble d'utilisateurs ou de serveurs à l'aide du déploiement, vous pouvez créer plusieurs déploiements, un pour chaque version, en suivant le modèle canari décrit dans <a href=/docs/concepts/cluster-administration/manage-deployment/#canary-deployments>gestion des ressources</a>.</p><h2 id=écriture-d-une-spécification-de-déploiement>Écriture d'une spécification de déploiement</h2><p>Comme pour toutes les autres configurations Kubernetes, un déploiement a besoin des champs <code>apiVersion</code>, <code>kind</code> et <code>metadata</code>.
Pour des informations générales sur l'utilisation des fichiers de configuration, voir <a href=/docs/tutorials/stateless-application/run-stateless-application-deployment/>déploiement d'applications</a>, configuration des conteneurs, et <a href=/docs/concepts/overview/working-with-objects/object-management/>Utilisation de kubectl pour gérer les ressources</a>.</p><p>Un déploiement nécessite également un <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>Les <code>.spec.template</code> et <code>.spec.selector</code> sont les seuls champs obligatoires du <code>.spec</code>.</p><p>Le <code>.spec.template</code> est un <a href=/fr/docs/concepts/workloads/pods/pod-overview/#pod-templates>Pod template</a>.
Il a exactement le même schéma qu'un <a href=/fr/docs/concepts/workloads/pods/pod/>Pod</a>, sauf qu'il est imbriqué et n'a pas de <code>apiVersion</code> ou de <code>kind</code>.</p><p>En plus des champs obligatoires pour un pod, un Pod Template dans un déploiement doit spécifier des labels appropriées et une stratégie de redémarrage appropriée.
Pour les labels, assurez-vous de ne pas chevaucher l'action d'autres contrôleurs.
Voir <a href=#selector>sélecteur</a>).</p><p>Seulement un <a href=/fr/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>.spec.template.spec.restartPolicy</code></a> égal à <code>Always</code> est autorisé, ce qui est la valeur par défaut s'il n'est pas spécifié.</p><h3 id=répliques>Répliques</h3><p><code>.spec.replicas</code> est un champ facultatif qui spécifie le nombre de pods souhaités.
Il vaut par défaut 1.</p><h3 id=sélecteur>Sélecteur</h3><p><code>.spec.selector</code> est un champ obligatoire qui spécifie un <a href=/docs/concepts/overview/working-with-objects/labels/>sélecteur de labels</a> pour les pods ciblés par ce déploiement.</p><p><code>.spec.selector</code> doit correspondre <code>.spec.template.metadata.labels</code>, ou il sera rejeté par l'API.</p><p>Dans la version d'API <code>apps/v1</code>, <code>.spec.selector</code> et <code>.metadata.labels</code> ne sont pas définis par défaut sur <code>.spec.template.metadata.labels</code> s'ils ne sont pas définis.
Ils doivent donc être définis explicitement.
Notez également que <code>.spec.selector</code> est immuable après la création du déploiement dans <code>apps/v1</code>.</p><p>Un déploiement peut mettre fin aux pods dont les étiquettes correspondent au sélecteur si leur modèle est différent de <code>.spec.template</code> ou si le nombre total de ces pods dépasse <code>.spec.replicas</code>.
Il fait apparaître de nouveaux pods avec <code>.spec.template</code> si le nombre de pods est inférieur au nombre souhaité.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Vous ne devez pas créer d'autres pods dont les labels correspondent à ce sélecteur, soit directement, en créant un autre déploiement, soit en créant un autre contrôleur tel qu'un ReplicaSet ou un ReplicationController.
Si vous le faites, le premier déploiement pense qu'il a créé ces autres pods.
Kubernetes ne vous empêche pas de le faire.</div><p>Si vous avez plusieurs contrôleurs qui ont des sélecteurs qui se chevauchent, les contrôleurs se battront entre eux et ne se comporteront pas correctement.</p><h3 id=stratégie>Stratégie</h3><p><code>.spec.strategy</code> spécifie la stratégie utilisée pour remplacer les anciens pods par de nouveaux.
<code>.spec.strategy.type</code> peut être "Recreate" ou "RollingUpdate".
"RollingUpdate" est la valeur par défaut.</p><h4 id=déploiment-recreate>Déploiment Recreate</h4><p>Tous les pods existants sont tués avant que de nouveaux ne soient créés lorsque <code>.spec.strategy.type==Recreate</code>.</p><h4 id=déploiement-de-mise-à-jour-continue>Déploiement de mise à jour continue</h4><p>Le déploiement met à jour les pods dans une <a href=/docs/tasks/run-application/rolling-update-replication-controller/>mise à jour continue</a> quand <code>.spec.strategy.type==RollingUpdate</code>.
Vous pouvez spécifier <code>maxUnavailable</code> et <code>maxSurge</code> pour contrôler le processus de mise à jour continue.</p><h5 id=max-non-disponible>Max non disponible</h5><p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> est un champ facultatif qui spécifie le nombre maximal de pods qui peuvent être indisponibles pendant le processus de mise à jour.
La valeur peut être un nombre absolu (par exemple, 5) ou un pourcentage des pods souhaités (par exemple, 10%).
Le nombre absolu est calculé à partir du pourcentage en arrondissant vers le bas.
La valeur ne peut pas être 0 si <code>.spec.strategy.rollingUpdate.maxSurge</code> est 0.
La valeur par défaut est 25%.</p><p>Par exemple, lorsque cette valeur est définie sur 30%, l'ancien ReplicaSet peut être réduit à 70% des pods souhaités immédiatement au démarrage de la mise à jour continue.
Une fois que les nouveaux pods sont prêts, l'ancien ReplicaSet peut être réduit davantage, suivi d'une augmentation du nouveau ReplicaSet, garantissant que le nombre total de pods disponibles à tout moment pendant la mise à jour est d'au moins 70% des pods souhaités.</p><h5 id=max-surge>Max Surge</h5><p><code>.spec.strategy.rollingUpdate.maxSurge</code> est un champ facultatif qui spécifie le nombre maximal de pods pouvant être créés sur le nombre de pods souhaité.
La valeur peut être un nombre absolu (par exemple, 5) ou un pourcentage des pods souhaités (par exemple, 10%).
La valeur ne peut pas être 0 si <code>MaxUnavailable</code> est 0.
Le nombre absolu est calculé à partir du pourcentage en arrondissant.
La valeur par défaut est 25%.</p><p>Par exemple, lorsque cette valeur est définie sur 30%, le nouveau ReplicaSet peut être mis à l'échelle immédiatement au démarrage de la mise à jour continue, de sorte que le nombre total d'anciens et de nouveaux pods ne dépasse pas 130% des pods souhaités.
Une fois que les anciens pods ont été détruits, le nouveau ReplicaSet peut être augmenté davantage, garantissant que le nombre total de pods en cours d'exécution à tout moment pendant la mise à jour est au maximum de 130% des pods souhaités.</p><h3 id=progress-deadline-seconds>Progress Deadline Seconds</h3><p><code>.spec.progressDeadlineSeconds</code> est un champ facultatif qui spécifie le nombre de secondes pendant lesquelles vous souhaitez attendre que votre déploiement progresse avant que le système ne signale que le déploiement a <a href=#failed-deployment>échoué</a> - refait surface comme une condition avec <code>Type=Progressing</code>, <code>Status=False</code> et <code>Reason=ProgressDeadlineExceeded</code> dans l'état de la ressource.
Le contrôleur de déploiement continuera de réessayer le déploiement.
À l'avenir, une fois la restauration automatique implémentée, le contrôleur de déploiement annulera un déploiement dès qu'il observera une telle condition.</p><p>S'il est spécifié, ce champ doit être supérieur à <code>.spec.minReadySeconds</code>.</p><h3 id=min-ready-seconds>Min Ready Seconds</h3><p><code>.spec.minReadySeconds</code> est un champ facultatif qui spécifie le nombre minimum de secondes pendant lequel un pod nouvellement créé doit être prêt sans qu'aucun de ses conteneurs ne plante, pour qu'il soit considéré comme disponible.
Cette valeur par défaut est 0 (le pod sera considéré comme disponible dès qu'il sera prêt).
Pour en savoir plus sur le moment où un pod est considéré comme prêt, consultez <a href=/fr/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>Sondes de conteneur</a>.</p><h3 id=rollback-to>Rollback To</h3><p>Le champ <code>.spec.rollbackTo</code> est obsolète dans les versions d'API <code>extensions/v1beta1</code> et <code>apps/v1beta1</code> et n'est plus pris en charge dans les versions d'API commençant par <code>apps/v1beta2</code>.
Utilisez, <code>kubectl rollout undo</code> pour <a href=#revenir-%C3%A0-une-r%C3%A9vision-pr%C3%A9c%C3%A9dente>Revenir à une révision précédente</a>.</p><h3 id=limite-de-l-historique-des-révisions>Limite de l'historique des révisions</h3><p>L'historique de révision d'un déploiement est stocké dans les ReplicaSets qu'il contrôle.</p><p><code>.spec.revisionHistoryLimit</code> est un champ facultatif qui spécifie le nombre d'anciens ReplicaSets à conserver pour permettre la restauration.
Ces anciens ReplicaSets consomment des ressources dans <code>etcd</code> et encombrent la sortie de <code>kubectl get rs</code>.
La configuration de chaque révision de déploiement est stockée dans ses ReplicaSets; par conséquent, une fois un ancien ReplicaSet supprimé, vous perdez la possibilité de revenir à cette révision du déploiement.
Par défaut, 10 anciens ReplicaSets seront conservés, mais sa valeur idéale dépend de la fréquence et de la stabilité des nouveaux déploiements.</p><p>Plus précisément, la définition de ce champ à zéro signifie que tous les anciens ReplicaSets avec 0 réplicas seront nettoyés.
Dans ce cas, un nouveau panneau déroulant Déploiement ne peut pas être annulé, car son historique de révision est nettoyé.</p><h3 id=paused>Paused</h3><p><code>.spec.paused</code> est un champ booléen facultatif pour suspendre et reprendre un déploiement.
La seule différence entre un déploiement suspendu et un autre qui n'est pas suspendu, c'est que toute modification apportée au <code>PodTemplateSpec</code> du déploiement suspendu ne déclenchera pas de nouveaux déploiements tant qu'il sera suspendu.
Un déploiement n'est pas suspendu par défaut lors de sa création.</p><h2 id=alternative-aux-déploiements>Alternative aux déploiements</h2><h3 id=kubectl-rolling-update>kubectl rolling-update</h3><p><a href=/docs/reference/generated/kubectl/kubectl-commands#rolling-update><code>kubectl rolling-update</code></a> met à jour les pods et les ReplicationControllers de la même manière.
Mais les déploiements sont recommandés, car ils sont déclaratifs, côté serveur et ont des fonctionnalités supplémentaires, telles que la restauration de toute révision précédente même après la mise à jour progressive..</p></div><div class=td-content style=page-break-before:always><h1 id=pg-6d72299952c37ca8cc61b416e5bdbcd4>2.3 - StatefulSets</h1><p>StatefulSet est l'objet de l'API de charge de travail utilisé pour gérer des applications avec état (<em>stateful</em>).</p><p>Gère le déploiement et la mise à l'échelle d'un ensemble de <a class=glossary-tooltip title='Le plus petit et le plus simple des objets Kubernetes. Un Pod est un ensemble de conteneurs fonctionnant sur votre cluster.' data-toggle=tooltip data-placement=top href=/fr/docs/concepts/workloads/pods/pod-overview/ target=_blank aria-label=Pods>Pods</a>, <em>et fournit des garanties sur l'ordre et l'unicité</em> de ces Pods.</p><p>Comme un <a class=glossary-tooltip title='Objet API gérant une application répliquée.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Déploiement>Déploiement</a>, un StatefulSet gère des Pods qui sont basés sur une même spécification de conteneur. Contrairement à un Deployment, un StatefulSet maintient une identité pour chacun de ces Pods. Ces Pods sont créés à partir de la même spec, mais ne sont pas interchangeables : chacun a un identifiant persistant qu'il garde à travers tous ses re-scheduling.</p><p>Si vous voulez utiliser des volumes de stockage pour fournir de la persistance à votre charge de travail, vous pouvez utiliser un StatefulSet comme partie de la solution. Même si des Pods individuels d'un StatefulSet sont susceptibles d'échouer, les identifiants persistants des Pods rendent plus facile de faire correspondre les volumes existants aux nouveaux Pods remplaçant ceux ayant échoué.</p><h2 id=utiliser-des-statefulsets>Utiliser des StatefulSets</h2><p>Les StatefulSets sont utiles pour des applications qui nécessitent une ou plusieurs des choses suivantes :</p><ul><li>Des identifiants réseau stables et uniques.</li><li>Un stockage persistant stable.</li><li>Un déploiement et une mise à l'échelle ordonnés et contrôlés.</li><li>Des mises à jour continues (<em>rolling update</em>) ordonnées et automatisées.</li></ul><p>Ci-dessus, stable est synonyme de persistance suite au (re)scheduling de Pods.
Si une application ne nécessite aucun identifiant stable ou de déploiement, suppression ou
mise à l'échelle stables, vous devriez déployer votre application en utilisant un objet de charge de travail
fournissant un ensemble de réplicas sans état (<em>stateless</em>).</p><p>Un <a href=/fr/docs/concepts/workloads/controllers/deployment/>Deployment</a> ou
<a href=/fr/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a> peut être mieux adapté pour vos applications sans état.</p><h2 id=limitations>Limitations</h2><ul><li>Le stockage pour un Pod donné doit être provisionné soit par un <a href=https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md>approvisionneur de PersistentVolume</a> basé sur un <code>storage class</code> donné, soit pré-provisionné par un admin.</li><li>Supprimer et/ou réduire l'échelle d'un StatefulSet à zéro ne supprimera <em>pas</em> les volumes associés avec le StatefulSet. Ceci est fait pour garantir la sécurité des données, ce qui a généralement plus de valeur qu'une purge automatique de toutes les ressources relatives à un StatefulSet.</li><li>Les StatefulSets nécessitent actuellement un <a href=/fr/docs/concepts/services-networking/service/#headless-services>Service Headless</a> qui est responsable de l'identité réseau des Pods. Vous êtes responsable de la création de ce Service.</li><li>Les StatefulSets ne fournissent aucune garantie de la terminaison des pods lorsqu'un StatefulSet est supprimé. Pour avoir une terminaison ordonnée et maîtrisée des pods du StatefulSet, il est possible de réduire l'échelle du StatefulSet à 0 avant de le supprimer.</li><li>Lors de l'utilisation de <a href=#rolling-updates>Rolling Updates</a> avec la
<a href=#politiques-de-gestion-dun-pod>Politique de gestion des Pods</a> par défaut (<code>OrderedReady</code>),
il est possible de tomber dans un état indéfini nécessitant une
<a href=#rollback-forc%C3%A9>intervention manuelle pour réparer</a>.</li></ul><h2 id=composants>Composants</h2><p>L'exemple ci-dessous décrit les composants d'un StatefulSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span>None<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># doit correspondre à .spec.template.metadata.labels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;nginx&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># est 1 par défaut</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># doit correspondre à .spec.selector.matchLabels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>k8s.gcr.io/nginx-slim:0.8<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/usr/share/nginx/html<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeClaimTemplates</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;ReadWriteOnce&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;my-storage-class&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></span></span></code></pre></div><p>Dans l'exemple ci-dessus :</p><ul><li>Un Service Headless, appelé <code>nginx</code>, est utilisé pour contrôler le domaine réseau.</li><li>Le StatefulSet, appelé <code>web</code>, a une Spec indiquant que 3 réplicas du container nginx seront démarrés dans des Pods.</li><li>Le <code>volumeClaimTemplates</code> fournira un stockage stable utilisant des <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a> provisionnés par un approvisionneur de PersistentVolume.</li></ul><p>Le nom d'un objet StatefulSet doit être un
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>nom de sous-domaine DNS</a> valide.</p><h2 id=sélecteur-de-pod>Sélecteur de Pod</h2><p>Vous devez renseigner le champ <code>.spec.selector</code> d'un StatefulSet pour qu'il corresponde aux labels de son <code>.spec.template.metadata.labels</code>. Avant Kubernetes 1.8, le champ <code>.spec.selector</code> était mis par défaut s'il était omis. Pour les versions 1.8 et ultérieures, ne pas spécifier de sélecteur de Pod résulte en une erreur de validation lors de la création du StatefulSet.</p><h2 id=identité-du-pod>Identité du Pod</h2><p>Les Pods d'un StatefulSet ont une identité unique comprenant un ordinal, une identité réseau stable et un stockage stable.
L'identité est accrochée au Pod, indépendamment du noeud sur lequel il est (re)programmé.</p><h3 id=index-ordinal>Index Ordinal</h3><p>Pour un StatefulSet avec N réplicas, chaque Pod du StatefulSet se verra assigné un ordinal entier, de 0 à N-1,
unique sur l'ensemble des pods.</p><h3 id=id-réseau-stable>ID réseau stable</h3><p>Chaque Pod dans un StatefulSet dérive son nom d'hôte du nom du StatefulSet
et de l'ordinal du Pod. Le modèle pour le nom d'hôte généré est
<code>$(nom statefulset)-$(ordinal)</code>. L'exemple ci-dessus créera trois Pods
nommés <code>web-0,web-1,web-2</code>.
Un StatefulSet peut utiliser un <a href=/docs/concepts/services-networking/service/#headless-services>Service Headless</a>
pour contrôler le domaine de ses Pods. Le domaine pris en charge par ce Service prend la forme :
<code>$(nom service).$(namespace).svc.cluster.local</code>, où "cluster.local" est le domaine du cluster.
Chaque fois qu'un Pod est créé, il obtient un sous-domaine DNS correspondant, prenant la forme :
<code>$(nom pod).$(domaine du service gouvernant)</code>, où le service gouvernant est défini par le
champ <code>serviceName</code> du StatefulSet.</p><p>En fonction de la façon dont est configuré le DNS dans votre cluster, vous ne pourrez peut-être pas rechercher immédiatement
le nom DNS d'un pod nouvellement exécuté. Ce problème peut se produire lorsque d'autres clients dans le
cluster ont déjà envoyé des requêtes pour le nom d'hôte du Pod avant sa création.
La mise en cache négative (normale pour le DNS) signifie que les résultats des recherches précédentes ayant échoué sont
mémorisés et réutilisés, même après que le Pod ait démarré, pendant au moins quelques secondes.</p><p>Si vous avez besoin de découvrir les Pods rapidement après leur création, vous avez plusieurs options :</p><ul><li>Interrogez directement l'API Kubernetes (par exemple, à l'aide d'un watch) plutôt que de vous fier aux recherches DNS.</li><li>Réduisez le temps de mise en cache dans votre fournisseur de DNS Kubernetes (cela signifie généralement modifier le ConfigMap de CoreDNS, qui met actuellement en cache pendant 30 secondes).</li></ul><p>Comme mentionné dans la section <a href=#limitations>limitations</a>, vous êtes responsable de
créer le <a href=/docs/concepts/services-networking/service/#headless-services>Service Headless</a>
responsable de l'identité réseau des Pods.</p><p>Voici quelques exemples de choix pour le domaine du cluster, le nom du service,
le nom du StatefulSet et comment cela affecte les noms DNS des pods du StatefulSet.</p><table><thead><tr><th>Domaine Cluster</th><th>Service (ns/nom)</th><th>StatefulSet (ns/nom)</th><th>Domaine StatefulSet</th><th>DNS Pod</th><th>Nom d'hôte</th></tr></thead><tbody><tr><td>cluster.local</td><td>default/nginx</td><td>default/web</td><td>nginx.default.svc.cluster.local</td><td>web-{0..N-1}.nginx.default.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>cluster.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.cluster.local</td><td>web-{0..N-1}.nginx.foo.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>kube.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.kube.local</td><td>web-{0..N-1}.nginx.foo.svc.kube.local</td><td>web-{0..N-1}</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Le domaine cluster sera <code>cluster.local</code> à moins qu'il soit
<a href=/docs/concepts/services-networking/dns-pod-service/>configuré autrement</a>.</div><h3 id=stockage-stable>Stockage stable</h3><p>Kubernetes crée un <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolume</a> pour chaque
VolumeClaimTemplate. Dans l'exemple nginx ci-dessus, chaque Pod se verra affecter un unique PersistentVolume
avec un StorageClass de <code>my-storage-class</code> et 1 Gib de stockage provisionné. Si aucun StorageClass
n'est spécifié, alors le StorageClass par défaut sera utilisé. Lorsqu'un Pod est (re)schedulé
sur un noeud, ses <code>volumeMounts</code> montent les PersistentVolumes associés aux<br>PersistentVolumeClaims. Notez que les PersistentVolumes associés avec les PersistentVolumeClaims des Pods
ne sont pas supprimés lorsque les Pods, ou le StatefulSet, sont supprimés.
Ceci doit être fait manuellement.</p><h3 id=étiquette-du-nom-de-pod>Étiquette du nom de Pod</h3><p>Lorsque le StatefulSet <a class=glossary-tooltip title="Boucle de contrôle surveillant l'état partagé du cluster à travers l'apiserver et effectuant des changements en essayant de déplacer l'état actuel vers l'état désiré." data-toggle=tooltip data-placement=top href=/docs/admin/kube-controller-manager/ target=_blank aria-label=Contrôleur>Contrôleur</a> crée un Pod,
il ajoute une étiquette, <code>statefulset.kubernetes.io/pod-name</code>, renseignée avec le nom du Pod.
Cette étiquette vous permet d'attacher un Service à un Pod spécifique du StatefulSet.</p><h2 id=garanties-de-déploiement-et-de-mise-à-l-échelle>Garanties de déploiement et de mise à l'échelle</h2><ul><li>Pour un StatefulSet avec N réplicas, lorsque les Pods sont déployés, ils sont créés de manière séquentielle, dans l'ordre {0..N-1}.</li><li>Lorsque les Pods sont supprimés, ils sont terminés dans l'ordre inverse, {N-1..0}.</li><li>Avant qu'une opération de mise à l'échelle soit appliquée à un Pod, tous ses prédécesseurs doivent être Running et Ready.</li><li>Avant qu'un Pod soit terminé, tous ses successeurs doivent être complètement arrêtés.</li></ul><p>Le StatefulSet ne devrait pas spécifier un <code>pod.Spec.TerminationGracePeriodSeconds</code> à 0. Cette pratique
est dangereuse et fortement déconseillée. Pour plus d'explications, veuillez vous référer à <a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>forcer la suppression de Pods de StatefulSet</a>.</p><p>Lorsque l'exemple nginx ci-dessus est créé, trois Pods seront déployés dans l'ordre
web-0, web-1, web-2. web-1 ne sera pas déployé avant que web-0 soit
<a href=/fr/docs/concepts/workloads/pods/pod-lifecycle/>Running et Ready</a>, et web-2 ne sera pas déployé avant que
web-1 soit Running et Ready. Si web-0 venait à échouer, après que web-1 soit Running et Ready, mais avant que
web-2 soit lancé, web-2 ne serait pas lancé avant que web-0 soit correctement relancé et redevienne Running et Ready.</p><p>Si un utilisateur venait à mettre à l'échelle l'exemple déployé en patchant le StatefulSet pour que
<code>replicas=1</code>, web-2 serait terminé en premier. web-1 ne serait pas terminé avant que web-2
ne soit complètement arrêté et supprimé. Si web-0 venait à échouer après que web-2 soit terminé et complètement arrêté,
mais avant que web-1 soit terminé, web-1 ne serait pas terminé avant que web-0 soit Running et Ready.</p><h3 id=politiques-de-gestion-d-un-pod>Politiques de gestion d'un Pod</h3><p>Dans Kubernetes 1.7 et ultérieurs, le StatefulSet vous permet d'assouplir ses garanties d'ordre,
tout en préservant ses garanties d'unicité et d'identité via son champ <code>.spec.podManagementPolicy</code>.</p><h4 id=gestion-de-pod-orderedready>Gestion de Pod OrderedReady</h4><p>La gestion de Pod <code>OrderedReady</code> est la valeur par défaut pour les StatefulSets. Il implémente le comportement décrit <a href=#garanties-de-d%C3%A9ploiment-et-de-mise-%C3%A0-l-%C3%A9chelle>ci-dessus</a>.</p><h4 id=gestion-de-pod-parallel>Gestion de Pod Parallel</h4><p>La gestion de Pod <code>Parallel</code> indique au contrôleur de StatefulSet de lancer ou
terminer tous les Pods en parallèle, et de ne pas attendre que les Pods deviennent Running
et Ready ou complètement terminés avant de lancer ou terminer un autre
Pod. Cette option affecte seulement le comportement pour les opérations de mise à l'échelle.
Les mises à jour ne sont pas affectées.</p><h2 id=stratégies-de-mise-à-jour>Stratégies de mise à jour</h2><p>Dans Kubernetes 1.7 et ultérieurs, le champ <code>.spec.updateStrategy</code> d'un StatefulSet vous permet
de configurer et désactiver les rolling updates automatisés pour les conteneurs, étiquettes,
requête/limites de ressources, et annotations pour les Pods d'un StatefulSet.</p><h3 id=on-delete>On Delete</h3><p>La stratégie de mise à jour <code>OnDelete</code> implémente l'ancien comportement (1.6 et précédents). Lorsque
<code>.spec.updateStrategy.type</code> d'un StatefulSet est mis à <code>OnDelete</code>, le contrôleur de StatefulSet
ne mettra pas à jour automatiquement les Pods dans un StatefulSet.
Les utilisateurs doivent supprimer manuellement les Pods pour forcer le contrôleur à créer de nouveaux
Pods qui réflètent les modifications faites à un <code>.spec.template</code> d'un StatefulSet.</p><h3 id=rolling-updates>Rolling Updates</h3><p>La stratégie de mise à jour <code>RollingUpdate</code> implémente le rolling update automatisé pour les Pods d'un
StatefulSet. C'est la stratégie par défaut lorsque <code>.spec.updateStrategy</code> n'est pas spécifié.
Lorsqu'un <code>.spec.updateStrategy.type</code> d'un StatefulSet est mis à <code>RollingUpdate</code>, le contrôleur de
StatefulSet va supprimer et recréer chaque Pod d'un StatefulSet. Il va procéder dans le même ordre
que pour la terminaison d'un Pod (de l'ordinal le plus grand au plus petit), mettant à jour chaque Pod,
un seul à la fois. Il va attendre qu'un Pod mis à jour soit Running et Ready avant de mettre à jour
son prédécesseur.</p><h4 id=partitions>Partitions</h4><p>La stratégie de mise à jour <code>RollingUpdate</code> peut être partitionnée, en spécifiant une
<code>.spec.updateStrategy.rollingUpdate.partition</code>. Si une partition est spécifiée, tous les Pods ayant un
ordinal plus grand ou égal à la partition seront mis à jour lorsque le
<code>.spec.template</code> du StatefulSet sera mis à jour. Tous les Pods ayant un ordinal inférieur à la partition
ne sera pas mis à jour, et, même s'ils sont supprimés, ils seront recréés avec l'ancienne version. Si une
<code>.spec.updateStrategy.rollingUpdate.partition</code> d'un StatefulSet est plus grand que son <code>.spec.replicas</code>,
les mises à jour de son <code>.spec.template</code> ne seront pas propagés à ses Pods.
Dans la plupart des cas vous n'aurez pas à utiliser de partition, mais elles sont utiles si vous désirez
organiser une mise à jour, déployer une version canari, ou effectuer un déploiement par étapes.</p><h4 id=rollback-forcé>Rollback forcé</h4><p>En utilisant des <a href=#rolling-updates>Rolling Updates</a> avec la
<a href=#politiques-de-gestion-dun-pod>politique de gestion d'un Pod</a> par défaut (<code>OrderedReady</code>),
il est possible de se retrouver dans un état inconsistant nécessitant une intervention manuelle pour réparation.</p><p>Si vous mettez à jour le template de Pod dans une configuration qui ne devient jamais Running et
Ready (par exemple, du fait d'un mauvais binaire ou d'une erreur de configuration au niveau de l'application),
le StatefulSet va arrêter le rollout et attendre.</p><p>Dans cet état, il n'est pas suffisant de revenir à une bonne configuration du template de Pod.
En raison d'une <a href=https://github.com/kubernetes/kubernetes/issues/67250>erreur connue</a>,
le StatefulSet va continuer à attendre que le Pod en échec Pod devienne Ready
(ce qui n'arrive jamais) avant qu'il tente de revenir à la bonne configuration.</p><p>Après être revenu au bon template, vous devez aussi supprimer tous les Pods que le StatefulSet
avait déjà essayé de démarrer avec la mauvaise configuration.
Le StatefulSet va alors commencer à recréer les Pods en utilisant le bon template.</p><h2 id=a-suivre>A suivre</h2><ul><li>Suivre un exemple de <a href=/docs/tutorials/stateful-application/basic-stateful-set/>déploiement d'une application stateful</a>.</li><li>Suivre un exemple de <a href=/docs/tutorials/stateful-application/cassandra/>déploiement de Cassandra avec des Stateful Sets</a>.</li><li>Suivre un exemple d'<a href=/docs/tasks/run-application/run-replicated-stateful-application/>exécution d'une application stateful redondante</a>.</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/fr/docs/home/>Accueil</a>
<a class=text-white href=/fr/blog/>Blog</a>
<a class=text-white href=/fr/partners/>Partenaires</a>
<a class=text-white href=/fr/community/>Communauté</a>
<a class=text-white href=/fr/case-studies/>Études de cas</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>