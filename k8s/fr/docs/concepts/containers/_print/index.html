<!doctype html><html lang=fr class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/concepts/containers/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/containers/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/containers/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/containers/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/containers/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/containers/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/containers/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/containers/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/containers/><link rel=alternate hreflang=vi href=https://kubernetes.io/vi/docs/concepts/containers/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/fr/docs/concepts/containers/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Les conteneurs | Kubernetes</title><meta property="og:title" content="Les conteneurs"><meta property="og:description" content="Conteneurs Kubernetes"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/fr/docs/concepts/containers/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Les conteneurs"><meta itemprop=description content="Conteneurs Kubernetes"><meta name=twitter:card content="summary"><meta name=twitter:title content="Les conteneurs"><meta name=twitter:description content="Conteneurs Kubernetes"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Conteneurs Kubernetes"><meta property="og:description" content="Conteneurs Kubernetes"><meta name=twitter:description content="Conteneurs Kubernetes"><meta property="og:url" content="https://kubernetes.io/fr/docs/concepts/containers/"><meta property="og:title" content="Les conteneurs"><meta name=twitter:title content="Les conteneurs"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/fr/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/fr/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/blog/>Blog de Kubernetes</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/partners/>Partenaires</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/community/>Communauté</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/case-studies/>Études de cas</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/fr/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/fr/docs/concepts/containers/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/fr/docs/concepts/containers/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/fr/docs/concepts/containers/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/fr/docs/concepts/containers/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/fr/docs/concepts/containers/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Français (French)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/concepts/containers/>English</a>
<a class=dropdown-item href=/zh-cn/docs/concepts/containers/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/containers/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/containers/>日本語 (Japanese)</a>
<a class=dropdown-item href=/it/docs/concepts/containers/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/containers/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/containers/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/containers/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/containers/>Bahasa Indonesia</a>
<a class=dropdown-item href=/vi/docs/concepts/containers/>Tiếng Việt (Vietnamese)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Version imprimable multipages.
<a href=# onclick="return print(),!1">Cliquer ici pour imprimer</a>.</p><p><a href=/fr/docs/concepts/containers/>Retour à la version par défaut</a>.</p></div><h1 class=title>Les conteneurs</h1><div class=lead>Conteneurs Kubernetes</div><ul><li>1: <a href=#pg-16042b4652ad19e565c7263824029a43>Images</a></li><li>2: <a href=#pg-a858027489648786a3b16264e451272b>Classe d'exécution (Runtime Class)</a></li><li>3: <a href=#pg-643212488f778acf04bebed65ba34441>L'environnement du conteneur</a></li><li>4: <a href=#pg-e6941d969d81540208a3e78bc56f43bc>Hooks de cycle de vie de conteneurs</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-16042b4652ad19e565c7263824029a43>1 - Images</h1><div class=lead>Images conteneur Kubernetes</div><p>Vous créez une image Docker et la poussez dans un registre avant de la référencer depuis un pod Kubernetes.</p><p>La propriété <code>image</code> d'un conteneur utilise la même syntaxe que la commande <code>docker</code>, y compris pour les registres privés et les tags.</p><h2 id=mettre-à-jour-des-images>Mettre à jour des images</h2><p>La politique de récupération par défaut est <code>IfNotPresent</code>, Kubelet ne récupère alors pas une image si elle est déjà présente sur le nœud.
Si vous voulez forcer une récupération à chaque fois, vous pouvez faire une des actions suivantes :</p><ul><li>définissez <code>imagePullPolicy</code> du conteneur à <code>Always</code>.</li><li>omettez <code>imagePullPolicy</code> et utilisez <code>:latest</code> comme tag pour l'image à utiliser.</li><li>omettez <code>imagePullPolicy</code> et le tag de l'image à utiliser.</li><li>activez l'admission controller <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>AlwaysPullImages</a>.</li></ul><p>Notez que vous devez éviter d'utiliser le tag <code>:latest</code>, voir <a href=/docs/concepts/configuration/overview/#container-images>Bonnes pratiques pour la configuration</a> pour plus d'informations.</p><h2 id=créer-des-images-multi-architecture-à-partir-de-manifestes>Créer des images multi-architecture à partir de manifestes</h2><p>La CLI Docker prend maintenant en charge la commande <code>docker manifest</code> avec des sous-commandes comme <code>create</code>, <code>annotate</code> et <code>push</code>. Ces commandes peuvent être utilisées pour construire et pousser les manifestes. Vous pouvez utiliser <code>docker manifest inspect</code> pour voir le manifeste.</p><p>Vous pouvez voir la documentation Docker ici :
<a href=https://docs.docker.com/edge/engine/reference/commandline/manifest/>https://docs.docker.com/edge/engine/reference/commandline/manifest/</a></p><p>Voici comment nous l'utilisons dans notre outil de build:
<a href="https://cs.k8s.io/?q=docker%20manifest%20(create%7Cpush%7Cannotate)&i=nope&files=&repos=">https://cs.k8s.io/?q=docker%20manifest%20(create%7Cpush%7Cannotate)&i=nope&files=&repos=</a></p><p>Ces commandes se basent et sont implémentées purement sur la CLI Docker. Vous devrez soit éditer <code>$HOME/.docker/config.json</code> et définir la clé <code>experimental</code> à <code>enabled</code> ou vous pouvez simplement définir la variable d'environnement <code>DOCKER_CLI_EXPERIMENTAL</code> à <code>enabled</code> lorsque vous appelez les commandes de la CLI.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Veuillez utiliser les versions <em>18.06 ou ultérieure</em>, les versions antérieures ayant des bugs ou ne prenant pas en charge l'option <code>experimental</code> pour la ligne de commande. Par exemple <a href=https://github.com/docker/cli/issues/1135>https://github.com/docker/cli/issues/1135</a> cause des problèmes sous <code>containerd</code>.</div><p>Si vous avez des problèmes en téléchargeant des manifestes viciés, nettoyez les anciens manifestes dans <code>$HOME/.docker/manifests</code> pour recommencer de zéro.</p><p>Pour Kubernetes, nous avons historiquement utilisé des images avec des suffixes <code>-$(ARCH)</code>. Pour une rétrocompatibilité, veuillez générer les anciennes images avec des suffixes. Par exemple, l'image <code>pause</code> qui a le manifeste pour toutes les architetures et l'image <code>pause-amd64</code> qui est rétrocompatible
pour d'anciennes configurations ou des fichiers YAML qui auraient codé en dur les images avec des suffixes.</p><h2 id=utiliser-un-registre-privé>Utiliser un registre privé</h2><p>Les registres privés peuvent demander des clés pour pouvoir lire leurs images.</p><p>Ces certificats peuvent être fournis de différentes manières :</p><ul><li>En utilisant la Google Container Registry<ul><li>par cluster</li><li>automatiqueent configuré dans Google Compute Engine ou Google Kubernetes Engine</li><li>tous les pods peuvent lire le registre privé du projet</li></ul></li><li>En utilisant Amazon Elastic Container Registry (ECR)<ul><li>utilise les rôles et politiques IAM pour contrôler l'accès aux dépôts ECR</li><li>rafraîchit automatiquement les certificats de login ECR</li></ul></li><li>En utilisant Oracle Cloud Infrastructure Registry (OCIR)<ul><li>utilise les rôles et politiques IAM pour contrôler l'accès aux dépôts OCIR</li></ul></li><li>En utilisant Azure Container Registry (ACR)</li><li>En utilisant IBM Cloud Container Registry<ul><li>utilise les rôles et politiques IAM pour contrôler l'accès à l'IBM Cloud Container Registry</li></ul></li><li>En configurant les nœuds pour s'authentifier auprès d'un registre privé<ul><li>tous les pods peuvent lire les registres privés configurés</li><li>nécessite la configuration des nœuds par un administrateur du cluster</li></ul></li><li>En utilisant des images pré-chargées<ul><li>tous les pods peuvent utiliser toutes les images mises en cache sur un nœud</li><li>nécessite l'accès root à tous les nœuds pour la mise en place</li></ul></li><li>En spécifiant ImagePullSecrets dans un Pod<ul><li>seuls les pods fournissant ses propres clés peuvent accéder au registre privé</li></ul></li></ul><p>Chaque option est décrite plus en détails ci-dessous.</p><h3 id=utiliser-la-google-container-registry>Utiliser la Google Container Registry</h3><p>Kubernetes prend en charge nativement la <a href=https://cloud.google.com/tools/container-registry/>Google Container
Registry (GCR)</a>, lorsqu'il s'exécute dans Google Compute
Engine (GCE). Si vous exécutez votre cluster dans GCE ou Google Kubernetes Engine, utilisez simplement le nom complet de l'image (par ex. gcr.io/my_project/image:tag).</p><p>Tous les pods dans un cluster auront un accès en lecture aux images dans le registre.</p><p>Kubelet va s'authentifier auprès de GCR en utilisant le compte de service Google de l'instance.
Le compte de service dans l'instance aura un <code>https://www.googleapis.com/auth/devstorage.read_only</code>,
afin qu'il puisse récupérer depuis le GCR du projet mais qu'il ne puisse pas pousser une image.</p><h3 id=utiliser-amazon-elastic-container-registry>Utiliser Amazon Elastic Container Registry</h3><p>Kubernetes prend en charge nativement <a href=https://aws.amazon.com/ecr/>Amazon Elastic Container Registry</a>, lorsque les nœuds sont des instances de AWS EC2.</p><p>Utilisez simplement le nom complet de l'image (par ex. <code>ACCOUNT.dkr.ecr.REGION.amazonaws.com/imagename:tag</code>)
dans la définition du Pod.</p><p>Tous les utilisateurs du cluster qui peuvent créer des pods auront la possibilité
d'exécuter des pods qui utilisent n'importe quelle image du registre ECR.</p><p>Kubelet va aller chercher et rafraîchir périodiquement les certificats ECR. Les permissions suivantes sont requises par kubelet :</p><ul><li><code>ecr:GetAuthorizationToken</code></li><li><code>ecr:BatchCheckLayerAvailability</code></li><li><code>ecr:GetDownloadUrlForLayer</code></li><li><code>ecr:GetRepositoryPolicy</code></li><li><code>ecr:DescribeRepositories</code></li><li><code>ecr:ListImages</code></li><li><code>ecr:BatchGetImage</code></li></ul><p>Exigences :</p><ul><li>Vous devez utiliser kubelet version <code>v1.2.0</code> ou ultérieure. (exécutez par ex. <code>/usr/bin/kubelet --version=true</code>).</li><li>Si vos nœuds sont dans une région différente de votre registre, vous devez utiliser la version <code>v1.3.0</code> ou ultérieure.</li><li>ECR doit être disponible dans votre région.</li></ul><p>Dépannage :</p><ul><li>Vérifiez toutes les exigences ci-dessus.</li><li>Copiez les certificats de $REGION (par ex. <code>us-west-2</code>) sur votre poste de travail. Connectez-vous en SSH sur l'hôte et exécutez Docker manuellement avec ces certificats. Est-ce que ça marche ?</li><li>Vérifiez que kubelet s'exécute avec <code>--cloud-provider=aws</code>.</li><li>Augmentez la verbosité des logs de kubelet à au moins 3 et recherchez dans les logs de kubelet (par exemple avec <code>journalctl -u kubelet</code>) des lignes similaires à :</li></ul><ul><li><ul><li><code>aws_credentials.go:109] unable to get ECR credentials from cache, checking ECR API</code></li></ul></li><li><ul><li><code>aws_credentials.go:116] Got ECR credentials from ECR API for &lt;AWS account ID for ECR>.dkr.ecr.&lt;AWS region>.amazonaws.com</code></li></ul></li></ul><h3 id=utiliser-azure-container-registry-acr>Utiliser Azure Container Registry (ACR)</h3><p>En utilisant <a href=https://azure.microsoft.com/en-us/services/container-registry/>Azure Container Registry</a>
vous pouvez vous authentifier en utilisant soit un utilisateur admin soit un service principal.
Dans les deux cas, l'authentification est faite via l'authentification standard de Docker. Ces instructions assument l'outil en ligne de commande <a href=https://github.com/azure/azure-cli>azure-cli</a>.</p><p>Vous devez d'abord créer un registre et générer des certificats, la documentation complète pour cela peut être touvée dans la <a href=https://docs.microsoft.com/en-us/azure/container-registry/container-registry-get-started-azure-cli>documentation de Azure container registry</a>.</p><p>Une fois votre registre de conteneurs créé, vous utiliserez les certificats suivants pour vous connecter :</p><ul><li><code>DOCKER_USER</code> : service principal ou utilisateur admin</li><li><code>DOCKER_PASSWORD</code>: mot de passe du service principal ou utilisateur admin</li><li><code>DOCKER_REGISTRY_SERVER</code>: <code>${un-nom-de-registre}.azurecr.io</code></li><li><code>DOCKER_EMAIL</code>: <code>${une-adresse-email}</code></li></ul><p>Une fois que vous avez défini ces variables, vous pouvez
<a href=/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod>configurer un Secret Kubernetes et l'utiliser pour déployer un Pod</a>.</p><h3 id=utiliser-ibm-cloud-container-registry>Utiliser IBM Cloud Container Registry</h3><p>IBM Cloud Container Registry fournit un registre d'images multi-tenant privé que vous pouvez utiliser pour stocker et partager de manière sécurisée vos images. Par défaut, les images de votre registre privé sont scannées par le Vulnerability Advisor intégré pour détecter des failles de sécurité et des vulnérabilités potentielles. Les utilisateurs de votre compte IBM Cloud peuvent accéder à vos images, ou vous pouvez des rôles et politiques IAM pour fournir l'accès aux namespaces de l'IBM Cloud Container Registry.</p><p>Pour installer le plugin du CLI de IBM Cloud Container Registry et créer un namespace pour vos images, voir <a href="https://cloud.ibm.com/docs/Registry?topic=registry-getting-started">Débuter avec IBM Cloud Container Registry</a>.</p><p>Si vous utilisez le même compte et la même région, vous pouvez déployer des images stockées dans IBM Cloud Container Registry vers la namespace <code>default</code> de votre cluster IBM Cloud Kubernetes Service sans configuration supplémentaire, voir <a href="https://cloud.ibm.com/docs/containers?topic=containers-images">Construire des conteneurs à partir d'images</a>. Pour les autres options de configuration, voir <a href="https://cloud.ibm.com/docs/containers?topic=containers-registry#cluster_registry_auth">Comprendre comment autoriser votre cluster à télécharger des images depuis un registre</a>.</p><h3 id=configurer-les-nœuds-pour-s-authentifier-auprès-d-un-registre-privé>Configurer les nœuds pour s'authentifier auprès d'un registre privé</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Si vous travaillez dans Google Kubernetes Engine, vous trouverez un <code>.dockercfg</code> sur chaque nœud avec les certificats pour Google Container Registry. Vous ne pourrez pas utiliser cette méthode.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Si vous travaillez dans AWS EC2 et utilisez EC2 Container Registry (ECR), kubelet sur chaque nœud va gérer et mettre à jour les certificats du login ECR. Vous ne pourrez pas utiliser cette méthode.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Cette méthode est utilisable si vous avez le contrôle sur la configuration des nœuds. Elle ne marchera pas
correctement sur GCE, et sur tout autre fournisseur cloud qui fait du remplacement de nœud automatique.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes prend pour l'instant en charge uniquement les sections <code>auths</code> et <code>HttpHeaders</code> de la config docker. Cela veut dire que les aides aux certificats (<code>credHelpers</code> ou <code>credsStore</code>) ne sont pas pris en charge.</div><p>Docker stocke les clés pour les regisres privés dans le fichier <code>$HOME/.dockercfg</code> ou <code>$HOME/.docker/config.json</code>. Si vous placez le même fichier dans un des chemins de recherche ci-dessous, kubelet l'utilise comme fournisseur de clés lorsque les images sont récupérées.</p><ul><li><code>{--root-dir:-/var/lib/kubelet}/config.json</code></li><li><code>{cwd of kubelet}/config.json</code></li><li><code>${HOME}/.docker/config.json</code></li><li><code>/.docker/config.json</code></li><li><code>{--root-dir:-/var/lib/kubelet}/.dockercfg</code></li><li><code>{cwd of kubelet}/.dockercfg</code></li><li><code>${HOME}/.dockercfg</code></li><li><code>/.dockercfg</code></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Vous pouvez avoir à définir <code>HOME=/root</code> explicitement dans votre fichier d'environnement pour kubelet.</div><p>Voici les étapes recommandées pour configurer vos nœuds pour qu'ils utilisent un registre privé. Dans cet exemple, exécutez-les sur votre poste de travail :</p><ol><li>Exécutez <code>docker login [server]</code> pour chaque jeu de certificats que vous désirez utiliser. Ceci met à jour <code>$HOME/.docker/config.json</code>.</li><li>Examinez <code>$HOME/.docker/config.json</code> dans un éditeur pour vous assurer qu'il contient uniquement les certificats que vous désirez utiliser.</li><li>Récupérez la liste de vos nœuds, par exemple :<ul><li>si vous voulez connaître les noms : <code>nodes=$(kubectl get nodes -o jsonpath='{range.items[*].metadata}{.name} {end}')</code></li><li>si vous voulez connaître les IPs : <code>nodes=$(kubectl get nodes -o jsonpath='{range .items[*].status.addresses[?(@.type=="ExternalIP")]}{.address} {end}')</code></li></ul></li><li>Copiez votre fichier <code>.docker/config.json</code> local dans un des chemins de recherche ci-dessus.<ul><li>par exemple : <code>for n in $nodes; do scp ~/.docker/config.json root@$n:/var/lib/kubelet/config.json; done</code></li></ul></li></ol><p>Vérifiez en créant un pod utilisant une image privée, par ex. :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f - <span style=color:#b44>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: Pod
</span></span></span><span style=display:flex><span><span style=color:#b44>metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>  name: test-image-privee-1
</span></span></span><span style=display:flex><span><span style=color:#b44>spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>  containers:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: utilise-image-privee
</span></span></span><span style=display:flex><span><span style=color:#b44>      image: $NOM_IMAGE_PRIVEE
</span></span></span><span style=display:flex><span><span style=color:#b44>      imagePullPolicy: Always
</span></span></span><span style=display:flex><span><span style=color:#b44>      command: [ &#34;echo&#34;, &#34;SUCCESS&#34; ]
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><pre tabindex=0><code>pod/test-image-privee-1 created
</code></pre><p>Si tout fonctionne, alors, après quelques instants, vous pouvez exécuter :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs test-image-privee-1
</span></span></code></pre></div><p>et voir que la commande affiche :</p><pre tabindex=0><code>SUCCESS
</code></pre><p>Si vous suspectez que la commande a échouée, vous pouvez exécuter :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pods/test-image-privee-1 | grep <span style=color:#b44>&#39;Failed&#39;</span>
</span></span></code></pre></div><p>En cas d'échec, l'affichage sera similaire à :</p><pre tabindex=0><code>  Fri, 26 Jun 2015 15:36:13 -0700    Fri, 26 Jun 2015 15:39:13 -0700    19    {kubelet node-i2hq}    spec.containers{uses-private-image}    failed        Failed to pull image &#34;user/privaterepo:v1&#34;: Error: image user/privaterepo:v1 not found
</code></pre><p>Vous devez vous assurer que tous les nœuds du cluster ont le même fichier <code>.docker/config.json</code>. Dans le cas contraire, les pods vont s'exécuter sur certains nœuds et échouer sur d'autres. Par exemple, si vous utilisez l'autoscaling des nœuds, alors chaque modèle d'instance doit inclure le fichier <code>.docker/config.json</code> ou monter un disque le contenant.</p><p>Tous les pods auront un accès en lecture aux images d'un registre privé dès que les clés du registre privé sont ajoutées au fichier <code>.docker/config.json</code>.</p><h3 id=images-pré-chargées>Images pré-chargées</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Si vous travaillez dans Google Kubernetes Engine, vous trouverez un <code>.dockercfg</code> sur chaque nœud avec les certificats pour Google Container Registry. Vous ne pourrez pas utiliser cette méthode.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Cette méthode est utilisable si vous avez le contrôle sur la configuration des nœuds. Elle ne marchera pas
correctement sur GCE, et sur tout autre fournisseur cloud qui fait du remplacement de nœud automatique.</div><p>Par défaut, kubelet essaiera de récupérer chaque image depuis le registre spécifié.
Cependant, si la propriété <code>imagePullPolicy</code> du conteneur est <code>IfNotPresent</code> ou <code>Never</code>,
alors une image locale est utilisée (respectivement de préférence ou exclusivement).</p><p>Si vous désirez vous reposer sur des images pré-chargées pour éviter l'authentification à un registre,
vous devez vous assurer que tous les nœuds du cluster ont les mêmes images pré-chargées.</p><p>Ceci peut être utilisé pour pré-charger certaines images pour gagner du temps, ou comme une alternative à l'authentification à un registre privé.</p><p>Tous les pods auront un accès en lecture aux images pré-chargées.</p><h3 id=spécifier-imagepullsecrets-dans-un-pod>Spécifier ImagePullSecrets dans un Pod</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Cette méthode est actuellement la méthode recommandée pour Google Kubernetes Engine, GCE, et tout autre fournisseur de cloud où la création de nœuds est automatisée.</div><p>Kubernetes permet de spécifier des clés de registre dans un pod.</p><h4 id=créer-un-secret-avec-une-config-docker>Créer un Secret avec une config Docker</h4><p>Exécutez la commande suivante, en substituant les valeurs en majuscule :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret docker-registry &lt;name&gt; --docker-server<span style=color:#666>=</span>SERVEUR_REGISTRE_DOCKER --docker-username<span style=color:#666>=</span>UTILISATEUR_DOCKER --docker-password<span style=color:#666>=</span>MOT_DE_PASSE_DOCKER --docker-email<span style=color:#666>=</span>EMAIL_DOCKER
</span></span><span style=display:flex><span>secret/myregistrykey created.
</span></span></code></pre></div><p>Si vous avez déjà un fichier de clés Docker, alors, plutôt que d'utiliser la commande ci-dessus,
vous pouvez importer le fichier de clés comme un Secret Kubernetes.
<a href=/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials>Créer un Secret basé sur des clés Docker existantes</a> explique comment s'y prendre.
Ceci est particulièrement utile si vous utilisez plusieurs registres privés, <code>kubectl create secret docker-registry</code> créant un Secret ne fonctionnant qu'avec un seul registre privé.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Les pods peuvent référencer des pull secrets dans leur propre namespace uniquement,
ces étapes doivent donc être faites pour chaque namespace.</div><h4 id=se-référer-à-un-imagepullsecrets-dans-un-pod>Se référer à un imagePullSecrets dans un Pod</h4><p>Vous pouvez maintenant créer des pods qui référencent ce secret en ajoutant une section <code>imagePullSecrets</code>
dans la définition du pod.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: Pod
</span></span></span><span style=display:flex><span><span style=color:#b44>metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>  name: foo
</span></span></span><span style=display:flex><span><span style=color:#b44>  namespace: awesomeapps
</span></span></span><span style=display:flex><span><span style=color:#b44>spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>  containers:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: foo
</span></span></span><span style=display:flex><span><span style=color:#b44>      image: janedoe/awesomeapp:v1
</span></span></span><span style=display:flex><span><span style=color:#b44>  imagePullSecrets:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: myregistrykey
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt;&gt; ./kustomization.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>resources:
</span></span></span><span style=display:flex><span><span style=color:#b44>- pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><p>Ceci doit être fait pour chaque pod utilisant un registre privé.</p><p>Cependant, la définition de ce champ peut être automatisé en définissant <code>imagePullSecrets</code>
dans une ressource <a href=/docs/user-guide/service-accounts>serviceAccount</a>.
Voyez <a href=/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account>Ajouter un ImagePullSecrets à un Service Account</a> pour des instructions détaillées.</p><p>Vous pouvez utiliser cette méthode en conjonction avec un <code>.docker/config.json</code> par nœud. Les certificats seront alors regroupés. Cette approche fonctionnera dans Google Kubernetes Engine.</p><h3 id=cas-d-utilisation>Cas d'utilisation</h3><p>Il y a plusieurs solutions pour configurer des registres privés. Voici quelques cas d'utilisation classiques et des propositions de solutions.</p><ol><li>Cluster exécutant uniquement des images non propriétaires (par ex. open-source). Inutile de protéger les images.<ul><li>Utilisez des images publiques dans le Hub Docker.<ul><li>Pas de configuration requise.</li><li>Dans GCE/Google Kubernetes Engine, un miroir local est automatiquement utilisé pour améliorer la vitesse et la disponibilité.</li></ul></li></ul></li><li>Cluster exécutant quelques images propriétaires qui doivent être protégées de l'extérieur de l'entreprise, mais visibles pour tous les utilisteurs du cluster.<ul><li>Utilisez un <a href=https://docs.docker.com/registry/>registre Docker</a> hébergé privé.<ul><li>Il peut être hébergé sur le <a href=https://hub.docker.com/signup>Hub Docker</a>, ou ailleurs.</li><li>Configurez manuellement .docker/config.json sur caque nœud comme décrit ci-dessus.</li></ul></li><li>Ou, utilisez un registre privé interne derrière votre pare-feu avec un accès ouvert en lecture.<ul><li>Aucune configuration Kubernetes n'est nécessaire.</li></ul></li><li>Ou, dans GCE/Google Kubernetes Engine, utilisez le Google Container Registry du projet.<ul><li>Cela fonctionnera mieux pour l'autoscaling du cluster que la configuration manuelle des nœuds.</li></ul></li><li>Ou, dans un cluster où le changement de la configuration des nœuds est difficile, utilisez <code>imagePullSecrets</code>.</li></ul></li><li>Cluster avec des images propriétaires, dont quelques-unes nécessitent un contrôle d'accès plus strict.<ul><li>Assurez-vous que <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>l'admission controller AlwaysPullImages</a> est actif. Autrement, tous les Pods ont potentiellement accès à toutes les images.</li><li>Déplacez les données sensibles dans une ressource "Secret", plutôt que de les intégrer dans une image.</li></ul></li><li>Un cluster multi-tenant où chaque <em>tenant</em> doit avoir son propre registre privé.<ul><li>Assurez-vous que <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>l'admission controller AlwaysPullImages</a> est actif. Autrement, tous les Pods ont potentiellement accès à toutes les images.</li><li>Utilisez un registre privé nécessitant l'autorisation.</li><li>Générez des certificats de registre pour chaque <em>tenant</em>, placez-les dans des secrets, et placez ces secrets dans les namespaces de chaque <em>tenant</em>.
pod - Le <em>tenant</em> ajoute ce secret dans les imagePullSecrets de chaque pod.</li></ul></li></ol><p>Si vous devez accéder à plusieurs registres, vous pouvez créer un secret pour chaque registre.
Kubelet va fusionner tous les <code>imagePullSecrets</code> dans un unique <code>.docker/config.json</code> virtuel.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-a858027489648786a3b16264e451272b>2 - Classe d'exécution (Runtime Class)</h1><div class=lead>Classe d'execution conteneur pour Kubernetes</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.12 [alpha]</code></div><p>Cette page décrit la ressource RuntimeClass et le mécanisme de sélection d'exécution (runtime).</p><h2 id=runtime-class>Runtime Class</h2><p>La RuntimeClass est une fonctionnalité alpha permettant de sélectionner la configuration d'exécution du conteneur
à utiliser pour exécuter les conteneurs d'un pod.</p><h3 id=installation>Installation</h3><p>En tant que nouvelle fonctionnalité alpha, certaines étapes de configuration supplémentaires doivent
être suivies pour utiliser la RuntimeClass:</p><ol><li>Activer la fonctionnalité RuntimeClass (sur les apiservers et les kubelets, nécessite la version 1.12+)</li><li>Installer la RuntimeClass CRD</li><li>Configurer l'implémentation CRI sur les nœuds (dépend du runtime)</li><li>Créer les ressources RuntimeClass correspondantes</li></ol><h4 id=1-activer-runtimeclass-feature-gate-portail-de-fonctionnalité>1. Activer RuntimeClass feature gate (portail de fonctionnalité)</h4><p>Voir <a href=/docs/reference/command-line-tools-reference/feature-gates/>Feature Gates</a> pour une explication
sur l'activation des feature gates. La <code>RuntimeClass</code> feature gate doit être activée sur les API servers <em>et</em>
les kubelets.</p><h4 id=2-installer-la-crd-runtimeclass>2. Installer la CRD RuntimeClass</h4><p>La RuntimeClass <a href=/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/>CustomResourceDefinition</a> (CRD) se trouve dans le répertoire addons du dépôt
Git Kubernetes: <a href=https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/runtimeclass/runtimeclass_crd.yaml>kubernetes/cluster/addons/runtimeclass/runtimeclass_crd.yaml</a></p><p>Installer la CRD avec <code>kubectl apply -f runtimeclass_crd.yaml</code>.</p><h4 id=3-configurer-l-implémentation-cri-sur-les-nœuds>3. Configurer l'implémentation CRI sur les nœuds</h4><p>Les configurations à sélectionner avec RuntimeClass dépendent de l'implémentation CRI. Consultez
la documentation correspondante pour votre implémentation CRI pour savoir comment le configurer.
Comme c'est une fonctionnalité alpha, tous les CRI ne prennent pas encore en charge plusieurs RuntimeClasses.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> La RuntimeClass suppose actuellement une configuration de nœud homogène sur l'ensemble du cluster
(ce qui signifie que tous les nœuds sont configurés de la même manière en ce qui concerne les environnements d'exécution de conteneur). Toute hétérogénéité (configuration variable) doit être
gérée indépendamment de RuntimeClass via des fonctions de planification (scheduling features) (voir <a href=/docs/concepts/configuration/assign-pod-node/>Affectation de pods sur les nœuds</a>).</div><p>Les configurations ont un nom <code>RuntimeHandler</code> correspondant , référencé par la RuntimeClass.
Le RuntimeHandler doit être un sous-domaine DNS valide selon la norme RFC 1123 (alphanumériques + <code>-</code> et <code>.</code> caractères).</p><h4 id=4-créer-les-ressources-runtimeclass-correspondantes>4. Créer les ressources RuntimeClass correspondantes</h4><p>Les configurations effectuées à l'étape 3 doivent chacune avoir un nom <code>RuntimeHandler</code> associé, qui
identifie la configuration. Pour chaque RuntimeHandler (et optionellement les handlers vides <code>""</code>),
créez un objet RuntimeClass correspondant.</p><p>La ressource RuntimeClass ne contient actuellement que 2 champs significatifs: le nom RuntimeClass
(<code>metadata.name</code>) et le RuntimeHandler (<code>spec.runtimeHandler</code>). la définition de l'objet ressemble à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1alpha1 <span style=color:#bbb> </span><span style=color:#080;font-style:italic># La RuntimeClass est définie dans le groupe d&#39;API node.k8s.io</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myclass <span style=color:#bbb> </span><span style=color:#080;font-style:italic># Le nom avec lequel la RuntimeClass sera référencée</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># La RuntimeClass est une ressource non cantonnées à un namespace</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeHandler</span>:<span style=color:#bbb> </span>myconfiguration <span style=color:#bbb> </span><span style=color:#080;font-style:italic># Le nom de la configuration CRI correspondante</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Il est recommandé de limiter les opérations d'écriture sur la RuntimeClass (create/update/patch/delete) à
l'administrateur du cluster. C'est la configuration par défault. Voir <a href=/docs/reference/access-authn-authz/authorization/>Vue d'ensemble d'autorisation</a> pour plus de détails.</div><h3 id=usage>Usage</h3><p>Une fois que les RuntimeClasses sont configurées pour le cluster, leur utilisation est très simple.
Spécifiez <code>runtimeClassName</code> dans la spécficiation du pod. Par exemple:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>myclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># ...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Cela indiquera à la kubelet d'utiliser la RuntimeClass spécifiée pour exécuter ce pod. Si la
RuntimeClass n'existe pas, ou si la CRI ne peut pas exécuter le handler correspondant, le pod passera finalement à
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase>l'état</a> <code>failed</code>. Recherchez
<a href=/docs/tasks/debug-application-cluster/debug-application-introspection/>l'événement</a> correspondant pour un
message d'erreur.</p><p>Si aucun <code>runtimeClassName</code> n'est spécifié, le RuntimeHandler par défault sera utilisé, qui équivaut
au comportement lorsque la fonctionnalité RuntimeClass est désactivée.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-643212488f778acf04bebed65ba34441>3 - L'environnement du conteneur</h1><div class=lead>L'environnement du conteneur Kubernetes</div><p>Cette page décrit les ressources disponibles pour les conteneurs dans l'environnement de conteneur.</p><h2 id=l-environnement-du-conteneur>L'environnement du conteneur</h2><p>L’environnement Kubernetes conteneur fournit plusieurs ressources importantes aux conteneurs:</p><ul><li>Un système de fichier, qui est une combinaison d'une <a href=/docs/concepts/containers/images/>image</a> et un ou plusieurs <a href=/docs/concepts/storage/volumes/>volumes</a>.</li><li>Informations sur le conteneur lui-même.</li><li>Informations sur les autres objets du cluster.</li></ul><h3 id=informations-sur-le-conteneur>Informations sur le conteneur</h3><p>Le nom d'<em>hôte</em> d'un conteneur est le nom du pod dans lequel le conteneur est en cours d'exécution.
Il est disponible via la commande <code>hostname</code> ou
<a href=http://man7.org/linux/man-pages/man2/gethostname.2.html><code>gethostname</code></a>
dans libc.</p><p>Le nom du pod et le namespace sont disponibles en tant que variables d'environnement via
<a href=/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/>l'API downward</a>.</p><p>Les variables d'environnement définies par l'utilisateur à partir de la définition de pod sont également disponibles pour le conteneur,
de même que toutes les variables d'environnement spécifiées de manière statique dans l'image Docker.</p><h3 id=informations-sur-le-cluster>Informations sur le cluster</h3><p>Une liste de tous les services en cours d'exécution lors de la création d'un conteneur est disponible pour ce conteneur en tant que variables d'environnement.
Ces variables d'environnement correspondent à la syntaxe des liens Docker.</p><p>Pour un service nommé <em>foo</em> qui correspond à un conteneur <em>bar</em>,
les variables suivantes sont définies:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_HOST</span><span style=color:#666>=</span>&lt;l<span>&#39;</span>hôte sur lequel le service est exécuté&gt;
</span></span><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_PORT</span><span style=color:#666>=</span>&lt;le port sur lequel le service fonctionne&gt;
</span></span></code></pre></div><p>Les services ont des adresses IP dédiées et sont disponibles pour le conteneur avec le DNS,
si le <a href=http://releases.k8s.io/master/cluster/addons/dns/>module DNS</a> est activé. </p><h2 id=a-suivre>A suivre</h2><ul><li>En savoir plus sur <a href=/docs/concepts/containers/container-lifecycle-hooks/>les hooks du cycle de vie d'un conteneur</a>.</li><li>Acquérir une expérience pratique
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>en attachant les handlers aux événements du cycle de vie du conteneur</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e6941d969d81540208a3e78bc56f43bc>4 - Hooks de cycle de vie de conteneurs</h1><p>Cette page décrit comment un conteneur pris en charge par kubelet peut utiliser
le framework de Hooks de cycle de vie de conteneurs pour exécuter du code déclenché par des
événements durant son cycle de vie.</p><h2 id=aperçu>Aperçu</h2><p>De manière similaire à quantité de frameworks de langages de programmation qui ont des hooks
de cycle de vie de composants, comme Angular, Kubernetes fournit aux conteneurs
des hooks de cycle de vie.
Les hooks permettent à un conteneur d'être au courant d'événements durant son cycle de vie
et d'exécuter du code implémenté dans un handler lorsque le hook de cycle de vie correspondant
est exécuté.</p><h2 id=hooks-de-conteneurs>Hooks de conteneurs</h2><p>Il existe deux hooks exposés aux conteneurs :</p><p><code>PostStart</code></p><p>Ce hook s'exécute immédiatement après qu'un conteneur soit créé.
Cependant, il n'y a aucune garantie que le hook s'exécute avant l'ENTRYPOINT du conteneur.
Aucun paramètre n'est passé au handler.</p><p><code>PreStop</code></p><p>Ce hook est appelé immédiatement avant qu'un conteneur se termine, en raison d'un appel à l'API
ou d'un événement comme un échec de la liveness probe, un droit de préemption, un conflit de ressources ou autres.
Un appel au hook preStop échoue si le conteneur est déjà dans l'état terminé ou complété.
Il est bloquant, ce qui veut dire qu'il est synchrone, et doit donc se terminer avant que l'appel pour supprimer le conteneur soit envoyé.
Aucun paramètre n'est passé au handler.</p><p>Une description plus précise du comportement de l'arrêt peut être trouvé dans
<a href=/fr/docs/concepts/workloads/pods/pod/#arr%C3%AAt-de-pods>Arrêt de Pods</a>.</p><h3 id=implémentation-d-un-handler-de-hook>Implémentation d'un handler de hook</h3><p>Les conteneurs peuvent accéder à un hook en implémentant et enregistrant un handler pour ce hook.
Il existe deux types de handlers de hook pouvant être implémentés pour des conteneurs :</p><ul><li>Exec - Exécute une commande donnée, comme <code>pre-stop.sh</code>, dans les cgroups et namespaces du conteneur.
Les ressources consommées par la commande sont comptabilisées pour le conteneur.</li><li>HTTP - Exécute une requête HTTP sur un endpoint spécifique du conteneur.</li></ul><h3 id=exécution-d-un-handler-de-hook>Exécution d'un handler de hook</h3><p>Lorsqu'un hook de cycle de vie de conteneur est appelé,
le système de gestion de Kubernetes exécute le handler dans le conteneur enregistré
pour ce hook.</p><p>Les appels aux handlers de hook sont synchrones dans le contexte du pod contenant le conteneur.
Ceci veut dire que pour un hook <code>PostStart</code>,
bien que l'ENTRYPOINT du conteneur et le hook soient lancés de manière asynchrone, si le hook prend trop de temps à s'exécuter ou se bloque,
le conteneur ne peut pas atteindre l'état <code>running</code>.</p><p>Le comportement est similaire pour un hook <code>PreStop</code>.
Si le hook se bloque durant l'exécution,
la phase du Pod reste en état <code>Terminating</code> et le hook est tué après <code>terminationGracePeriodSeconds</code> que le pod se termine.
Si un hook <code>PostStart</code> ou <code>PreStop</code> échoue,
le conteneur est tué.</p><p>Les utilisateurs doivent rendre leurs handlers de hook aussi légers que possible.
Il existe des cas, cependant, où de longues commandes ont un intérêt,
comme pour enregistrer un état avant de stopper un conteneur.</p><h3 id=garanties-de-déclenchement-d-un-hook>Garanties de déclenchement d'un hook</h3><p>La politique de déclenchement d'un hook est <em>au moins une fois</em>,
ce qui veut dire qu'un hook peut être déclenché plus d'une fois pour un événement donné,
comme <code>PostStart</code> ou <code>PreStop</code>.
Il appartient à l'implémentation du hook de prendre en compte correctement ce comportement.</p><p>En général, un seul déclenchement est fait.
Si, par exemple, un récepteur de hook HTTP est hors service et ne peut pas
prendre en charge du trafic, il n'y a aucune tentative de renvoi.
Dans quelques rares cas, cependant, un double envoi peut se produire.
Par exemple, si kubelet redémarre au milieu d'un déclenchement de hook,
le hook pourrait être re-déclenché après que kubelet redémarre.</p><h3 id=débugger-des-handlers-de-hook>Débugger des handlers de hook</h3><p>Les logs pour un handler de hook ne sont pas exposés dans les événements du Pod.
Si un handler échoue pour une raison particulière, il envoie un événement.
Pour <code>PostStart</code>, c'est l'événement <code>FailedPostStartHook</code>
et pour <code>PreStop</code>, c'est l'événement <code>FailedPreStopHook</code>.
Vous pouvez voir ces événements en exécutant <code>kubectl describe pod &lt;pod_name></code>.
Voici un exemple d'affichage d'événements lors de l'exécution de cette commande :</p><pre tabindex=0><code>Events:
  FirstSeen  LastSeen  Count  From                                                   SubObjectPath          Type      Reason               Message
  ---------  --------  -----  ----                                                   -------------          --------  ------               -------
  1m         1m        1      {default-scheduler }                                                          Normal    Scheduled            Successfully assigned test-1730497541-cq1d2 to gke-test-cluster-default-pool-a07e5d30-siqd
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulling              pulling image &#34;test:1.0&#34;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Created              Created container with docker id 5c6a256a2567; Security:[seccomp=unconfined]
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Pulled               Successfully pulled image &#34;test:1.0&#34;
  1m         1m        1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Started              Started container with docker id 5c6a256a2567
  38s        38s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 5c6a256a2567: PostStart handler: Error executing in Docker Container: 1
  37s        37s       1      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Normal    Killing              Killing container with docker id 8df9fdfd7054: PostStart handler: Error executing in Docker Container: 1
  38s        37s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}                         Warning   FailedSync           Error syncing pod, skipping: failed to &#34;StartContainer&#34; for &#34;main&#34; with RunContainerError: &#34;PostStart handler: Error executing in Docker Container: 1&#34;
  1m         22s       2      {kubelet gke-test-cluster-default-pool-a07e5d30-siqd}  spec.containers{main}  Warning   FailedPostStartHook
</code></pre><h2 id=a-suivre>A suivre</h2><ul><li>En savoir plus sur l'<a href=/fr/docs/concepts/containers/container-environment/>Environnement d'un conteneur</a>.</li><li>Entraînez-vous à
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>attacher des handlers de conteneurs à des événements de cycle de vie</a>.</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/fr/docs/home/>Accueil</a>
<a class=text-white href=/fr/blog/>Blog</a>
<a class=text-white href=/fr/partners/>Partenaires</a>
<a class=text-white href=/fr/community/>Communauté</a>
<a class=text-white href=/fr/case-studies/>Études de cas</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>