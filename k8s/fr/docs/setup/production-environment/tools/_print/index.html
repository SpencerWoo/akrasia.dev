<!doctype html><html lang=fr class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=en href=https://kubernetes.io/docs/setup/production-environment/tools/><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/production-environment/tools/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/production-environment/tools/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/production-environment/tools/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/production-environment/tools/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/fr/docs/setup/production-environment/tools/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Installer Kubernetes avec les outils de déploiement | Kubernetes</title><meta property="og:title" content="Installer Kubernetes avec les outils de déploiement"><meta property="og:description" content="Solution professionnelle d’orchestration de conteneurs"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/fr/docs/setup/production-environment/tools/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Installer Kubernetes avec les outils de déploiement"><meta itemprop=description content="Solution professionnelle d’orchestration de conteneurs"><meta name=twitter:card content="summary"><meta name=twitter:title content="Installer Kubernetes avec les outils de déploiement"><meta name=twitter:description content="Solution professionnelle d’orchestration de conteneurs"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/fr/docs/setup/production-environment/tools/"><meta property="og:title" content="Installer Kubernetes avec les outils de déploiement"><meta name=twitter:title content="Installer Kubernetes avec les outils de déploiement"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/fr/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/fr/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/blog/>Blog de Kubernetes</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/partners/>Partenaires</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/community/>Communauté</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/fr/case-studies/>Études de cas</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/fr/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/fr/docs/setup/production-environment/tools/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/fr/docs/setup/production-environment/tools/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/fr/docs/setup/production-environment/tools/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/fr/docs/setup/production-environment/tools/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/fr/docs/setup/production-environment/tools/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Français (French)</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/docs/setup/production-environment/tools/>English</a>
<a class=dropdown-item href=/zh-cn/docs/setup/production-environment/tools/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/setup/production-environment/tools/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/setup/production-environment/tools/>日本語 (Japanese)</a>
<a class=dropdown-item href=/id/docs/setup/production-environment/tools/>Bahasa Indonesia</a>
<a class=dropdown-item href=/uk/docs/setup/production-environment/tools/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>Version imprimable multipages.
<a href=# onclick="return print(),!1">Cliquer ici pour imprimer</a>.</p><p><a href=/fr/docs/setup/production-environment/tools/>Retour à la version par défaut</a>.</p></div><h1 class=title>Installer Kubernetes avec les outils de déploiement</h1><ul><li>1: <a href=#pg-a16f59f325a17cdeed324d5c889f7f73>Déploiement d'un cluster avec kubeadm</a></li><ul><li>1.1: <a href=#pg-29e59491dd6118b23072dfe9ebb93323>Installer kubeadm</a></li><li>1.2: <a href=#pg-134ed1f6142a98e6ac681a1ba4920e53>Création d'un Cluster a master unique avec kubeadm</a></li><li>1.3: <a href=#pg-4c656c5eda3e1c06ad1aedebdc04a211>Personnalisation de la configuration du control plane avec kubeadm</a></li><li>1.4: <a href=#pg-015edbc7cc688d31b1d1edce7c186135>Options pour la topologie en haute disponibilité</a></li><li>1.5: <a href=#pg-3941d5c3409342219bf7e03128b8ecb6>Création de clusters hautement disponibles avec kubeadm</a></li><li>1.6: <a href=#pg-8160424c22d24f7d2d63c521e107dbf8>Configurer un cluster etcd en haute disponibilité avec kubeadm</a></li><li>1.7: <a href=#pg-07709e71de6b4ac2573041c31213dbeb>Configuration des kubelet de votre cluster avec kubeadm</a></li><li>1.8: <a href=#pg-c3689df4b0c61a998e79d91a865aa244>Dépanner kubeadm</a></li></ul></ul><div class=content></div></div><div class=td-content><h1 id=pg-a16f59f325a17cdeed324d5c889f7f73>1 - Déploiement d'un cluster avec kubeadm</h1></div><div class=td-content><h1 id=pg-29e59491dd6118b23072dfe9ebb93323>1.1 - Installer kubeadm</h1><p><img src=/images/kubeadm-stacked-color.png align=right width=150px>Cette page vous apprend comment installer la boîte à outils <code>kubeadm</code>.
Pour plus d'informations sur la création d'un cluster avec kubeadm, une fois que vous avez effectué ce processus d'installation, voir la page: <a href=/fr/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>Utiliser kubeadm pour créer un cluster</a>.</p><h2 id=pré-requis>Pré-requis</h2><ul><li>Une ou plusieurs machines exécutant:<ul><li>Ubuntu 16.04+</li><li>Debian 9+</li><li>CentOS 7</li><li>Red Hat Enterprise Linux (RHEL) 7</li><li>Fedora 25+</li><li>HypriotOS v1.0.1+</li><li>Flatcar Container Linux (testé avec 2512.3.0)</li></ul></li><li>2 Go ou plus de RAM par machine (toute quantité inférieure laissera peu de place à vos applications)</li><li>2 processeurs ou plus</li><li>Connectivité réseau complète entre toutes les machines du cluster (réseau public ou privé)</li><li>Nom d'hôte, adresse MAC et product_uuid uniques pour chaque nœud. Voir <a href=#verify-the-mac-address-and-product-uuid-are-unique-for-every-node>ici</a> pour plus de détails.</li><li>Certains ports doivent êtres ouverts sur vos machines. Voir <a href=#check-required-ports>ici</a> pour plus de détails.</li><li>Swap désactivé. Vous <strong>devez</strong> impérativement désactiver le swap pour que la kubelet fonctionne correctement.</li></ul><h2 id=verify-mac-address>Vérifiez que les adresses MAC et product_uuid sont uniques pour chaque nœud</h2><ul><li>Vous pouvez obtenir l'adresse MAC des interfaces réseau en utilisant la commande <code>ip link</code> ou<code> ifconfig -a</code></li><li>Le product_uuid peut être vérifié en utilisant la commande <code>sudo cat /sys/class/dmi/id/product_uuid</code></li></ul><p>Il est très probable que les périphériques matériels aient des adresses uniques, bien que
certaines machines virtuelles puissent avoir des valeurs identiques. Kubernetes utilise ces valeurs pour identifier de manière unique les nœuds du cluster.
Si ces valeurs ne sont pas uniques à chaque nœud, le processus d'installation
peut <a href=https://github.com/kubernetes/kubeadm/issues/31>échouer</a>.</p><h2 id=vérifiez-les-cartes-réseaux>Vérifiez les cartes réseaux</h2><p>Si vous avez plusieurs cartes réseaux et que vos composants Kubernetes ne sont pas accessibles par la route par défaut,
nous vous recommandons d’ajouter une ou plusieurs routes IP afin que les adresses de cluster Kubernetes soient acheminées via la carte approprié.</p><h2 id=permettre-à-iptables-de-voir-le-trafic-ponté>Permettre à iptables de voir le trafic ponté</h2><p>Assurez-vous que le module <code>br_netfilter</code> est chargé. Cela peut être fait en exécutant <code>lsmod | grep br_netfilter</code>. Pour le charger explicitement, appelez <code>sudo modprobe br_netfilter</code>.</p><p>Pour que les iptables de votre nœud Linux voient correctement le trafic ponté, vous devez vous assurer que <code>net.bridge.bridge-nf-call-iptables</code> est défini sur 1 dans votre configuration<code> sysctl</code>, par ex.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-iptables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>sudo sysctl --system
</span></span></code></pre></div><p>Pour plus de détails, veuillez consulter la page <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>Configuration requise pour le plug-in réseau</a>.</p><h2 id=check-required-ports>Vérifiez les ports requis</h2><h3 id=nœuds-maîtres-masters>nœuds maîtres (masters)</h3><table><thead><tr><th>Protocole</th><th>Direction</th><th>Plage de Port</th><th>Utilisé pour</th><th>Utilisé par</th></tr></thead><tbody><tr><td>TCP</td><td>Entrant</td><td>6443*</td><td>Kubernetes API server</td><td>Tous</td></tr><tr><td>TCP</td><td>Entrant</td><td>2379-2380</td><td>Etcd server client API</td><td>kube-apiserver, etcd</td></tr><tr><td>TCP</td><td>Entrant</td><td>10250</td><td>Kubelet API</td><td>Lui-même, Control plane</td></tr><tr><td>TCP</td><td>Entrant</td><td>10251</td><td>kube-scheduler</td><td>Lui-même</td></tr><tr><td>TCP</td><td>Entrant</td><td>10252</td><td>kube-controller-manager</td><td>Lui-même</td></tr></tbody></table><h3 id=nœuds-workers>nœuds workers</h3><table><thead><tr><th>Protocole</th><th>Direction</th><th>Plage de Port</th><th>Utilisé pour</th><th>Utilisé par</th></tr></thead><tbody><tr><td>TCP</td><td>Entrant</td><td>10250</td><td>Kubelet API</td><td>Lui-même, Control plane</td></tr><tr><td>TCP</td><td>Entrant</td><td>30000-32767</td><td>NodePort Services**</td><td>Eux-mêmes</td></tr></tbody></table><p>** Plage de ports par défaut pour les <a href=/docs/concepts/services-networking/service/>Services NodePort</a>.</p><p>Tous les numéros de port marqués d'un * sont écrasables. Vous devrez donc vous assurer que
les ports personnalisés que vous utilisez sont également ouverts.</p><p>Bien que les ports etcd soient inclus dans les nœuds masters, vous pouvez également héberger
votre propre cluster etcd en externe ou sur des ports personnalisés.</p><p>Le plug-in de réseau de pod que vous utilisez (voir ci-dessous) peut également nécessiter certains ports à ouvrir.
Étant donné que cela diffère d’un plugin à l’autre, veuillez vous reporter à la
documentation des plugins sur le(s) port(s) requis(s).</p><h2 id=installing-runtime>Installation du runtime</h2><p>Pour exécuter des conteneurs dans des pods, Kubernetes utilise un
<a class=glossary-tooltip title="L'environnement d'exécution de conteneurs est le logiciel responsable de l'exécution des conteneurs." data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>.</p><ul class="nav nav-tabs" id=container-runtime role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#container-runtime-0 role=tab aria-controls=container-runtime-0 aria-selected=true>Linux nodes</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#container-runtime-1 role=tab aria-controls=container-runtime-1>autres systèmes d'exploitation</a></li></ul><div class=tab-content id=container-runtime><div id=container-runtime-0 class="tab-pane show active" role=tabpanel aria-labelledby=container-runtime-0><p><p>Par défaut, Kubernetes utilise le
<a class=glossary-tooltip title='Une API pour les runtimes de conteneurs à intégrer avec kubelet' data-toggle=tooltip data-placement=top href=https://kubernetes.io/docs/concepts/overview/components/#container-runtime target=_blank aria-label='Container Runtime Interface'>Container Runtime Interface</a> (CRI)
pour s'interfacer avec votre environnement d'exécution de conteneur choisi.</p><p>Si vous ne spécifiez pas de runtime, kubeadm essaie automatiquement de détecter un
Runtime de conteneur en parcourant une liste de sockets de domaine Unix bien connus.
Le tableau suivant répertorie les environnements d'exécution des conteneurs et leurs chemins de socket associés:</p><table><caption style=display:none>Les environnements d'exécution des conteneurs et leurs chemins de socket</caption><thead><tr><th>Runtime</th><th>Chemin vers le socket de domaine Unix</th></tr></thead><tbody><tr><td>Docker</td><td><code>/var/run/docker.sock</code></td></tr><tr><td>containerd</td><td><code>/run/containerd/containerd.sock</code></td></tr><tr><td>CRI-O</td><td><code>/var/run/crio/crio.sock</code></td></tr></tbody></table><br>Si Docker et containerd sont détectés, Docker est prioritaire. C'est
nécessaire car Docker 18.09 est livré avec containerd et les deux sont détectables même si vous
installez Docker.
Si deux autres environnements d'exécution ou plus sont détectés, kubeadm se ferme avec une erreur.<p>Le kubelet s'intègre à Docker via l'implémentation CRI intégrée de <code>dockershim</code>.</p><p>Voir <a href=/docs/setup/production-environment/container-runtimes/>runtimes de conteneur</a>
pour plus d'informations.</p></div><div id=container-runtime-1 class=tab-pane role=tabpanel aria-labelledby=container-runtime-1><p><p>Par défaut, kubeadm utilise <a class=glossary-tooltip title="Docker est un logiciel fournissant une virtualisation au niveau du système d'exploitation, également connue sous le nom de conteneurs." data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a> comme environnement d'exécution du conteneur.
Le kubelet s'intègre à Docker via l'implémentation CRI intégrée de <code>dockershim</code>.</p><p>Voir <a href=/docs/setup/production-environment/container-runtimes/>runtimes de conteneur</a>
pour plus d'informations.</p></div></div><h2 id=installation-de-kubeadm-des-kubelets-et-de-kubectl>Installation de kubeadm, des kubelets et de kubectl</h2><p>Vous installerez ces paquets sur toutes vos machines:</p><ul><li><p><code>kubeadm</code>: la commande pour initialiser le cluster.</p></li><li><p>la <code>kubelet</code>: le composant qui s'exécute sur toutes les machines de votre cluster et fait des actions
comme le démarrage des pods et des conteneurs.</p></li><li><p><code>kubectl</code>: la ligne de commande utilisée pour parler à votre cluster.</p></li></ul><p>kubeadm <strong>n'installera pas</strong> ni ne gèrera les <code>kubelet</code> ou<code> kubectl</code> pour vous.
Vous devez vous assurer qu'ils correspondent à la version du control plane de Kubernetes que vous souhaitez que kubeadm installe pour vous. Si vous ne le faites pas, vous risquez qu'une
erreur de version se produise, qui pourrait conduire à un comportement inattendu.
Cependant, une version mineure entre les kubelets et le control plane est pris en charge,
mais la version de la kubelet ne doit jamais dépasser la version de l'API server.
Par exemple, les kubelets exécutant la version 1.7.0 devraient être entièrement compatibles avec un API server en 1.8.0,
mais pas l'inverse.</p><p>For information about installing <code>kubectl</code>, see <a href=/fr/docs/tasks/tools/install-kubectl/>Installation et configuration kubectl</a>.</p><div class="alert alert-danger warning callout" role=alert><strong>Attention:</strong> Ces instructions excluent tous les packages Kubernetes de toutes les mises à niveau du système d'exploitation.
C’est parce que kubeadm et Kubernetes ont besoin d'une
<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-11/>attention particulière lors de la mise à niveau</a>.</div><p>Pour plus d'informations sur les compatibilités de version, voir:</p><ul><li>Kubernetes <a href=/docs/setup/version-skew-policy/>version et politique de compatibilité de version</a></li><li>Kubeadm-specific <a href=/fr/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy>politique de compatibilité de version</a></li></ul><ul class="nav nav-tabs" id=k8s-install role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-0 role=tab aria-controls=k8s-install-0 aria-selected=true>Ubuntu, Debian or HypriotOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-1 role=tab aria-controls=k8s-install-1>CentOS, RHEL or Fedora</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-2 role=tab aria-controls=k8s-install-2>Fedora CoreOS ou Flatcar Container Linux</a></li></ul><div class=tab-content id=k8s-install><div id=k8s-install-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-0><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo apt-get update <span style=color:#666>&amp;&amp;</span> sudo apt-get install -y apt-transport-https curl
</span></span><span style=display:flex><span>curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span></span><span style=display:flex><span><span style=color:#b44>deb https://apt.kubernetes.io/ kubernetes-xenial main
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install -y kubelet kubeadm kubectl
</span></span><span style=display:flex><span>sudo apt-mark hold kubelet kubeadm kubectl
</span></span></code></pre></div></div><div id=k8s-install-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span></span></span><span style=display:flex><span><span style=color:#b44>[kubernetes]
</span></span></span><span style=display:flex><span><span style=color:#b44>name=Kubernetes
</span></span></span><span style=display:flex><span><span style=color:#b44>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
</span></span></span><span style=display:flex><span><span style=color:#b44>enabled=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#b44>repo_gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#b44>exclude=kubelet kubeadm kubectl
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Mettre SELinux en mode permissif (le désactiver efficacement)</span>
</span></span><span style=display:flex><span>sudo setenforce <span style=color:#666>0</span>
</span></span><span style=display:flex><span>sudo sed -i <span style=color:#b44>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo yum install -y kubelet kubeadm kubectl --disableexcludes<span style=color:#666>=</span>kubernetes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div><p><strong>Note:</strong></p><ul><li><p>Mettre SELinux en mode permissif en lançant <code>setenforce 0</code> et <code>sed ... </code>le désactive efficacement.
C'est nécessaire pour permettre aux conteneurs d'accéder au système de fichiers hôte, qui est nécessaire par exemple pour les réseaux de pod.
Vous devez le faire jusqu'à ce que le support de SELinux soit amélioré dans Kubelet.</p></li><li><p>Vous pouvez laisser SELinux activé si vous savez comment le configurer, mais il peut nécessiter des paramètres qui ne sont pas pris en charge par kubeadm.</p></li></ul></div><div id=k8s-install-2 class=tab-pane role=tabpanel aria-labelledby=k8s-install-2><p><p>Installez les plugins CNI (requis pour la plupart des réseaux de pods) :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.8.2&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>sudo mkdir -p /opt/cni/bin
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cni-plugins-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tgz&#34;</span> | sudo tar -C /opt/cni/bin -xz
</span></span></code></pre></div><p>Définissez le répertoire pour télécharger les fichiers de commande</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> La variable DOWNLOAD_DIR doit être définie sur un répertoire accessible en écriture.
Si vous exécutez Flatcar Container Linux, définissez DOWNLOAD_DIR=/opt/bin</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#666>=</span>/usr/local/bin
</span></span><span style=display:flex><span>sudo mkdir -p <span style=color:#b8860b>$DOWNLOAD_DIR</span>
</span></span></code></pre></div><p>Installez crictl (requis pour Kubeadm / Kubelet Container Runtime Interface (CRI))</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.22.0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/crictl-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tar.gz&#34;</span> | sudo tar -C <span style=color:#b8860b>$DOWNLOAD_DIR</span> -xz
</span></span></code></pre></div><p>Installez <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code> et ajoutez un service systemd <code>kubelet</code>:</p><p>RELEASE_VERSION="v0.6.0"</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>RELEASE</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>curl -sSL https://dl.k8s.io/release/stable.txt<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f>cd</span> <span style=color:#b8860b>$DOWNLOAD_DIR</span>
</span></span><span style=display:flex><span>sudo curl -L --remote-name-all https://storage.googleapis.com/kubernetes-release/release/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span>/bin/linux/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span>/<span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>
</span></span><span style=display:flex><span>sudo chmod +x <span style=color:#666>{</span>kubeadm,kubelet,kubectl<span style=color:#666>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service
</span></span><span style=display:flex><span>sudo mkdir -p /etc/systemd/system/kubelet.service.d
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</span></span></code></pre></div><p>Activez et démarrez <code>kubelet</code> :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> La distribution Linux Flatcar Container monte le répertoire <code>/usr</code> comme un système de fichiers en lecture seule.
Avant de démarrer votre cluster, vous devez effectuer des étapes supplémentaires pour configurer un répertoire accessible en écriture.
Consultez le <a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#usr-mounted-read-only/>Guide de dépannage de Kubeadm</a> pour savoir comment configurer un répertoire accessible en écriture.</div></div></div><p>Kubelet redémarre maintenant toutes les quelques secondes,
car il attend les instructions de kubeadm dans une boucle de crash.</p><h2 id=configurer-le-driver-de-cgroup-utilisé-par-la-kubelet-sur-un-nœud-master>Configurer le driver de cgroup utilisé par la kubelet sur un nœud master</h2><p>Lorsque vous utilisez Docker, kubeadm détecte automatiquement le pilote ( driver ) de cgroup pour kubelet
et le configure dans le fichier <code>/var/lib/kubelet/config.yaml</code> lors de son éxecution.</p><p>Si vous utilisez un autre CRI, vous devez passer votre valeur <code>cgroupDriver</code> avec <code>kubeadm init</code>, comme ceci :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>cgroupDriver</span>:<span style=color:#bbb> </span>&lt;value&gt;<span style=color:#bbb>
</span></span></span></code></pre></div><p>Pour plus de détails, veuillez lire <a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>Utilisation de kubeadm init avec un fichier de configuration</a>.</p><p>Veuillez noter que vous devez <strong>seulement</strong> le faire si le driver de cgroupe de votre CRI
n'est pas <code>cgroupfs</code>, car c'est déjà la valeur par défaut dans la kubelet.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Depuis que le paramètre <code>--cgroup-driver</code> est obsolète par kubelet, si vous l'avez dans<code>/var/lib/kubelet/kubeadm-flags.env</code>
ou <code>/etc/default/kubelet</code>(<code>/etc/sysconfig/kubelet</code> pour les RPM), veuillez le supprimer et utiliser à la place KubeletConfiguration
(stocké dans<code>/var/lib/kubelet/config.yaml</code> par défaut).</div><p>Il est nécessaire de redémarrer la kubelet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo systemctl daemon-reload
</span></span><span style=display:flex><span>sudo systemctl restart kubelet
</span></span></code></pre></div><p>La détection automatique du pilote cgroup pour d'autres runtimes de conteneur
comme CRI-O et containerd est un travail en cours.</p><h2 id=dépannage>Dépannage</h2><p>Si vous rencontrez des difficultés avec kubeadm, veuillez consulter notre <a href=/fr/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>documentation de dépannage</a>.</p><h2 id=a-suivre>A suivre</h2><ul><li><a href=/fr/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>Utiliser kubeadm pour créer un cluster</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-134ed1f6142a98e6ac681a1ba4920e53>1.2 - Création d'un Cluster a master unique avec kubeadm</h1><div class=lead>Création d'un Cluster a master unique avec kubeadm</div><p><img src=https://raw.githubusercontent.com/cncf/artwork/master/projects/kubernetes/certified-kubernetes/versionless/color/certified-kubernetes-color.png align=right width=150px><strong>kubeadm</strong> vous aide à démarrer un cluster Kubernetes minimum,
viable et conforme aux meilleures pratiques. Avec kubeadm, votre cluster
doit passer les <a href=https://kubernetes.io/blog/2017/10/software-conformance-certification>tests de Conformance Kubernetes</a>.
Kubeadm prend également en charge d'autres fonctions du cycle de vie, telles que les mises
à niveau, la rétrogradation et la gestion des
<a href=/docs/reference/access-authn-authz/bootstrap-tokens/>bootstrap tokens</a>.</p><p>Comme vous pouvez installer kubeadm sur différents types de machines (par exemple, un ordinateur
portable, un serveur,
Raspberry Pi, etc.), il est parfaitement adapté à l'intégration avec des systèmes d'approvisionnement
comme Terraform ou Ansible.</p><p>La simplicité de kubeadm lui permet d'être utilisé dans une large gamme de cas d'utilisation:</p><ul><li>Les nouveaux utilisateurs peuvent commencer par kubeadm pour essayer Kubernetes pour la première
fois.</li><li>Les utilisateurs familiarisés avec Kubernetes peuvent créer des clusters avec kubeadm et tester
leurs applications.</li><li>Les projets plus importants peuvent inclure kubeadm en tant que brique de base dans un système
plus complexe pouvant également inclure d'autres outils d'installation.</li></ul><p>Kubeadm est conçu pour être un moyen simple pour les nouveaux utilisateurs de commencer à essayer
Kubernetes, pour la première fois éventuellement. C'est un moyen pour les utilisateurs avancés de
tester leur application en même temps qu'un cluster facilement, et aussi être
une brique de base dans un autre écosystème et/ou un outil d’installation avec une plus grand
portée.</p><p>Vous pouvez installer très facilement <em>kubeadm</em> sur des systèmes d'exploitation prenant en charge
l'installation des paquets deb ou rpm. Le SIG responsable de kubeadm,
<a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle>SIG Cluster Lifecycle</a>,
fournit ces paquets pré-construits pour vous,
mais vous pouvez également les construire à partir des sources pour d'autres systèmes d'exploitation.</p><h3 id=maturité-de-kubeadm>Maturité de kubeadm</h3><table><thead><tr><th>Elément</th><th>Niveau de maturité</th></tr></thead><tbody><tr><td>Command line UX</td><td>GA</td></tr><tr><td>Implementation</td><td>GA</td></tr><tr><td>Config file API</td><td>beta</td></tr><tr><td>CoreDNS</td><td>GA</td></tr><tr><td>kubeadm alpha subcommands</td><td>alpha</td></tr><tr><td>High availability</td><td>alpha</td></tr><tr><td>DynamicKubeletConfig</td><td>alpha</td></tr><tr><td>Self-hosting</td><td>alpha</td></tr></tbody></table><p>Les fonctionnalités globales de kubeadm sont <strong>GA</strong>. Quelques sous-fonctionnalités, comme
la configuration, les API de fichiers sont toujours en cours de développement. L'implémentation de la création du cluster
peut changer légèrement au fur et à mesure que l'outil évolue, mais la mise en œuvre globale devrait être assez stable.
Toutes les commandes sous <code>kubeadm alpha</code> sont par définition prises en charge au niveau alpha.</p><h3 id=calendrier-de-support>Calendrier de support</h3><p>Les versions de Kubernetes sont généralement prises en charge pendant neuf mois et pendant cette
période, une version de correctif peut être publiée à partir de la branche de publication si un bug grave ou un
problème de sécurité est trouvé. Voici les dernières versions de Kubernetes et le calendrier de support
qui s'applique également à <code>kubeadm</code>.</p><table><thead><tr><th>Version de Kubernetes</th><th>Date de sortie de la version</th><th>Fin de vie</th></tr></thead><tbody><tr><td>v1.6.x</td><td>Mars 2017</td><td>Décembre 2017</td></tr><tr><td>v1.7.x</td><td>Juin 2017</td><td>Mars 2018</td></tr><tr><td>v1.8.x</td><td>Septembre 2017</td><td>Juin 2018</td></tr><tr><td>v1.9.x</td><td>Décembre 2017</td><td>Septembre 2018</td></tr><tr><td>v1.10.x</td><td>Mars 2018</td><td>Décembre 2018</td></tr><tr><td>v1.11.x</td><td>Juin 2018</td><td>Mars 2019</td></tr><tr><td>v1.12.x</td><td>Septembre 2018</td><td>Juin 2019</td></tr><tr><td>v1.13.x</td><td>Décembre 2018</td><td>Septembre 2019</td></tr></tbody></table><h2 id=pré-requis>Pré-requis</h2><ul><li>Une ou plusieurs machines exécutant un système d'exploitation compatible deb/rpm, par exemple Ubuntu ou CentOS</li><li>2 Go ou plus de RAM par machine. Si vous essayez moins cela laissera trop peu de place pour vos applications.</li><li>2 processeurs ou plus sur le master</li><li>Connectivité réseau entre toutes les machines du cluster, qu'il soit public ou privé.</li></ul><h2 id=objectifs>Objectifs</h2><ul><li>Installer un cluster Kubernetes à master unique ou un
<a href=/fr/docs/setup/production-environment/tools/kubeadm/high-availability/>cluster à haute disponibilité</a></li><li>Installez un réseau de pods sur le cluster afin que vos pods puissent se parler</li></ul><h2 id=instructions>Instructions</h2><h3 id=installer-kubeadm-sur-vos-hôtes>Installer kubeadm sur vos hôtes</h3><p>Voir <a href=/fr/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>"Installation de kubeadm"</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Si vous avez déjà installé kubeadm, lancez <code>apt-get update && apt-get upgrade</code> ou <code>yum update</code> pour obtenir la dernière version de kubeadm.</p><p>Lorsque vous effectuez une mise à niveau, la kubelet redémarre plusieurs fois au bout de quelques
secondes car elle attend dans une boucle de blocage
kubeadm pour lui dire quoi faire. Ce fonctionnement est normal.
Une fois que vous avez initialisé votre master, la kubelet s'exécute normalement.</p></div><h3 id=initialiser-votre-master>Initialiser votre master</h3><p>Le master est la machine sur laquelle s'exécutent les composants du control plane, y compris
etcd (la base de données du cluster) et l'API serveur (avec lequel la CLI kubectl communique).</p><ol><li>Choisissez un add-on réseau pour les pods et vérifiez s’il nécessite des arguments à
passer à l'initialisation de kubeadm. Selon le
fournisseur tiers que vous choisissez, vous devrez peut-être définir le <code>--pod-network-cidr</code> sur
une valeur spécifique au fournisseur. Voir <a href=#pod-network>Installation d'un add-on réseau de pod</a>.</li><li>(Facultatif) Sauf indication contraire, kubeadm utilise l'interface réseau associée
avec la passerelle par défaut pour annoncer l’IP du master. Pour utiliser une autre
interface réseau, spécifiez l'option <code>--apiserver-advertise-address=&lt;ip-address></code>
à <code>kubeadm init</code>. Pour déployer un cluster Kubernetes en utilisant l’adressage IPv6, vous devez
spécifier une adresse IPv6, par exemple <code>--apiserver-advertise-address=fd00::101</code></li><li>(Optional) Lancez <code>kubeadm config images pull</code> avant de faire <code>kubeadm init</code> pour vérifier la
connectivité aux registres gcr.io.</li></ol><p>Maintenant, lancez:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init &lt;args&gt;
</span></span></code></pre></div><h3 id=plus-d-information>Plus d'information</h3><p>Pour plus d'informations sur les arguments de <code>kubeadm init</code>, voir le
<a href=/docs/reference/setup-tools/kubeadm/kubeadm/>guide de référence kubeadm</a>.</p><p>Pour une liste complète des options de configuration, voir la
<a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>documentation du fichier de configuration</a>.</p><p>Pour personnaliser les composants du control plane, y compris l'affectation facultative d'IPv6
à la sonde liveness, pour les composants du control plane et du serveur etcd, fournissez des arguments
supplémentaires à chaque composant, comme indiqué dans les <a href=/docs/admin/kubeadm#custom-args>arguments personnalisés</a>.</p><p>Pour lancer encore une fois <code>kubeadm init</code>, vous devez d'abord <a href=#tear-down>détruire le cluster</a>.</p><p>Si vous joignez un nœud avec une architecture différente par rapport à votre cluster, créez un
Déploiement ou DaemonSet pour <code>kube-proxy</code> et<code> kube-dns</code> sur le nœud. C’est nécéssaire car les images Docker pour ces
composants ne prennent actuellement pas en charge la multi-architecture.</p><p><code>kubeadm init</code> exécute d’abord une série de vérifications préalables pour s’assurer que la machine
est prête à exécuter Kubernetes. Ces vérifications préalables exposent des avertissements et se terminent
en cas d'erreur. Ensuite <code>kubeadm init</code> télécharge et installe les composants du control plane du cluster.
Cela peut prendre plusieurs minutes. l'output devrait ressembler à:</p><pre tabindex=0><code class=language-none data-lang=none>[init] Using Kubernetes version: vX.Y.Z
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;
[kubelet-start] Writing kubelet environment file with flags to file &#34;/var/lib/kubelet/kubeadm-flags.env&#34;
[kubelet-start] Writing kubelet configuration to file &#34;/var/lib/kubelet/config.yaml&#34;
[kubelet-start] Activating the kubelet service
[certs] Using certificateDir folder &#34;/etc/kubernetes/pki&#34;
[certs] Generating &#34;etcd/ca&#34; certificate and key
[certs] Generating &#34;etcd/server&#34; certificate and key
[certs] etcd/server serving cert is signed for DNS names [kubeadm-master localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating &#34;etcd/healthcheck-client&#34; certificate and key
[certs] Generating &#34;etcd/peer&#34; certificate and key
[certs] etcd/peer serving cert is signed for DNS names [kubeadm-master localhost] and IPs [10.138.0.4 127.0.0.1 ::1]
[certs] Generating &#34;apiserver-etcd-client&#34; certificate and key
[certs] Generating &#34;ca&#34; certificate and key
[certs] Generating &#34;apiserver&#34; certificate and key
[certs] apiserver serving cert is signed for DNS names [kubeadm-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4]
[certs] Generating &#34;apiserver-kubelet-client&#34; certificate and key
[certs] Generating &#34;front-proxy-ca&#34; certificate and key
[certs] Generating &#34;front-proxy-client&#34; certificate and key
[certs] Generating &#34;sa&#34; key and public key
[kubeconfig] Using kubeconfig folder &#34;/etc/kubernetes&#34;
[kubeconfig] Writing &#34;admin.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;kubelet.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;controller-manager.conf&#34; kubeconfig file
[kubeconfig] Writing &#34;scheduler.conf&#34; kubeconfig file
[control-plane] Using manifest folder &#34;/etc/kubernetes/manifests&#34;
[control-plane] Creating static Pod manifest for &#34;kube-apiserver&#34;
[control-plane] Creating static Pod manifest for &#34;kube-controller-manager&#34;
[control-plane] Creating static Pod manifest for &#34;kube-scheduler&#34;
[etcd] Creating static Pod manifest for local etcd in &#34;/etc/kubernetes/manifests&#34;
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &#34;/etc/kubernetes/manifests&#34;. This can take up to 4m0s
[apiclient] All control plane components are healthy after 31.501735 seconds
[uploadconfig] storing the configuration used in ConfigMap &#34;kubeadm-config&#34; in the &#34;kube-system&#34; Namespace
[kubelet] Creating a ConfigMap &#34;kubelet-config-X.Y&#34; in namespace kube-system with the configuration for the kubelets in the cluster
[patchnode] Uploading the CRI Socket information &#34;/var/run/dockershim.sock&#34; to the Node API object &#34;kubeadm-master&#34; as an annotation
[mark-control-plane] Marking the node kubeadm-master as control-plane by adding the label &#34;node-role.kubernetes.io/master=&#39;&#39;&#34;
[mark-control-plane] Marking the node kubeadm-master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: &lt;token&gt;
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] creating the &#34;cluster-info&#34; ConfigMap in the &#34;kube-public&#34; namespace
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  https://kubernetes.io/fr/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;master-ip&gt;:&lt;master-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><p>Pour que kubectl fonctionne pour votre utilisateur non root, exécutez ces commandes, qui font
également partie du resultat de la commande <code>kubeadm init</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p <span style=color:#b8860b>$HOME</span>/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div><p>Alternativement, si vous êtes <code>root</code>, vous pouvez exécuter:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div><p>Faites un enregistrement du retour de la commande <code>kubeadm join</code> que <code>kubeadm init</code> génère. Vous avez
besoin de cette commande pour <a href=#join-nodes>joindre des noeuds à votre cluster</a>.</p><p>Le jeton est utilisé pour l'authentification mutuelle entre le master et les nœuds qui veulent le rejoindre.
Le jeton est secret. Gardez-le en sécurité, parce que n'importe qui avec ce
jeton peut ajouter des nœuds authentifiés à votre cluster. Ces jetons peuvent être listés,
créés et supprimés avec la commande <code>kubeadm token</code>. Voir le
<a href=/docs/reference/setup-tools/kubeadm/kubeadm-token/>Guide de référence kubeadm</a>.</p><h3 id=pod-network>Installation d'un add-on réseau</h3><div class="alert alert-warning caution callout" role=alert><strong>Avertissement:</strong> Cette section contient des informations importantes sur l’ordre d’installation et de déploiement. Lisez-la attentivement avant de continuer.</div><p>Vous devez installer un add-on réseau pour pod afin que vos pods puissent communiquer les uns
avec les autres.</p><p><strong>Le réseau doit être déployé avant toute application. De plus, CoreDNS ne démarrera pas avant
l’installation d’un réseau.
kubeadm ne prend en charge que les réseaux basés sur un CNI (et ne prend pas
en charge kubenet).</strong></p><p>Plusieurs projets fournissent des réseaux de pod Kubernetes utilisant CNI, dont certains
supportent les <a href=/docs/concepts/services-networking/networkpolicies/>network policies</a>.
Allez voir la <a href=/docs/concepts/cluster-administration/addons/>page des add-ons</a> pour une liste complète
des add-ons réseau disponibles.</p><ul><li>Le support IPv6 a été ajouté dans <a href=https://github.com/containernetworking/cni/releases/tag/v0.6.0>CNI v0.6.0</a>.</li><li><a href=https://github.com/containernetworking/plugins/blob/master/plugins/main/bridge/README.md>CNI bridge</a> et
<a href=https://github.com/containernetworking/plugins/blob/master/plugins/ipam/host-local/README.md>local-ipam</a>
sont les seuls plug-ins de réseau IPv6 pris en charge dans Kubernetes version 1.9.</li></ul><p>Notez que kubeadm configure un cluster sécurisé par défaut et impose l’utilisation de
<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>.
Assurez-vous que votre manifeste de réseau prend en charge RBAC.</p><p>Veuillez également à ce que votre réseau Pod ne se superpose à aucun des réseaux hôtes,
car cela pourrait entraîner des problèmes.
Si vous constatez une collision entre le réseau de pod de votre plug-in de réseau et certains
de vos réseaux hôtes,
vous devriez penser à un remplacement de CIDR approprié et l'utiliser lors de <code>kubeadm init</code> avec
<code>--pod-network-cidr</code> et en remplacement du YAML de votre plugin réseau.
Vous pouvez installer un add-on réseau de pod avec la commande suivante:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f &lt;add-on.yaml&gt;
</span></span></code></pre></div><p>Vous ne pouvez installer qu'un seul réseau de pod par cluster.</p><ul class="nav nav-tabs" id=tabs-pod-install role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tabs-pod-install-0 role=tab aria-controls=tabs-pod-install-0 aria-selected=true>Choisissez-en un...</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-pod-install-1 role=tab aria-controls=tabs-pod-install-1>Calico</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-pod-install-2 role=tab aria-controls=tabs-pod-install-2>Canal</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-pod-install-3 role=tab aria-controls=tabs-pod-install-3>Cilium</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-pod-install-4 role=tab aria-controls=tabs-pod-install-4>Flannel</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-pod-install-5 role=tab aria-controls=tabs-pod-install-5>Kube-router</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-pod-install-6 role=tab aria-controls=tabs-pod-install-6>Romana</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-pod-install-7 role=tab aria-controls=tabs-pod-install-7>Weave Net</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-pod-install-8 role=tab aria-controls=tabs-pod-install-8>JuniperContrail/TungstenFabric</a></li></ul><div class=tab-content id=tabs-pod-install><div id=tabs-pod-install-0 class="tab-pane show active" role=tabpanel aria-labelledby=tabs-pod-install-0><p><p>Sélectionnez l'un des onglets pour consulter les instructions d'installation du fournisseur
de réseau de pods.</p></div><div id=tabs-pod-install-1 class=tab-pane role=tabpanel aria-labelledby=tabs-pod-install-1><p><p>Pour plus d'informations sur l'utilisation de Calico, voir
<a href=https://docs.projectcalico.org/latest/getting-started/kubernetes/>Guide de démarrage rapide de Calico sur Kubernetes</a>,
<a href=https://docs.projectcalico.org/latest/getting-started/kubernetes/installation/calico>Installation de Calico pour les netpols ( network policies ) et le réseau</a>, ainsi que d'autres resources liées à ce sujet.</p><p>Pour que Calico fonctionne correctement, vous devez passer <code>--pod-network-cidr = 192.168.0.0 / 16</code>
à <code>kubeadm init</code> ou mettre à jour le fichier <code>calico.yml</code> pour qu'il corresponde à votre réseau de Pod.
Notez que Calico fonctionne uniquement sur <code>amd64</code>, <code>arm64</code>, <code>ppc64le</code> et <code>s390x</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/calico.yaml
</span></span></code></pre></div></div><div id=tabs-pod-install-2 class=tab-pane role=tabpanel aria-labelledby=tabs-pod-install-2><p><p>Canal utilise Calico pour les netpols et Flannel pour la mise en réseau. Reportez-vous à la
documentation Calico pour obtenir le <a href=https://docs.projectcalico.org/latest/getting-started/kubernetes/installation/flannel>guide de démarrage officiel</a>.</p><p>Pour que Canal fonctionne correctement, <code>--pod-network-cidr = 10.244.0.0 / 16</code> doit être passé à
<code>kubeadm init</code>. Notez que Canal ne fonctionne que sur <code>amd64</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://docs.projectcalico.org/v3.8/manifests/canal.yaml
</span></span></code></pre></div></div><div id=tabs-pod-install-3 class=tab-pane role=tabpanel aria-labelledby=tabs-pod-install-3><p><p>Pour plus d'informations sur l'utilisation de Cilium avec Kubernetes, voir
<a href=https://docs.cilium.io/en/stable/kubernetes/>Guide d'installation de Kubernetes pour Cilium</a>.</p><p>Ces commandes déploieront Cilium avec son propre etcd géré par l'opérateur etcd.</p><p>Note: Si vous utilisez kubeadm dans un seul noeud, veuillez enlever sa marque (taint) pour que
les pods etcd-operator puissent être déployés dans le nœud du control plane.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes &lt;node-name&gt; node-role.kubernetes.io/master:NoSchedule-
</span></span></code></pre></div><p>Pour déployer Cilium, il vous suffit de lancer:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f https://raw.githubusercontent.com/cilium/cilium/v1.4/examples/kubernetes/1.13/cilium.yaml
</span></span></code></pre></div><p>Une fois que tous les pods Cilium sont marqués «READY», vous commencez à utiliser votre cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>$ kubectl get pods -n kube-system --selector<span style=color:#666>=</span>k8s-app<span style=color:#666>=</span>cilium
</span></span><span style=display:flex><span>NAME           READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>cilium-drxkl   1/1     Running   <span style=color:#666>0</span>          18m
</span></span></code></pre></div></div><div id=tabs-pod-install-4 class=tab-pane role=tabpanel aria-labelledby=tabs-pod-install-4><p><p>Pour que <code>flannel</code> fonctionne correctement, vous devez passer <code>--pod-network-cidr = 10.244.0.0 / 16</code> à <code>kubeadm init</code>.
Paramétrez <code>/proc/sys/net/bridge/bridge-nf-call-iptables</code> à «1» en exécutant
<code>sysctl net.bridge.bridge-nf-call-iptables = 1</code>
passez le trafic IPv4 bridged à iptables. Ceci est nécessaire pour que certains plugins CNI
fonctionnent, pour plus d'informations
allez voir <a href=/docs/concepts/cluster-administration/network-plugins/#network-plugin-requirements>ici</a>.</p><p>Notez que <code>flannel</code> fonctionne sur <code>amd64</code>, <code>arm</code>, <code>arm64</code>, <code>ppc64le</code> et <code>s390x</code> sous Linux.
Windows (<code>amd64</code>) est annoncé comme supporté dans la v0.11.0 mais son utilisation n’est pas
documentée.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/a70459be0084506e4ec919aa1c114638878db11b/Documentation/kube-flannel.yml
</span></span></code></pre></div><p>Pour plus d’informations sur <code>flannel</code>, voir <a href=https://github.com/coreos/flannel>le dépôt CoreOS sur GitHub</a>.</p></div><div id=tabs-pod-install-5 class=tab-pane role=tabpanel aria-labelledby=tabs-pod-install-5><p><p>Paramétrez <code>/proc/sys/net/bridge/bridge-nf-call-iptables</code> à «1» en exécutant
<code>sysctl net.bridge.bridge-nf-call-iptables = 1</code>
Cette commande indiquera de passer le trafic IPv4 bridgé à iptables.
Ceci est nécessaire pour que certains plugins CNI fonctionnent, pour plus d'informations
s'il vous plaît allez voir <a href=/docs/concepts/cluster-administration/network-plugins/#network-plugin-requirements>ici</a>.</p><p>Kube-router s'appuie sur kube-controller-manager pour allouer le pod CIDR aux nœuds. Par conséquent,
utilisez <code>kubeadm init</code> avec l'option <code>--pod-network-cidr</code>.</p><p>Kube-router fournit un réseau de pod, une stratégie réseau et un proxy de service basé sur un
IP Virtual Server (IPVS) / Linux Virtual Server (LVS) hautement performant.</p><p>Pour plus d'informations sur la configuration du cluster Kubernetes avec Kube-router à l'aide de kubeadm,
veuillez consulter le <a href=https://github.com/cloudnativelabs/kube-router/blob/master/docs/kubeadm.md>guide d'installation</a>.</p></div><div id=tabs-pod-install-6 class=tab-pane role=tabpanel aria-labelledby=tabs-pod-install-6><p><p>Paramétrez <code>/proc/sys/net/bridge/bridge-nf-call-iptables</code> à <code>1</code> en exécutant
<code>sysctl net.bridge.bridge-nf-call-iptables = 1</code>
Cette commande indiquera de passer le trafic IPv4 bridged à iptables. Ceci est nécessaire pour que certains plugins CNI fonctionnent,
pour plus d'informations
veuillez consulter la documentation <a href=/docs/concepts/cluster-administration/network-plugins/#network-plugin-requirements>ici</a>.</p><p>Le guide d'installation officiel de Romana est <a href=https://github.com/romana/romana/tree/master/containerize#using-kubeadm>ici</a>.</p><p>Romana ne fonctionne que sur <code>amd64</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/romana/romana/master/containerize/specs/romana-kubeadm.yml
</span></span></code></pre></div></div><div id=tabs-pod-install-7 class=tab-pane role=tabpanel aria-labelledby=tabs-pod-install-7><p><p>Paramétrez <code>/proc/sys/net/bridge/bridge-nf-call-iptables</code> à «1» en exécutant <code>sysctl net.bridge.bridge-nf-call-iptables = 1</code>
Cette commande indiquera de passer le trafic IPv4 bridged à iptables. Ceci est nécessaire pour que certains plugins CNI fonctionnent, pour plus d'informations
s'il vous plaît allez voir <a href=/docs/concepts/cluster-administration/network-plugins/#network-plugin-requirements>ici</a>.</p><p>Le guide de configuration officiel de Weave Net est <a href=https://www.weave.works/docs/net/latest/kube-addon/>ici</a>.</p><p>Weave Net fonctionne sur <code>amd64</code>, <code>arm</code>, <code>arm64</code> et <code>ppc64le</code> sans aucune action supplémentaire requise.
Weave Net paramètre le mode hairpin par défaut. Cela permet aux pods de se connecter via leur adresse IP de service
s'ils ne connaissent pas leur Pod IP.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div></div><div id=tabs-pod-install-8 class=tab-pane role=tabpanel aria-labelledby=tabs-pod-install-8><p><p>Fournit une solution SDN superposée, offrant un réseau multicouches, un réseau de cloud hybride,
prise en charge simultanée des couches superposées, application de la stratégie réseau, isolation du réseau,
chaînage de service et équilibrage de charge flexible.</p><p>Il existe de nombreuses manières flexibles d’installer JuniperContrail / TungstenFabric CNI.</p><p>Veuillez vous référer à ce guide de démarrage rapide: <a href=https://tungstenfabric.github.io/website/>TungstenFabric</a></p></div></div><p>Une fois qu'un réseau de pod a été installé, vous pouvez vérifier qu'il fonctionne en
vérifiant que le pod CoreDNS est en cours d’exécution dans l'output de <code>kubectl get pods --all-namespaces</code>.
Et une fois que le pod CoreDNS est opérationnel, vous pouvez continuer en joignant vos nœuds.</p><p>Si votre réseau ne fonctionne pas ou si CoreDNS n'est pas en cours d'exécution, vérifiez
notre <a href=/fr/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>documentation de dépannage</a>.</p><h3 id=isolation-des-nœuds-du-control-plane>Isolation des nœuds du control plane</h3><p>Par défaut, votre cluster ne déploie pas de pods sur le master pour des raisons de sécurité.
Si vous souhaitez pouvoir déployer des pods sur le master, par exemple, pour un
cluster Kubernetes mono-machine pour le développement, exécutez:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl taint nodes --all node-role.kubernetes.io/master-
</span></span></code></pre></div><p>Avec un resultat ressemblant à quelque chose comme:</p><pre tabindex=0><code>node &#34;test-01&#34; untainted
taint &#34;node-role.kubernetes.io/master:&#34; not found
taint &#34;node-role.kubernetes.io/master:&#34; not found
</code></pre><p>Cela supprimera la marque <code>node-role.kubernetes.io/master</code> de tous les nœuds qui
l'ont, y compris du nœud master, ce qui signifie que le scheduler sera alors capable
de déployer des pods partout.</p><h3 id=join-nodes>Faire rejoindre vos nœuds</h3><p>Les nœuds sont ceux sur lesquels vos workloads (conteneurs, pods, etc.) sont exécutées.
Pour ajouter de nouveaux nœuds à votre cluster, procédez comme suit pour chaque machine:</p><ul><li>SSH vers la machine</li><li>Devenir root (par exemple, <code>sudo su-</code>)</li><li>Exécutez la commande qui a été récupérée sur l'output de <code>kubeadm init</code>. Par exemple:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm join --token &lt;token&gt; &lt;master-ip&gt;:&lt;master-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</span></span></code></pre></div><p>Si vous n'avez pas le jeton, vous pouvez l'obtenir en exécutant la commande suivante sur le nœud master:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token list
</span></span></code></pre></div><p>L'output est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
</span></span></span><span style=display:flex><span><span style=color:#888>8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                   signing          token generated by     bootstrappers:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                    &#39;kubeadm init&#39;.        kubeadm:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                                           default-node-token
</span></span></span></code></pre></div><p>Par défaut, les jetons expirent après 24 heures. Si vous joignez un nœud au cluster après
l’expiration du jeton actuel,
vous pouvez créer un nouveau jeton en exécutant la commande suivante sur le nœud maître:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token create
</span></span></code></pre></div><p>L'output est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>5didvk.d09sbcov8ph2amjw
</span></span></span></code></pre></div><p>Si vous n'avez pas la valeur <code>--discovery-token-ca-cert-hash</code>, vous pouvez l'obtenir en
exécutant la suite de commande suivante sur le nœud master:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>   openssl dgst -sha256 -hex | sed <span style=color:#b44>&#39;s/^.* //&#39;</span>
</span></span></code></pre></div><p>L'output est similaire à ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Pour spécifier un tuple IPv6 pour <code>&lt;maître-ip>: &lt;maître-port></code>, l'adresse IPv6 doit être placée
entre crochets, par exemple: <code>[fd00 :: 101]: 2073</code>.</div><p>Le resultat devrait ressembler à quelque chose comme:</p><pre tabindex=0><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to master and response
  received.
* Kubelet informed of new secure connection details.

Run &#39;kubectl get nodes&#39; on the master to see this machine join.
</code></pre><p>Quelques secondes plus tard, vous remarquerez ce nœud dans l'output de <code>kubectl get node</code>.</p><h3 id=optionnel-contrôler-votre-cluster-à-partir-de-machines-autres-que-le-master>(Optionnel) Contrôler votre cluster à partir de machines autres que le master</h3><p>Afin d'utiliser kubectl sur une autre machine (par exemple, un ordinateur portable) pour communiquer avec votre
cluster, vous devez copier le fichier administrateur kubeconfig de votre master
sur votre poste de travail comme ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;master ip&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf get nodes
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>L'exemple ci-dessus suppose que l'accès SSH est activé pour root. Si ce n'est pas le cas,
vous pouvez copier le fichier <code>admin.conf</code> pour qu'il soit accessible à un autre utilisateur.
et <code>scp</code> en utilisant cet autre utilisateur à la place.</p><p>Le fichier <code>admin.conf</code> donne à l'utilisateur <em>superuser</em> des privilèges sur le cluster.
Ce fichier doit être utilisé avec parcimonie. Pour les utilisateurs normaux, il est recommandé de
générer une information d'identification unique pour laquelle vous ajoutez des privilèges à la liste blanche
(whitelist).
Vous pouvez faire ceci avec <code>kubeadm alpha kubeconfig utilisateur --nom-client &lt;CN></code>.
Le resultat de cette commande génèrera un fichier KubeConfig qui sera envoyé sur STDOUT, que vous
devrez enregistrer dans un fichier et donner à votre utilisateur. Après cela, créez la whitelist des
privilèges en utilisant <code>kubectl create (cluster) rolebinding.</code></p></div><h3 id=facultatif-proxifier-l-api-server-vers-localhost>(Facultatif) Proxifier l'API Server vers localhost</h3><p>Si vous souhaitez vous connecter à l'API server à partir de l'éxterieur du cluster, vous pouvez utiliser
<code>kubectl proxy</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;master ip&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf proxy
</span></span></code></pre></div><p>Vous pouvez maintenant accéder à l'API server localement à <code>http://localhost:8001/api/v1</code></p><h2 id=tear-down>Destruction</h2><p>Pour annuler ce que kubeadm a fait, vous devez d’abord
<a href=/docs/reference/generated/kubectl/kubectl-commands#drain>drainer le nœud</a>
et assurez-vous que le nœud est vide avant de l'arrêter.
En communiquant avec le master en utilisant les informations d'identification appropriées, exécutez:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain &lt;node name&gt; --delete-local-data --force --ignore-daemonsets
</span></span><span style=display:flex><span>kubectl delete node &lt;node name&gt;
</span></span></code></pre></div><p>Ensuite, sur le nœud en cours de suppression, réinitialisez l'état de tout ce qui concerne kubeadm:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm reset
</span></span></code></pre></div><p>Le processus de réinitialisation ne réinitialise pas et ne nettoie pas les règles iptables ni les
tables IPVS. Si vous souhaitez réinitialiser iptables, vous devez le faire manuellement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>iptables -F <span style=color:#666>&amp;&amp;</span> iptables -t nat -F <span style=color:#666>&amp;&amp;</span> iptables -t mangle -F <span style=color:#666>&amp;&amp;</span> iptables -X
</span></span></code></pre></div><p>Si vous souhaitez réinitialiser les tables IPVS, vous devez exécuter la commande suivante:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -C
</span></span></code></pre></div><p>Si vous souhaitez recommencer Il suffit de lancer <code>kubeadm init</code> ou <code>kubeadm join</code> avec les
arguments appropriés.
Plus d'options et d'informations sur la
<a href=/docs/reference/setup-tools/kubeadm/kubeadm-reset/><code>commande de réinitialisation de kubeadm</code></a>.</p><h2 id=lifecycle>Maintenir un cluster</h2><p>Vous trouverez des instructions pour la maintenance des clusters kubeadm (mises à niveau,
rétrogradation, etc.) <a href=/docs/tasks/administer-cluster/kubeadm>ici</a></p><h2 id=other-addons>Explorer les autres add-ons</h2><p>Parcourez la <a href=/docs/concepts/cluster-administration/addons/>liste des add-ons</a>,
y compris des outils pour la journalisation, la surveillance, la stratégie réseau, la visualisation
et le contrôle de votre cluster Kubernetes.</p><h2 id=whats-next>Et après ?</h2><ul><li>Vérifiez que votre cluster fonctionne correctement avec <a href=https://github.com/heptio/sonobuoy>Sonobuoy</a></li><li>En savoir plus sur l'utilisation avancée de kubeadm dans la
<a href=/docs/reference/setup-tools/kubeadm/kubeadm>documentation de référence de kubeadm</a></li><li>En savoir plus sur Kubernetes <a href=/docs/concepts/>concepts</a> et <a href=/docs/user-guide/kubectl-overview/><code>kubectl</code></a>.</li><li>Configurez la rotation des logs. Vous pouvez utiliser <strong>logrotate</strong> pour cela. Lorsque vous utilisez Docker,
vous pouvez spécifier des options de rotation des logs pour le démon Docker, par exemple
<code>--log-driver = fichier_json --log-opt = taille_max = 10m --log-opt = fichier_max = 5</code>.
Consultez <a href=https://docs.docker.com/engine/admin/>Configurer et dépanner le démon Docker</a> pour plus de détails.</li></ul><h2 id=feedback>Feedback</h2><ul><li>Pour les bugs, visitez <a href=https://github.com/kubernetes/kubeadm/issues>kubeadm GitHub issue tracker</a></li><li>Pour le support, rendez vous sur le Channel Slack kubeadm:
<a href=https://kubernetes.slack.com/messages/kubeadm/>#kubeadm</a></li><li>Le Channel Slack: General SIG Cluster Lifecycle Development:
<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle/>#sig-cluster-lifecycle</a></li><li><a href=#TODO>SIG Cluster Lifecycle SIG information</a></li><li>SIG Cluster Lifecycle Mailing List:
<a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle>kubernetes-sig-cluster-lifecycle</a></li></ul><h2 id=version-skew-policy>Politique de compatibilité de versions</h2><p>L'outil CLI kubeadm de la version vX.Y peut déployer des clusters avec un control
plane de la version vX.Y ou vX. (Y-1).
kubeadm CLI vX.Y peut également mettre à niveau un cluster existant créé par kubeadm
de la version vX. (Y-1).</p><p>Pour cette raison, nous ne pouvons pas voir plus loin, kubeadm CLI vX.Y peut ou pas être
en mesure de déployer des clusters vX. (Y + 1).</p><p>Exemple: kubeadm v1.8 peut déployer des clusters v1.7 et v1.8 et mettre à niveau des
clusters v1.7 créés par kubeadm vers
v1.8.</p><p>Ces ressources fournissent plus d'informations sur le saut de version pris en
charge entre les kubelets et le control plane, ainsi que sur d'autres composants Kubernetes:</p><ul><li><a href=/docs/setup/version-skew-policy/>politique de compatibilité de versions</a> de Kubernetes</li><li><a href=/fr/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl>Guide d'installation</a>
spécifique à Kubeadm</li></ul><h2 id=multi-platform>kubeadm fonctionne sur plusieurs plates-formes</h2><p>Les packages et fichiers binaires de kubeadm deb/rpm sont conçus pour amd64, arm (32 bits), arm64, ppc64le et s390x
suite à la <a href=https://github.com/kubernetes/community/blob/master/contributors/design-proposals/multi-platform.md>multiplateforme proposal</a>.</p><p>Les images de conteneur multiplatform pour le control plane et les addons sont également pris en charge depuis la v1.12.</p><p>Seuls certains fournisseurs de réseau proposent des solutions pour toutes les plateformes. Veuillez consulter la liste des
fournisseurs de réseau ci-dessus ou la documentation de chaque fournisseur pour déterminer si le fournisseur
prend en charge votre plate-forme.</p><h2 id=limitations>Limitations</h2><p>Remarque: kubeadm évolue continuellement et ces limitations seront résolues en temps voulu.</p><ul><li>Le cluster créé ici a un seul master, avec une seule base de données etcd. Cela signifie que
si le master est irrécupérable, votre cluster peut perdre ses données et peut avoir besoin d'être recréé à
partir de zéro. L'ajout du support HA (plusieurs serveurs etcd, plusieurs API servers, etc.)
à kubeadm est encore en cours de developpement.</li></ul><p>   Contournement: régulièrement <a href=https://coreos.com/etcd/docs/latest/admin_guide.html>sauvegarder etcd</a>.
le répertoire des données etcd configuré par kubeadm se trouve dans <code>/var/lib/etcd</code> sur le master.</p><h2 id=troubleshooting>Diagnostic</h2><p>Si vous rencontrez des difficultés avec kubeadm, veuillez consulter nos
<a href=/fr/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>troubleshooting docs</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c656c5eda3e1c06ad1aedebdc04a211>1.3 - Personnalisation de la configuration du control plane avec kubeadm</h1><div class=lead>Personnalisation de la configuration du control plane avec kubeadm</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.12 [stable]</code></div><p>L'objet <code>ClusterConfiguration</code> de kubeadm expose le champ <code>extraArgs</code> qui peut
remplacer les indicateurs par défaut transmis au control plane à des composants
tels que l'APIServer, le ControllerManager et le Scheduler. Les composants sont
définis à l'aide des champs suivants:</p><ul><li><code>apiServer</code></li><li><code>controllerManager</code></li><li><code>scheduler</code></li></ul><p>Le champ <code>extraArgs</code> se compose de paires<code> clé: valeur</code>. Pour remplacer un indicateur
pour un composant du control plane:</p><ol><li>Ajoutez les champs appropriés à votre configuration.</li><li>Ajoutez les indicateurs à remplacer dans le champ.</li></ol><p>Pour plus de détails sur chaque champ de la configuration, vous pouvez accéder aux
<a href=https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm#ClusterConfiguration>pages de référence de l'API</a>.</p><h2 id=paramètres-pour-l-api-server>Paramètres pour l'API Server</h2><p>Pour plus de détails, voir la <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>documentation de référence pour kube-apiserver</a>.</p><p>Exemple d'utilisation:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.13.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#666>1.13</span>-sample<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>advertise-address</span>:<span style=color:#bbb> </span><span style=color:#666>192.168.0.103</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>anonymous-auth</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>enable-admission-plugins</span>:<span style=color:#bbb> </span>AlwaysPullImages,DefaultStorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>audit-log-path</span>:<span style=color:#bbb> </span>/home/johndoe/audit.log<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=paramètres-pour-le-controllermanager>Paramètres pour le ControllerManager</h2><p>Pour plus de détails, voir la <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>documentation de référence pour kube-controller-manager</a>.</p><p>Exemple d'utilisation:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.13.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#666>1.13</span>-sample<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-key-file</span>:<span style=color:#bbb> </span>/home/johndoe/keys/ca.key<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deployment-controller-sync-period</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=paramètres-pour-le-scheduler>Paramètres pour le Scheduler</h2><p>Pour plus de détails, voir la <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>documentation de référence pour kube-scheduler</a>.</p><p>Example usage:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.13.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#666>1.13</span>-sample<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduler</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bind-address</span>:<span style=color:#bbb> </span><span style=color:#666>0.0.0.0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>config</span>:<span style=color:#bbb> </span>/home/johndoe/schedconfig.yaml<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubeconfig</span>:<span style=color:#bbb> </span>/home/johndoe/kubeconfig.yaml<span style=color:#bbb>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-015edbc7cc688d31b1d1edce7c186135>1.4 - Options pour la topologie en haute disponibilité</h1><div class=lead>Topologie haute-disponibilité Kubernetes</div><p>Cette page explique les deux options de configuration de topologie de vos clusters Kubernetes
pour la haute disponibilité.</p><p>Vous pouvez configurer un cluster en haute disponibilité:</p><ul><li>Avec des nœuds du control plane empilés, les nœuds etcd étant co-localisés avec des nœuds du control plane</li><li>Avec des nœuds etcd externes, où etcd s'exécute sur des nœuds distincts du control plane</li></ul><p>Vous devez examiner attentivement les avantages et les inconvénients de chaque topologie avant
de configurer un cluster en haute disponibilité.</p><h2 id=topologie-etcd-empilée>Topologie etcd empilée</h2><p>Un cluster HA empilé est une <a href=https://fr.wikipedia.org/wiki/Topologie_de_r%C3%A9seau>topologie réseau</a>
où le cluster de stockage de données distribuées est fourni par etcd et est superposé au
cluster formé par les noeuds gérés par kubeadm qui exécute les composants du control plane.</p><p>Chaque nœud du control plane exécute une instance de <code>kube-apiserver</code>, <code>kube-scheduler</code> et
<code>kube-controller-manager</code>.
Le <code>kube-apiserver</code> est exposé aux nœuds à l'aide d'un loadbalancer.</p><p>Chaque nœud du control plane crée un membre etcd local et ce membre etcd communique uniquement avec
le <code>kube-apiserver</code> de ce noeud. Il en va de même pour le <code>kube-controller-manager</code> local
et les instances de <code>kube-scheduler</code>.</p><p>Cette topologie couple les control planes et les membres etcd sur les mêmes nœuds. C'est
plus simple à mettre en place qu'un cluster avec des nœuds etcd externes et plus simple à
gérer pour la réplication.</p><p>Cependant, un cluster empilé présente un risque d'échec du couplage. Si un noeud tombe en panne,
un membre etcd et une instance du control plane sont perdus et la redondance est compromise. Vous
pouvez atténuer ce risque en ajoutant plus de nœuds au control plane.</p><p>Par conséquent, vous devez exécuter au moins trois nœuds de control plane empilés pour un cluster
en haute disponibilité.</p><p>C'est la topologie par défaut dans kubeadm. Un membre etcd local est créé automatiquement
sur les noeuds du control plane en utilisant <code>kubeadm init</code> et <code>kubeadm join --experimental-control-plane</code>.</p><p>Schéma de la <a href=/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg>Topologie etcd empilée</a></p><h2 id=topologie-etcd-externe>Topologie etcd externe</h2><p>Un cluster haute disponibilité avec un etcd externe est une
<a href=https://fr.wikipedia.org/wiki/Topologie_de_r%C3%A9seau>topologie réseau</a> où le cluster de stockage de données
distribuées fourni par etcd est externe au cluster formé par les nœuds qui exécutent les composants
du control plane.</p><p>Comme la topologie etcd empilée, chaque nœud du control plane d'une topologie etcd externe exécute
une instance de <code>kube-apiserver</code>, <code>kube-scheduler</code> et <code>kube-controller-manager</code>. Et le <code>kube-apiserver</code>
est exposé aux nœuds workers à l’aide d’un load-balancer. Cependant, les membres etcd s'exécutent sur
des hôtes distincts et chaque hôte etcd communique avec le <code>kube-apiserver</code> de chaque nœud du control plane.</p><p>Cette topologie dissocie le control plane et le membre etcd. Elle fournit donc une configuration HA où
perdre une instance de control plane ou un membre etcd a moins d'impact et n'affecte pas la redondance du
cluster autant que la topologie HA empilée.</p><p>Cependant, cette topologie requiert le double du nombre d'hôtes de la topologie HA integrée.
Un minimum de trois machines pour les nœuds du control plane et de trois machines
pour les nœuds etcd est requis pour un cluster HA avec cette topologie.</p><p>Schéma de la <a href=/images/kubeadm/kubeadm-ha-topology-external-etcd.svg>Topologie externe etcd</a></p><h2 id=a-suivre>A suivre</h2><ul><li><a href=/fr/docs/setup/production-environment/tools/kubeadm/high-availability/>Configurer un cluster hautement disponible avec kubeadm</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3941d5c3409342219bf7e03128b8ecb6>1.5 - Création de clusters hautement disponibles avec kubeadm</h1><div class=lead>Cluster Kubernetes haute-disponibilité kubeadm</div><p>Cette page explique deux approches différentes pour configurer un Kubernetes à haute disponibilité.
cluster utilisant kubeadm:</p><ul><li>Avec des nœuds de control plane empilés. Cette approche nécessite moins d'infrastructure.
Les membres etcd et les nœuds du control plane sont co-localisés.</li><li>Avec un cluster etcd externe cette approche nécessite plus d'infrastructure.
Les nœuds du control plane et les membres etcd sont séparés.</li></ul><p>Avant de poursuivre, vous devez déterminer avec soin quelle approche répond le mieux
aux besoins de vos applications et de l'environnement. <a href=/fr/docs/setup/production-environment/tools/kubeadm/ha-topology/>Cette comparaison</a>
décrit les avantages et les inconvénients de chacune.</p><p>Vos clusters doivent exécuter Kubernetes version 1.12 ou ultérieure. Vous devriez aussi savoir que
la mise en place de clusters HA avec kubeadm est toujours expérimentale et sera simplifiée davantage
dans les futures versions. Vous pouvez par exemple rencontrer des problèmes lors de la mise à niveau de vos clusters.
Nous vous encourageons à essayer l’une ou l’autre approche et à nous faire part de vos commentaires dans
<a href=https://github.com/kubernetes/kubeadm/issues/new>Suivi des problèmes Kubeadm</a>.</p><p>Notez que la fonctionnalité alpha <code>HighAvailability</code> est obsolète dans la version 1.12 et supprimée dans la version 1.13</p><p>Voir aussi <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha-1-13>La documentation de mise à niveau HA</a>.</p><div class="alert alert-warning caution callout" role=alert><strong>Avertissement:</strong> Cette page ne traite pas de l'exécution de votre cluster sur un fournisseur de cloud. Dans un
environnement Cloud, les approches documentées ici ne fonctionnent ni avec des objets de type
load balancer, ni avec des volumes persistants dynamiques.</div><h2 id=pré-requis>Pré-requis</h2><p>Pour les deux méthodes, vous avez besoin de cette infrastructure:</p><ul><li>Trois machines qui répondent aux pré-requis des <a href=/fr/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>exigences de kubeadm</a> pour les maîtres (masters)</li><li>Trois machines qui répondent aux pré-requis des <a href=/fr/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>exigences de kubeadm</a> pour les workers</li><li>Connectivité réseau complète entre toutes les machines du cluster (public ou réseau privé)</li><li>Privilèges sudo sur toutes les machines</li><li>Accès SSH d'une machine à tous les nœuds du cluster</li><li><code>kubeadm</code> et une <code>kubelet</code> installés sur toutes les machines. <code>kubectl</code> est optionnel.</li></ul><p>Pour le cluster etcd externe uniquement, vous avez besoin également de:</p><ul><li>Trois machines supplémentaires pour les membres etcd</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Les exemples suivants utilisent Calico en tant que fournisseur de réseau de Pod. Si vous utilisez un autre
CNI, pensez à remplacer les valeurs par défaut si nécessaire.</div><h2 id=premières-étapes-pour-les-deux-méthodes>Premières étapes pour les deux méthodes</h2><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Toutes les commandes d'un control plane ou d'un noeud etcd doivent être
éxecutées en tant que root.</div><ul><li>Certains plugins réseau CNI tels que Calico nécessitent un CIDR tel que <code>192.168.0.0 / 16</code> et 
certains comme Weave n'en ont pas besoin. Voir la
<a href=/fr/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>Documentation du CNI réseau</a>.
Pour ajouter un CIDR de pod, définissez le champ <code>podSubnet: 192.168.0.0 / 16</code> sous
  l'objet <code>networking</code> de<code> ClusterConfiguration</code>.</li></ul><h3 id=créez-un-load-balancer-pour-kube-apiserver>Créez un load balancer pour kube-apiserver</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Il existe de nombreuses configurations pour les équilibreurs de charge (load balancers).
L'exemple suivant n'est qu'un exemple. Vos exigences pour votre cluster peuvent nécessiter une configuration différente.</div><ol><li><p>Créez un load balancer kube-apiserver avec un nom résolu en DNS.</p><ul><li><p>Dans un environnement cloud, placez vos nœuds du control plane derrière un load balancer TCP.
Ce load balancer distribue le trafic à tous les nœuds du control plane sains dans sa liste.
La vérification de la bonne santé d'un apiserver est une vérification TCP sur le port que
kube-apiserver écoute (valeur par défaut: <code>6443</code>).</p></li><li><p>Il n'est pas recommandé d'utiliser une adresse IP directement dans un environnement cloud.</p></li><li><p>Le load balancer doit pouvoir communiquer avec tous les nœuds du control plane sur le
port apiserver. Il doit également autoriser le trafic entrant sur son réseau de port d'écoute.</p></li><li><p><a href=http://www.haproxy.org/>HAProxy</a> peut être utilisé comme load balancer.</p></li><li><p>Assurez-vous que l'adresse du load balancer correspond toujours à
      l'adresse de <code>ControlPlaneEndpoint</code> de kubeadm.</p></li></ul></li><li><p>Ajoutez les premiers nœuds du control plane au load balancer et testez la connexion:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>nc -v LOAD_BALANCER_IP PORT
</span></span></code></pre></div><ul><li>Une erreur <code>connection refused</code> est attendue car l'apiserver n'est pas encore en fonctionnement.
Cependant, un timeout signifie que le load balancer ne peut pas communiquer avec le nœud du
control plane. Si un timeout survient, reconfigurez le load balancer pour communiquer avec le nœud du control plane.</li></ul></li><li><p>Ajouter les nœuds du control plane restants au groupe cible du load balancer.</p></li></ol><h3 id=configurer-ssh>Configurer SSH</h3><p>SSH est requis si vous souhaitez contrôler tous les nœuds à partir d'une seule machine.</p><ol><li><p>Activer ssh-agent sur votre machine ayant accès à tous les autres nœuds du cluster:</p><pre tabindex=0><code>eval $(ssh-agent)
</code></pre></li><li><p>Ajoutez votre clé SSH à la session:</p><pre tabindex=0><code>ssh-add ~/.ssh/path_to_private_key
</code></pre></li><li><p>SSH entre les nœuds pour vérifier que la connexion fonctionne correctement.</p><ul><li><p>Lorsque vous faites un SSH sur un noeud, assurez-vous d’ajouter l’option <code>-A</code>:</p><pre tabindex=0><code>ssh -A 10.0.0.7
</code></pre></li><li><p>Lorsque vous utilisez sudo sur n’importe quel nœud, veillez à préserver l’environnement afin que le SSH forwarding fonctionne:</p><pre tabindex=0><code>sudo -E -s
</code></pre></li></ul></li></ol><h2 id=control-plane-empilé-et-nœuds-etcd>Control plane empilé et nœuds etcd</h2><h3 id=étapes-pour-le-premier-nœud-du-control-plane>Étapes pour le premier nœud du control plane</h3><ol><li><p>Sur le premier nœud du control plane, créez un fichier de configuration appelé <code>kubeadm-config.yaml</code>:</p><pre><code>apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
kubernetesVersion: stable
apiServer:
  certSANs:
  - &quot;LOAD_BALANCER_DNS&quot;
controlPlaneEndpoint: &quot;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&quot;
</code></pre><ul><li><code>kubernetesVersion</code> doit représenter la version de Kubernetes à utiliser. Cet exemple utilise <code>stable</code>.</li><li><code>controlPlaneEndpoint</code> doit correspondre à l'adresse ou au DNS et au port du load balancer.</li><li>Il est recommandé que les versions de kubeadm, kubelet, kubectl et kubernetes correspondent.</li></ul></li><li><p>Assurez-vous que le nœud est dans un état sain:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm init --config<span style=color:#666>=</span>kubeadm-config.yaml
</span></span></code></pre></div><p>Vous devriez voir quelque chose comme:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>...
</span></span></code></pre></div></li></ol><p>Vous pouvez à présent joindre n'importe quelle machine au cluster en lancant la commande suivante sur
chaque nœeud en tant que root:</p><pre><code>kubeadm join 192.168.0.200:6443 --token j04n3m.octy8zely83cy2ts --discovery-token-ca-cert-hash    sha256:84938d2a22203a8e56a787ec0c6ddad7bc7dbd52ebabc62fd5f4dbea72b14d1f
```
</code></pre><ol><li><p>Copiez ce jeton dans un fichier texte. Vous en aurez besoin plus tard pour joindre
d’autres nœuds du control plane au cluster.</p></li><li><p>Activez l'extension CNI Weave:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div></li><li><p>Tapez ce qui suit et observez les pods des composants démarrer:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl get pod -n kube-system -w
</span></span></code></pre></div><ul><li>Il est recommandé de ne joindre les nouveaux nœuds du control plane qu'après l'initialisation du premier nœud.</li></ul></li><li><p>Copiez les fichiers de certificat du premier nœud du control plane dans les autres:</p><p>Dans l'exemple suivant, remplacez <code>CONTROL_PLANE_IPS</code> par les adresses IP des autres nœuds du control plane.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># customizable</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#666>=</span><span style=color:#b44>&#34;10.0.0.7 10.0.0.8&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> host in <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#b68;font-weight:700>}</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.pub <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.crt
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.key
</span></span><span style=display:flex><span>    scp /etc/kubernetes/admin.conf <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div></li></ol><div class="alert alert-warning caution callout" role=alert><strong>Avertissement:</strong> N'utilisez que les certificats de la liste ci-dessus. kubeadm se chargera de générer le reste des certificats avec les SANs requis pour les instances du control plane qui se joignent.
Si vous copiez tous les certificats par erreur, la création de noeuds supplémentaires pourrait
échouer en raison d'un manque de SANs requis.</div><h3 id=étapes-pour-le-reste-des-nœuds-du-control-plane>Étapes pour le reste des nœuds du control plane</h3><ol><li><p>Déplacer les fichiers créés à l'étape précédente où <code>scp</code> était utilisé:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># customizable</span>
</span></span><span style=display:flex><span>mkdir -p /etc/kubernetes/pki/etcd
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.pub /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/admin.conf /etc/kubernetes/admin.conf
</span></span></code></pre></div><p>Ce processus écrit tous les fichiers demandés dans le dossier <code>/etc/kubernetes</code>.</p></li><li><p>Lancez <code>kubeadm join</code> sur ce nœud en utilisant la commande de join qui vous avait été précédemment
donnée par<code> kubeadm init</code> sur le premier noeud. Ça devrait ressembler a quelque chose
comme ça:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm join 192.168.0.200:6443 --token j04n3m.octy8zely83cy2ts --discovery-token-ca-cert-hash sha256:84938d2a22203a8e56a787ec0c6ddad7bc7dbd52ebabc62fd5f4dbea72b14d1f --experimental-control-plane
</span></span></code></pre></div><ul><li>Remarquez l'ajout de l'option <code>--experimental-control-plane</code>. Ce paramètre automatise l'adhésion au
control plane du cluster.</li></ul></li><li><p>Tapez ce qui suit et observez les pods des composants démarrer:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl get pod -n kube-system -w
</span></span></code></pre></div></li><li><p>Répétez ces étapes pour le reste des nœuds du control plane.</p></li></ol><h2 id=noeuds-etcd-externes>Noeuds etcd externes</h2><h3 id=configurer-le-cluster-etcd>Configurer le cluster etcd</h3><ul><li>Suivez ces <a href=/fr/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>instructions</a>
pour configurer le cluster etcd.</li></ul><h3 id=configurer-le-premier-nœud-du-control-plane>Configurer le premier nœud du control plane</h3><ol><li><p>Copiez les fichiers suivants de n’importe quel nœud du cluster etcd vers ce nœud.:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#666>=</span><span style=color:#b44>&#34;ubuntu@10.0.0.7&#34;</span>
</span></span><span style=display:flex><span>+scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>+scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>+scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span></code></pre></div><ul><li>Remplacez la valeur de <code>CONTROL_PLANE</code> par l'<code>utilisateur@hostname</code> de cette machine.</li></ul></li><li><p>Créez un fichier YAML appelé <code>kubeadm-config.yaml</code> avec le contenu suivant:</p><pre><code>apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
kubernetesVersion: stable
apiServer:
  certSANs:
  - &quot;LOAD_BALANCER_DNS&quot;
controlPlaneEndpoint: &quot;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&quot;
etcd:
    external:
        endpoints:
        - https://ETCD_0_IP:2379
        - https://ETCD_1_IP:2379
        - https://ETCD_2_IP:2379
        caFile: /etc/kubernetes/pki/etcd/ca.crt
        certFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
        keyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
</code></pre><ul><li><p>La différence entre etcd empilé et externe, c’est que nous utilisons le champ <code>external</code>
pour <code>etcd</code> dans la configuration de kubeadm. Dans le cas de la topologie etcd empilée,
c'est géré automatiquement.</p></li><li><p>Remplacez les variables suivantes dans le modèle (template) par les valeurs appropriées
pour votre cluster:</p><ul><li><code>LOAD_BALANCER_DNS</code></li><li><code>LOAD_BALANCER_PORT</code></li><li><code>ETCD_0_IP</code></li><li><code>ETCD_1_IP</code></li><li><code>ETCD_2_IP</code></li></ul></li></ul></li><li><p>Lancez <code>kubeadm init --config kubeadm-config.yaml</code> sur ce nœud.</p></li><li><p>Ecrivez le résultat de la commande de join dans un fichier texte pour une utilisation ultérieure.</p></li><li><p>Appliquer le plugin CNI Weave:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl apply -f <span style=color:#b44>&#34;https://cloud.weave.works/k8s/net?k8s-version=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl version | base64 | tr -d <span style=color:#b44>&#39;\n&#39;</span><span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div></li></ol><h3 id=étapes-pour-le-reste-des-nœuds-du-control-plane-1>Étapes pour le reste des nœuds du control plane</h3><p>Pour ajouter le reste des nœuds du control plane, suivez <a href=#%C3%A9tapes-pour-le-reste-des-n%C5%93uds-du-control-plane>ces instructions</a>.
Les étapes sont les mêmes que pour la configuration etcd empilée, à l’exception du fait qu'un membre
etcd local n'est pas créé.</p><p>Pour résumer:</p><ul><li>Assurez-vous que le premier nœud du control plane soit complètement initialisé.</li><li>Copier les certificats entre le premier nœud du control plane et les autres nœuds du control plane.</li><li>Joignez chaque nœud du control plane à l'aide de la commande de join que vous avez enregistrée dans
un fichier texte, puis ajoutez l'option <code>--experimental-control-plane</code>.</li></ul><h2 id=tâches-courantes-après-l-amorçage-du-control-plane>Tâches courantes après l'amorçage du control plane</h2><h3 id=installer-un-réseau-de-pod>Installer un réseau de pod</h3><p><a href=/fr/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>Suivez ces instructions</a> afin
d'installer le réseau de pod. Assurez-vous que cela correspond au pod CIDR que vous avez fourni
dans le fichier de configuration principal.</p><h3 id=installer-les-workers>Installer les workers</h3><p>Chaque nœud worker peut maintenant être joint au cluster avec la commande renvoyée à partir du resultat
de n’importe quelle commande <code>kubeadm init</code>. L'option <code>--experimental-control-plane</code> ne doit pas
être ajouté aux nœuds workers.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-8160424c22d24f7d2d63c521e107dbf8>1.6 - Configurer un cluster etcd en haute disponibilité avec kubeadm</h1><div class=lead>Configuration d'un cluster etcd en haute disponibilité avec kubeadm</div><p>Par défaut, Kubeadm exécute un cluster etcd mono nœud dans un pod statique géré
par la kubelet sur le nœud du plan de contrôle (control plane). Ce n'est pas une configuration haute disponibilité puisque le cluster etcd ne contient qu'un seul membre et ne peut donc supporter
qu'aucun membre ne devienne indisponible. Cette page vous accompagne dans le processus de création
d'un cluster etcd à trois membres en haute disponibilité, pouvant être utilisé en tant que cluster externe lors de l’utilisation de kubeadm pour configurer un cluster kubernetes.</p><h2 id=pré-requis>Pré-requis</h2><ul><li>Trois machines pouvant communiquer entre elles via les ports 2379 et 2380. Cette 
methode utilise ces ports par défaut. Cependant, ils sont configurables via 
le fichier de configuration kubeadm.</li><li>Chaque hôte doit avoir <a href=/fr/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>docker, kubelet et kubeadm installés</a>.</li><li>Certains paquets pour copier des fichiers entre les hôtes. Par exemple, <code>ssh</code> et<code> scp</code>.</li></ul><h2 id=mise-en-place-du-cluster>Mise en place du cluster</h2><p>L’approche générale consiste à générer tous les certificats sur un nœud et à ne distribuer que
les fichiers <em>nécessaires</em> aux autres nœuds.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> kubeadm contient tout ce qui est nécessaire pour générer les certificats décrits ci-dessous;
aucun autre outil de chiffrement n'est requis pour cet exemple.</div><ol><li><p>Configurez la kubelet pour qu'elle soit un gestionnaire de service pour etcd.</p><p>Etant donné qu'etcd a été créé en premier, vous devez remplacer la priorité de service en
créant un nouveau fichier unit qui a une priorité plus élevée que le fichier unit de la kubelet fourni
par kubeadm.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>[Service]
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests
</span></span></span><span style=display:flex><span><span style=color:#b44>Restart=always
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li><li><p>Créez des fichiers de configuration pour kubeadm.</p><p>Générez un fichier de configuration kubeadm pour chaque machine qui éxécutera un membre etcd
en utilisant le script suivant.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#080;font-style:italic># Update HOST0, HOST1, and HOST2 with the IPs or resolvable names of your hosts</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST0</span><span style=color:#666>=</span>10.0.0.6
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST1</span><span style=color:#666>=</span>10.0.0.7
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST2</span><span style=color:#666>=</span>10.0.0.8
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Create temp directories to store files that will end up on other hosts.</span>
</span></span><span style=display:flex><span>mkdir -p /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ETCDHOSTS</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAMES</span><span style=color:#666>=(</span><span style=color:#b44>&#34;infra0&#34;</span> <span style=color:#b44>&#34;infra1&#34;</span> <span style=color:#b44>&#34;infra2&#34;</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span>!ETCDHOSTS[@]<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCDHOSTS</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAME</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAMES</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta1&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: ClusterConfiguration
</span></span></span><span style=display:flex><span><span style=color:#b44>etcd:
</span></span></span><span style=display:flex><span><span style=color:#b44>    local:
</span></span></span><span style=display:flex><span><span style=color:#b44>        serverCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        peerCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        extraArgs:
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster: ${NAMES[0]}=https://${ETCDHOSTS[0]}:2380,${NAMES[1]}=https://${ETCDHOSTS[1]}:2380,${NAMES[2]}=https://${ETCDHOSTS[2]}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster-state: new
</span></span></span><span style=display:flex><span><span style=color:#b44>            name: ${NAME}
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            advertise-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-advertise-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div></li><li><p>Générer l'autorité de certification</p><p>Si vous avez déjà une autorité de certification, alors la seule action qui est faite copie
les fichiers <code>crt</code> et <code>key</code> de la CA dans <code>/etc/kubernetes/pki/etcd/ca.crt</code> et
<code>/etc/kubernetes/pki/etcd/ca.key</code>. Une fois ces fichiers copiés,
    passez à l'étape suivante, "Créer des certificats pour chaque membre".</p><p>Si vous ne possédez pas déjà de CA, exécutez cette commande sur <code>$HOST0</code> (où vous
avez généré les fichiers de configuration pour kubeadm).</p><pre tabindex=0><code>kubeadm init phase certs etcd-ca
</code></pre><p>Cela crée deux fichiers</p><ul><li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li><li><code>/etc/kubernetes/pki/etcd/ca.key</code></li></ul></li><li><p>Créer des certificats pour chaque membre</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># cleanup non-reusable certificates</span>
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># No need to move the certs because they are for HOST0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># clean up certs that should not be copied off this host</span>
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span></code></pre></div></li><li><p>Copier les certificats et les configurations kubeadm</p><p>Les certificats ont été générés et doivent maintenant être déplacés vers leur
hôtes respectifs.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>scp -r /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>/* <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>:
</span></span><span style=display:flex><span>ssh <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>USER@HOST $ sudo -Es
</span></span><span style=display:flex><span>root@HOST $ chown -R root:root pki
</span></span><span style=display:flex><span>root@HOST $ mv pki /etc/kubernetes/
</span></span></code></pre></div></li><li><p>S'assurer que tous les fichiers attendus existent</p><p>La liste complète des fichiers requis sur <code>$HOST0</code> est la suivante:</p><pre tabindex=0><code>/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>Sur <code>$HOST1</code>:</p><pre tabindex=0><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>Sur <code>$HOST2</code>:</p><pre tabindex=0><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre></li><li><p>Créer les manifestes de pod statiques</p><p>Maintenant que les certificats et les configurations sont en place, il est temps de créer les
manifestes. Sur chaque hôte, exécutez la commande <code>kubeadm</code> pour générer un manifeste statique
pour etcd.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>root@HOST0 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>root@HOST1 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>root@HOST2 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span></code></pre></div></li><li><p>Facultatif: Vérifiez la santé du cluster</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>docker run --rm -it <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--net host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-v /etc/kubernetes:/etc/kubernetes quay.io/coreos/etcd:<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCD_TAG</span><span style=color:#b68;font-weight:700>}</span> etcdctl <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--cert-file /etc/kubernetes/pki/etcd/peer.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--key-file /etc/kubernetes/pki/etcd/peer.key <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--ca-file /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--endpoints https://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>:2379 cluster-health
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>cluster is healthy
</span></span></code></pre></div><ul><li>Configurez <code>${ETCD_TAG}</code> avec la version de votre image etcd. Par exemple <code>v3.2.24</code>.</li><li>Configurez <code>${HOST0}</code> avec l'adresse IP de l'hôte que vous testez.</li></ul></li></ol><h2 id=a-suivre>A suivre</h2><p>Une fois que vous avez un cluster de 3 membres etcd qui fonctionne, vous pouvez continuer à
configurer un control plane hautement disponible utilisant la
<a href=/fr/docs/setup/production-environment/tools/kubeadm/high-availability/>méthode etcd externe avec kubeadm</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-07709e71de6b4ac2573041c31213dbeb>1.7 - Configuration des kubelet de votre cluster avec kubeadm</h1><div class=lead>Configuration kubelet Kubernetes cluster kubeadm</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.11 [stable]</code></div><p>Le cycle de vie de l’outil CLI kubeadm est découplé de celui de la
<a href=/docs/reference/command-line-tools-reference/kubelet>kubelet</a>, qui est un démon qui s'éxécute
sur chaque noeud du cluster Kubernetes. L'outil CLI de kubeadm est exécuté par l'utilisateur lorsque
Kubernetes est initialisé ou mis à niveau, alors que la kubelet est toujours exécutée en arrière-plan.</p><p>Comme la kubelet est un démon, elle doit être maintenue par une sorte d'init système ou un gestionnaire
de service. Lorsque la kubelet est installée à l'aide de DEB ou de RPM,
systemd est configuré pour gérer la kubelet. Vous pouvez utiliser un gestionnaire différent à la place,
mais vous devez le configurer manuellement.</p><p>Certains détails de configuration de la kubelet doivent être identiques pour
toutes les kubelets du cluster, tandis que d’autres aspects de la configuration
doivent être définis par nœud, pour tenir compte des différentes caractéristiques
d’une machine donnée, telles que le système d’exploitation, le stockage et la
mise en réseau. Vous pouvez gérer la configuration manuellement de vos kubelets,
mais <a href=#configure-kubelets-using-kubeadm>kubeadm fournit maintenant un type d’API <code>KubeletConfiguration</code> pour la gestion centralisée de vos configurations de kubelets</a>.</p><h2 id=patterns-de-configuration-des-kubelets>Patterns de configuration des Kubelets</h2><p>Les sections suivantes décrivent les modèles de configuration de kubelet simplifiés en
utilisant kubeadm, plutôt que de gérer manuellement la configuration des kubelets pour chaque nœud.</p><h3 id=propagating-cluster-level-configuration-to-each-kubelet>Propagation de la configuration niveau cluster à chaque kubelet</h3><p>Vous pouvez fournir à la kubelet les valeurs par défaut à utiliser par les commandes <code>kubeadm init</code> et
<code>kubeadm join</code>. Des exemples intéressants incluent l’utilisation d’un runtime CRI différent ou la
définition du sous-réseau par défaut utilisé par les services.</p><p>Si vous souhaitez que vos services utilisent le sous-réseau <code>10.96.0.0 / 12</code> par défaut pour les
services, vous pouvez passer le paramètre <code>--service-cidr</code> à kubeadm:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init --service-cidr 10.96.0.0/12
</span></span></code></pre></div><p>Les adresses IP virtuelles pour les services sont maintenant attribuées à partir de ce sous-réseau.
Vous devez également définir l'adresse DNS utilisée par la kubelet, en utilisant l'option
<code>--cluster-dns</code>. Ce paramètre doit être le même pour chaque kubelet sur chaque master et worker
du cluster. La kubelet fournit un objet API structuré versionné qui peut configurer la plupart des
paramètres dans la kubelet et pousser cette configuration à chaque exécution de la kubelet dans
le cluster. Cet objet s'appelle la <strong>ComponentConfig</strong> de la kubelet.
La ComponentConfig permet à l’utilisateur de spécifier des options tels que les adresses IP DNS du
cluster exprimées en une liste de valeurs pour une clé formatée en CamelCased, illustrée par l'exemple suivant:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusterDNS</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:#666>10.96.0.10</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Pour plus de détails sur ComponentConfig, jetez un œil à <a href=#configure-kubelets-using-kubeadm>cette section</a>.</p><h3 id=providing-instance-specific-configuration-details>Fournir des détails de configuration spécifiques à l'instance</h3><p>Certaines machines nécessitent des configurations de kubelet spécifiques, en raison de la différences de
matériel, de système d’exploitation, réseau ou d’autres paramètres spécifiques à l’hôte. La liste suivante
fournit quelques exemples.</p><ul><li><p>Le chemin d'accès au fichier de résolution DNS, tel que spécifié par l'option de configuration
de la kubelet <code>--resolv-conf</code>, peut différer selon les systèmes d'exploitation ou selon que vous utilisez
ou non <code>systemd-resolved</code>. Si ce chemin est incorrect, la résolution DNS échouera sur le nœud
dont la kubelet est configuré de manière incorrecte.</p></li><li><p>L'objet API de nœud <code>.metadata.name</code> est défini par défaut sur le hostname de la machine,
sauf si vous utilisez un fournisseur de cloud. Vous pouvez utiliser l’indicateur <code>--hostname-override</code>
pour remplacer le comportement par défaut si vous devez spécifier un nom de nœud différent du hostname
de la machine.</p></li><li><p>Actuellement, la kubelet ne peut pas détecter automatiquement le driver cgroup utilisé par le
runtime CRI, mais la valeur de <code>--cgroup-driver</code> doit correspondre au driver cgroup
utilisé par le runtime CRI pour garantir la santé de la kubelet.</p></li><li><p>En fonction du runtime du CRI utilisé par votre cluster, vous devrez peut-être spécifier des
options différentes pour la kubelet. Par exemple, lorsque vous utilisez Docker,
vous devez spécifier des options telles que
<code>--network-plugin = cni</code>, mais si vous utilisez un environnement d’exécution externe, vous devez spécifier
<code>--container-runtime = remote</code> et spécifier le CRI endpoint en utilisant l'option
<code>--container-runtime-path-endpoint = &lt;chemin></code>.</p></li></ul><p>Vous pouvez spécifier ces options en modifiant la configuration d’une kubelet individuelle dans
votre gestionnaire de service, tel que systemd.</p><h2 id=configure-kubelets-using-kubeadm>Configurer les kubelets en utilisant kubeadm</h2><p>Il est possible de configurer la kubelet que kubeadm va démarrer si un objet API personnalisé
<code>KubeletConfiguration</code> est passé en paramètre via un fichier de configuration comme
<code>kubeadm ... --config some-config-file.yaml</code>.</p><p>En appelant <code>kubeadm config print-default --api-objects KubeletConfiguration</code> vous
pouvez voir toutes les valeurs par défaut pour cette structure.</p><p>Regardez aussi la <a href=https://godoc.org/k8s.io/kubernetes/pkg/kubelet/apis/config#KubeletConfiguration>référence API pour le composant ComponentConfig des kubelets</a>
pour plus d'informations sur les champs individuels.</p><h3 id=workflow-lors-de-l-utilisation-de-kubeadm-init>Workflow lors de l'utilisation de <code>kubeadm init</code></h3><p>Lorsque vous appelez <code>kubeadm init</code>, la configuration de la kubelet est organisée sur le disque
sur <code>/var/lib/kubelet/config.yaml</code>, et également chargé sur une ConfigMap du cluster. La ConfigMap
est nommé <code>kubelet-config-1.X</code>, où <code>.X</code> est la version mineure de la version de Kubernetes
que vous êtes en train d'initialiser. Un fichier de configuration de kubelet est également écrit dans
<code>/etc/kubernetes/kubelet.conf</code> avec la configuration de base à l'échelle du cluster pour tous les
kubelets du cluster. Ce fichier de configuration pointe vers les certificats clients permettant aux
kubelets de communiquer avec l'API server. Ceci répond au besoin de
<a href=#propagating-cluster-level-configuration-to-each-kubelet>propager la configuration niveau cluster à chaque kubelet</a>.</p><p>Pour répondre au besoin de
<a href=#providing-instance-specific-configuration-details>fournir des détails de configuration spécifiques à l'instance de kubelet</a>,
kubeadm écrit un fichier d'environnement dans <code>/var/lib/kubelet/kubeadm-flags.env</code>, qui contient une liste
d'options à passer à la kubelet quand elle démarre. Les options sont représentées dans le fichier comme ceci:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>KUBELET_KUBEADM_ARGS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--flag1=value1 --flag2=value2 ...&#34;</span>
</span></span></code></pre></div><p>Outre les indicateurs utilisés lors du démarrage de la kubelet, le fichier contient également des
informations dynamiques comme des paramètres tels que le driver cgroup et s'il faut utiliser un autre
socket de runtime CRI (<code>--cri-socket</code>).</p><p>Après avoir rassemblé ces deux fichiers sur le disque, kubeadm tente d’exécuter ces deux commandes,
si vous utilisez systemd:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>Si le rechargement et le redémarrage réussissent, le workflow normal de <code>kubeadm init</code> continue.</p><h3 id=workflow-en-utilisant-kubeadm-join>Workflow en utilisant <code>kubeadm join</code></h3><p>Lorsque vous exécutez <code>kubeadm join</code>, kubeadm utilise les informations d'identification du bootstrap
token pour faire un bootstrap TLS, qui récupère les informations d’identité nécessaires pour télécharger le
<code>kubelet-config-1.X</code> ConfigMap puis l'écrit dans <code>/var/lib/kubelet/config.yaml</code>. Le fichier d’environnement
dynamique est généré exactement de la même manière que <code>kubeadm init</code>.</p><p>Ensuite, <code>kubeadm</code> exécute les deux commandes suivantes pour charger la nouvelle configuration dans la kubelet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>Après le chargement de la nouvelle configuration par la kubelet, kubeadm écrit le fichier KubeConfig
<code>/etc/kubernetes/bootstrap-kubelet.conf</code>, qui contient un certificat de CA et un jeton Bootstrap.
Ceux-ci sont utilisés par la kubelet pour effectuer le TLS Bootstrap et obtenir une information
d'identification unique, qui est stocké dans <code>/etc/kubernetes/kubelet.conf</code>. Quand ce fichier est
écrit, la kubelet a terminé l'exécution du bootstrap TLS.</p><h2 id=the-kubelet-drop-in-file-for-systemd>Le fichier kubelet généré pour systemd</h2><p>Le fichier de configuration installé par le package DEB ou RPM de kubeadm est écrit dans
<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> et est utilisé par systemd.</p><pre tabindex=0><code class=language-none data-lang=none>[Service]
Environment=&#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf
--kubeconfig=/etc/kubernetes/kubelet.conf&#34;
Environment=&#34;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&#34;
# This is a file that &#34;kubeadm init&#34; and &#34;kubeadm join&#34; generates at runtime, populating
the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably,
# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.
# KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
</code></pre><p>Ce fichier spécifie les emplacements par défaut pour tous les fichiers gérés par kubeadm pour la kubelet.</p><ul><li>Le fichier KubeConfig à utiliser pour le TLS Bootstrap est <code>/etc/kubernetes/bootstrap-kubelet.conf</code>,
mais il n'est utilisé que si <code>/etc/kubernetes/kubelet.conf</code> n'existe pas.</li><li>Le fichier KubeConfig avec l’identité unique de la kubelet est <code>/etc/kubernetes/kubelet.conf</code>.</li><li>Le fichier contenant le ComponentConfig de la kubelet est <code>/var/lib/kubelet/config.yaml</code>.</li><li>Le fichier d'environnement dynamique qui contient <code>KUBELET_KUBEADM_ARGS</code> est sourcé à partir de
<code>/var/lib/kubelet/kubeadm-flags.env</code>.</li><li>Le fichier qui peut contenir les paramètres surchargés par l'utilisateur avec <code>KUBELET_EXTRA_ARGS</code>
provient de <code>/etc/default/kubelet</code> (pour les DEBs), ou <code>/etc/sysconfig/kubelet</code> (pour les RPMs)
<code>KUBELET_EXTRA_ARGS</code> est le dernier de la chaîne d'options et a la priorité la plus élevée en cas
de conflit de paramètres.</li></ul><h2 id=fichiers-binaires-de-kubernetes-et-contenu-du-package>Fichiers binaires de Kubernetes et contenu du package</h2><p>Les packages DEB et RPM fournis avec les versions de Kubernetes sont les suivants:</p><table><thead><tr><th>Nom du paquet</th><th>Description</th></tr></thead><tbody><tr><td><code>kubeadm</code></td><td>Installe l'outil CLI <code>/usr/bin/kubeadm</code> et <a href=#the-kubelet-drop-in-file-for-systemd>le fichier instantané de kubelet</a> pour la kubelet.</td></tr><tr><td><code>kubelet</code></td><td>Installe <code>/usr/bin/kubelet</code>.</td></tr><tr><td><code>kubectl</code></td><td>Installe <code>/usr/bin/kubectl</code>.</td></tr><tr><td><code>kubernetes-cni</code></td><td>Installe les binaires officiels du CNI dans le repertoire <code>/opt/cni/bin</code>.</td></tr><tr><td><code>cri-tools</code></td><td>Installe <code>/usr/bin/crictl</code> à partir de <a href=https://github.com/kubernetes-incubator/cri-tools>https://github.com/kubernetes-incubator/cri-tools</a>.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-c3689df4b0c61a998e79d91a865aa244>1.8 - Dépanner kubeadm</h1><div class=lead>Diagnostic pannes kubeadm debug</div><p>Comme avec n'importe quel programme, vous pourriez rencontrer une erreur lors de l'installation ou de
l'exécution de kubeadm.
Cette page répertorie certains scénarios d’échec courants et propose des étapes pouvant vous aider à
comprendre et résoudre le problème.</p><p>Si votre problème ne figure pas dans la liste ci-dessous, procédez comme suit:</p><ul><li><p>Si vous pensez que votre problème est un bug avec kubeadm:</p><ul><li>Aller à <a href=https://github.com/kubernetes/kubeadm/issues>github.com/kubernetes/kubeadm</a> et rechercher
les problèmes existants.</li><li>Si aucune issue n'existe, veuillez <a href=https://github.com/kubernetes/kubeadm/issues/new>en ouvrir une</a> et
suivez le modèle ( template ) d'issue</li></ul></li><li><p>Si vous ne savez pas comment fonctionne kubeadm, vous pouvez demander sur <a href=http://slack.k8s.io/>Slack</a>
dans le canal #kubeadm, ou posez une questions sur
<a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a>. Merci d'ajouter les tags pertinents
comme <code>#kubernetes</code> et <code>#kubeadm</code>, ainsi on pourra vous aider.</p></li></ul><h2 id=ebtables-ou-un-exécutable-similaire-introuvable-lors-de-l-installation><code>ebtables</code> ou un exécutable similaire introuvable lors de l'installation</h2><p>Si vous voyez les warnings suivants lors de l'exécution <code>kubeadm init</code></p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ebtables not found in system path
</span></span><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> WARNING: ethtool not found in system path
</span></span></code></pre></div><p>Ensuite, il peut vous manquer <code>ebtables</code>, <code>ethtool</code> ou un exécutable similaire sur votre nœud. Vous
pouvez l'installer avec les commandes suivantes:</p><ul><li>For Ubuntu/Debian users, run <code>apt install ebtables ethtool</code>.</li><li>For CentOS/Fedora users, run <code>yum install ebtables ethtool</code>.</li></ul><h2 id=kubeadm-reste-bloqué-en-attente-du-control-plane-pendant-l-installation>kubeadm reste bloqué en attente du control plane pendant l'installation</h2><p>Si vous remarquez que <code>kubeadm init</code> se bloque après la ligne suivante:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#666>[</span>apiclient<span style=color:#666>]</span> Created API client, waiting <span style=color:#a2f;font-weight:700>for</span> the control plane to become ready
</span></span></code></pre></div><p>Cela peut être causé par un certain nombre de problèmes. Les plus communs sont:</p><ul><li><p>problèmes de connexion réseau. Vérifiez que votre machine dispose d'une connectivité réseau
complète avant de continuer.</p></li><li><p>la configuration du driver cgroup par défaut pour la kubelet diffère de celle utilisée par Docker.
  Vérifiez le fichier journal du système (par exemple, <code>/var/log/message</code>) ou examinez le résultat
de <code>journalctl -u kubelet</code>. Si vous voyez quelque chose comme ce qui suit:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>error: failed to run Kubelet: failed to create kubelet:
</span></span><span style=display:flex><span>misconfiguration: kubelet cgroup driver: <span style=color:#b44>&#34;systemd&#34;</span> is different from docker cgroup driver: <span style=color:#b44>&#34;cgroupfs&#34;</span>
</span></span></code></pre></div><p>Il existe deux méthodes courantes pour résoudre le problème du driver cgroup:</p></li></ul><ol><li>Installez à nouveau Docker en suivant les instructions
<a href=/fr/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-docker>ici</a>.</li><li>Changez manuellement la configuration de la kubelet pour correspondre au driver Docker cgroup, vous pouvez vous référer à
    <a href=/fr/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#configure-cgroup-driver-used-by-kubelet-on-master-node>Configurez le driver de cgroupe utilisé par la kubelet sur le Nœud Master</a>
pour des instruction détaillées.</li></ol><ul><li>Les conteneurs Docker du control plane sont en crashloop ou suspendus. Vous pouvez le vérifier en lançant <code>docker ps</code> et étudier chaque conteneur en exécutant <code>docker logs</code>.</li></ul><h2 id=kubeadm-bloque-lors-de-la-suppression-de-conteneurs-gérés>kubeadm bloque lors de la suppression de conteneurs gérés</h2><p>Les événements suivants peuvent se produire si Docker s'arrête et ne supprime pas les conteneurs gérés
par Kubernetes:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo kubeadm reset
</span></span><span style=display:flex><span><span style=color:#666>[</span>preflight<span style=color:#666>]</span> Running pre-flight checks
</span></span><span style=display:flex><span><span style=color:#666>[</span>reset<span style=color:#666>]</span> Stopping the kubelet service
</span></span><span style=display:flex><span><span style=color:#666>[</span>reset<span style=color:#666>]</span> Unmounting mounted directories in <span style=color:#b44>&#34;/var/lib/kubelet&#34;</span>
</span></span><span style=display:flex><span><span style=color:#666>[</span>reset<span style=color:#666>]</span> Removing kubernetes-managed containers
</span></span><span style=display:flex><span><span style=color:#666>(</span>block<span style=color:#666>)</span>
</span></span></code></pre></div><p>Une solution possible consiste à redémarrer le service Docker, puis à réexécuter <code>kubeadm reset</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo systemctl restart docker.service
</span></span><span style=display:flex><span>sudo kubeadm reset
</span></span></code></pre></div><p>L'inspection des journaux de Docker peut également être utile:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>journalctl -ul docker
</span></span></code></pre></div><h2 id=pods-dans-l-état-runcontainererror-crashloopbackoff-ou-error>Pods dans l'état <code>RunContainerError</code>, <code>CrashLoopBackOff</code> ou <code>Error</code></h2><p>Juste après <code>kubeadm init</code>, il ne devrait pas y avoir de pods dans ces états.</p><ul><li>S'il existe des pods dans l'un de ces états <em>juste après</em> <code>kubeadm init</code>, veuillez ouvrir un
issue dans le dépôt de Kubeadm. <code>coredns</code> (ou<code> kube-dns</code>) devrait être dans l'état <code>Pending</code>
  jusqu'à ce que vous ayez déployé la solution réseau.</li><li>Si vous voyez des pods dans les états <code>RunContainerError</code>, <code>CrashLoopBackOff</code> ou <code>Error</code>
  après le déploiement de la solution réseau et que rien ne se passe pour <code>coredns</code> (ou<code> kube-dns</code>),
  il est très probable que la solution Pod Network que vous avez installée est en quelque sorte
endommagée. Vous devrez peut-être lui accorder plus de privilèges RBAC ou utiliser une version
plus récente. S'il vous plaît créez une issue dans le dépôt du fournisseur de réseau de Pod.</li><li>Si vous installez une version de Docker antérieure à 1.12.1, supprimez l'option <code>MountFlags = slave</code>
  lors du démarrage de <code>dockerd</code> avec <code>systemd</code> et redémarrez <code>docker</code>. Vous pouvez voir les options
de montage dans <code>/usr/lib/systemd/system/docker.service</code>.
Les options de montage peuvent interférer avec les volumes montés par Kubernetes et mettre les
pods dans l'état <code>CrashLoopBackOff</code>. L'erreur se produit lorsque Kubernetes ne trouve pas les fichiers
<code>var/run/secrets/kubernetes.io/serviceaccount</code>.</li></ul><h2 id=coredns-ou-kube-dns-est-bloqué-dans-l-état-pending><code>coredns</code> (ou <code>kube-dns</code>) est bloqué dans l'état <code>Pending</code></h2><p>Ceci est <strong>prévu</strong> et fait partie du design. kubeadm est agnostique vis-à-vis du fournisseur
de réseau, ainsi l'administrateur devrait <a href=/docs/concepts/cluster-administration/addons/>installer la solution réseau pod</a>
de choix. Vous devez installer un réseau de pods avant que CoreDNS ne soit complètement déployé.
D'où l' état <code>Pending</code> avant la mise en place du réseau.</p><h2 id=les-services-hostport-ne-fonctionnent-pas>Les services <code>HostPort</code> ne fonctionnent pas</h2><p>Les fonctionnalités <code>HostPort</code> et <code>HostIP</code> sont disponibles en fonction de votre fournisseur
de réseau de pod. Veuillez contacter l’auteur de la solution de réseau de Pod pour savoir si
Les fonctionnalités <code>HostPort</code> et <code>HostIP</code> sont disponibles.</p><p>Les fournisseurs de CNI Calico, Canal, et Flannel supportent HostPort.</p><p>Pour plus d'informations, voir la <a href=https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md>CNI portmap documentation</a>.</p><p>Si votre fournisseur de réseau ne prend pas en charge le plug-in portmap CNI, vous devrez peut-être utiliser le
<a href=/docs/concepts/services-networking/service/#nodeport>NodePort feature of services</a> ou utiliser <code>HostNetwork=true</code>.</p><h2 id=les-pods-ne-sont-pas-accessibles-via-leur-ip-de-service>Les pods ne sont pas accessibles via leur IP de service</h2><ul><li><p>De nombreux add-ons réseau ne permettent pas encore
<a href=/docs/tasks/debug-application-cluster/debug-service/#a-pod-cannot-reach-itself-via-service-ip>hairpin mode</a>
qui permet aux pods d’accéder à eux-mêmes via leur IP de service. Ceci est un problème lié
au <a href=https://github.com/containernetworking/cni/issues/476>CNI</a>. S'il vous plaît contacter
le fournisseur d'add-on réseau afin d'obtenir des informations en matière de prise en charge du mode hairpin.</p></li><li><p>Si vous utilisez VirtualBox (directement ou via Vagrant), vous devrez vous assurez que
<code>hostname -i</code> renvoie une adresse IP routable. Par défaut la première interface est connectée
à un réseau d’ <code>hôte uniquement</code> non routable. En contournement vous pouvez modifier <code>/etc/hosts</code>,
jetez un œil à ce <a href=https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11>Vagrantfile</a> par exemple.</p></li></ul><h2 id=erreurs-de-certificats-tls>Erreurs de certificats TLS</h2><p>L'erreur suivante indique une possible incompatibilité de certificat.</p><pre tabindex=0><code class=language-none data-lang=none># kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of
&#34;crypto/rsa: verification error&#34; while trying to verify candidate authority certificate &#34;kubernetes&#34;)
</code></pre><ul><li><p>Vérifiez que le fichier <code>$HOME/.kube/config</code> contient un certificat valide, et 
re-générer un certificat si nécessaire. Les certificats dans un fichier kubeconfig 
sont encodés en base64. La commande <code>base64 -d</code> peut être utilisée pour décoder le certificat 
et <code>openssl x509 -text -noout</code> peut être utilisé pour afficher les informations du certificat.</p></li><li><p>Une autre solution consiste à écraser le <code>kubeconfig</code> existant pour l'utilisateur" admin ":</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>mv  <span style=color:#b8860b>$HOME</span>/.kube <span style=color:#b8860b>$HOME</span>/.kube.bak
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div></li></ul><h2 id=carte-réseau-par-défaut-lors-de-l-utilisation-de-flannel-comme-réseau-de-pod-dans-vagrant>Carte réseau par défaut lors de l'utilisation de flannel comme réseau de pod dans Vagrant</h2><p>L'erreur suivante peut indiquer que quelque chose n'allait pas dans le réseau de pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Error from server <span style=color:#666>(</span>NotFound<span style=color:#666>)</span>: the server could not find the requested resource
</span></span></code></pre></div><ul><li>Si vous utilisez flannel comme réseau de pod dans Vagrant, vous devrez spécifier le
nom d'interface par défaut pour flannel.</li></ul><p>  Vagrant attribue généralement deux interfaces à tous les ordinateurs virtuels. La
première, pour laquel tous les hôtes se voient attribuer l’adresse IP <code>10.0.2.15</code>,
est pour le trafic externe qui est NATé.</p><p>  Cela peut entraîner des problèmes avec Flannel, qui utilise par défaut la première
interface sur un hôte. Ceci conduit au fait que tous les hôtes pensent qu'ils ont la
même adresse IP publique. Pour éviter cela, passez l'option <code>--iface eth1</code> sur Flannel
pour que la deuxième interface soit choisie.</p><h2 id=ip-non-publique-utilisée-pour-les-conteneurs>IP non publique utilisée pour les conteneurs</h2><p>Dans certaines situations, les commandes <code>kubectl logs</code> et <code>kubectl run</code> peuvent
renvoyer les erreurs suivantes dans un cluster par ailleurs fonctionnel:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql:
</span></span><span style=display:flex><span>dial tcp 10.19.0.41:10250: getsockopt: no route to host
</span></span></code></pre></div><ul><li><p>Cela peut être dû au fait que Kubernetes utilise une adresse IP qui ne peut pas communiquer
avec d’autres adresses IP même sous-réseau, éventuellement à cause d'une politique mise en place
par le fournisseur de la machine.</p></li><li><p>Digital Ocean attribue une adresse IP publique à <code>eth0</code> ainsi qu’une adresse privée à
utiliser en interne comme IP d'ancrage pour leur fonction IP flottante, mais <code>kubelet</code> choisira cette
dernière comme <code>InternalIP</code> du noeud au lieu du public.</p><p>Utilisez <code>ip addr show</code> pour verifier ce scénario au lieu de <code>ifconfig</code> car <code>ifconfig</code> n'affichera pas
l'alias de l'adresse IP incriminée. Sinon, une API spécifique à Digital Ocean 
permet de rechercher l'adresse IP d'ancrage à partir du droplet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
</span></span></code></pre></div><p>La solution consiste à indiquer à la <code>kubelet</code> l'adresse IP à utiliser avec<code> --node-ip</code>. Lors de
l'utilisation de Digital Ocean, il peut être public (assigné à <code>eth0</code>) ou privé (assigné à<code> eth1</code>)
si vous voulez utiliser le réseau privé optionnel. la
<a href=https://github.com/kubernetes/kubernetes/blob/release-1.13/cmd/kubeadm/app/apis/kubeadm/v1beta1/types.go>la section <code>KubeletExtraArgs</code> de kubeadm <code>NodeRegistrationOptions</code> structure</a> peut être utilisé pour cela.</p><p>Puis redémarrer la <code>kubelet</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li></ul><h2 id=les-pods-coredns-sont-en-état-crashloopbackoff-ou-error>Les pods <code>coredns</code> sont en état<code> CrashLoopBackOff</code> ou <code>Error</code></h2><p>Si vous avez des nœuds qui exécutent SELinux avec une version plus ancienne de Docker, vous risquez
de rencontrer un problème ou les pods de <code>coredns</code> ne démarrent pas. Pour résoudre ce problème, vous pouvez essayer l'une des options suivantes:</p><ul><li>Mise à niveau vers une <a href=/fr/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-docker>nouvelle version de Docker</a>.</li><li><a href=https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux>Désactiver SELinux</a>.</li><li>Modifiez le déploiement de <code>coredns</code> pour définir<code> allowPrivilegeEscalation</code> à <code>true</code>:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kube-system get deployment coredns -o yaml | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  sed <span style=color:#b44>&#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g&#39;</span> | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  kubectl apply -f -
</span></span></code></pre></div><p>une autre raison pour laquelle CoreDNS peut se retrouver dans l'état <code>CrashLoopBackOff</code> est lorsqu'un
Pod de CoreDNS déployé dans Kubernetes détecte une boucle. <a href=https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters>Un certain nombre de solutions de contournement</a>
sont disponibles pour éviter que Kubernetes ne tente de redémarrer le pod CoreDNS chaque fois que CoreDNS détecte une boucle et s'arrête.</p><div class="alert alert-danger warning callout" role=alert><strong>Attention:</strong> Désactiver SELinux ou paramètrer <code>allowPrivilegeEscalation</code> sur <code>true</code> peut compromettre
la sécurité de votre cluster.</div><h2 id=les-pods-etcd-redémarrent-continuellement>Les pods etcd redémarrent continuellement</h2><p>Si vous rencontrez l'erreur suivante:</p><pre tabindex=0><code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container
process caused &#34;process_linux.go:110: decoding init error from pipe caused \&#34;read parent: connection
reset by peer\&#34;&#34;
</code></pre><p>ce problème apparaît si vous exécutez CentOS 7 avec Docker 1.13.1.84.
Cette version de Docker peut empêcher la kubelet de s'exécuter dans le conteneur etcd.</p><p>Pour contourner le problème, choisissez l'une de ces options.:</p><ul><li>Revenir à une version antérieure de Docker, telle que la 1.13.1-75:</li></ul><pre tabindex=0><code>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
</code></pre><ul><li>Installez l'une des versions les plus récentes recommandées, telles que la 18.06:</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span>yum install docker-ce-18.06.1.ce-3.el7.x86_64
</span></span></code></pre></div></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/fr/docs/home/>Accueil</a>
<a class=text-white href=/fr/blog/>Blog</a>
<a class=text-white href=/fr/partners/>Partenaires</a>
<a class=text-white href=/fr/community/>Communauté</a>
<a class=text-white href=/fr/case-studies/>Études de cas</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>