<!doctype html><html lang=en class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/setup/best-practices/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/best-practices/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/best-practices/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/best-practices/><link rel=alternate hreflang=hi href=https://kubernetes.io/hi/docs/setup/best-practices/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/best-practices/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/setup/best-practices/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Best practices | Kubernetes</title><meta property="og:title" content="Best practices"><meta property="og:description" content="Production-Grade Container Orchestration"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/setup/best-practices/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Best practices"><meta itemprop=description content="Production-Grade Container Orchestration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Best practices"><meta name=twitter:description content="Production-Grade Container Orchestration"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/docs/setup/best-practices/"><meta property="og:title" content="Best practices"><meta name=twitter:title content="Best practices"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/setup/best-practices/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/setup/best-practices/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/setup/best-practices/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/setup/best-practices/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/setup/best-practices/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/docs/setup/best-practices/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/setup/best-practices/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/setup/best-practices/>日本語 (Japanese)</a>
<a class=dropdown-item href=/id/docs/setup/best-practices/>Bahasa Indonesia</a>
<a class=dropdown-item href=/hi/docs/setup/best-practices/>हिन्दी (Hindi)</a>
<a class=dropdown-item href=/uk/docs/setup/best-practices/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/setup/best-practices/>Return to the regular view of this page</a>.</p></div><h1 class=title>Best practices</h1><ul><li>1: <a href=#pg-c797ee17120176c685455db89ae091a9>Considerations for large clusters</a></li><li>2: <a href=#pg-970615c97499e3651fd3a98e0387cefc>Running in multiple zones</a></li><li>3: <a href=#pg-f89867de1d34943f1524f67a241f5cc9>Validate node setup</a></li><li>4: <a href=#pg-92a61cf5b0575aa3500f7665b68127d1>Enforcing Pod Security Standards</a></li><li>5: <a href=#pg-0394f813094b7a35058dffe5b8bacd20>PKI certificates and requirements</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-c797ee17120176c685455db89ae091a9>1 - Considerations for large clusters</h1><p>A cluster is a set of <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodes>nodes</a> (physical
or virtual machines) running Kubernetes agents, managed by the
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>.
Kubernetes v1.25 supports clusters with up to 5000 nodes. More specifically,
Kubernetes is designed to accommodate configurations that meet <em>all</em> of the following criteria:</p><ul><li>No more than 110 pods per node</li><li>No more than 5000 nodes</li><li>No more than 150000 total pods</li><li>No more than 300000 total containers</li></ul><p>You can scale your cluster by adding or removing nodes. The way you do this depends
on how your cluster is deployed.</p><h2 id=quota-issues>Cloud provider resource quotas</h2><p>To avoid running into cloud provider quota issues, when creating a cluster with many nodes,
consider:</p><ul><li>Requesting a quota increase for cloud resources such as:<ul><li>Computer instances</li><li>CPUs</li><li>Storage volumes</li><li>In-use IP addresses</li><li>Packet filtering rule sets</li><li>Number of load balancers</li><li>Network subnets</li><li>Log streams</li></ul></li><li>Gating the cluster scaling actions to bring up new nodes in batches, with a pause
between batches, because some cloud providers rate limit the creation of new instances.</li></ul><h2 id=control-plane-components>Control plane components</h2><p>For a large cluster, you need a control plane with sufficient compute and other
resources.</p><p>Typically you would run one or two control plane instances per failure zone,
scaling those instances vertically first and then scaling horizontally after reaching
the point of falling returns to (vertical) scale.</p><p>You should run at least one instance per failure zone to provide fault-tolerance. Kubernetes
nodes do not automatically steer traffic towards control-plane endpoints that are in the
same failure zone; however, your cloud provider might have its own mechanisms to do this.</p><p>For example, using a managed load balancer, you configure the load balancer to send traffic
that originates from the kubelet and Pods in failure zone <em>A</em>, and direct that traffic only
to the control plane hosts that are also in zone <em>A</em>. If a single control-plane host or
endpoint failure zone <em>A</em> goes offline, that means that all the control-plane traffic for
nodes in zone <em>A</em> is now being sent between zones. Running multiple control plane hosts in
each zone makes that outcome less likely.</p><h3 id=etcd-storage>etcd storage</h3><p>To improve performance of large clusters, you can store Event objects in a separate
dedicated etcd instance.</p><p>When creating a cluster, you can (using custom tooling):</p><ul><li>start and configure additional etcd instance</li><li>configure the <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a> to use it for storing events</li></ul><p>See <a href=/docs/tasks/administer-cluster/configure-upgrade-etcd/>Operating etcd clusters for Kubernetes</a> and
<a href=/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>Set up a High Availability etcd cluster with kubeadm</a>
for details on configuring and managing etcd for a large cluster.</p><h2 id=addon-resources>Addon resources</h2><p>Kubernetes <a href=/docs/concepts/configuration/manage-resources-containers/>resource limits</a>
help to minimize the impact of memory leaks and other ways that pods and containers can
impact on other components. These resource limits apply to
<a class=glossary-tooltip title='Resources that extend the functionality of Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/cluster-administration/addons/ target=_blank aria-label=addon>addon</a> resources just as they apply to application workloads.</p><p>For example, you can set CPU and memory limits for a logging component:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-cloud-logging<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>fluent/fluentd-kubernetes-daemonset:v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span></code></pre></div><p>Addons' default limits are typically based on data collected from experience running
each addon on small or medium Kubernetes clusters. When running on large
clusters, addons often consume more of some resources than their default limits.
If a large cluster is deployed without adjusting these values, the addon(s)
may continuously get killed because they keep hitting the memory limit.
Alternatively, the addon may run but with poor performance due to CPU time
slice restrictions.</p><p>To avoid running into cluster addon resource issues, when creating a cluster with
many nodes, consider the following:</p><ul><li>Some addons scale vertically - there is one replica of the addon for the cluster
or serving a whole failure zone. For these addons, increase requests and limits
as you scale out your cluster.</li><li>Many addons scale horizontally - you add capacity by running more pods - but with
a very large cluster you may also need to raise CPU or memory limits slightly.
The VerticalPodAutoscaler can run in <em>recommender</em> mode to provide suggested
figures for requests and limits.</li><li>Some addons run as one copy per node, controlled by a <a class=glossary-tooltip title='Ensures a copy of a Pod is running across a set of nodes in a cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a>: for example, a node-level log aggregator. Similar to
the case with horizontally-scaled addons, you may also need to raise CPU or memory
limits slightly.</li></ul><h2 id=what-s-next>What's next</h2><p><code>VerticalPodAutoscaler</code> is a custom resource that you can deploy into your cluster
to help you manage resource requests and limits for pods.<br>Visit <a href=https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#readme>Vertical Pod Autoscaler</a>
to learn more about <code>VerticalPodAutoscaler</code> and how you can use it to scale cluster
components, including cluster-critical addons.</p><p>The <a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme>cluster autoscaler</a>
integrates with a number of cloud providers to help you run the right number of
nodes for the level of resource demand in your cluster.</p><p>The <a href=https://github.com/kubernetes/autoscaler/tree/master/addon-resizer#readme>addon resizer</a>
helps you in resizing the addons automatically as your cluster's scale changes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-970615c97499e3651fd3a98e0387cefc>2 - Running in multiple zones</h1><p>This page describes running Kubernetes across multiple zones.</p><h2 id=background>Background</h2><p>Kubernetes is designed so that a single Kubernetes cluster can run
across multiple failure zones, typically where these zones fit within
a logical grouping called a <em>region</em>. Major cloud providers define a region
as a set of failure zones (also called <em>availability zones</em>) that provide
a consistent set of features: within a region, each zone offers the same
APIs and services.</p><p>Typical cloud architectures aim to minimize the chance that a failure in
one zone also impairs services in another zone.</p><h2 id=control-plane-behavior>Control plane behavior</h2><p>All <a href=/docs/concepts/overview/components/#control-plane-components>control plane components</a>
support running as a pool of interchangeable resources, replicated per
component.</p><p>When you deploy a cluster control plane, place replicas of
control plane components across multiple failure zones. If availability is
an important concern, select at least three failure zones and replicate
each individual control plane component (API server, scheduler, etcd,
cluster controller manager) across at least three failure zones.
If you are running a cloud controller manager then you should
also replicate this across all the failure zones you selected.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes does not provide cross-zone resilience for the API server
endpoints. You can use various techniques to improve availability for
the cluster API server, including DNS round-robin, SRV records, or
a third-party load balancing solution with health checking.</div><h2 id=node-behavior>Node behavior</h2><p>Kubernetes automatically spreads the Pods for
workload resources (such as <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>
or <a class=glossary-tooltip title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>)
across different nodes in a cluster. This spreading helps
reduce the impact of failures.</p><p>When nodes start up, the kubelet on each node automatically adds
<a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a> to the Node object
that represents that specific kubelet in the Kubernetes API.
These labels can include
<a href=/docs/reference/labels-annotations-taints/#topologykubernetesiozone>zone information</a>.</p><p>If your cluster spans multiple zones or regions, you can use node labels
in conjunction with
<a href=/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod topology spread constraints</a>
to control how Pods are spread across your cluster among fault domains:
regions, zones, and even specific nodes.
These hints enable the
<a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a> to place
Pods for better expected availability, reducing the risk that a correlated
failure affects your whole workload.</p><p>For example, you can set a constraint to make sure that the
3 replicas of a StatefulSet are all running in different zones to each
other, whenever that is feasible. You can define this declaratively
without explicitly defining which availability zones are in use for
each workload.</p><h3 id=distributing-nodes-across-zones>Distributing nodes across zones</h3><p>Kubernetes' core does not create nodes for you; you need to do that yourself,
or use a tool such as the <a href=https://cluster-api.sigs.k8s.io/>Cluster API</a> to
manage nodes on your behalf.</p><p>Using tools such as the Cluster API you can define sets of machines to run as
worker nodes for your cluster across multiple failure domains, and rules to
automatically heal the cluster in case of whole-zone service disruption.</p><h2 id=manual-zone-assignment-for-pods>Manual zone assignment for Pods</h2><p>You can apply <a href=/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector>node selector constraints</a>
to Pods that you create, as well as to Pod templates in workload resources
such as Deployment, StatefulSet, or Job.</p><h2 id=storage-access-for-zones>Storage access for zones</h2><p>When persistent volumes are created, the <code>PersistentVolumeLabel</code>
<a href=/docs/reference/access-authn-authz/admission-controllers/>admission controller</a>
automatically adds zone labels to any PersistentVolumes that are linked to a specific
zone. The <a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a> then ensures,
through its <code>NoVolumeZoneConflict</code> predicate, that pods which claim a given PersistentVolume
are only placed into the same zone as that volume.</p><p>You can specify a <a class=glossary-tooltip title='A StorageClass provides a way for administrators to describe different available storage types.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/storage-classes target=_blank aria-label=StorageClass>StorageClass</a>
for PersistentVolumeClaims that specifies the failure domains (zones) that the
storage in that class may use.
To learn about configuring a StorageClass that is aware of failure domains or zones,
see <a href=/docs/concepts/storage/storage-classes/#allowed-topologies>Allowed topologies</a>.</p><h2 id=networking>Networking</h2><p>By itself, Kubernetes does not include zone-aware networking. You can use a
<a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>network plugin</a>
to configure cluster networking, and that network solution might have zone-specific
elements. For example, if your cloud provider supports Services with
<code>type=LoadBalancer</code>, the load balancer might only send traffic to Pods running in the
same zone as the load balancer element processing a given connection.
Check your cloud provider's documentation for details.</p><p>For custom or on-premises deployments, similar considerations apply.
<a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> and
<a class=glossary-tooltip title='An API object that manages external access to the services in a cluster, typically HTTP.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/ingress/ target=_blank aria-label=Ingress>Ingress</a> behavior, including handling
of different failure zones, does vary depending on exactly how your cluster is set up.</p><h2 id=fault-recovery>Fault recovery</h2><p>When you set up your cluster, you might also need to consider whether and how
your setup can restore service if all the failure zones in a region go
off-line at the same time. For example, do you rely on there being at least
one node able to run Pods in a zone?<br>Make sure that any cluster-critical repair work does not rely
on there being at least one healthy node in your cluster. For example: if all nodes
are unhealthy, you might need to run a repair Job with a special
<a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=toleration>toleration</a> so that the repair
can complete enough to bring at least one node into service.</p><p>Kubernetes doesn't come with an answer for this challenge; however, it's
something to consider.</p><h2 id=what-s-next>What's next</h2><p>To learn how the scheduler places Pods in a cluster, honoring the configured constraints,
visit <a href=/docs/concepts/scheduling-eviction/>Scheduling and Eviction</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-f89867de1d34943f1524f67a241f5cc9>3 - Validate node setup</h1><h2 id=node-conformance-test>Node Conformance Test</h2><p><em>Node conformance test</em> is a containerized test framework that provides a system
verification and functionality test for a node. The test validates whether the
node meets the minimum requirements for Kubernetes; a node that passes the test
is qualified to join a Kubernetes cluster.</p><h2 id=node-prerequisite>Node Prerequisite</h2><p>To run node conformance test, a node must satisfy the same prerequisites as a
standard Kubernetes node. At a minimum, the node should have the following
daemons installed:</p><ul><li>Container Runtime (Docker)</li><li>Kubelet</li></ul><h2 id=running-node-conformance-test>Running Node Conformance Test</h2><p>To run the node conformance test, perform the following steps:</p><ol><li>Work out the value of the <code>--kubeconfig</code> option for the kubelet; for example:
<code>--kubeconfig=/var/lib/kubelet/config.yaml</code>.
Because the test framework starts a local control plane to test the kubelet,
use <code>http://localhost:8080</code> as the URL of the API server.
There are some other kubelet command line parameters you may want to use:</li></ol><ul><li><code>--cloud-provider</code>: If you are using <code>--cloud-provider=gce</code>, you should
remove the flag to run the test.</li></ul><ol start=2><li>Run the node conformance test with command:</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># $CONFIG_DIR is the pod manifest path of your Kubelet.</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># $LOG_DIR is the test output path.</span>
</span></span><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  registry.k8s.io/node-test:0.2
</span></span></code></pre></div><h2 id=running-node-conformance-test-for-other-architectures>Running Node Conformance Test for Other Architectures</h2><p>Kubernetes also provides node conformance test docker images for other
architectures:</p><table><thead><tr><th>Arch</th><th style=text-align:center>Image</th></tr></thead><tbody><tr><td>amd64</td><td style=text-align:center>node-test-amd64</td></tr><tr><td>arm</td><td style=text-align:center>node-test-arm</td></tr><tr><td>arm64</td><td style=text-align:center>node-test-arm64</td></tr></tbody></table><h2 id=running-selected-test>Running Selected Test</h2><p>To run specific tests, overwrite the environment variable <code>FOCUS</code> with the
regular expression of tests you want to run.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>FOCUS</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># Only run MirrorPod test</span>
</span></span><span style=display:flex><span>  registry.k8s.io/node-test:0.2
</span></span></code></pre></div><p>To skip specific tests, overwrite the environment variable <code>SKIP</code> with the
regular expression of tests you want to skip.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo docker run -it --rm --privileged --net<span style=color:#666>=</span>host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -v /:/rootfs:ro -v <span style=color:#b8860b>$CONFIG_DIR</span>:<span style=color:#b8860b>$CONFIG_DIR</span> -v <span style=color:#b8860b>$LOG_DIR</span>:/var/result <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  -e <span style=color:#b8860b>SKIP</span><span style=color:#666>=</span>MirrorPod <span style=color:#b62;font-weight:700>\ </span><span style=color:#080;font-style:italic># Run all conformance tests but skip MirrorPod test</span>
</span></span><span style=display:flex><span>  registry.k8s.io/node-test:0.2
</span></span></code></pre></div><p>Node conformance test is a containerized version of <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/e2e-node-tests.md>node e2e test</a>.
By default, it runs all conformance tests.</p><p>Theoretically, you can run any node e2e test if you configure the container and
mount required volumes properly. But <strong>it is strongly recommended to only run conformance
test</strong>, because it requires much more complex configuration to run non-conformance test.</p><h2 id=caveats>Caveats</h2><ul><li>The test leaves some docker images on the node, including the node conformance
test image and images of containers used in the functionality
test.</li><li>The test leaves dead containers on the node. These containers are created
during the functionality test.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-92a61cf5b0575aa3500f7665b68127d1>4 - Enforcing Pod Security Standards</h1><p>This page provides an overview of best practices when it comes to enforcing
<a href=/docs/concepts/security/pod-security-standards>Pod Security Standards</a>.</p><h2 id=using-the-built-in-pod-security-admission-controller>Using the built-in Pod Security Admission Controller</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>The <a href=/docs/reference/access-authn-authz/admission-controllers/#podsecurity>Pod Security Admission Controller</a>
intends to replace the deprecated PodSecurityPolicies.</p><h3 id=configure-all-cluster-namespaces>Configure all cluster namespaces</h3><p>Namespaces that lack any configuration at all should be considered significant gaps in your cluster
security model. We recommend taking the time to analyze the types of workloads occurring in each
namespace, and by referencing the Pod Security Standards, decide on an appropriate level for
each of them. Unlabeled namespaces should only indicate that they've yet to be evaluated.</p><p>In the scenario that all workloads in all namespaces have the same security requirements,
we provide an <a href=/docs/tasks/configure-pod-container/enforce-standards-namespace-labels/#applying-to-all-namespaces>example</a>
that illustrates how the PodSecurity labels can be applied in bulk.</p><h3 id=embrace-the-principle-of-least-privilege>Embrace the principle of least privilege</h3><p>In an ideal world, every pod in every namespace would meet the requirements of the <code>restricted</code>
policy. However, this is not possible nor practical, as some workloads will require elevated
privileges for legitimate reasons.</p><ul><li>Namespaces allowing <code>privileged</code> workloads should establish and enforce appropriate access controls.</li><li>For workloads running in those permissive namespaces, maintain documentation about their unique
security requirements. If at all possible, consider how those requirements could be further
constrained.</li></ul><h3 id=adopt-a-multi-mode-strategy>Adopt a multi-mode strategy</h3><p>The <code>audit</code> and <code>warn</code> modes of the Pod Security Standards admission controller make it easy to
collect important security insights about your pods without breaking existing workloads.</p><p>It is good practice to enable these modes for all namespaces, setting them to the <em>desired</em> level
and version you would eventually like to <code>enforce</code>. The warnings and audit annotations generated in
this phase can guide you toward that state. If you expect workload authors to make changes to fit
within the desired level, enable the <code>warn</code> mode. If you expect to use audit logs to monitor/drive
changes to fit within the desired level, enable the <code>audit</code> mode.</p><p>When you have the <code>enforce</code> mode set to your desired value, these modes can still be useful in a
few different ways:</p><ul><li>By setting <code>warn</code> to the same level as <code>enforce</code>, clients will receive warnings when attempting
to create Pods (or resources that have Pod templates) that do not pass validation. This will help
them update those resources to become compliant.</li><li>In Namespaces that pin <code>enforce</code> to a specific non-latest version, setting the <code>audit</code> and <code>warn</code>
modes to the same level as <code>enforce</code>, but to the <code>latest</code> version, gives visibility into settings
that were allowed by previous versions but are not allowed per current best practices.</li></ul><h2 id=third-party-alternatives>Third-party alternatives</h2><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>Other alternatives for enforcing security profiles are being developed in the Kubernetes
ecosystem:</p><ul><li><a href=https://github.com/kubewarden>Kubewarden</a>.</li><li><a href=https://kyverno.io/policies/>Kyverno</a>.</li><li><a href=https://github.com/open-policy-agent/gatekeeper>OPA Gatekeeper</a>.</li></ul><p>The decision to go with a <em>built-in</em> solution (e.g. PodSecurity admission controller) versus a
third-party tool is entirely dependent on your own situation. When evaluating any solution,
trust of your supply chain is crucial. Ultimately, using <em>any</em> of the aforementioned approaches
will be better than doing nothing.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0394f813094b7a35058dffe5b8bacd20>5 - PKI certificates and requirements</h1><p>Kubernetes requires PKI certificates for authentication over TLS.
If you install Kubernetes with <a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a>, the certificates that your cluster requires are automatically generated.
You can also generate your own certificates -- for example, to keep your private keys more secure by not storing them on the API server.
This page explains the certificates that your cluster requires.</p><h2 id=how-certificates-are-used-by-your-cluster>How certificates are used by your cluster</h2><p>Kubernetes requires PKI for the following operations:</p><ul><li>Client certificates for the kubelet to authenticate to the API server</li><li>Kubelet <a href=/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/#client-and-serving-certificates>server certificates</a>
for the API server to talk to the kubelets</li><li>Server certificate for the API server endpoint</li><li>Client certificates for administrators of the cluster to authenticate to the API server</li><li>Client certificates for the API server to talk to the kubelets</li><li>Client certificate for the API server to talk to etcd</li><li>Client certificate/kubeconfig for the controller manager to talk to the API server</li><li>Client certificate/kubeconfig for the scheduler to talk to the API server.</li><li>Client and server certificates for the <a href=/docs/tasks/extend-kubernetes/configure-aggregation-layer/>front-proxy</a></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> <code>front-proxy</code> certificates are required only if you run kube-proxy to support <a href=/docs/tasks/extend-kubernetes/setup-extension-api-server/>an extension API server</a>.</div><p>etcd also implements mutual TLS to authenticate clients and peers.</p><h2 id=where-certificates-are-stored>Where certificates are stored</h2><p>If you install Kubernetes with kubeadm, most certificates are stored in <code>/etc/kubernetes/pki</code>. All paths in this documentation are relative to that directory, with the exception of user account certificates which kubeadm places in <code>/etc/kubernetes</code>.</p><h2 id=configure-certificates-manually>Configure certificates manually</h2><p>If you don't want kubeadm to generate the required certificates, you can create them using a single root CA or by providing all certificates. See <a href=/docs/tasks/administer-cluster/certificates/>Certificates</a> for details on creating your own certificate authority.
See <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/>Certificate Management with kubeadm</a> for more on managing certificates.</p><h3 id=single-root-ca>Single root CA</h3><p>You can create a single root CA, controlled by an administrator. This root CA can then create multiple intermediate CAs, and delegate all further creation to Kubernetes itself.</p><p>Required CAs:</p><table><thead><tr><th>path</th><th>Default CN</th><th>description</th></tr></thead><tbody><tr><td>ca.crt,key</td><td>kubernetes-ca</td><td>Kubernetes general CA</td></tr><tr><td>etcd/ca.crt,key</td><td>etcd-ca</td><td>For all etcd-related functions</td></tr><tr><td>front-proxy-ca.crt,key</td><td>kubernetes-front-proxy-ca</td><td>For the <a href=/docs/tasks/extend-kubernetes/configure-aggregation-layer/>front-end proxy</a></td></tr></tbody></table><p>On top of the above CAs, it is also necessary to get a public/private key pair for service account management, <code>sa.key</code> and <code>sa.pub</code>.
The following example illustrates the CA key and certificate files shown in the previous table:</p><pre tabindex=0><code>/etc/kubernetes/pki/ca.crt
/etc/kubernetes/pki/ca.key
/etc/kubernetes/pki/etcd/ca.crt
/etc/kubernetes/pki/etcd/ca.key
/etc/kubernetes/pki/front-proxy-ca.crt
/etc/kubernetes/pki/front-proxy-ca.key
</code></pre><h3 id=all-certificates>All certificates</h3><p>If you don't wish to copy the CA private keys to your cluster, you can generate all certificates yourself.</p><p>Required certificates:</p><table><thead><tr><th>Default CN</th><th>Parent CA</th><th>O (in Subject)</th><th>kind</th><th>hosts (SAN)</th></tr></thead><tbody><tr><td>kube-etcd</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-peer</td><td>etcd-ca</td><td></td><td>server, client</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>localhost</code>, <code>127.0.0.1</code></td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd-ca</td><td></td><td>client</td><td></td></tr><tr><td>kube-apiserver-etcd-client</td><td>etcd-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>kube-apiserver</td><td>kubernetes-ca</td><td></td><td>server</td><td><code>&lt;hostname></code>, <code>&lt;Host_IP></code>, <code>&lt;advertise_IP></code>, <code>[1]</code></td></tr><tr><td>kube-apiserver-kubelet-client</td><td>kubernetes-ca</td><td>system:masters</td><td>client</td><td></td></tr><tr><td>front-proxy-client</td><td>kubernetes-front-proxy-ca</td><td></td><td>client</td><td></td></tr></tbody></table><p>[1]: any other IP or DNS name you contact your cluster on (as used by <a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a>
the load balancer stable IP and/or DNS name, <code>kubernetes</code>, <code>kubernetes.default</code>, <code>kubernetes.default.svc</code>,
<code>kubernetes.default.svc.cluster</code>, <code>kubernetes.default.svc.cluster.local</code>)</p><p>where <code>kind</code> maps to one or more of the <a href=https://pkg.go.dev/k8s.io/api/certificates/v1beta1#KeyUsage>x509 key usage</a> types:</p><table><thead><tr><th>kind</th><th>Key usage</th></tr></thead><tbody><tr><td>server</td><td>digital signature, key encipherment, server auth</td></tr><tr><td>client</td><td>digital signature, key encipherment, client auth</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Hosts/SAN listed above are the recommended ones for getting a working cluster; if required by a specific setup, it is possible to add additional SANs on all the server certificates.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>For kubeadm users only:</p><ul><li>The scenario where you are copying to your cluster CA certificates without private keys is referred as external CA in the kubeadm documentation.</li><li>If you are comparing the above list with a kubeadm generated PKI, please be aware that <code>kube-etcd</code>, <code>kube-etcd-peer</code> and <code>kube-etcd-healthcheck-client</code> certificates
are not generated in case of external etcd.</li></ul></div><h3 id=certificate-paths>Certificate paths</h3><p>Certificates should be placed in a recommended path (as used by <a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a>).
Paths should be specified using the given argument regardless of location.</p><table><thead><tr><th>Default CN</th><th>recommended key path</th><th>recommended cert path</th><th>command</th><th>key argument</th><th>cert argument</th></tr></thead><tbody><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>kube-apiserver</td><td></td><td>--etcd-cafile</td></tr><tr><td>kube-apiserver-etcd-client</td><td>apiserver-etcd-client.key</td><td>apiserver-etcd-client.crt</td><td>kube-apiserver</td><td>--etcd-keyfile</td><td>--etcd-certfile</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-apiserver</td><td></td><td>--client-ca-file</td></tr><tr><td>kubernetes-ca</td><td>ca.key</td><td>ca.crt</td><td>kube-controller-manager</td><td>--cluster-signing-key-file</td><td>--client-ca-file, --root-ca-file, --cluster-signing-cert-file</td></tr><tr><td>kube-apiserver</td><td>apiserver.key</td><td>apiserver.crt</td><td>kube-apiserver</td><td>--tls-private-key-file</td><td>--tls-cert-file</td></tr><tr><td>kube-apiserver-kubelet-client</td><td>apiserver-kubelet-client.key</td><td>apiserver-kubelet-client.crt</td><td>kube-apiserver</td><td>--kubelet-client-key</td><td>--kubelet-client-certificate</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-apiserver</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-ca</td><td>front-proxy-ca.key</td><td>front-proxy-ca.crt</td><td>kube-controller-manager</td><td></td><td>--requestheader-client-ca-file</td></tr><tr><td>front-proxy-client</td><td>front-proxy-client.key</td><td>front-proxy-client.crt</td><td>kube-apiserver</td><td>--proxy-client-key-file</td><td>--proxy-client-cert-file</td></tr><tr><td>etcd-ca</td><td>etcd/ca.key</td><td>etcd/ca.crt</td><td>etcd</td><td></td><td>--trusted-ca-file, --peer-trusted-ca-file</td></tr><tr><td>kube-etcd</td><td>etcd/server.key</td><td>etcd/server.crt</td><td>etcd</td><td>--key-file</td><td>--cert-file</td></tr><tr><td>kube-etcd-peer</td><td>etcd/peer.key</td><td>etcd/peer.crt</td><td>etcd</td><td>--peer-key-file</td><td>--peer-cert-file</td></tr><tr><td>etcd-ca</td><td></td><td>etcd/ca.crt</td><td>etcdctl</td><td></td><td>--cacert</td></tr><tr><td>kube-etcd-healthcheck-client</td><td>etcd/healthcheck-client.key</td><td>etcd/healthcheck-client.crt</td><td>etcdctl</td><td>--key</td><td>--cert</td></tr></tbody></table><p>Same considerations apply for the service account key pair:</p><table><thead><tr><th>private key path</th><th>public key path</th><th>command</th><th>argument</th></tr></thead><tbody><tr><td>sa.key</td><td></td><td>kube-controller-manager</td><td>--service-account-private-key-file</td></tr><tr><td></td><td>sa.pub</td><td>kube-apiserver</td><td>--service-account-key-file</td></tr></tbody></table><p>The following example illustrates the file paths <a href=/docs/setup/best-practices/certificates/#certificate-paths>from the previous tables</a> you need to provide if you are generating all of your own keys and certificates:</p><pre tabindex=0><code>/etc/kubernetes/pki/etcd/ca.key
/etc/kubernetes/pki/etcd/ca.crt
/etc/kubernetes/pki/apiserver-etcd-client.key
/etc/kubernetes/pki/apiserver-etcd-client.crt
/etc/kubernetes/pki/ca.key
/etc/kubernetes/pki/ca.crt
/etc/kubernetes/pki/apiserver.key
/etc/kubernetes/pki/apiserver.crt
/etc/kubernetes/pki/apiserver-kubelet-client.key
/etc/kubernetes/pki/apiserver-kubelet-client.crt
/etc/kubernetes/pki/front-proxy-ca.key
/etc/kubernetes/pki/front-proxy-ca.crt
/etc/kubernetes/pki/front-proxy-client.key
/etc/kubernetes/pki/front-proxy-client.crt
/etc/kubernetes/pki/etcd/server.key
/etc/kubernetes/pki/etcd/server.crt
/etc/kubernetes/pki/etcd/peer.key
/etc/kubernetes/pki/etcd/peer.crt
/etc/kubernetes/pki/etcd/healthcheck-client.key
/etc/kubernetes/pki/etcd/healthcheck-client.crt
/etc/kubernetes/pki/sa.key
/etc/kubernetes/pki/sa.pub
</code></pre><h2 id=configure-certificates-for-user-accounts>Configure certificates for user accounts</h2><p>You must manually configure these administrator account and service accounts:</p><table><thead><tr><th>filename</th><th>credential name</th><th>Default CN</th><th>O (in Subject)</th></tr></thead><tbody><tr><td>admin.conf</td><td>default-admin</td><td>kubernetes-admin</td><td>system:masters</td></tr><tr><td>kubelet.conf</td><td>default-auth</td><td>system:node:<code>&lt;nodeName></code> (see note)</td><td>system:nodes</td></tr><tr><td>controller-manager.conf</td><td>default-controller-manager</td><td>system:kube-controller-manager</td><td></td></tr><tr><td>scheduler.conf</td><td>default-scheduler</td><td>system:kube-scheduler</td><td></td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The value of <code>&lt;nodeName></code> for <code>kubelet.conf</code> <strong>must</strong> match precisely the value of the node name provided by the kubelet as it registers with the apiserver. For further details, read the <a href=/docs/reference/access-authn-authz/node/>Node Authorization</a>.</div><ol><li><p>For each config, generate an x509 cert/key pair with the given CN and O.</p></li><li><p>Run <code>kubectl</code> as follows for each config:</p></li></ol><pre tabindex=0><code>KUBECONFIG=&lt;filename&gt; kubectl config set-cluster default-cluster --server=https://&lt;host ip&gt;:6443 --certificate-authority &lt;path-to-kubernetes-ca&gt; --embed-certs
KUBECONFIG=&lt;filename&gt; kubectl config set-credentials &lt;credential-name&gt; --client-key &lt;path-to-key&gt;.pem --client-certificate &lt;path-to-cert&gt;.pem --embed-certs
KUBECONFIG=&lt;filename&gt; kubectl config set-context default-system --cluster default-cluster --user &lt;credential-name&gt;
KUBECONFIG=&lt;filename&gt; kubectl config use-context default-system
</code></pre><p>These files are used as follows:</p><table><thead><tr><th>filename</th><th>command</th><th>comment</th></tr></thead><tbody><tr><td>admin.conf</td><td>kubectl</td><td>Configures administrator user for the cluster</td></tr><tr><td>kubelet.conf</td><td>kubelet</td><td>One required for each node in the cluster.</td></tr><tr><td>controller-manager.conf</td><td>kube-controller-manager</td><td>Must be added to manifest in <code>manifests/kube-controller-manager.yaml</code></td></tr><tr><td>scheduler.conf</td><td>kube-scheduler</td><td>Must be added to manifest in <code>manifests/kube-scheduler.yaml</code></td></tr></tbody></table><p>The following files illustrate full paths to the files listed in the previous table:</p><pre tabindex=0><code>/etc/kubernetes/admin.conf
/etc/kubernetes/kubelet.conf
/etc/kubernetes/controller-manager.conf
/etc/kubernetes/scheduler.conf
</code></pre></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>