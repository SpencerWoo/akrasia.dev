<!doctype html><html lang=en class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/setup/production-environment/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/setup/production-environment/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/setup/production-environment/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/setup/production-environment/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/setup/production-environment/><link rel=alternate hreflang=hi href=https://kubernetes.io/hi/docs/setup/production-environment/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/setup/production-environment/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/setup/production-environment/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Production environment | Kubernetes</title><meta property="og:title" content="Production environment"><meta property="og:description" content="Create a production-quality Kubernetes cluster"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/setup/production-environment/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Production environment"><meta itemprop=description content="Create a production-quality Kubernetes cluster"><meta name=twitter:card content="summary"><meta name=twitter:title content="Production environment"><meta name=twitter:description content="Create a production-quality Kubernetes cluster"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Create a production-quality Kubernetes cluster"><meta property="og:description" content="Create a production-quality Kubernetes cluster"><meta name=twitter:description content="Create a production-quality Kubernetes cluster"><meta property="og:url" content="https://kubernetes.io/docs/setup/production-environment/"><meta property="og:title" content="Production environment"><meta name=twitter:title content="Production environment"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/setup/production-environment/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/setup/production-environment/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/setup/production-environment/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/setup/production-environment/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/setup/production-environment/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/docs/setup/production-environment/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/setup/production-environment/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/setup/production-environment/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/setup/production-environment/>Français (French)</a>
<a class=dropdown-item href=/id/docs/setup/production-environment/>Bahasa Indonesia</a>
<a class=dropdown-item href=/hi/docs/setup/production-environment/>हिन्दी (Hindi)</a>
<a class=dropdown-item href=/uk/docs/setup/production-environment/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/setup/production-environment/>Return to the regular view of this page</a>.</p></div><h1 class=title>Production environment</h1><div class=lead>Create a production-quality Kubernetes cluster</div><ul><li>1: <a href=#pg-a77d3feb6e6d9978f32fa14622642e9a>Container Runtimes</a></li><li>2: <a href=#pg-00e1646f68aeb89f9722cf6f6cfcad94>Installing Kubernetes with deployment tools</a></li><ul><li>2.1: <a href=#pg-a16f59f325a17cdeed324d5c889f7f73>Bootstrapping clusters with kubeadm</a></li><ul><li>2.1.1: <a href=#pg-29e59491dd6118b23072dfe9ebb93323>Installing kubeadm</a></li><li>2.1.2: <a href=#pg-c3689df4b0c61a998e79d91a865aa244>Troubleshooting kubeadm</a></li><li>2.1.3: <a href=#pg-134ed1f6142a98e6ac681a1ba4920e53>Creating a cluster with kubeadm</a></li><li>2.1.4: <a href=#pg-4c656c5eda3e1c06ad1aedebdc04a211>Customizing components with the kubeadm API</a></li><li>2.1.5: <a href=#pg-015edbc7cc688d31b1d1edce7c186135>Options for Highly Available Topology</a></li><li>2.1.6: <a href=#pg-3941d5c3409342219bf7e03128b8ecb6>Creating Highly Available Clusters with kubeadm</a></li><li>2.1.7: <a href=#pg-8160424c22d24f7d2d63c521e107dbf8>Set up a High Availability etcd Cluster with kubeadm</a></li><li>2.1.8: <a href=#pg-07709e71de6b4ac2573041c31213dbeb>Configuring each kubelet in your cluster using kubeadm</a></li><li>2.1.9: <a href=#pg-df2f3f20d404ebe2b03fcda1fcee50e7>Dual-stack support with kubeadm</a></li></ul><li>2.2: <a href=#pg-478acca1934b6d89a0bc00fb25bfe5b6>Installing Kubernetes with kOps</a></li><li>2.3: <a href=#pg-f8b4964187fe973644e06ee629eff1de>Installing Kubernetes with Kubespray</a></li></ul><li>3: <a href=#pg-d2f55eefe7222b7c637875af9c3ec199>Turnkey Cloud Solutions</a></li></ul><div class=content><p>A production-quality Kubernetes cluster requires planning and preparation.
If your Kubernetes cluster is to run critical workloads, it must be configured to be resilient.
This page explains steps you can take to set up a production-ready cluster,
or to promote an existing cluster for production use.
If you're already familiar with production setup and want the links, skip to
<a href=#what-s-next>What's next</a>.</p><h2 id=production-considerations>Production considerations</h2><p>Typically, a production Kubernetes cluster environment has more requirements than a
personal learning, development, or test environment Kubernetes. A production environment may require
secure access by many users, consistent availability, and the resources to adapt
to changing demands.</p><p>As you decide where you want your production Kubernetes environment to live
(on premises or in a cloud) and the amount of management you want to take
on or hand to others, consider how your requirements for a Kubernetes cluster
are influenced by the following issues:</p><ul><li><p><em>Availability</em>: A single-machine Kubernetes <a href=/docs/setup/#learning-environment>learning environment</a>
has a single point of failure. Creating a highly available cluster means considering:</p><ul><li>Separating the control plane from the worker nodes.</li><li>Replicating the control plane components on multiple nodes.</li><li>Load balancing traffic to the cluster’s <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a>.</li><li>Having enough worker nodes available, or able to quickly become available, as changing workloads warrant it.</li></ul></li><li><p><em>Scale</em>: If you expect your production Kubernetes environment to receive a stable amount of
demand, you might be able to set up for the capacity you need and be done. However,
if you expect demand to grow over time or change dramatically based on things like
season or special events, you need to plan how to scale to relieve increased
pressure from more requests to the control plane and worker nodes or scale down to reduce unused
resources.</p></li><li><p><em>Security and access management</em>: You have full admin privileges on your own
Kubernetes learning cluster. But shared clusters with important workloads, and
more than one or two users, require a more refined approach to who and what can
access cluster resources. You can use role-based access control
(<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>) and other
security mechanisms to make sure that users and workloads can get access to the
resources they need, while keeping workloads, and the cluster itself, secure.
You can set limits on the resources that users and workloads can access
by managing <a href=/docs/concepts/policy/>policies</a> and
<a href=/docs/concepts/configuration/manage-resources-containers/>container resources</a>.</p></li></ul><p>Before building a Kubernetes production environment on your own, consider
handing off some or all of this job to
<a href=/docs/setup/production-environment/turnkey-solutions/>Turnkey Cloud Solutions</a>
providers or other <a href=/partners/>Kubernetes Partners</a>.
Options include:</p><ul><li><em>Serverless</em>: Just run workloads on third-party equipment without managing
a cluster at all. You will be charged for things like CPU usage, memory, and
disk requests.</li><li><em>Managed control plane</em>: Let the provider manage the scale and availability
of the cluster's control plane, as well as handle patches and upgrades.</li><li><em>Managed worker nodes</em>: Configure pools of nodes to meet your needs,
then the provider makes sure those nodes are available and ready to implement
upgrades when needed.</li><li><em>Integration</em>: There are providers that integrate Kubernetes with other
services you may need, such as storage, container registries, authentication
methods, and development tools.</li></ul><p>Whether you build a production Kubernetes cluster yourself or work with
partners, review the following sections to evaluate your needs as they relate
to your cluster’s <em>control plane</em>, <em>worker nodes</em>, <em>user access</em>, and
<em>workload resources</em>.</p><h2 id=production-cluster-setup>Production cluster setup</h2><p>In a production-quality Kubernetes cluster, the control plane manages the
cluster from services that can be spread across multiple computers
in different ways. Each worker node, however, represents a single entity that
is configured to run Kubernetes pods.</p><h3 id=production-control-plane>Production control plane</h3><p>The simplest Kubernetes cluster has the entire control plane and worker node
services running on the same machine. You can grow that environment by adding
worker nodes, as reflected in the diagram illustrated in
<a href=/docs/concepts/overview/components/>Kubernetes Components</a>.
If the cluster is meant to be available for a short period of time, or can be
discarded if something goes seriously wrong, this might meet your needs.</p><p>If you need a more permanent, highly available cluster, however, you should
consider ways of extending the control plane. By design, one-machine control
plane services running on a single machine are not highly available.
If keeping the cluster up and running
and ensuring that it can be repaired if something goes wrong is important,
consider these steps:</p><ul><li><em>Choose deployment tools</em>: You can deploy a control plane using tools such
as kubeadm, kops, and kubespray. See
<a href=/docs/setup/production-environment/tools/>Installing Kubernetes with deployment tools</a>
to learn tips for production-quality deployments using each of those deployment
methods. Different <a href=/docs/setup/production-environment/container-runtimes/>Container Runtimes</a>
are available to use with your deployments.</li><li><em>Manage certificates</em>: Secure communications between control plane services
are implemented using certificates. Certificates are automatically generated
during deployment or you can generate them using your own certificate authority.
See <a href=/docs/setup/best-practices/certificates/>PKI certificates and requirements</a> for details.</li><li><em>Configure load balancer for apiserver</em>: Configure a load balancer
to distribute external API requests to the apiserver service instances running on different nodes. See
<a href=/docs/tasks/access-application-cluster/create-external-load-balancer/>Create an External Load Balancer</a>
for details.</li><li><em>Separate and backup etcd service</em>: The etcd services can either run on the
same machines as other control plane services or run on separate machines, for
extra security and availability. Because etcd stores cluster configuration data,
backing up the etcd database should be done regularly to ensure that you can
repair that database if needed.
See the <a href=https://etcd.io/docs/v3.5/faq/>etcd FAQ</a> for details on configuring and using etcd.
See <a href=/docs/tasks/administer-cluster/configure-upgrade-etcd/>Operating etcd clusters for Kubernetes</a>
and <a href=/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>Set up a High Availability etcd cluster with kubeadm</a>
for details.</li><li><em>Create multiple control plane systems</em>: For high availability, the
control plane should not be limited to a single machine. If the control plane
services are run by an init service (such as systemd), each service should run on at
least three machines. However, running control plane services as pods in
Kubernetes ensures that the replicated number of services that you request
will always be available.
The scheduler should be fault tolerant,
but not highly available. Some deployment tools set up <a href=https://raft.github.io/>Raft</a>
consensus algorithm to do leader election of Kubernetes services. If the
primary goes away, another service elects itself and take over.</li><li><em>Span multiple zones</em>: If keeping your cluster available at all times is
critical, consider creating a cluster that runs across multiple data centers,
referred to as zones in cloud environments. Groups of zones are referred to as regions.
By spreading a cluster across
multiple zones in the same region, it can improve the chances that your
cluster will continue to function even if one zone becomes unavailable.
See <a href=/docs/setup/best-practices/multiple-zones/>Running in multiple zones</a> for details.</li><li><em>Manage on-going features</em>: If you plan to keep your cluster over time,
there are tasks you need to do to maintain its health and security. For example,
if you installed with kubeadm, there are instructions to help you with
<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/>Certificate Management</a>
and <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading kubeadm clusters</a>.
See <a href=/docs/tasks/administer-cluster/>Administer a Cluster</a>
for a longer list of Kubernetes administrative tasks.</li></ul><p>To learn about available options when you run control plane services, see
<a href=/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver</a>,
<a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>kube-controller-manager</a>,
and <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a>
component pages. For highly available control plane examples, see
<a href=/docs/setup/production-environment/tools/kubeadm/ha-topology/>Options for Highly Available topology</a>,
<a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>Creating Highly Available clusters with kubeadm</a>,
and <a href=/docs/tasks/administer-cluster/configure-upgrade-etcd/>Operating etcd clusters for Kubernetes</a>.
See <a href=/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster>Backing up an etcd cluster</a>
for information on making an etcd backup plan.</p><h3 id=production-worker-nodes>Production worker nodes</h3><p>Production-quality workloads need to be resilient and anything they rely
on needs to be resilient (such as CoreDNS). Whether you manage your own
control plane or have a cloud provider do it for you, you still need to
consider how you want to manage your worker nodes (also referred to
simply as <em>nodes</em>).</p><ul><li><em>Configure nodes</em>: Nodes can be physical or virtual machines. If you want to
create and manage your own nodes, you can install a supported operating system,
then add and run the appropriate
<a href=/docs/concepts/overview/components/#node-components>Node services</a>. Consider:<ul><li>The demands of your workloads when you set up nodes by having appropriate memory, CPU, and disk speed and storage capacity available.</li><li>Whether generic computer systems will do or you have workloads that need GPU processors, Windows nodes, or VM isolation.</li></ul></li><li><em>Validate nodes</em>: See <a href=/docs/setup/best-practices/node-conformance/>Valid node setup</a>
for information on how to ensure that a node meets the requirements to join
a Kubernetes cluster.</li><li><em>Add nodes to the cluster</em>: If you are managing your own cluster you can
add nodes by setting up your own machines and either adding them manually or
having them register themselves to the cluster’s apiserver. See the
<a href=/docs/concepts/architecture/nodes/>Nodes</a> section for information on how to set up Kubernetes to add nodes in these ways.</li><li><em>Scale nodes</em>: Have a plan for expanding the capacity your cluster will
eventually need. See <a href=/docs/setup/best-practices/cluster-large/>Considerations for large clusters</a>
to help determine how many nodes you need, based on the number of pods and
containers you need to run. If you are managing nodes yourself, this can mean
purchasing and installing your own physical equipment.</li><li><em>Autoscale nodes</em>: Most cloud providers support
<a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#readme>Cluster Autoscaler</a>
to replace unhealthy nodes or grow and shrink the number of nodes as demand requires. See the
<a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md>Frequently Asked Questions</a>
for how the autoscaler works and
<a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment>Deployment</a>
for how it is implemented by different cloud providers. For on-premises, there
are some virtualization platforms that can be scripted to spin up new nodes
based on demand.</li><li><em>Set up node health checks</em>: For important workloads, you want to make sure
that the nodes and pods running on those nodes are healthy. Using the
<a href=/docs/tasks/debug/debug-cluster/monitor-node-health/>Node Problem Detector</a>
daemon, you can ensure your nodes are healthy.</li></ul><h2 id=production-user-management>Production user management</h2><p>In production, you may be moving from a model where you or a small group of
people are accessing the cluster to where there may potentially be dozens or
hundreds of people. In a learning environment or platform prototype, you might have a single
administrative account for everything you do. In production, you will want
more accounts with different levels of access to different namespaces.</p><p>Taking on a production-quality cluster means deciding how you
want to selectively allow access by other users. In particular, you need to
select strategies for validating the identities of those who try to access your
cluster (authentication) and deciding if they have permissions to do what they
are asking (authorization):</p><ul><li><em>Authentication</em>: The apiserver can authenticate users using client
certificates, bearer tokens, an authenticating proxy, or HTTP basic auth.
You can choose which authentication methods you want to use.
Using plugins, the apiserver can leverage your organization’s existing
authentication methods, such as LDAP or Kerberos. See
<a href=/docs/reference/access-authn-authz/authentication/>Authentication</a>
for a description of these different methods of authenticating Kubernetes users.</li><li><em>Authorization</em>: When you set out to authorize your regular users, you will probably choose
between RBAC and ABAC authorization. See <a href=/docs/reference/access-authn-authz/authorization/>Authorization Overview</a>
to review different modes for authorizing user accounts (as well as service account access to
your cluster):<ul><li><em>Role-based access control</em> (<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>): Lets you
assign access to your cluster by allowing specific sets of permissions to authenticated users.
Permissions can be assigned for a specific namespace (Role) or across the entire cluster
(ClusterRole). Then using RoleBindings and ClusterRoleBindings, those permissions can be attached
to particular users.</li><li><em>Attribute-based access control</em> (<a href=/docs/reference/access-authn-authz/abac/>ABAC</a>): Lets you
create policies based on resource attributes in the cluster and will allow or deny access
based on those attributes. Each line of a policy file identifies versioning properties (apiVersion
and kind) and a map of spec properties to match the subject (user or group), resource property,
non-resource property (/version or /apis), and readonly. See
<a href=/docs/reference/access-authn-authz/abac/#examples>Examples</a> for details.</li></ul></li></ul><p>As someone setting up authentication and authorization on your production Kubernetes cluster, here are some things to consider:</p><ul><li><em>Set the authorization mode</em>: When the Kubernetes API server
(<a href=/docs/reference/command-line-tools-reference/kube-apiserver/>kube-apiserver</a>)
starts, the supported authentication modes must be set using the <em>--authorization-mode</em>
flag. For example, that flag in the <em>kube-adminserver.yaml</em> file (in <em>/etc/kubernetes/manifests</em>)
could be set to Node,RBAC. This would allow Node and RBAC authorization for authenticated requests.</li><li><em>Create user certificates and role bindings (RBAC)</em>: If you are using RBAC
authorization, users can create a CertificateSigningRequest (CSR) that can be
signed by the cluster CA. Then you can bind Roles and ClusterRoles to each user.
See <a href=/docs/reference/access-authn-authz/certificate-signing-requests/>Certificate Signing Requests</a>
for details.</li><li><em>Create policies that combine attributes (ABAC)</em>: If you are using ABAC
authorization, you can assign combinations of attributes to form policies to
authorize selected users or groups to access particular resources (such as a
pod), namespace, or apiGroup. For more information, see
<a href=/docs/reference/access-authn-authz/abac/#examples>Examples</a>.</li><li><em>Consider Admission Controllers</em>: Additional forms of authorization for
requests that can come in through the API server include
<a href=/docs/reference/access-authn-authz/authentication/#webhook-token-authentication>Webhook Token Authentication</a>.
Webhooks and other special authorization types need to be enabled by adding
<a href=/docs/reference/access-authn-authz/admission-controllers/>Admission Controllers</a>
to the API server.</li></ul><h2 id=set-limits-on-workload-resources>Set limits on workload resources</h2><p>Demands from production workloads can cause pressure both inside and outside
of the Kubernetes control plane. Consider these items when setting up for the
needs of your cluster's workloads:</p><ul><li><em>Set namespace limits</em>: Set per-namespace quotas on things like memory and CPU. See
<a href=/docs/tasks/administer-cluster/manage-resources/>Manage Memory, CPU, and API Resources</a>
for details. You can also set
<a href=/blog/2020/08/14/introducing-hierarchical-namespaces/>Hierarchical Namespaces</a>
for inheriting limits.</li><li><em>Prepare for DNS demand</em>: If you expect workloads to massively scale up,
your DNS service must be ready to scale up as well. See
<a href=/docs/tasks/administer-cluster/dns-horizontal-autoscaling/>Autoscale the DNS service in a Cluster</a>.</li><li><em>Create additional service accounts</em>: User accounts determine what users can
do on a cluster, while a service account defines pod access within a particular
namespace. By default, a pod takes on the default service account from its namespace.
See <a href=/docs/reference/access-authn-authz/service-accounts-admin/>Managing Service Accounts</a>
for information on creating a new service account. For example, you might want to:<ul><li>Add secrets that a pod could use to pull images from a particular container registry. See
<a href=/docs/tasks/configure-pod-container/configure-service-account/>Configure Service Accounts for Pods</a>
for an example.</li><li>Assign RBAC permissions to a service account. See
<a href=/docs/reference/access-authn-authz/rbac/#service-account-permissions>ServiceAccount permissions</a>
for details.</li></ul></li></ul><h2 id=what-s-next>What's next</h2><ul><li>Decide if you want to build your own production Kubernetes or obtain one from
available <a href=/docs/setup/production-environment/turnkey-solutions/>Turnkey Cloud Solutions</a>
or <a href=/partners/>Kubernetes Partners</a>.</li><li>If you choose to build your own cluster, plan how you want to
handle <a href=/docs/setup/best-practices/certificates/>certificates</a>
and set up high availability for features such as
<a href=/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>etcd</a>
and the
<a href=/docs/setup/production-environment/tools/kubeadm/ha-topology/>API server</a>.</li><li>Choose from <a href=/docs/setup/production-environment/tools/kubeadm/>kubeadm</a>,
<a href=/docs/setup/production-environment/tools/kops/>kops</a> or
<a href=/docs/setup/production-environment/tools/kubespray/>Kubespray</a>
deployment methods.</li><li>Configure user management by determining your
<a href=/docs/reference/access-authn-authz/authentication/>Authentication</a> and
<a href=/docs/reference/access-authn-authz/authorization/>Authorization</a> methods.</li><li>Prepare for application workloads by setting up
<a href=/docs/tasks/administer-cluster/manage-resources/>resource limits</a>,
<a href=/docs/tasks/administer-cluster/dns-horizontal-autoscaling/>DNS autoscaling</a>
and <a href=/docs/reference/access-authn-authz/service-accounts-admin/>service accounts</a>.</li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a77d3feb6e6d9978f32fa14622642e9a>1 - Container Runtimes</h1><div class="alert alert-secondary callout note" role=alert><strong>Note:</strong> Dockershim has been removed from the Kubernetes project as of release 1.24. Read the <a href=/dockershim>Dockershim Removal FAQ</a> for further details.</div><p>You need to install a
<a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>
into each node in the cluster so that Pods can run there. This page outlines
what is involved and describes related tasks for setting up nodes.</p><p>Kubernetes 1.25 requires that you use a runtime that
conforms with the
<a class=glossary-tooltip title='An API for container runtimes to integrate with kubelet' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#container-runtime target=_blank aria-label='Container Runtime Interface'>Container Runtime Interface</a> (CRI).</p><p>See <a href=#cri-versions>CRI version support</a> for more information.</p><p>This page provides an outline of how to use several common container runtimes with
Kubernetes.</p><ul><li><a href=#containerd>containerd</a></li><li><a href=#cri-o>CRI-O</a></li><li><a href=#docker>Docker Engine</a></li><li><a href=#mcr>Mirantis Container Runtime</a></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Kubernetes releases before v1.24 included a direct integration with Docker Engine,
using a component named <em>dockershim</em>. That special direct integration is no longer
part of Kubernetes (this removal was
<a href=/blog/2020/12/08/kubernetes-1-20-release-announcement/#dockershim-deprecation>announced</a>
as part of the v1.20 release).
You can read
<a href=/docs/tasks/administer-cluster/migrating-from-dockershim/check-if-dockershim-removal-affects-you/>Check whether Dockershim removal affects you</a>
to understand how this removal might affect you. To learn about migrating from using dockershim, see
<a href=/docs/tasks/administer-cluster/migrating-from-dockershim/>Migrating from dockershim</a>.</p><p>If you are running a version of Kubernetes other than v1.25,
check the documentation for that version.</p></div><h2 id=install-and-configure-prerequisites>Install and configure prerequisites</h2><p>The following steps apply common settings for Kubernetes nodes on Linux.</p><p>You can skip a particular setting if you're certain you don't need it.</p><p>For more information, see <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>Network Plugin Requirements</a> or the documentation for your specific container runtime.</p><h3 id=forwarding-ipv4-and-letting-iptables-see-bridged-traffic>Forwarding IPv4 and letting iptables see bridged traffic</h3><p>Verify that the <code>br_netfilter</code> module is loaded by running <code>lsmod | grep br_netfilter</code>.</p><p>To load it explicitly, run <code>sudo modprobe br_netfilter</code>.</p><p>In order for a Linux node's iptables to correctly view bridged traffic, verify that <code>net.bridge.bridge-nf-call-iptables</code> is set to 1 in your <code>sysctl</code> config. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>overlay
</span></span></span><span style=display:flex><span><span style=color:#b44>br_netfilter
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo modprobe overlay
</span></span><span style=display:flex><span>sudo modprobe br_netfilter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># sysctl params required by setup, params persist across reboots</span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-iptables  = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>net.ipv4.ip_forward                 = 1
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Apply sysctl params without reboot</span>
</span></span><span style=display:flex><span>sudo sysctl --system
</span></span></code></pre></div><h2 id=cgroup-drivers>Cgroup drivers</h2><p>On Linux, <a class=glossary-tooltip title='A group of Linux processes with optional resource isolation, accounting and limits.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label='control groups'>control groups</a>
are used to constrain resources that are allocated to processes.</p><p>Both <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> and the
underlying container runtime need to interface with control groups to enforce
<a href=/docs/concepts/configuration/manage-resources-containers/>resource management for pods and containers</a> and set
resources such as cpu/memory requests and limits. To interface with control
groups, the kubelet and the container runtime need to use a <em>cgroup driver</em>.
It's critical that the kubelet and the container runtime uses the same cgroup
driver and are configured the same.</p><p>There are two cgroup drivers available:</p><ul><li><a href=#cgroupfs-cgroup-driver><code>cgroupfs</code></a></li><li><a href=#systemd-cgroup-driver><code>systemd</code></a></li></ul><h3 id=cgroupfs-cgroup-driver>cgroupfs driver</h3><p>The <code>cgroupfs</code> driver is the default cgroup driver in the kubelet. When the <code>cgroupfs</code>
driver is used, the kubelet and the container runtime directly interface with
the cgroup filesystem to configure cgroups.</p><p>The <code>cgroupfs</code> driver is <strong>not</strong> recommended when
<a href=https://www.freedesktop.org/wiki/Software/systemd/>systemd</a> is the
init system because systemd expects a single cgroup manager on
the system. Additionally, if you use <a href=/docs/concepts/architecture/cgroups>cgroup v2</a>
, use the <code>systemd</code> cgroup driver instead of
<code>cgroupfs</code>.</p><h3 id=systemd-cgroup-driver>systemd cgroup driver</h3><p>When <a href=https://www.freedesktop.org/wiki/Software/systemd/>systemd</a> is chosen as the init
system for a Linux distribution, the init process generates and consumes a root control group
(<code>cgroup</code>) and acts as a cgroup manager.</p><p>systemd has a tight integration with cgroups and allocates a cgroup per systemd
unit. As a result, if you use <code>systemd</code> as the init system with the <code>cgroupfs</code>
driver, the system gets two different cgroup managers.</p><p>Two cgroup managers result in two views of the available and in-use resources in
the system. In some cases, nodes that are configured to use <code>cgroupfs</code> for the
kubelet and container runtime, but use <code>systemd</code> for the rest of the processes become
unstable under resource pressure.</p><p>The approach to mitigate this instability is to use <code>systemd</code> as the cgroup driver for
the kubelet and the container runtime when systemd is the selected init system.</p><p>To set <code>systemd</code> as the cgroup driver, edit the
<a href=/docs/tasks/administer-cluster/kubelet-config-file/><code>KubeletConfiguration</code></a>
option of <code>cgroupDriver</code> and set it to <code>systemd</code>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>cgroupDriver</span>:<span style=color:#bbb> </span>systemd<span style=color:#bbb>
</span></span></span></code></pre></div><p>If you configure <code>systemd</code> as the cgroup driver for the kubelet, you must also
configure <code>systemd</code> as the cgroup driver for the container runtime. Refer to
the documentation for your container runtime for instructions. For example:</p><ul><li><a href=#containerd-systemd>containerd</a></li><li><a href=#cri-o>CRI-O</a></li></ul><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>Changing the cgroup driver of a Node that has joined a cluster is a sensitive operation.
If the kubelet has created Pods using the semantics of one cgroup driver, changing the container
runtime to another cgroup driver can cause errors when trying to re-create the Pod sandbox
for such existing Pods. Restarting the kubelet may not solve such errors.</p><p>If you have automation that makes it feasible, replace the node with another using the updated
configuration, or reinstall it using automation.</p></div><h3 id=migrating-to-the-systemd-driver-in-kubeadm-managed-clusters>Migrating to the <code>systemd</code> driver in kubeadm managed clusters</h3><p>If you wish to migrate to the <code>systemd</code> cgroup driver in existing kubeadm managed clusters,
follow <a href=/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/>configuring a cgroup driver</a>.</p><h2 id=cri-versions>CRI version support</h2><p>Your container runtime must support at least v1alpha2 of the container runtime interface.</p><p>Kubernetes 1.25 defaults to using v1 of the CRI API.
If a container runtime does not support the v1 API, the kubelet falls back to
using the (deprecated) v1alpha2 API instead.</p><h2 id=container-runtimes>Container runtimes</h2><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><h3 id=containerd>containerd</h3><p>This section outlines the necessary steps to use containerd as CRI runtime.</p><p>Use the following commands to install Containerd on your system:</p><p>Follow the instructions for <a href=https://github.com/containerd/containerd/blob/main/docs/getting-started.md>getting started with containerd</a>. Return to this step once you've created a valid configuration file, <code>config.toml</code>.</p><ul class="nav nav-tabs" id=finding-your-config-toml-file role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#finding-your-config-toml-file-0 role=tab aria-controls=finding-your-config-toml-file-0 aria-selected=true>Linux</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#finding-your-config-toml-file-1 role=tab aria-controls=finding-your-config-toml-file-1>Windows</a></li></ul><div class=tab-content id=finding-your-config-toml-file><div id=finding-your-config-toml-file-0 class="tab-pane show active" role=tabpanel aria-labelledby=finding-your-config-toml-file-0><p><p>You can find this file under the path <code>/etc/containerd/config.toml</code>.</p></div><div id=finding-your-config-toml-file-1 class=tab-pane role=tabpanel aria-labelledby=finding-your-config-toml-file-1><p><p>You can find this file under the path <code>C:\Program Files\containerd\config.toml</code>.</p></div></div><p>On Linux the default CRI socket for containerd is <code>/run/containerd/containerd.sock</code>.
On Windows the default CRI endpoint is <code>npipe://./pipe/containerd-containerd</code>.</p><h4 id=containerd-systemd>Configuring the <code>systemd</code> cgroup driver</h4><p>To use the <code>systemd</code> cgroup driver in <code>/etc/containerd/config.toml</code> with <code>runc</code>, set</p><pre tabindex=0><code>[plugins.&#34;io.containerd.grpc.v1.cri&#34;.containerd.runtimes.runc]
  ...
  [plugins.&#34;io.containerd.grpc.v1.cri&#34;.containerd.runtimes.runc.options]
    SystemdCgroup = true
</code></pre><p>The <code>systemd</code> cgroup driver is recommended if you use <a href=/docs/concepts/architecture/cgroups>cgroup v2</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>If you installed containerd from a package (for example, RPM or <code>.deb</code>), you may find
that the CRI integration plugin is disabled by default.</p><p>You need CRI support enabled to use containerd with Kubernetes. Make sure that <code>cri</code>
is not included in the<code>disabled_plugins</code> list within <code>/etc/containerd/config.toml</code>;
if you made changes to that file, also restart <code>containerd</code>.</p></div><p>If you apply this change, make sure to restart containerd:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo systemctl restart containerd
</span></span></code></pre></div><p>When using kubeadm, manually configure the
<a href=/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/#configuring-the-kubelet-cgroup-driver>cgroup driver for kubelet</a>.</p><h4 id=override-pause-image-containerd>Overriding the sandbox (pause) image</h4><p>In your <a href=https://github.com/containerd/containerd/blob/main/docs/cri/config.md>containerd config</a> you can overwrite the
sandbox image by setting the following config:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[plugins.<span style=color:#b44>&#34;io.containerd.grpc.v1.cri&#34;</span>]
</span></span><span style=display:flex><span>  sandbox_image = <span style=color:#b44>&#34;registry.k8s.io/pause:3.2&#34;</span>
</span></span></code></pre></div><p>You might need to restart <code>containerd</code> as well once you've updated the config file: <code>systemctl restart containerd</code>.</p><h3 id=cri-o>CRI-O</h3><p>This section contains the necessary steps to install CRI-O as a container runtime.</p><p>To install CRI-O, follow <a href=https://github.com/cri-o/cri-o/blob/main/install.md#readme>CRI-O Install Instructions</a>.</p><h4 id=cgroup-driver>cgroup driver</h4><p>CRI-O uses the systemd cgroup driver per default, which is likely to work fine
for you. To switch to the <code>cgroupfs</code> cgroup driver, either edit
<code>/etc/crio/crio.conf</code> or place a drop-in configuration in
<code>/etc/crio/crio.conf.d/02-cgroup-manager.conf</code>, for example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[crio.runtime]
</span></span><span style=display:flex><span>conmon_cgroup = <span style=color:#b44>&#34;pod&#34;</span>
</span></span><span style=display:flex><span>cgroup_manager = <span style=color:#b44>&#34;cgroupfs&#34;</span>
</span></span></code></pre></div><p>You should also note the changed <code>conmon_cgroup</code>, which has to be set to the value
<code>pod</code> when using CRI-O with <code>cgroupfs</code>. It is generally necessary to keep the
cgroup driver configuration of the kubelet (usually done via kubeadm) and CRI-O
in sync.</p><p>For CRI-O, the CRI socket is <code>/var/run/crio/crio.sock</code> by default.</p><h4 id=override-pause-image-cri-o>Overriding the sandbox (pause) image</h4><p>In your <a href=https://github.com/cri-o/cri-o/blob/main/docs/crio.conf.5.md>CRI-O config</a> you can set the following
config value:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-toml data-lang=toml><span style=display:flex><span>[crio.image]
</span></span><span style=display:flex><span>pause_image=<span style=color:#b44>&#34;registry.k8s.io/pause:3.6&#34;</span>
</span></span></code></pre></div><p>This config option supports live configuration reload to apply this change: <code>systemctl reload crio</code> or by sending
<code>SIGHUP</code> to the <code>crio</code> process.</p><h3 id=docker>Docker Engine</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> These instructions assume that you are using the
<a href=https://github.com/Mirantis/cri-dockerd><code>cri-dockerd</code></a> adapter to integrate
Docker Engine with Kubernetes.</div><ol><li><p>On each of your nodes, install Docker for your Linux distribution as per
<a href=https://docs.docker.com/engine/install/#server>Install Docker Engine</a>.</p></li><li><p>Install <a href=https://github.com/Mirantis/cri-dockerd><code>cri-dockerd</code></a>, following
the instructions in that source code repository.</p></li></ol><p>For <code>cri-dockerd</code>, the CRI socket is <code>/run/cri-dockerd.sock</code> by default.</p><h3 id=mcr>Mirantis Container Runtime</h3><p><a href=https://docs.mirantis.com/mcr/20.10/overview.html>Mirantis Container Runtime</a> (MCR) is a commercially
available container runtime that was formerly known as Docker Enterprise Edition.</p><p>You can use Mirantis Container Runtime with Kubernetes using the open source
<a href=https://github.com/Mirantis/cri-dockerd><code>cri-dockerd</code></a> component, included with MCR.</p><p>To learn more about how to install Mirantis Container Runtime,
visit <a href=https://docs.mirantis.com/mcr/20.10/install.html>MCR Deployment Guide</a>.</p><p>Check the systemd unit named <code>cri-docker.socket</code> to find out the path to the CRI
socket.</p><h4 id=override-pause-image-cri-dockerd-mcr>Overriding the sandbox (pause) image</h4><p>The <code>cri-dockerd</code> adapter accepts a command line argument for
specifying which container image to use as the Pod infrastructure container (“pause image”).
The command line argument to use is <code>--pod-infra-container-image</code>.</p><h2 id=what-s-next>What's next</h2><p>As well as a container runtime, your cluster will need a working
<a href=/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>network plugin</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-00e1646f68aeb89f9722cf6f6cfcad94>2 - Installing Kubernetes with deployment tools</h1></div><div class=td-content><h1 id=pg-a16f59f325a17cdeed324d5c889f7f73>2.1 - Bootstrapping clusters with kubeadm</h1></div><div class=td-content><h1 id=pg-29e59491dd6118b23072dfe9ebb93323>2.1.1 - Installing kubeadm</h1><p><img src=/images/kubeadm-stacked-color.png align=right width=150px></img>
This page shows how to install the <code>kubeadm</code> toolbox.
For information on how to create a cluster with kubeadm once you have performed this installation process, see the <a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>Creating a cluster with kubeadm</a> page.</p><h2 id=before-you-begin>Before you begin</h2><ul><li>A compatible Linux host. The Kubernetes project provides generic instructions for Linux distributions based on Debian and Red Hat, and those distributions without a package manager.</li><li>2 GB or more of RAM per machine (any less will leave little room for your apps).</li><li>2 CPUs or more.</li><li>Full network connectivity between all machines in the cluster (public or private network is fine).</li><li>Unique hostname, MAC address, and product_uuid for every node. See <a href=#verify-mac-address>here</a> for more details.</li><li>Certain ports are open on your machines. See <a href=#check-required-ports>here</a> for more details.</li><li>Swap disabled. You <strong>MUST</strong> disable swap in order for the kubelet to work properly.</li></ul><h2 id=verify-mac-address>Verify the MAC address and product_uuid are unique for every node</h2><ul><li>You can get the MAC address of the network interfaces using the command <code>ip link</code> or <code>ifconfig -a</code></li><li>The product_uuid can be checked by using the command <code>sudo cat /sys/class/dmi/id/product_uuid</code></li></ul><p>It is very likely that hardware devices will have unique addresses, although some virtual machines may have
identical values. Kubernetes uses these values to uniquely identify the nodes in the cluster.
If these values are not unique to each node, the installation process
may <a href=https://github.com/kubernetes/kubeadm/issues/31>fail</a>.</p><h2 id=check-network-adapters>Check network adapters</h2><p>If you have more than one network adapter, and your Kubernetes components are not reachable on the default
route, we recommend you add IP route(s) so Kubernetes cluster addresses go via the appropriate adapter.</p><h2 id=check-required-ports>Check required ports</h2><p>These
<a href=/docs/reference/ports-and-protocols/>required ports</a>
need to be open in order for Kubernetes components to communicate with each other. You can use tools like netcat to check if a port is open. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>nc 127.0.0.1 <span style=color:#666>6443</span>
</span></span></code></pre></div><p>The pod network plugin you use may also require certain ports to be
open. Since this differs with each pod network plugin, please see the
documentation for the plugins about what port(s) those need.</p><h2 id=installing-runtime>Installing a container runtime</h2><p>To run containers in Pods, Kubernetes uses a
<a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>.</p><p>By default, Kubernetes uses the
<a class=glossary-tooltip title='An API for container runtimes to integrate with kubelet' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#container-runtime target=_blank aria-label='Container Runtime Interface'>Container Runtime Interface</a> (CRI)
to interface with your chosen container runtime.</p><p>If you don't specify a runtime, kubeadm automatically tries to detect an installed
container runtime by scanning through a list of known endpoints.</p><p>If multiple or no container runtimes are detected kubeadm will throw an error
and will request that you specify which one you want to use.</p><p>See <a href=/docs/setup/production-environment/container-runtimes/>container runtimes</a>
for more information.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Docker Engine does not implement the <a href=/docs/concepts/architecture/cri/>CRI</a>
which is a requirement for a container runtime to work with Kubernetes.
For that reason, an additional service <a href=https://github.com/Mirantis/cri-dockerd>cri-dockerd</a>
has to be installed. cri-dockerd is a project based on the legacy built-in
Docker Engine support that was <a href=/dockershim>removed</a> from the kubelet in version 1.24.</div><p>The tables below include the known endpoints for supported operating systems:</p><ul class="nav nav-tabs" id=container-runtime role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#container-runtime-0 role=tab aria-controls=container-runtime-0 aria-selected=true>Linux</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#container-runtime-1 role=tab aria-controls=container-runtime-1>Windows</a></li></ul><div class=tab-content id=container-runtime><div id=container-runtime-0 class="tab-pane show active" role=tabpanel aria-labelledby=container-runtime-0><p><table><caption style=display:none>Linux container runtimes</caption><thead><tr><th>Runtime</th><th>Path to Unix domain socket</th></tr></thead><tbody><tr><td>containerd</td><td><code>unix:///var/run/containerd/containerd.sock</code></td></tr><tr><td>CRI-O</td><td><code>unix:///var/run/crio/crio.sock</code></td></tr><tr><td>Docker Engine (using cri-dockerd)</td><td><code>unix:///var/run/cri-dockerd.sock</code></td></tr></tbody></table></div><div id=container-runtime-1 class=tab-pane role=tabpanel aria-labelledby=container-runtime-1><p><table><caption style=display:none>Windows container runtimes</caption><thead><tr><th>Runtime</th><th>Path to Windows named pipe</th></tr></thead><tbody><tr><td>containerd</td><td><code>npipe:////./pipe/containerd-containerd</code></td></tr><tr><td>Docker Engine (using cri-dockerd)</td><td><code>npipe:////./pipe/cri-dockerd</code></td></tr></tbody></table></div></div><h2 id=installing-kubeadm-kubelet-and-kubectl>Installing kubeadm, kubelet and kubectl</h2><p>You will install these packages on all of your machines:</p><ul><li><p><code>kubeadm</code>: the command to bootstrap the cluster.</p></li><li><p><code>kubelet</code>: the component that runs on all of the machines in your cluster
and does things like starting pods and containers.</p></li><li><p><code>kubectl</code>: the command line util to talk to your cluster.</p></li></ul><p>kubeadm <strong>will not</strong> install or manage <code>kubelet</code> or <code>kubectl</code> for you, so you will
need to ensure they match the version of the Kubernetes control plane you want
kubeadm to install for you. If you do not, there is a risk of a version skew occurring that
can lead to unexpected, buggy behaviour. However, <em>one</em> minor version skew between the
kubelet and the control plane is supported, but the kubelet version may never exceed the API
server version. For example, the kubelet running 1.7.0 should be fully compatible with a 1.8.0 API server,
but not vice versa.</p><p>For information about installing <code>kubectl</code>, see <a href=/docs/tasks/tools/>Install and set up kubectl</a>.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> These instructions exclude all Kubernetes packages from any system upgrades.
This is because kubeadm and Kubernetes require
<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>special attention to upgrade</a>.</div><p>For more information on version skews, see:</p><ul><li>Kubernetes <a href=/docs/setup/release/version-skew-policy/>version and version-skew policy</a></li><li>Kubeadm-specific <a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy>version skew policy</a></li></ul><ul class="nav nav-tabs" id=k8s-install role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#k8s-install-0 role=tab aria-controls=k8s-install-0 aria-selected=true>Debian-based distributions</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-1 role=tab aria-controls=k8s-install-1>Red Hat-based distributions</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#k8s-install-2 role=tab aria-controls=k8s-install-2>Without a package manager</a></li></ul><div class=tab-content id=k8s-install><div id=k8s-install-0 class="tab-pane show active" role=tabpanel aria-labelledby=k8s-install-0><p><ol><li><p>Update the <code>apt</code> package index and install packages needed to use the Kubernetes <code>apt</code> repository:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install -y apt-transport-https ca-certificates curl
</span></span></code></pre></div></li><li><p>Download the Google Cloud public signing key:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo curl -fsSLo /etc/apt/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg
</span></span></code></pre></div></li><li><p>Add the Kubernetes <code>apt</code> repository:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;deb [signed-by=/etc/apt/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main&#34;</span> | sudo tee /etc/apt/sources.list.d/kubernetes.list
</span></span></code></pre></div></li><li><p>Update <code>apt</code> package index, install kubelet, kubeadm and kubectl, and pin their version:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo apt-get update
</span></span><span style=display:flex><span>sudo apt-get install -y kubelet kubeadm kubectl
</span></span><span style=display:flex><span>sudo apt-mark hold kubelet kubeadm kubectl
</span></span></code></pre></div></li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In releases older than Debian 12 and Ubuntu 22.04, <code>/etc/apt/keyrings</code> does not exist by default.
You can create this directory if you need to, making it world-readable but writeable only by admins.</div></div><div id=k8s-install-1 class=tab-pane role=tabpanel aria-labelledby=k8s-install-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span></span></span><span style=display:flex><span><span style=color:#b44>[kubernetes]
</span></span></span><span style=display:flex><span><span style=color:#b44>name=Kubernetes
</span></span></span><span style=display:flex><span><span style=color:#b44>baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch
</span></span></span><span style=display:flex><span><span style=color:#b44>enabled=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgcheck=1
</span></span></span><span style=display:flex><span><span style=color:#b44>gpgkey=https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#b44>exclude=kubelet kubeadm kubectl
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Set SELinux in permissive mode (effectively disabling it)</span>
</span></span><span style=display:flex><span>sudo setenforce <span style=color:#666>0</span>
</span></span><span style=display:flex><span>sudo sed -i <span style=color:#b44>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo yum install -y kubelet kubeadm kubectl --disableexcludes<span style=color:#666>=</span>kubernetes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sudo systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div><p><strong>Notes:</strong></p><ul><li><p>Setting SELinux in permissive mode by running <code>setenforce 0</code> and <code>sed ...</code> effectively disables it.
This is required to allow containers to access the host filesystem, which is needed by pod networks for example.
You have to do this until SELinux support is improved in the kubelet.</p></li><li><p>You can leave SELinux enabled if you know how to configure it but it may require settings that are not supported by kubeadm.</p></li><li><p>If the <code>baseurl</code> fails because your Red Hat-based distribution cannot interpret <code>basearch</code>, replace <code>\$basearch</code> with your computer's architecture.
Type <code>uname -m</code> to see that value.
For example, the <code>baseurl</code> URL for <code>x86_64</code> could be: <code>https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64</code>.</p></li></ul></div><div id=k8s-install-2 class=tab-pane role=tabpanel aria-labelledby=k8s-install-2><p><p>Install CNI plugins (required for most pod network):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CNI_PLUGINS_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.1.1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>DEST</span><span style=color:#666>=</span><span style=color:#b44>&#34;/opt/cni/bin&#34;</span>
</span></span><span style=display:flex><span>sudo mkdir -p <span style=color:#b44>&#34;</span><span style=color:#b8860b>$DEST</span><span style=color:#b44>&#34;</span>
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/containernetworking/plugins/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_PLUGINS_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cni-plugins-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CNI_PLUGINS_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tgz&#34;</span> | sudo tar -C <span style=color:#b44>&#34;</span><span style=color:#b8860b>$DEST</span><span style=color:#b44>&#34;</span> -xz
</span></span></code></pre></div><p>Define the directory to download command files</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>DOWNLOAD_DIR</code> variable must be set to a writable directory.
If you are running Flatcar Container Linux, set <code>DOWNLOAD_DIR="/opt/bin"</code>.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#666>=</span><span style=color:#b44>&#34;/usr/local/bin&#34;</span>
</span></span><span style=display:flex><span>sudo mkdir -p <span style=color:#b44>&#34;</span><span style=color:#b8860b>$DOWNLOAD_DIR</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div><p>Install crictl (required for kubeadm / Kubelet Container Runtime Interface (CRI))</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v1.25.0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span>curl -L <span style=color:#b44>&#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/crictl-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CRICTL_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>-linux-</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>.tar.gz&#34;</span> | sudo tar -C <span style=color:#b8860b>$DOWNLOAD_DIR</span> -xz
</span></span></code></pre></div><p>Install <code>kubeadm</code>, <code>kubelet</code>, <code>kubectl</code> and add a <code>kubelet</code> systemd service:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>RELEASE</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>curl -sSL https://dl.k8s.io/release/stable.txt<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>ARCH</span><span style=color:#666>=</span><span style=color:#b44>&#34;amd64&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f>cd</span> <span style=color:#b8860b>$DOWNLOAD_DIR</span>
</span></span><span style=display:flex><span>sudo curl -L --remote-name-all https://dl.k8s.io/release/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE</span><span style=color:#b68;font-weight:700>}</span>/bin/linux/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ARCH</span><span style=color:#b68;font-weight:700>}</span>/<span style=color:#666>{</span>kubeadm,kubelet<span style=color:#666>}</span>
</span></span><span style=display:flex><span>sudo chmod +x <span style=color:#666>{</span>kubeadm,kubelet<span style=color:#666>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#666>=</span><span style=color:#b44>&#34;v0.4.0&#34;</span>
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service
</span></span><span style=display:flex><span>sudo mkdir -p /etc/systemd/system/kubelet.service.d
</span></span><span style=display:flex><span>curl -sSL <span style=color:#b44>&#34;https://raw.githubusercontent.com/kubernetes/release/</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>RELEASE_VERSION</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf&#34;</span> | sed <span style=color:#b44>&#34;s:/usr/bin:</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>DOWNLOAD_DIR</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>:g&#34;</span> | sudo tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</span></span></code></pre></div><p>Install <code>kubectl</code> by following the instructions on <a href=/docs/tasks/tools/#kubectl>Install Tools page</a>.</p><p>Enable and start <code>kubelet</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl <span style=color:#a2f>enable</span> --now kubelet
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The Flatcar Container Linux distribution mounts the <code>/usr</code> directory as a read-only filesystem.
Before bootstrapping your cluster, you need to take additional steps to configure a writable directory.
See the <a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/#usr-mounted-read-only/>Kubeadm Troubleshooting guide</a> to learn how to set up a writable directory.</div></div></div><p>The kubelet is now restarting every few seconds, as it waits in a crashloop for
kubeadm to tell it what to do.</p><h2 id=configuring-a-cgroup-driver>Configuring a cgroup driver</h2><p>Both the container runtime and the kubelet have a property called
<a href=/docs/setup/production-environment/container-runtimes/>"cgroup driver"</a>, which is important
for the management of cgroups on Linux machines.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>Matching the container runtime and kubelet cgroup drivers is required or otherwise the kubelet process will fail.</p><p>See <a href=/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/>Configuring a cgroup driver</a> for more details.</p></div><h2 id=troubleshooting>Troubleshooting</h2><p>If you are running into difficulties with kubeadm, please consult our <a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>troubleshooting docs</a>.</p><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/>Using kubeadm to Create a Cluster</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c3689df4b0c61a998e79d91a865aa244>2.1.2 - Troubleshooting kubeadm</h1><p>As with any program, you might run into an error installing or running kubeadm.
This page lists some common failure scenarios and have provided steps that can help you understand and fix the problem.</p><p>If your problem is not listed below, please follow the following steps:</p><ul><li><p>If you think your problem is a bug with kubeadm:</p><ul><li>Go to <a href=https://github.com/kubernetes/kubeadm/issues>github.com/kubernetes/kubeadm</a> and search for existing issues.</li><li>If no issue exists, please <a href=https://github.com/kubernetes/kubeadm/issues/new>open one</a> and follow the issue template.</li></ul></li><li><p>If you are unsure about how kubeadm works, you can ask on <a href=https://slack.k8s.io/>Slack</a> in <code>#kubeadm</code>,
or open a question on <a href=https://stackoverflow.com/questions/tagged/kubernetes>StackOverflow</a>. Please include
relevant tags like <code>#kubernetes</code> and <code>#kubeadm</code> so folks can help you.</p></li></ul><h2 id=not-possible-to-join-a-v1-18-node-to-a-v1-17-cluster-due-to-missing-rbac>Not possible to join a v1.18 Node to a v1.17 cluster due to missing RBAC</h2><p>In v1.18 kubeadm added prevention for joining a Node in the cluster if a Node with the same name already exists.
This required adding RBAC for the bootstrap-token user to be able to GET a Node object.</p><p>However this causes an issue where <code>kubeadm join</code> from v1.18 cannot join a cluster created by kubeadm v1.17.</p><p>To workaround the issue you have two options:</p><p>Execute <code>kubeadm init phase bootstrap-token</code> on a control-plane node using kubeadm v1.18.
Note that this enables the rest of the bootstrap-token permissions as well.</p><p>or</p><p>Apply the following RBAC manually using <code>kubectl apply -f ...</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRoleBinding<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>roleRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kubeadm:get-nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Group<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:bootstrappers:kubeadm:default-node-token<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=ebtables-or-some-similar-executable-not-found-during-installation><code>ebtables</code> or some similar executable not found during installation</h2><p>If you see the following warnings while running <code>kubeadm init</code></p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>[preflight] WARNING: ebtables not found in system path
</span></span></span><span style=display:flex><span><span style=color:#888>[preflight] WARNING: ethtool not found in system path
</span></span></span></code></pre></div><p>Then you may be missing <code>ebtables</code>, <code>ethtool</code> or a similar executable on your node. You can install them with the following commands:</p><ul><li>For Ubuntu/Debian users, run <code>apt install ebtables ethtool</code>.</li><li>For CentOS/Fedora users, run <code>yum install ebtables ethtool</code>.</li></ul><h2 id=kubeadm-blocks-waiting-for-control-plane-during-installation>kubeadm blocks waiting for control plane during installation</h2><p>If you notice that <code>kubeadm init</code> hangs after printing out the following line:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>[apiclient] Created API client, waiting for the control plane to become ready
</span></span></span></code></pre></div><p>This may be caused by a number of problems. The most common are:</p><ul><li>network connection problems. Check that your machine has full network connectivity before continuing.</li><li>the cgroup driver of the container runtime differs from that of the kubelet. To understand how to
configure it properly see <a href=/docs/tasks/administer-cluster/kubeadm/configure-cgroup-driver/>Configuring a cgroup driver</a>.</li><li>control plane containers are crashlooping or hanging. You can check this by running <code>docker ps</code>
and investigating each container by running <code>docker logs</code>. For other container runtime see
<a href=/docs/tasks/debug/debug-cluster/crictl/>Debugging Kubernetes nodes with crictl</a>.</li></ul><h2 id=kubeadm-blocks-when-removing-managed-containers>kubeadm blocks when removing managed containers</h2><p>The following could happen if the container runtime halts and does not remove
any Kubernetes-managed containers:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo kubeadm reset
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>[preflight] Running pre-flight checks
</span></span></span><span style=display:flex><span><span style=color:#888>[reset] Stopping the kubelet service
</span></span></span><span style=display:flex><span><span style=color:#888>[reset] Unmounting mounted directories in &#34;/var/lib/kubelet&#34;
</span></span></span><span style=display:flex><span><span style=color:#888>[reset] Removing kubernetes-managed containers
</span></span></span><span style=display:flex><span><span style=color:#888>(block)
</span></span></span></code></pre></div><p>A possible solution is to restart the container runtime and then re-run <code>kubeadm reset</code>.
You can also use <code>crictl</code> to debug the state of the container runtime. See
<a href=/docs/tasks/debug/debug-cluster/crictl/>Debugging Kubernetes nodes with crictl</a>.</p><h2 id=pods-in-runcontainererror-crashloopbackoff-or-error-state>Pods in <code>RunContainerError</code>, <code>CrashLoopBackOff</code> or <code>Error</code> state</h2><p>Right after <code>kubeadm init</code> there should not be any pods in these states.</p><ul><li>If there are pods in one of these states <em>right after</em> <code>kubeadm init</code>, please open an
issue in the kubeadm repo. <code>coredns</code> (or <code>kube-dns</code>) should be in the <code>Pending</code> state
until you have deployed the network add-on.</li><li>If you see Pods in the <code>RunContainerError</code>, <code>CrashLoopBackOff</code> or <code>Error</code> state
after deploying the network add-on and nothing happens to <code>coredns</code> (or <code>kube-dns</code>),
it's very likely that the Pod Network add-on that you installed is somehow broken.
You might have to grant it more RBAC privileges or use a newer version. Please file
an issue in the Pod Network providers' issue tracker and get the issue triaged there.</li></ul><h2 id=coredns-is-stuck-in-the-pending-state><code>coredns</code> is stuck in the <code>Pending</code> state</h2><p>This is <strong>expected</strong> and part of the design. kubeadm is network provider-agnostic, so the admin
should <a href=/docs/concepts/cluster-administration/addons/>install the pod network add-on</a>
of choice. You have to install a Pod Network
before CoreDNS may be deployed fully. Hence the <code>Pending</code> state before the network is set up.</p><h2 id=hostport-services-do-not-work><code>HostPort</code> services do not work</h2><p>The <code>HostPort</code> and <code>HostIP</code> functionality is available depending on your Pod Network
provider. Please contact the author of the Pod Network add-on to find out whether
<code>HostPort</code> and <code>HostIP</code> functionality are available.</p><p>Calico, Canal, and Flannel CNI providers are verified to support HostPort.</p><p>For more information, see the <a href=https://github.com/containernetworking/plugins/blob/master/plugins/meta/portmap/README.md>CNI portmap documentation</a>.</p><p>If your network provider does not support the portmap CNI plugin, you may need to use the <a href=/docs/concepts/services-networking/service/#type-nodeport>NodePort feature of
services</a> or use <code>HostNetwork=true</code>.</p><h2 id=pods-are-not-accessible-via-their-service-ip>Pods are not accessible via their Service IP</h2><ul><li><p>Many network add-ons do not yet enable <a href=/docs/tasks/debug/debug-application/debug-service/#a-pod-fails-to-reach-itself-via-the-service-ip>hairpin mode</a>
which allows pods to access themselves via their Service IP. This is an issue related to
<a href=https://github.com/containernetworking/cni/issues/476>CNI</a>. Please contact the network
add-on provider to get the latest status of their support for hairpin mode.</p></li><li><p>If you are using VirtualBox (directly or via Vagrant), you will need to
ensure that <code>hostname -i</code> returns a routable IP address. By default the first
interface is connected to a non-routable host-only network. A work around
is to modify <code>/etc/hosts</code>, see this <a href=https://github.com/errordeveloper/k8s-playground/blob/22dd39dfc06111235620e6c4404a96ae146f26fd/Vagrantfile#L11>Vagrantfile</a>
for an example.</p></li></ul><h2 id=tls-certificate-errors>TLS certificate errors</h2><p>The following error indicates a possible certificate mismatch.</p><pre tabindex=0><code class=language-none data-lang=none># kubectl get pods
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &#34;crypto/rsa: verification error&#34; while trying to verify candidate authority certificate &#34;kubernetes&#34;)
</code></pre><ul><li><p>Verify that the <code>$HOME/.kube/config</code> file contains a valid certificate, and
regenerate a certificate if necessary. The certificates in a kubeconfig file
are base64 encoded. The <code>base64 --decode</code> command can be used to decode the certificate
and <code>openssl x509 -text -noout</code> can be used for viewing the certificate information.</p></li><li><p>Unset the <code>KUBECONFIG</code> environment variable using:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>unset</span> KUBECONFIG
</span></span></code></pre></div><p>Or set it to the default <code>KUBECONFIG</code> location:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div></li><li><p>Another workaround is to overwrite the existing <code>kubeconfig</code> for the "admin" user:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>mv  <span style=color:#b8860b>$HOME</span>/.kube <span style=color:#b8860b>$HOME</span>/.kube.bak
</span></span><span style=display:flex><span>mkdir <span style=color:#b8860b>$HOME</span>/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div></li></ul><h2 id=kubelet-client-cert>Kubelet client certificate rotation fails</h2><p>By default, kubeadm configures a kubelet with automatic rotation of client certificates by using the <code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> symlink specified in <code>/etc/kubernetes/kubelet.conf</code>.
If this rotation process fails you might see errors such as <code>x509: certificate has expired or is not yet valid</code>
in kube-apiserver logs. To fix the issue you must follow these steps:</p><ol><li><p>Backup and delete <code>/etc/kubernetes/kubelet.conf</code> and <code>/var/lib/kubelet/pki/kubelet-client*</code> from the failed node.</p></li><li><p>From a working control plane node in the cluster that has <code>/etc/kubernetes/pki/ca.key</code> execute
<code>kubeadm kubeconfig user --org system:nodes --client-name system:node:$NODE > kubelet.conf</code>.
<code>$NODE</code> must be set to the name of the existing failed node in the cluster.
Modify the resulted <code>kubelet.conf</code> manually to adjust the cluster name and server endpoint,
or pass <code>kubeconfig user --config</code> (it accepts <code>InitConfiguration</code>). If your cluster does not have
the <code>ca.key</code> you must sign the embedded certificates in the <code>kubelet.conf</code> externally.</p></li><li><p>Copy this resulted <code>kubelet.conf</code> to <code>/etc/kubernetes/kubelet.conf</code> on the failed node.</p></li><li><p>Restart the kubelet (<code>systemctl restart kubelet</code>) on the failed node and wait for
<code>/var/lib/kubelet/pki/kubelet-client-current.pem</code> to be recreated.</p></li><li><p>Manually edit the <code>kubelet.conf</code> to point to the rotated kubelet client certificates, by replacing
<code>client-certificate-data</code> and <code>client-key-data</code> with:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>client-certificate</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>client-key</span>:<span style=color:#bbb> </span>/var/lib/kubelet/pki/kubelet-client-current.pem<span style=color:#bbb>
</span></span></span></code></pre></div></li><li><p>Restart the kubelet.</p></li><li><p>Make sure the node becomes <code>Ready</code>.</p></li></ol><h2 id=default-nic-when-using-flannel-as-the-pod-network-in-vagrant>Default NIC When using flannel as the pod network in Vagrant</h2><p>The following error might indicate that something was wrong in the pod network:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>Error from server <span style=color:#666>(</span>NotFound<span style=color:#666>)</span>: the server could not find the requested resource
</span></span></code></pre></div><ul><li><p>If you're using flannel as the pod network inside Vagrant, then you will have to specify the default interface name for flannel.</p><p>Vagrant typically assigns two interfaces to all VMs. The first, for which all hosts are assigned the IP address <code>10.0.2.15</code>, is for external traffic that gets NATed.</p><p>This may lead to problems with flannel, which defaults to the first interface on a host. This leads to all hosts thinking they have the same public IP address. To prevent this, pass the <code>--iface eth1</code> flag to flannel so that the second interface is chosen.</p></li></ul><h2 id=non-public-ip-used-for-containers>Non-public IP used for containers</h2><p>In some situations <code>kubectl logs</code> and <code>kubectl run</code> commands may return with the following errors in an otherwise functional cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>Error from server: Get https://10.19.0.41:10250/containerLogs/default/mysql-ddc65b868-glc5m/mysql: dial tcp 10.19.0.41:10250: getsockopt: no route to host
</span></span></span></code></pre></div><ul><li><p>This may be due to Kubernetes using an IP that can not communicate with other IPs on the seemingly same subnet, possibly by policy of the machine provider.</p></li><li><p>DigitalOcean assigns a public IP to <code>eth0</code> as well as a private one to be used internally as anchor for their floating IP feature, yet <code>kubelet</code> will pick the latter as the node's <code>InternalIP</code> instead of the public one.</p><p>Use <code>ip addr show</code> to check for this scenario instead of <code>ifconfig</code> because <code>ifconfig</code> will not display the offending alias IP address. Alternatively an API endpoint specific to DigitalOcean allows to query for the anchor IP from the droplet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>curl http://169.254.169.254/metadata/v1/interfaces/public/0/anchor_ipv4/address
</span></span></code></pre></div><p>The workaround is to tell <code>kubelet</code> which IP to use using <code>--node-ip</code>.
When using DigitalOcean, it can be the public one (assigned to <code>eth0</code>) or
the private one (assigned to <code>eth1</code>) should you want to use the optional
private network. The <code>kubeletExtraArgs</code> section of the kubeadm
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/#kubeadm-k8s-io-v1beta3-NodeRegistrationOptions><code>NodeRegistrationOptions</code> structure</a>
can be used for this.</p><p>Then restart <code>kubelet</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div></li></ul><h2 id=coredns-pods-have-crashloopbackoff-or-error-state><code>coredns</code> pods have <code>CrashLoopBackOff</code> or <code>Error</code> state</h2><p>If you have nodes that are running SELinux with an older version of Docker you might experience a scenario
where the <code>coredns</code> pods are not starting. To solve that you can try one of the following options:</p><ul><li><p>Upgrade to a <a href=/docs/setup/production-environment/container-runtimes/#docker>newer version of Docker</a>.</p></li><li><p><a href=https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/security-enhanced_linux/sect-security-enhanced_linux-enabling_and_disabling_selinux-disabling_selinux>Disable SELinux</a>.</p></li><li><p>Modify the <code>coredns</code> deployment to set <code>allowPrivilegeEscalation</code> to <code>true</code>:</p></li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl -n kube-system get deployment coredns -o yaml | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  sed <span style=color:#b44>&#39;s/allowPrivilegeEscalation: false/allowPrivilegeEscalation: true/g&#39;</span> | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  kubectl apply -f -
</span></span></code></pre></div><p>Another cause for CoreDNS to have <code>CrashLoopBackOff</code> is when a CoreDNS Pod deployed in Kubernetes detects a loop. <a href=https://github.com/coredns/coredns/tree/master/plugin/loop#troubleshooting-loops-in-kubernetes-clusters>A number of workarounds</a>
are available to avoid Kubernetes trying to restart the CoreDNS Pod every time CoreDNS detects the loop and exits.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> Disabling SELinux or setting <code>allowPrivilegeEscalation</code> to <code>true</code> can compromise
the security of your cluster.</div><h2 id=etcd-pods-restart-continually>etcd pods restart continually</h2><p>If you encounter the following error:</p><pre tabindex=0><code>rpc error: code = 2 desc = oci runtime error: exec failed: container_linux.go:247: starting container process caused &#34;process_linux.go:110: decoding init error from pipe caused \&#34;read parent: connection reset by peer\&#34;&#34;
</code></pre><p>this issue appears if you run CentOS 7 with Docker 1.13.1.84.
This version of Docker can prevent the kubelet from executing into the etcd container.</p><p>To work around the issue, choose one of these options:</p><ul><li><p>Roll back to an earlier version of Docker, such as 1.13.1-75</p><pre tabindex=0><code>yum downgrade docker-1.13.1-75.git8633870.el7.centos.x86_64 docker-client-1.13.1-75.git8633870.el7.centos.x86_64 docker-common-1.13.1-75.git8633870.el7.centos.x86_64
</code></pre></li><li><p>Install one of the more recent recommended versions, such as 18.06:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span>yum install docker-ce-18.06.1.ce-3.el7.x86_64
</span></span></code></pre></div></li></ul><h2 id=not-possible-to-pass-a-comma-separated-list-of-values-to-arguments-inside-a-component-extra-args-flag>Not possible to pass a comma separated list of values to arguments inside a <code>--component-extra-args</code> flag</h2><p><code>kubeadm init</code> flags such as <code>--component-extra-args</code> allow you to pass custom arguments to a control-plane
component like the kube-apiserver. However, this mechanism is limited due to the underlying type used for parsing
the values (<code>mapStringString</code>).</p><p>If you decide to pass an argument that supports multiple, comma-separated values such as
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,NamespaceExists"</code> this flag will fail with
<code>flag: malformed pair, expect string=string</code>. This happens because the list of arguments for
<code>--apiserver-extra-args</code> expects <code>key=value</code> pairs and in this case <code>NamespacesExists</code> is considered
as a key that is missing a value.</p><p>Alternatively, you can try separating the <code>key=value</code> pairs like so:
<code>--apiserver-extra-args "enable-admission-plugins=LimitRanger,enable-admission-plugins=NamespaceExists"</code>
but this will result in the key <code>enable-admission-plugins</code> only having the value of <code>NamespaceExists</code>.</p><p>A known workaround is to use the kubeadm <a href=/docs/reference/config-api/kubeadm-config.v1beta3/>configuration file</a>.</p><h2 id=kube-proxy-scheduled-before-node-is-initialized-by-cloud-controller-manager>kube-proxy scheduled before node is initialized by cloud-controller-manager</h2><p>In cloud provider scenarios, kube-proxy can end up being scheduled on new worker nodes before
the cloud-controller-manager has initialized the node addresses. This causes kube-proxy to fail
to pick up the node's IP address properly and has knock-on effects to the proxy function managing
load balancers.</p><p>The following error can be seen in kube-proxy Pods:</p><pre tabindex=0><code>server.go:610] Failed to retrieve node IP: host IP unknown; known addresses: []
proxier.go:340] invalid nodeIP, initializing kube-proxy with 127.0.0.1 as nodeIP
</code></pre><p>A known solution is to patch the kube-proxy DaemonSet to allow scheduling it on control-plane
nodes regardless of their conditions, keeping it off of other nodes until their initial guarding
conditions abate:</p><pre tabindex=0><code>kubectl -n kube-system patch ds kube-proxy -p=&#39;{ &#34;spec&#34;: { &#34;template&#34;: { &#34;spec&#34;: { &#34;tolerations&#34;: [ { &#34;key&#34;: &#34;CriticalAddonsOnly&#34;, &#34;operator&#34;: &#34;Exists&#34; }, { &#34;effect&#34;: &#34;NoSchedule&#34;, &#34;key&#34;: &#34;node-role.kubernetes.io/control-plane&#34; } ] } } } }&#39;
</code></pre><p>The tracking issue for this problem is <a href=https://github.com/kubernetes/kubeadm/issues/1027>here</a>.</p><h2 id=usr-mounted-read-only><code>/usr</code> is mounted read-only on nodes</h2><p>On Linux distributions such as Fedora CoreOS or Flatcar Container Linux, the directory <code>/usr</code> is mounted as a read-only filesystem.
For <a href=https://github.com/kubernetes/community/blob/ab55d85/contributors/devel/sig-storage/flexvolume.md>flex-volume support</a>,
Kubernetes components like the kubelet and kube-controller-manager use the default path of
<code>/usr/libexec/kubernetes/kubelet-plugins/volume/exec/</code>, yet the flex-volume directory <em>must be writeable</em>
for the feature to work.
(<strong>Note</strong>: FlexVolume was deprecated in the Kubernetes v1.23 release)</p><p>To workaround this issue you can configure the flex-volume directory using the kubeadm
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/>configuration file</a>.</p><p>On the primary control-plane Node (created using <code>kubeadm init</code>) pass the following
file using <code>--config</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>flex-volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>On joining Nodes:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volume-plugin-dir</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Alternatively, you can modify <code>/etc/fstab</code> to make the <code>/usr</code> mount writeable, but please
be advised that this is modifying a design principle of the Linux distribution.</p><h2 id=kubeadm-upgrade-plan-prints-out-context-deadline-exceeded-error-message><code>kubeadm upgrade plan</code> prints out <code>context deadline exceeded</code> error message</h2><p>This error message is shown when upgrading a Kubernetes cluster with <code>kubeadm</code> in the case of running an external etcd. This is not a critical bug and happens because older versions of kubeadm perform a version check on the external etcd cluster. You can proceed with <code>kubeadm upgrade apply ...</code>.</p><p>This issue is fixed as of version 1.19.</p><h2 id=kubeadm-reset-unmounts-var-lib-kubelet><code>kubeadm reset</code> unmounts <code>/var/lib/kubelet</code></h2><p>If <code>/var/lib/kubelet</code> is being mounted, performing a <code>kubeadm reset</code> will effectively unmount it.</p><p>To workaround the issue, re-mount the <code>/var/lib/kubelet</code> directory after performing the <code>kubeadm reset</code> operation.</p><p>This is a regression introduced in kubeadm 1.15. The issue is fixed in 1.20.</p><h2 id=cannot-use-the-metrics-server-securely-in-a-kubeadm-cluster>Cannot use the metrics-server securely in a kubeadm cluster</h2><p>In a kubeadm cluster, the <a href=https://github.com/kubernetes-sigs/metrics-server>metrics-server</a>
can be used insecurely by passing the <code>--kubelet-insecure-tls</code> to it. This is not recommended for production clusters.</p><p>If you want to use TLS between the metrics-server and the kubelet there is a problem,
since kubeadm deploys a self-signed serving certificate for the kubelet. This can cause the following errors
on the side of the metrics-server:</p><pre tabindex=0><code>x509: certificate signed by unknown authority
x509: certificate is valid for IP-foo not IP-bar
</code></pre><p>See <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs>Enabling signed kubelet serving certificates</a>
to understand how to configure the kubelets in a kubeadm cluster to have properly signed serving certificates.</p><p>Also see <a href=https://github.com/kubernetes-sigs/metrics-server/blob/master/FAQ.md#how-to-run-metrics-server-securely>How to run the metrics-server securely</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-134ed1f6142a98e6ac681a1ba4920e53>2.1.3 - Creating a cluster with kubeadm</h1><p><img src=/images/kubeadm-stacked-color.png align=right width=150px></img>
Using <code>kubeadm</code>, you can create a minimum viable Kubernetes cluster that conforms to best practices.
In fact, you can use <code>kubeadm</code> to set up a cluster that will pass the
<a href=/blog/2017/10/software-conformance-certification/>Kubernetes Conformance tests</a>.
<code>kubeadm</code> also supports other cluster lifecycle functions, such as
<a href=/docs/reference/access-authn-authz/bootstrap-tokens/>bootstrap tokens</a> and cluster upgrades.</p><p>The <code>kubeadm</code> tool is good if you need:</p><ul><li>A simple way for you to try out Kubernetes, possibly for the first time.</li><li>A way for existing users to automate setting up a cluster and test their application.</li><li>A building block in other ecosystem and/or installer tools with a larger
scope.</li></ul><p>You can install and use <code>kubeadm</code> on various machines: your laptop, a set
of cloud servers, a Raspberry Pi, and more. Whether you're deploying into the
cloud or on-premises, you can integrate <code>kubeadm</code> into provisioning systems such
as Ansible or Terraform.</p><h2 id=before-you-begin>Before you begin</h2><p>To follow this guide, you need:</p><ul><li>One or more machines running a deb/rpm-compatible Linux OS; for example: Ubuntu or CentOS.</li><li>2 GiB or more of RAM per machine--any less leaves little room for your
apps.</li><li>At least 2 CPUs on the machine that you use as a control-plane node.</li><li>Full network connectivity among all machines in the cluster. You can use either a
public or a private network.</li></ul><p>You also need to use a version of <code>kubeadm</code> that can deploy the version
of Kubernetes that you want to use in your new cluster.</p><p><a href=/docs/setup/release/version-skew-policy/#supported-versions>Kubernetes' version and version skew support policy</a>
applies to <code>kubeadm</code> as well as to Kubernetes overall.
Check that policy to learn about what versions of Kubernetes and <code>kubeadm</code>
are supported. This page is written for Kubernetes v1.25.</p><p>The <code>kubeadm</code> tool's overall feature state is General Availability (GA). Some sub-features are
still under active development. The implementation of creating the cluster may change
slightly as the tool evolves, but the overall implementation should be pretty stable.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Any commands under <code>kubeadm alpha</code> are, by definition, supported on an alpha level.</div><h2 id=objectives>Objectives</h2><ul><li>Install a single control-plane Kubernetes cluster</li><li>Install a Pod network on the cluster so that your Pods can
talk to each other</li></ul><h2 id=instructions>Instructions</h2><h3 id=preparing-the-hosts>Preparing the hosts</h3><p>Install a <a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a> and kubeadm on all the hosts.
For detailed instructions and other prerequisites, see <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>Installing kubeadm</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>If you have already installed kubeadm, run
<code>apt-get update && apt-get upgrade</code> or
<code>yum update</code> to get the latest version of kubeadm.</p><p>When you upgrade, the kubelet restarts every few seconds as it waits in a crashloop for
kubeadm to tell it what to do. This crashloop is expected and normal.
After you initialize your control-plane, the kubelet runs normally.</p></div><h3 id=preparing-the-required-container-images>Preparing the required container images</h3><p>This step is optional and only applies in case you wish <code>kubeadm init</code> and <code>kubeadm join</code>
to not download the default container images which are hosted at <code>registry.k8s.io</code>.</p><p>Kubeadm has commands that can help you pre-pull the required images
when creating a cluster without an internet connection on its nodes.
See <a href=/docs/reference/setup-tools/kubeadm/kubeadm-init#without-internet-connection>Running kubeadm without an internet connection</a>
for more details.</p><p>Kubeadm allows you to use a custom image repository for the required images.
See <a href=/docs/reference/setup-tools/kubeadm/kubeadm-init#custom-images>Using custom images</a>
for more details.</p><h3 id=initializing-your-control-plane-node>Initializing your control-plane node</h3><p>The control-plane node is the machine where the control plane components run, including
<a class=glossary-tooltip title='Consistent and highly-available key value store used as backing store of Kubernetes for all cluster data.' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a> (the cluster database) and the
<a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API Server'>API Server</a>
(which the <a class=glossary-tooltip title='A command line tool for communicating with a Kubernetes cluster.' data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a> command line tool
communicates with).</p><ol><li>(Recommended) If you have plans to upgrade this single control-plane <code>kubeadm</code> cluster
to high availability you should specify the <code>--control-plane-endpoint</code> to set the shared endpoint
for all control-plane nodes. Such an endpoint can be either a DNS name or an IP address of a load-balancer.</li><li>Choose a Pod network add-on, and verify whether it requires any arguments to
be passed to <code>kubeadm init</code>. Depending on which
third-party provider you choose, you might need to set the <code>--pod-network-cidr</code> to
a provider-specific value. See <a href=#pod-network>Installing a Pod network add-on</a>.</li><li>(Optional) <code>kubeadm</code> tries to detect the container runtime by using a list of well
known endpoints. To use different container runtime or if there are more than one installed
on the provisioned node, specify the <code>--cri-socket</code> argument to <code>kubeadm</code>. See
<a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime>Installing a runtime</a>.</li><li>(Optional) Unless otherwise specified, <code>kubeadm</code> uses the network interface associated
with the default gateway to set the advertise address for this particular control-plane node's API server.
To use a different network interface, specify the <code>--apiserver-advertise-address=&lt;ip-address></code> argument
to <code>kubeadm init</code>. To deploy an IPv6 Kubernetes cluster using IPv6 addressing, you
must specify an IPv6 address, for example <code>--apiserver-advertise-address=2001:db8::101</code></li></ol><p>To initialize the control-plane node run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init &lt;args&gt;
</span></span></code></pre></div><h3 id=considerations-about-apiserver-advertise-address-and-controlplaneendpoint>Considerations about apiserver-advertise-address and ControlPlaneEndpoint</h3><p>While <code>--apiserver-advertise-address</code> can be used to set the advertise address for this particular
control-plane node's API server, <code>--control-plane-endpoint</code> can be used to set the shared endpoint
for all control-plane nodes.</p><p><code>--control-plane-endpoint</code> allows both IP addresses and DNS names that can map to IP addresses.
Please contact your network administrator to evaluate possible solutions with respect to such mapping.</p><p>Here is an example mapping:</p><pre tabindex=0><code>192.168.0.102 cluster-endpoint
</code></pre><p>Where <code>192.168.0.102</code> is the IP address of this node and <code>cluster-endpoint</code> is a custom DNS name that maps to this IP.
This will allow you to pass <code>--control-plane-endpoint=cluster-endpoint</code> to <code>kubeadm init</code> and pass the same DNS name to
<code>kubeadm join</code>. Later you can modify <code>cluster-endpoint</code> to point to the address of your load-balancer in an
high availability scenario.</p><p>Turning a single control plane cluster created without <code>--control-plane-endpoint</code> into a highly available cluster
is not supported by kubeadm.</p><h3 id=more-information>More information</h3><p>For more information about <code>kubeadm init</code> arguments, see the <a href=/docs/reference/setup-tools/kubeadm/>kubeadm reference guide</a>.</p><p>To configure <code>kubeadm init</code> with a configuration file see
<a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file>Using kubeadm init with a configuration file</a>.</p><p>To customize control plane components, including optional IPv6 assignment to liveness probe
for control plane components and etcd server, provide extra arguments to each component as documented in
<a href=/docs/setup/production-environment/tools/kubeadm/control-plane-flags/>custom arguments</a>.</p><p>To reconfigure a cluster that has already been created see
<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure>Reconfiguring a kubeadm cluster</a>.</p><p>To run <code>kubeadm init</code> again, you must first <a href=#tear-down>tear down the cluster</a>.</p><p>If you join a node with a different architecture to your cluster, make sure that your deployed DaemonSets
have container image support for this architecture.</p><p><code>kubeadm init</code> first runs a series of prechecks to ensure that the machine
is ready to run Kubernetes. These prechecks expose warnings and exit on errors. <code>kubeadm init</code>
then downloads and installs the cluster control plane components. This may take several minutes.
After it finishes you should see:</p><pre tabindex=0><code class=language-none data-lang=none>Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a Pod network to the cluster.
Run &#34;kubectl apply -f [podnetwork].yaml&#34; with one of the options listed at:
  /docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</code></pre><p>To make kubectl work for your non-root user, run these commands, which are
also part of the <code>kubeadm init</code> output:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>mkdir -p <span style=color:#b8860b>$HOME</span>/.kube
</span></span><span style=display:flex><span>sudo cp -i /etc/kubernetes/admin.conf <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span><span style=display:flex><span>sudo chown <span style=color:#a2f;font-weight:700>$(</span>id -u<span style=color:#a2f;font-weight:700>)</span>:<span style=color:#a2f;font-weight:700>$(</span>id -g<span style=color:#a2f;font-weight:700>)</span> <span style=color:#b8860b>$HOME</span>/.kube/config
</span></span></code></pre></div><p>Alternatively, if you are the <code>root</code> user, you can run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>KUBECONFIG</span><span style=color:#666>=</span>/etc/kubernetes/admin.conf
</span></span></code></pre></div><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> Kubeadm signs the certificate in the <code>admin.conf</code> to have <code>Subject: O = system:masters, CN = kubernetes-admin</code>.
<code>system:masters</code> is a break-glass, super user group that bypasses the authorization layer (e.g. RBAC).
Do not share the <code>admin.conf</code> file with anyone and instead grant users custom permissions by generating
them a kubeconfig file using the <code>kubeadm kubeconfig user</code> command. For more details see
<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-certs#kubeconfig-additional-users>Generating kubeconfig files for additional users</a>.</div><p>Make a record of the <code>kubeadm join</code> command that <code>kubeadm init</code> outputs. You
need this command to <a href=#join-nodes>join nodes to your cluster</a>.</p><p>The token is used for mutual authentication between the control-plane node and the joining
nodes. The token included here is secret. Keep it safe, because anyone with this
token can add authenticated nodes to your cluster. These tokens can be listed,
created, and deleted with the <code>kubeadm token</code> command. See the
<a href=/docs/reference/setup-tools/kubeadm/kubeadm-token/>kubeadm reference guide</a>.</p><h3 id=pod-network>Installing a Pod network add-on</h3><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>This section contains important information about networking setup and
deployment order.
Read all of this advice carefully before proceeding.</p><p><strong>You must deploy a
<a class=glossary-tooltip title='Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label='Container Network Interface'>Container Network Interface</a>
(CNI) based Pod network add-on so that your Pods can communicate with each other.
Cluster DNS (CoreDNS) will not start up before a network is installed.</strong></p><ul><li><p>Take care that your Pod network must not overlap with any of the host
networks: you are likely to see problems if there is any overlap.
(If you find a collision between your network plugin's preferred Pod
network and some of your host networks, you should think of a suitable
CIDR block to use instead, then use that during <code>kubeadm init</code> with
<code>--pod-network-cidr</code> and as a replacement in your network plugin's YAML).</p></li><li><p>By default, <code>kubeadm</code> sets up your cluster to use and enforce use of
<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a> (role based access
control).
Make sure that your Pod network plugin supports RBAC, and so do any manifests
that you use to deploy it.</p></li><li><p>If you want to use IPv6--either dual-stack, or single-stack IPv6 only
networking--for your cluster, make sure that your Pod network plugin
supports IPv6.
IPv6 support was added to CNI in <a href=https://github.com/containernetworking/cni/releases/tag/v0.6.0>v0.6.0</a>.</p></li></ul></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubeadm should be CNI agnostic and the validation of CNI providers is out of the scope of our current e2e testing.
If you find an issue related to a CNI plugin you should log a ticket in its respective issue
tracker instead of the kubeadm or kubernetes issue trackers.</div><p>Several external projects provide Kubernetes Pod networks using CNI, some of which also
support <a href=/docs/concepts/services-networking/network-policies/>Network Policy</a>.</p><p>See a list of add-ons that implement the
<a href=/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>Kubernetes networking model</a>.</p><p>You can install a Pod network add-on with the following command on the
control-plane node or a node that has the kubeconfig credentials:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f &lt;add-on.yaml&gt;
</span></span></code></pre></div><p>You can install only one Pod network per cluster.</p><p>Once a Pod network has been installed, you can confirm that it is working by
checking that the CoreDNS Pod is <code>Running</code> in the output of <code>kubectl get pods --all-namespaces</code>.
And once the CoreDNS Pod is up and running, you can continue by joining your nodes.</p><p>If your network is not working or CoreDNS is not in the <code>Running</code> state, check out the
<a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>troubleshooting guide</a>
for <code>kubeadm</code>.</p><h3 id=managed-node-labels>Managed node labels</h3><p>By default, kubeadm enables the <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction</a>
admission controller that restricts what labels can be self-applied by kubelets on node registration.
The admission controller documentation covers what labels are permitted to be used with the kubelet <code>--node-labels</code> option.
The <code>node-role.kubernetes.io/control-plane</code> label is such a restricted label and kubeadm manually applies it using
a privileged client after a node has been created. To do that manually you can do the same by using <code>kubectl label</code>
and ensure it is using a privileged kubeconfig such as the kubeadm managed <code>/etc/kubernetes/admin.conf</code>.</p><h3 id=control-plane-node-isolation>Control plane node isolation</h3><p>By default, your cluster will not schedule Pods on the control plane nodes for security
reasons. If you want to be able to schedule Pods on the control plane nodes,
for example for a single machine Kubernetes cluster, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl taint nodes --all node-role.kubernetes.io/control-plane-
</span></span></code></pre></div><p>The output will look something like:</p><pre tabindex=0><code>node &#34;test-01&#34; untainted
...
</code></pre><p>This will remove the <code>node-role.kubernetes.io/control-plane:NoSchedule</code> taint
from any nodes that have it, including the control plane nodes, meaning that the
scheduler will then be able to schedule Pods everywhere.</p><h3 id=join-nodes>Joining your nodes</h3><p>The nodes are where your workloads (containers and Pods, etc) run. To add new nodes to your cluster do the following for each machine:</p><ul><li><p>SSH to the machine</p></li><li><p>Become root (e.g. <code>sudo su -</code>)</p></li><li><p><a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-runtime>Install a runtime</a>
if needed</p></li><li><p>Run the command that was output by <code>kubeadm init</code>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm join --token &lt;token&gt; &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;
</span></span></code></pre></div></li></ul><p>If you do not have the token, you can get it by running the following command on the control-plane node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token list
</span></span></code></pre></div><p>The output is similar to this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>TOKEN                    TTL  EXPIRES              USAGES           DESCRIPTION            EXTRA GROUPS
</span></span></span><span style=display:flex><span><span style=color:#888>8ewj1p.9r9hcjoqgajrj4gi  23h  2018-06-12T02:51:28Z authentication,  The default bootstrap  system:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                   signing          token generated by     bootstrappers:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                    &#39;kubeadm init&#39;.        kubeadm:
</span></span></span><span style=display:flex><span><span style=color:#888>                                                                                           default-node-token
</span></span></span></code></pre></div><p>By default, tokens expire after 24 hours. If you are joining a node to the cluster after the current token has expired,
you can create a new token by running the following command on the control-plane node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm token create
</span></span></code></pre></div><p>The output is similar to this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>5didvk.d09sbcov8ph2amjw
</span></span></span></code></pre></div><p>If you don't have the value of <code>--discovery-token-ca-cert-hash</code>, you can get it by running the
following command chain on the control-plane node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>   openssl dgst -sha256 -hex | sed <span style=color:#b44>&#39;s/^.* //&#39;</span>
</span></span></code></pre></div><p>The output is similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>8cb2de97839780a412b93877f8507ad6c94f73add17d5d7058e91741c9d5ec78
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> To specify an IPv6 tuple for <code>&lt;control-plane-host>:&lt;control-plane-port></code>, IPv6 address must be enclosed in square brackets, for example: <code>[2001:db8::101]:2073</code>.</div><p>The output should look something like:</p><pre tabindex=0><code>[preflight] Running pre-flight checks

... (log output of join workflow) ...

Node join complete:
* Certificate signing request sent to control-plane and response
  received.
* Kubelet informed of new secure connection details.

Run &#39;kubectl get nodes&#39; on control-plane to see this machine join.
</code></pre><p>A few seconds later, you should notice this node in the output from <code>kubectl get nodes</code> when run on the control-plane node.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> As the cluster nodes are usually initialized sequentially, the CoreDNS Pods are likely to all run
on the first control-plane node. To provide higher availability, please rebalance the CoreDNS Pods
with <code>kubectl -n kube-system rollout restart deployment coredns</code> after at least one new node is joined.</div><h3 id=optional-controlling-your-cluster-from-machines-other-than-the-control-plane-node>(Optional) Controlling your cluster from machines other than the control-plane node</h3><p>In order to get a kubectl on some other computer (e.g. laptop) to talk to your
cluster, you need to copy the administrator kubeconfig file from your control-plane node
to your workstation like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf get nodes
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>The example above assumes SSH access is enabled for root. If that is not the
case, you can copy the <code>admin.conf</code> file to be accessible by some other user
and <code>scp</code> using that other user instead.</p><p>The <code>admin.conf</code> file gives the user <em>superuser</em> privileges over the cluster.
This file should be used sparingly. For normal users, it's recommended to
generate an unique credential to which you grant privileges. You can do
this with the <code>kubeadm alpha kubeconfig user --client-name &lt;CN></code>
command. That command will print out a KubeConfig file to STDOUT which you
should save to a file and distribute to your user. After that, grant
privileges by using <code>kubectl create (cluster)rolebinding</code>.</p></div><h3 id=optional-proxying-api-server-to-localhost>(Optional) Proxying API Server to localhost</h3><p>If you want to connect to the API Server from outside the cluster you can use
<code>kubectl proxy</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scp root@&lt;control-plane-host&gt;:/etc/kubernetes/admin.conf .
</span></span><span style=display:flex><span>kubectl --kubeconfig ./admin.conf proxy
</span></span></code></pre></div><p>You can now access the API Server locally at <code>http://localhost:8001/api/v1</code></p><h2 id=tear-down>Clean up</h2><p>If you used disposable servers for your cluster, for testing, you can
switch those off and do no further clean up. You can use
<code>kubectl config delete-cluster</code> to delete your local references to the
cluster.</p><p>However, if you want to deprovision your cluster more cleanly, you should
first <a href=/docs/reference/generated/kubectl/kubectl-commands#drain>drain the node</a>
and make sure that the node is empty, then deconfigure the node.</p><h3 id=remove-the-node>Remove the node</h3><p>Talking to the control-plane node with the appropriate credentials, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl drain &lt;node name&gt; --delete-emptydir-data --force --ignore-daemonsets
</span></span></code></pre></div><p>Before removing the node, reset the state installed by <code>kubeadm</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm reset
</span></span></code></pre></div><p>The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>iptables -F <span style=color:#666>&amp;&amp;</span> iptables -t nat -F <span style=color:#666>&amp;&amp;</span> iptables -t mangle -F <span style=color:#666>&amp;&amp;</span> iptables -X
</span></span></code></pre></div><p>If you want to reset the IPVS tables, you must run the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ipvsadm -C
</span></span></code></pre></div><p>Now remove the node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl delete node &lt;node name&gt;
</span></span></code></pre></div><p>If you wish to start over, run <code>kubeadm init</code> or <code>kubeadm join</code> with the
appropriate arguments.</p><h3 id=clean-up-the-control-plane>Clean up the control plane</h3><p>You can use <code>kubeadm reset</code> on the control plane host to trigger a best-effort
clean up.</p><p>See the <a href=/docs/reference/setup-tools/kubeadm/kubeadm-reset/><code>kubeadm reset</code></a>
reference documentation for more information about this subcommand and its
options.</p><h2 id=whats-next>What's next</h2><ul><li>Verify that your cluster is running properly with <a href=https://github.com/heptio/sonobuoy>Sonobuoy</a></li><li><a id=lifecycle>See <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>Upgrading kubeadm clusters</a>
for details about upgrading your cluster using <code>kubeadm</code>.</li><li>Learn about advanced <code>kubeadm</code> usage in the <a href=/docs/reference/setup-tools/kubeadm/>kubeadm reference documentation</a></li><li>Learn more about Kubernetes <a href=/docs/concepts/>concepts</a> and <a href=/docs/reference/kubectl/><code>kubectl</code></a>.</li><li>See the <a href=/docs/concepts/cluster-administration/networking/>Cluster Networking</a> page for a bigger list
of Pod network add-ons.</li><li><a id=other-addons>See the <a href=/docs/concepts/cluster-administration/addons/>list of add-ons</a> to
explore other add-ons, including tools for logging, monitoring, network policy, visualization &
control of your Kubernetes cluster.</li><li>Configure how your cluster handles logs for cluster events and from
applications running in Pods.
See <a href=/docs/concepts/cluster-administration/logging/>Logging Architecture</a> for
an overview of what is involved.</li></ul><h3 id=feedback>Feedback</h3><ul><li>For bugs, visit the <a href=https://github.com/kubernetes/kubeadm/issues>kubeadm GitHub issue tracker</a></li><li>For support, visit the
<a href=https://kubernetes.slack.com/messages/kubeadm/>#kubeadm</a> Slack channel</li><li>General SIG Cluster Lifecycle development Slack channel:
<a href=https://kubernetes.slack.com/messages/sig-cluster-lifecycle/>#sig-cluster-lifecycle</a></li><li>SIG Cluster Lifecycle <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme>SIG information</a></li><li>SIG Cluster Lifecycle mailing list:
<a href=https://groups.google.com/forum/#!forum/kubernetes-sig-cluster-lifecycle>kubernetes-sig-cluster-lifecycle</a></li></ul><h2 id=version-skew-policy>Version skew policy</h2><p>While kubeadm allows version skew against some components that it manages, it is recommended that you
match the kubeadm version with the versions of the control plane components, kube-proxy and kubelet.</p><h3 id=kubeadm-s-skew-against-the-kubernetes-version>kubeadm's skew against the Kubernetes version</h3><p>kubeadm can be used with Kubernetes components that are the same version as kubeadm
or one version older. The Kubernetes version can be specified to kubeadm by using the
<code>--kubernetes-version</code> flag of <code>kubeadm init</code> or the
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/><code>ClusterConfiguration.kubernetesVersion</code></a>
field when using <code>--config</code>. This option will control the versions
of kube-apiserver, kube-controller-manager, kube-scheduler and kube-proxy.</p><p>Example:</p><ul><li>kubeadm is at 1.25</li><li><code>kubernetesVersion</code> must be at 1.25 or 1.24</li></ul><h3 id=kubeadm-s-skew-against-the-kubelet>kubeadm's skew against the kubelet</h3><p>Similarly to the Kubernetes version, kubeadm can be used with a kubelet version that is the same
version as kubeadm or one version older.</p><p>Example:</p><ul><li>kubeadm is at 1.25</li><li>kubelet on the host must be at 1.25 or 1.24</li></ul><h3 id=kubeadm-s-skew-against-kubeadm>kubeadm's skew against kubeadm</h3><p>There are certain limitations on how kubeadm commands can operate on existing nodes or whole clusters
managed by kubeadm.</p><p>If new nodes are joined to the cluster, the kubeadm binary used for <code>kubeadm join</code> must match
the last version of kubeadm used to either create the cluster with <code>kubeadm init</code> or to upgrade
the same node with <code>kubeadm upgrade</code>. Similar rules apply to the rest of the kubeadm commands
with the exception of <code>kubeadm upgrade</code>.</p><p>Example for <code>kubeadm join</code>:</p><ul><li>kubeadm version 1.25 was used to create a cluster with <code>kubeadm init</code></li><li>Joining nodes must use a kubeadm binary that is at version 1.25</li></ul><p>Nodes that are being upgraded must use a version of kubeadm that is the same MINOR
version or one MINOR version newer than the version of kubeadm used for managing the
node.</p><p>Example for <code>kubeadm upgrade</code>:</p><ul><li>kubeadm version 1.24 was used to create or upgrade the node</li><li>The version of kubeadm used for upgrading the node must be at 1.24
or 1.25</li></ul><p>To learn more about the version skew between the different Kubernetes component see
the <a href=/releases/version-skew-policy/>Version Skew Policy</a>.</p><h2 id=limitations>Limitations</h2><h3 id=resilience>Cluster resilience</h3><p>The cluster created here has a single control-plane node, with a single etcd database
running on it. This means that if the control-plane node fails, your cluster may lose
data and may need to be recreated from scratch.</p><p>Workarounds:</p><ul><li><p>Regularly <a href=https://etcd.io/docs/v3.5/op-guide/recovery/>back up etcd</a>. The
etcd data directory configured by kubeadm is at <code>/var/lib/etcd</code> on the control-plane node.</p></li><li><p>Use multiple control-plane nodes. You can read
<a href=/docs/setup/production-environment/tools/kubeadm/ha-topology/>Options for Highly Available topology</a> to pick a cluster
topology that provides <a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>high-availability</a>.</p></li></ul><h3 id=multi-platform>Platform compatibility</h3><p>kubeadm deb/rpm packages and binaries are built for amd64, arm (32-bit), arm64, ppc64le, and s390x
following the <a href=https://git.k8s.io/design-proposals-archive/multi-platform.md>multi-platform
proposal</a>.</p><p>Multiplatform container images for the control plane and addons are also supported since v1.12.</p><p>Only some of the network providers offer solutions for all platforms. Please consult the list of
network providers above or the documentation from each provider to figure out whether the provider
supports your chosen platform.</p><h2 id=troubleshooting>Troubleshooting</h2><p>If you are running into difficulties with kubeadm, please consult our
<a href=/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/>troubleshooting docs</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4c656c5eda3e1c06ad1aedebdc04a211>2.1.4 - Customizing components with the kubeadm API</h1><p>This page covers how to customize the components that kubeadm deploys. For control plane components
you can use flags in the <code>ClusterConfiguration</code> structure or patches per-node. For the kubelet
and kube-proxy you can use <code>KubeletConfiguration</code> and <code>KubeProxyConfiguration</code>, accordingly.</p><p>All of these options are possible via the kubeadm configuration API.
For more details on each field in the configuration you can navigate to our
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/>API reference pages</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Customizing the CoreDNS deployment of kubeadm is currently not supported. You must manually
patch the <code>kube-system/coredns</code> <a class=glossary-tooltip title='An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a>
and recreate the CoreDNS <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> after that. Alternatively,
you can skip the default CoreDNS deployment and deploy your own variant.
For more details on that see <a href=/docs/reference/setup-tools/kubeadm/kubeadm-init/#init-phases>Using init phases with kubeadm</a>.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> To reconfigure a cluster that has already been created see
<a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-reconfigure>Reconfiguring a kubeadm cluster</a>.</div><h2 id=customizing-the-control-plane-with-flags-in-clusterconfiguration>Customizing the control plane with flags in <code>ClusterConfiguration</code></h2><p>The kubeadm <code>ClusterConfiguration</code> object exposes a way for users to override the default
flags passed to control plane components such as the APIServer, ControllerManager, Scheduler and Etcd.
The components are defined using the following structures:</p><ul><li><code>apiServer</code></li><li><code>controllerManager</code></li><li><code>scheduler</code></li><li><code>etcd</code></li></ul><p>These structures contain a common <code>extraArgs</code> field, that consists of <code>key: value</code> pairs.
To override a flag for a control plane component:</p><ol><li>Add the appropriate <code>extraArgs</code> to your configuration.</li><li>Add flags to the <code>extraArgs</code> field.</li><li>Run <code>kubeadm init</code> with <code>--config &lt;YOUR CONFIG YAML></code>.</li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You can generate a <code>ClusterConfiguration</code> object with default values by running <code>kubeadm config print init-defaults</code>
and saving the output to a file of your choice.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>ClusterConfiguration</code> object is currently global in kubeadm clusters. This means that any flags that you add,
will apply to all instances of the same component on different nodes. To apply individual configuration per component
on different nodes you can use <a href=#patches>patches</a>.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Duplicate flags (keys), or passing the same flag <code>--foo</code> multiple times, is currently not supported.
To workaround that you must use <a href=#patches>patches</a>.</div><h3 id=apiserver-flags>APIServer flags</h3><p>For details, see the <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>reference documentation for kube-apiserver</a>.</p><p>Example usage:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiServer</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>anonymous-auth</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;false&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>enable-admission-plugins</span>:<span style=color:#bbb> </span>AlwaysPullImages,DefaultStorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>audit-log-path</span>:<span style=color:#bbb> </span>/home/johndoe/audit.log<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=controllermanager-flags>ControllerManager flags</h3><p>For details, see the <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>reference documentation for kube-controller-manager</a>.</p><p>Example usage:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controllerManager</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cluster-signing-key-file</span>:<span style=color:#bbb> </span>/home/johndoe/keys/ca.key<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>deployment-controller-sync-period</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;50&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=scheduler-flags>Scheduler flags</h3><p>For details, see the <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>reference documentation for kube-scheduler</a>.</p><p>Example usage:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>v1.16.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduler</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>config</span>:<span style=color:#bbb> </span>/etc/kubernetes/scheduler-config.yaml<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extraVolumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>schedulerconfig<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb> </span>/home/johndoe/schedconfig.yaml<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/kubernetes/scheduler-config.yaml<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;File&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=etcd-flags>Etcd flags</h3><p>For details, see the <a href=https://etcd.io/docs/>etcd server documentation</a>.</p><p>Example usage:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>etcd</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>local</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>extraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>election-timeout</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=patches>Customizing with patches</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code></div><p>Kubeadm allows you to pass a directory with patch files to <code>InitConfiguration</code> and <code>JoinConfiguration</code>
on individual nodes. These patches can be used as the last customization step before component configuration
is written to disk.</p><p>You can pass this file to <code>kubeadm init</code> with <code>--config &lt;YOUR CONFIG YAML></code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>patches</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>directory</span>:<span style=color:#bbb> </span>/home/user/somedir<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> For <code>kubeadm init</code> you can pass a file containing both a <code>ClusterConfiguration</code> and <code>InitConfiguration</code>
separated by <code>---</code>.</div><p>You can pass this file to <code>kubeadm join</code> with <code>--config &lt;YOUR CONFIG YAML></code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>patches</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>directory</span>:<span style=color:#bbb> </span>/home/user/somedir<span style=color:#bbb>
</span></span></span></code></pre></div><p>The directory must contain files named <code>target[suffix][+patchtype].extension</code>.
For example, <code>kube-apiserver0+merge.yaml</code> or just <code>etcd.json</code>.</p><ul><li><code>target</code> can be one of <code>kube-apiserver</code>, <code>kube-controller-manager</code>, <code>kube-scheduler</code>, <code>etcd</code>
and <code>kubeletconfiguration</code>.</li><li><code>patchtype</code> can be one of <code>strategic</code>, <code>merge</code> or <code>json</code> and these must match the patching formats
<a href=/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch>supported by kubectl</a>.
The default <code>patchtype</code> is <code>strategic</code>.</li><li><code>extension</code> must be either <code>json</code> or <code>yaml</code>.</li><li><code>suffix</code> is an optional string that can be used to determine which patches are applied first
alpha-numerically.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you are using <code>kubeadm upgrade</code> to upgrade your kubeadm nodes you must again provide the same
patches, so that the customization is preserved after upgrade. To do that you can use the <code>--patches</code>
flag, which must point to the same directory. <code>kubeadm upgrade</code> currently does not support a configuration
API structure that can be used for the same purpose.</div><h2 id=kubelet>Customizing the kubelet</h2><p>To customize the kubelet you can add a <a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>
next to the <code>ClusterConfiguration</code> or <code>InitConfiguration</code> separated by <code>---</code> within the same configuration file.
This file can then be passed to <code>kubeadm init</code> and kubeadm will apply the same base <code>KubeletConfiguration</code>
to all nodes in the cluster.</p><p>For applying instance-specific configuration over the base <code>KubeletConfiguration</code> you can use the
<a href=#patches><code>kubeletconfiguration</code> patch target</a>.</p><p>Alternatively, you can use kubelet flags as overrides by passing them in the
<code>nodeRegistration.kubeletExtraArgs</code> field supported by both <code>InitConfiguration</code> and <code>JoinConfiguration</code>.
Some kubelet flags are deprecated, so check their status in the
<a href=/docs/reference/command-line-tools-reference/kubelet>kubelet reference documentation</a> before using them.</p><p>For additional details see <a href=/docs/setup/production-environment/tools/kubeadm/kubelet-integration>Configuring each kubelet in your cluster using kubeadm</a></p><h2 id=customizing-kube-proxy>Customizing kube-proxy</h2><p>To customize kube-proxy you can pass a <code>KubeProxyConfiguration</code> next your <code>ClusterConfiguration</code> or
<code>InitConfiguration</code> to <code>kubeadm init</code> separated by <code>---</code>.</p><p>For more details you can navigate to our <a href=/docs/reference/config-api/kubeadm-config.v1beta3/>API reference pages</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> kubeadm deploys kube-proxy as a <a class=glossary-tooltip title='Ensures a copy of a Pod is running across a set of nodes in a cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a>, which means
that the <code>KubeProxyConfiguration</code> would apply to all instances of kube-proxy in the cluster.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-015edbc7cc688d31b1d1edce7c186135>2.1.5 - Options for Highly Available Topology</h1><p>This page explains the two options for configuring the topology of your highly available (HA) Kubernetes clusters.</p><p>You can set up an HA cluster:</p><ul><li>With stacked control plane nodes, where etcd nodes are colocated with control plane nodes</li><li>With external etcd nodes, where etcd runs on separate nodes from the control plane</li></ul><p>You should carefully consider the advantages and disadvantages of each topology before setting up an HA cluster.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> kubeadm bootstraps the etcd cluster statically. Read the etcd <a href=https://github.com/etcd-io/etcd/blob/release-3.4/Documentation/op-guide/clustering.md#static>Clustering Guide</a>
for more details.</div><h2 id=stacked-etcd-topology>Stacked etcd topology</h2><p>A stacked HA cluster is a <a href=https://en.wikipedia.org/wiki/Network_topology>topology</a> where the distributed
data storage cluster provided by etcd is stacked on top of the cluster formed by the nodes managed by
kubeadm that run control plane components.</p><p>Each control plane node runs an instance of the <code>kube-apiserver</code>, <code>kube-scheduler</code>, and <code>kube-controller-manager</code>.
The <code>kube-apiserver</code> is exposed to worker nodes using a load balancer.</p><p>Each control plane node creates a local etcd member and this etcd member communicates only with
the <code>kube-apiserver</code> of this node. The same applies to the local <code>kube-controller-manager</code>
and <code>kube-scheduler</code> instances.</p><p>This topology couples the control planes and etcd members on the same nodes. It is simpler to set up than a cluster
with external etcd nodes, and simpler to manage for replication.</p><p>However, a stacked cluster runs the risk of failed coupling. If one node goes down, both an etcd member and a control
plane instance are lost, and redundancy is compromised. You can mitigate this risk by adding more control plane nodes.</p><p>You should therefore run a minimum of three stacked control plane nodes for an HA cluster.</p><p>This is the default topology in kubeadm. A local etcd member is created automatically
on control plane nodes when using <code>kubeadm init</code> and <code>kubeadm join --control-plane</code>.</p><p><img src=/images/kubeadm/kubeadm-ha-topology-stacked-etcd.svg alt="Stacked etcd topology"></p><h2 id=external-etcd-topology>External etcd topology</h2><p>An HA cluster with external etcd is a <a href=https://en.wikipedia.org/wiki/Network_topology>topology</a> where the distributed data storage cluster provided by etcd is external to the cluster formed by the nodes that run control plane components.</p><p>Like the stacked etcd topology, each control plane node in an external etcd topology runs an instance of the <code>kube-apiserver</code>, <code>kube-scheduler</code>, and <code>kube-controller-manager</code>. And the <code>kube-apiserver</code> is exposed to worker nodes using a load balancer. However, etcd members run on separate hosts, and each etcd host communicates with the <code>kube-apiserver</code> of each control plane node.</p><p>This topology decouples the control plane and etcd member. It therefore provides an HA setup where
losing a control plane instance or an etcd member has less impact and does not affect
the cluster redundancy as much as the stacked HA topology.</p><p>However, this topology requires twice the number of hosts as the stacked HA topology.
A minimum of three hosts for control plane nodes and three hosts for etcd nodes are required for an HA cluster with this topology.</p><p><img src=/images/kubeadm/kubeadm-ha-topology-external-etcd.svg alt="External etcd topology"></p><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>Set up a highly available cluster with kubeadm</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3941d5c3409342219bf7e03128b8ecb6>2.1.6 - Creating Highly Available Clusters with kubeadm</h1><p>This page explains two different approaches to setting up a highly available Kubernetes
cluster using kubeadm:</p><ul><li>With stacked control plane nodes. This approach requires less infrastructure. The etcd members
and control plane nodes are co-located.</li><li>With an external etcd cluster. This approach requires more infrastructure. The
control plane nodes and etcd members are separated.</li></ul><p>Before proceeding, you should carefully consider which approach best meets the needs of your applications
and environment. <a href=/docs/setup/production-environment/tools/kubeadm/ha-topology/>Options for Highly Available topology</a> outlines the advantages and disadvantages of each.</p><p>If you encounter issues with setting up the HA cluster, please report these
in the kubeadm <a href=https://github.com/kubernetes/kubeadm/issues/new>issue tracker</a>.</p><p>See also the <a href=/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>upgrade documentation</a>.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> This page does not address running your cluster on a cloud provider. In a cloud
environment, neither approach documented here works with Service objects of type
LoadBalancer, or with dynamic PersistentVolumes.</div><h2 id=before-you-begin>Before you begin</h2><p>The prerequisites depend on which topology you have selected for your cluster's
control plane:</p><ul class="nav nav-tabs" id=prerequisite-tabs role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#prerequisite-tabs-0 role=tab aria-controls=prerequisite-tabs-0 aria-selected=true>Stacked etcd</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#prerequisite-tabs-1 role=tab aria-controls=prerequisite-tabs-1>External etcd</a></li></ul><div class=tab-content id=prerequisite-tabs><div id=prerequisite-tabs-0 class="tab-pane show active" role=tabpanel aria-labelledby=prerequisite-tabs-0><p><p>You need:</p><ul><li>Three or more machines that meet <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>kubeadm's minimum requirements</a> for
the control-plane nodes. Having an odd number of control plane nodes can help
with leader selection in the case of machine or zone failure.<ul><li>including a <a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>, already set up and working</li></ul></li><li>Three or more machines that meet <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>kubeadm's minimum
requirements</a> for the workers<ul><li>including a container runtime, already set up and working</li></ul></li><li>Full network connectivity between all machines in the cluster (public or
private network)</li><li>Superuser privileges on all machines using <code>sudo</code><ul><li>You can use a different tool; this guide uses <code>sudo</code> in the examples.</li></ul></li><li>SSH access from one device to all nodes in the system</li><li><code>kubeadm</code> and <code>kubelet</code> already installed on all machines.</li></ul><p><em>See <a href=/docs/setup/production-environment/tools/kubeadm/ha-topology/#stacked-etcd-topology>Stacked etcd topology</a> for context.</em></p></div><div id=prerequisite-tabs-1 class=tab-pane role=tabpanel aria-labelledby=prerequisite-tabs-1><p><p>You need:</p><ul><li>Three or more machines that meet <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>kubeadm's minimum requirements</a> for
the control-plane nodes. Having an odd number of control plane nodes can help
with leader selection in the case of machine or zone failure.<ul><li>including a <a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>, already set up and working</li></ul></li><li>Three or more machines that meet <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin>kubeadm's minimum
requirements</a> for the workers<ul><li>including a container runtime, already set up and working</li></ul></li><li>Full network connectivity between all machines in the cluster (public or
private network)</li><li>Superuser privileges on all machines using <code>sudo</code><ul><li>You can use a different tool; this guide uses <code>sudo</code> in the examples.</li></ul></li><li>SSH access from one device to all nodes in the system</li><li><code>kubeadm</code> and <code>kubelet</code> already installed on all machines.</li></ul><p>And you also need:</p><ul><li>Three or more additional machines, that will become etcd cluster members.
Having an odd number of members in the etcd cluster is a requirement for achieving
optimal voting quorum.<ul><li>These machines again need to have <code>kubeadm</code> and <code>kubelet</code> installed.</li><li>These machines also require a container runtime, that is already set up and working.</li></ul></li></ul><p><em>See <a href=/docs/setup/production-environment/tools/kubeadm/ha-topology/#external-etcd-topology>External etcd topology</a> for context.</em></p></div></div><h3 id=container-images>Container images</h3><p>Each host should have access read and fetch images from the Kubernetes container image registry, <code>registry.k8s.io</code>.
If you want to deploy a highly-available cluster where the hosts do not have access to pull images, this is possible. You must ensure by some other means that the correct container images are already available on the relevant hosts.</p><h3 id=kubectl>Command line interface</h3><p>To manage Kubernetes once your cluster is set up, you should
<a href=/docs/tasks/tools/#kubectl>install kubectl</a> on your PC. It is also useful
to install the <code>kubectl</code> tool on each control plane node, as this can be
helpful for troubleshooting.</p><h2 id=first-steps-for-both-methods>First steps for both methods</h2><h3 id=create-load-balancer-for-kube-apiserver>Create load balancer for kube-apiserver</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> There are many configurations for load balancers. The following example is only one
option. Your cluster requirements may need a different configuration.</div><ol><li><p>Create a kube-apiserver load balancer with a name that resolves to DNS.</p><ul><li><p>In a cloud environment you should place your control plane nodes behind a TCP
forwarding load balancer. This load balancer distributes traffic to all
healthy control plane nodes in its target list. The health check for
an apiserver is a TCP check on the port the kube-apiserver listens on
(default value <code>:6443</code>).</p></li><li><p>It is not recommended to use an IP address directly in a cloud environment.</p></li><li><p>The load balancer must be able to communicate with all control plane nodes
on the apiserver port. It must also allow incoming traffic on its
listening port.</p></li><li><p>Make sure the address of the load balancer always matches
the address of kubeadm's <code>ControlPlaneEndpoint</code>.</p></li><li><p>Read the <a href=https://git.k8s.io/kubeadm/docs/ha-considerations.md#options-for-software-load-balancing>Options for Software Load Balancing</a>
guide for more details.</p></li></ul></li><li><p>Add the first control plane node to the load balancer, and test the
connection:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>nc -v &lt;LOAD_BALANCER_IP&gt; &lt;PORT&gt;
</span></span></code></pre></div><p>A connection refused error is expected because the API server is not yet
running. A timeout, however, means the load balancer cannot communicate
with the control plane node. If a timeout occurs, reconfigure the load
balancer to communicate with the control plane node.</p></li><li><p>Add the remaining control plane nodes to the load balancer target group.</p></li></ol><h2 id=stacked-control-plane-and-etcd-nodes>Stacked control plane and etcd nodes</h2><h3 id=steps-for-the-first-control-plane-node>Steps for the first control plane node</h3><ol><li><p>Initialize the control plane:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm init --control-plane-endpoint <span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span> --upload-certs
</span></span></code></pre></div><ul><li><p>You can use the <code>--kubernetes-version</code> flag to set the Kubernetes version to use.
It is recommended that the versions of kubeadm, kubelet, kubectl and Kubernetes match.</p></li><li><p>The <code>--control-plane-endpoint</code> flag should be set to the address or DNS and port of the load balancer.</p></li><li><p>The <code>--upload-certs</code> flag is used to upload the certificates that should be shared
across all the control-plane instances to the cluster. If instead, you prefer to copy certs across
control-plane nodes manually or using automation tools, please remove this flag and refer to <a href=#manual-certs>Manual
certificate distribution</a> section below.</p></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>kubeadm init</code> flags <code>--config</code> and <code>--certificate-key</code> cannot be mixed, therefore if you want
to use the <a href=/docs/reference/config-api/kubeadm-config.v1beta3/>kubeadm configuration</a>
you must add the <code>certificateKey</code> field in the appropriate config locations
(under <code>InitConfiguration</code> and <code>JoinConfiguration: controlPlane</code>).</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Some CNI network plugins require additional configuration, for example specifying the pod IP CIDR, while others do not.
See the <a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>CNI network documentation</a>.
To add a pod CIDR pass the flag <code>--pod-network-cidr</code>, or if you are using a kubeadm configuration file
set the <code>podSubnet</code> field under the <code>networking</code> object of <code>ClusterConfiguration</code>.</div><p>The output looks similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>...
</span></span><span style=display:flex><span>You can now join any number of control-plane node by running the following <span style=color:#a2f>command</span> on each as a root:
</span></span><span style=display:flex><span>    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
</span></span><span style=display:flex><span>As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use kubeadm init phase upload-certs to reload certs afterward.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style=display:flex><span>    kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><ul><li><p>Copy this output to a text file. You will need it later to join control plane and worker nodes to
the cluster.</p></li><li><p>When <code>--upload-certs</code> is used with <code>kubeadm init</code>, the certificates of the primary control plane
are encrypted and uploaded in the <code>kubeadm-certs</code> Secret.</p></li><li><p>To re-upload the certificates and generate a new decryption key, use the following command on a
control plane
node that is already joined to the cluster:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm init phase upload-certs --upload-certs
</span></span></code></pre></div></li><li><p>You can also specify a custom <code>--certificate-key</code> during <code>init</code> that can later be used by <code>join</code>.
To generate such a key you can use the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubeadm certs certificate-key
</span></span></code></pre></div></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>kubeadm-certs</code> Secret and decryption key expire after two hours.</div><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> As stated in the command output, the certificate key gives access to cluster sensitive data, keep it secret!</div></li><li><p>Apply the CNI plugin of your choice:
<a href=/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network>Follow these instructions</a>
to install the CNI provider. Make sure the configuration corresponds to the Pod CIDR specified in the
kubeadm configuration file (if applicable).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must pick a network plugin that suits your use case and deploy it before you move on to next step.
If you don't do this, you will not be able to launch your cluster properly.</div></li><li><p>Type the following and watch the pods of the control plane components get started:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl get pod -n kube-system -w
</span></span></code></pre></div></li></ol><h3 id=steps-for-the-rest-of-the-control-plane-nodes>Steps for the rest of the control plane nodes</h3><p>For each additional control plane node you should:</p><ol><li><p>Execute the join command that was previously given to you by the <code>kubeadm init</code> output on the first node.
It should look something like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866 --control-plane --certificate-key f8902e114ef118304e561c3ecd4d0b543adc226b7a07f675f56564185ffe0c07
</span></span></code></pre></div><ul><li>The <code>--control-plane</code> flag tells <code>kubeadm join</code> to create a new control plane.</li><li>The <code>--certificate-key ...</code> will cause the control plane certificates to be downloaded
from the <code>kubeadm-certs</code> Secret in the cluster and be decrypted using the given key.</li></ul></li></ol><p>You can join multiple control-plane nodes in parallel.</p><h2 id=external-etcd-nodes>External etcd nodes</h2><p>Setting up a cluster with external etcd nodes is similar to the procedure used for stacked etcd
with the exception that you should setup etcd first, and you should pass the etcd information
in the kubeadm config file.</p><h3 id=set-up-the-etcd-cluster>Set up the etcd cluster</h3><ol><li><p>Follow these <a href=/docs/setup/production-environment/tools/kubeadm/setup-ha-etcd-with-kubeadm/>instructions</a> to set up the etcd cluster.</p></li><li><p>Set up SSH as described <a href=#manual-certs>here</a>.</p></li><li><p>Copy the following files from any etcd node in the cluster to the first control plane node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#666>=</span><span style=color:#b44>&#34;ubuntu@10.0.0.7&#34;</span>
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/apiserver-etcd-client.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span><span style=display:flex><span>scp /etc/kubernetes/pki/apiserver-etcd-client.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>:
</span></span></code></pre></div><ul><li>Replace the value of <code>CONTROL_PLANE</code> with the <code>user@host</code> of the first control-plane node.</li></ul></li></ol><h3 id=set-up-the-first-control-plane-node>Set up the first control plane node</h3><ol><li><p>Create a file called <code>kubeadm-config.yaml</code> with the following contents:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kubernetesVersion</span>:<span style=color:#bbb> </span>stable<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controlPlaneEndpoint</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;LOAD_BALANCER_DNS:LOAD_BALANCER_PORT&#34;</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># change this (see below)</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>etcd</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>external</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>endpoints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- https://ETCD_0_IP:2379<span style=color:#bbb> </span><span style=color:#080;font-style:italic># change ETCD_0_IP appropriately</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- https://ETCD_1_IP:2379<span style=color:#bbb> </span><span style=color:#080;font-style:italic># change ETCD_1_IP appropriately</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- https://ETCD_2_IP:2379<span style=color:#bbb> </span><span style=color:#080;font-style:italic># change ETCD_2_IP appropriately</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caFile</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/etcd/ca.crt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>certFile</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/apiserver-etcd-client.crt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>keyFile</span>:<span style=color:#bbb> </span>/etc/kubernetes/pki/apiserver-etcd-client.key<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The difference between stacked etcd and external etcd here is that the external etcd setup requires
a configuration file with the etcd endpoints under the <code>external</code> object for <code>etcd</code>.
In the case of the stacked etcd topology, this is managed automatically.</div><ul><li><p>Replace the following variables in the config template with the appropriate values for your cluster:</p><ul><li><code>LOAD_BALANCER_DNS</code></li><li><code>LOAD_BALANCER_PORT</code></li><li><code>ETCD_0_IP</code></li><li><code>ETCD_1_IP</code></li><li><code>ETCD_2_IP</code></li></ul></li></ul></li></ol><p>The following steps are similar to the stacked etcd setup:</p><ol><li><p>Run <code>sudo kubeadm init --config kubeadm-config.yaml --upload-certs</code> on this node.</p></li><li><p>Write the output join commands that are returned to a text file for later use.</p></li><li><p>Apply the CNI plugin of your choice.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must pick a network plugin that suits your use case and deploy it before you move on to next step.
If you don't do this, you will not be able to launch your cluster properly.</div></li></ol><h3 id=steps-for-the-rest-of-the-control-plane-nodes-1>Steps for the rest of the control plane nodes</h3><p>The steps are the same as for the stacked etcd setup:</p><ul><li>Make sure the first control plane node is fully initialized.</li><li>Join each control plane node with the join command you saved to a text file. It's recommended
to join the control plane nodes one at a time.</li><li>Don't forget that the decryption key from <code>--certificate-key</code> expires after two hours, by default.</li></ul><h2 id=common-tasks-after-bootstrapping-control-plane>Common tasks after bootstrapping control plane</h2><h3 id=install-workers>Install workers</h3><p>Worker nodes can be joined to the cluster with the command you stored previously
as the output from the <code>kubeadm init</code> command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>sudo kubeadm join 192.168.0.200:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866
</span></span></code></pre></div><h2 id=manual-certs>Manual certificate distribution</h2><p>If you choose to not use <code>kubeadm init</code> with the <code>--upload-certs</code> flag this means that
you are going to have to manually copy the certificates from the primary control plane node to the
joining control plane nodes.</p><p>There are many ways to do this. The following example uses <code>ssh</code> and <code>scp</code>:</p><p>SSH is required if you want to control all nodes from a single machine.</p><ol><li><p>Enable ssh-agent on your main device that has access to all other nodes in
the system:</p><pre tabindex=0><code>eval $(ssh-agent)
</code></pre></li><li><p>Add your SSH identity to the session:</p><pre tabindex=0><code>ssh-add ~/.ssh/path_to_private_key
</code></pre></li><li><p>SSH between nodes to check that the connection is working correctly.</p><ul><li><p>When you SSH to any node, add the <code>-A</code> flag. This flag allows the node that you
have logged into via SSH to access the SSH agent on your PC. Consider alternative
methods if you do not fully trust the security of your user session on the node.</p><pre tabindex=0><code>ssh -A 10.0.0.7
</code></pre></li><li><p>When using sudo on any node, make sure to preserve the environment so SSH
forwarding works:</p><pre tabindex=0><code>sudo -E -s
</code></pre></li></ul></li><li><p>After configuring SSH on all the nodes you should run the following script on the first
control plane node after running <code>kubeadm init</code>. This script will copy the certificates from
the first control plane node to the other control plane nodes:</p><p>In the following example, replace <code>CONTROL_PLANE_IPS</code> with the IP addresses of the
other control plane nodes.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># customizable</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#666>=</span><span style=color:#b44>&#34;10.0.0.7 10.0.0.8&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> host in <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>CONTROL_PLANE_IPS</span><span style=color:#b68;font-weight:700>}</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/sa.pub <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/front-proxy-ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.crt
</span></span><span style=display:flex><span>    <span style=color:#080;font-style:italic># Skip the next line if you are using external etcd</span>
</span></span><span style=display:flex><span>    scp /etc/kubernetes/pki/etcd/ca.key <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>@<span style=color:#b8860b>$host</span>:etcd-ca.key
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Copy only the certificates in the above list. kubeadm will take care of generating the rest of the certificates
with the required SANs for the joining control-plane instances. If you copy all the certificates by mistake,
the creation of additional nodes could fail due to a lack of required SANs.</div></li><li><p>Then on each joining control plane node you have to run the following script before running <code>kubeadm join</code>.
This script will move the previously copied certificates from the home directory to <code>/etc/kubernetes/pki</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu <span style=color:#080;font-style:italic># customizable</span>
</span></span><span style=display:flex><span>mkdir -p /etc/kubernetes/pki/etcd
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.pub /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/sa.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.crt /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/front-proxy-ca.key /etc/kubernetes/pki/
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Skip the next line if you are using external etcd</span>
</span></span><span style=display:flex><span>mv /home/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key
</span></span></code></pre></div></li></ol></div><div class=td-content style=page-break-before:always><h1 id=pg-8160424c22d24f7d2d63c521e107dbf8>2.1.7 - Set up a High Availability etcd Cluster with kubeadm</h1><div class="alert alert-info note callout" role=alert><strong>Note:</strong> While kubeadm is being used as the management tool for external etcd nodes
in this guide, please note that kubeadm does not plan to support certificate rotation
or upgrades for such nodes. The long-term plan is to empower the tool
<a href=https://github.com/kubernetes-sigs/etcdadm>etcdadm</a> to manage these
aspects.</div><p>By default, kubeadm runs a local etcd instance on each control plane node.
It is also possible to treat the etcd cluster as external and provision
etcd instances on separate hosts. The differences between the two approaches are covered in the
<a href=/docs/setup/production-environment/tools/kubeadm/ha-topology>Options for Highly Available topology</a> page.</p><p>This task walks through the process of creating a high availability external
etcd cluster of three members that can be used by kubeadm during cluster creation.</p><h2 id=before-you-begin>Before you begin</h2><ul><li>Three hosts that can talk to each other over TCP ports 2379 and 2380. This
document assumes these default ports. However, they are configurable through
the kubeadm config file.</li><li>Each host must have systemd and a bash compatible shell installed.</li><li>Each host must <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>have a container runtime, kubelet, and kubeadm installed</a>.</li><li>Each host should have access to the Kubernetes container image registry (<code>registry.k8s.io</code>) or list/pull the required etcd image using
<code>kubeadm config images list/pull</code>. This guide will set up etcd instances as
<a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a> managed by a kubelet.</li><li>Some infrastructure to copy files between hosts. For example <code>ssh</code> and <code>scp</code>
can satisfy this requirement.</li></ul><h2 id=setting-up-the-cluster>Setting up the cluster</h2><p>The general approach is to generate all certs on one node and only distribute
the <em>necessary</em> files to the other nodes.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> kubeadm contains all the necessary cryptographic machinery to generate
the certificates described below; no other cryptographic tooling is required for
this example.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The examples below use IPv4 addresses but you can also configure kubeadm, the kubelet and etcd
to use IPv6 addresses. Dual-stack is supported by some Kubernetes options, but not by etcd. For more details
on Kubernetes dual-stack support see <a href=/docs/setup/production-environment/tools/kubeadm/dual-stack-support/>Dual-stack support with kubeadm</a>.</div><ol><li><p>Configure the kubelet to be a service manager for etcd.</p><p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must do this on every host where etcd should be running.</div>Since etcd was created first, you must override the service priority by creating a new unit file
that has higher precedence than the kubeadm-provided kubelet unit file.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf
</span></span></span><span style=display:flex><span><span style=color:#b44>[Service]
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=
</span></span></span><span style=display:flex><span><span style=color:#b44># Replace &#34;systemd&#34; with the cgroup driver of your container runtime. The default value in the kubelet is &#34;cgroupfs&#34;.
</span></span></span><span style=display:flex><span><span style=color:#b44># Replace the value of &#34;--container-runtime-endpoint&#34; for a different container runtime if needed.
</span></span></span><span style=display:flex><span><span style=color:#b44>ExecStart=/usr/bin/kubelet --address=127.0.0.1 --pod-manifest-path=/etc/kubernetes/manifests --cgroup-driver=systemd --container-runtime=remote --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock
</span></span></span><span style=display:flex><span><span style=color:#b44>Restart=always
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart kubelet
</span></span></code></pre></div><p>Check the kubelet status to ensure it is running.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>systemctl status kubelet
</span></span></code></pre></div></li><li><p>Create configuration files for kubeadm.</p><p>Generate one kubeadm configuration file for each host that will have an etcd
member running on it using the following script.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#080;font-style:italic># Update HOST0, HOST1 and HOST2 with the IPs of your hosts</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST0</span><span style=color:#666>=</span>10.0.0.6
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST1</span><span style=color:#666>=</span>10.0.0.7
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>HOST2</span><span style=color:#666>=</span>10.0.0.8
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Update NAME0, NAME1 and NAME2 with the hostnames of your hosts</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>NAME0</span><span style=color:#666>=</span><span style=color:#b44>&#34;infra0&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>NAME1</span><span style=color:#666>=</span><span style=color:#b44>&#34;infra1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a2f>export</span> <span style=color:#b8860b>NAME2</span><span style=color:#666>=</span><span style=color:#b44>&#34;infra2&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Create temp directories to store files that will end up on other hosts</span>
</span></span><span style=display:flex><span>mkdir -p /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/ /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#b8860b>HOSTS</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAMES</span><span style=color:#666>=(</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAME0</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAME1</span><span style=color:#b68;font-weight:700>}</span> <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAME2</span><span style=color:#b68;font-weight:700>}</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>${</span>!HOSTS[@]<span style=color:#b68;font-weight:700>}</span><span style=color:#b44>&#34;</span>; <span style=color:#a2f;font-weight:700>do</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOSTS</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>NAME</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>NAMES</span>[<span style=color:#b8860b>$i</span>]<span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt; EOF &gt; /tmp/${HOST}/kubeadmcfg.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>---
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta3&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: InitConfiguration
</span></span></span><span style=display:flex><span><span style=color:#b44>nodeRegistration:
</span></span></span><span style=display:flex><span><span style=color:#b44>    name: ${NAME}
</span></span></span><span style=display:flex><span><span style=color:#b44>localAPIEndpoint:
</span></span></span><span style=display:flex><span><span style=color:#b44>    advertiseAddress: ${HOST}
</span></span></span><span style=display:flex><span><span style=color:#b44>---
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: &#34;kubeadm.k8s.io/v1beta3&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: ClusterConfiguration
</span></span></span><span style=display:flex><span><span style=color:#b44>etcd:
</span></span></span><span style=display:flex><span><span style=color:#b44>    local:
</span></span></span><span style=display:flex><span><span style=color:#b44>        serverCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        peerCertSANs:
</span></span></span><span style=display:flex><span><span style=color:#b44>        - &#34;${HOST}&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>        extraArgs:
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster: ${NAMES[0]}=https://${HOSTS[0]}:2380,${NAMES[1]}=https://${HOSTS[1]}:2380,${NAMES[2]}=https://${HOSTS[2]}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-cluster-state: new
</span></span></span><span style=display:flex><span><span style=color:#b44>            name: ${NAME}
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>            listen-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            advertise-client-urls: https://${HOST}:2379
</span></span></span><span style=display:flex><span><span style=color:#b44>            initial-advertise-peer-urls: https://${HOST}:2380
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>done</span>
</span></span></code></pre></div></li><li><p>Generate the certificate authority.</p><p>If you already have a CA then the only action that is copying the CA's <code>crt</code> and
<code>key</code> file to <code>/etc/kubernetes/pki/etcd/ca.crt</code> and
<code>/etc/kubernetes/pki/etcd/ca.key</code>. After those files have been copied,
proceed to the next step, "Create certificates for each member".</p><p>If you do not already have a CA then run this command on <code>$HOST0</code> (where you
generated the configuration files for kubeadm).</p><pre tabindex=0><code>kubeadm init phase certs etcd-ca
</code></pre><p>This creates two files:</p><ul><li><code>/etc/kubernetes/pki/etcd/ca.crt</code></li><li><code>/etc/kubernetes/pki/etcd/ca.key</code></li></ul></li><li><p>Create certificates for each member.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># cleanup non-reusable certificates</span>
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>cp -R /etc/kubernetes/pki /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>/
</span></span><span style=display:flex><span>find /etc/kubernetes/pki -not -name ca.crt -not -name ca.key -type f -delete
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-server --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-peer --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs etcd-healthcheck-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>kubeadm init phase certs apiserver-etcd-client --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># No need to move the certs because they are for HOST0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># clean up certs that should not be copied off this host</span>
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST2</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span><span style=display:flex><span>find /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span> -name ca.key -type f -delete
</span></span></code></pre></div></li><li><p>Copy certificates and kubeadm configs.</p><p>The certificates have been generated and now they must be moved to their
respective hosts.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span><span style=color:#b8860b>USER</span><span style=color:#666>=</span>ubuntu
</span></span><span style=display:flex><span><span style=color:#b8860b>HOST</span><span style=color:#666>=</span><span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST1</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>scp -r /tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>/* <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>:
</span></span><span style=display:flex><span>ssh <span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>USER</span><span style=color:#b68;font-weight:700>}</span>@<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST</span><span style=color:#b68;font-weight:700>}</span>
</span></span><span style=display:flex><span>USER@HOST $ sudo -Es
</span></span><span style=display:flex><span>root@HOST $ chown -R root:root pki
</span></span><span style=display:flex><span>root@HOST $ mv pki /etc/kubernetes/
</span></span></code></pre></div></li><li><p>Ensure all expected files exist.</p><p>The complete list of required files on <code>$HOST0</code> is:</p><pre tabindex=0><code>/tmp/${HOST0}
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── ca.key
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>On <code>$HOST1</code>:</p><pre tabindex=0><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre><p>On <code>$HOST2</code>:</p><pre tabindex=0><code>$HOME
└── kubeadmcfg.yaml
---
/etc/kubernetes/pki
├── apiserver-etcd-client.crt
├── apiserver-etcd-client.key
└── etcd
    ├── ca.crt
    ├── healthcheck-client.crt
    ├── healthcheck-client.key
    ├── peer.crt
    ├── peer.key
    ├── server.crt
    └── server.key
</code></pre></li><li><p>Create the static pod manifests.</p><p>Now that the certificates and configs are in place it's time to create the
manifests. On each host run the <code>kubeadm</code> command to generate a static manifest
for etcd.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>root@HOST0 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span>/tmp/<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>root@HOST1 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span><span style=display:flex><span>root@HOST2 $ kubeadm init phase etcd <span style=color:#a2f>local</span> --config<span style=color:#666>=</span><span style=color:#b8860b>$HOME</span>/kubeadmcfg.yaml
</span></span></code></pre></div></li><li><p>Optional: Check the cluster health.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>docker run --rm -it <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--net host <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>-v /etc/kubernetes:/etc/kubernetes registry.k8s.io/etcd:<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>ETCD_TAG</span><span style=color:#b68;font-weight:700>}</span> etcdctl <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--cert /etc/kubernetes/pki/etcd/peer.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--key /etc/kubernetes/pki/etcd/peer.key <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--cacert /etc/kubernetes/pki/etcd/ca.crt <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--endpoints https://<span style=color:#b68;font-weight:700>${</span><span style=color:#b8860b>HOST0</span><span style=color:#b68;font-weight:700>}</span>:2379 endpoint health --cluster
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST0 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 16.283339ms
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST1 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 19.44402ms
</span></span><span style=display:flex><span>https://<span style=color:#666>[</span>HOST2 IP<span style=color:#666>]</span>:2379 is healthy: successfully committed proposal: <span style=color:#b8860b>took</span> <span style=color:#666>=</span> 35.926451ms
</span></span></code></pre></div><ul><li>Set <code>${ETCD_TAG}</code> to the version tag of your etcd image. For example <code>3.4.3-0</code>. To see the etcd image and tag that kubeadm uses execute <code>kubeadm config images list --kubernetes-version ${K8S_VERSION}</code>, where <code>${K8S_VERSION}</code> is for example <code>v1.17.0</code>.</li><li>Set <code>${HOST0}</code>to the IP address of the host you are testing.</li></ul></li></ol><h2 id=what-s-next>What's next</h2><p>Once you have an etcd cluster with 3 working members, you can continue setting up a
highly available control plane using the
<a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>external etcd method with kubeadm</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-07709e71de6b4ac2573041c31213dbeb>2.1.8 - Configuring each kubelet in your cluster using kubeadm</h1><div class="alert alert-secondary callout note" role=alert><strong>Note:</strong> Dockershim has been removed from the Kubernetes project as of release 1.24. Read the <a href=/dockershim>Dockershim Removal FAQ</a> for further details.</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.11 [stable]</code></div><p>The lifecycle of the kubeadm CLI tool is decoupled from the
<a href=/docs/reference/command-line-tools-reference/kubelet>kubelet</a>, which is a daemon that runs
on each node within the Kubernetes cluster. The kubeadm CLI tool is executed by the user when Kubernetes is
initialized or upgraded, whereas the kubelet is always running in the background.</p><p>Since the kubelet is a daemon, it needs to be maintained by some kind of an init
system or service manager. When the kubelet is installed using DEBs or RPMs,
systemd is configured to manage the kubelet. You can use a different service
manager instead, but you need to configure it manually.</p><p>Some kubelet configuration details need to be the same across all kubelets involved in the cluster, while
other configuration aspects need to be set on a per-kubelet basis to accommodate the different
characteristics of a given machine (such as OS, storage, and networking). You can manage the configuration
of your kubelets manually, but kubeadm now provides a <code>KubeletConfiguration</code> API type for
<a href=#configure-kubelets-using-kubeadm>managing your kubelet configurations centrally</a>.</p><h2 id=kubelet-configuration-patterns>Kubelet configuration patterns</h2><p>The following sections describe patterns to kubelet configuration that are simplified by
using kubeadm, rather than managing the kubelet configuration for each Node manually.</p><h3 id=propagating-cluster-level-configuration-to-each-kubelet>Propagating cluster-level configuration to each kubelet</h3><p>You can provide the kubelet with default values to be used by <code>kubeadm init</code> and <code>kubeadm join</code>
commands. Interesting examples include using a different container runtime or setting the default subnet
used by services.</p><p>If you want your services to use the subnet <code>10.96.0.0/12</code> as the default for services, you can pass
the <code>--service-cidr</code> parameter to kubeadm:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubeadm init --service-cidr 10.96.0.0/12
</span></span></code></pre></div><p>Virtual IPs for services are now allocated from this subnet. You also need to set the DNS address used
by the kubelet, using the <code>--cluster-dns</code> flag. This setting needs to be the same for every kubelet
on every manager and Node in the cluster. The kubelet provides a versioned, structured API object
that can configure most parameters in the kubelet and push out this configuration to each running
kubelet in the cluster. This object is called
<a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>.
The <code>KubeletConfiguration</code> allows the user to specify flags such as the cluster DNS IP addresses expressed as
a list of values to a camelCased key, illustrated by the following example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusterDNS</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:#666>10.96.0.10</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>For more details on the <code>KubeletConfiguration</code> have a look at <a href=#configure-kubelets-using-kubeadm>this section</a>.</p><h3 id=providing-instance-specific-configuration-details>Providing instance-specific configuration details</h3><p>Some hosts require specific kubelet configurations due to differences in hardware, operating system,
networking, or other host-specific parameters. The following list provides a few examples.</p><ul><li><p>The path to the DNS resolution file, as specified by the <code>--resolv-conf</code> kubelet
configuration flag, may differ among operating systems, or depending on whether you are using
<code>systemd-resolved</code>. If this path is wrong, DNS resolution will fail on the Node whose kubelet
is configured incorrectly.</p></li><li><p>The Node API object <code>.metadata.name</code> is set to the machine's hostname by default,
unless you are using a cloud provider. You can use the <code>--hostname-override</code> flag to override the
default behavior if you need to specify a Node name different from the machine's hostname.</p></li><li><p>Currently, the kubelet cannot automatically detect the cgroup driver used by the container runtime,
but the value of <code>--cgroup-driver</code> must match the cgroup driver used by the container runtime to ensure
the health of the kubelet.</p></li><li><p>To specify the container runtime you must set its endpoint with the
<code>--container-runtime-endpoint=&lt;path></code> flag.</p></li></ul><p>The recommended way of applying such instance-specific configuration is by using
<a href=/docs/setup/production-environment/tools/kubeadm/control-plane-flags#patches><code>KubeletConfiguration</code> patches</a>.</p><h2 id=configure-kubelets-using-kubeadm>Configure kubelets using kubeadm</h2><p>It is possible to configure the kubelet that kubeadm will start if a custom
<a href=/docs/reference/config-api/kubelet-config.v1beta1/><code>KubeletConfiguration</code></a>
API object is passed with a configuration file like so <code>kubeadm ... --config some-config-file.yaml</code>.</p><p>By calling <code>kubeadm config print init-defaults --component-configs KubeletConfiguration</code> you can
see all the default values for this structure.</p><p>It is also possible to apply instance-specific patches over the base <code>KubeletConfiguration</code>.
Have a look at <a href=/docs/setup/production-environment/tools/kubeadm/control-plane-flags#customizing-the-kubelet>Customizing the kubelet</a>
for more details.</p><h3 id=workflow-when-using-kubeadm-init>Workflow when using <code>kubeadm init</code></h3><p>When you call <code>kubeadm init</code>, the kubelet configuration is marshalled to disk
at <code>/var/lib/kubelet/config.yaml</code>, and also uploaded to a <code>kubelet-config</code> ConfigMap in the <code>kube-system</code>
namespace of the cluster. A kubelet configuration file is also written to <code>/etc/kubernetes/kubelet.conf</code>
with the baseline cluster-wide configuration for all kubelets in the cluster. This configuration file
points to the client certificates that allow the kubelet to communicate with the API server. This
addresses the need to
<a href=#propagating-cluster-level-configuration-to-each-kubelet>propagate cluster-level configuration to each kubelet</a>.</p><p>To address the second pattern of
<a href=#providing-instance-specific-configuration-details>providing instance-specific configuration details</a>,
kubeadm writes an environment file to <code>/var/lib/kubelet/kubeadm-flags.env</code>, which contains a list of
flags to pass to the kubelet when it starts. The flags are presented in the file like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#b8860b>KUBELET_KUBEADM_ARGS</span><span style=color:#666>=</span><span style=color:#b44>&#34;--flag1=value1 --flag2=value2 ...&#34;</span>
</span></span></code></pre></div><p>In addition to the flags used when starting the kubelet, the file also contains dynamic
parameters such as the cgroup driver and whether to use a different container runtime socket
(<code>--cri-socket</code>).</p><p>After marshalling these two files to disk, kubeadm attempts to run the following two
commands, if you are using systemd:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>If the reload and restart are successful, the normal <code>kubeadm init</code> workflow continues.</p><h3 id=workflow-when-using-kubeadm-join>Workflow when using <code>kubeadm join</code></h3><p>When you run <code>kubeadm join</code>, kubeadm uses the Bootstrap Token credential to perform
a TLS bootstrap, which fetches the credential needed to download the
<code>kubelet-config</code> ConfigMap and writes it to <code>/var/lib/kubelet/config.yaml</code>. The dynamic
environment file is generated in exactly the same way as <code>kubeadm init</code>.</p><p>Next, <code>kubeadm</code> runs the following two commands to load the new configuration into the kubelet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>systemctl daemon-reload <span style=color:#666>&amp;&amp;</span> systemctl restart kubelet
</span></span></code></pre></div><p>After the kubelet loads the new configuration, kubeadm writes the
<code>/etc/kubernetes/bootstrap-kubelet.conf</code> KubeConfig file, which contains a CA certificate and Bootstrap
Token. These are used by the kubelet to perform the TLS Bootstrap and obtain a unique
credential, which is stored in <code>/etc/kubernetes/kubelet.conf</code>.</p><p>When the <code>/etc/kubernetes/kubelet.conf</code> file is written, the kubelet has finished performing the TLS Bootstrap.
Kubeadm deletes the <code>/etc/kubernetes/bootstrap-kubelet.conf</code> file after completing the TLS Bootstrap.</p><h2 id=the-kubelet-drop-in-file-for-systemd>The kubelet drop-in file for systemd</h2><p><code>kubeadm</code> ships with configuration for how systemd should run the kubelet.
Note that the kubeadm CLI command never touches this drop-in file.</p><p>This configuration file installed by the <code>kubeadm</code>
<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf>DEB</a> or
<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubeadm/10-kubeadm.conf>RPM package</a> is written to
<code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code> and is used by systemd.
It augments the basic
<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/rpm/kubelet/kubelet.service><code>kubelet.service</code> for RPM</a> or
<a href=https://github.com/kubernetes/release/blob/master/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service><code>kubelet.service</code> for DEB</a>:</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The contents below are just an example. If you don't want to use a package manager
follow the guide outlined in the <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#k8s-install-2>Without a package manager</a>)
section.</div><pre tabindex=0><code class=language-none data-lang=none>[Service]
Environment=&#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&#34;
Environment=&#34;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml&#34;
# This is a file that &#34;kubeadm init&#34; and &#34;kubeadm join&#34; generate at runtime, populating
# the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably,
# the user should use the .NodeRegistration.KubeletExtraArgs object in the configuration files instead.
# KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS
</code></pre><p>This file specifies the default locations for all of the files managed by kubeadm for the kubelet.</p><ul><li>The KubeConfig file to use for the TLS Bootstrap is <code>/etc/kubernetes/bootstrap-kubelet.conf</code>,
but it is only used if <code>/etc/kubernetes/kubelet.conf</code> does not exist.</li><li>The KubeConfig file with the unique kubelet identity is <code>/etc/kubernetes/kubelet.conf</code>.</li><li>The file containing the kubelet's ComponentConfig is <code>/var/lib/kubelet/config.yaml</code>.</li><li>The dynamic environment file that contains <code>KUBELET_KUBEADM_ARGS</code> is sourced from <code>/var/lib/kubelet/kubeadm-flags.env</code>.</li><li>The file that can contain user-specified flag overrides with <code>KUBELET_EXTRA_ARGS</code> is sourced from
<code>/etc/default/kubelet</code> (for DEBs), or <code>/etc/sysconfig/kubelet</code> (for RPMs). <code>KUBELET_EXTRA_ARGS</code>
is last in the flag chain and has the highest priority in the event of conflicting settings.</li></ul><h2 id=kubernetes-binaries-and-package-contents>Kubernetes binaries and package contents</h2><p>The DEB and RPM packages shipped with the Kubernetes releases are:</p><table><thead><tr><th>Package name</th><th>Description</th></tr></thead><tbody><tr><td><code>kubeadm</code></td><td>Installs the <code>/usr/bin/kubeadm</code> CLI tool and the <a href=#the-kubelet-drop-in-file-for-systemd>kubelet drop-in file</a> for the kubelet.</td></tr><tr><td><code>kubelet</code></td><td>Installs the <code>/usr/bin/kubelet</code> binary.</td></tr><tr><td><code>kubectl</code></td><td>Installs the <code>/usr/bin/kubectl</code> binary.</td></tr><tr><td><code>cri-tools</code></td><td>Installs the <code>/usr/bin/crictl</code> binary from the <a href=https://github.com/kubernetes-sigs/cri-tools>cri-tools git repository</a>.</td></tr><tr><td><code>kubernetes-cni</code></td><td>Installs the <code>/opt/cni/bin</code> binaries from the <a href=https://github.com/containernetworking/plugins>plugins git repository</a>.</td></tr></tbody></table></div><div class=td-content style=page-break-before:always><h1 id=pg-df2f3f20d404ebe2b03fcda1fcee50e7>2.1.9 - Dual-stack support with kubeadm</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code></div><p>Your Kubernetes cluster includes <a href=/docs/concepts/services-networking/dual-stack/>dual-stack</a> networking, which means that cluster networking lets you use either address family. In a cluster, the control plane can assign both an IPv4 address and an IPv6 address to a single <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> or a <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>.</p><h2 id=before-you-begin>Before you begin</h2><p>You need to have installed the <a class=glossary-tooltip title='A tool for quickly installing Kubernetes and setting up a secure cluster.' data-toggle=tooltip data-placement=top href=/docs/admin/kubeadm/ target=_blank aria-label=kubeadm>kubeadm</a> tool, following the steps from <a href=/docs/setup/production-environment/tools/kubeadm/install-kubeadm/>Installing kubeadm</a>.</p><p>For each server that you want to use as a <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a>, make sure it allows IPv6 forwarding. On Linux, you can set this by running run <code>sysctl -w net.ipv6.conf.all.forwarding=1</code> as the root user on each server.</p><p>You need to have an IPv4 and and IPv6 address range to use. Cluster operators typically
use private address ranges for IPv4. For IPv6, a cluster operator typically chooses a global
unicast address block from within <code>2000::/3</code>, using a range that is assigned to the operator.
You don't have to route the cluster's IP address ranges to the public internet.</p><p>The size of the IP address allocations should be suitable for the number of Pods and
Services that you are planning to run.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you are upgrading an existing cluster with the <code>kubeadm upgrade</code> command,
<code>kubeadm</code> does not support making modifications to the pod IP address range
(“cluster CIDR”) nor to the cluster's Service address range (“Service CIDR”).</div><h3 id=create-a-dual-stack-cluster>Create a dual-stack cluster</h3><p>To create a dual-stack cluster with <code>kubeadm init</code> you can pass command line arguments
similar to the following example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># These address ranges are examples</span>
</span></span><span style=display:flex><span>kubeadm init --pod-network-cidr<span style=color:#666>=</span>10.244.0.0/16,2001:db8:42:0::/56 --service-cidr<span style=color:#666>=</span>10.96.0.0/16,2001:db8:42:1::/112
</span></span></code></pre></div><p>To make things clearer, here is an example kubeadm
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/>configuration file</a>
<code>kubeadm-config.yaml</code> for the primary dual-stack control plane node.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>networking</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.244.0.0</span>/16,2001:db8:42:0::/56<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.96.0.0</span>/16,2001:db8:42:1::/112<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>InitConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>localAPIEndpoint</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>advertiseAddress</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10.100.0.1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bindPort</span>:<span style=color:#bbb> </span><span style=color:#666>6443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node-ip</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.2</span>,fd00:1:2:3::2<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>advertiseAddress</code> in InitConfiguration specifies the IP address that the API Server will advertise it is listening on. The value of <code>advertiseAddress</code> equals the <code>--apiserver-advertise-address</code> flag of <code>kubeadm init</code></p><p>Run kubeadm to initiate the dual-stack control plane node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubeadm init --config<span style=color:#666>=</span>kubeadm-config.yaml
</span></span></code></pre></div><p>The kube-controller-manager flags <code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code> are set with default values. See <a href=/docs/concepts/services-networking/dual-stack#configure-ipv4-ipv6-dual-stack>configure IPv4/IPv6 dual stack</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>--apiserver-advertise-address</code> flag does not support dual-stack.</div><h3 id=join-a-node-to-dual-stack-cluster>Join a node to dual-stack cluster</h3><p>Before joining a node, make sure that the node has IPv6 routable network interface and allows IPv6 forwarding.</p><p>Here is an example kubeadm <a href=/docs/reference/config-api/kubeadm-config.v1beta3/>configuration file</a>
<code>kubeadm-config.yaml</code> for joining a worker node to the cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>discovery</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bootstrapToken</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiServerEndpoint</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.1</span>:<span style=color:#666>6443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>token</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;clvldh.vjjwg16ucnhp94qr&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caCertHashes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># change auth info above to match the actual token and CA certificate hash for your cluster</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node-ip</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.3</span>,fd00:1:2:3::3<span style=color:#bbb>
</span></span></span></code></pre></div><p>Also, here is an example kubeadm <a href=/docs/reference/config-api/kubeadm-config.v1beta3/>configuration file</a>
<code>kubeadm-config.yaml</code> for joining another control plane node to the cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>JoinConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>controlPlane</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>localAPIEndpoint</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>advertiseAddress</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10.100.0.2&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>bindPort</span>:<span style=color:#bbb> </span><span style=color:#666>6443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>discovery</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>bootstrapToken</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiServerEndpoint</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.1</span>:<span style=color:#666>6443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>token</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;clvldh.vjjwg16ucnhp94qr&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>caCertHashes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;sha256:a4863cde706cfc580a439f842cc65d5ef112b7b2be31628513a9881cf0d9fe0e&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># change auth info above to match the actual token and CA certificate hash for your cluster</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>nodeRegistration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kubeletExtraArgs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node-ip</span>:<span style=color:#bbb> </span><span style=color:#666>10.100.0.4</span>,fd00:1:2:3::4<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>advertiseAddress</code> in JoinConfiguration.controlPlane specifies the IP address that the API Server will advertise it is listening on. The value of <code>advertiseAddress</code> equals the <code>--apiserver-advertise-address</code> flag of <code>kubeadm join</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubeadm join --config<span style=color:#666>=</span>kubeadm-config.yaml
</span></span></code></pre></div><h3 id=create-a-single-stack-cluster>Create a single-stack cluster</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Dual-stack support doesn't mean that you need to use dual-stack addressing.
You can deploy a single-stack cluster that has the dual-stack networking feature enabled.</div><p>To make things more clear, here is an example kubeadm
<a href=/docs/reference/config-api/kubeadm-config.v1beta3/>configuration file</a>
<code>kubeadm-config.yaml</code> for the single-stack control plane node.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubeadm.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>networking</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.244.0.0</span>/16<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceSubnet</span>:<span style=color:#bbb> </span><span style=color:#666>10.96.0.0</span>/16<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/tasks/network/validate-dual-stack>Validate IPv4/IPv6 dual-stack</a> networking</li><li>Read about <a href=/docs/concepts/services-networking/dual-stack/>Dual-stack</a> cluster networking</li><li>Learn more about the kubeadm <a href=/docs/reference/config-api/kubeadm-config.v1beta3/>configuration format</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-478acca1934b6d89a0bc00fb25bfe5b6>2.2 - Installing Kubernetes with kOps</h1><p>This quickstart shows you how to easily install a Kubernetes cluster on AWS.
It uses a tool called <a href=https://github.com/kubernetes/kops><code>kOps</code></a>.</p><p><code>kOps</code> is an automated provisioning system:</p><ul><li>Fully automated installation</li><li>Uses DNS to identify clusters</li><li>Self-healing: everything runs in Auto-Scaling Groups</li><li>Multiple OS support (Amazon Linux, Debian, Flatcar, RHEL, Rocky and Ubuntu) - see the <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/images.md>images.md</a></li><li>High-Availability support - see the <a href=https://github.com/kubernetes/kops/blob/master/docs/operations/high_availability.md>high_availability.md</a></li><li>Can directly provision, or generate terraform manifests - see the <a href=https://github.com/kubernetes/kops/blob/master/docs/terraform.md>terraform.md</a></li></ul><h2 id=before-you-begin>Before you begin</h2><ul><li><p>You must have <a href=/docs/tasks/tools/>kubectl</a> installed.</p></li><li><p>You must <a href=https://github.com/kubernetes/kops#installing>install</a> <code>kops</code> on a 64-bit (AMD64 and Intel 64) device architecture.</p></li><li><p>You must have an <a href=https://docs.aws.amazon.com/polly/latest/dg/setting-up.html>AWS account</a>, generate <a href=https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#access-keys-and-secret-access-keys>IAM keys</a> and <a href=https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html#cli-quick-configuration>configure</a> them. The IAM user will need <a href=https://github.com/kubernetes/kops/blob/master/docs/getting_started/aws.md#setup-iam-user>adequate permissions</a>.</p></li></ul><h2 id=creating-a-cluster>Creating a cluster</h2><h3 id=1-5-install-kops>(1/5) Install kops</h3><h4 id=installation>Installation</h4><p>Download kops from the <a href=https://github.com/kubernetes/kops/releases>releases page</a> (it is also convenient to build from source):</p><ul class="nav nav-tabs" id=kops-installation role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#kops-installation-0 role=tab aria-controls=kops-installation-0 aria-selected=true>macOS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#kops-installation-1 role=tab aria-controls=kops-installation-1>Linux</a></li></ul><div class=tab-content id=kops-installation><div id=kops-installation-0 class="tab-pane show active" role=tabpanel aria-labelledby=kops-installation-0><p><p>Download the latest release with the command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-darwin-amd64
</span></span></code></pre></div><p>To download a specific version, replace the following portion of the command with the specific kops version.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</span></span></code></pre></div><p>For example, to download kops version v1.20.0 type:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-darwin-amd64
</span></span></code></pre></div><p>Make the kops binary executable.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>chmod +x kops-darwin-amd64
</span></span></code></pre></div><p>Move the kops binary in to your PATH.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo mv kops-darwin-amd64 /usr/local/bin/kops
</span></span></code></pre></div><p>You can also install kops using <a href=https://brew.sh/>Homebrew</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</span></span></code></pre></div></div><div id=kops-installation-1 class=tab-pane role=tabpanel aria-labelledby=kops-installation-1><p><p>Download the latest release with the command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/<span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>/kops-linux-amd64
</span></span></code></pre></div><p>To download a specific version of kops, replace the following portion of the command with the specific kops version.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>$(</span>curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d <span style=color:#b44>&#39;&#34;&#39;</span> -f 4<span style=color:#a2f;font-weight:700>)</span>
</span></span></code></pre></div><p>For example, to download kops version v1.20.0 type:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -LO https://github.com/kubernetes/kops/releases/download/v1.20.0/kops-linux-amd64
</span></span></code></pre></div><p>Make the kops binary executable</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>chmod +x kops-linux-amd64
</span></span></code></pre></div><p>Move the kops binary in to your PATH.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo mv kops-linux-amd64 /usr/local/bin/kops
</span></span></code></pre></div><p>You can also install kops using <a href=https://docs.brew.sh/Homebrew-on-Linux>Homebrew</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>brew update <span style=color:#666>&amp;&amp;</span> brew install kops
</span></span></code></pre></div></div></div><h3 id=2-5-create-a-route53-domain-for-your-cluster>(2/5) Create a route53 domain for your cluster</h3><p>kops uses DNS for discovery, both inside the cluster and outside, so that you can reach the kubernetes API server
from clients.</p><p>kops has a strong opinion on the cluster name: it should be a valid DNS name. By doing so you will
no longer get your clusters confused, you can share clusters with your colleagues unambiguously,
and you can reach them without relying on remembering an IP address.</p><p>You can, and probably should, use subdomains to divide your clusters. As our example we will use
<code>useast1.dev.example.com</code>. The API server endpoint will then be <code>api.useast1.dev.example.com</code>.</p><p>A Route53 hosted zone can serve subdomains. Your hosted zone could be <code>useast1.dev.example.com</code>,
but also <code>dev.example.com</code> or even <code>example.com</code>. kops works with any of these, so typically
you choose for organization reasons (e.g. you are allowed to create records under <code>dev.example.com</code>,
but not under <code>example.com</code>).</p><p>Let's assume you're using <code>dev.example.com</code> as your hosted zone. You create that hosted zone using
the <a href=https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/CreatingNewSubdomain.html>normal process</a>, or
with a command such as <code>aws route53 create-hosted-zone --name dev.example.com --caller-reference 1</code>.</p><p>You must then set up your NS records in the parent domain, so that records in the domain will resolve. Here,
you would create NS records in <code>example.com</code> for <code>dev</code>. If it is a root domain name you would configure the NS
records at your domain registrar (e.g. <code>example.com</code> would need to be configured where you bought <code>example.com</code>).</p><p>Verify your route53 domain setup (it is the #1 cause of problems!). You can double-check that
your cluster is configured correctly if you have the dig tool by running:</p><p><code>dig NS dev.example.com</code></p><p>You should see the 4 NS records that Route53 assigned your hosted zone.</p><h3 id=3-5-create-an-s3-bucket-to-store-your-clusters-state>(3/5) Create an S3 bucket to store your clusters state</h3><p>kops lets you manage your clusters even after installation. To do this, it must keep track of the clusters
that you have created, along with their configuration, the keys they are using etc. This information is stored
in an S3 bucket. S3 permissions are used to control access to the bucket.</p><p>Multiple clusters can use the same S3 bucket, and you can share an S3 bucket between your colleagues that
administer the same clusters - this is much easier than passing around kubecfg files. But anyone with access
to the S3 bucket will have administrative access to all your clusters, so you don't want to share it beyond
the operations team.</p><p>So typically you have one S3 bucket for each ops team (and often the name will correspond
to the name of the hosted zone above!)</p><p>In our example, we chose <code>dev.example.com</code> as our hosted zone, so let's pick <code>clusters.dev.example.com</code> as
the S3 bucket name.</p><ul><li><p>Export <code>AWS_PROFILE</code> (if you need to select a profile for the AWS CLI to work)</p></li><li><p>Create the S3 bucket using <code>aws s3 mb s3://clusters.dev.example.com</code></p></li><li><p>You can <code>export KOPS_STATE_STORE=s3://clusters.dev.example.com</code> and then kops will use this location by default.
We suggest putting this in your bash profile or similar.</p></li></ul><h3 id=4-5-build-your-cluster-configuration>(4/5) Build your cluster configuration</h3><p>Run <code>kops create cluster</code> to create your cluster configuration:</p><p><code>kops create cluster --zones=us-east-1c useast1.dev.example.com</code></p><p>kops will create the configuration for your cluster. Note that it <em>only</em> creates the configuration, it does
not actually create the cloud resources - you'll do that in the next step with a <code>kops update cluster</code>. This
give you an opportunity to review the configuration or change it.</p><p>It prints commands you can use to explore further:</p><ul><li>List your clusters with: <code>kops get cluster</code></li><li>Edit this cluster with: <code>kops edit cluster useast1.dev.example.com</code></li><li>Edit your node instance group: <code>kops edit ig --name=useast1.dev.example.com nodes</code></li><li>Edit your master instance group: <code>kops edit ig --name=useast1.dev.example.com master-us-east-1c</code></li></ul><p>If this is your first time using kops, do spend a few minutes to try those out! An instance group is a
set of instances, which will be registered as kubernetes nodes. On AWS this is implemented via auto-scaling-groups.
You can have several instance groups, for example if you wanted nodes that are a mix of spot and on-demand instances, or
GPU and non-GPU instances.</p><h3 id=5-5-create-the-cluster-in-aws>(5/5) Create the cluster in AWS</h3><p>Run <code>kops update cluster</code> to create your cluster in AWS:</p><p><code>kops update cluster useast1.dev.example.com --yes</code></p><p>That takes a few seconds to run, but then your cluster will likely take a few minutes to actually be ready.
<code>kops update cluster</code> will be the tool you'll use whenever you change the configuration of your cluster; it
applies the changes you have made to the configuration to your cluster - reconfiguring AWS or kubernetes as needed.</p><p>For example, after you <code>kops edit ig nodes</code>, then <code>kops update cluster --yes</code> to apply your configuration, and
sometimes you will also have to <code>kops rolling-update cluster</code> to roll out the configuration immediately.</p><p>Without <code>--yes</code>, <code>kops update cluster</code> will show you a preview of what it is going to do. This is handy
for production clusters!</p><h3 id=explore-other-add-ons>Explore other add-ons</h3><p>See the <a href=/docs/concepts/cluster-administration/addons/>list of add-ons</a> to explore other add-ons, including tools for logging, monitoring, network policy, visualization, and control of your Kubernetes cluster.</p><h2 id=cleanup>Cleanup</h2><ul><li>To delete your cluster: <code>kops delete cluster useast1.dev.example.com --yes</code></li></ul><h2 id=what-s-next>What's next</h2><ul><li>Learn more about Kubernetes <a href=/docs/concepts/>concepts</a> and <a href=/docs/reference/kubectl/><code>kubectl</code></a>.</li><li>Learn more about <code>kOps</code> <a href=https://kops.sigs.k8s.io/>advanced usage</a> for tutorials, best practices and advanced configuration options.</li><li>Follow <code>kOps</code> community discussions on Slack: <a href=https://github.com/kubernetes/kops#other-ways-to-communicate-with-the-contributors>community discussions</a>.</li><li>Contribute to <code>kOps</code> by addressing or raising an issue <a href=https://github.com/kubernetes/kops/issues>GitHub Issues</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f8b4964187fe973644e06ee629eff1de>2.3 - Installing Kubernetes with Kubespray</h1><p>This quickstart helps to install a Kubernetes cluster hosted on GCE, Azure, OpenStack, AWS, vSphere, Equinix Metal (formerly Packet), Oracle Cloud Infrastructure (Experimental) or Baremetal with <a href=https://github.com/kubernetes-sigs/kubespray>Kubespray</a>.</p><p>Kubespray is a composition of <a href=https://docs.ansible.com/>Ansible</a> playbooks, <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/ansible.md#inventory>inventory</a>, provisioning tools, and domain knowledge for generic OS/Kubernetes clusters configuration management tasks.</p><p>Kubespray provides:</p><ul><li>Highly available cluster.</li><li>Composable (Choice of the network plugin for instance).</li><li>Supports most popular Linux distributions:<ul><li>Flatcar Container Linux by Kinvolk</li><li>Debian Bullseye, Buster, Jessie, Stretch</li><li>Ubuntu 16.04, 18.04, 20.04, 22.04</li><li>CentOS/RHEL 7, 8, 9</li><li>Fedora 35, 36</li><li>Fedora CoreOS</li><li>openSUSE Leap 15.x/Tumbleweed</li><li>Oracle Linux 7, 8, 9</li><li>Alma Linux 8, 9</li><li>Rocky Linux 8, 9</li><li>Kylin Linux Advanced Server V10</li><li>Amazon Linux 2</li></ul></li><li>Continuous integration tests.</li></ul><p>To choose a tool which best fits your use case, read <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md>this comparison</a> to
<a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a> and <a href=/docs/setup/production-environment/tools/kops/>kops</a>.</p><h2 id=creating-a-cluster>Creating a cluster</h2><h3 id=1-5-meet-the-underlay-requirements>(1/5) Meet the underlay requirements</h3><p>Provision servers with the following <a href=https://github.com/kubernetes-sigs/kubespray#requirements>requirements</a>:</p><ul><li><strong>Minimum required version of Kubernetes is v1.22</strong></li><li><strong>Ansible v2.11+, Jinja 2.11+ and python-netaddr is installed on the machine that will run Ansible commands</strong></li><li>The target servers must have <strong>access to the Internet</strong> in order to pull docker images. Otherwise, additional configuration is required See (<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/offline-environment.md>Offline Environment</a>)</li><li>The target servers are configured to allow <strong>IPv4 forwarding</strong>.</li><li>If using IPv6 for pods and services, the target servers are configured to allow <strong>IPv6 forwarding</strong>.</li><li>The <strong>firewalls are not managed</strong>, you'll need to implement your own rules the way you used to. in order to avoid any issue during deployment you should disable your firewall.</li><li>If kubespray is run from non-root user account, correct privilege escalation method should be configured in the target servers. Then the <code>ansible_become</code> flag or command parameters <code>--become</code> or <code>-b</code> should be specified.</li></ul><p>Kubespray provides the following utilities to help provision your environment:</p><ul><li><a href=https://www.terraform.io/>Terraform</a> scripts for the following cloud providers:<ul><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/aws>AWS</a></li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/openstack>OpenStack</a></li><li><a href=https://github.com/kubernetes-sigs/kubespray/tree/master/contrib/terraform/metal>Equinix Metal</a></li></ul></li></ul><h3 id=2-5-compose-an-inventory-file>(2/5) Compose an inventory file</h3><p>After you provision your servers, create an <a href=https://docs.ansible.com/ansible/latest/network/getting_started/first_inventory.html>inventory file for Ansible</a>. You can do this manually or via a dynamic inventory script. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#building-your-own-inventory>Building your own inventory</a>".</p><h3 id=3-5-plan-your-cluster-deployment>(3/5) Plan your cluster deployment</h3><p>Kubespray provides the ability to customize many aspects of the deployment:</p><ul><li>Choice deployment mode: kubeadm or non-kubeadm</li><li>CNI (networking) plugins</li><li>DNS configuration</li><li>Choice of control plane: native/binary or containerized</li><li>Component versions</li><li>Calico route reflectors</li><li>Component runtime options<ul><li><a class=glossary-tooltip title='Docker is a software technology providing operating-system-level virtualization also known as containers.' data-toggle=tooltip data-placement=top href=https://docs.docker.com/engine/ target=_blank aria-label=Docker>Docker</a></li><li><a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></li><li><a class=glossary-tooltip title='A lightweight container runtime specifically for Kubernetes' data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></li></ul></li><li>Certificate generation methods</li></ul><p>Kubespray customizations can be made to a <a href=https://docs.ansible.com/ansible/latest/user_guide/playbooks_variables.html>variable file</a>. If you are getting started with Kubespray, consider using the Kubespray defaults to deploy your cluster and explore Kubernetes.</p><h3 id=4-5-deploy-a-cluster>(4/5) Deploy a Cluster</h3><p>Next, deploy your cluster:</p><p>Cluster deployment using <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#starting-custom-deployment>ansible-playbook</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ansible-playbook -i your/inventory/inventory.ini cluster.yml -b -v <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --private-key<span style=color:#666>=</span>~/.ssh/private_key
</span></span></code></pre></div><p>Large deployments (100+ nodes) may require <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/large-deployments.md>specific adjustments</a> for best results.</p><h3 id=5-5-verify-the-deployment>(5/5) Verify the deployment</h3><p>Kubespray provides a way to verify inter-pod connectivity and DNS resolve with <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/netcheck.md>Netchecker</a>. Netchecker ensures the netchecker-agents pods can resolve DNS requests and ping each over within the default namespace. Those pods mimic similar behavior as the rest of the workloads and serve as cluster health indicators.</p><h2 id=cluster-operations>Cluster operations</h2><p>Kubespray provides additional playbooks to manage your cluster: <em>scale</em> and <em>upgrade</em>.</p><h3 id=scale-your-cluster>Scale your cluster</h3><p>You can add worker nodes from your cluster by running the scale playbook. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#adding-nodes>Adding nodes</a>".
You can remove worker nodes from your cluster by running the remove-node playbook. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting-started.md#remove-nodes>Remove nodes</a>".</p><h3 id=upgrade-your-cluster>Upgrade your cluster</h3><p>You can upgrade your cluster by running the upgrade-cluster playbook. For more information, see "<a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/upgrades.md>Upgrades</a>".</p><h2 id=cleanup>Cleanup</h2><p>You can reset your nodes and wipe out all components installed with Kubespray via the <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/reset.yml>reset playbook</a>.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> When running the reset playbook, be sure not to accidentally target your production cluster!</div><h2 id=feedback>Feedback</h2><ul><li>Slack Channel: <a href=https://kubernetes.slack.com/messages/kubespray/>#kubespray</a> (You can get your invite <a href=https://slack.k8s.io/>here</a>).</li><li><a href=https://github.com/kubernetes-sigs/kubespray/issues>GitHub Issues</a>.</li></ul><h2 id=what-s-next>What's next</h2><ul><li>Check out planned work on Kubespray's <a href=https://github.com/kubernetes-sigs/kubespray/blob/master/docs/roadmap.md>roadmap</a>.</li><li>Learn more about <a href=https://github.com/kubernetes-sigs/kubespray>Kubespray</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d2f55eefe7222b7c637875af9c3ec199>3 - Turnkey Cloud Solutions</h1><p>This page provides a list of Kubernetes certified solution providers. From each
provider page, you can learn how to install and setup production
ready clusters.</p><script>function updateLandscapeSource(e,t){console.log({button:e,shouldUpdateFragment:t});try{if(t)window.location.hash="#"+e.id;else{var n=document.querySelectorAll("#landscape");let t=e.dataset.landscapeTypes,s="https://landscape.cncf.io/card-mode?category="+encodeURIComponent(t)+"&grouping=category&embed=yes";n[0].src=s}}catch(e){console.log({message:"error handling Landscape switch",error:e})}}document.addEventListener("DOMContentLoaded",function(){let t=()=>{if(window.location.hash){let e=document.querySelectorAll(".landscape-trigger"+window.location.hash);e.length==1&&(landscapeSource=e[0],console.log("Updating Landscape source based on fragment:",window.location.hash.substring(1)),updateLandscapeSource(landscapeSource,!1))}};var e,n=document.querySelectorAll(".landscape-trigger");if(n.forEach(e=>{e.onclick=function(){updateLandscapeSource(e,!0)}}),e=document.querySelectorAll(".landscape-trigger.landscape-default"),e.length==1){let t=e[0];updateLandscapeSource(t,!1)}window.addEventListener("hashchange",t,!1),t()})</script><div id=frameHolder><iframe frameborder=0 id=landscape scrolling=no src="https://landscape.cncf.io/card-mode?category=certified-kubernetes-hosted&grouping=category&embed=yes" style=width:1px;min-width:100%></iframe>
<script src=https://landscape.cncf.io/iframeResizer.js></script></div></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>