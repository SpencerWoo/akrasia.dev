<!doctype html><html lang=en class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/cluster-administration/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/cluster-administration/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/cluster-administration/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/cluster-administration/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/cluster-administration/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/cluster-administration/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/cluster-administration/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/cluster-administration/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/cluster-administration/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/concepts/cluster-administration/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Cluster Administration | Kubernetes</title><meta property="og:title" content="Cluster Administration"><meta property="og:description" content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/concepts/cluster-administration/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Cluster Administration"><meta itemprop=description content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta name=twitter:card content="summary"><meta name=twitter:title content="Cluster Administration"><meta name=twitter:description content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta property="og:description" content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta name=twitter:description content="Lower-level detail relevant to creating or administering a Kubernetes cluster.
"><meta property="og:url" content="https://kubernetes.io/docs/concepts/cluster-administration/"><meta property="og:title" content="Cluster Administration"><meta name=twitter:title content="Cluster Administration"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/concepts/cluster-administration/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/concepts/cluster-administration/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/concepts/cluster-administration/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/concepts/cluster-administration/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/concepts/cluster-administration/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/docs/concepts/cluster-administration/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/cluster-administration/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/cluster-administration/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/cluster-administration/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/cluster-administration/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/cluster-administration/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/cluster-administration/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/cluster-administration/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/cluster-administration/>Bahasa Indonesia</a>
<a class=dropdown-item href=/ru/docs/concepts/cluster-administration/>Русский (Russian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/concepts/cluster-administration/>Return to the regular view of this page</a>.</p></div><h1 class=title>Cluster Administration</h1><div class=lead>Lower-level detail relevant to creating or administering a Kubernetes cluster.</div><ul><li>1: <a href=#pg-2bf9a93ab5ba014fb6ff70b22c29d432>Certificates</a></li><li>2: <a href=#pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>Managing Resources</a></li><li>3: <a href=#pg-d649067a69d8d5c7e71564b42b96909e>Cluster Networking</a></li><li>4: <a href=#pg-c4b1e87a84441f8a90699a345ce48d68>Logging Architecture</a></li><li>5: <a href=#pg-cbfd3654996eae9fcdef009f70fa83f0>Metrics For Kubernetes System Components</a></li><li>6: <a href=#pg-5cc31ecfba86467f8884856412cfb6b2>System Logs</a></li><li>7: <a href=#pg-3da54ad355f6fe6574d67bd9a9a42bcb>Traces For Kubernetes System Components</a></li><li>8: <a href=#pg-08e94e6a480e0d6b2de72d84a1b97617>Proxies in Kubernetes</a></li><li>9: <a href=#pg-31c9327d2332c585341b64ddafa19cdd>API Priority and Fairness</a></li><li>10: <a href=#pg-85d633ae590aa20ec024f1b7af1d74fc>Installing Addons</a></li></ul><div class=content><p>The cluster administration overview is for anyone creating or administering a Kubernetes cluster.
It assumes some familiarity with core Kubernetes <a href=/docs/concepts/>concepts</a>.</p><h2 id=planning-a-cluster>Planning a cluster</h2><p>See the guides in <a href=/docs/setup/>Setup</a> for examples of how to plan, set up, and configure
Kubernetes clusters. The solutions listed in this article are called <em>distros</em>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Not all distros are actively maintained. Choose distros which have been tested with a recent
version of Kubernetes.</div><p>Before choosing a guide, here are some considerations:</p><ul><li>Do you want to try out Kubernetes on your computer, or do you want to build a high-availability,
multi-node cluster? Choose distros best suited for your needs.</li><li>Will you be using <strong>a hosted Kubernetes cluster</strong>, such as
<a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a>, or <strong>hosting your own cluster</strong>?</li><li>Will your cluster be <strong>on-premises</strong>, or <strong>in the cloud (IaaS)</strong>? Kubernetes does not directly
support hybrid clusters. Instead, you can set up multiple clusters.</li><li><strong>If you are configuring Kubernetes on-premises</strong>, consider which
<a href=/docs/concepts/cluster-administration/networking/>networking model</a> fits best.</li><li>Will you be running Kubernetes on <strong>"bare metal" hardware</strong> or on <strong>virtual machines (VMs)</strong>?</li><li>Do you <strong>want to run a cluster</strong>, or do you expect to do <strong>active development of Kubernetes project code</strong>?
If the latter, choose an actively-developed distro. Some distros only use binary releases, but
offer a greater variety of choices.</li><li>Familiarize yourself with the <a href=/docs/concepts/overview/components/>components</a> needed to run a cluster.</li></ul><h2 id=managing-a-cluster>Managing a cluster</h2><ul><li><p>Learn how to <a href=/docs/concepts/architecture/nodes/>manage nodes</a>.</p></li><li><p>Learn how to set up and manage the <a href=/docs/concepts/policy/resource-quotas/>resource quota</a> for shared clusters.</p></li></ul><h2 id=securing-a-cluster>Securing a cluster</h2><ul><li><p><a href=/docs/tasks/administer-cluster/certificates/>Generate Certificates</a> describes the steps to
generate certificates using different tool chains.</p></li><li><p><a href=/docs/concepts/containers/container-environment/>Kubernetes Container Environment</a> describes
the environment for Kubelet managed containers on a Kubernetes node.</p></li><li><p><a href=/docs/concepts/security/controlling-access>Controlling Access to the Kubernetes API</a> describes
how Kubernetes implements access control for its own API.</p></li><li><p><a href=/docs/reference/access-authn-authz/authentication/>Authenticating</a> explains authentication in
Kubernetes, including the various authentication options.</p></li><li><p><a href=/docs/reference/access-authn-authz/authorization/>Authorization</a> is separate from
authentication, and controls how HTTP calls are handled.</p></li><li><p><a href=/docs/reference/access-authn-authz/admission-controllers/>Using Admission Controllers</a>
explains plug-ins which intercepts requests to the Kubernetes API server after authentication
and authorization.</p></li><li><p><a href=/docs/tasks/administer-cluster/sysctl-cluster/>Using Sysctls in a Kubernetes Cluster</a>
describes to an administrator how to use the <code>sysctl</code> command-line tool to set kernel parameters
.</p></li><li><p><a href=/docs/tasks/debug/debug-cluster/audit/>Auditing</a> describes how to interact with Kubernetes'
audit logs.</p></li></ul><h3 id=securing-the-kubelet>Securing the kubelet</h3><ul><li><a href=/docs/concepts/architecture/control-plane-node-communication/>Control Plane-Node communication</a></li><li><a href=/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/>TLS bootstrapping</a></li><li><a href=/docs/reference/access-authn-authz/kubelet-authn-authz/>Kubelet authentication/authorization</a></li></ul><h2 id=optional-cluster-services>Optional Cluster Services</h2><ul><li><p><a href=/docs/concepts/services-networking/dns-pod-service/>DNS Integration</a> describes how to resolve
a DNS name directly to a Kubernetes service.</p></li><li><p><a href=/docs/concepts/cluster-administration/logging/>Logging and Monitoring Cluster Activity</a>
explains how logging in Kubernetes works and how to implement it.</p></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-2bf9a93ab5ba014fb6ff70b22c29d432>1 - Certificates</h1><p>To learn how to generate certificates for your cluster, see <a href=/docs/tasks/administer-cluster/certificates/>Certificates</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>2 - Managing Resources</h1><p>You've deployed your application and exposed it via a service. Now what? Kubernetes provides a number of tools to help you manage your application deployment, including scaling and updating. Among the features that we will discuss in more depth are <a href=/docs/concepts/configuration/overview/>configuration files</a> and <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a>.</p><h2 id=organizing-resource-configurations>Organizing resource configurations</h2><p>Many applications require multiple resources to be created, such as a Deployment and a Service. Management of multiple resources can be simplified by grouping them together in the same file (separated by <code>---</code> in YAML). For example:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/nginx-app.yaml download=application/nginx-app.yaml><code>application/nginx-app.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("application-nginx-app-yaml")' title="Copy application/nginx-app.yaml to clipboard"></img></div><div class=includecode id=application-nginx-app-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx-svc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Multiple resources can be created the same way as a single resource:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>service/my-nginx-svc created
</span></span><span style=display:flex><span>deployment.apps/my-nginx created
</span></span></code></pre></div><p>The resources will be created in the order they appear in the file. Therefore, it's best to specify the service first, since that will ensure the scheduler can spread the pods associated with the service as they are created by the controller(s), such as Deployment.</p><p><code>kubectl apply</code> also accepts multiple <code>-f</code> arguments:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><p>And a directory can be specified rather than or in addition to individual files:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx/
</span></span></code></pre></div><p><code>kubectl</code> will read any files with suffixes <code>.yaml</code>, <code>.yml</code>, or <code>.json</code>.</p><p>It is a recommended practice to put resources related to the same microservice or application tier into the same file, and to group all of the files associated with your application in the same directory. If the tiers of your application bind to each other using DNS, you can deploy all of the components of your stack together.</p><p>A URL can also be specified as a configuration source, which is handy for deploying directly from configuration files checked into GitHub:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx created
</span></span></code></pre></div><h2 id=bulk-operations-in-kubectl>Bulk operations in kubectl</h2><p>Resource creation isn't the only operation that <code>kubectl</code> can perform in bulk. It can also extract resource names from configuration files in order to perform other operations, in particular to delete the same resources you created:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
</span></span><span style=display:flex><span>service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</span></span></code></pre></div><p>In the case of two resources, you can specify both resources on the command line using the resource/name syntax:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete deployments/my-nginx services/my-nginx-svc
</span></span></code></pre></div><p>For larger numbers of resources, you'll find it easier to specify the selector (label query) specified using <code>-l</code> or <code>--selector</code>, to filter resources by their labels:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete deployment,services -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
</span></span><span style=display:flex><span>service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</span></span></code></pre></div><p>Because <code>kubectl</code> outputs resource names in the same syntax it accepts, you can chain operations using <code>$()</code> or <code>xargs</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get <span style=color:#a2f;font-weight:700>$(</span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service<span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service | xargs -i kubectl get <span style=color:#666>{}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT<span style=color:#666>(</span>S<span style=color:#666>)</span>      AGE
</span></span><span style=display:flex><span>my-nginx-svc   LoadBalancer   10.0.0.208   &lt;pending&gt;     80/TCP       0s
</span></span></code></pre></div><p>With the above commands, we first create resources under <code>examples/application/nginx/</code> and print the resources created with <code>-o name</code> output format
(print each resource as resource/name). Then we <code>grep</code> only the "service", and then print it with <code>kubectl get</code>.</p><p>If you happen to organize your resources across several subdirectories within a particular directory, you can recursively perform the operations on the subdirectories also, by specifying <code>--recursive</code> or <code>-R</code> alongside the <code>--filename,-f</code> flag.</p><p>For instance, assume there is a directory <code>project/k8s/development</code> that holds all of the <a class=glossary-tooltip title='A serialized specification of one or more Kubernetes API objects.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-manifest' target=_blank aria-label=manifests>manifests</a> needed for the development environment, organized by resource type:</p><pre tabindex=0><code>project/k8s/development
├── configmap
│   └── my-configmap.yaml
├── deployment
│   └── my-deployment.yaml
└── pvc
    └── my-pvc.yaml
</code></pre><p>By default, performing a bulk operation on <code>project/k8s/development</code> will stop at the first level of the directory, not processing any subdirectories. If we had tried to create the resources in this directory using the following command, we would have encountered an error:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f project/k8s/development
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>error: you must provide one or more resources by argument or filename <span style=color:#666>(</span>.json|.yaml|.yml|stdin<span style=color:#666>)</span>
</span></span></code></pre></div><p>Instead, specify the <code>--recursive</code> or <code>-R</code> flag with the <code>--filename,-f</code> flag as such:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f project/k8s/development --recursive
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>configmap/my-config created
</span></span><span style=display:flex><span>deployment.apps/my-deployment created
</span></span><span style=display:flex><span>persistentvolumeclaim/my-pvc created
</span></span></code></pre></div><p>The <code>--recursive</code> flag works with any operation that accepts the <code>--filename,-f</code> flag such as: <code>kubectl {create,get,delete,describe,rollout}</code> etc.</p><p>The <code>--recursive</code> flag also works when multiple <code>-f</code> arguments are provided:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>namespace/development created
</span></span><span style=display:flex><span>namespace/staging created
</span></span><span style=display:flex><span>configmap/my-config created
</span></span><span style=display:flex><span>deployment.apps/my-deployment created
</span></span><span style=display:flex><span>persistentvolumeclaim/my-pvc created
</span></span></code></pre></div><p>If you're interested in learning more about <code>kubectl</code>, go ahead and read <a href=/docs/reference/kubectl/>Command line tool (kubectl)</a>.</p><h2 id=using-labels-effectively>Using labels effectively</h2><p>The examples we've used so far apply at most a single label to any resource. There are many scenarios where multiple labels should be used to distinguish sets from one another.</p><p>For instance, different applications would use different values for the <code>app</code> label, but a multi-tier application, such as the <a href=https://github.com/kubernetes/examples/tree/master/guestbook/>guestbook example</a>, would additionally need to distinguish each tier. The frontend could carry the following labels:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span></code></pre></div><p>while the Redis master and slave would have different <code>tier</code> labels, and perhaps even an additional <code>role</code> label:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>master<span style=color:#bbb>
</span></span></span></code></pre></div><p>and</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>slave<span style=color:#bbb>
</span></span></span></code></pre></div><p>The labels allow us to slice and dice our resources along any dimension specified by a label:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml
</span></span><span style=display:flex><span>kubectl get pods -Lapp -Ltier -Lrole
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                           READY     STATUS    RESTARTS   AGE       APP         TIER       ROLE
</span></span><span style=display:flex><span>guestbook-fe-4nlpb             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
</span></span><span style=display:flex><span>guestbook-fe-ght6d             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
</span></span><span style=display:flex><span>guestbook-fe-jpy62             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
</span></span><span style=display:flex><span>guestbook-redis-master-5pg3b   1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    master
</span></span><span style=display:flex><span>guestbook-redis-slave-2q2yf    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
</span></span><span style=display:flex><span>guestbook-redis-slave-qgazl    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
</span></span><span style=display:flex><span>my-nginx-divi2                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
</span></span><span style=display:flex><span>my-nginx-o0ef1                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -lapp<span style=color:#666>=</span>guestbook,role<span style=color:#666>=</span>slave
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                          READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>guestbook-redis-slave-2q2yf   1/1       Running   <span style=color:#666>0</span>          3m
</span></span><span style=display:flex><span>guestbook-redis-slave-qgazl   1/1       Running   <span style=color:#666>0</span>          3m
</span></span></code></pre></div><h2 id=canary-deployments>Canary deployments</h2><p>Another scenario where multiple labels are needed is to distinguish deployments of different releases or configurations of the same component. It is common practice to deploy a <em>canary</em> of a new application release (specified via image tag in the pod template) side by side with the previous release so that the new release can receive live production traffic before fully rolling it out.</p><p>For instance, you can use a <code>track</code> label to differentiate different releases.</p><p>The primary, stable release would have a <code>track</code> label with value as <code>stable</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>stable<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v3<span style=color:#bbb>
</span></span></span></code></pre></div><p>and then you can create a new release of the guestbook frontend that carries the <code>track</code> label with different value (i.e. <code>canary</code>), so that two sets of pods would not overlap:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-canary<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>canary<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v4<span style=color:#bbb>
</span></span></span></code></pre></div><p>The frontend service would span both sets of replicas by selecting the common subset of their labels (i.e. omitting the <code>track</code> label), so that the traffic will be redirected to both applications:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span></code></pre></div><p>You can tweak the number of replicas of the stable and canary releases to determine the ratio of each release that will receive live production traffic (in this case, 3:1).
Once you're confident, you can update the stable track to the new application release and remove the canary one.</p><p>For a more concrete example, check the <a href=https://github.com/kelseyhightower/talks/tree/master/kubecon-eu-2016/demo#deploy-a-canary>tutorial of deploying Ghost</a>.</p><h2 id=updating-labels>Updating labels</h2><p>Sometimes existing pods and other resources need to be relabeled before creating new resources. This can be done with <code>kubectl label</code>.
For example, if you want to label all your nginx pods as frontend tier, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl label pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx <span style=color:#b8860b>tier</span><span style=color:#666>=</span>fe
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pod/my-nginx-2035384211-j5fhi labeled
</span></span><span style=display:flex><span>pod/my-nginx-2035384211-u2c7e labeled
</span></span><span style=display:flex><span>pod/my-nginx-2035384211-u3t6x labeled
</span></span></code></pre></div><p>This first filters all pods with the label "app=nginx", and then labels them with the "tier=fe".
To see the pods you labeled, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx -L tier
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                        READY     STATUS    RESTARTS   AGE       TIER
</span></span><span style=display:flex><span>my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          23m       fe
</span></span><span style=display:flex><span>my-nginx-2035384211-u2c7e   1/1       Running   <span style=color:#666>0</span>          23m       fe
</span></span><span style=display:flex><span>my-nginx-2035384211-u3t6x   1/1       Running   <span style=color:#666>0</span>          23m       fe
</span></span></code></pre></div><p>This outputs all "app=nginx" pods, with an additional label column of pods' tier (specified with <code>-L</code> or <code>--label-columns</code>).</p><p>For more information, please see <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a> and <a href=/docs/reference/generated/kubectl/kubectl-commands/#label>kubectl label</a>.</p><h2 id=updating-annotations>Updating annotations</h2><p>Sometimes you would want to attach annotations to resources. Annotations are arbitrary non-identifying metadata for retrieval by API clients such as tools, libraries, etc. This can be done with <code>kubectl annotate</code>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl annotate pods my-nginx-v4-9gw19 <span style=color:#b8860b>description</span><span style=color:#666>=</span><span style=color:#b44>&#39;my frontend running nginx&#39;</span>
</span></span><span style=display:flex><span>kubectl get pods my-nginx-v4-9gw19 -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    description: my frontend running nginx
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>For more information, please see <a href=/docs/concepts/overview/working-with-objects/annotations/>annotations</a> and <a href=/docs/reference/generated/kubectl/kubectl-commands/#annotate>kubectl annotate</a> document.</p><h2 id=scaling-your-application>Scaling your application</h2><p>When load on your application grows or shrinks, use <code>kubectl</code> to scale your application. For instance, to decrease the number of nginx replicas from 3 to 1, do:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment/my-nginx --replicas<span style=color:#666>=</span><span style=color:#666>1</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx scaled
</span></span></code></pre></div><p>Now you only have one pod managed by the deployment.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                        READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          30m
</span></span></code></pre></div><p>To have the system automatically choose the number of nginx replicas as needed, ranging from 1 to 3, do:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale deployment/my-nginx --min<span style=color:#666>=</span><span style=color:#666>1</span> --max<span style=color:#666>=</span><span style=color:#666>3</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>horizontalpodautoscaler.autoscaling/my-nginx autoscaled
</span></span></code></pre></div><p>Now your nginx replicas will be scaled up and down as needed, automatically.</p><p>For more information, please see <a href=/docs/reference/generated/kubectl/kubectl-commands/#scale>kubectl scale</a>, <a href=/docs/reference/generated/kubectl/kubectl-commands/#autoscale>kubectl autoscale</a> and <a href=/docs/tasks/run-application/horizontal-pod-autoscale/>horizontal pod autoscaler</a> document.</p><h2 id=in-place-updates-of-resources>In-place updates of resources</h2><p>Sometimes it's necessary to make narrow, non-disruptive updates to resources you've created.</p><h3 id=kubectl-apply>kubectl apply</h3><p>It is suggested to maintain a set of configuration files in source control
(see <a href=https://martinfowler.com/bliki/InfrastructureAsCode.html>configuration as code</a>),
so that they can be maintained and versioned along with the code for the resources they configure.
Then, you can use <a href=/docs/reference/generated/kubectl/kubectl-commands/#apply><code>kubectl apply</code></a> to push your configuration changes to the cluster.</p><p>This command will compare the version of the configuration that you're pushing with the previous version and apply the changes you've made, without overwriting any automated changes to properties you haven't specified.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span><span style=display:flex><span>deployment.apps/my-nginx configured
</span></span></code></pre></div><p>Note that <code>kubectl apply</code> attaches an annotation to the resource in order to determine the changes to the configuration since the previous invocation. When it's invoked, <code>kubectl apply</code> does a three-way diff between the previous configuration, the provided input and the current configuration of the resource, in order to determine how to modify the resource.</p><p>Currently, resources are created without this annotation, so the first invocation of <code>kubectl apply</code> will fall back to a two-way diff between the provided input and the current configuration of the resource. During this first invocation, it cannot detect the deletion of properties set when the resource was created. For this reason, it will not remove them.</p><p>All subsequent calls to <code>kubectl apply</code>, and other commands that modify the configuration, such as <code>kubectl replace</code> and <code>kubectl edit</code>, will update the annotation, allowing subsequent calls to <code>kubectl apply</code> to detect and perform deletions using a three-way diff.</p><h3 id=kubectl-edit>kubectl edit</h3><p>Alternatively, you may also update resources with <code>kubectl edit</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment/my-nginx
</span></span></code></pre></div><p>This is equivalent to first <code>get</code> the resource, edit it in text editor, and then <code>apply</code> the resource with the updated version:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment my-nginx -o yaml &gt; /tmp/nginx.yaml
</span></span><span style=display:flex><span>vi /tmp/nginx.yaml
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># do some edit, and then save the file</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl apply -f /tmp/nginx.yaml
</span></span><span style=display:flex><span>deployment.apps/my-nginx configured
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rm /tmp/nginx.yaml
</span></span></code></pre></div><p>This allows you to do more significant changes more easily. Note that you can specify the editor with your <code>EDITOR</code> or <code>KUBE_EDITOR</code> environment variables.</p><p>For more information, please see <a href=/docs/reference/generated/kubectl/kubectl-commands/#edit>kubectl edit</a> document.</p><h3 id=kubectl-patch>kubectl patch</h3><p>You can use <code>kubectl patch</code> to update API objects in place. This command supports JSON patch,
JSON merge patch, and strategic merge patch. See
<a href=/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/>Update API Objects in Place Using kubectl patch</a>
and
<a href=/docs/reference/generated/kubectl/kubectl-commands/#patch>kubectl patch</a>.</p><h2 id=disruptive-updates>Disruptive updates</h2><p>In some cases, you may need to update resource fields that cannot be updated once initialized, or you may want to make a recursive change immediately, such as to fix broken pods created by a Deployment. To change such fields, use <code>replace --force</code>, which deletes and re-creates the resource. In this case, you can modify your original configuration file:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx deleted
</span></span><span style=display:flex><span>deployment.apps/my-nginx replaced
</span></span></code></pre></div><h2 id=updating-your-application-without-a-service-outage>Updating your application without a service outage</h2><p>At some point, you'll eventually need to update your deployed application, typically by specifying a new image or image tag, as in the canary deployment scenario above. <code>kubectl</code> supports several update operations, each of which is applicable to different scenarios.</p><p>We'll guide you through how to create and update applications with Deployments.</p><p>Let's say you were running version 1.14.2 of nginx:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create deployment my-nginx --image<span style=color:#666>=</span>nginx:1.14.2
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx created
</span></span></code></pre></div><p>with 3 replicas (so the old and new revisions can coexist):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment my-nginx --current-replicas<span style=color:#666>=</span><span style=color:#666>1</span> --replicas<span style=color:#666>=</span><span style=color:#666>3</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/my-nginx scaled
</code></pre><p>To update to version 1.16.1, change <code>.spec.template.spec.containers[0].image</code> from <code>nginx:1.14.2</code> to <code>nginx:1.16.1</code> using the previous kubectl commands.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment/my-nginx
</span></span></code></pre></div><p>That's it! The Deployment will declaratively update the deployed nginx application progressively behind the scene. It ensures that only a certain number of old replicas may be down while they are being updated, and only a certain number of new replicas may be created above the desired number of pods. To learn more details about it, visit <a href=/docs/concepts/workloads/controllers/deployment/>Deployment page</a>.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/tasks/debug/debug-application/debug-running-pod/>how to use <code>kubectl</code> for application introspection and debugging</a>.</li><li>See <a href=/docs/concepts/configuration/overview/>Configuration Best Practices and Tips</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d649067a69d8d5c7e71564b42b96909e>3 - Cluster Networking</h1><p>Networking is a central part of Kubernetes, but it can be challenging to
understand exactly how it is expected to work. There are 4 distinct networking
problems to address:</p><ol><li>Highly-coupled container-to-container communications: this is solved by
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> and <code>localhost</code> communications.</li><li>Pod-to-Pod communications: this is the primary focus of this document.</li><li>Pod-to-Service communications: this is covered by <a href=/docs/concepts/services-networking/service/>Services</a>.</li><li>External-to-Service communications: this is also covered by Services.</li></ol><p>Kubernetes is all about sharing machines between applications. Typically,
sharing machines requires ensuring that two applications do not try to use the
same ports. Coordinating ports across multiple developers is very difficult to
do at scale and exposes users to cluster-level issues outside of their control.</p><p>Dynamic port allocation brings a lot of complications to the system - every
application has to take ports as flags, the API servers have to know how to
insert dynamic port numbers into configuration blocks, services have to know
how to find each other, etc. Rather than deal with this, Kubernetes takes a
different approach.</p><p>To learn about the Kubernetes networking model, see <a href=/docs/concepts/services-networking/>here</a>.</p><h2 id=how-to-implement-the-kubernetes-network-model>How to implement the Kubernetes network model</h2><p>The network model is implemented by the container runtime on each node. The most common container runtimes use <a href=https://github.com/containernetworking/cni>Container Network Interface</a> (CNI) plugins to manage their network and security capabilities. Many different CNI plugins exist from many different vendors. Some of these provide only basic features of adding and removing network interfaces, while others provide more sophisticated solutions, such as integration with other container orchestration systems, running multiple CNI plugins, advanced IPAM features etc.</p><p>See <a href=/docs/concepts/cluster-administration/addons/#networking-and-network-policy>this page</a> for a non-exhaustive list of networking addons supported by Kubernetes.</p><h2 id=what-s-next>What's next</h2><p>The early design of the networking model and its rationale, and some future
plans are described in more detail in the
<a href=https://git.k8s.io/design-proposals-archive/network/networking.md>networking design document</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c4b1e87a84441f8a90699a345ce48d68>4 - Logging Architecture</h1><p>Application logs can help you understand what is happening inside your application. The
logs are particularly useful for debugging problems and monitoring cluster activity. Most
modern applications have some kind of logging mechanism. Likewise, container engines
are designed to support logging. The easiest and most adopted logging method for
containerized applications is writing to standard output and standard error streams.</p><p>However, the native functionality provided by a container engine or runtime is usually
not enough for a complete logging solution.</p><p>For example, you may want to access your application's logs if a container crashes,
a pod gets evicted, or a node dies.</p><p>In a cluster, logs should have a separate storage and lifecycle independent of nodes,
pods, or containers. This concept is called
<a href=#cluster-level-logging-architectures>cluster-level logging</a>.</p><p>Cluster-level logging architectures require a separate backend to store, analyze, and
query logs. Kubernetes does not provide a native storage solution for log data. Instead,
there are many logging solutions that integrate with Kubernetes. The following sections
describe how to handle and store logs on nodes.</p><h2 id=basic-logging-in-kubernetes>Pod and container logs</h2><p>Kubernetes captures logs from each container in a running Pod.</p><p>This example uses a manifest for a <code>Pod</code> with a container
that writes text to the standard output stream, once per second.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/debug/counter-pod.yaml download=debug/counter-pod.yaml><code>debug/counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("debug-counter-pod-yaml")' title="Copy debug/counter-pod.yaml to clipboard"></img></div><div class=includecode id=debug-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c,<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:#b44>&#39;i=0; while true; do echo &#34;$i: $(date)&#34;; i=$((i+1)); sleep 1; done&#39;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>To run this pod, use the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</span></span></code></pre></div><p>The output is:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>pod/counter created
</span></span></span></code></pre></div><p>To fetch the logs, use the <code>kubectl logs</code> command, as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter
</span></span></code></pre></div><p>The output is similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>0: Fri Apr  1 11:42:23 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>1: Fri Apr  1 11:42:24 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>2: Fri Apr  1 11:42:25 UTC 2022
</span></span></span></code></pre></div><p>You can use <code>kubectl logs --previous</code> to retrieve logs from a previous instantiation of a container.
If your pod has multiple containers, specify which container's logs you want to access by
appending a container name to the command, with a <code>-c</code> flag, like so:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>kubectl logs counter -c count
</span></span></span></code></pre></div><p>See the <a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code> documentation</a> for more details.</p><h3 id=how-nodes-handle-container-logs>How nodes handle container logs</h3><p><img src=/images/docs/user-guide/logging/logging-node-level.png alt="Node level logging"></p><p>A container runtime handles and redirects any output generated to a containerized application's <code>stdout</code> and <code>stderr</code> streams.
Different container runtimes implement this in different ways; however, the integration with the kubelet is standardized
as the <em>CRI logging format</em>.</p><p>By default, if a container restarts, the kubelet keeps one terminated container with its logs. If a pod is evicted from the node,
all corresponding containers are also evicted, along with their logs.</p><p>The kubelet makes logs available to clients via a special feature of the Kubernetes API. The usual way to access this is
by running <code>kubectl logs</code>.</p><h3 id=log-rotation>Log rotation</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>You can configure the kubelet to rotate logs automatically.</p><p>If you configure rotation, the kubelet is responsible for rotating container logs and managing the logging directory structure.
The kubelet sends this information to the container runtime (using CRI),
and the runtime writes the container logs to the given location.</p><p>You can configure two kubelet <a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration>configuration settings</a>,
<code>containerLogMaxSize</code> and <code>containerLogMaxFiles</code>,
using the <a href=/docs/tasks/administer-cluster/kubelet-config-file/>kubelet configuration file</a>.
These settings let you configure the maximum size for each log file and the maximum number of files allowed for each container respectively.</p><p>When you run <a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code></a> as in
the basic logging example, the kubelet on the node handles the request and
reads directly from the log file. The kubelet returns the content of the log file.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Only the contents of the latest log file are available through
<code>kubectl logs</code>.</p><p>For example, if a Pod writes 40 MiB of logs and the kubelet rotates logs
after 10 MiB, running <code>kubectl logs</code> returns at most 10MiB of data.</p></div><h2 id=system-component-logs>System component logs</h2><p>There are two types of system components: those that typically run in a container,
and those components directly involved in running containers. For example:</p><ul><li>The kubelet and container runtime do not run in containers. The kubelet runs
your containers (grouped together in <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=pods>pods</a>)</li><li>The Kubernetes scheduler, controller manager, and API server run within pods
(usually <a class=glossary-tooltip title='A pod managed directly by the kubelet daemon on a specific node.' data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/static-pod/ target=_blank aria-label='static Pods'>static Pods</a>).
The etcd component runs in the control plane, and most commonly also as a static pod.
If your cluster uses kube-proxy, you typically run this as a <code>DaemonSet</code>.</li></ul><h3 id=log-location-node>Log locations</h3><p>The way that the kubelet and container runtime write logs depends on the operating
system that the node uses:</p><ul class="nav nav-tabs" id=log-location-node-tabs role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#log-location-node-tabs-0 role=tab aria-controls=log-location-node-tabs-0 aria-selected=true>Linux</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#log-location-node-tabs-1 role=tab aria-controls=log-location-node-tabs-1>Windows</a></li></ul><div class=tab-content id=log-location-node-tabs><div id=log-location-node-tabs-0 class="tab-pane show active" role=tabpanel aria-labelledby=log-location-node-tabs-0><p><p>On Linux nodes that use systemd, the kubelet and container runtime write to journald
by default. You use <code>journalctl</code> to read the systemd journal; for example:
<code>journalctl -u kubelet</code>.</p><p>If systemd is not present, the kubelet and container runtime write to <code>.log</code> files in the
<code>/var/log</code> directory. If you want to have logs written elsewhere, you can indirectly
run the kubelet via a helper tool, <code>kube-log-runner</code>, and use that tool to redirect
kubelet logs to a directory that you choose.</p><p>You can also set a logging directory using the deprecated kubelet command line
argument <code>--log-dir</code>. However, the kubelet always directs your container runtime to
write logs into directories within <code>/var/log/pods</code>.</p><p>For more information on <code>kube-log-runner</code>, read <a href=/docs/concepts/cluster-administration/system-logs/#klog>System Logs</a>.</p></div><div id=log-location-node-tabs-1 class=tab-pane role=tabpanel aria-labelledby=log-location-node-tabs-1><p><p>By default, the kubelet writes logs to files within the directory <code>C:\var\logs</code>
(notice that this is not <code>C:\var\log</code>).</p><p>Although <code>C:\var\log</code> is the Kubernetes default location for these logs, several
cluster deployment tools set up Windows nodes to log to <code>C:\var\log\kubelet</code> instead.</p><p>If you want to have logs written elsewhere, you can indirectly
run the kubelet via a helper tool, <code>kube-log-runner</code>, and use that tool to redirect
kubelet logs to a directory that you choose.</p><p>However, the kubelet always directs your container runtime to write logs within the
directory <code>C:\var\log\pods</code>.</p><p>For more information on <code>kube-log-runner</code>, read <a href=/docs/concepts/cluster-administration/system-logs/#klog>System Logs</a>.</p></div></div><p><br></p><p>For Kubernetes cluster components that run in pods, these write to files inside
the <code>/var/log</code> directory, bypassing the default logging mechanism (the components
do not write to the systemd journal). You can use Kubernetes' storage mechanisms
to map persistent storage into the container that runs the component.</p><p>For details about etcd and its logs, view the <a href=https://etcd.io/docs/>etcd documentation</a>.
Again, you can use Kubernetes' storage mechanisms to map persistent storage into
the container that runs the component.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>If you deploy Kubernetes cluster components (such as the scheduler) to log to
a volume shared from the parent node, you need to consider and ensure that those
logs are rotated. <strong>Kubernetes does not manage that log rotation</strong>.</p><p>Your operating system may automatically implement some log rotation - for example,
if you share the directory <code>/var/log</code> into a static Pod for a component, node-level
log rotation treats a file in that directory the same as a file written by any component
outside Kubernetes.</p><p>Some deploy tools account for that log rotation and automate it; others leave this
as your responsibility.</p></div><h2 id=cluster-level-logging-architectures>Cluster-level logging architectures</h2><p>While Kubernetes does not provide a native solution for cluster-level logging, there are several common approaches you can consider. Here are some options:</p><ul><li>Use a node-level logging agent that runs on every node.</li><li>Include a dedicated sidecar container for logging in an application pod.</li><li>Push logs directly to a backend from within an application.</li></ul><h3 id=using-a-node-logging-agent>Using a node logging agent</h3><p><img src=/images/docs/user-guide/logging/logging-with-node-agent.png alt="Using a node level logging agent"></p><p>You can implement cluster-level logging by including a <em>node-level logging agent</em> on each node. The logging agent is a dedicated tool that exposes logs or pushes logs to a backend. Commonly, the logging agent is a container that has access to a directory with log files from all of the application containers on that node.</p><p>Because the logging agent must run on every node, it is recommended to run the agent
as a <code>DaemonSet</code>.</p><p>Node-level logging creates only one agent per node and doesn't require any changes to the applications running on the node.</p><p>Containers write to stdout and stderr, but with no agreed format. A node-level agent collects these logs and forwards them for aggregation.</p><h3 id=sidecar-container-with-logging-agent>Using a sidecar container with the logging agent</h3><p>You can use a sidecar container in one of the following ways:</p><ul><li>The sidecar container streams application logs to its own <code>stdout</code>.</li><li>The sidecar container runs a logging agent, which is configured to pick up logs from an application container.</li></ul><h4 id=streaming-sidecar-container>Streaming sidecar container</h4><p><img src=/images/docs/user-guide/logging/logging-with-streaming-sidecar.png alt="Sidecar container with a streaming container"></p><p>By having your sidecar containers write to their own <code>stdout</code> and <code>stderr</code>
streams, you can take advantage of the kubelet and the logging agent that
already run on each node. The sidecar containers read logs from a file, a socket,
or journald. Each sidecar container prints a log to its own <code>stdout</code> or <code>stderr</code> stream.</p><p>This approach allows you to separate several log streams from different
parts of your application, some of which can lack support
for writing to <code>stdout</code> or <code>stderr</code>. The logic behind redirecting logs
is minimal, so it's not a significant overhead. Additionally, because
<code>stdout</code> and <code>stderr</code> are handled by the kubelet, you can use built-in tools
like <code>kubectl logs</code>.</p><p>For example, a pod runs a single container, and the container
writes to two different log files using two different formats. Here's a
manifest for the Pod:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod.yaml download=admin/logging/two-files-counter-pod.yaml><code>admin/logging/two-files-counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-yaml")' title="Copy admin/logging/two-files-counter-pod.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>It is not recommended to write log entries with different formats to the same log
stream, even if you managed to redirect both components to the <code>stdout</code> stream of
the container. Instead, you can create two sidecar containers. Each sidecar
container could tail a particular log file from a shared volume and then redirect
the logs to its own <code>stdout</code> stream.</p><p>Here's a manifest for a pod that has two sidecar containers:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml download=admin/logging/two-files-counter-pod-streaming-sidecar.yaml><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-streaming-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-streaming-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/1.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/2.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Now when you run this pod, you can access each log stream separately by
running the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-1
</span></span></code></pre></div><p>The output is similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>0: Fri Apr  1 11:42:26 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>1: Fri Apr  1 11:42:27 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>2: Fri Apr  1 11:42:28 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-2
</span></span></code></pre></div><p>The output is similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>Fri Apr  1 11:42:29 UTC 2022 INFO 0
</span></span></span><span style=display:flex><span><span style=color:#888>Fri Apr  1 11:42:30 UTC 2022 INFO 0
</span></span></span><span style=display:flex><span><span style=color:#888>Fri Apr  1 11:42:31 UTC 2022 INFO 0
</span></span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span></code></pre></div><p>If you installed a node-level agent in your cluster, that agent picks up those log
streams automatically without any further configuration. If you like, you can configure
the agent to parse log lines depending on the source container.</p><p>Even for Pods that only have low CPU and memory usage (order of a couple of millicores
for cpu and order of several megabytes for memory), writing logs to a file and
then streaming them to <code>stdout</code> can double how much storage you need on the node.
If you have an application that writes to a single file, it's recommended to set
<code>/dev/stdout</code> as the destination rather than implement the streaming sidecar
container approach.</p><p>Sidecar containers can also be used to rotate log files that cannot be rotated by
the application itself. An example of this approach is a small container running
<code>logrotate</code> periodically.
However, it's more straightforward to use <code>stdout</code> and <code>stderr</code> directly, and
leave rotation and retention policies to the kubelet.</p><h4 id=sidecar-container-with-a-logging-agent>Sidecar container with a logging agent</h4><p><img src=/images/docs/user-guide/logging/logging-with-sidecar-agent.png alt="Sidecar container with a logging agent"></p><p>If the node-level logging agent is not flexible enough for your situation, you
can create a sidecar container with a separate logging agent that you have
configured specifically to run with your application.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Using a logging agent in a sidecar container can lead
to significant resource consumption. Moreover, you won't be able to access
those logs using <code>kubectl logs</code> because they are not controlled
by the kubelet.</div><p>Here are two example manifests that you can use to implement a sidecar container with a logging agent.
The first manifest contains a <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/><code>ConfigMap</code></a>
to configure fluentd.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/fluentd-sidecar-config.yaml download=admin/logging/fluentd-sidecar-config.yaml><code>admin/logging/fluentd-sidecar-config.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-fluentd-sidecar-config-yaml")' title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard"></img></div><div class=includecode id=admin-logging-fluentd-sidecar-config-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fluentd.conf</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/1.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/1.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format1
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/2.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/2.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format2
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;match **&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type google_cloud
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/match&gt;</span><span style=color:#bbb>    
</span></span></span></code></pre></div></div></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In the sample configurations, you can replace fluentd with any logging agent, reading
from any source inside an application container.</div><p>The second manifest describes a pod that has a sidecar container running fluentd.
The pod mounts a volume where fluentd can pick up its configuration data.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml download=admin/logging/two-files-counter-pod-agent-sidecar.yaml><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-agent-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-agent-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-agent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/fluentd-gcp:1.30<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>FLUENTD_ARGS<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>-c /etc/fluentd-config/fluentd.conf<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h3 id=exposing-logs-directly-from-the-application>Exposing logs directly from the application</h3><p><img src=/images/docs/user-guide/logging/logging-from-application.png alt="Exposing logs directly from the application"></p><p>Cluster-logging that exposes or pushes logs directly from every application is outside the scope of Kubernetes.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/cluster-administration/system-logs/>Kubernetes system logs</a></li><li>Learn about <a href=/docs/concepts/cluster-administration/system-traces/>Traces For Kubernetes System Components</a></li><li>Learn how to <a href=/docs/tasks/debug/debug-application/determine-reason-pod-failure/#customizing-the-termination-message>customise the termination message</a> that Kubernetes records when a Pod fails</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cbfd3654996eae9fcdef009f70fa83f0>5 - Metrics For Kubernetes System Components</h1><p>System component metrics can give a better look into what is happening inside them. Metrics are
particularly useful for building dashboards and alerts.</p><p>Kubernetes components emit metrics in <a href=https://prometheus.io/docs/instrumenting/exposition_formats/>Prometheus format</a>.
This format is structured plain text, designed so that people and machines can both read it.</p><h2 id=metrics-in-kubernetes>Metrics in Kubernetes</h2><p>In most cases metrics are available on <code>/metrics</code> endpoint of the HTTP server. For components that
doesn't expose endpoint by default it can be enabled using <code>--bind-address</code> flag.</p><p>Examples of those components:</p><ul><li><a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li><li><a class=glossary-tooltip title='kube-proxy is a network proxy that runs on each node in the cluster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a></li><li><a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li><li><a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li><li><a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a></li></ul><p>In a production environment you may want to configure <a href=https://prometheus.io/>Prometheus Server</a>
or some other metrics scraper to periodically gather these metrics and make them available in some
kind of time series database.</p><p>Note that <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> also exposes metrics in
<code>/metrics/cadvisor</code>, <code>/metrics/resource</code> and <code>/metrics/probes</code> endpoints. Those metrics do not
have same lifecycle.</p><p>If your cluster uses <a class=glossary-tooltip title='Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/rbac/ target=_blank aria-label=RBAC>RBAC</a>, reading metrics requires
authorization via a user, group or ServiceAccount with a ClusterRole that allows accessing
<code>/metrics</code>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prometheus<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>nonResourceURLs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/metrics&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- get<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=metric-lifecycle>Metric lifecycle</h2><p>Alpha metric → Stable metric → Deprecated metric → Hidden metric → Deleted metric</p><p>Alpha metrics have no stability guarantees. These metrics can be modified or deleted at any time.</p><p>Stable metrics are guaranteed to not change. This means:</p><ul><li>A stable metric without a deprecated signature will not be deleted or renamed</li><li>A stable metric's type will not be modified</li></ul><p>Deprecated metrics are slated for deletion, but are still available for use.
These metrics include an annotation about the version in which they became deprecated.</p><p>For example:</p><ul><li><p>Before deprecation</p><pre tabindex=0><code># HELP some_counter this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li><li><p>After deprecation</p><pre tabindex=0><code># HELP some_counter (Deprecated since 1.15.0) this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li></ul><p>Hidden metrics are no longer published for scraping, but are still available for use. To use a
hidden metric, please refer to the <a href=#show-hidden-metrics>Show hidden metrics</a> section.</p><p>Deleted metrics are no longer published and cannot be used.</p><h2 id=show-hidden-metrics>Show hidden metrics</h2><p>As described above, admins can enable hidden metrics through a command-line flag on a specific
binary. This intends to be used as an escape hatch for admins if they missed the migration of the
metrics deprecated in the last release.</p><p>The flag <code>show-hidden-metrics-for-version</code> takes a version for which you want to show metrics
deprecated in that release. The version is expressed as x.y, where x is the major version, y is
the minor version. The patch version is not needed even though a metrics can be deprecated in a
patch release, the reason for that is the metrics deprecation policy runs against the minor release.</p><p>The flag can only take the previous minor version as it's value. All metrics hidden in previous
will be emitted if admins set the previous version to <code>show-hidden-metrics-for-version</code>. The too
old version is not allowed because this violates the metrics deprecated policy.</p><p>Take metric <code>A</code> as an example, here assumed that <code>A</code> is deprecated in 1.n. According to metrics
deprecated policy, we can reach the following conclusion:</p><ul><li>In release <code>1.n</code>, the metric is deprecated, and it can be emitted by default.</li><li>In release <code>1.n+1</code>, the metric is hidden by default and it can be emitted by command line
<code>show-hidden-metrics-for-version=1.n</code>.</li><li>In release <code>1.n+2</code>, the metric should be removed from the codebase. No escape hatch anymore.</li></ul><p>If you're upgrading from release <code>1.12</code> to <code>1.13</code>, but still depend on a metric <code>A</code> deprecated in
<code>1.12</code>, you should set hidden metrics via command line: <code>--show-hidden-metrics=1.12</code> and remember
to remove this metric dependency before upgrading to <code>1.14</code></p><h2 id=disable-accelerator-metrics>Disable accelerator metrics</h2><p>The kubelet collects accelerator metrics through cAdvisor. To collect these metrics, for
accelerators like NVIDIA GPUs, kubelet held an open handle on the driver. This meant that in order
to perform infrastructure changes (for example, updating the driver), a cluster administrator
needed to stop the kubelet agent.</p><p>The responsibility for collecting accelerator metrics now belongs to the vendor rather than the
kubelet. Vendors must provide a container that collects metrics and exposes them to the metrics
service (for example, Prometheus).</p><p>The <a href=/docs/reference/command-line-tools-reference/feature-gates/><code>DisableAcceleratorUsageMetrics</code> feature gate</a>
disables metrics collected by the kubelet, with a
<a href=https://github.com/kubernetes/enhancements/tree/411e51027db842355bd489691af897afc1a41a5e/keps/sig-node/1867-disable-accelerator-usage-metrics#graduation-criteria>timeline for enabling this feature by default</a>.</p><h2 id=component-metrics>Component metrics</h2><h3 id=kube-controller-manager-metrics>kube-controller-manager metrics</h3><p>Controller manager metrics provide important insight into the performance and health of the
controller manager. These metrics include common Go language runtime metrics such as go_routine
count and controller specific metrics such as etcd request latencies or Cloudprovider (AWS, GCE,
OpenStack) API latencies that can be used to gauge the health of a cluster.</p><p>Starting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operations
for GCE, AWS, Vsphere and OpenStack.
These metrics can be used to monitor health of persistent volume operations.</p><p>For example, for GCE these metrics are called:</p><pre tabindex=0><code>cloudprovider_gce_api_request_duration_seconds { request = &#34;instance_list&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_insert&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_delete&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;attach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;detach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;list_disk&#34;}
</code></pre><h3 id=kube-scheduler-metrics>kube-scheduler metrics</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code></div><p>The scheduler exposes optional metrics that reports the requested resources and the desired limits
of all running pods. These metrics can be used to build capacity planning dashboards, assess
current or historical scheduling limits, quickly identify workloads that cannot schedule due to
lack of resources, and compare actual usage to the pod's request.</p><p>The kube-scheduler identifies the resource <a href=/docs/concepts/configuration/manage-resources-containers/>requests and limits</a>
configured for each Pod; when either a request or limit is non-zero, the kube-scheduler reports a
metrics timeseries. The time series is labelled by:</p><ul><li>namespace</li><li>pod name</li><li>the node where the pod is scheduled or an empty string if not yet scheduled</li><li>priority</li><li>the assigned scheduler for that pod</li><li>the name of the resource (for example, <code>cpu</code>)</li><li>the unit of the resource if known (for example, <code>cores</code>)</li></ul><p>Once a pod reaches completion (has a <code>restartPolicy</code> of <code>Never</code> or <code>OnFailure</code> and is in the
<code>Succeeded</code> or <code>Failed</code> pod phase, or has been deleted and all containers have a terminated state)
the series is no longer reported since the scheduler is now free to schedule other pods to run.
The two metrics are called <code>kube_pod_resource_request</code> and <code>kube_pod_resource_limit</code>.</p><p>The metrics are exposed at the HTTP endpoint <code>/metrics/resources</code> and require the same
authorization as the <code>/metrics</code> endpoint on the scheduler. You must use the
<code>--show-hidden-metrics-for-version=1.20</code> flag to expose these alpha stability metrics.</p><h2 id=disabling-metrics>Disabling metrics</h2><p>You can explicitly turn off metrics via command line flag <code>--disabled-metrics</code>. This may be
desired if, for example, a metric is causing a performance problem. The input is a list of
disabled metrics (i.e. <code>--disabled-metrics=metric1,metric2</code>).</p><h2 id=metric-cardinality-enforcement>Metric cardinality enforcement</h2><p>Metrics with unbounded dimensions could cause memory issues in the components they instrument. To
limit resource use, you can use the <code>--allow-label-value</code> command line option to dynamically
configure an allow-list of label values for a metric.</p><p>In alpha stage, the flag can only take in a series of mappings as metric label allow-list.
Each mapping is of the format <code>&lt;metric_name>,&lt;label_name>=&lt;allowed_labels></code> where
<code>&lt;allowed_labels></code> is a comma-separated list of acceptable label names.</p><p>The overall format looks like:</p><pre tabindex=0><code>--allow-label-value &lt;metric_name&gt;,&lt;label_name&gt;=&#39;&lt;allow_value1&gt;, &lt;allow_value2&gt;...&#39;, &lt;metric_name2&gt;,&lt;label_name&gt;=&#39;&lt;allow_value1&gt;, &lt;allow_value2&gt;...&#39;, ...
</code></pre><p>Here is an example:</p><pre tabindex=0><code class=language-none data-lang=none>--allow-label-value number_count_metric,odd_number=&#39;1,3,5&#39;, number_count_metric,even_number=&#39;2,4,6&#39;, date_gauge_metric,weekend=&#39;Saturday,Sunday&#39;
</code></pre><h2 id=what-s-next>What's next</h2><ul><li>Read about the <a href=https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format>Prometheus text format</a>
for metrics</li><li>See the list of <a href=https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml>stable Kubernetes metrics</a></li><li>Read about the <a href=/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior>Kubernetes deprecation policy</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5cc31ecfba86467f8884856412cfb6b2>6 - System Logs</h1><p>System component logs record events happening in cluster, which can be very useful for debugging.
You can configure log verbosity to see more or less detail.
Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showing
step-by-step traces of events (like HTTP access logs, pod state changes, controller actions, or
scheduler decisions).</p><h2 id=klog>Klog</h2><p>klog is the Kubernetes logging library. <a href=https://github.com/kubernetes/klog>klog</a>
generates log messages for the Kubernetes system components.</p><p>For more information about klog configuration, see the <a href=/docs/reference/command-line-tools-reference/>Command line tool reference</a>.</p><p>Kubernetes is in the process of simplifying logging in its components.
The following klog command line flags
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components>are deprecated</a>
starting with Kubernetes 1.23 and will be removed in a future release:</p><ul><li><code>--add-dir-header</code></li><li><code>--alsologtostderr</code></li><li><code>--log-backtrace-at</code></li><li><code>--log-dir</code></li><li><code>--log-file</code></li><li><code>--log-file-max-size</code></li><li><code>--logtostderr</code></li><li><code>--one-output</code></li><li><code>--skip-headers</code></li><li><code>--skip-log-headers</code></li><li><code>--stderrthreshold</code></li></ul><p>Output will always be written to stderr, regardless of the output format. Output redirection is
expected to be handled by the component which invokes a Kubernetes component. This can be a POSIX
shell or a tool like systemd.</p><p>In some cases, for example a distroless container or a Windows system service, those options are
not available. Then the
<a href=https://github.com/kubernetes/kubernetes/blob/d2a8a81639fcff8d1221b900f66d28361a170654/staging/src/k8s.io/component-base/logs/kube-log-runner/README.md><code>kube-log-runner</code></a>
binary can be used as wrapper around a Kubernetes component to redirect
output. A prebuilt binary is included in several Kubernetes base images under
its traditional name as <code>/go-runner</code> and as <code>kube-log-runner</code> in server and
node release archives.</p><p>This table shows how <code>kube-log-runner</code> invocations correspond to shell redirection:</p><table><thead><tr><th>Usage</th><th>POSIX shell (such as bash)</th><th><code>kube-log-runner &lt;options> &lt;cmd></code></th></tr></thead><tbody><tr><td>Merge stderr and stdout, write to stdout</td><td><code>2>&1</code></td><td><code>kube-log-runner</code> (default behavior)</td></tr><tr><td>Redirect both into log file</td><td><code>1>>/tmp/log 2>&1</code></td><td><code>kube-log-runner -log-file=/tmp/log</code></td></tr><tr><td>Copy into log file and to stdout</td><td><code>2>&1 | tee -a /tmp/log</code></td><td><code>kube-log-runner -log-file=/tmp/log -also-stdout</code></td></tr><tr><td>Redirect only stdout into log file</td><td><code>>/tmp/log</code></td><td><code>kube-log-runner -log-file=/tmp/log -redirect-stderr=false</code></td></tr></tbody></table><h3 id=klog-output>Klog output</h3><p>An example of the traditional klog native format:</p><pre tabindex=0><code>I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]
</code></pre><p>The message string may contain line breaks:</p><pre tabindex=0><code>I1025 00:15:15.525108       1 example.go:79] This is a message
which has a line break.
</code></pre><h3 id=structured-logging>Structured Logging</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>Migration to structured log messages is an ongoing process. Not all log messages are structured in
this version. When parsing log files, you must also handle unstructured log messages.</p><p>Log formatting and value serialization are subject to change.</p></div><p>Structured logging introduces a uniform structure in log messages allowing for programmatic
extraction of information. You can store and process structured logs with less effort and cost.
The code which generates a log message determines whether it uses the traditional unstructured
klog output or structured logging.</p><p>The default formatting of structured log messages is as text, with a format that is backward
compatible with traditional klog:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#b44>&lt;klog header&gt; &#34;&lt;message&gt;&#34; &lt;key1&gt;</span><span style=color:#666>=</span><span style=color:#b44>&#34;&lt;value1&gt;&#34; &lt;key2&gt;=&#34;&lt;value2&gt;&#34; ...</span>
</span></span></code></pre></div><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#b44>I1025 00:15:15.525108       1 controller_utils.go:116] &#34;Pod status updated&#34; pod</span><span style=color:#666>=</span><span style=color:#b44>&#34;kube-system/kubedns&#34; status=&#34;ready&#34;</span>
</span></span></code></pre></div><p>Strings are quoted. Other values are formatted with
<a href=https://pkg.go.dev/fmt#hdr-Printing><code>%+v</code></a>, which may cause log messages to
continue on the next line <a href=https://github.com/kubernetes/kubernetes/issues/106428>depending on the data</a>.</p><pre tabindex=0><code>I1025 00:15:15.525108       1 example.go:116] &#34;Example&#34; data=&#34;This is text with a line break\nand \&#34;quotation marks\&#34;.&#34; someInt=1 someFloat=0.1 someStruct={StringField: First line,
second line.}
</code></pre><h3 id=contextual-logging>Contextual Logging</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [alpha]</code></div><p>Contextual logging builds on top of structured logging. It is primarily about
how developers use logging calls: code based on that concept is more flexible
and supports additional use cases as described in the <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging>Contextual Logging
KEP</a>.</p><p>If developers use additional functions like <code>WithValues</code> or <code>WithName</code> in
their components, then log entries contain additional information that gets
passed into functions by their caller.</p><p>Currently this is gated behind the <code>StructuredLogging</code> feature gate and
disabled by default. The infrastructure for this was added in 1.24 without
modifying components. The
<a href=https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go><code>component-base/logs/example</code></a>
command demonstrates how to use the new logging calls and how a component
behaves that supports contextual logging.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:navy;font-weight:700>$</span> <span style=color:#a2f>cd</span> <span style=color:#b8860b>$GOPATH</span>/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/
</span></span><span style=display:flex><span><span style=color:navy;font-weight:700>$</span> go run . --help
</span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span><span style=display:flex><span><span style=color:#888>      --feature-gates mapStringBool  A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:
</span></span></span><span style=display:flex><span><span style=color:#888>                                     AllAlpha=true|false (ALPHA - default=false)
</span></span></span><span style=display:flex><span><span style=color:#888>                                     AllBeta=true|false (BETA - default=false)
</span></span></span><span style=display:flex><span><span style=color:#888>                                     ContextualLogging=true|false (ALPHA - default=false)
</span></span></span><span style=display:flex><span><span style=color:#888></span><span style=color:navy;font-weight:700>$</span> go run . --feature-gates <span style=color:#b8860b>ContextualLogging</span><span style=color:#666>=</span><span style=color:#a2f>true</span>
</span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span><span style=display:flex><span><span style=color:#888>I0404 18:00:02.916429  451895 logger.go:94] &#34;example/myname: runtime&#34; foo=&#34;bar&#34; duration=&#34;1m0s&#34;
</span></span></span><span style=display:flex><span><span style=color:#888>I0404 18:00:02.916447  451895 logger.go:95] &#34;example: another runtime&#34; foo=&#34;bar&#34; duration=&#34;1m0s&#34;
</span></span></span></code></pre></div><p>The <code>example</code> prefix and <code>foo="bar"</code> were added by the caller of the function
which logs the <code>runtime</code> message and <code>duration="1m0s"</code> value, without having to
modify that function.</p><p>With contextual logging disable, <code>WithValues</code> and <code>WithName</code> do nothing and log
calls go through the global klog logger. Therefore this additional information
is not in the log output anymore:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:navy;font-weight:700>$</span> go run . --feature-gates <span style=color:#b8860b>ContextualLogging</span><span style=color:#666>=</span><span style=color:#a2f>false</span>
</span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span><span style=display:flex><span><span style=color:#888>I0404 18:03:31.171945  452150 logger.go:94] &#34;runtime&#34; duration=&#34;1m0s&#34;
</span></span></span><span style=display:flex><span><span style=color:#888>I0404 18:03:31.171962  452150 logger.go:95] &#34;another runtime&#34; duration=&#34;1m0s&#34;
</span></span></span></code></pre></div><h3 id=json-log-format>JSON log format</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [alpha]</code></div><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>JSON output does not support many standard klog flags. For list of unsupported klog flags, see the
<a href=/docs/reference/command-line-tools-reference/>Command line tool reference</a>.</p><p>Not all logs are guaranteed to be written in JSON format (for example, during process start).
If you intend to parse logs, make sure you can handle log lines that are not JSON as well.</p><p>Field names and JSON serialization are subject to change.</p></div><p>The <code>--logging-format=json</code> flag changes the format of logs from klog native format to JSON format.
Example of JSON log format (pretty printed):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;ts&#34;</span>: <span style=color:#666>1580306777.04728</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;v&#34;</span>: <span style=color:#666>4</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;msg&#34;</span>: <span style=color:#b44>&#34;Pod status updated&#34;</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;pod&#34;</span>:{
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;nginx-1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
</span></span><span style=display:flex><span>   },
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;ready&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Keys with special meaning:</p><ul><li><code>ts</code> - timestamp as Unix time (required, float)</li><li><code>v</code> - verbosity (only for info and not for error messages, int)</li><li><code>err</code> - error string (optional, string)</li><li><code>msg</code> - message (required, string)</li></ul><p>List of components currently supporting JSON format:</p><ul><li><a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li><li><a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li><li><a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li><li><a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a></li></ul><h3 id=log-verbosity-level>Log verbosity level</h3><p>The <code>-v</code> flag controls log verbosity. Increasing the value increases the number of logged events.
Decreasing the value decreases the number of logged events. Increasing verbosity settings logs
increasingly less severe events. A verbosity setting of 0 logs only critical events.</p><h3 id=log-location>Log location</h3><p>There are two types of system components: those that run in a container and those
that do not run in a container. For example:</p><ul><li>The Kubernetes scheduler and kube-proxy run in a container.</li><li>The kubelet and <a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>
do not run in containers.</li></ul><p>On machines with systemd, the kubelet and container runtime write to journald.
Otherwise, they write to <code>.log</code> files in the <code>/var/log</code> directory.
System components inside containers always write to <code>.log</code> files in the <code>/var/log</code> directory,
bypassing the default logging mechanism.
Similar to the container logs, you should rotate system component logs in the <code>/var/log</code> directory.
In Kubernetes clusters created by the <code>kube-up.sh</code> script, log rotation is configured by the <code>logrotate</code> tool.
The <code>logrotate</code> tool rotates logs daily, or once the log size is greater than 100MB.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about the <a href=/docs/concepts/cluster-administration/logging/>Kubernetes Logging Architecture</a></li><li>Read about <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging>Structured Logging</a></li><li>Read about <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging>Contextual Logging</a></li><li>Read about <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components>deprecation of klog flags</a></li><li>Read about the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>Conventions for logging severity</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3da54ad355f6fe6574d67bd9a9a42bcb>7 - Traces For Kubernetes System Components</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code></div><p>System component traces record the latency of and relationships between operations in the cluster.</p><p>Kubernetes components emit traces using the
<a href=https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md#opentelemetry-protocol-specification>OpenTelemetry Protocol</a>
with the gRPC exporter and can be collected and routed to tracing backends using an
<a href=https://github.com/open-telemetry/opentelemetry-collector#-opentelemetry-collector>OpenTelemetry Collector</a>.</p><h2 id=trace-collection>Trace Collection</h2><p>For a complete guide to collecting traces and using the collector, see
<a href=https://opentelemetry.io/docs/collector/getting-started/>Getting Started with the OpenTelemetry Collector</a>.
However, there are a few things to note that are specific to Kubernetes components.</p><p>By default, Kubernetes components export traces using the grpc exporter for OTLP on the
<a href="https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=opentelemetry">IANA OpenTelemetry port</a>, 4317.
As an example, if the collector is running as a sidecar to a Kubernetes component,
the following receiver configuration will collect spans and log them to standard output:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>receivers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>otlp</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocols</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>grpc</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>exporters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Replace this exporter with the exporter for your backend</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>logging</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>logLevel</span>:<span style=color:#bbb> </span>debug<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>pipelines</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>traces</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>receivers</span>:<span style=color:#bbb> </span>[otlp]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>exporters</span>:<span style=color:#bbb> </span>[logging]<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=component-traces>Component traces</h2><h3 id=kube-apiserver-traces>kube-apiserver traces</h3><p>The kube-apiserver generates spans for incoming HTTP requests, and for outgoing requests
to webhooks, etcd, and re-entrant requests. It propagates the
<a href=https://www.w3.org/TR/trace-context/>W3C Trace Context</a> with outgoing requests
but does not make use of the trace context attached to incoming requests,
as the kube-apiserver is often a public endpoint.</p><h4 id=enabling-tracing-in-the-kube-apiserver>Enabling tracing in the kube-apiserver</h4><p>To enable tracing, enable the <code>APIServerTracing</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the kube-apiserver. Also, provide the kube-apiserver with a tracing configuration file
with <code>--tracing-config-file=&lt;path-to-config></code>. This is an example config that records
spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1alpha1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>TracingConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># default value</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic>#endpoint: localhost:4317</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>samplingRatePerMillion</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>For more information about the <code>TracingConfiguration</code> struct, see
<a href=/docs/reference/config-api/apiserver-config.v1alpha1/#apiserver-k8s-io-v1alpha1-TracingConfiguration>API server config API (v1alpha1)</a>.</p><h3 id=kubelet-traces>kubelet traces</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><p>The kubelet CRI interface and authenticated http servers are instrumented to generate
trace spans. As with the apiserver, the endpoint and sampling rate are configurable.
Trace context propagation is also configured. A parent span's sampling decision is always respected.
A provided tracing configuration sampling rate will apply to spans without a parent.
Enabled without a configured endpoint, the default OpenTelemetry Collector reciever address of "localhost:4317" is set.</p><h4 id=enabling-tracing-in-the-kubelet>Enabling tracing in the kubelet</h4><p>To enable tracing, enable the <code>KubeletTracing</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the kubelet. Also, provide the kubelet with a
<a href=https://github.com/kubernetes/component-base/blob/release-1.25/tracing/api/v1/types.go>tracing configuration</a>.
This is an example snippet of a kubelet config that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>featureGates</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>KubeletTracing</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>tracing</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># default value</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic>#endpoint: localhost:4317</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>samplingRatePerMillion</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=stability>Stability</h2><p>Tracing instrumentation is still under active development, and may change
in a variety of ways. This includes span names, attached attributes,
instrumented endpoints, etc. Until this feature graduates to stable,
there are no guarantees of backwards compatibility for tracing instrumentation.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=https://opentelemetry.io/docs/collector/getting-started/>Getting Started with the OpenTelemetry Collector</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-08e94e6a480e0d6b2de72d84a1b97617>8 - Proxies in Kubernetes</h1><p>This page explains proxies used with Kubernetes.</p><h2 id=proxies>Proxies</h2><p>There are several different proxies you may encounter when using Kubernetes:</p><ol><li><p>The <a href=/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api>kubectl proxy</a>:</p><ul><li>runs on a user's desktop or in a pod</li><li>proxies from a localhost address to the Kubernetes apiserver</li><li>client to proxy uses HTTP</li><li>proxy to apiserver uses HTTPS</li><li>locates apiserver</li><li>adds authentication headers</li></ul></li><li><p>The <a href=/docs/tasks/access-application-cluster/access-cluster-services/#discovering-builtin-services>apiserver proxy</a>:</p><ul><li>is a bastion built into the apiserver</li><li>connects a user outside of the cluster to cluster IPs which otherwise might not be reachable</li><li>runs in the apiserver processes</li><li>client to proxy uses HTTPS (or http if apiserver so configured)</li><li>proxy to target may use HTTP or HTTPS as chosen by proxy using available information</li><li>can be used to reach a Node, Pod, or Service</li><li>does load balancing when used to reach a Service</li></ul></li><li><p>The <a href=/docs/concepts/services-networking/service/#ips-and-vips>kube proxy</a>:</p><ul><li>runs on each node</li><li>proxies UDP, TCP and SCTP</li><li>does not understand HTTP</li><li>provides load balancing</li><li>is only used to reach services</li></ul></li><li><p>A Proxy/Load-balancer in front of apiserver(s):</p><ul><li>existence and implementation varies from cluster to cluster (e.g. nginx)</li><li>sits between all clients and one or more apiservers</li><li>acts as load balancer if there are several apiservers.</li></ul></li><li><p>Cloud Load Balancers on external services:</p><ul><li>are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)</li><li>are created automatically when the Kubernetes service has type <code>LoadBalancer</code></li><li>usually supports UDP/TCP only</li><li>SCTP support is up to the load balancer implementation of the cloud provider</li><li>implementation varies by cloud provider.</li></ul></li></ol><p>Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin
will typically ensure that the latter types are set up correctly.</p><h2 id=requesting-redirects>Requesting redirects</h2><p>Proxies have replaced redirect capabilities. Redirects have been deprecated.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-31c9327d2332c585341b64ddafa19cdd>9 - API Priority and Fairness</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code></div><p>Controlling the behavior of the Kubernetes API server in an overload situation
is a key task for cluster administrators. The <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a> has some controls available
(i.e. the <code>--max-requests-inflight</code> and <code>--max-mutating-requests-inflight</code>
command-line flags) to limit the amount of outstanding work that will be
accepted, preventing a flood of inbound requests from overloading and
potentially crashing the API server, but these flags are not enough to ensure
that the most important requests get through in a period of high traffic.</p><p>The API Priority and Fairness feature (APF) is an alternative that improves upon
aforementioned max-inflight limitations. APF classifies
and isolates requests in a more fine-grained way. It also introduces
a limited amount of queuing, so that no requests are rejected in cases
of very brief bursts. Requests are dispatched from queues using a
fair queuing technique so that, for example, a poorly-behaved
<a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> need not
starve others (even at the same priority level).</p><p>This feature is designed to work well with standard controllers, which
use informers and react to failures of API requests with exponential
back-off, and other clients that also work this way.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Some requests classified as "long-running"—such as remote
command execution or log tailing—are not subject to the API
Priority and Fairness filter. This is also true for the
<code>--max-requests-inflight</code> flag without the API Priority and Fairness
feature enabled. API Priority and Fairness <em>does</em> apply to <strong>watch</strong>
requests. When API Priority and Fairness is disabled, <strong>watch</strong> requests
are not subject to the <code>--max-requests-inflight</code> limit.</div><h2 id=enabling-disabling-api-priority-and-fairness>Enabling/Disabling API Priority and Fairness</h2><p>The API Priority and Fairness feature is controlled by a feature gate
and is enabled by default. See <a href=/docs/reference/command-line-tools-reference/feature-gates/>Feature
Gates</a>
for a general explanation of feature gates and how to enable and
disable them. The name of the feature gate for APF is
"APIPriorityAndFairness". This feature also involves an <a class=glossary-tooltip title='A set of related paths in the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning target=_blank aria-label='API Group'>API Group</a> with: (a) a
<code>v1alpha1</code> version and a <code>v1beta1</code> version, disabled by default, and
(b) <code>v1beta2</code> and <code>v1beta3</code> versions, enabled by default. You can
disable the feature gate and API group beta versions by adding the
following command-line flags to your <code>kube-apiserver</code> invocation:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kube-apiserver <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--feature-gates<span style=color:#666>=</span><span style=color:#b8860b>APIPriorityAndFairness</span><span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--runtime-config<span style=color:#666>=</span>flowcontrol.apiserver.k8s.io/v1beta2<span style=color:#666>=</span>false,flowcontrol.apiserver.k8s.io/v1beta3<span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span> <span style=color:#080;font-style:italic># …and other flags as usual</span>
</span></span></code></pre></div><p>Alternatively, you can enable the v1alpha1 and v1beta1 versions of the API group
with <code>--runtime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true,flowcontrol.apiserver.k8s.io/v1beta1=true</code>.</p><p>The command-line flag <code>--enable-priority-and-fairness=false</code> will disable the
API Priority and Fairness feature, even if other flags have enabled it.</p><h2 id=concepts>Concepts</h2><p>There are several distinct features involved in the API Priority and Fairness
feature. Incoming requests are classified by attributes of the request using
<em>FlowSchemas</em>, and assigned to priority levels. Priority levels add a degree of
isolation by maintaining separate concurrency limits, so that requests assigned
to different priority levels cannot starve each other. Within a priority level,
a fair-queuing algorithm prevents requests from different <em>flows</em> from starving
each other, and allows for requests to be queued to prevent bursty traffic from
causing failed requests when the average load is acceptably low.</p><h3 id=priority-levels>Priority Levels</h3><p>Without APF enabled, overall concurrency in the API server is limited by the
<code>kube-apiserver</code> flags <code>--max-requests-inflight</code> and
<code>--max-mutating-requests-inflight</code>. With APF enabled, the concurrency limits
defined by these flags are summed and then the sum is divided up among a
configurable set of <em>priority levels</em>. Each incoming request is assigned to a
single priority level, and each priority level will only dispatch as many
concurrent requests as its particular limit allows.</p><p>The default configuration, for example, includes separate priority levels for
leader-election requests, requests from built-in controllers, and requests from
Pods. This means that an ill-behaved Pod that floods the API server with
requests cannot prevent leader election or actions by the built-in controllers
from succeeding.</p><p>The concurrency limits of the priority levels are periodically
adjusted, allowing under-utilized priority levels to temporarily lend
concurrency to heavily-utilized levels. These limits are based on
nominal limits and bounds on how much concurrency a priority level may
lend and how much it may borrow, all derived from the configuration
objects mentioned below.</p><h3 id=seats-occupied-by-a-request>Seats Occupied by a Request</h3><p>The above description of concurrency management is the baseline story.
In it, requests have different durations but are counted equally at
any given moment when comparing against a priority level's concurrency
limit. In the baseline story, each request occupies one unit of
concurrency. The word "seat" is used to mean one unit of concurrency,
inspired by the way each passenger on a train or aircraft takes up one
of the fixed supply of seats.</p><p>But some requests take up more than one seat. Some of these are <strong>list</strong>
requests that the server estimates will return a large number of
objects. These have been found to put an exceptionally heavy burden
on the server, among requests that take a similar amount of time to
run. For this reason, the server estimates the number of objects that
will be returned and considers the request to take a number of seats
that is proportional to that estimated number.</p><h3 id=execution-time-tweaks-for-watch-requests>Execution time tweaks for watch requests</h3><p>API Priority and Fairness manages <strong>watch</strong> requests, but this involves a
couple more excursions from the baseline behavior. The first concerns
how long a <strong>watch</strong> request is considered to occupy its seat. Depending
on request parameters, the response to a <strong>watch</strong> request may or may not
begin with <strong>create</strong> notifications for all the relevant pre-existing
objects. API Priority and Fairness considers a <strong>watch</strong> request to be
done with its seat once that initial burst of notifications, if any,
is over.</p><p>The normal notifications are sent in a concurrent burst to all
relevant <strong>watch</strong> response streams whenever the server is notified of an
object create/update/delete. To account for this work, API Priority
and Fairness considers every write request to spend some additional
time occupying seats after the actual writing is done. The server
estimates the number of notifications to be sent and adjusts the write
request's number of seats and seat occupancy time to include this
extra work.</p><h3 id=queuing>Queuing</h3><p>Even within a priority level there may be a large number of distinct sources of
traffic. In an overload situation, it is valuable to prevent one stream of
requests from starving others (in particular, in the relatively common case of a
single buggy client flooding the kube-apiserver with requests, that buggy client
would ideally not have much measurable impact on other clients at all). This is
handled by use of a fair-queuing algorithm to process requests that are assigned
the same priority level. Each request is assigned to a <em>flow</em>, identified by the
name of the matching FlowSchema plus a <em>flow distinguisher</em> — which
is either the requesting user, the target resource's namespace, or nothing — and the
system attempts to give approximately equal weight to requests in different
flows of the same priority level.
To enable distinct handling of distinct instances, controllers that have
many instances should authenticate with distinct usernames</p><p>After classifying a request into a flow, the API Priority and Fairness
feature then may assign the request to a queue. This assignment uses
a technique known as <a class=glossary-tooltip title='A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-shuffle-sharding' target=_blank aria-label='shuffle sharding'>shuffle sharding</a>, which makes relatively efficient use of
queues to insulate low-intensity flows from high-intensity flows.</p><p>The details of the queuing algorithm are tunable for each priority level, and
allow administrators to trade off memory use, fairness (the property that
independent flows will all make progress when total traffic exceeds capacity),
tolerance for bursty traffic, and the added latency induced by queuing.</p><h3 id=exempt-requests>Exempt requests</h3><p>Some requests are considered sufficiently important that they are not subject to
any of the limitations imposed by this feature. These exemptions prevent an
improperly-configured flow control configuration from totally disabling an API
server.</p><h2 id=resources>Resources</h2><p>The flow control API involves two kinds of resources.
<a href=/docs/reference/generated/kubernetes-api/v1.25/#prioritylevelconfiguration-v1beta2-flowcontrol-apiserver-k8s-io>PriorityLevelConfigurations</a>
define the available priority levels, the share of the available concurrency
budget that each can handle, and allow for fine-tuning queuing behavior.
<a href=/docs/reference/generated/kubernetes-api/v1.25/#flowschema-v1beta2-flowcontrol-apiserver-k8s-io>FlowSchemas</a>
are used to classify individual inbound requests, matching each to a
single PriorityLevelConfiguration. There is also a <code>v1alpha1</code> version
of the same API group, and it has the same Kinds with the same syntax and
semantics.</p><h3 id=prioritylevelconfiguration>PriorityLevelConfiguration</h3><p>A PriorityLevelConfiguration represents a single priority level. Each
PriorityLevelConfiguration has an independent limit on the number of outstanding
requests, and limitations on the number of queued requests.</p><p>The nominal oncurrency limit for a PriorityLevelConfiguration is not
specified in an absolute number of seats, but rather in "nominal
concurrency shares." The total concurrency limit for the API Server is
distributed among the existing PriorityLevelConfigurations in
proportion to these shares, to give each level its nominal limit in
terms of seats. This allows a cluster administrator to scale up or
down the total amount of traffic to a server by restarting
<code>kube-apiserver</code> with a different value for <code>--max-requests-inflight</code>
(or <code>--max-mutating-requests-inflight</code>), and all
PriorityLevelConfigurations will see their maximum allowed concurrency
go up (or down) by the same fraction.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> In the versions before <code>v1beta3</code> the relevant
PriorityLevelConfiguration field is named "assured concurrency shares"
rather than "nominal concurrency shares". Also, in Kubernetes release
1.25 and earlier there were no periodic adjustments: the
nominal/assured limits were always applied without adjustment.</div><p>The bounds on how much concurrency a priority level may lend and how
much it may borrow are expressed in the PriorityLevelConfiguration as
percentages of the level's nominal limit. These are resolved to
absolute numbers of seats by multiplying with the nominal limit /
100.0 and rounding. The dynamically adjusted concurrency limit of a
priority level is constrained to lie between (a) a lower bound of its
nominal limit minus its lendable seats and (b) an upper bound of its
nominal limit plus the seats it may borrow. At each adjustment the
dynamic limits are derived by each priority level reclaiming any lent
seats for which demand recently appeared and then jointly fairly
responding to the recent seat demand on the priority levels, within
the bounds just described.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> With the Priority and Fairness feature enabled, the total concurrency limit for
the server is set to the sum of <code>--max-requests-inflight</code> and
<code>--max-mutating-requests-inflight</code>. There is no longer any distinction made
between mutating and non-mutating requests; if you want to treat them
separately for a given resource, make separate FlowSchemas that match the
mutating and non-mutating verbs respectively.</div><p>When the volume of inbound requests assigned to a single
PriorityLevelConfiguration is more than its permitted concurrency level, the
<code>type</code> field of its specification determines what will happen to extra requests.
A type of <code>Reject</code> means that excess traffic will immediately be rejected with
an HTTP 429 (Too Many Requests) error. A type of <code>Queue</code> means that requests
above the threshold will be queued, with the shuffle sharding and fair queuing techniques used
to balance progress between request flows.</p><p>The queuing configuration allows tuning the fair queuing algorithm for a
priority level. Details of the algorithm can be read in the
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness>enhancement proposal</a>, but in short:</p><ul><li><p>Increasing <code>queues</code> reduces the rate of collisions between different flows, at
the cost of increased memory usage. A value of 1 here effectively disables the
fair-queuing logic, but still allows requests to be queued.</p></li><li><p>Increasing <code>queueLengthLimit</code> allows larger bursts of traffic to be
sustained without dropping any requests, at the cost of increased
latency and memory usage.</p></li><li><p>Changing <code>handSize</code> allows you to adjust the probability of collisions between
different flows and the overall concurrency available to a single flow in an
overload situation.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A larger <code>handSize</code> makes it less likely for two individual flows to collide
(and therefore for one to be able to starve the other), but more likely that
a small number of flows can dominate the apiserver. A larger <code>handSize</code> also
potentially increases the amount of latency that a single high-traffic flow
can cause. The maximum number of queued requests possible from a
single flow is <code>handSize * queueLengthLimit</code>.</div></li></ul><p>Following is a table showing an interesting collection of shuffle
sharding configurations, showing for each the probability that a
given mouse (low-intensity flow) is squished by the elephants (high-intensity flows) for
an illustrative collection of numbers of elephants. See
<a href=https://play.golang.org/p/Gi0PLgVHiUg>https://play.golang.org/p/Gi0PLgVHiUg</a> , which computes this table.</p><table><caption style=display:none>Example Shuffle Sharding Configurations</caption><thead><tr><th>HandSize</th><th>Queues</th><th>1 elephant</th><th>4 elephants</th><th>16 elephants</th></tr></thead><tbody><tr><td>12</td><td>32</td><td>4.428838398950118e-09</td><td>0.11431348830099144</td><td>0.9935089607656024</td></tr><tr><td>10</td><td>32</td><td>1.550093439632541e-08</td><td>0.0626479840223545</td><td>0.9753101519027554</td></tr><tr><td>10</td><td>64</td><td>6.601827268370426e-12</td><td>0.00045571320990370776</td><td>0.49999929150089345</td></tr><tr><td>9</td><td>64</td><td>3.6310049976037345e-11</td><td>0.00045501212304112273</td><td>0.4282314876454858</td></tr><tr><td>8</td><td>64</td><td>2.25929199850899e-10</td><td>0.0004886697053040446</td><td>0.35935114681123076</td></tr><tr><td>8</td><td>128</td><td>6.994461389026097e-13</td><td>3.4055790161620863e-06</td><td>0.02746173137155063</td></tr><tr><td>7</td><td>128</td><td>1.0579122850901972e-11</td><td>6.960839379258192e-06</td><td>0.02406157386340147</td></tr><tr><td>7</td><td>256</td><td>7.597695465552631e-14</td><td>6.728547142019406e-08</td><td>0.0006709661542533682</td></tr><tr><td>6</td><td>256</td><td>2.7134626662687968e-12</td><td>2.9516464018476436e-07</td><td>0.0008895654642000348</td></tr><tr><td>6</td><td>512</td><td>4.116062922897309e-14</td><td>4.982983350480894e-09</td><td>2.26025764343413e-05</td></tr><tr><td>6</td><td>1024</td><td>6.337324016514285e-16</td><td>8.09060164312957e-11</td><td>4.517408062903668e-07</td></tr></tbody></table><h3 id=flowschema>FlowSchema</h3><p>A FlowSchema matches some inbound requests and assigns them to a
priority level. Every inbound request is tested against every
FlowSchema in turn, starting with those with numerically lowest ---
which we take to be the logically highest --- <code>matchingPrecedence</code> and
working onward. The first match wins.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Only the first matching FlowSchema for a given request matters. If multiple
FlowSchemas match a single inbound request, it will be assigned based on the one
with the highest <code>matchingPrecedence</code>. If multiple FlowSchemas with equal
<code>matchingPrecedence</code> match the same request, the one with lexicographically
smaller <code>name</code> will win, but it's better not to rely on this, and instead to
ensure that no two FlowSchemas have the same <code>matchingPrecedence</code>.</div><p>A FlowSchema matches a given request if at least one of its <code>rules</code>
matches. A rule matches if at least one of its <code>subjects</code> <em>and</em> at least
one of its <code>resourceRules</code> or <code>nonResourceRules</code> (depending on whether the
incoming request is for a resource or non-resource URL) matches the request.</p><p>For the <code>name</code> field in subjects, and the <code>verbs</code>, <code>apiGroups</code>, <code>resources</code>,
<code>namespaces</code>, and <code>nonResourceURLs</code> fields of resource and non-resource rules,
the wildcard <code>*</code> may be specified to match all values for the given field,
effectively removing it from consideration.</p><p>A FlowSchema's <code>distinguisherMethod.type</code> determines how requests matching that
schema will be separated into flows. It may be
either <code>ByUser</code>, in which case one requesting user will not be able to starve
other users of capacity, or <code>ByNamespace</code>, in which case requests for resources
in one namespace will not be able to starve requests for resources in other
namespaces of capacity, or it may be blank (or <code>distinguisherMethod</code> may be
omitted entirely), in which case all requests matched by this FlowSchema will be
considered part of a single flow. The correct choice for a given FlowSchema
depends on the resource and your particular environment.</p><h2 id=defaults>Defaults</h2><p>Each kube-apiserver maintains two sorts of APF configuration objects:
mandatory and suggested.</p><h3 id=mandatory-configuration-objects>Mandatory Configuration Objects</h3><p>The four mandatory configuration objects reflect fixed built-in
guardrail behavior. This is behavior that the servers have before
those objects exist, and when those objects exist their specs reflect
this behavior. The four mandatory objects are as follows.</p><ul><li><p>The mandatory <code>exempt</code> priority level is used for requests that are
not subject to flow control at all: they will always be dispatched
immediately. The mandatory <code>exempt</code> FlowSchema classifies all
requests from the <code>system:masters</code> group into this priority
level. You may define other FlowSchemas that direct other requests
to this priority level, if appropriate.</p></li><li><p>The mandatory <code>catch-all</code> priority level is used in combination with
the mandatory <code>catch-all</code> FlowSchema to make sure that every request
gets some kind of classification. Typically you should not rely on
this catch-all configuration, and should create your own catch-all
FlowSchema and PriorityLevelConfiguration (or use the suggested
<code>global-default</code> priority level that is installed by default) as
appropriate. Because it is not expected to be used normally, the
mandatory <code>catch-all</code> priority level has a very small concurrency
share and does not queue requests.</p></li></ul><h3 id=suggested-configuration-objects>Suggested Configuration Objects</h3><p>The suggested FlowSchemas and PriorityLevelConfigurations constitute a
reasonable default configuration. You can modify these and/or create
additional configuration objects if you want. If your cluster is
likely to experience heavy load then you should consider what
configuration will work best.</p><p>The suggested configuration groups requests into six priority levels:</p><ul><li><p>The <code>node-high</code> priority level is for health updates from nodes.</p></li><li><p>The <code>system</code> priority level is for non-health requests from the
<code>system:nodes</code> group, i.e. Kubelets, which must be able to contact
the API server in order for workloads to be able to schedule on
them.</p></li><li><p>The <code>leader-election</code> priority level is for leader election requests from
built-in controllers (in particular, requests for <code>endpoints</code>, <code>configmaps</code>,
or <code>leases</code> coming from the <code>system:kube-controller-manager</code> or
<code>system:kube-scheduler</code> users and service accounts in the <code>kube-system</code>
namespace). These are important to isolate from other traffic because failures
in leader election cause their controllers to fail and restart, which in turn
causes more expensive traffic as the new controllers sync their informers.</p></li><li><p>The <code>workload-high</code> priority level is for other requests from built-in
controllers.</p></li><li><p>The <code>workload-low</code> priority level is for requests from any other service
account, which will typically include all requests from controllers running in
Pods.</p></li><li><p>The <code>global-default</code> priority level handles all other traffic, e.g.
interactive <code>kubectl</code> commands run by nonprivileged users.</p></li></ul><p>The suggested FlowSchemas serve to steer requests into the above
priority levels, and are not enumerated here.</p><h3 id=maintenance-of-the-mandatory-and-suggested-configuration-objects>Maintenance of the Mandatory and Suggested Configuration Objects</h3><p>Each <code>kube-apiserver</code> independently maintains the mandatory and
suggested configuration objects, using initial and periodic behavior.
Thus, in a situation with a mixture of servers of different versions
there may be thrashing as long as different servers have different
opinions of the proper content of these objects.</p><p>Each <code>kube-apiserver</code> makes an initial maintenance pass over the
mandatory and suggested configuration objects, and after that does
periodic maintenance (once per minute) of those objects.</p><p>For the mandatory configuration objects, maintenance consists of
ensuring that the object exists and, if it does, has the proper spec.
The server refuses to allow a creation or update with a spec that is
inconsistent with the server's guardrail behavior.</p><p>Maintenance of suggested configuration objects is designed to allow
their specs to be overridden. Deletion, on the other hand, is not
respected: maintenance will restore the object. If you do not want a
suggested configuration object then you need to keep it around but set
its spec to have minimal consequences. Maintenance of suggested
objects is also designed to support automatic migration when a new
version of the <code>kube-apiserver</code> is rolled out, albeit potentially with
thrashing while there is a mixed population of servers.</p><p>Maintenance of a suggested configuration object consists of creating
it --- with the server's suggested spec --- if the object does not
exist. OTOH, if the object already exists, maintenance behavior
depends on whether the <code>kube-apiservers</code> or the users control the
object. In the former case, the server ensures that the object's spec
is what the server suggests; in the latter case, the spec is left
alone.</p><p>The question of who controls the object is answered by first looking
for an annotation with key <code>apf.kubernetes.io/autoupdate-spec</code>. If
there is such an annotation and its value is <code>true</code> then the
kube-apiservers control the object. If there is such an annotation
and its value is <code>false</code> then the users control the object. If
neither of those condtions holds then the <code>metadata.generation</code> of the
object is consulted. If that is 1 then the kube-apiservers control
the object. Otherwise the users control the object. These rules were
introduced in release 1.22 and their consideration of
<code>metadata.generation</code> is for the sake of migration from the simpler
earlier behavior. Users who wish to control a suggested configuration
object should set its <code>apf.kubernetes.io/autoupdate-spec</code> annotation
to <code>false</code>.</p><p>Maintenance of a mandatory or suggested configuration object also
includes ensuring that it has an <code>apf.kubernetes.io/autoupdate-spec</code>
annotation that accurately reflects whether the kube-apiservers
control the object.</p><p>Maintenance also includes deleting objects that are neither mandatory
nor suggested but are annotated
<code>apf.kubernetes.io/autoupdate-spec=true</code>.</p><h2 id=health-check-concurrency-exemption>Health check concurrency exemption</h2><p>The suggested configuration gives no special treatment to the health
check requests on kube-apiservers from their local kubelets --- which
tend to use the secured port but supply no credentials. With the
suggested config, these requests get assigned to the <code>global-default</code>
FlowSchema and the corresponding <code>global-default</code> priority level,
where other traffic can crowd them out.</p><p>If you add the following additional FlowSchema, this exempts those
requests from rate limiting.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Making this change also allows any hostile party to then send
health-check requests that match this FlowSchema, at any volume they
like. If you have a web traffic filter or similar external security
mechanism to protect your cluster's API server from general internet
traffic, you can configure rules to block any health check requests
that originate from outside your cluster.</div><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/priority-and-fairness/health-for-strangers.yaml download=priority-and-fairness/health-for-strangers.yaml><code>priority-and-fairness/health-for-strangers.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("priority-and-fairness-health-for-strangers-yaml")' title="Copy priority-and-fairness/health-for-strangers.yaml to clipboard"></img></div><div class=includecode id=priority-and-fairness-health-for-strangers-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>flowcontrol.apiserver.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>FlowSchema<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>health-for-strangers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>matchingPrecedence</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityLevelConfiguration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>exempt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>nonResourceRules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>nonResourceURLs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/healthz&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/livez&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/readyz&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Group<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>group</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:unauthenticated<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h2 id=diagnostics>Diagnostics</h2><p>Every HTTP response from an API server with the priority and fairness feature
enabled has two extra headers: <code>X-Kubernetes-PF-FlowSchema-UID</code> and
<code>X-Kubernetes-PF-PriorityLevel-UID</code>, noting the flow schema that matched the request
and the priority level to which it was assigned, respectively. The API objects'
names are not included in these headers in case the requesting user does not
have permission to view them, so when debugging you can use a command like</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get flowschemas -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span><span style=display:flex><span>kubectl get prioritylevelconfigurations -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span></code></pre></div><p>to get a mapping of UIDs to names for both FlowSchemas and
PriorityLevelConfigurations.</p><h2 id=observability>Observability</h2><h3 id=metrics>Metrics</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In versions of Kubernetes before v1.20, the labels <code>flow_schema</code> and
<code>priority_level</code> were inconsistently named <code>flowSchema</code> and <code>priorityLevel</code>,
respectively. If you're running Kubernetes versions v1.19 and earlier, you
should refer to the documentation for your version.</div><p>When you enable the API Priority and Fairness feature, the kube-apiserver
exports additional metrics. Monitoring these can help you determine whether your
configuration is inappropriately throttling important traffic, or find
poorly-behaved workloads that may be harming system health.</p><ul><li><p><code>apiserver_flowcontrol_rejected_requests_total</code> is a counter vector
(cumulative since server start) of requests that were rejected,
broken down by the labels <code>flow_schema</code> (indicating the one that
matched the request), <code>priority_level</code> (indicating the one to which
the request was assigned), and <code>reason</code>. The <code>reason</code> label will be
have one of the following values:</p><ul><li><code>queue-full</code>, indicating that too many requests were already
queued,</li><li><code>concurrency-limit</code>, indicating that the
PriorityLevelConfiguration is configured to reject rather than
queue excess requests, or</li><li><code>time-out</code>, indicating that the request was still in the queue
when its queuing time limit expired.</li></ul></li><li><p><code>apiserver_flowcontrol_dispatched_requests_total</code> is a counter
vector (cumulative since server start) of requests that began
executing, broken down by the labels <code>flow_schema</code> (indicating the
one that matched the request) and <code>priority_level</code> (indicating the
one to which the request was assigned).</p></li><li><p><code>apiserver_current_inqueue_requests</code> is a gauge vector of recent
high water marks of the number of queued requests, grouped by a
label named <code>request_kind</code> whose value is <code>mutating</code> or <code>readOnly</code>.
These high water marks describe the largest number seen in the one
second window most recently completed. These complement the older
<code>apiserver_current_inflight_requests</code> gauge vector that holds the
last window's high water mark of number of requests actively being
served.</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_request_count_samples</code> is a
histogram vector of observations of the then-current number of
requests, broken down by the labels <code>phase</code> (which takes on the
values <code>waiting</code> and <code>executing</code>) and <code>request_kind</code> (which takes on
the values <code>mutating</code> and <code>readOnly</code>). The observations are made
periodically at a high rate. Each observed value is a ratio,
between 0 and 1, of a number of requests divided by the
corresponding limit on the number of requests (queue length limit
for waiting and concurrency limit for executing).</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_request_count_watermarks</code> is a
histogram vector of high or low water marks of the number of
requests (divided by the corresponding limit to get a ratio in the
range 0 to 1) broken down by the labels <code>phase</code> (which takes on the
values <code>waiting</code> and <code>executing</code>) and <code>request_kind</code> (which takes on
the values <code>mutating</code> and <code>readOnly</code>); the label <code>mark</code> takes on
values <code>high</code> and <code>low</code>. The water marks are accumulated over
windows bounded by the times when an observation was added to
<code>apiserver_flowcontrol_read_vs_write_request_count_samples</code>. These
water marks show the range of values that occurred between samples.</p></li><li><p><code>apiserver_flowcontrol_current_inqueue_requests</code> is a gauge vector
holding the instantaneous number of queued (not executing) requests,
broken down by the labels <code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_current_executing_requests</code> is a gauge vector
holding the instantaneous number of executing (not waiting in a
queue) requests, broken down by the labels <code>priority_level</code> and
<code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_request_concurrency_in_use</code> is a gauge vector
holding the instantaneous number of occupied seats, broken down by
the labels <code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_count_samples</code> is a
histogram vector of observations of the then-current number of
requests broken down by the labels <code>phase</code> (which takes on the
values <code>waiting</code> and <code>executing</code>) and <code>priority_level</code>. Each
histogram gets observations taken periodically, up through the last
activity of the relevant sort. The observations are made at a high
rate. Each observed value is a ratio, between 0 and 1, of a number
of requests divided by the corresponding limit on the number of
requests (queue length limit for waiting and concurrency limit for
executing).</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_count_watermarks</code> is a
histogram vector of high or low water marks of the number of
requests (divided by the corresponding limit to get a ratio in the
range 0 to 1) broken down by the labels <code>phase</code> (which takes on the
values <code>waiting</code> and <code>executing</code>) and <code>priority_level</code>; the label
<code>mark</code> takes on values <code>high</code> and <code>low</code>. The water marks are
accumulated over windows bounded by the times when an observation
was added to
<code>apiserver_flowcontrol_priority_level_request_count_samples</code>. These
water marks show the range of values that occurred between samples.</p></li><li><p><code>apiserver_flowcontrol_priority_level_seat_count_samples</code> is a
histogram vector of observations of the utilization of a priority
level's concurrency limit, broken down by <code>priority_level</code>. This
utilization is the fraction (number of seats occupied) /
(concurrency limit). This metric considers all stages of execution
(both normal and the extra delay at the end of a write to cover for
the corresponding notification work) of all requests except WATCHes;
for those it considers only the initial stage that delivers
notifications of pre-existing objects. Each histogram in the vector
is also labeled with <code>phase: executing</code> (there is no seat limit for
the waiting phase). Each histogram gets observations taken
periodically, up through the last activity of the relevant sort.
The observations
are made at a high rate.</p></li><li><p><code>apiserver_flowcontrol_priority_level_seat_count_watermarks</code> is a
histogram vector of high or low water marks of the utilization of a
priority level's concurrency limit, broken down by <code>priority_level</code>
and <code>mark</code> (which takes on values <code>high</code> and <code>low</code>). Each histogram
in the vector is also labeled with <code>phase: executing</code> (there is no
seat limit for the waiting phase). The water marks are accumulated
over windows bounded by the times when an observation was added to
<code>apiserver_flowcontrol_priority_level_seat_count_samples</code>. These
water marks show the range of values that occurred between samples.</p></li><li><p><code>apiserver_flowcontrol_request_queue_length_after_enqueue</code> is a
histogram vector of queue lengths for the queues, broken down by
the labels <code>priority_level</code> and <code>flow_schema</code>, as sampled by the
enqueued requests. Each request that gets queued contributes one
sample to its histogram, reporting the length of the queue immediately
after the request was added. Note that this produces different
statistics than an unbiased survey would.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> An outlier value in a histogram here means it is likely that a single flow
(i.e., requests by one user or for one namespace, depending on
configuration) is flooding the API server, and being throttled. By contrast,
if one priority level's histogram shows that all queues for that priority
level are longer than those for other priority levels, it may be appropriate
to increase that PriorityLevelConfiguration's concurrency shares.</div></li><li><p><code>apiserver_flowcontrol_request_concurrency_limit</code> is the same as
<code>apiserver_flowcontrol_nominal_limit_seats</code>. Before the
introduction of concurrency borrowing between priority levels, this
was always equal to <code>apiserver_flowcontrol_current_limit_seats</code>
(which did not exist as a distinct metric).</p></li><li><p><code>apiserver_flowcontrol_nominal_limit_seats</code> is a gauge vector
holding each priority level's nominal concurrency limit, computed
from the API server's total concurrency limit and the priority
level's configured nominal concurrency shares.</p></li><li><p><code>apiserver_flowcontrol_lower_limit_seats</code> is a gauge vector holding
the lower bound on each priority level's dynamic concurrency limit.</p></li><li><p><code>apiserver_flowcontrol_upper_limit_seats</code> is a gauge vector holding
the upper bound on each priority level's dynamic concurrency limit.</p></li><li><p><code>apiserver_flowcontrol_demand_seats</code> is a histogram vector counting
observations, at the end of every nanosecond, of each priority
level's ratio of (seat demand) / (nominal concurrency limit). A
priority level's seat demand is the sum, over both queued requests
and those in the initial phase of execution, of the maximum of the
number of seats occupied in the request's initial and final
execution phases.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_high_watermark</code> is a gauge vector
holding, for each priority level, the maximum seat demand seen
during the last concurrency borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_average</code> is a gauge vector
holding, for each priority level, the time-weighted average seat
demand seen during the last concurrency borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_stdev</code> is a gauge vector
holding, for each priority level, the time-weighted population
standard deviation of seat demand seen during the last concurrency
borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_target_seats</code> is a gauge vector holding, for
each priority level, the concurrency target going into the borrowing
allocation problem.</p></li><li><p><code>apiserver_flowcontrol_seat_fair_frac</code> is a gauge holding the fair
allocation fraction determined in the last borrowing adjustment.</p></li><li><p><code>apiserver_flowcontrol_current_limit_seats</code> is a gauge vector
holding, for each priority level, the dynamic concurrency limit
derived in the last adjustment.</p></li><li><p><code>apiserver_flowcontrol_request_wait_duration_seconds</code> is a histogram
vector of how long requests spent queued, broken down by the labels
<code>flow_schema</code> (indicating which one matched the request),
<code>priority_level</code> (indicating the one to which the request was
assigned), and <code>execute</code> (indicating whether the request started
executing).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Since each FlowSchema always assigns requests to a single
PriorityLevelConfiguration, you can add the histograms for all the
FlowSchemas for one priority level to get the effective histogram for
requests assigned to that priority level.</div></li><li><p><code>apiserver_flowcontrol_request_execution_seconds</code> is a histogram
vector of how long requests took to actually execute, broken down by
the labels <code>flow_schema</code> (indicating which one matched the request)
and <code>priority_level</code> (indicating the one to which the request was
assigned).</p></li><li><p><code>apiserver_flowcontrol_watch_count_samples</code> is a histogram vector of
the number of active WATCH requests relevant to a given write,
broken down by <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_work_estimated_seats</code> is a histogram vector
of the number of estimated seats (maximum of initial and final stage
of execution) associated with requests, broken down by <code>flow_schema</code>
and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_request_dispatch_no_accommodation_total</code> is a
counter vec of the number of events that in principle could have led
to a request being dispatched but did not, due to lack of available
concurrency, broken down by <code>flow_schema</code> and <code>priority_level</code>. The
relevant sorts of events are arrival of a request and completion of
a request.</p></li></ul><h3 id=debug-endpoints>Debug endpoints</h3><p>When you enable the API Priority and Fairness feature, the <code>kube-apiserver</code>
serves the following additional paths at its HTTP[S] ports.</p><ul><li><p><code>/debug/api_priority_and_fairness/dump_priority_levels</code> - a listing of
all the priority levels and the current state of each. You can fetch like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_priority_levels
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, ActiveQueues, IsIdle, IsQuiescing, WaitingRequests, ExecutingRequests,
workload-low,      0,            true,   false,       0,               0,
global-default,    0,            true,   false,       0,               0,
exempt,            &lt;none&gt;,       &lt;none&gt;, &lt;none&gt;,      &lt;none&gt;,          &lt;none&gt;,
catch-all,         0,            true,   false,       0,               0,
system,            0,            true,   false,       0,               0,
leader-election,   0,            true,   false,       0,               0,
workload-high,     0,            true,   false,       0,               0,
</code></pre></li><li><p><code>/debug/api_priority_and_fairness/dump_queues</code> - a listing of all the
queues and their current state. You can fetch like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_queues
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, Index,  PendingRequests, ExecutingRequests, VirtualStart,
workload-high,     0,      0,               0,                 0.0000,
workload-high,     1,      0,               0,                 0.0000,
workload-high,     2,      0,               0,                 0.0000,
...
leader-election,   14,     0,               0,                 0.0000,
leader-election,   15,     0,               0,                 0.0000,
</code></pre></li><li><p><code>/debug/api_priority_and_fairness/dump_requests</code> - a listing of all the requests
that are currently waiting in a queue. You can fetch like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_requests
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,
exempt,            &lt;none&gt;,         &lt;none&gt;,     &lt;none&gt;,              &lt;none&gt;,                &lt;none&gt;,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:26:57.179170694Z,
</code></pre><p>In addition to the queued requests, the output includes one phantom line
for each priority level that is exempt from limitation.</p><p>You can get a more detailed listing with a command like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw <span style=color:#b44>&#39;/debug/api_priority_and_fairness/dump_requests?includeRequestDetails=1&#39;</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,                     UserName,              Verb,   APIPath,                                                     Namespace, Name,   APIVersion, Resource, SubResource,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:31:03.583823404Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
system,            system-nodes,   12,         1,                   system:node:127.0.0.1, 2020-07-23T15:31:03.594555947Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
</code></pre></li></ul><h2 id=what-s-next>What's next</h2><p>For background information on design details for API priority and fairness, see
the <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness>enhancement proposal</a>.
You can make suggestions and feature requests via <a href=https://github.com/kubernetes/community/tree/master/sig-api-machinery>SIG API Machinery</a>
or the feature's <a href=https://kubernetes.slack.com/messages/api-priority-and-fairness>slack channel</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-85d633ae590aa20ec024f1b7af1d74fc>10 - Installing Addons</h1><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>Add-ons extend the functionality of Kubernetes.</p><p>This page lists some of the available add-ons and links to their respective installation instructions. The list does not try to be exhaustive.</p><h2 id=networking-and-network-policy>Networking and Network Policy</h2><ul><li><a href=https://www.github.com/noironetworks/aci-containers>ACI</a> provides integrated container networking and network security with Cisco ACI.</li><li><a href=https://antrea.io/>Antrea</a> operates at Layer 3/4 to provide networking and security services for Kubernetes, leveraging Open vSwitch as the networking data plane.</li><li><a href=https://docs.projectcalico.org/latest/introduction/>Calico</a> is a networking and network policy provider. Calico supports a flexible set of networking options so you can choose the most efficient option for your situation, including non-overlay and overlay networks, with or without BGP. Calico uses the same engine to enforce network policy for hosts, pods, and (if using Istio & Envoy) applications at the service mesh layer.</li><li><a href=https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel>Canal</a> unites Flannel and Calico, providing networking and network policy.</li><li><a href=https://github.com/cilium/cilium>Cilium</a> is a networking, observability, and security solution with an eBPF-based data plane. Cilium provides a simple flat Layer 3 network with the ability to span multiple clusters in either a native routing or overlay/encapsulation mode, and can enforce network policies on L3-L7 using an identity-based security model that is decoupled from network addressing. Cilium can act as a replacement for kube-proxy; it also offers additional, opt-in observability and security features.</li><li><a href=https://github.com/cni-genie/CNI-Genie>CNI-Genie</a> enables Kubernetes to seamlessly connect to a choice of CNI plugins, such as Calico, Canal, Flannel, or Weave.</li><li><a href=https://contivpp.io/>Contiv</a> provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy framework. Contiv project is fully <a href=https://github.com/contiv>open sourced</a>. The <a href=https://github.com/contiv/install>installer</a> provides both kubeadm and non-kubeadm based installation options.</li><li><a href=https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a>, based on <a href=https://tungsten.io>Tungsten Fabric</a>, is an open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads.</li><li><a href=https://github.com/flannel-io/flannel#deploying-flannel-manually>Flannel</a> is an overlay network provider that can be used with Kubernetes.</li><li><a href=https://github.com/ZTE/Knitter/>Knitter</a> is a plugin to support multiple network interfaces in a Kubernetes pod.</li><li><a href=https://github.com/k8snetworkplumbingwg/multus-cni>Multus</a> is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and VPP based workloads in Kubernetes.</li><li><a href=https://github.com/ovn-org/ovn-kubernetes/>OVN-Kubernetes</a> is a networking provider for Kubernetes based on <a href=https://github.com/ovn-org/ovn/>OVN (Open Virtual Network)</a>, a virtual networking implementation that came out of the Open vSwitch (OVS) project. OVN-Kubernetes provides an overlay based networking implementation for Kubernetes, including an OVS based implementation of load balancing and network policy.</li><li><a href=https://github.com/akraino-edge-stack/icn-nodus>Nodus</a> is an OVN based CNI controller plugin to provide cloud native based Service function chaining(SFC).</li><li><a href=https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html>NSX-T</a> Container Plug-in (NCP) provides integration between VMware NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and OpenShift.</li><li><a href=https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst>Nuage</a> is an SDN platform that provides policy-based networking between Kubernetes Pods and non-Kubernetes environments with visibility and security monitoring.</li><li><a href=https://github.com/romana>Romana</a> is a Layer 3 networking solution for pod networks that also supports the <a href=/docs/concepts/services-networking/network-policies/>NetworkPolicy</a> API.</li><li><a href=https://www.weave.works/docs/net/latest/kubernetes/kube-addon/>Weave Net</a> provides networking and network policy, will carry on working on both sides of a network partition, and does not require an external database.</li></ul><h2 id=service-discovery>Service Discovery</h2><ul><li><a href=https://coredns.io>CoreDNS</a> is a flexible, extensible DNS server which can be <a href=https://github.com/coredns/deployment/tree/master/kubernetes>installed</a> as the in-cluster DNS for pods.</li></ul><h2 id=visualization-amp-control>Visualization & Control</h2><ul><li><a href=https://github.com/kubernetes/dashboard#kubernetes-dashboard>Dashboard</a> is a dashboard web interface for Kubernetes.</li><li><a href=https://www.weave.works/documentation/scope-latest-installing/#k8s>Weave Scope</a> is a tool for graphically visualizing your containers, pods, services etc. Use it in conjunction with a <a href=https://cloud.weave.works/>Weave Cloud account</a> or host the UI yourself.</li></ul><h2 id=infrastructure>Infrastructure</h2><ul><li><a href=https://kubevirt.io/user-guide/#/installation/installation>KubeVirt</a> is an add-on to run virtual machines on Kubernetes. Usually run on bare-metal clusters.</li><li>The
<a href=https://github.com/kubernetes/node-problem-detector>node problem detector</a>
runs on Linux nodes and reports system issues as either
<a href=/docs/reference/kubernetes-api/cluster-resources/event-v1/>Events</a> or
<a href=/docs/concepts/architecture/nodes/#condition>Node conditions</a>.</li></ul><h2 id=legacy-add-ons>Legacy Add-ons</h2><p>There are several other add-ons documented in the deprecated <a href=https://git.k8s.io/kubernetes/cluster/addons>cluster/addons</a> directory.</p><p>Well-maintained ones should be linked to here. PRs welcome!</p></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>