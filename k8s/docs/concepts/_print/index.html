<!doctype html><html lang=en class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/><link rel=alternate hreflang=ru href=https://kubernetes.io/ru/docs/concepts/><link rel=alternate hreflang=pl href=https://kubernetes.io/pl/docs/concepts/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/concepts/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/concepts/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Concepts | Kubernetes</title><meta property="og:title" content="Concepts"><meta property="og:description" content="Production-Grade Container Orchestration"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/concepts/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Concepts"><meta itemprop=description content="Production-Grade Container Orchestration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Concepts"><meta name=twitter:description content="Production-Grade Container Orchestration"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your cluster, and helps you obtain a deeper understanding of how Kubernetes works."><meta property="og:description" content="The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your cluster, and helps you obtain a deeper understanding of how Kubernetes works."><meta name=twitter:description content="The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your cluster, and helps you obtain a deeper understanding of how Kubernetes works."><meta property="og:url" content="https://kubernetes.io/docs/concepts/"><meta property="og:title" content="Concepts"><meta name=twitter:title content="Concepts"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/concepts/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/concepts/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/concepts/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/concepts/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/concepts/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/docs/concepts/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/>Bahasa Indonesia</a>
<a class=dropdown-item href=/ru/docs/concepts/>Русский (Russian)</a>
<a class=dropdown-item href=/pl/docs/concepts/>Polski (Polish)</a>
<a class=dropdown-item href=/uk/docs/concepts/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/concepts/>Return to the regular view of this page</a>.</p></div><h1 class=title>Concepts</h1><ul><li>1: <a href=#pg-0554ac387412eaf4e6e89b2f847dacde>Overview</a></li><ul><li>1.1: <a href=#pg-13b0f1dbe89228e3d76d2ac231e245f1>Kubernetes Components</a></li><li>1.2: <a href=#pg-0c745f42e623d2b70a53bc0e6db73d95>The Kubernetes API</a></li><li>1.3: <a href=#pg-110f33530cf761140cb1dab536baef04>Working with Kubernetes Objects</a></li><ul><li>1.3.1: <a href=#pg-9f5adfa77f48c50d5cc81155a3cecb98>Understanding Kubernetes Objects</a></li><li>1.3.2: <a href=#pg-6751db8ff5409476de8225d17d6c42dd>Kubernetes Object Management</a></li><li>1.3.3: <a href=#pg-f37749a83c2916b63279ea60f3cfe53e>Object Names and IDs</a></li><li>1.3.4: <a href=#pg-f1dec4557fb8ffbac9f11390aaaf9fa4>Labels and Selectors</a></li><li>1.3.5: <a href=#pg-1127165f472b7181b9c1d5a0b187d620>Namespaces</a></li><li>1.3.6: <a href=#pg-93cd7a1d4e1623e2bf01afc49a5af69c>Annotations</a></li><li>1.3.7: <a href=#pg-046c03090d47bc4b89b818dc645c3865>Field Selectors</a></li><li>1.3.8: <a href=#pg-13ce5627ef1dc8cbb4530ed231cb7d38>Finalizers</a></li><li>1.3.9: <a href=#pg-efaa7a58910b58892dafd50e3b43c93c>Owners and Dependents</a></li><li>1.3.10: <a href=#pg-5dd62c6a4a481b4cf1ac50f6799eb581>Recommended Labels</a></li></ul></ul><li>2: <a href=#pg-2bf36ccd6b3dbeafecf87c39761b07c7>Cluster Architecture</a></li><ul><li>2.1: <a href=#pg-9ef2890698e773b6c0d24fd2c20146f5>Nodes</a></li><li>2.2: <a href=#pg-c0251def6da29b30afebfb04549f1703>Communication between Nodes and the Control Plane</a></li><li>2.3: <a href=#pg-ca8819042a505291540e831283da66df>Controllers</a></li><li>2.4: <a href=#pg-bc804b02614d67025b4c788f1ca87fbc>Cloud Controller Manager</a></li><li>2.5: <a href=#pg-c20ec7d296cc2c8668bb204c2af31180>About cgroup v2</a></li><li>2.6: <a href=#pg-c0ea5310f52e22c5de34dc84d9ab5e0d>Container Runtime Interface (CRI)</a></li><li>2.7: <a href=#pg-44a2e2e592af0846101e970aff9243e5>Garbage Collection</a></li></ul><li>3: <a href=#pg-a5f7383c83ab9eb9cd0e3c4c020b3ae6>Containers</a></li><ul><li>3.1: <a href=#pg-16042b4652ad19e565c7263824029a43>Images</a></li><li>3.2: <a href=#pg-643212488f778acf04bebed65ba34441>Container Environment</a></li><li>3.3: <a href=#pg-a858027489648786a3b16264e451272b>Runtime Class</a></li><li>3.4: <a href=#pg-e6941d969d81540208a3e78bc56f43bc>Container Lifecycle Hooks</a></li></ul><li>4: <a href=#pg-05a1231ecbfe48ec554e6d078818aca4>Windows in Kubernetes</a></li><ul><li>4.1: <a href=#pg-849246a35c3de66980f66e1b0944ceb4>Windows containers in Kubernetes</a></li><li>4.2: <a href=#pg-0d8bfd3be43b3580681c56f6fec9d6dc>Guide for scheduling Windows containers in Kubernetes</a></li></ul><li>5: <a href=#pg-d52aadda80edd9f8c514cfe2321363c2>Workloads</a></li><ul><li>5.1: <a href=#pg-4d68b0ccf9c683e6368ffdcc40c838d4>Pods</a></li><ul><li>5.1.1: <a href=#pg-c3c2b9cf30915ec9d46c147201da3332>Pod Lifecycle</a></li><li>5.1.2: <a href=#pg-1ccbd4eeded6ab138d98b59175bd557e>Init Containers</a></li><li>5.1.3: <a href=#pg-4aaf43c715cd764bc8ed4436f3537e68>Disruptions</a></li><li>5.1.4: <a href=#pg-53a1005011e1bda2ce81819aad7c8b32>Ephemeral Containers</a></li><li>5.1.5: <a href=#pg-868be91dc02aab6dc768102e4abf5eff>User Namespaces</a></li><li>5.1.6: <a href=#pg-420713565efe2f940e277f6b4824ad9a>Downward API</a></li></ul><li>5.2: <a href=#pg-89637410cacae45a36ab1cc278c482eb>Workload Resources</a></li><ul><li>5.2.1: <a href=#pg-a2dc0393e0c4079e1c504b6429844e86>Deployments</a></li><li>5.2.2: <a href=#pg-d459b930218774655fa7fd1620625539>ReplicaSet</a></li><li>5.2.3: <a href=#pg-6d72299952c37ca8cc61b416e5bdbcd4>StatefulSets</a></li><li>5.2.4: <a href=#pg-41600eb8b6631c88848156f381e9d588>DaemonSet</a></li><li>5.2.5: <a href=#pg-cc7cc3c4907039d9f863162e20bfbbef>Jobs</a></li><li>5.2.6: <a href=#pg-4de50a37ebb6f2340484192126cb7a04>Automatic Clean-up for Finished Jobs</a></li><li>5.2.7: <a href=#pg-2e4cec01c525b45eccd6010e21cc76d9>CronJob</a></li><li>5.2.8: <a href=#pg-27f1331d515d95f76aa1156088b4ad91>ReplicationController</a></li></ul></ul><li>6: <a href=#pg-0a0a7eca3e302a3c08f8c85e15d337fd>Services, Load Balancing, and Networking</a></li><ul><li>6.1: <a href=#pg-5701136fd2ce258047b6ddc389112352>Service</a></li><li>6.2: <a href=#pg-199bcc92443dbc9bed44819467d7eb75>Ingress</a></li><li>6.3: <a href=#pg-5a8edeb1f2dc8e38cd6d561bb08b0d78>Ingress Controllers</a></li><li>6.4: <a href=#pg-f51db1097575de8072afe1f5b156a70c>EndpointSlices</a></li><li>6.5: <a href=#pg-ded1daafdcd293023ee333728007ca61>Network Policies</a></li><li>6.6: <a href=#pg-91cb8a4438b003df11bc1c426a81b756>DNS for Services and Pods</a></li><li>6.7: <a href=#pg-21f8d19c60c33914baab66224c3d46a7>IPv4/IPv6 dual-stack</a></li><li>6.8: <a href=#pg-374e5c954990aec58a0797adc70a5039>Topology Aware Hints</a></li><li>6.9: <a href=#pg-9092684b3a27432bc9041d56b7a4a8ba>Networking on Windows</a></li><li>6.10: <a href=#pg-cd7657b1056ad32451974db57a951ba5>Service Internal Traffic Policy</a></li><li>6.11: <a href=#pg-3a38878244d862dfdb8d7adb32f77584>Topology-aware traffic routing with topology keys</a></li></ul><li>7: <a href=#pg-f018f568c6723865753f150c3c59bdda>Storage</a></li><ul><li>7.1: <a href=#pg-27795584640a03bd2024f1fe3b3ab754>Volumes</a></li><li>7.2: <a href=#pg-ffd12528a12882b282e1bd19e29f9e75>Persistent Volumes</a></li><li>7.3: <a href=#pg-2db414b26d4daec3ebed19dd837830c3>Projected Volumes</a></li><li>7.4: <a href=#pg-df33eab51202c17bb0fe551d1d5cc5d2>Ephemeral Volumes</a></li><li>7.5: <a href=#pg-f0276d05eef111249272a1c932a91e2c>Storage Classes</a></li><li>7.6: <a href=#pg-018f0a7fc6e2f6d16da37702fc39b4f3>Dynamic Volume Provisioning</a></li><li>7.7: <a href=#pg-c262af210c6828dec445d2f55a1d877a>Volume Snapshots</a></li><li>7.8: <a href=#pg-4d00116c86dade62bdd5be7dc2afa1ca>Volume Snapshot Classes</a></li><li>7.9: <a href=#pg-707ca81a34eb1ca202f34692e9917d1e>CSI Volume Cloning</a></li><li>7.10: <a href=#pg-00cd24f4570b7acaac75c2551c948bc7>Storage Capacity</a></li><li>7.11: <a href=#pg-b2e4b16ac37988c678a3312a4a6639f8>Node-specific Volume Limits</a></li><li>7.12: <a href=#pg-4f40cb95a671e51b4f0156a409d95c6d>Volume Health Monitoring</a></li><li>7.13: <a href=#pg-055a8df536f8ba8f3aa0217bd2db5437>Windows Storage</a></li></ul><li>8: <a href=#pg-275bea454e1cf4c5adeca4058b5af988>Configuration</a></li><ul><li>8.1: <a href=#pg-ddef6fd0e47bb51c6f05e8e7fb11d2dd>Configuration Best Practices</a></li><li>8.2: <a href=#pg-6b5ccadd699df0904e8e9917c5450c4b>ConfigMaps</a></li><li>8.3: <a href=#pg-e511ed821ada65d0053341dbd8ad2bb5>Secrets</a></li><li>8.4: <a href=#pg-436057b96151ecb8a4a9a9f456b5d0fc>Resource Management for Pods and Containers</a></li><li>8.5: <a href=#pg-ab6d20f33ad930a67ee7ef57bff6c75e>Organizing Cluster Access Using kubeconfig Files</a></li><li>8.6: <a href=#pg-0f628478dbd58516389164933f9d7da2>Resource Management for Windows nodes</a></li></ul><li>9: <a href=#pg-712cb3c03ff14a39e5a83a6d9b71d203>Security</a></li><ul><li>9.1: <a href=#pg-04eeb110d75afc8acb2cf7a3db743985>Overview of Cloud Native Security</a></li><li>9.2: <a href=#pg-1fb24c1dd155f43849da490a74c4b8c5>Pod Security Standards</a></li><li>9.3: <a href=#pg-bc9934fccfeaf880eec6ea79025c0381>Pod Security Admission</a></li><li>9.4: <a href=#pg-ac71855bb20cbf21edc666e810f4103a>Pod Security Policies</a></li><li>9.5: <a href=#pg-9a68f631b6bc38c279bbc9a145e34ef2>Security For Windows Nodes</a></li><li>9.6: <a href=#pg-4d77d1ae4c06aa14f54b385191627881>Controlling Access to the Kubernetes API</a></li><li>9.7: <a href=#pg-07f58aa0218d666795499c2e2306ff96>Role Based Access Control Good Practices</a></li><li>9.8: <a href=#pg-a7863bfad3d69f33f5b318b9028eecb8>Good practices for Kubernetes Secrets</a></li><li>9.9: <a href=#pg-9dd9b8c71fa39ff803fd15b0e784069d>Multi-tenancy</a></li><li>9.10: <a href=#pg-265c06c3d1349382453ced9f2a7ecfde>Kubernetes API Server Bypass Risks</a></li><li>9.11: <a href=#pg-6f8354561fd5286f997909e14b13110c>Security Checklist</a></li></ul><li>10: <a href=#pg-ac9161c6d952925b083ad9602b4e8e7f>Policies</a></li><ul><li>10.1: <a href=#pg-a935ff8c59eb116b43494255cc67f69a>Limit Ranges</a></li><li>10.2: <a href=#pg-94ddc6e901c30f256138db11d09f05a3>Resource Quotas</a></li><li>10.3: <a href=#pg-7352434db5f5954d2f7656b46fe5a324>Process ID Limits And Reservations</a></li><li>10.4: <a href=#pg-b528c4464c030f3f044124b38d778f04>Node Resource Managers</a></li></ul><li>11: <a href=#pg-c21d05f31057c5bcd2ebdd01f4e62a0e>Scheduling, Preemption and Eviction</a></li><ul><li>11.1: <a href=#pg-598f36d691ab197f9d995784574b0a12>Kubernetes Scheduler</a></li><li>11.2: <a href=#pg-21169f516071aea5d16734a4c27789a5>Assigning Pods to Nodes</a></li><li>11.3: <a href=#pg-da22fe2278df236f71efbe672f392677>Pod Overhead</a></li><li>11.4: <a href=#pg-6b8c85a6a88f4a81e6b79e197c293c31>Pod Topology Spread Constraints</a></li><li>11.5: <a href=#pg-ede4960b56a3529ee0bfe7c8fe2d09a5>Taints and Tolerations</a></li><li>11.6: <a href=#pg-602208c95fe7b1f1170310ce993f5814>Scheduling Framework</a></li><li>11.7: <a href=#pg-d9574a30fcbc631b0d2a57850e161e89>Scheduler Performance Tuning</a></li><li>11.8: <a href=#pg-961126cd43559012893979e568396a49>Resource Bin Packing</a></li><li>11.9: <a href=#pg-60e5a2861609e0848d58ce8bf99c4a31>Pod Priority and Preemption</a></li><li>11.10: <a href=#pg-78e0431b4b7516092662a7c289cbb304>Node-pressure Eviction</a></li><li>11.11: <a href=#pg-b87723bf81b079042860f0ebd37b0a64>API-initiated Eviction</a></li></ul><li>12: <a href=#pg-285a3785fd3d20f437c28d87ca4dadca>Cluster Administration</a></li><ul><li>12.1: <a href=#pg-2bf9a93ab5ba014fb6ff70b22c29d432>Certificates</a></li><li>12.2: <a href=#pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>Managing Resources</a></li><li>12.3: <a href=#pg-d649067a69d8d5c7e71564b42b96909e>Cluster Networking</a></li><li>12.4: <a href=#pg-c4b1e87a84441f8a90699a345ce48d68>Logging Architecture</a></li><li>12.5: <a href=#pg-cbfd3654996eae9fcdef009f70fa83f0>Metrics For Kubernetes System Components</a></li><li>12.6: <a href=#pg-5cc31ecfba86467f8884856412cfb6b2>System Logs</a></li><li>12.7: <a href=#pg-3da54ad355f6fe6574d67bd9a9a42bcb>Traces For Kubernetes System Components</a></li><li>12.8: <a href=#pg-08e94e6a480e0d6b2de72d84a1b97617>Proxies in Kubernetes</a></li><li>12.9: <a href=#pg-31c9327d2332c585341b64ddafa19cdd>API Priority and Fairness</a></li><li>12.10: <a href=#pg-85d633ae590aa20ec024f1b7af1d74fc>Installing Addons</a></li></ul><li>13: <a href=#pg-7e0d97616b15e2c383c6a0a96ec442cb>Extending Kubernetes</a></li><ul><li>13.1: <a href=#pg-c8937cdc9df96f3328becf04f8211292>Compute, Storage, and Networking Extensions</a></li><ul><li>13.1.1: <a href=#pg-1ac2260db9ecccbf0303a899bc27ce6d>Network Plugins</a></li><li>13.1.2: <a href=#pg-53e1ea8892ceca307ba19e8d6a7b8d32>Device Plugins</a></li></ul><li>13.2: <a href=#pg-0af41d3bd7c785621b58b7564793396a>Extending the Kubernetes API</a></li><ul><li>13.2.1: <a href=#pg-342388440304e19ce30c0f8ada1c77ce>Custom Resources</a></li><li>13.2.2: <a href=#pg-1ea4977c0ebf97569bf54a477faa7fa5>Kubernetes API Aggregation Layer</a></li></ul><li>13.3: <a href=#pg-3131452556176159fb269593c1a52012>Operator pattern</a></li></ul></ul><div class=content><p>The Concepts section helps you learn about the parts of the Kubernetes system and the abstractions Kubernetes uses to represent your <a class=glossary-tooltip title='A set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=cluster>cluster</a>, and helps you obtain a deeper understanding of how Kubernetes works.</p></div></div><div class=td-content><h1 id=pg-0554ac387412eaf4e6e89b2f847dacde>1 - Overview</h1><div class=lead>Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</div><p>This page is an overview of Kubernetes.</p><p>Kubernetes is a portable, extensible, open source platform for managing containerized
workloads and services, that facilitates both declarative configuration and automation.
It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.</p><p>The name Kubernetes originates from Greek, meaning helmsman or pilot. K8s as an abbreviation
results from counting the eight letters between the "K" and the "s". Google open-sourced the
Kubernetes project in 2014. Kubernetes combines
<a href=/blog/2015/04/borg-predecessor-to-kubernetes/>over 15 years of Google's experience</a> running
production workloads at scale with best-of-breed ideas and practices from the community.</p><h2 id=going-back-in-time>Going back in time</h2><p>Let's take a look at why Kubernetes is so useful by going back in time.</p><p><img src=/images/docs/Container_Evolution.svg alt="Deployment evolution"></p><p><strong>Traditional deployment era:</strong>
Early on, organizations ran applications on physical servers. There was no way to define
resource boundaries for applications in a physical server, and this caused resource
allocation issues. For example, if multiple applications run on a physical server, there
can be instances where one application would take up most of the resources, and as a result,
the other applications would underperform. A solution for this would be to run each application
on a different physical server. But this did not scale as resources were underutilized, and it
was expensive for organizations to maintain many physical servers.</p><p><strong>Virtualized deployment era:</strong> As a solution, virtualization was introduced. It allows you
to run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization
allows applications to be isolated between VMs and provides a level of security as the
information of one application cannot be freely accessed by another application.</p><p>Virtualization allows better utilization of resources in a physical server and allows
better scalability because an application can be added or updated easily, reduces
hardware costs, and much more. With virtualization you can present a set of physical
resources as a cluster of disposable virtual machines.</p><p>Each VM is a full machine running all the components, including its own operating
system, on top of the virtualized hardware.</p><p><strong>Container deployment era:</strong> Containers are similar to VMs, but they have relaxed
isolation properties to share the Operating System (OS) among the applications.
Therefore, containers are considered lightweight. Similar to a VM, a container
has its own filesystem, share of CPU, memory, process space, and more. As they
are decoupled from the underlying infrastructure, they are portable across clouds
and OS distributions.</p><p>Containers have become popular because they provide extra benefits, such as:</p><ul><li>Agile application creation and deployment: increased ease and efficiency of
container image creation compared to VM image use.</li><li>Continuous development, integration, and deployment: provides for reliable
and frequent container image build and deployment with quick and efficient
rollbacks (due to image immutability).</li><li>Dev and Ops separation of concerns: create application container images at
build/release time rather than deployment time, thereby decoupling
applications from infrastructure.</li><li>Observability: not only surfaces OS-level information and metrics, but also
application health and other signals.</li><li>Environmental consistency across development, testing, and production: Runs
the same on a laptop as it does in the cloud.</li><li>Cloud and OS distribution portability: Runs on Ubuntu, RHEL, CoreOS, on-premises,
on major public clouds, and anywhere else.</li><li>Application-centric management: Raises the level of abstraction from running an
OS on virtual hardware to running an application on an OS using logical resources.</li><li>Loosely coupled, distributed, elastic, liberated micro-services: applications are
broken into smaller, independent pieces and can be deployed and managed dynamically –
not a monolithic stack running on one big single-purpose machine.</li><li>Resource isolation: predictable application performance.</li><li>Resource utilization: high efficiency and density.</li></ul><h2 id=why-you-need-kubernetes-and-what-can-it-do>Why you need Kubernetes and what it can do</h2><p>Containers are a good way to bundle and run your applications. In a production
environment, you need to manage the containers that run the applications and
ensure that there is no downtime. For example, if a container goes down, another
container needs to start. Wouldn't it be easier if this behavior was handled by a system?</p><p>That's how Kubernetes comes to the rescue! Kubernetes provides you with a framework
to run distributed systems resiliently. It takes care of scaling and failover for
your application, provides deployment patterns, and more. For example: Kubernetes
can easily manage a canary deployment for your system.</p><p>Kubernetes provides you with:</p><ul><li><strong>Service discovery and load balancing</strong>
Kubernetes can expose a container using the DNS name or using their own IP address.
If traffic to a container is high, Kubernetes is able to load balance and distribute
the network traffic so that the deployment is stable.</li><li><strong>Storage orchestration</strong>
Kubernetes allows you to automatically mount a storage system of your choice, such as
local storages, public cloud providers, and more.</li><li><strong>Automated rollouts and rollbacks</strong>
You can describe the desired state for your deployed containers using Kubernetes,
and it can change the actual state to the desired state at a controlled rate.
For example, you can automate Kubernetes to create new containers for your
deployment, remove existing containers and adopt all their resources to the new container.</li><li><strong>Automatic bin packing</strong>
You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks.
You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit
containers onto your nodes to make the best use of your resources.</li><li><strong>Self-healing</strong>
Kubernetes restarts containers that fail, replaces containers, kills containers that don't
respond to your user-defined health check, and doesn't advertise them to clients until they
are ready to serve.</li><li><strong>Secret and configuration management</strong>
Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens,
and SSH keys. You can deploy and update secrets and application configuration without
rebuilding your container images, and without exposing secrets in your stack configuration.</li></ul><h2 id=what-kubernetes-is-not>What Kubernetes is not</h2><p>Kubernetes is not a traditional, all-inclusive PaaS (Platform as a Service) system.
Since Kubernetes operates at the container level rather than at the hardware level,
it provides some generally applicable features common to PaaS offerings, such as
deployment, scaling, load balancing, and lets users integrate their logging, monitoring,
and alerting solutions. However, Kubernetes is not monolithic, and these default solutions
are optional and pluggable. Kubernetes provides the building blocks for building developer
platforms, but preserves user choice and flexibility where it is important.</p><p>Kubernetes:</p><ul><li>Does not limit the types of applications supported. Kubernetes aims to support an
extremely diverse variety of workloads, including stateless, stateful, and data-processing
workloads. If an application can run in a container, it should run great on Kubernetes.</li><li>Does not deploy source code and does not build your application. Continuous Integration,
Delivery, and Deployment (CI/CD) workflows are determined by organization cultures and
preferences as well as technical requirements.</li><li>Does not provide application-level services, such as middleware (for example, message buses),
data-processing frameworks (for example, Spark), databases (for example, MySQL), caches, nor
cluster storage systems (for example, Ceph) as built-in services. Such components can run on
Kubernetes, and/or can be accessed by applications running on Kubernetes through portable
mechanisms, such as the <a href=https://openservicebrokerapi.org/>Open Service Broker</a>.</li><li>Does not dictate logging, monitoring, or alerting solutions. It provides some integrations
as proof of concept, and mechanisms to collect and export metrics.</li><li>Does not provide nor mandate a configuration language/system (for example, Jsonnet). It provides
a declarative API that may be targeted by arbitrary forms of declarative specifications.</li><li>Does not provide nor adopt any comprehensive machine configuration, maintenance, management,
or self-healing systems.</li><li>Additionally, Kubernetes is not a mere orchestration system. In fact, it eliminates the need
for orchestration. The technical definition of orchestration is execution of a defined workflow:
first do A, then B, then C. In contrast, Kubernetes comprises a set of independent, composable
control processes that continuously drive the current state towards the provided desired state.
It shouldn't matter how you get from A to C. Centralized control is also not required. This
results in a system that is easier to use and more powerful, robust, resilient, and extensible.</li></ul><h2 id=what-s-next>What's next</h2><ul><li>Take a look at the <a href=/docs/concepts/overview/components/>Kubernetes Components</a></li><li>Take a look at the <a href=/docs/concepts/overview/kubernetes-api/>The Kubernetes API</a></li><li>Take a look at the <a href=/docs/concepts/architecture/>Cluster Architecture</a></li><li>Ready to <a href=/docs/setup/>Get Started</a>?</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-13b0f1dbe89228e3d76d2ac231e245f1>1.1 - Kubernetes Components</h1><div class=lead>A Kubernetes cluster consists of the components that are a part of the control plane and a set of machines called nodes.</div><p>When you deploy Kubernetes, you get a cluster.<p><p>A Kubernetes cluster consists of a set of worker machines, called <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodes>nodes</a>,
that run containerized applications. Every cluster has at least one worker node.</p></p><p>The worker node(s) host the <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> that are
the components of the application workload. The
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> manages the worker
nodes and the Pods in the cluster. In production environments, the control plane usually
runs across multiple computers and a cluster usually runs multiple nodes, providing
fault-tolerance and high availability.</p></p><p>This document outlines the various components you need to have for
a complete and working Kubernetes cluster.</p><figure class=diagram-large><img src=/images/docs/components-of-kubernetes.svg alt="Components of Kubernetes"><figcaption><p>The components of a Kubernetes cluster</p></figcaption></figure><h2 id=control-plane-components>Control Plane Components</h2><p>The control plane's components make global decisions about the cluster (for example, scheduling), as well as detecting and responding to cluster events (for example, starting up a new <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=pod>pod</a> when a deployment's <code>replicas</code> field is unsatisfied).</p><p>Control plane components can be run on any machine in the cluster. However,
for simplicity, set up scripts typically start all control plane components on
the same machine, and do not run user containers on this machine. See
<a href=/docs/setup/production-environment/tools/kubeadm/high-availability/>Creating Highly Available clusters with kubeadm</a>
for an example control plane setup that runs across multiple machines.</p><h3 id=kube-apiserver>kube-apiserver</h3><p>The API server is a component of the Kubernetes
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> that exposes the Kubernetes API.
The API server is the front end for the Kubernetes control plane.</p><p>The main implementation of a Kubernetes API server is <a href=/docs/reference/generated/kube-apiserver/>kube-apiserver</a>.
kube-apiserver is designed to scale horizontally—that is, it scales by deploying more instances.
You can run several instances of kube-apiserver and balance traffic between those instances.</p><h3 id=etcd>etcd</h3><p>Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.</p><p>If your Kubernetes cluster uses etcd as its backing store, make sure you have a
<a href=/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster>back up</a> plan
for those data.</p><p>You can find in-depth information about etcd in the official <a href=https://etcd.io/docs/>documentation</a>.</p><h3 id=kube-scheduler>kube-scheduler</h3><p>Control plane component that watches for newly created
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> with no assigned
<a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a>, and selects a node for them
to run on.</p><p>Factors taken into account for scheduling decisions include:
individual and collective resource requirements, hardware/software/policy
constraints, affinity and anti-affinity specifications, data locality,
inter-workload interference, and deadlines.</p><h3 id=kube-controller-manager>kube-controller-manager</h3><p>Control plane component that runs <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> processes.</p><p>Logically, each <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> is a separate process, but to reduce complexity, they are all compiled into a single binary and run in a single process.</p><p>Some types of these controllers are:</p><ul><li>Node controller: Responsible for noticing and responding when nodes go down.</li><li>Job controller: Watches for Job objects that represent one-off tasks, then creates
Pods to run those tasks to completion.</li><li>EndpointSlice controller: Populates EndpointSlice objects (to provide a link between Services and Pods).</li><li>ServiceAccount controller: Create default ServiceAccounts for new namespaces.</li></ul><h3 id=cloud-controller-manager>cloud-controller-manager</h3>A Kubernetes <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.<p>The cloud-controller-manager only runs controllers that are specific to your cloud provider.
If you are running Kubernetes on your own premises, or in a learning environment inside your
own PC, the cluster does not have a cloud controller manager.</p><p>As with the kube-controller-manager, the cloud-controller-manager combines several logically
independent control loops into a single binary that you run as a single process. You can
scale horizontally (run more than one copy) to improve performance or to help tolerate failures.</p><p>The following controllers can have cloud provider dependencies:</p><ul><li>Node controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding</li><li>Route controller: For setting up routes in the underlying cloud infrastructure</li><li>Service controller: For creating, updating and deleting cloud provider load balancers</li></ul><h2 id=node-components>Node Components</h2><p>Node components run on every node, maintaining running pods and providing the Kubernetes runtime environment.</p><h3 id=kubelet>kubelet</h3><p>An agent that runs on each <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> in the cluster. It makes sure that <a class=glossary-tooltip title='A lightweight and portable executable image that contains software and all of its dependencies.' data-toggle=tooltip data-placement=top href=/docs/concepts/containers/ target=_blank aria-label=containers>containers</a> are running in a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>.</p><p>The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes.</p><h3 id=kube-proxy>kube-proxy</h3><p>kube-proxy is a network proxy that runs on each
<a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> in your cluster,
implementing part of the Kubernetes
<a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> concept.</p><p><a href=/docs/reference/command-line-tools-reference/kube-proxy/>kube-proxy</a>
maintains network rules on nodes. These network rules allow network
communication to your Pods from network sessions inside or outside of
your cluster.</p><p>kube-proxy uses the operating system packet filtering layer if there is one
and it's available. Otherwise, kube-proxy forwards the traffic itself.</p><h3 id=container-runtime>Container runtime</h3><p>The container runtime is the software that is responsible for running containers.</p><p>Kubernetes supports container runtimes such as
<a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a>, <a class=glossary-tooltip title='A lightweight container runtime specifically for Kubernetes' data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a>,
and any other implementation of the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Kubernetes CRI (Container Runtime
Interface)</a>.</p><h2 id=addons>Addons</h2><p>Addons use Kubernetes resources (<a class=glossary-tooltip title='Ensures a copy of a Pod is running across a set of nodes in a cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a>,
<a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>, etc)
to implement cluster features. Because these are providing cluster-level features, namespaced resources
for addons belong within the <code>kube-system</code> namespace.</p><p>Selected addons are described below; for an extended list of available addons, please
see <a href=/docs/concepts/cluster-administration/addons/>Addons</a>.</p><h3 id=dns>DNS</h3><p>While the other addons are not strictly required, all Kubernetes clusters should have <a href=/docs/concepts/services-networking/dns-pod-service/>cluster DNS</a>, as many examples rely on it.</p><p>Cluster DNS is a DNS server, in addition to the other DNS server(s) in your environment, which serves DNS records for Kubernetes services.</p><p>Containers started by Kubernetes automatically include this DNS server in their DNS searches.</p><h3 id=web-ui-dashboard>Web UI (Dashboard)</h3><p><a href=/docs/tasks/access-application-cluster/web-ui-dashboard/>Dashboard</a> is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications running in the cluster, as well as the cluster itself.</p><h3 id=container-resource-monitoring>Container Resource Monitoring</h3><p><a href=/docs/tasks/debug/debug-cluster/resource-usage-monitoring/>Container Resource Monitoring</a> records generic time-series metrics
about containers in a central database, and provides a UI for browsing that data.</p><h3 id=cluster-level-logging>Cluster-level Logging</h3><p>A <a href=/docs/concepts/cluster-administration/logging/>cluster-level logging</a> mechanism is responsible for
saving container logs to a central log store with search/browsing interface.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/architecture/nodes/>Nodes</a></li><li>Learn about <a href=/docs/concepts/architecture/controller/>Controllers</a></li><li>Learn about <a href=/docs/concepts/scheduling-eviction/kube-scheduler/>kube-scheduler</a></li><li>Read etcd's official <a href=https://etcd.io/docs/>documentation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0c745f42e623d2b70a53bc0e6db73d95>1.2 - The Kubernetes API</h1><div class=lead>The Kubernetes API lets you query and manipulate the state of objects in Kubernetes. The core of Kubernetes' control plane is the API server and the HTTP API that it exposes. Users, the different parts of your cluster, and external components all communicate with one another through the API server.</div><p>The core of Kubernetes' <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>
is the <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a>. The API server
exposes an HTTP API that lets end users, different parts of your cluster, and
external components communicate with one another.</p><p>The Kubernetes API lets you query and manipulate the state of API objects in Kubernetes
(for example: Pods, Namespaces, ConfigMaps, and Events).</p><p>Most operations can be performed through the
<a href=/docs/reference/kubectl/>kubectl</a> command-line interface or other
command-line tools, such as
<a href=/docs/reference/setup-tools/kubeadm/>kubeadm</a>, which in turn use the
API. However, you can also access the API directly using REST calls.</p><p>Consider using one of the <a href=/docs/reference/using-api/client-libraries/>client libraries</a>
if you are writing an application using the Kubernetes API.</p><h2 id=api-specification>OpenAPI specification</h2><p>Complete API details are documented using <a href=https://www.openapis.org/>OpenAPI</a>.</p><h3 id=openapi-v2>OpenAPI V2</h3><p>The Kubernetes API server serves an aggregated OpenAPI v2 spec via the
<code>/openapi/v2</code> endpoint. You can request the response format using
request headers as follows:</p><table><caption style=display:none>Valid request header values for OpenAPI v2 queries</caption><thead><tr><th>Header</th><th style=min-width:50%>Possible values</th><th>Notes</th></tr></thead><tbody><tr><td><code>Accept-Encoding</code></td><td><code>gzip</code></td><td><em>not supplying this header is also acceptable</em></td></tr><tr><td rowspan=3><code>Accept</code></td><td><code>application/com.github.proto-openapi.spec.v2@v1.0+protobuf</code></td><td><em>mainly for intra-cluster use</em></td></tr><tr><td><code>application/json</code></td><td><em>default</em></td></tr><tr><td><code>*</code></td><td><em>serves </em><code>application/json</code></td></tr></tbody></table><p>Kubernetes implements an alternative Protobuf based serialization format that
is primarily intended for intra-cluster communication. For more information
about this format, see the <a href=https://git.k8s.io/design-proposals-archive/api-machinery/protobuf.md>Kubernetes Protobuf serialization</a> design proposal and the
Interface Definition Language (IDL) files for each schema located in the Go
packages that define the API objects.</p><h3 id=openapi-v3>OpenAPI V3</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [beta]</code></div><p>Kubernetes v1.25 offers beta support for publishing its APIs as OpenAPI v3; this is a
beta feature that is enabled by default.
You can disable the beta feature by turning off the
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> named <code>OpenAPIV3</code>
for the kube-apiserver component.</p><p>A discovery endpoint <code>/openapi/v3</code> is provided to see a list of all
group/versions available. This endpoint only returns JSON. These group/versions
are provided in the following format:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>{<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>&#34;paths&#34;: </span>{<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>...,<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>&#34;api/v1&#34;: </span>{<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>&#34;serverRelativeURL&#34;: </span><span style=color:#b44>&#34;/openapi/v3/api/v1?hash=CC0E9BFD992D8C59AEC98A1E2336F899E8318D3CF4C68944C3DEC640AF5AB52D864AC50DAA8D145B3494F75FA3CFF939FCBDDA431DAD3CA79738B297795818CF&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>},<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>&#34;apis/admissionregistration.k8s.io/v1&#34;: </span>{<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>&#34;serverRelativeURL&#34;: </span><span style=color:#b44>&#34;/openapi/v3/apis/admissionregistration.k8s.io/v1?hash=E19CC93A116982CE5422FC42B590A8AFAD92CDE9AE4D59B5CAAD568F083AD07946E6CB5817531680BCE6E215C16973CD39003B0425F3477CFD854E89A9DB6597&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>},<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>....<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>}<span style=color:#bbb>
</span></span></span></code></pre></div><p>The relative URLs are pointing to immutable OpenAPI descriptions, in
order to improve client-side caching. The proper HTTP caching headers
are also set by the API server for that purpose (<code>Expires</code> to 1 year in
the future, and <code>Cache-Control</code> to <code>immutable</code>). When an obsolete URL is
used, the API server returns a redirect to the newest URL.</p><p>The Kubernetes API server publishes an OpenAPI v3 spec per Kubernetes
group version at the <code>/openapi/v3/apis/&lt;group>/&lt;version>?hash=&lt;hash></code>
endpoint.</p><p>Refer to the table below for accepted request headers.</p><table><caption style=display:none>Valid request header values for OpenAPI v3 queries</caption><thead><tr><th>Header</th><th style=min-width:50%>Possible values</th><th>Notes</th></tr></thead><tbody><tr><td><code>Accept-Encoding</code></td><td><code>gzip</code></td><td><em>not supplying this header is also acceptable</em></td></tr><tr><td rowspan=3><code>Accept</code></td><td><code>application/com.github.proto-openapi.spec.v3@v1.0+protobuf</code></td><td><em>mainly for intra-cluster use</em></td></tr><tr><td><code>application/json</code></td><td><em>default</em></td></tr><tr><td><code>*</code></td><td><em>serves </em><code>application/json</code></td></tr></tbody></table><h2 id=persistence>Persistence</h2><p>Kubernetes stores the serialized state of objects by writing them into
<a class=glossary-tooltip title='Consistent and highly-available key value store used as backing store of Kubernetes for all cluster data.' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>.</p><h2 id=api-groups-and-versioning>API groups and versioning</h2><p>To make it easier to eliminate fields or restructure resource representations,
Kubernetes supports multiple API versions, each at a different API path, such
as <code>/api/v1</code> or <code>/apis/rbac.authorization.k8s.io/v1alpha1</code>.</p><p>Versioning is done at the API level rather than at the resource or field level
to ensure that the API presents a clear, consistent view of system resources
and behavior, and to enable controlling access to end-of-life and/or
experimental APIs.</p><p>To make it easier to evolve and to extend its API, Kubernetes implements
<a href=/docs/reference/using-api/#api-groups>API groups</a> that can be
<a href=/docs/reference/using-api/#enabling-or-disabling>enabled or disabled</a>.</p><p>API resources are distinguished by their API group, resource type, namespace
(for namespaced resources), and name. The API server handles the conversion between
API versions transparently: all the different versions are actually representations
of the same persisted data. The API server may serve the same underlying data
through multiple API versions.</p><p>For example, suppose there are two API versions, <code>v1</code> and <code>v1beta1</code>, for the same
resource. If you originally created an object using the <code>v1beta1</code> version of its
API, you can later read, update, or delete that object using either the <code>v1beta1</code>
or the <code>v1</code> API version, until the <code>v1beta1</code> version is deprecated and removed.
At that point you can continue accessing and modifying the object using the <code>v1</code> API.</p><h3 id=api-changes>API changes</h3><p>Any system that is successful needs to grow and change as new use cases emerge or existing ones change.
Therefore, Kubernetes has designed the Kubernetes API to continuously change and grow.
The Kubernetes project aims to <em>not</em> break compatibility with existing clients, and to maintain that
compatibility for a length of time so that other projects have an opportunity to adapt.</p><p>In general, new API resources and new resource fields can be added often and frequently.
Elimination of resources or fields requires following the
<a href=/docs/reference/using-api/deprecation-policy/>API deprecation policy</a>.</p><p>Kubernetes makes a strong commitment to maintain compatibility for official Kubernetes APIs
once they reach general availability (GA), typically at API version <code>v1</code>. Additionally,
Kubernetes maintains compatibility with data persisted via <em>beta</em> API versions of official Kubernetes APIs,
and ensures that data can be converted and accessed via GA API versions when the feature goes stable.</p><p>If you adopt a beta API version, you will need to transition to a subsequent beta or stable API version
once the API graduates. The best time to do this is while the beta API is in its deprecation period,
since objects are simultaneously accessible via both API versions. Once the beta API completes its
deprecation period and is no longer served, the replacement API version must be used.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Although Kubernetes also aims to maintain compatibility for <em>alpha</em> APIs versions, in some
circumstances this is not possible. If you use any alpha API versions, check the release notes
for Kubernetes when upgrading your cluster, in case the API did change in incompatible
ways that require deleting all existing alpha objects prior to upgrade.</div><p>Refer to <a href=/docs/reference/using-api/#api-versioning>API versions reference</a>
for more details on the API version level definitions.</p><h2 id=api-extension>API Extension</h2><p>The Kubernetes API can be extended in one of two ways:</p><ol><li><a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>Custom resources</a>
let you declaratively define how the API server should provide your chosen resource API.</li><li>You can also extend the Kubernetes API by implementing an
<a href=/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>aggregation layer</a>.</li></ol><h2 id=what-s-next>What's next</h2><ul><li>Learn how to extend the Kubernetes API by adding your own
<a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/>CustomResourceDefinition</a>.</li><li><a href=/docs/concepts/security/controlling-access/>Controlling Access To The Kubernetes API</a> describes
how the cluster manages authentication and authorization for API access.</li><li>Learn about API endpoints, resource types and samples by reading
<a href=/docs/reference/kubernetes-api/>API Reference</a>.</li><li>Learn about what constitutes a compatible change, and how to change the API, from
<a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api_changes.md#readme>API changes</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-110f33530cf761140cb1dab536baef04>1.3 - Working with Kubernetes Objects</h1><div class=lead>Kubernetes objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Learn about the Kubernetes object model and how to work with these objects.</div></div><div class=td-content><h1 id=pg-9f5adfa77f48c50d5cc81155a3cecb98>1.3.1 - Understanding Kubernetes Objects</h1><p>This page explains how Kubernetes objects are represented in the Kubernetes API, and how you can
express them in <code>.yaml</code> format.</p><h2 id=kubernetes-objects>Understanding Kubernetes objects</h2><p><em>Kubernetes objects</em> are persistent entities in the Kubernetes system. Kubernetes uses these
entities to represent the state of your cluster. Specifically, they can describe:</p><ul><li>What containerized applications are running (and on which nodes)</li><li>The resources available to those applications</li><li>The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance</li></ul><p>A Kubernetes object is a "record of intent"--once you create the object, the Kubernetes system
will constantly work to ensure that object exists. By creating an object, you're effectively
telling the Kubernetes system what you want your cluster's workload to look like; this is your
cluster's <em>desired state</em>.</p><p>To work with Kubernetes objects--whether to create, modify, or delete them--you'll need to use the
<a href=/docs/concepts/overview/kubernetes-api/>Kubernetes API</a>. When you use the <code>kubectl</code> command-line
interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use
the Kubernetes API directly in your own programs using one of the
<a href=/docs/reference/using-api/client-libraries/>Client Libraries</a>.</p><h3 id=object-spec-and-status>Object spec and status</h3><p>Almost every Kubernetes object includes two nested object fields that govern
the object's configuration: the object <em><code>spec</code></em> and the object <em><code>status</code></em>.
For objects that have a <code>spec</code>, you have to set this when you create the object,
providing a description of the characteristics you want the resource to have:
its <em>desired state</em>.</p><p>The <code>status</code> describes the <em>current state</em> of the object, supplied and updated
by the Kubernetes system and its components. The Kubernetes
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> continually
and actively manages every object's actual state to match the desired state you
supplied.</p><p>For example: in Kubernetes, a Deployment is an object that can represent an
application running on your cluster. When you create the Deployment, you
might set the Deployment <code>spec</code> to specify that you want three replicas of
the application to be running. The Kubernetes system reads the Deployment
spec and starts three instances of your desired application--updating
the status to match your spec. If any of those instances should fail
(a status change), the Kubernetes system responds to the difference
between spec and status by making a correction--in this case, starting
a replacement instance.</p><p>For more information on the object spec, status, and metadata, see the
<a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md>Kubernetes API Conventions</a>.</p><h3 id=describing-a-kubernetes-object>Describing a Kubernetes object</h3><p>When you create an object in Kubernetes, you must provide the object spec that describes its
desired state, as well as some basic information about the object (such as a name). When you use
the Kubernetes API to create the object (either directly or via <code>kubectl</code>), that API request must
include that information as JSON in the request body. <strong>Most often, you provide the information to
<code>kubectl</code> in a .yaml file.</strong> <code>kubectl</code> converts the information to JSON when making the API
request.</p><p>Here's an example <code>.yaml</code> file that shows the required fields and object spec for a Kubernetes Deployment:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/deployment.yaml download=application/deployment.yaml><code>application/deployment.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("application-deployment-yaml")' title="Copy application/deployment.yaml to clipboard"></img></div><div class=includecode id=application-deployment-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># tells deployment to run 2 pods matching the template</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>One way to create a Deployment using a <code>.yaml</code> file like the one above is to use the
<a href=/docs/reference/generated/kubectl/kubectl-commands#apply><code>kubectl apply</code></a> command
in the <code>kubectl</code> command-line interface, passing the <code>.yaml</code> file as an argument. Here's an example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/deployment.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment created
</code></pre><h3 id=required-fields>Required fields</h3><p>In the <code>.yaml</code> file for the Kubernetes object you want to create, you'll need to set values for the following fields:</p><ul><li><code>apiVersion</code> - Which version of the Kubernetes API you're using to create this object</li><li><code>kind</code> - What kind of object you want to create</li><li><code>metadata</code> - Data that helps uniquely identify the object, including a <code>name</code> string, <code>UID</code>, and optional <code>namespace</code></li><li><code>spec</code> - What state you desire for the object</li></ul><p>The precise format of the object <code>spec</code> is different for every Kubernetes object, and contains
nested fields specific to that object. The <a href=/docs/reference/kubernetes-api/>Kubernetes API Reference</a>
can help you find the spec format for all of the objects you can create using Kubernetes.</p><p>For example, see the <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec><code>spec</code> field</a>
for the Pod API reference.
For each Pod, the <code>.spec</code> field specifies the pod and its desired state (such as the container image name for
each container within that pod).
Another example of an object specification is the
<a href=/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/#StatefulSetSpec><code>spec</code> field</a>
for the StatefulSet API. For StatefulSet, the <code>.spec</code> field specifies the StatefulSet and
its desired state.
Within the <code>.spec</code> of a StatefulSet is a <a href=/docs/concepts/workloads/pods/#pod-templates>template</a>
for Pod objects. That template describes Pods that the StatefulSet controller will create in order to
satisfy the StatefulSet specification.
Different kinds of object can also have different <code>.status</code>; again, the API reference pages
detail the structure of that <code>.status</code> field, and its content for each different type of object.</p><h2 id=what-s-next>What's next</h2><p>Learn more about the following:</p><ul><li><a href=/docs/concepts/workloads/pods/>Pods</a> which are the most important basic Kubernetes objects.</li><li><a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a> objects.</li><li><a href=/docs/concepts/architecture/controller/>Controllers</a> in Kubernetes.</li><li><a href=/docs/reference/using-api/>Kubernetes API overview</a> which explains some more API concepts.</li><li><a href=/docs/reference/kubectl/>kubectl</a> and <a href=/docs/reference/generated/kubectl/kubectl-commands>kubectl commands</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6751db8ff5409476de8225d17d6c42dd>1.3.2 - Kubernetes Object Management</h1><p>The <code>kubectl</code> command-line tool supports several different ways to create and manage
Kubernetes objects. This document provides an overview of the different
approaches. Read the <a href=https://kubectl.docs.kubernetes.io>Kubectl book</a> for
details of managing objects by Kubectl.</p><h2 id=management-techniques>Management techniques</h2><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> A Kubernetes object should be managed using only one technique. Mixing
and matching techniques for the same object results in undefined behavior.</div><table><thead><tr><th>Management technique</th><th>Operates on</th><th>Recommended environment</th><th>Supported writers</th><th>Learning curve</th></tr></thead><tbody><tr><td>Imperative commands</td><td>Live objects</td><td>Development projects</td><td>1+</td><td>Lowest</td></tr><tr><td>Imperative object configuration</td><td>Individual files</td><td>Production projects</td><td>1</td><td>Moderate</td></tr><tr><td>Declarative object configuration</td><td>Directories of files</td><td>Production projects</td><td>1+</td><td>Highest</td></tr></tbody></table><h2 id=imperative-commands>Imperative commands</h2><p>When using imperative commands, a user operates directly on live objects
in a cluster. The user provides operations to
the <code>kubectl</code> command as arguments or flags.</p><p>This is the recommended way to get started or to run a one-off task in
a cluster. Because this technique operates directly on live
objects, it provides no history of previous configurations.</p><h3 id=examples>Examples</h3><p>Run an instance of the nginx container by creating a Deployment object:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl create deployment nginx --image nginx
</span></span></code></pre></div><h3 id=trade-offs>Trade-offs</h3><p>Advantages compared to object configuration:</p><ul><li>Commands are expressed as a single action word.</li><li>Commands require only a single step to make changes to the cluster.</li></ul><p>Disadvantages compared to object configuration:</p><ul><li>Commands do not integrate with change review processes.</li><li>Commands do not provide an audit trail associated with changes.</li><li>Commands do not provide a source of records except for what is live.</li><li>Commands do not provide a template for creating new objects.</li></ul><h2 id=imperative-object-configuration>Imperative object configuration</h2><p>In imperative object configuration, the kubectl command specifies the
operation (create, replace, etc.), optional flags and at least one file
name. The file specified must contain a full definition of the object
in YAML or JSON format.</p><p>See the <a href=/docs/reference/generated/kubernetes-api/v1.25/>API reference</a>
for more details on object definitions.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> The imperative <code>replace</code> command replaces the existing
spec with the newly provided one, dropping all changes to the object missing from
the configuration file. This approach should not be used with resource
types whose specs are updated independently of the configuration file.
Services of type <code>LoadBalancer</code>, for example, have their <code>externalIPs</code> field updated
independently from the configuration by the cluster.</div><h3 id=examples-1>Examples</h3><p>Create the objects defined in a configuration file:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl create -f nginx.yaml
</span></span></code></pre></div><p>Delete the objects defined in two configuration files:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl delete -f nginx.yaml -f redis.yaml
</span></span></code></pre></div><p>Update the objects defined in a configuration file by overwriting
the live configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl replace -f nginx.yaml
</span></span></code></pre></div><h3 id=trade-offs-1>Trade-offs</h3><p>Advantages compared to imperative commands:</p><ul><li>Object configuration can be stored in a source control system such as Git.</li><li>Object configuration can integrate with processes such as reviewing changes before push and audit trails.</li><li>Object configuration provides a template for creating new objects.</li></ul><p>Disadvantages compared to imperative commands:</p><ul><li>Object configuration requires basic understanding of the object schema.</li><li>Object configuration requires the additional step of writing a YAML file.</li></ul><p>Advantages compared to declarative object configuration:</p><ul><li>Imperative object configuration behavior is simpler and easier to understand.</li><li>As of Kubernetes version 1.5, imperative object configuration is more mature.</li></ul><p>Disadvantages compared to declarative object configuration:</p><ul><li>Imperative object configuration works best on files, not directories.</li><li>Updates to live objects must be reflected in configuration files, or they will be lost during the next replacement.</li></ul><h2 id=declarative-object-configuration>Declarative object configuration</h2><p>When using declarative object configuration, a user operates on object
configuration files stored locally, however the user does not define the
operations to be taken on the files. Create, update, and delete operations
are automatically detected per-object by <code>kubectl</code>. This enables working on
directories, where different operations might be needed for different objects.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Declarative object configuration retains changes made by other
writers, even if the changes are not merged back to the object configuration file.
This is possible by using the <code>patch</code> API operation to write only
observed differences, instead of using the <code>replace</code>
API operation to replace the entire object configuration.</div><h3 id=examples-2>Examples</h3><p>Process all object configuration files in the <code>configs</code> directory, and create or
patch the live objects. You can first <code>diff</code> to see what changes are going to be
made, and then apply:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl diff -f configs/
</span></span><span style=display:flex><span>kubectl apply -f configs/
</span></span></code></pre></div><p>Recursively process directories:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>kubectl diff -R -f configs/
</span></span><span style=display:flex><span>kubectl apply -R -f configs/
</span></span></code></pre></div><h3 id=trade-offs-2>Trade-offs</h3><p>Advantages compared to imperative object configuration:</p><ul><li>Changes made directly to live objects are retained, even if they are not merged back into the configuration files.</li><li>Declarative object configuration has better support for operating on directories and automatically detecting operation types (create, patch, delete) per-object.</li></ul><p>Disadvantages compared to imperative object configuration:</p><ul><li>Declarative object configuration is harder to debug and understand results when they are unexpected.</li><li>Partial updates using diffs create complex merge and patch operations.</li></ul><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/tasks/manage-kubernetes-objects/imperative-command/>Managing Kubernetes Objects Using Imperative Commands</a></li><li><a href=/docs/tasks/manage-kubernetes-objects/imperative-config/>Imperative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href=/docs/tasks/manage-kubernetes-objects/declarative-config/>Declarative Management of Kubernetes Objects Using Configuration Files</a></li><li><a href=/docs/tasks/manage-kubernetes-objects/kustomization/>Declarative Management of Kubernetes Objects Using Kustomize</a></li><li><a href=/docs/reference/generated/kubectl/kubectl-commands/>Kubectl Command Reference</a></li><li><a href=https://kubectl.docs.kubernetes.io>Kubectl Book</a></li><li><a href=/docs/reference/generated/kubernetes-api/v1.25/>Kubernetes API Reference</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f37749a83c2916b63279ea60f3cfe53e>1.3.3 - Object Names and IDs</h1><p>Each object in your cluster has a <a href=#names><em>Name</em></a> that is unique for that type of resource.
Every Kubernetes object also has a <a href=#uids><em>UID</em></a> that is unique across your whole cluster.</p><p>For example, you can only have one Pod named <code>myapp-1234</code> within the same <a href=/docs/concepts/overview/working-with-objects/namespaces/>namespace</a>, but you can have one Pod and one Deployment that are each named <code>myapp-1234</code>.</p><p>For non-unique user-provided attributes, Kubernetes provides <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a> and <a href=/docs/concepts/overview/working-with-objects/annotations/>annotations</a>.</p><h2 id=names>Names</h2><p>A client-provided string that refers to an object in a resource URL, such as <code>/api/v1/pods/some-name</code>.</p><p>Only one object of a given kind can have a given name at a time. However, if you delete the object, you can make a new object with the same name.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In cases when objects represent a physical entity, like a Node representing a physical host, when the host is re-created under the same name without deleting and re-creating the Node, Kubernetes treats the new host as the old one, which may lead to inconsistencies.</div><p>Below are four types of commonly used name constraints for resources.</p><h3 id=dns-subdomain-names>DNS Subdomain Names</h3><p>Most resource types require a name that can be used as a DNS subdomain name
as defined in <a href=https://tools.ietf.org/html/rfc1123>RFC 1123</a>.
This means the name must:</p><ul><li>contain no more than 253 characters</li><li>contain only lowercase alphanumeric characters, '-' or '.'</li><li>start with an alphanumeric character</li><li>end with an alphanumeric character</li></ul><h3 id=dns-label-names>RFC 1123 Label Names</h3><p>Some resource types require their names to follow the DNS
label standard as defined in <a href=https://tools.ietf.org/html/rfc1123>RFC 1123</a>.
This means the name must:</p><ul><li>contain at most 63 characters</li><li>contain only lowercase alphanumeric characters or '-'</li><li>start with an alphanumeric character</li><li>end with an alphanumeric character</li></ul><h3 id=rfc-1035-label-names>RFC 1035 Label Names</h3><p>Some resource types require their names to follow the DNS
label standard as defined in <a href=https://tools.ietf.org/html/rfc1035>RFC 1035</a>.
This means the name must:</p><ul><li>contain at most 63 characters</li><li>contain only lowercase alphanumeric characters or '-'</li><li>start with an alphabetic character</li><li>end with an alphanumeric character</li></ul><h3 id=path-segment-names>Path Segment Names</h3><p>Some resource types require their names to be able to be safely encoded as a
path segment. In other words, the name may not be "." or ".." and the name may
not contain "/" or "%".</p><p>Here's an example manifest for a Pod named <code>nginx-demo</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Some resource types have additional restrictions on their names.</div><h2 id=uids>UIDs</h2><p>A Kubernetes systems-generated string to uniquely identify objects.</p><p>Every object created over the whole lifetime of a Kubernetes cluster has a distinct UID. It is intended to distinguish between historical occurrences of similar entities.</p><p>Kubernetes UIDs are universally unique identifiers (also known as UUIDs).
UUIDs are standardized as ISO/IEC 9834-8 and as ITU-T X.667.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a> and <a href=/docs/concepts/overview/working-with-objects/annotations/>annotations</a> in Kubernetes.</li><li>See the <a href=https://git.k8s.io/design-proposals-archive/architecture/identifiers.md>Identifiers and Names in Kubernetes</a> design document.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f1dec4557fb8ffbac9f11390aaaf9fa4>1.3.4 - Labels and Selectors</h1><p><em>Labels</em> are key/value pairs that are attached to objects, such as pods.
Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users, but do not directly imply semantics to the core system.
Labels can be used to organize and to select subsets of objects. Labels can be attached to objects at creation time and subsequently added and modified at any time.
Each object can have a set of key/value labels defined. Each Key must be unique for a given object.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;metadata&#34;</span><span>:</span> {
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;key1&#34;</span> : <span style=color:#b44>&#34;value1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;key2&#34;</span> : <span style=color:#b44>&#34;value2&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Labels allow for efficient queries and watches and are ideal for use in UIs
and CLIs. Non-identifying information should be recorded using
<a href=/docs/concepts/overview/working-with-objects/annotations/>annotations</a>.</p><h2 id=motivation>Motivation</h2><p>Labels enable users to map their own organizational structures onto system objects in a loosely coupled fashion, without requiring clients to store these mappings.</p><p>Service deployments and batch processing pipelines are often multi-dimensional entities (e.g., multiple partitions or deployments, multiple release tracks, multiple tiers, multiple micro-services per tier). Management often requires cross-cutting operations, which breaks encapsulation of strictly hierarchical representations, especially rigid hierarchies determined by the infrastructure rather than by users.</p><p>Example labels:</p><ul><li><code>"release" : "stable"</code>, <code>"release" : "canary"</code></li><li><code>"environment" : "dev"</code>, <code>"environment" : "qa"</code>, <code>"environment" : "production"</code></li><li><code>"tier" : "frontend"</code>, <code>"tier" : "backend"</code>, <code>"tier" : "cache"</code></li><li><code>"partition" : "customerA"</code>, <code>"partition" : "customerB"</code></li><li><code>"track" : "daily"</code>, <code>"track" : "weekly"</code></li></ul><p>These are examples of <a href=/docs/concepts/overview/working-with-objects/common-labels/>commonly used labels</a>; you are free to develop your own conventions. Keep in mind that label Key must be unique for a given object.</p><h2 id=syntax-and-character-set>Syntax and character set</h2><p><em>Labels</em> are key/value pairs. Valid label keys have two segments: an optional prefix and name, separated by a slash (<code>/</code>). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character (<code>[a-z0-9A-Z]</code>) with dashes (<code>-</code>), underscores (<code>_</code>), dots (<code>.</code>), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (<code>.</code>), not longer than 253 characters in total, followed by a slash (<code>/</code>).</p><p>If the prefix is omitted, the label Key is presumed to be private to the user. Automated system components (e.g. <code>kube-scheduler</code>, <code>kube-controller-manager</code>, <code>kube-apiserver</code>, <code>kubectl</code>, or other third-party automation) which add labels to end-user objects must specify a prefix.</p><p>The <code>kubernetes.io/</code> and <code>k8s.io/</code> prefixes are <a href=/docs/reference/labels-annotations-taints/>reserved</a> for Kubernetes core components.</p><p>Valid label value:</p><ul><li>must be 63 characters or less (can be empty),</li><li>unless empty, must begin and end with an alphanumeric character (<code>[a-z0-9A-Z]</code>),</li><li>could contain dashes (<code>-</code>), underscores (<code>_</code>), dots (<code>.</code>), and alphanumerics between.</li></ul><p>For example, here's the configuration file for a Pod that has two labels <code>environment: production</code> and <code>app: nginx</code> :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>label-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>environment</span>:<span style=color:#bbb> </span>production<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=label-selectors>Label selectors</h2><p>Unlike <a href=/docs/concepts/overview/working-with-objects/names/>names and UIDs</a>, labels do not provide uniqueness. In general, we expect many objects to carry the same label(s).</p><p>Via a <em>label selector</em>, the client/user can identify a set of objects. The label selector is the core grouping primitive in Kubernetes.</p><p>The API currently supports two types of selectors: <em>equality-based</em> and <em>set-based</em>.
A label selector can be made of multiple <em>requirements</em> which are comma-separated. In the case of multiple requirements, all must be satisfied so the comma separator acts as a logical <em>AND</em> (<code>&&</code>) operator.</p><p>The semantics of empty or non-specified selectors are dependent on the context,
and API types that use selectors should document the validity and meaning of
them.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> For some API types, such as ReplicaSets, the label selectors of two instances must not overlap within a namespace, or the controller can see that as conflicting instructions and fail to determine how many replicas should be present.</div><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> For both equality-based and set-based conditions there is no logical <em>OR</em> (<code>||</code>) operator. Ensure your filter statements are structured accordingly.</div><h3 id=equality-based-requirement><em>Equality-based</em> requirement</h3><p><em>Equality-</em> or <em>inequality-based</em> requirements allow filtering by label keys and values. Matching objects must satisfy all of the specified label constraints, though they may have additional labels as well.
Three kinds of operators are admitted <code>=</code>,<code>==</code>,<code>!=</code>. The first two represent <em>equality</em> (and are synonyms), while the latter represents <em>inequality</em>. For example:</p><pre tabindex=0><code>environment = production
tier != frontend
</code></pre><p>The former selects all resources with key equal to <code>environment</code> and value equal to <code>production</code>.
The latter selects all resources with key equal to <code>tier</code> and value distinct from <code>frontend</code>, and all resources with no labels with the <code>tier</code> key.
One could filter for resources in <code>production</code> excluding <code>frontend</code> using the comma operator: <code>environment=production,tier!=frontend</code></p><p>One usage scenario for equality-based label requirement is for Pods to specify
node selection criteria. For example, the sample Pod below selects nodes with
the label "<code>accelerator=nvidia-tesla-p100</code>".</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cuda-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cuda-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;registry.k8s.io/cuda-vector-add:v0.1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>nvidia.com/gpu</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>accelerator</span>:<span style=color:#bbb> </span>nvidia-tesla-p100<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=set-based-requirement><em>Set-based</em> requirement</h3><p><em>Set-based</em> label requirements allow filtering keys according to a set of values. Three kinds of operators are supported: <code>in</code>,<code>notin</code> and <code>exists</code> (only the key identifier). For example:</p><pre tabindex=0><code>environment in (production, qa)
tier notin (frontend, backend)
partition
!partition
</code></pre><ul><li>The first example selects all resources with key equal to <code>environment</code> and value equal to <code>production</code> or <code>qa</code>.</li><li>The second example selects all resources with key equal to <code>tier</code> and values other than <code>frontend</code> and <code>backend</code>, and all resources with no labels with the <code>tier</code> key.</li><li>The third example selects all resources including a label with key <code>partition</code>; no values are checked.</li><li>The fourth example selects all resources without a label with key <code>partition</code>; no values are checked.</li></ul><p>Similarly the comma separator acts as an <em>AND</em> operator. So filtering resources with a <code>partition</code> key (no matter the value) and with <code>environment</code> different than  <code>qa</code> can be achieved using <code>partition,environment notin (qa)</code>.
The <em>set-based</em> label selector is a general form of equality since <code>environment=production</code> is equivalent to <code>environment in (production)</code>; similarly for <code>!=</code> and <code>notin</code>.</p><p><em>Set-based</em> requirements can be mixed with <em>equality-based</em> requirements. For example: <code>partition in (customerA, customerB),environment!=qa</code>.</p><h2 id=api>API</h2><h3 id=list-and-watch-filtering>LIST and WATCH filtering</h3><p>LIST and WATCH operations may specify label selectors to filter the sets of objects returned using a query parameter. Both requirements are permitted (presented here as they would appear in a URL query string):</p><ul><li><em>equality-based</em> requirements: <code>?labelSelector=environment%3Dproduction,tier%3Dfrontend</code></li><li><em>set-based</em> requirements: <code>?labelSelector=environment+in+%28production%2Cqa%29%2Ctier+in+%28frontend%29</code></li></ul><p>Both label selector styles can be used to list or watch resources via a REST client. For example, targeting <code>apiserver</code> with <code>kubectl</code> and using <em>equality-based</em> one may write:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>environment</span><span style=color:#666>=</span>production,tier<span style=color:#666>=</span>frontend
</span></span></code></pre></div><p>or using <em>set-based</em> requirements:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b44>&#39;environment in (production),tier in (frontend)&#39;</span>
</span></span></code></pre></div><p>As already mentioned <em>set-based</em> requirements are more expressive.  For instance, they can implement the <em>OR</em> operator on values:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b44>&#39;environment in (production, qa)&#39;</span>
</span></span></code></pre></div><p>or restricting negative matching via <em>exists</em> operator:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b44>&#39;environment,environment notin (frontend)&#39;</span>
</span></span></code></pre></div><h3 id=set-references-in-api-objects>Set references in API objects</h3><p>Some Kubernetes objects, such as <a href=/docs/concepts/services-networking/service/><code>services</code></a>
and <a href=/docs/concepts/workloads/controllers/replicationcontroller/><code>replicationcontrollers</code></a>,
also use label selectors to specify sets of other resources, such as
<a href=/docs/concepts/workloads/pods/>pods</a>.</p><h4 id=service-and-replicationcontroller>Service and ReplicationController</h4><p>The set of pods that a <code>service</code> targets is defined with a label selector. Similarly, the population of pods that a <code>replicationcontroller</code> should manage is also defined with a label selector.</p><p>Labels selectors for both objects are defined in <code>json</code> or <code>yaml</code> files using maps, and only <em>equality-based</em> requirement selectors are supported:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;selector&#34;</span><span>:</span> {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;component&#34;</span> : <span style=color:#b44>&#34;redis&#34;</span>,
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>or</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span></code></pre></div><p>this selector (respectively in <code>json</code> or <code>yaml</code> format) is equivalent to <code>component=redis</code> or <code>component in (redis)</code>.</p><h4 id=resources-that-support-set-based-requirements>Resources that support set-based requirements</h4><p>Newer resources, such as <a href=/docs/concepts/workloads/controllers/job/><code>Job</code></a>,
<a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a>,
<a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a>, and
<a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a>,
support <em>set-based</em> requirements as well.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>component</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- {<span style=color:green;font-weight:700>key: tier, operator: In, values</span>:<span style=color:#bbb> </span>[cache]}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- {<span style=color:green;font-weight:700>key: environment, operator: NotIn, values</span>:<span style=color:#bbb> </span>[dev]}<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>matchLabels</code> is a map of <code>{key,value}</code> pairs. A single <code>{key,value}</code> in the <code>matchLabels</code> map is equivalent to an element of <code>matchExpressions</code>, whose <code>key</code> field is "key", the <code>operator</code> is "In", and the <code>values</code> array contains only "value". <code>matchExpressions</code> is a list of pod selector requirements. Valid operators include In, NotIn, Exists, and DoesNotExist. The values set must be non-empty in the case of In and NotIn. All of the requirements, from both <code>matchLabels</code> and <code>matchExpressions</code> are ANDed together -- they must all be satisfied in order to match.</p><h4 id=selecting-sets-of-nodes>Selecting sets of nodes</h4><p>One use case for selecting over labels is to constrain the set of nodes onto which a pod can schedule.
See the documentation on <a href=/docs/concepts/scheduling-eviction/assign-pod-node/>node selection</a> for more information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-1127165f472b7181b9c1d5a0b187d620>1.3.5 - Namespaces</h1><p>In Kubernetes, <em>namespaces</em> provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects <em>(e.g. Deployments, Services, etc)</em> and not for cluster-wide objects <em>(e.g. StorageClass, Nodes, PersistentVolumes, etc)</em>.</p><h2 id=when-to-use-multiple-namespaces>When to Use Multiple Namespaces</h2><p>Namespaces are intended for use in environments with many users spread across multiple
teams, or projects. For clusters with a few to tens of users, you should not
need to create or think about namespaces at all. Start using namespaces when you
need the features they provide.</p><p>Namespaces provide a scope for names. Names of resources need to be unique within a namespace,
but not across namespaces. Namespaces cannot be nested inside one another and each Kubernetes
resource can only be in one namespace.</p><p>Namespaces are a way to divide cluster resources between multiple users (via <a href=/docs/concepts/policy/resource-quotas/>resource quota</a>).</p><p>It is not necessary to use multiple namespaces to separate slightly different
resources, such as different versions of the same software: use
<a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a> to distinguish
resources within the same namespace.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> For a production cluster, consider <em>not</em> using the <code>default</code> namespace. Instead, make other namespaces and use those.</div><h2 id=initial-namespaces>Initial namespaces</h2><p>Kubernetes starts with four initial namespaces:</p><dl><dt><code>default</code></dt><dd>Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.</dd><dt><code>kube-node-lease</code></dt><dd>This namespace holds <a href=/docs/reference/kubernetes-api/cluster-resources/lease-v1/>Lease</a> objects associated with each node. Node leases allow the kubelet to send <a href=/docs/concepts/architecture/nodes/#heartbeats>heartbeats</a> so that the control plane can detect node failure.</dd><dt><code>kube-public</code></dt><dd>This namespace is readable by <em>all</em> clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.</dd><dt><code>kube-system</code></dt><dd>The namespace for objects created by the Kubernetes system.</dd></dl><h2 id=working-with-namespaces>Working with Namespaces</h2><p>Creation and deletion of namespaces are described in the
<a href=/docs/tasks/administer-cluster/namespaces>Admin Guide documentation for namespaces</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Avoid creating namespaces with the prefix <code>kube-</code>, since it is reserved for Kubernetes system namespaces.</div><h3 id=viewing-namespaces>Viewing namespaces</h3><p>You can list the current namespaces in a cluster using:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get namespace
</span></span></code></pre></div><pre tabindex=0><code>NAME              STATUS   AGE
default           Active   1d
kube-node-lease   Active   1d
kube-public       Active   1d
kube-system       Active   1d
</code></pre><h3 id=setting-the-namespace-for-a-request>Setting the namespace for a request</h3><p>To set the namespace for a current request, use the <code>--namespace</code> flag.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl run nginx --image<span style=color:#666>=</span>nginx --namespace<span style=color:#666>=</span>&lt;insert-namespace-name-here&gt;
</span></span><span style=display:flex><span>kubectl get pods --namespace<span style=color:#666>=</span>&lt;insert-namespace-name-here&gt;
</span></span></code></pre></div><h3 id=setting-the-namespace-preference>Setting the namespace preference</h3><p>You can permanently save the namespace for all subsequent kubectl commands in that
context.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl config set-context --current --namespace<span style=color:#666>=</span>&lt;insert-namespace-name-here&gt;
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Validate it</span>
</span></span><span style=display:flex><span>kubectl config view --minify | grep namespace:
</span></span></code></pre></div><h2 id=namespaces-and-dns>Namespaces and DNS</h2><p>When you create a <a href=/docs/concepts/services-networking/service/>Service</a>,
it creates a corresponding <a href=/docs/concepts/services-networking/dns-pod-service/>DNS entry</a>.
This entry is of the form <code>&lt;service-name>.&lt;namespace-name>.svc.cluster.local</code>, which means
that if a container only uses <code>&lt;service-name></code>, it will resolve to the service which
is local to a namespace. This is useful for using the same configuration across
multiple namespaces such as Development, Staging and Production. If you want to reach
across namespaces, you need to use the fully qualified domain name (FQDN).</p><p>As a result, all namespace names must be valid
<a href=/docs/concepts/overview/working-with-objects/names/#dns-label-names>RFC 1123 DNS labels</a>.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>By creating namespaces with the same name as <a href=https://data.iana.org/TLD/tlds-alpha-by-domain.txt>public top-level
domains</a>, Services in these
namespaces can have short DNS names that overlap with public DNS records.
Workloads from any namespace performing a DNS lookup without a <a href=https://datatracker.ietf.org/doc/html/rfc1034#page-8>trailing dot</a> will
be redirected to those services, taking precedence over public DNS.</p><p>To mitigate this, limit privileges for creating namespaces to trusted users. If
required, you could additionally configure third-party security controls, such
as <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/>admission
webhooks</a>,
to block creating any namespace with the name of <a href=https://data.iana.org/TLD/tlds-alpha-by-domain.txt>public
TLDs</a>.</p></div><h2 id=not-all-objects-are-in-a-namespace>Not all objects are in a namespace</h2><p>Most Kubernetes resources (e.g. pods, services, replication controllers, and others) are
in some namespaces. However namespace resources are not themselves in a namespace.
And low-level resources, such as
<a href=/docs/concepts/architecture/nodes/>nodes</a> and
<a href=/docs/concepts/storage/persistent-volumes/>persistentVolumes</a>, are not in any namespace.</p><p>To see which Kubernetes resources are and aren't in a namespace:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#080;font-style:italic># In a namespace</span>
</span></span><span style=display:flex><span>kubectl api-resources --namespaced<span style=color:#666>=</span><span style=color:#a2f>true</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Not in a namespace</span>
</span></span><span style=display:flex><span>kubectl api-resources --namespaced<span style=color:#666>=</span><span style=color:#a2f>false</span>
</span></span></code></pre></div><h2 id=automatic-labelling>Automatic labelling</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.21 [beta]</code></div><p>The Kubernetes control plane sets an immutable <a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=label>label</a>
<code>kubernetes.io/metadata.name</code> on all namespaces, provided that the <code>NamespaceDefaultLabelName</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> is enabled.
The value of the label is the namespace name.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=/docs/tasks/administer-cluster/namespaces/#creating-a-new-namespace>creating a new namespace</a>.</li><li>Learn more about <a href=/docs/tasks/administer-cluster/namespaces/#deleting-a-namespace>deleting a namespace</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-93cd7a1d4e1623e2bf01afc49a5af69c>1.3.6 - Annotations</h1><p>You can use Kubernetes annotations to attach arbitrary non-identifying metadata
to objects. Clients such as tools and libraries can retrieve this metadata.</p><h2 id=attaching-metadata-to-objects>Attaching metadata to objects</h2><p>You can use either labels or annotations to attach metadata to Kubernetes
objects. Labels can be used to select objects and to find
collections of objects that satisfy certain conditions. In contrast, annotations
are not used to identify and select objects. The metadata
in an annotation can be small or large, structured or unstructured, and can
include characters not permitted by labels.</p><p>Annotations, like labels, are key/value maps:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;metadata&#34;</span><span>:</span> {
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;annotations&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;key1&#34;</span> : <span style=color:#b44>&#34;value1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;key2&#34;</span> : <span style=color:#b44>&#34;value2&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The keys and the values in the map must be strings. In other words, you cannot use
numeric, boolean, list or other types for either the keys or the values.</div><p>Here are some examples of information that could be recorded in annotations:</p><ul><li><p>Fields managed by a declarative configuration layer. Attaching these fields
as annotations distinguishes them from default values set by clients or
servers, and from auto-generated fields and fields set by
auto-sizing or auto-scaling systems.</p></li><li><p>Build, release, or image information like timestamps, release IDs, git branch,
PR numbers, image hashes, and registry address.</p></li><li><p>Pointers to logging, monitoring, analytics, or audit repositories.</p></li><li><p>Client library or tool information that can be used for debugging purposes:
for example, name, version, and build information.</p></li><li><p>User or tool/system provenance information, such as URLs of related objects
from other ecosystem components.</p></li><li><p>Lightweight rollout tool metadata: for example, config or checkpoints.</p></li><li><p>Phone or pager numbers of persons responsible, or directory entries that
specify where that information can be found, such as a team web site.</p></li><li><p>Directives from the end-user to the implementations to modify behavior or
engage non-standard features.</p></li></ul><p>Instead of using annotations, you could store this type of information in an
external database or directory, but that would make it much harder to produce
shared client libraries and tools for deployment, management, introspection,
and the like.</p><h2 id=syntax-and-character-set>Syntax and character set</h2><p><em>Annotations</em> are key/value pairs. Valid annotation keys have two segments: an optional prefix and name, separated by a slash (<code>/</code>). The name segment is required and must be 63 characters or less, beginning and ending with an alphanumeric character (<code>[a-z0-9A-Z]</code>) with dashes (<code>-</code>), underscores (<code>_</code>), dots (<code>.</code>), and alphanumerics between. The prefix is optional. If specified, the prefix must be a DNS subdomain: a series of DNS labels separated by dots (<code>.</code>), not longer than 253 characters in total, followed by a slash (<code>/</code>).</p><p>If the prefix is omitted, the annotation Key is presumed to be private to the user. Automated system components (e.g. <code>kube-scheduler</code>, <code>kube-controller-manager</code>, <code>kube-apiserver</code>, <code>kubectl</code>, or other third-party automation) which add annotations to end-user objects must specify a prefix.</p><p>The <code>kubernetes.io/</code> and <code>k8s.io/</code> prefixes are reserved for Kubernetes core components.</p><p>For example, here's the configuration file for a Pod that has the annotation <code>imageregistry: https://hub.docker.com/</code> :</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>annotations-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imageregistry</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;https://hub.docker.com/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=what-s-next>What's next</h2><p>Learn more about <a href=/docs/concepts/overview/working-with-objects/labels/>Labels and Selectors</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-046c03090d47bc4b89b818dc645c3865>1.3.7 - Field Selectors</h1><p><em>Field selectors</em> let you <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects>select Kubernetes resources</a> based on the value of one or more resource fields. Here are some examples of field selector queries:</p><ul><li><code>metadata.name=my-service</code></li><li><code>metadata.namespace!=default</code></li><li><code>status.phase=Pending</code></li></ul><p>This <code>kubectl</code> command selects all Pods for which the value of the <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase><code>status.phase</code></a> field is <code>Running</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods --field-selector status.phase<span style=color:#666>=</span>Running
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Field selectors are essentially resource <em>filters</em>. By default, no selectors/filters are applied, meaning that all resources of the specified type are selected. This makes the <code>kubectl</code> queries <code>kubectl get pods</code> and <code>kubectl get pods --field-selector ""</code> equivalent.</div><h2 id=supported-fields>Supported fields</h2><p>Supported field selectors vary by Kubernetes resource type. All resource types support the <code>metadata.name</code> and <code>metadata.namespace</code> fields. Using unsupported field selectors produces an error. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get ingress --field-selector foo.bar<span style=color:#666>=</span>baz
</span></span></code></pre></div><pre tabindex=0><code>Error from server (BadRequest): Unable to find &#34;ingresses&#34; that match label selector &#34;&#34;, field selector &#34;foo.bar=baz&#34;: &#34;foo.bar&#34; is not a known field selector: only &#34;metadata.name&#34;, &#34;metadata.namespace&#34;
</code></pre><h2 id=supported-operators>Supported operators</h2><p>You can use the <code>=</code>, <code>==</code>, and <code>!=</code> operators with field selectors (<code>=</code> and <code>==</code> mean the same thing). This <code>kubectl</code> command, for example, selects all Kubernetes Services that aren't in the <code>default</code> namespace:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get services  --all-namespaces --field-selector metadata.namespace!<span style=color:#666>=</span>default
</span></span></code></pre></div><h2 id=chained-selectors>Chained selectors</h2><p>As with <a href=/docs/concepts/overview/working-with-objects/labels>label</a> and other selectors, field selectors can be chained together as a comma-separated list. This <code>kubectl</code> command selects all Pods for which the <code>status.phase</code> does not equal <code>Running</code> and the <code>spec.restartPolicy</code> field equals <code>Always</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods --field-selector<span style=color:#666>=</span>status.phase!<span style=color:#666>=</span>Running,spec.restartPolicy<span style=color:#666>=</span>Always
</span></span></code></pre></div><h2 id=multiple-resource-types>Multiple resource types</h2><p>You can use field selectors across multiple resource types. This <code>kubectl</code> command selects all Statefulsets and Services that are not in the <code>default</code> namespace:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!<span style=color:#666>=</span>default
</span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-13ce5627ef1dc8cbb4530ed231cb7d38>1.3.8 - Finalizers</h1><p>Finalizers are namespaced keys that tell Kubernetes to wait until specific
conditions are met before it fully deletes resources marked for deletion.
Finalizers alert <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controllers>controllers</a>
to clean up resources the deleted object owned.</p><p>When you tell Kubernetes to delete an object that has finalizers specified for
it, the Kubernetes API marks the object for deletion by populating <code>.metadata.deletionTimestamp</code>,
and returns a <code>202</code> status code (HTTP "Accepted"). The target object remains in a terminating state while the
control plane, or other components, take the actions defined by the finalizers.
After these actions are complete, the controller removes the relevant finalizers
from the target object. When the <code>metadata.finalizers</code> field is empty,
Kubernetes considers the deletion complete and deletes the object.</p><p>You can use finalizers to control <a class=glossary-tooltip title='A collective term for the various mechanisms Kubernetes uses to clean up cluster resources.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/garbage-collection/ target=_blank aria-label='garbage collection'>garbage collection</a>
of resources. For example, you can define a finalizer to clean up related resources or
infrastructure before the controller deletes the target resource.</p><p>You can use finalizers to control <a class=glossary-tooltip title='A collective term for the various mechanisms Kubernetes uses to clean up cluster resources.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/garbage-collection/ target=_blank aria-label='garbage collection'>garbage collection</a>
of resources by alerting <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controllers>controllers</a> to perform specific cleanup tasks before
deleting the target resource.</p><p>Finalizers don't usually specify the code to execute. Instead, they are
typically lists of keys on a specific resource similar to annotations.
Kubernetes specifies some finalizers automatically, but you can also specify
your own.</p><h2 id=how-finalizers-work>How finalizers work</h2><p>When you create a resource using a manifest file, you can specify finalizers in
the <code>metadata.finalizers</code> field. When you attempt to delete the resource, the
API server handling the delete request notices the values in the <code>finalizers</code> field
and does the following:</p><ul><li>Modifies the object to add a <code>metadata.deletionTimestamp</code> field with the
time you started the deletion.</li><li>Prevents the object from being removed until its <code>metadata.finalizers</code> field is empty.</li><li>Returns a <code>202</code> status code (HTTP "Accepted")</li></ul><p>The controller managing that finalizer notices the update to the object setting the
<code>metadata.deletionTimestamp</code>, indicating deletion of the object has been requested.
The controller then attempts to satisfy the requirements of the finalizers
specified for that resource. Each time a finalizer condition is satisfied, the
controller removes that key from the resource's <code>finalizers</code> field. When the
<code>finalizers</code> field is emptied, an object with a <code>deletionTimestamp</code> field set
is automatically deleted. You can also use finalizers to prevent deletion of unmanaged resources.</p><p>A common example of a finalizer is <code>kubernetes.io/pv-protection</code>, which prevents
accidental deletion of <code>PersistentVolume</code> objects. When a <code>PersistentVolume</code>
object is in use by a Pod, Kubernetes adds the <code>pv-protection</code> finalizer. If you
try to delete the <code>PersistentVolume</code>, it enters a <code>Terminating</code> status, but the
controller can't delete it because the finalizer exists. When the Pod stops
using the <code>PersistentVolume</code>, Kubernetes clears the <code>pv-protection</code> finalizer,
and the controller deletes the volume.</p><h2 id=owners-labels-finalizers>Owner references, labels, and finalizers</h2><p>Like <a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a>,
<a href=/docs/concepts/overview/working-with-objects/owners-dependents/>owner references</a>
describe the relationships between objects in Kubernetes, but are used for a
different purpose. When a
<a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> manages objects
like Pods, it uses labels to track changes to groups of related objects. For
example, when a <a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Job>Job</a> creates one or
more Pods, the Job controller applies labels to those pods and tracks changes to
any Pods in the cluster with the same label.</p><p>The Job controller also adds <em>owner references</em> to those Pods, pointing at the
Job that created the Pods. If you delete the Job while these Pods are running,
Kubernetes uses the owner references (not labels) to determine which Pods in the
cluster need cleanup.</p><p>Kubernetes also processes finalizers when it identifies owner references on a
resource targeted for deletion.</p><p>In some situations, finalizers can block the deletion of dependent objects,
which can cause the targeted owner object to remain for
longer than expected without being fully deleted. In these situations, you
should check finalizers and owner references on the target owner and dependent
objects to troubleshoot the cause.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In cases where objects are stuck in a deleting state, avoid manually
removing finalizers to allow deletion to continue. Finalizers are usually added
to resources for a reason, so forcefully removing them can lead to issues in
your cluster. This should only be done when the purpose of the finalizer is
understood and is accomplished in another way (for example, manually cleaning
up some dependent object).</div><h2 id=what-s-next>What's next</h2><ul><li>Read <a href=/blog/2021/05/14/using-finalizers-to-control-deletion/>Using Finalizers to Control Deletion</a>
on the Kubernetes blog.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-efaa7a58910b58892dafd50e3b43c93c>1.3.9 - Owners and Dependents</h1><p>In Kubernetes, some objects are <em>owners</em> of other objects. For example, a
<a class=glossary-tooltip title='ReplicaSet ensures that a specified number of Pod replicas are running at one time' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/replicaset/ target=_blank aria-label=ReplicaSet>ReplicaSet</a> is the owner of a set of Pods. These owned objects are <em>dependents</em>
of their owner.</p><p>Ownership is different from the <a href=/docs/concepts/overview/working-with-objects/labels/>labels and selectors</a>
mechanism that some resources also use. For example, consider a Service that
creates <code>EndpointSlice</code> objects. The Service uses labels to allow the control plane to
determine which <code>EndpointSlice</code> objects are used for that Service. In addition
to the labels, each <code>EndpointSlice</code> that is managed on behalf of a Service has
an owner reference. Owner references help different parts of Kubernetes avoid
interfering with objects they don’t control.</p><h2 id=owner-references-in-object-specifications>Owner references in object specifications</h2><p>Dependent objects have a <code>metadata.ownerReferences</code> field that references their
owner object. A valid owner reference consists of the object name and a UID
within the same namespace as the dependent object. Kubernetes sets the value of
this field automatically for objects that are dependents of other objects like
ReplicaSets, DaemonSets, Deployments, Jobs and CronJobs, and ReplicationControllers.
You can also configure these relationships manually by changing the value of
this field. However, you usually don't need to and can allow Kubernetes to
automatically manage the relationships.</p><p>Dependent objects also have an <code>ownerReferences.blockOwnerDeletion</code> field that
takes a boolean value and controls whether specific dependents can block garbage
collection from deleting their owner object. Kubernetes automatically sets this
field to <code>true</code> if a <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>
(for example, the Deployment controller) sets the value of the
<code>metadata.ownerReferences</code> field. You can also set the value of the
<code>blockOwnerDeletion</code> field manually to control which dependents block garbage
collection.</p><p>A Kubernetes admission controller controls user access to change this field for
dependent resources, based on the delete permissions of the owner. This control
prevents unauthorized users from delaying owner object deletion.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Cross-namespace owner references are disallowed by design.
Namespaced dependents can specify cluster-scoped or namespaced owners.
A namespaced owner <strong>must</strong> exist in the same namespace as the dependent.
If it does not, the owner reference is treated as absent, and the dependent
is subject to deletion once all owners are verified absent.</p><p>Cluster-scoped dependents can only specify cluster-scoped owners.
In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,
it is treated as having an unresolvable owner reference, and is not able to be garbage collected.</p><p>In v1.20+, if the garbage collector detects an invalid cross-namespace <code>ownerReference</code>,
or a cluster-scoped dependent with an <code>ownerReference</code> referencing a namespaced kind, a warning Event
with a reason of <code>OwnerRefInvalidNamespace</code> and an <code>involvedObject</code> of the invalid dependent is reported.
You can check for that kind of Event by running
<code>kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace</code>.</p></div><h2 id=ownership-and-finalizers>Ownership and finalizers</h2><p>When you tell Kubernetes to delete a resource, the API server allows the
managing controller to process any <a href=/docs/concepts/overview/working-with-objects/finalizers/>finalizer rules</a>
for the resource. <a class=glossary-tooltip title='A namespaced key that tells Kubernetes to wait until specific conditions are met before it fully deletes an object marked for deletion.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/finalizers/ target=_blank aria-label=Finalizers>Finalizers</a>
prevent accidental deletion of resources your cluster may still need to function
correctly. For example, if you try to delete a <code>PersistentVolume</code> that is still
in use by a Pod, the deletion does not happen immediately because the
<code>PersistentVolume</code> has the <code>kubernetes.io/pv-protection</code> finalizer on it.
Instead, the volume remains in the <code>Terminating</code> status until Kubernetes clears
the finalizer, which only happens after the <code>PersistentVolume</code> is no longer
bound to a Pod.</p><p>Kubernetes also adds finalizers to an owner resource when you use either
<a href=/docs/concepts/architecture/garbage-collection/#cascading-deletion>foreground or orphan cascading deletion</a>.
In foreground deletion, it adds the <code>foreground</code> finalizer so that the
controller must delete dependent resources that also have
<code>ownerReferences.blockOwnerDeletion=true</code> before it deletes the owner. If you
specify an orphan deletion policy, Kubernetes adds the <code>orphan</code> finalizer so
that the controller ignores dependent resources after it deletes the owner
object.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=/docs/concepts/overview/working-with-objects/finalizers/>Kubernetes finalizers</a>.</li><li>Learn about <a href=/docs/concepts/architecture/garbage-collection>garbage collection</a>.</li><li>Read the API reference for <a href=/docs/reference/kubernetes-api/common-definitions/object-meta/#System>object metadata</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5dd62c6a4a481b4cf1ac50f6799eb581>1.3.10 - Recommended Labels</h1><p>You can visualize and manage Kubernetes objects with more tools than kubectl and
the dashboard. A common set of labels allows tools to work interoperably, describing
objects in a common manner that all tools can understand.</p><p>In addition to supporting tooling, the recommended labels describe applications
in a way that can be queried.</p><p>The metadata is organized around the concept of an <em>application</em>. Kubernetes is not
a platform as a service (PaaS) and doesn't have or enforce a formal notion of an application.
Instead, applications are informal and described with metadata. The definition of
what an application contains is loose.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> These are recommended labels. They make it easier to manage applications
but aren't required for any core tooling.</div><p>Shared labels and annotations share a common prefix: <code>app.kubernetes.io</code>. Labels
without a prefix are private to users. The shared prefix ensures that shared labels
do not interfere with custom user labels.</p><h2 id=labels>Labels</h2><p>In order to take full advantage of using these labels, they should be applied
on every resource object.</p><table><thead><tr><th>Key</th><th>Description</th><th>Example</th><th>Type</th></tr></thead><tbody><tr><td><code>app.kubernetes.io/name</code></td><td>The name of the application</td><td><code>mysql</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/instance</code></td><td>A unique name identifying the instance of an application</td><td><code>mysql-abcxzy</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/version</code></td><td>The current version of the application (e.g., a semantic version, revision hash, etc.)</td><td><code>5.7.21</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/component</code></td><td>The component within the architecture</td><td><code>database</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/part-of</code></td><td>The name of a higher level application this one is part of</td><td><code>wordpress</code></td><td>string</td></tr><tr><td><code>app.kubernetes.io/managed-by</code></td><td>The tool being used to manage the operation of an application</td><td><code>helm</code></td><td>string</td></tr></tbody></table><p>To illustrate these labels in action, consider the following <a class=glossary-tooltip title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a> object:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#080;font-style:italic># This is an excerpt</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>mysql-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5.7.21&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>database<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=applications-and-instances-of-applications>Applications And Instances Of Applications</h2><p>An application can be installed one or more times into a Kubernetes cluster and,
in some cases, the same namespace. For example, WordPress can be installed more
than once where different websites are different installations of WordPress.</p><p>The name of an application and the instance name are recorded separately. For
example, WordPress has a <code>app.kubernetes.io/name</code> of <code>wordpress</code> while it has
an instance name, represented as <code>app.kubernetes.io/instance</code> with a value of
<code>wordpress-abcxzy</code>. This enables the application and instance of the application
to be identifiable. Every instance of an application must have a unique name.</p><h2 id=examples>Examples</h2><p>To illustrate different ways to use these labels the following examples have varying complexity.</p><h3 id=a-simple-stateless-service>A Simple Stateless Service</h3><p>Consider the case for a simple stateless service deployed using <code>Deployment</code> and <code>Service</code> objects. The following two snippets represent how the labels could be used in their simplest form.</p><p>The <code>Deployment</code> is used to oversee the pods running the application itself.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>myservice-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The <code>Service</code> is used to expose the application.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>myservice-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=web-application-with-a-database>Web Application With A Database</h3><p>Consider a slightly more complicated application: a web application (WordPress)
using a database (MySQL), installed using Helm. The following snippets illustrate
the start of objects used to deploy this application.</p><p>The start to the following <code>Deployment</code> is used for WordPress:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>wordpress-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4.9.4&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The <code>Service</code> is used to expose WordPress:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>wordpress-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4.9.4&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>MySQL is exposed as a <code>StatefulSet</code> with metadata for both it and the larger application it belongs to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>mysql-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5.7.21&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>database<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The <code>Service</code> is used to expose MySQL as part of WordPress:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/instance</span>:<span style=color:#bbb> </span>mysql-abcxzy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/version</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5.7.21&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/managed-by</span>:<span style=color:#bbb> </span>helm<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>database<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/part-of</span>:<span style=color:#bbb> </span>wordpress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>With the MySQL <code>StatefulSet</code> and <code>Service</code> you'll notice information about both MySQL and WordPress, the broader application, are included.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-2bf36ccd6b3dbeafecf87c39761b07c7>2 - Cluster Architecture</h1><div class=lead>The architectural concepts behind Kubernetes.</div></div><div class=td-content><h1 id=pg-9ef2890698e773b6c0d24fd2c20146f5>2.1 - Nodes</h1><p>Kubernetes runs your workload by placing containers into Pods to run on <em>Nodes</em>.
A node may be a virtual or physical machine, depending on the cluster. Each node
is managed by the
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>
and contains the services necessary to run
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>.</p><p>Typically you have several nodes in a cluster; in a learning or resource-limited
environment, you might have only one node.</p><p>The <a href=/docs/concepts/overview/components/#node-components>components</a> on a node include the
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>, a
<a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>, and the
<a class=glossary-tooltip title='kube-proxy is a network proxy that runs on each node in the cluster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a>.</p><h2 id=management>Management</h2><p>There are two main ways to have Nodes added to the <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a>:</p><ol><li>The kubelet on a node self-registers to the control plane</li><li>You (or another human user) manually add a Node object</li></ol><p>After you create a Node <a class=glossary-tooltip title='A entity in the Kubernetes system, representing part of the state of your cluster.' data-toggle=tooltip data-placement=top href=https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-objects target=_blank aria-label=object>object</a>,
or the kubelet on a node self-registers, the control plane checks whether the new Node object is
valid. For example, if you try to create a Node from the following JSON manifest:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Node&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;10.240.79.157&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;labels&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;my-first-k8s-node&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Kubernetes creates a Node object internally (the representation). Kubernetes checks
that a kubelet has registered to the API server that matches the <code>metadata.name</code>
field of the Node. If the node is healthy (i.e. all necessary services are running),
then it is eligible to run a Pod. Otherwise, that node is ignored for any cluster activity
until it becomes healthy.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Kubernetes keeps the object for the invalid Node and continues checking to see whether
it becomes healthy.</p><p>You, or a <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>, must explicitly
delete the Node object to stop that health checking.</p></div><p>The name of a Node object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><h3 id=node-name-uniqueness>Node name uniqueness</h3><p>The <a href=/docs/concepts/overview/working-with-objects/names#names>name</a> identifies a Node. Two Nodes
cannot have the same name at the same time. Kubernetes also assumes that a resource with the same
name is the same object. In case of a Node, it is implicitly assumed that an instance using the
same name will have the same state (e.g. network settings, root disk contents)
and attributes like node labels. This may lead to
inconsistencies if an instance was modified without changing its name. If the Node needs to be
replaced or updated significantly, the existing Node object needs to be removed from API server
first and re-added after the update.</p><h3 id=self-registration-of-nodes>Self-registration of Nodes</h3><p>When the kubelet flag <code>--register-node</code> is true (the default), the kubelet will attempt to
register itself with the API server. This is the preferred pattern, used by most distros.</p><p>For self-registration, the kubelet is started with the following options:</p><ul><li><p><code>--kubeconfig</code> - Path to credentials to authenticate itself to the API server.</p></li><li><p><code>--cloud-provider</code> - How to talk to a <a class=glossary-tooltip title='An organization that offers a cloud computing platform.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cloud-provider' target=_blank aria-label='cloud provider'>cloud provider</a>
to read metadata about itself.</p></li><li><p><code>--register-node</code> - Automatically register with the API server.</p></li><li><p><code>--register-with-taints</code> - Register the node with the given list of
<a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=taints>taints</a> (comma separated <code>&lt;key>=&lt;value>:&lt;effect></code>).</p><p>No-op if <code>register-node</code> is false.</p></li><li><p><code>--node-ip</code> - IP address of the node.</p></li><li><p><code>--node-labels</code> - <a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=Labels>Labels</a> to add when registering the node
in the cluster (see label restrictions enforced by the
<a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction admission plugin</a>).</p></li><li><p><code>--node-status-update-frequency</code> - Specifies how often kubelet posts its node status to the API server.</p></li></ul><p>When the <a href=/docs/reference/access-authn-authz/node/>Node authorization mode</a> and
<a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction admission plugin</a>
are enabled, kubelets are only authorized to create/modify their own Node resource.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>As mentioned in the <a href=#node-name-uniqueness>Node name uniqueness</a> section,
when Node configuration needs to be updated, it is a good practice to re-register
the node with the API server. For example, if the kubelet being restarted with
the new set of <code>--node-labels</code>, but the same Node name is used, the change will
not take an effect, as labels are being set on the Node registration.</p><p>Pods already scheduled on the Node may misbehave or cause issues if the Node
configuration will be changed on kubelet restart. For example, already running
Pod may be tainted against the new labels assigned to the Node, while other
Pods, that are incompatible with that Pod will be scheduled based on this new
label. Node re-registration ensures all Pods will be drained and properly
re-scheduled.</p></div><h3 id=manual-node-administration>Manual Node administration</h3><p>You can create and modify Node objects using
<a class=glossary-tooltip title='A command line tool for communicating with a Kubernetes cluster.' data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a>.</p><p>When you want to create Node objects manually, set the kubelet flag <code>--register-node=false</code>.</p><p>You can modify Node objects regardless of the setting of <code>--register-node</code>.
For example, you can set labels on an existing Node or mark it unschedulable.</p><p>You can use labels on Nodes in conjunction with node selectors on Pods to control
scheduling. For example, you can constrain a Pod to only be eligible to run on
a subset of the available nodes.</p><p>Marking a node as unschedulable prevents the scheduler from placing new pods onto
that Node but does not affect existing Pods on the Node. This is useful as a
preparatory step before a node reboot or other maintenance.</p><p>To mark a Node unschedulable, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl cordon <span style=color:#b8860b>$NODENAME</span>
</span></span></code></pre></div><p>See <a href=/docs/tasks/administer-cluster/safely-drain-node/>Safely Drain a Node</a>
for more details.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Pods that are part of a <a class=glossary-tooltip title='Ensures a copy of a Pod is running across a set of nodes in a cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a> tolerate
being run on an unschedulable Node. DaemonSets typically provide node-local services
that should run on the Node even if it is being drained of workload applications.</div><h2 id=node-status>Node status</h2><p>A Node's status contains the following information:</p><ul><li><a href=#addresses>Addresses</a></li><li><a href=#condition>Conditions</a></li><li><a href=#capacity>Capacity and Allocatable</a></li><li><a href=#info>Info</a></li></ul><p>You can use <code>kubectl</code> to view a Node's status and other details:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe node &lt;insert-node-name-here&gt;
</span></span></code></pre></div><p>Each section of the output is described below.</p><h3 id=addresses>Addresses</h3><p>The usage of these fields varies depending on your cloud provider or bare metal configuration.</p><ul><li>HostName: The hostname as reported by the node's kernel. Can be overridden via the kubelet
<code>--hostname-override</code> parameter.</li><li>ExternalIP: Typically the IP address of the node that is externally routable (available from
outside the cluster).</li><li>InternalIP: Typically the IP address of the node that is routable only within the cluster.</li></ul><h3 id=condition>Conditions</h3><p>The <code>conditions</code> field describes the status of all <code>Running</code> nodes. Examples of conditions include:</p><table><caption style=display:none>Node conditions, and a description of when each condition applies.</caption><thead><tr><th>Node Condition</th><th>Description</th></tr></thead><tbody><tr><td><code>Ready</code></td><td><code>True</code> if the node is healthy and ready to accept pods, <code>False</code> if the node is not healthy and is not accepting pods, and <code>Unknown</code> if the node controller has not heard from the node in the last <code>node-monitor-grace-period</code> (default is 40 seconds)</td></tr><tr><td><code>DiskPressure</code></td><td><code>True</code> if pressure exists on the disk size—that is, if the disk capacity is low; otherwise <code>False</code></td></tr><tr><td><code>MemoryPressure</code></td><td><code>True</code> if pressure exists on the node memory—that is, if the node memory is low; otherwise <code>False</code></td></tr><tr><td><code>PIDPressure</code></td><td><code>True</code> if pressure exists on the processes—that is, if there are too many processes on the node; otherwise <code>False</code></td></tr><tr><td><code>NetworkUnavailable</code></td><td><code>True</code> if the network for the node is not correctly configured, otherwise <code>False</code></td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you use command-line tools to print details of a cordoned Node, the Condition includes
<code>SchedulingDisabled</code>. <code>SchedulingDisabled</code> is not a Condition in the Kubernetes API; instead,
cordoned nodes are marked Unschedulable in their spec.</div><p>In the Kubernetes API, a node's condition is represented as part of the <code>.status</code>
of the Node resource. For example, the following JSON structure describes a healthy node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#b44>&#34;conditions&#34;</span><span>:</span> [
</span></span><span style=display:flex><span>  {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;Ready&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;True&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;reason&#34;</span>: <span style=color:#b44>&#34;KubeletReady&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;message&#34;</span>: <span style=color:#b44>&#34;kubelet is posting ready status&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;lastHeartbeatTime&#34;</span>: <span style=color:#b44>&#34;2019-06-05T18:38:35Z&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;lastTransitionTime&#34;</span>: <span style=color:#b44>&#34;2019-06-05T11:41:27Z&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>If the <code>status</code> of the Ready condition remains <code>Unknown</code> or <code>False</code> for longer
than the <code>pod-eviction-timeout</code> (an argument passed to the
<a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>), then the <a href=#node-controller>node controller</a> triggers
<a class=glossary-tooltip title='API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/api-eviction/ target=_blank aria-label='API-initiated eviction'>API-initiated eviction</a>
for all Pods assigned to that node. The default eviction timeout duration is
<strong>five minutes</strong>.
In some cases when the node is unreachable, the API server is unable to communicate
with the kubelet on the node. The decision to delete the pods cannot be communicated to
the kubelet until communication with the API server is re-established. In the meantime,
the pods that are scheduled for deletion may continue to run on the partitioned node.</p><p>The node controller does not force delete pods until it is confirmed that they have stopped
running in the cluster. You can see the pods that might be running on an unreachable node as
being in the <code>Terminating</code> or <code>Unknown</code> state. In cases where Kubernetes cannot deduce from the
underlying infrastructure if a node has permanently left a cluster, the cluster administrator
may need to delete the node object by hand. Deleting the node object from Kubernetes causes
all the Pod objects running on the node to be deleted from the API server and frees up their
names.</p><p>When problems occur on nodes, the Kubernetes control plane automatically creates
<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>taints</a> that match the conditions
affecting the node.
The scheduler takes the Node's taints into consideration when assigning a Pod to a Node.
Pods can also have <a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=tolerations>tolerations</a> that let
them run on a Node even though it has a specific taint.</p><p>See <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/#taint-nodes-by-condition>Taint Nodes by Condition</a>
for more details.</p><h3 id=capacity>Capacity and Allocatable</h3><p>Describes the resources available on the node: CPU, memory, and the maximum
number of pods that can be scheduled onto the node.</p><p>The fields in the capacity block indicate the total amount of resources that a
Node has. The allocatable block indicates the amount of resources on a
Node that is available to be consumed by normal Pods.</p><p>You may read more about capacity and allocatable resources while learning how
to <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>reserve compute resources</a>
on a Node.</p><h3 id=info>Info</h3><p>Describes general information about the node, such as kernel version, Kubernetes
version (kubelet and kube-proxy version), container runtime details, and which
operating system the node uses.
The kubelet gathers this information from the node and publishes it into
the Kubernetes API.</p><h2 id=heartbeats>Heartbeats</h2><p>Heartbeats, sent by Kubernetes nodes, help your cluster determine the
availability of each node, and to take action when failures are detected.</p><p>For nodes there are two forms of heartbeats:</p><ul><li>updates to the <code>.status</code> of a Node</li><li><a href=/docs/reference/kubernetes-api/cluster-resources/lease-v1/>Lease</a> objects
within the <code>kube-node-lease</code>
<a class=glossary-tooltip title='An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>.
Each Node has an associated Lease object.</li></ul><p>Compared to updates to <code>.status</code> of a Node, a Lease is a lightweight resource.
Using Leases for heartbeats reduces the performance impact of these updates
for large clusters.</p><p>The kubelet is responsible for creating and updating the <code>.status</code> of Nodes,
and for updating their related Leases.</p><ul><li>The kubelet updates the node's <code>.status</code> either when there is change in status
or if there has been no update for a configured interval. The default interval
for <code>.status</code> updates to Nodes is 5 minutes, which is much longer than the 40
second default timeout for unreachable nodes.</li><li>The kubelet creates and then updates its Lease object every 10 seconds
(the default update interval). Lease updates occur independently from
updates to the Node's <code>.status</code>. If the Lease update fails, the kubelet retries,
using exponential backoff that starts at 200 milliseconds and capped at 7 seconds.</li></ul><h2 id=node-controller>Node controller</h2><p>The node <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> is a
Kubernetes control plane component that manages various aspects of nodes.</p><p>The node controller has multiple roles in a node's life. The first is assigning a
CIDR block to the node when it is registered (if CIDR assignment is turned on).</p><p>The second is keeping the node controller's internal list of nodes up to date with
the cloud provider's list of available machines. When running in a cloud
environment and whenever a node is unhealthy, the node controller asks the cloud
provider if the VM for that node is still available. If not, the node
controller deletes the node from its list of nodes.</p><p>The third is monitoring the nodes' health. The node controller is
responsible for:</p><ul><li>In the case that a node becomes unreachable, updating the <code>Ready</code> condition
in the Node's <code>.status</code> field. In this case the node controller sets the
<code>Ready</code> condition to <code>Unknown</code>.</li><li>If a node remains unreachable: triggering
<a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated eviction</a>
for all of the Pods on the unreachable node. By default, the node controller
waits 5 minutes between marking the node as <code>Unknown</code> and submitting
the first eviction request.</li></ul><p>By default, the node controller checks the state of each node every 5 seconds.
This period can be configured using the <code>--node-monitor-period</code> flag on the
<code>kube-controller-manager</code> component.</p><h3 id=rate-limits-on-eviction>Rate limits on eviction</h3><p>In most cases, the node controller limits the eviction rate to
<code>--node-eviction-rate</code> (default 0.1) per second, meaning it won't evict pods
from more than 1 node per 10 seconds.</p><p>The node eviction behavior changes when a node in a given availability zone
becomes unhealthy. The node controller checks what percentage of nodes in the zone
are unhealthy (the <code>Ready</code> condition is <code>Unknown</code> or <code>False</code>) at
the same time:</p><ul><li>If the fraction of unhealthy nodes is at least <code>--unhealthy-zone-threshold</code>
(default 0.55), then the eviction rate is reduced.</li><li>If the cluster is small (i.e. has less than or equal to
<code>--large-cluster-size-threshold</code> nodes - default 50), then evictions are stopped.</li><li>Otherwise, the eviction rate is reduced to <code>--secondary-node-eviction-rate</code>
(default 0.01) per second.</li></ul><p>The reason these policies are implemented per availability zone is because one
availability zone might become partitioned from the control plane while the others remain
connected. If your cluster does not span multiple cloud provider availability zones,
then the eviction mechanism does not take per-zone unavailability into account.</p><p>A key reason for spreading your nodes across availability zones is so that the
workload can be shifted to healthy zones when one entire zone goes down.
Therefore, if all nodes in a zone are unhealthy, then the node controller evicts at
the normal rate of <code>--node-eviction-rate</code>. The corner case is when all zones are
completely unhealthy (none of the nodes in the cluster are healthy). In such a
case, the node controller assumes that there is some problem with connectivity
between the control plane and the nodes, and doesn't perform any evictions.
(If there has been an outage and some nodes reappear, the node controller does
evict pods from the remaining nodes that are unhealthy or unreachable).</p><p>The node controller is also responsible for evicting pods running on nodes with
<code>NoExecute</code> taints, unless those pods tolerate that taint.
The node controller also adds <a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=taints>taints</a>
corresponding to node problems like node unreachable or not ready. This means
that the scheduler won't place Pods onto unhealthy nodes.</p><h2 id=node-capacity>Resource capacity tracking</h2><p>Node objects track information about the Node's resource capacity: for example, the amount
of memory available and the number of CPUs.
Nodes that <a href=#self-registration-of-nodes>self register</a> report their capacity during
registration. If you <a href=#manual-node-administration>manually</a> add a Node, then
you need to set the node's capacity information when you add it.</p><p>The Kubernetes <a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a> ensures that
there are enough resources for all the Pods on a Node. The scheduler checks that the sum
of the requests of containers on the node is no greater than the node's capacity.
That sum of requests includes all containers managed by the kubelet, but excludes any
containers started directly by the container runtime, and also excludes any
processes running outside of the kubelet's control.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you want to explicitly reserve resources for non-Pod processes, see
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved>reserve resources for system daemons</a>.</div><h2 id=node-topology>Node topology</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code></div><p>If you have enabled the <code>TopologyManager</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>, then
the kubelet can use topology hints when making resource assignment decisions.
See <a href=/docs/tasks/administer-cluster/topology-manager/>Control Topology Management Policies on a Node</a>
for more information.</p><h2 id=graceful-node-shutdown>Graceful node shutdown</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code></div><p>The kubelet attempts to detect node system shutdown and terminates pods running on the node.</p><p>Kubelet ensures that pods follow the normal
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>pod termination process</a>
during the node shutdown.</p><p>The Graceful node shutdown feature depends on systemd since it takes advantage of
<a href=https://www.freedesktop.org/wiki/Software/systemd/inhibit/>systemd inhibitor locks</a> to
delay the node shutdown with a given duration.</p><p>Graceful node shutdown is controlled with the <code>GracefulNodeShutdown</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> which is
enabled by default in 1.21.</p><p>Note that by default, both configuration options described below,
<code>shutdownGracePeriod</code> and <code>shutdownGracePeriodCriticalPods</code> are set to zero,
thus not activating the graceful node shutdown functionality.
To activate the feature, the two kubelet config settings should be configured appropriately and
set to non-zero values.</p><p>During a graceful shutdown, kubelet terminates pods in two phases:</p><ol><li>Terminate regular pods running on the node.</li><li>Terminate <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>critical pods</a>
running on the node.</li></ol><p>Graceful node shutdown feature is configured with two
<a href=/docs/tasks/administer-cluster/kubelet-config-file/><code>KubeletConfiguration</code></a> options:</p><ul><li><code>shutdownGracePeriod</code>:<ul><li>Specifies the total duration that the node should delay the shutdown by. This is the total
grace period for pod termination for both regular and
<a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>critical pods</a>.</li></ul></li><li><code>shutdownGracePeriodCriticalPods</code>:<ul><li>Specifies the duration used to terminate
<a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>critical pods</a>
during a node shutdown. This value should be less than <code>shutdownGracePeriod</code>.</li></ul></li></ul><p>For example, if <code>shutdownGracePeriod=30s</code>, and
<code>shutdownGracePeriodCriticalPods=10s</code>, kubelet will delay the node shutdown by
30 seconds. During the shutdown, the first 20 (30-10) seconds would be reserved
for gracefully terminating normal pods, and the last 10 seconds would be
reserved for terminating <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical>critical pods</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>When pods were evicted during the graceful node shutdown, they are marked as shutdown.
Running <code>kubectl get pods</code> shows the status of the evicted pods as <code>Terminated</code>.
And <code>kubectl describe pod</code> indicates that the pod was evicted because of node shutdown:</p><pre tabindex=0><code>Reason:         Terminated
Message:        Pod was terminated in response to imminent node shutdown.
</code></pre></div><h2 id=non-graceful-node-shutdown>Non Graceful node shutdown</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [alpha]</code></div><p>A node shutdown action may not be detected by kubelet's Node Shutdown Manager,
either because the command does not trigger the inhibitor locks mechanism used by
kubelet or because of a user error, i.e., the ShutdownGracePeriod and
ShutdownGracePeriodCriticalPods are not configured properly. Please refer to above
section <a href=#graceful-node-shutdown>Graceful Node Shutdown</a> for more details.</p><p>When a node is shutdown but not detected by kubelet's Node Shutdown Manager, the pods
that are part of a StatefulSet will be stuck in terminating status on
the shutdown node and cannot move to a new running node. This is because kubelet on
the shutdown node is not available to delete the pods so the StatefulSet cannot
create a new pod with the same name. If there are volumes used by the pods, the
VolumeAttachments will not be deleted from the original shutdown node so the volumes
used by these pods cannot be attached to a new running node. As a result, the
application running on the StatefulSet cannot function properly. If the original
shutdown node comes up, the pods will be deleted by kubelet and new pods will be
created on a different running node. If the original shutdown node does not come up,<br>these pods will be stuck in terminating status on the shutdown node forever.</p><p>To mitigate the above situation, a user can manually add the taint <code>node.kubernetes.io/out-of-service</code> with either <code>NoExecute</code>
or <code>NoSchedule</code> effect to a Node marking it out-of-service.
If the <code>NodeOutOfServiceVolumeDetach</code><a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
is enabled on <code>kube-controller-manager</code>, and a Node is marked out-of-service with this taint, the
pods on the node will be forcefully deleted if there are no matching tolerations on it and volume
detach operations for the pods terminating on the node will happen immediately. This allows the
Pods on the out-of-service node to recover quickly on a different node.</p><p>During a non-graceful shutdown, Pods are terminated in the two phases:</p><ol><li>Force delete the Pods that do not have matching <code>out-of-service</code> tolerations.</li><li>Immediately perform detach volume operation for such pods.</li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong><ul><li>Before adding the taint <code>node.kubernetes.io/out-of-service</code> , it should be verified
that the node is already in shutdown or power off state (not in the middle of
restarting).</li><li>The user is required to manually remove the out-of-service taint after the pods are
moved to a new node and the user has checked that the shutdown node has been
recovered since the user was the one who originally added the taint.</li></ul></div><h3 id=pod-priority-graceful-node-shutdown>Pod Priority based graceful node shutdown</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code></div><p>To provide more flexibility during graceful node shutdown around the ordering
of pods during shutdown, graceful node shutdown honors the PriorityClass for
Pods, provided that you enabled this feature in your cluster. The feature
allows cluster administers to explicitly define the ordering of pods
during graceful node shutdown based on
<a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass>priority classes</a>.</p><p>The <a href=#graceful-node-shutdown>Graceful Node Shutdown</a> feature, as described
above, shuts down pods in two phases, non-critical pods, followed by critical
pods. If additional flexibility is needed to explicitly define the ordering of
pods during shutdown in a more granular way, pod priority based graceful
shutdown can be used.</p><p>When graceful node shutdown honors pod priorities, this makes it possible to do
graceful node shutdown in multiple phases, each phase shutting down a
particular priority class of pods. The kubelet can be configured with the exact
phases and shutdown time per phase.</p><p>Assuming the following custom pod
<a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass>priority classes</a>
in a cluster,</p><table><thead><tr><th>Pod priority class name</th><th>Pod priority class value</th></tr></thead><tbody><tr><td><code>custom-class-a</code></td><td>100000</td></tr><tr><td><code>custom-class-b</code></td><td>10000</td></tr><tr><td><code>custom-class-c</code></td><td>1000</td></tr><tr><td><code>regular/unset</code></td><td>0</td></tr></tbody></table><p>Within the <a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration>kubelet configuration</a>
the settings for <code>shutdownGracePeriodByPodPriority</code> could look like:</p><table><thead><tr><th>Pod priority class value</th><th>Shutdown period</th></tr></thead><tbody><tr><td>100000</td><td>10 seconds</td></tr><tr><td>10000</td><td>180 seconds</td></tr><tr><td>1000</td><td>120 seconds</td></tr><tr><td>0</td><td>60 seconds</td></tr></tbody></table><p>The corresponding kubelet config YAML configuration would be:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>shutdownGracePeriodByPodPriority</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>100000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>10000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>180</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>120</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>priority</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>shutdownGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>60</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The above table implies that any pod with <code>priority</code> value >= 100000 will get
just 10 seconds to stop, any pod with value >= 10000 and &lt; 100000 will get 180
seconds to stop, any pod with value >= 1000 and &lt; 10000 will get 120 seconds to stop.
Finally, all other pods will get 60 seconds to stop.</p><p>One doesn't have to specify values corresponding to all of the classes. For
example, you could instead use these settings:</p><table><thead><tr><th>Pod priority class value</th><th>Shutdown period</th></tr></thead><tbody><tr><td>100000</td><td>300 seconds</td></tr><tr><td>1000</td><td>120 seconds</td></tr><tr><td>0</td><td>60 seconds</td></tr></tbody></table><p>In the above case, the pods with <code>custom-class-b</code> will go into the same bucket
as <code>custom-class-c</code> for shutdown.</p><p>If there are no pods in a particular range, then the kubelet does not wait
for pods in that priority range. Instead, the kubelet immediately skips to the
next priority class value range.</p><p>If this feature is enabled and no configuration is provided, then no ordering
action will be taken.</p><p>Using this feature requires enabling the <code>GracefulNodeShutdownBasedOnPodPriority</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
, and setting <code>ShutdownGracePeriodByPodPriority</code> in the
<a href=/docs/reference/config-api/kubelet-config.v1beta1/>kubelet config</a>
to the desired configuration containing the pod priority class values and
their respective shutdown periods.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The ability to take Pod priority into account during graceful node shutdown was introduced
as an Alpha feature in Kubernetes v1.23. In Kubernetes 1.25
the feature is Beta and is enabled by default.</div><p>Metrics <code>graceful_shutdown_start_time_seconds</code> and <code>graceful_shutdown_end_time_seconds</code>
are emitted under the kubelet subsystem to monitor node shutdowns.</p><h2 id=swap-memory>Swap memory management</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code></div><p>Prior to Kubernetes 1.22, nodes did not support the use of swap memory, and a
kubelet would by default fail to start if swap was detected on a node. In 1.22
onwards, swap memory support can be enabled on a per-node basis.</p><p>To enable swap on a node, the <code>NodeSwap</code> feature gate must be enabled on
the kubelet, and the <code>--fail-swap-on</code> command line flag or <code>failSwapOn</code>
<a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration>configuration setting</a>
must be set to false.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> When the memory swap feature is turned on, Kubernetes data such as the content
of Secret objects that were written to tmpfs now could be swapped to disk.</div><p>A user can also optionally configure <code>memorySwap.swapBehavior</code> in order to
specify how a node will use swap memory. For example,</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>memorySwap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>swapBehavior</span>:<span style=color:#bbb> </span>LimitedSwap<span style=color:#bbb>
</span></span></span></code></pre></div><p>The available configuration options for <code>swapBehavior</code> are:</p><ul><li><code>LimitedSwap</code>: Kubernetes workloads are limited in how much swap they can
use. Workloads on the node not managed by Kubernetes can still swap.</li><li><code>UnlimitedSwap</code>: Kubernetes workloads can use as much swap memory as they
request, up to the system limit.</li></ul><p>If configuration for <code>memorySwap</code> is not specified and the feature gate is
enabled, by default the kubelet will apply the same behaviour as the
<code>LimitedSwap</code> setting.</p><p>The behaviour of the <code>LimitedSwap</code> setting depends if the node is running with
v1 or v2 of control groups (also known as "cgroups"):</p><ul><li><strong>cgroupsv1:</strong> Kubernetes workloads can use any combination of memory and
swap, up to the pod's memory limit, if set.</li><li><strong>cgroupsv2:</strong> Kubernetes workloads cannot use swap memory.</li></ul><p>For more information, and to assist with testing and provide feedback, please
see <a href=https://github.com/kubernetes/enhancements/issues/2400>KEP-2400</a> and its
<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md>design proposal</a>.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about the <a href=/docs/concepts/overview/components/#node-components>components</a> that make up a node.</li><li>Read the <a href=/docs/reference/generated/kubernetes-api/v1.25/#node-v1-core>API definition for Node</a>.</li><li>Read the <a href=https://git.k8s.io/design-proposals-archive/architecture/architecture.md#the-kubernetes-node>Node</a>
section of the architecture design document.</li><li>Read about <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>taints and tolerations</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c0251def6da29b30afebfb04549f1703>2.2 - Communication between Nodes and the Control Plane</h1><p>This document catalogs the communication paths between the API server and the Kubernetes cluster.
The intent is to allow users to customize their installation to harden the network configuration
such that the cluster can be run on an untrusted network (or on fully public IPs on a cloud
provider).</p><h2 id=node-to-control-plane>Node to Control Plane</h2><p>Kubernetes has a "hub-and-spoke" API pattern. All API usage from nodes (or the pods they run)
terminates at the API server. None of the other control plane components are designed to expose
remote services. The API server is configured to listen for remote connections on a secure HTTPS
port (typically 443) with one or more forms of client
<a href=/docs/reference/access-authn-authz/authentication/>authentication</a> enabled.
One or more forms of <a href=/docs/reference/access-authn-authz/authorization/>authorization</a> should be
enabled, especially if <a href=/docs/reference/access-authn-authz/authentication/#anonymous-requests>anonymous requests</a>
or <a href=/docs/reference/access-authn-authz/authentication/#service-account-tokens>service account tokens</a>
are allowed.</p><p>Nodes should be provisioned with the public root certificate for the cluster such that they can
connect securely to the API server along with valid client credentials. A good approach is that the
client credentials provided to the kubelet are in the form of a client certificate. See
<a href=/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/>kubelet TLS bootstrapping</a>
for automated provisioning of kubelet client certificates.</p><p>Pods that wish to connect to the API server can do so securely by leveraging a service account so
that Kubernetes will automatically inject the public root certificate and a valid bearer token
into the pod when it is instantiated.
The <code>kubernetes</code> service (in <code>default</code> namespace) is configured with a virtual IP address that is
redirected (via <code>kube-proxy</code>) to the HTTPS endpoint on the API server.</p><p>The control plane components also communicate with the API server over the secure port.</p><p>As a result, the default operating mode for connections from the nodes and pods running on the
nodes to the control plane is secured by default and can run over untrusted and/or public
networks.</p><h2 id=control-plane-to-node>Control plane to node</h2><p>There are two primary communication paths from the control plane (the API server) to the nodes.
The first is from the API server to the kubelet process which runs on each node in the cluster.
The second is from the API server to any node, pod, or service through the API server's <em>proxy</em>
functionality.</p><h3 id=api-server-to-kubelet>API server to kubelet</h3><p>The connections from the API server to the kubelet are used for:</p><ul><li>Fetching logs for pods.</li><li>Attaching (usually through <code>kubectl</code>) to running pods.</li><li>Providing the kubelet's port-forwarding functionality.</li></ul><p>These connections terminate at the kubelet's HTTPS endpoint. By default, the API server does not
verify the kubelet's serving certificate, which makes the connection subject to man-in-the-middle
attacks and <strong>unsafe</strong> to run over untrusted and/or public networks.</p><p>To verify this connection, use the <code>--kubelet-certificate-authority</code> flag to provide the API
server with a root certificate bundle to use to verify the kubelet's serving certificate.</p><p>If that is not possible, use <a href=#ssh-tunnels>SSH tunneling</a> between the API server and kubelet if
required to avoid connecting over an
untrusted or public network.</p><p>Finally, <a href=/docs/reference/access-authn-authz/kubelet-authn-authz/>Kubelet authentication and/or authorization</a>
should be enabled to secure the kubelet API.</p><h3 id=api-server-to-nodes-pods-and-services>API server to nodes, pods, and services</h3><p>The connections from the API server to a node, pod, or service default to plain HTTP connections
and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS
connection by prefixing <code>https:</code> to the node, pod, or service name in the API URL, but they will
not validate the certificate provided by the HTTPS endpoint nor provide client credentials. So
while the connection will be encrypted, it will not provide any guarantees of integrity. These
connections <strong>are not currently safe</strong> to run over untrusted or public networks.</p><h3 id=ssh-tunnels>SSH tunnels</h3><p>Kubernetes supports SSH tunnels to protect the control plane to nodes communication paths. In this
configuration, the API server initiates an SSH tunnel to each node in the cluster (connecting to
the SSH server listening on port 22) and passes all traffic destined for a kubelet, node, pod, or
service through the tunnel.
This tunnel ensures that the traffic is not exposed outside of the network in which the nodes are
running.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> SSH tunnels are currently deprecated, so you shouldn't opt to use them unless you know what you
are doing. The <a href=#konnectivity-service>Konnectivity service</a> is a replacement for this
communication channel.</div><h3 id=konnectivity-service>Konnectivity service</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code></div><p>As a replacement to the SSH tunnels, the Konnectivity service provides TCP level proxy for the
control plane to cluster communication. The Konnectivity service consists of two parts: the
Konnectivity server in the control plane network and the Konnectivity agents in the nodes network.
The Konnectivity agents initiate connections to the Konnectivity server and maintain the network
connections.
After enabling the Konnectivity service, all control plane to nodes traffic goes through these
connections.</p><p>Follow the <a href=/docs/tasks/extend-kubernetes/setup-konnectivity/>Konnectivity service task</a> to set
up the Konnectivity service in your cluster.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ca8819042a505291540e831283da66df>2.3 - Controllers</h1><p>In robotics and automation, a <em>control loop</em> is
a non-terminating loop that regulates the state of a system.</p><p>Here is one example of a control loop: a thermostat in a room.</p><p>When you set the temperature, that's telling the thermostat
about your <em>desired state</em>. The actual room temperature is the
<em>current state</em>. The thermostat acts to bring the current state
closer to the desired state, by turning equipment on or off.</p>In Kubernetes, controllers are control loops that watch the state of your
<a class=glossary-tooltip title='A set of worker machines, called nodes, that run containerized applications. Every cluster has at least one worker node.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cluster' target=_blank aria-label=cluster>cluster</a>, then make or request
changes where needed.
Each controller tries to move the current cluster state closer to the desired
state.<h2 id=controller-pattern>Controller pattern</h2><p>A controller tracks at least one Kubernetes resource type.
These <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/#kubernetes-objects>objects</a>
have a spec field that represents the desired state. The
controller(s) for that resource are responsible for making the current
state come closer to that desired state.</p><p>The controller might carry the action out itself; more commonly, in Kubernetes,
a controller will send messages to the
<a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a> that have
useful side effects. You'll see examples of this below.</p><h3 id=control-via-api-server>Control via API server</h3><p>The <a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Job>Job</a> controller is an example of a
Kubernetes built-in controller. Built-in controllers manage state by
interacting with the cluster API server.</p><p>Job is a Kubernetes resource that runs a
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, or perhaps several Pods, to carry out
a task and then stop.</p><p>(Once <a href=/docs/concepts/scheduling-eviction/>scheduled</a>, Pod objects become part of the
desired state for a kubelet).</p><p>When the Job controller sees a new task it makes sure that, somewhere
in your cluster, the kubelets on a set of Nodes are running the right
number of Pods to get the work done.
The Job controller does not run any Pods or containers
itself. Instead, the Job controller tells the API server to create or remove
Pods.
Other components in the
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>
act on the new information (there are new Pods to schedule and run),
and eventually the work is done.</p><p>After you create a new Job, the desired state is for that Job to be completed.
The Job controller makes the current state for that Job be nearer to your
desired state: creating Pods that do the work you wanted for that Job, so that
the Job is closer to completion.</p><p>Controllers also update the objects that configure them.
For example: once the work is done for a Job, the Job controller
updates that Job object to mark it <code>Finished</code>.</p><p>(This is a bit like how some thermostats turn a light off to
indicate that your room is now at the temperature you set).</p><h3 id=direct-control>Direct control</h3><p>In contrast with Job, some controllers need to make changes to
things outside of your cluster.</p><p>For example, if you use a control loop to make sure there
are enough <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a>
in your cluster, then that controller needs something outside the
current cluster to set up new Nodes when needed.</p><p>Controllers that interact with external state find their desired state from
the API server, then communicate directly with an external system to bring
the current state closer in line.</p><p>(There actually is a <a href=https://github.com/kubernetes/autoscaler/>controller</a>
that horizontally scales the nodes in your cluster.)</p><p>The important point here is that the controller makes some changes to bring about
your desired state, and then reports the current state back to your cluster's API server.
Other control loops can observe that reported data and take their own actions.</p><p>In the thermostat example, if the room is very cold then a different controller
might also turn on a frost protection heater. With Kubernetes clusters, the control
plane indirectly works with IP address management tools, storage services,
cloud provider APIs, and other services by
<a href=/docs/concepts/extend-kubernetes/>extending Kubernetes</a> to implement that.</p><h2 id=desired-vs-current>Desired versus current state</h2><p>Kubernetes takes a cloud-native view of systems, and is able to handle
constant change.</p><p>Your cluster could be changing at any point as work happens and
control loops automatically fix failures. This means that,
potentially, your cluster never reaches a stable state.</p><p>As long as the controllers for your cluster are running and able to make
useful changes, it doesn't matter if the overall state is stable or not.</p><h2 id=design>Design</h2><p>As a tenet of its design, Kubernetes uses lots of controllers that each manage
a particular aspect of cluster state. Most commonly, a particular control loop
(controller) uses one kind of resource as its desired state, and has a different
kind of resource that it manages to make that desired state happen. For example,
a controller for Jobs tracks Job objects (to discover new work) and Pod objects
(to run the Jobs, and then to see when the work is finished). In this case
something else creates the Jobs, whereas the Job controller creates Pods.</p><p>It's useful to have simple controllers rather than one, monolithic set of control
loops that are interlinked. Controllers can fail, so Kubernetes is designed to
allow for that.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>There can be several controllers that create or update the same kind of object.
Behind the scenes, Kubernetes controllers make sure that they only pay attention
to the resources linked to their controlling resource.</p><p>For example, you can have Deployments and Jobs; these both create Pods.
The Job controller does not delete the Pods that your Deployment created,
because there is information (<a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a>)
the controllers can use to tell those Pods apart.</p></div><h2 id=running-controllers>Ways of running controllers</h2><p>Kubernetes comes with a set of built-in controllers that run inside
the <a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>. These
built-in controllers provide important core behaviors.</p><p>The Deployment controller and Job controller are examples of controllers that
come as part of Kubernetes itself ("built-in" controllers).
Kubernetes lets you run a resilient control plane, so that if any of the built-in
controllers were to fail, another part of the control plane will take over the work.</p><p>You can find controllers that run outside the control plane, to extend Kubernetes.
Or, if you want, you can write a new controller yourself.
You can run your own controller as a set of Pods,
or externally to Kubernetes. What fits best will depend on what that particular
controller does.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about the <a href=/docs/concepts/overview/components/#control-plane-components>Kubernetes control plane</a></li><li>Discover some of the basic <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/>Kubernetes objects</a></li><li>Learn more about the <a href=/docs/concepts/overview/kubernetes-api/>Kubernetes API</a></li><li>If you want to write your own controller, see
<a href=/docs/concepts/extend-kubernetes/#extension-patterns>Extension Patterns</a>
in Extending Kubernetes.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-bc804b02614d67025b4c788f1ca87fbc>2.4 - Cloud Controller Manager</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code></div><p>Cloud infrastructure technologies let you run Kubernetes on public, private, and hybrid clouds.
Kubernetes believes in automated, API-driven infrastructure without tight coupling between
components.</p><p><p>The cloud-controller-manager is a Kubernetes <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> component
that embeds cloud-specific control logic. The cloud controller manager lets you link your
cluster into your cloud provider's API, and separates out the components that interact
with that cloud platform from components that only interact with your cluster.</p></p><p>By decoupling the interoperability logic between Kubernetes and the underlying cloud
infrastructure, the cloud-controller-manager component enables cloud providers to release
features at a different pace compared to the main Kubernetes project.</p><p>The cloud-controller-manager is structured using a plugin
mechanism that allows different cloud providers to integrate their platforms with Kubernetes.</p><h2 id=design>Design</h2><p><img src=/images/docs/components-of-kubernetes.svg alt="Kubernetes components"></p><p>The cloud controller manager runs in the control plane as a replicated set of processes
(usually, these are containers in Pods). Each cloud-controller-manager implements
multiple <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controllers>controllers</a> in a single
process.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You can also run the cloud controller manager as a Kubernetes
<a class=glossary-tooltip title='Resources that extend the functionality of Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/cluster-administration/addons/ target=_blank aria-label=addon>addon</a> rather than as part
of the control plane.</div><h2 id=functions-of-the-ccm>Cloud controller manager functions</h2><p>The controllers inside the cloud controller manager include:</p><h3 id=node-controller>Node controller</h3><p>The node controller is responsible for updating <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Node>Node</a> objects
when new servers are created in your cloud infrastructure. The node controller obtains information about the
hosts running inside your tenancy with the cloud provider. The node controller performs the following functions:</p><ol><li>Update a Node object with the corresponding server's unique identifier obtained from the cloud provider API.</li><li>Annotating and labelling the Node object with cloud-specific information, such as the region the node
is deployed into and the resources (CPU, memory, etc) that it has available.</li><li>Obtain the node's hostname and network addresses.</li><li>Verifying the node's health. In case a node becomes unresponsive, this controller checks with
your cloud provider's API to see if the server has been deactivated / deleted / terminated.
If the node has been deleted from the cloud, the controller deletes the Node object from your Kubernetes
cluster.</li></ol><p>Some cloud provider implementations split this into a node controller and a separate node
lifecycle controller.</p><h3 id=route-controller>Route controller</h3><p>The route controller is responsible for configuring routes in the cloud
appropriately so that containers on different nodes in your Kubernetes
cluster can communicate with each other.</p><p>Depending on the cloud provider, the route controller might also allocate blocks
of IP addresses for the Pod network.</p><h3 id=service-controller>Service controller</h3><p><a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Services>Services</a> integrate with cloud
infrastructure components such as managed load balancers, IP addresses, network
packet filtering, and target health checking. The service controller interacts with your
cloud provider's APIs to set up load balancers and other infrastructure components
when you declare a Service resource that requires them.</p><h2 id=authorization>Authorization</h2><p>This section breaks down the access that the cloud controller manager requires
on various API objects, in order to perform its operations.</p><h3 id=authorization-node-controller>Node controller</h3><p>The Node controller only works with Node objects. It requires full access
to read and modify Node objects.</p><p><code>v1/Node</code>:</p><ul><li>Get</li><li>List</li><li>Create</li><li>Update</li><li>Patch</li><li>Watch</li><li>Delete</li></ul><h3 id=authorization-route-controller>Route controller</h3><p>The route controller listens to Node object creation and configures
routes appropriately. It requires Get access to Node objects.</p><p><code>v1/Node</code>:</p><ul><li>Get</li></ul><h3 id=authorization-service-controller>Service controller</h3><p>The service controller listens to Service object Create, Update and Delete events and then configures Endpoints for those Services appropriately (for EndpointSlices, the kube-controller-manager manages these on demand).</p><p>To access Services, it requires List, and Watch access. To update Services, it requires Patch and Update access.</p><p>To set up Endpoints resources for the Services, it requires access to Create, List, Get, Watch, and Update.</p><p><code>v1/Service</code>:</p><ul><li>List</li><li>Get</li><li>Watch</li><li>Patch</li><li>Update</li></ul><h3 id=authorization-miscellaneous>Others</h3><p>The implementation of the core of the cloud controller manager requires access to create Event objects, and to ensure secure operation, it requires access to create ServiceAccounts.</p><p><code>v1/Event</code>:</p><ul><li>Create</li><li>Patch</li><li>Update</li></ul><p><code>v1/ServiceAccount</code>:</p><ul><li>Create</li></ul><p>The <a class=glossary-tooltip title='Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/rbac/ target=_blank aria-label=RBAC>RBAC</a> ClusterRole for the cloud
controller manager looks like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloud-controller-manager<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- events<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#39;*&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- nodes/status<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- services<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- patch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- serviceaccounts<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- persistentvolumes<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiGroups</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- endpoints<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- create<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- get<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- list<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- watch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- update<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=what-s-next>What's next</h2><p><a href=/docs/tasks/administer-cluster/running-cloud-controller/#cloud-controller-manager>Cloud Controller Manager Administration</a>
has instructions on running and managing the cloud controller manager.</p><p>To upgrade a HA control plane to use the cloud controller manager, see <a href=/docs/tasks/administer-cluster/controller-manager-leader-migration/>Migrate Replicated Control Plane To Use Cloud Controller Manager</a>.</p><p>Want to know how to implement your own cloud controller manager, or extend an existing project?</p><p>The cloud controller manager uses Go interfaces to allow implementations from any cloud to be plugged in. Specifically, it uses the <code>CloudProvider</code> interface defined in <a href=https://github.com/kubernetes/cloud-provider/blob/release-1.21/cloud.go#L42-L69><code>cloud.go</code></a> from <a href=https://github.com/kubernetes/cloud-provider>kubernetes/cloud-provider</a>.</p><p>The implementation of the shared controllers highlighted in this document (Node, Route, and Service), and some scaffolding along with the shared cloudprovider interface, is part of the Kubernetes core. Implementations specific to cloud providers are outside the core of Kubernetes and implement the <code>CloudProvider</code> interface.</p><p>For more information about developing plugins, see <a href=/docs/tasks/administer-cluster/developing-cloud-controller-manager/>Developing Cloud Controller Manager</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c20ec7d296cc2c8668bb204c2af31180>2.5 - About cgroup v2</h1><p>On Linux, <a class=glossary-tooltip title='A group of Linux processes with optional resource isolation, accounting and limits.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label='control groups'>control groups</a>
constrain resources that are allocated to processes.</p><p>The <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> and the
underlying container runtime need to interface with cgroups to enforce
<a href=/docs/concepts/configuration/manage-resources-containers/>resource management for pods and containers</a> which
includes cpu/memory requests and limits for containerized workloads.</p><p>There are two versions of cgroups in Linux: cgroup v1 and cgroup v2. cgroup v2 is
the new generation of the <code>cgroup</code> API.</p><h2 id=cgroup-v2>What is cgroup v2?</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>cgroup v2 is the next version of the Linux <code>cgroup</code> API. cgroup v2 provides a
unified control system with enhanced resource management
capabilities.</p><p>cgroup v2 offers several improvements over cgroup v1, such as the following:</p><ul><li>Single unified hierarchy design in API</li><li>Safer sub-tree delegation to containers</li><li>Newer features like <a href=https://www.kernel.org/doc/html/latest/accounting/psi.html>Pressure Stall Information</a></li><li>Enhanced resource allocation management and isolation across multiple resources<ul><li>Unified accounting for different types of memory allocations (network memory, kernel memory, etc)</li><li>Accounting for non-immediate resource changes such as page cache write backs</li></ul></li></ul><p>Some Kubernetes features exclusively use cgroup v2 for enhanced resource
management and isolation. For example, the
<a href=/blog/2021/11/26/qos-memory-resources/>MemoryQoS</a> feature improves memory QoS
and relies on cgroup v2 primitives.</p><h2 id=using-cgroupv2>Using cgroup v2</h2><p>The recommended way to use cgroup v2 is to use a Linux distribution that
enables and uses cgroup v2 by default.</p><p>To check if your distribution uses cgroup v2, refer to <a href=#check-cgroup-version>Identify cgroup version on Linux nodes</a>.</p><h3 id=requirements>Requirements</h3><p>cgroup v2 has the following requirements:</p><ul><li>OS distribution enables cgroup v2</li><li>Linux Kernel version is 5.8 or later</li><li>Container runtime supports cgroup v2. For example:<ul><li><a href=https://containerd.io/>containerd</a> v1.4 and later</li><li><a href=https://cri-o.io/>cri-o</a> v1.20 and later</li></ul></li><li>The kubelet and the container runtime are configured to use the <a href=/docs/setup/production-environment/container-runtimes#systemd-cgroup-driver>systemd cgroup driver</a></li></ul><h3 id=linux-distribution-cgroup-v2-support>Linux Distribution cgroup v2 support</h3><p>For a list of Linux distributions that use cgroup v2, refer to the <a href=https://github.com/opencontainers/runc/blob/main/docs/cgroup-v2.md>cgroup v2 documentation</a></p><ul><li>Container Optimized OS (since M97)</li><li>Ubuntu (since 21.10, 22.04+ recommended)</li><li>Debian GNU/Linux (since Debian 11 bullseye)</li><li>Fedora (since 31)</li><li>Arch Linux (since April 2021)</li><li>RHEL and RHEL-like distributions (since 9)</li></ul><p>To check if your distribution is using cgroup v2, refer to your distribution's
documentation or follow the instructions in <a href=#check-cgroup-version>Identify the cgroup version on Linux nodes</a>.</p><p>You can also enable cgroup v2 manually on your Linux distribution by modifying
the kernel cmdline boot arguments. If your distribution uses GRUB,
<code>systemd.unified_cgroup_hierarchy=1</code> should be added in <code>GRUB_CMDLINE_LINUX</code>
under <code>/etc/default/grub</code>, followed by <code>sudo update-grub</code>. However, the
recommended approach is to use a distribution that already enables cgroup v2 by
default.</p><h3 id=migrating-cgroupv2>Migrating to cgroup v2</h3><p>To migrate to cgroup v2, ensure that you meet the <a href=#requirements>requirements</a>, then upgrade
to a kernel version that enables cgroup v2 by default.</p><p>The kubelet automatically detects that the OS is running on cgroup v2 and
performs accordingly with no additional configuration required.</p><p>There should not be any noticeable difference in the user experience when
switching to cgroup v2, unless users are accessing the cgroup file system
directly, either on the node or from within the containers.</p><p>cgroup v2 uses a different API than cgroup v1, so if there are any
applications that directly access the cgroup file system, they need to be
updated to newer versions that support cgroup v2. For example:</p><ul><li>Some third-party monitoring and security agents may depend on the cgroup filesystem.
Update these agents to versions that support cgroup v2.</li><li>If you run <a href=https://github.com/google/cadvisor>cAdvisor</a> as a stand-alone
DaemonSet for monitoring pods and containers, update it to v0.43.0 or later.</li><li>If you use JDK, prefer to use JDK 11.0.16 and later or JDK 15 and later, which <a href=https://bugs.openjdk.org/browse/JDK-8230305>fully support cgroup v2</a>.</li></ul><h2 id=check-cgroup-version>Identify the cgroup version on Linux Nodes</h2><p>The cgroup version depends on the Linux distribution being used and the
default cgroup version configured on the OS. To check which cgroup version your
distribution uses, run the <code>stat -fc %T /sys/fs/cgroup/</code> command on
the node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>stat -fc %T /sys/fs/cgroup/
</span></span></code></pre></div><p>For cgroup v2, the output is <code>cgroup2fs</code>.</p><p>For cgroup v1, the output is <code>tmpfs.</code></p><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=https://man7.org/linux/man-pages/man7/cgroups.7.html>cgroups</a></li><li>Learn more about <a href=/docs/concepts/architecture/cri>container runtime</a></li><li>Learn more about <a href=/docs/setup/production-environment/container-runtimes#cgroup-drivers>cgroup drivers</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c0ea5310f52e22c5de34dc84d9ab5e0d>2.6 - Container Runtime Interface (CRI)</h1><p>The CRI is a plugin interface which enables the kubelet to use a wide variety of
container runtimes, without having a need to recompile the cluster components.</p><p>You need a working
<a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a> on
each Node in your cluster, so that the
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> can launch
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> and their containers.</p><p><p>The Container Runtime Interface (CRI) is the main protocol for the communication between the kubelet and Container Runtime.</p></p><p>The Kubernetes Container Runtime Interface (CRI) defines the main
<a href=https://grpc.io>gRPC</a> protocol for the communication between the
<a href=/docs/concepts/overview/components/#node-components>cluster components</a>
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> and
<a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>.</p><h2 id=api>The API</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code></div><p>The kubelet acts as a client when connecting to the container runtime via gRPC.
The runtime and image service endpoints have to be available in the container
runtime, which can be configured separately within the kubelet by using the
<code>--image-service-endpoint</code> and <code>--container-runtime-endpoint</code> <a href=/docs/reference/command-line-tools-reference/kubelet>command line
flags</a></p><p>For Kubernetes v1.25, the kubelet prefers to use CRI <code>v1</code>.
If a container runtime does not support <code>v1</code> of the CRI, then the kubelet tries to
negotiate any older supported version.
The v1.25 kubelet can also negotiate CRI <code>v1alpha2</code>, but
this version is considered as deprecated.
If the kubelet cannot negotiate a supported CRI version, the kubelet gives up
and doesn't register as a node.</p><h2 id=upgrading>Upgrading</h2><p>When upgrading Kubernetes, the kubelet tries to automatically select the
latest CRI version on restart of the component. If that fails, then the fallback
will take place as mentioned above. If a gRPC re-dial was required because the
container runtime has been upgraded, then the container runtime must also
support the initially selected version or the redial is expected to fail. This
requires a restart of the kubelet.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn more about the CRI <a href=https://github.com/kubernetes/cri-api/blob/c75ef5b/pkg/apis/runtime/v1/api.proto>protocol definition</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-44a2e2e592af0846101e970aff9243e5>2.7 - Garbage Collection</h1><p>Garbage collection is a collective term for the various mechanisms Kubernetes uses to clean up
cluster resources. This
allows the clean up of resources like the following:</p><ul><li><a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection>Terminated pods</a></li><li><a href=/docs/concepts/workloads/controllers/ttlafterfinished/>Completed Jobs</a></li><li><a href=#owners-dependents>Objects without owner references</a></li><li><a href=#containers-images>Unused containers and container images</a></li><li><a href=/docs/concepts/storage/persistent-volumes/#delete>Dynamically provisioned PersistentVolumes with a StorageClass reclaim policy of Delete</a></li><li><a href=/docs/reference/access-authn-authz/certificate-signing-requests/#request-signing-process>Stale or expired CertificateSigningRequests (CSRs)</a></li><li><a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a> deleted in the following scenarios:<ul><li>On a cloud when the cluster uses a <a href=/docs/concepts/architecture/cloud-controller/>cloud controller manager</a></li><li>On-premises when the cluster uses an addon similar to a cloud controller
manager</li></ul></li><li><a href=/docs/concepts/architecture/nodes/#heartbeats>Node Lease objects</a></li></ul><h2 id=owners-dependents>Owners and dependents</h2><p>Many objects in Kubernetes link to each other through <a href=/docs/concepts/overview/working-with-objects/owners-dependents/><em>owner references</em></a>.
Owner references tell the control plane which objects are dependent on others.
Kubernetes uses owner references to give the control plane, and other API
clients, the opportunity to clean up related resources before deleting an
object. In most cases, Kubernetes manages owner references automatically.</p><p>Ownership is different from the <a href=/docs/concepts/overview/working-with-objects/labels/>labels and selectors</a>
mechanism that some resources also use. For example, consider a
<a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> that creates
<code>EndpointSlice</code> objects. The Service uses <em>labels</em> to allow the control plane to
determine which <code>EndpointSlice</code> objects are used for that Service. In addition
to the labels, each <code>EndpointSlice</code> that is managed on behalf of a Service has
an owner reference. Owner references help different parts of Kubernetes avoid
interfering with objects they don’t control.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Cross-namespace owner references are disallowed by design.
Namespaced dependents can specify cluster-scoped or namespaced owners.
A namespaced owner <strong>must</strong> exist in the same namespace as the dependent.
If it does not, the owner reference is treated as absent, and the dependent
is subject to deletion once all owners are verified absent.</p><p>Cluster-scoped dependents can only specify cluster-scoped owners.
In v1.20+, if a cluster-scoped dependent specifies a namespaced kind as an owner,
it is treated as having an unresolvable owner reference, and is not able to be garbage collected.</p><p>In v1.20+, if the garbage collector detects an invalid cross-namespace <code>ownerReference</code>,
or a cluster-scoped dependent with an <code>ownerReference</code> referencing a namespaced kind, a warning Event
with a reason of <code>OwnerRefInvalidNamespace</code> and an <code>involvedObject</code> of the invalid dependent is reported.
You can check for that kind of Event by running
<code>kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace</code>.</p></div><h2 id=cascading-deletion>Cascading deletion</h2><p>Kubernetes checks for and deletes objects that no longer have owner
references, like the pods left behind when you delete a ReplicaSet. When you
delete an object, you can control whether Kubernetes deletes the object's
dependents automatically, in a process called <em>cascading deletion</em>. There are
two types of cascading deletion, as follows:</p><ul><li>Foreground cascading deletion</li><li>Background cascading deletion</li></ul><p>You can also control how and when garbage collection deletes resources that have
owner references using Kubernetes <a class=glossary-tooltip title='A namespaced key that tells Kubernetes to wait until specific conditions are met before it fully deletes an object marked for deletion.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/finalizers/ target=_blank aria-label=finalizers>finalizers</a>.</p><h3 id=foreground-deletion>Foreground cascading deletion</h3><p>In foreground cascading deletion, the owner object you're deleting first enters
a <em>deletion in progress</em> state. In this state, the following happens to the
owner object:</p><ul><li>The Kubernetes API server sets the object's <code>metadata.deletionTimestamp</code>
field to the time the object was marked for deletion.</li><li>The Kubernetes API server also sets the <code>metadata.finalizers</code> field to
<code>foregroundDeletion</code>.</li><li>The object remains visible through the Kubernetes API until the deletion
process is complete.</li></ul><p>After the owner object enters the deletion in progress state, the controller
deletes the dependents. After deleting all the dependent objects, the controller
deletes the owner object. At this point, the object is no longer visible in the
Kubernetes API.</p><p>During foreground cascading deletion, the only dependents that block owner
deletion are those that have the <code>ownerReference.blockOwnerDeletion=true</code> field.
See <a href=/docs/tasks/administer-cluster/use-cascading-deletion/#use-foreground-cascading-deletion>Use foreground cascading deletion</a>
to learn more.</p><h3 id=background-deletion>Background cascading deletion</h3><p>In background cascading deletion, the Kubernetes API server deletes the owner
object immediately and the controller cleans up the dependent objects in
the background. By default, Kubernetes uses background cascading deletion unless
you manually use foreground deletion or choose to orphan the dependent objects.</p><p>See <a href=/docs/tasks/administer-cluster/use-cascading-deletion/#use-background-cascading-deletion>Use background cascading deletion</a>
to learn more.</p><h3 id=orphaned-dependents>Orphaned dependents</h3><p>When Kubernetes deletes an owner object, the dependents left behind are called
<em>orphan</em> objects. By default, Kubernetes deletes dependent objects. To learn how
to override this behaviour, see <a href=/docs/tasks/administer-cluster/use-cascading-deletion/#set-orphan-deletion-policy>Delete owner objects and orphan dependents</a>.</p><h2 id=containers-images>Garbage collection of unused containers and images</h2><p>The <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> performs garbage
collection on unused images every five minutes and on unused containers every
minute. You should avoid using external garbage collection tools, as these can
break the kubelet behavior and remove containers that should exist.</p><p>To configure options for unused container and image garbage collection, tune the
kubelet using a <a href=/docs/tasks/administer-cluster/kubelet-config-file/>configuration file</a>
and change the parameters related to garbage collection using the
<a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration><code>KubeletConfiguration</code></a>
resource type.</p><h3 id=container-image-lifecycle>Container image lifecycle</h3><p>Kubernetes manages the lifecycle of all images through its <em>image manager</em>,
which is part of the kubelet, with the cooperation of
<a class=glossary-tooltip title='Tool that provides understanding of the resource usage and performance characteristics for containers' data-toggle=tooltip data-placement=top href=https://github.com/google/cadvisor/ target=_blank aria-label=cadvisor>cadvisor</a>. The kubelet
considers the following disk usage limits when making garbage collection
decisions:</p><ul><li><code>HighThresholdPercent</code></li><li><code>LowThresholdPercent</code></li></ul><p>Disk usage above the configured <code>HighThresholdPercent</code> value triggers garbage
collection, which deletes images in order based on the last time they were used,
starting with the oldest first. The kubelet deletes images
until disk usage reaches the <code>LowThresholdPercent</code> value.</p><h3 id=container-image-garbage-collection>Container garbage collection</h3><p>The kubelet garbage collects unused containers based on the following variables,
which you can define:</p><ul><li><code>MinAge</code>: the minimum age at which the kubelet can garbage collect a
container. Disable by setting to <code>0</code>.</li><li><code>MaxPerPodContainer</code>: the maximum number of dead containers each Pod pair
can have. Disable by setting to less than <code>0</code>.</li><li><code>MaxContainers</code>: the maximum number of dead containers the cluster can have.
Disable by setting to less than <code>0</code>.</li></ul><p>In addition to these variables, the kubelet garbage collects unidentified and
deleted containers, typically starting with the oldest first.</p><p><code>MaxPerPodContainer</code> and <code>MaxContainers</code> may potentially conflict with each other
in situations where retaining the maximum number of containers per Pod
(<code>MaxPerPodContainer</code>) would go outside the allowable total of global dead
containers (<code>MaxContainers</code>). In this situation, the kubelet adjusts
<code>MaxPerPodContainer</code> to address the conflict. A worst-case scenario would be to
downgrade <code>MaxPerPodContainer</code> to <code>1</code> and evict the oldest containers.
Additionally, containers owned by pods that have been deleted are removed once
they are older than <code>MinAge</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The kubelet only garbage collects the containers it manages.</div><h2 id=configuring-gc>Configuring garbage collection</h2><p>You can tune garbage collection of resources by configuring options specific to
the controllers managing those resources. The following pages show you how to
configure garbage collection:</p><ul><li><a href=/docs/tasks/administer-cluster/use-cascading-deletion/>Configuring cascading deletion of Kubernetes objects</a></li><li><a href=/docs/concepts/workloads/controllers/ttlafterfinished/>Configuring cleanup of finished Jobs</a></li></ul><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=/docs/concepts/overview/working-with-objects/owners-dependents/>ownership of Kubernetes objects</a>.</li><li>Learn more about Kubernetes <a href=/docs/concepts/overview/working-with-objects/finalizers/>finalizers</a>.</li><li>Learn about the <a href=/docs/concepts/workloads/controllers/ttlafterfinished/>TTL controller</a> (beta) that cleans up finished Jobs.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a5f7383c83ab9eb9cd0e3c4c020b3ae6>3 - Containers</h1><div class=lead>Technology for packaging an application along with its runtime dependencies.</div><p>Each container that you run is repeatable; the standardization from having
dependencies included means that you get the same behavior wherever you
run it.</p><p>Containers decouple applications from underlying host infrastructure.
This makes deployment easier in different cloud or OS environments.</p><p>Each <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> in a Kubernetes
cluster runs the containers that form the
<a href=/docs/concepts/workloads/pods/>Pods</a> assigned to that node.
Containers in a Pod are co-located and co-scheduled to run on the same node.</p><h2 id=container-images>Container images</h2><p>A <a href=/docs/concepts/containers/images/>container image</a> is a ready-to-run
software package, containing everything needed to run an application:
the code and any runtime it requires, application and system libraries,
and default values for any essential settings.</p><p>Containers are intended to be stateless and
<a href=https://glossary.cncf.io/immutable-infrastructure/>immutable</a>:
you should not change
the code of a container that is already running. If you have a containerized
application and want to make changes, the correct process is to build a new
image that includes the change, then recreate the container to start from the
updated image.</p><h2 id=container-runtimes>Container runtimes</h2><p>The container runtime is the software that is responsible for running containers.</p><p>Kubernetes supports container runtimes such as
<a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a>, <a class=glossary-tooltip title='A lightweight container runtime specifically for Kubernetes' data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a>,
and any other implementation of the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Kubernetes CRI (Container Runtime
Interface)</a>.</p><p>Usually, you can allow your cluster to pick the default container runtime
for a Pod. If you need to use more than one container runtime in your cluster,
you can specify the <a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a>
for a Pod to make sure that Kubernetes runs those containers using a
particular container runtime.</p><p>You can also use RuntimeClass to run different Pods with the same container
runtime but with different settings.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-16042b4652ad19e565c7263824029a43>3.1 - Images</h1><p>A container image represents binary data that encapsulates an application and all its
software dependencies. Container images are executable software bundles that can run
standalone and that make very well defined assumptions about their runtime environment.</p><p>You typically create a container image of your application and push it to a registry
before referring to it in a
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a></p><p>This page provides an outline of the container image concept.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you are looking for the container images for a Kubernetes
release (such as v1.25, the latest minor release),
visit <a href=https://kubernetes.io/releases/download/>Download Kubernetes</a>.</div><h2 id=image-names>Image names</h2><p>Container images are usually given a name such as <code>pause</code>, <code>example/mycontainer</code>, or <code>kube-apiserver</code>.
Images can also include a registry hostname; for example: <code>fictional.registry.example/imagename</code>,
and possibly a port number as well; for example: <code>fictional.registry.example:10443/imagename</code>.</p><p>If you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public registry.</p><p>After the image name part you can add a <em>tag</em> (in the same way you would when using with commands like <code>docker</code> or <code>podman</code>).
Tags let you identify different versions of the same series of images.</p><p>Image tags consist of lowercase and uppercase letters, digits, underscores (<code>_</code>),
periods (<code>.</code>), and dashes (<code>-</code>).<br>There are additional rules about where you can place the separator
characters (<code>_</code>, <code>-</code>, and <code>.</code>) inside an image tag.<br>If you don't specify a tag, Kubernetes assumes you mean the tag <code>latest</code>.</p><h2 id=updating-images>Updating images</h2><p>When you first create a <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>,
<a class=glossary-tooltip title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>, Pod, or other
object that includes a Pod template, then by default the pull policy of all
containers in that pod will be set to <code>IfNotPresent</code> if it is not explicitly
specified. This policy causes the
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> to skip pulling an
image if it already exists.</p><h3 id=image-pull-policy>Image pull policy</h3><p>The <code>imagePullPolicy</code> for a container and the tag of the image affect when the
<a href=/docs/reference/command-line-tools-reference/kubelet/>kubelet</a> attempts to pull (download) the specified image.</p><p>Here's a list of the values you can set for <code>imagePullPolicy</code> and the effects
these values have:</p><dl><dt><code>IfNotPresent</code></dt><dd>the image is pulled only if it is not already present locally.</dd><dt><code>Always</code></dt><dd>every time the kubelet launches a container, the kubelet queries the container
image registry to resolve the name to an image
<a href=https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier>digest</a>. If the kubelet has a
container image with that exact digest cached locally, the kubelet uses its cached
image; otherwise, the kubelet pulls the image with the resolved digest,
and uses that image to launch the container.</dd><dt><code>Never</code></dt><dd>the kubelet does not try fetching the image. If the image is somehow already present
locally, the kubelet attempts to start the container; otherwise, startup fails.
See <a href=#pre-pulled-images>pre-pulled images</a> for more details.</dd></dl><p>The caching semantics of the underlying image provider make even
<code>imagePullPolicy: Always</code> efficient, as long as the registry is reliably accessible.
Your container runtime can notice that the image layers already exist on the node
so that they don't need to be downloaded again.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>You should avoid using the <code>:latest</code> tag when deploying containers in production as
it is harder to track which version of the image is running and more difficult to
roll back properly.</p><p>Instead, specify a meaningful tag such as <code>v1.42.0</code>.</p></div><p>To make sure the Pod always uses the same version of a container image, you can specify
the image's digest;
replace <code>&lt;image-name>:&lt;tag></code> with <code>&lt;image-name>@&lt;digest></code>
(for example, <code>image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2</code>).</p><p>When using image tags, if the image registry were to change the code that the tag on that image represents, you might end up with a mix of Pods running the old and new code. An image digest uniquely identifies a specific version of the image, so Kubernetes runs the same code every time it starts a container with that image name and digest specified. Specifying an image by digest fixes the code that you run so that a change at the registry cannot lead to that mix of versions.</p><p>There are third-party <a href=/docs/reference/access-authn-authz/admission-controllers/>admission controllers</a>
that mutate Pods (and pod templates) when they are created, so that the
running workload is defined based on an image digest rather than a tag.
That might be useful if you want to make sure that all your workload is
running the same code no matter what tag changes happen at the registry.</p><h4 id=imagepullpolicy-defaulting>Default image pull policy</h4><p>When you (or a controller) submit a new Pod to the API server, your cluster sets the
<code>imagePullPolicy</code> field when specific conditions are met:</p><ul><li>if you omit the <code>imagePullPolicy</code> field, and the tag for the container image is
<code>:latest</code>, <code>imagePullPolicy</code> is automatically set to <code>Always</code>;</li><li>if you omit the <code>imagePullPolicy</code> field, and you don't specify the tag for the
container image, <code>imagePullPolicy</code> is automatically set to <code>Always</code>;</li><li>if you omit the <code>imagePullPolicy</code> field, and you specify the tag for the
container image that isn't <code>:latest</code>, the <code>imagePullPolicy</code> is automatically set to
<code>IfNotPresent</code>.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>The value of <code>imagePullPolicy</code> of the container is always set when the object is
first <em>created</em>, and is not updated if the image's tag later changes.</p><p>For example, if you create a Deployment with an image whose tag is <em>not</em>
<code>:latest</code>, and later update that Deployment's image to a <code>:latest</code> tag, the
<code>imagePullPolicy</code> field will <em>not</em> change to <code>Always</code>. You must manually change
the pull policy of any object after its initial creation.</p></div><h4 id=required-image-pull>Required image pull</h4><p>If you would like to always force a pull, you can do one of the following:</p><ul><li>Set the <code>imagePullPolicy</code> of the container to <code>Always</code>.</li><li>Omit the <code>imagePullPolicy</code> and use <code>:latest</code> as the tag for the image to use;
Kubernetes will set the policy to <code>Always</code> when you submit the Pod.</li><li>Omit the <code>imagePullPolicy</code> and the tag for the image to use;
Kubernetes will set the policy to <code>Always</code> when you submit the Pod.</li><li>Enable the <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>AlwaysPullImages</a> admission controller.</li></ul><h3 id=imagepullbackoff>ImagePullBackOff</h3><p>When a kubelet starts creating containers for a Pod using a container runtime,
it might be possible the container is in <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-state-waiting>Waiting</a>
state because of <code>ImagePullBackOff</code>.</p><p>The status <code>ImagePullBackOff</code> means that a container could not start because Kubernetes
could not pull a container image (for reasons such as invalid image name, or pulling
from a private registry without <code>imagePullSecret</code>). The <code>BackOff</code> part indicates
that Kubernetes will keep trying to pull the image, with an increasing back-off delay.</p><p>Kubernetes raises the delay between each attempt until it reaches a compiled-in limit,
which is 300 seconds (5 minutes).</p><h2 id=multi-architecture-images-with-image-indexes>Multi-architecture images with image indexes</h2><p>As well as providing binary images, a container registry can also serve a <a href=https://github.com/opencontainers/image-spec/blob/master/image-index.md>container image index</a>. An image index can point to multiple <a href=https://github.com/opencontainers/image-spec/blob/master/manifest.md>image manifests</a> for architecture-specific versions of a container. The idea is that you can have a name for an image (for example: <code>pause</code>, <code>example/mycontainer</code>, <code>kube-apiserver</code>) and allow different systems to fetch the right binary image for the machine architecture they are using.</p><p>Kubernetes itself typically names container images with a suffix <code>-$(ARCH)</code>. For backward compatibility, please generate the older images with suffixes. The idea is to generate say <code>pause</code> image which has the manifest for all the arch(es) and say <code>pause-amd64</code> which is backwards compatible for older configurations or YAML files which may have hard coded the images with suffixes.</p><h2 id=using-a-private-registry>Using a private registry</h2><p>Private registries may require keys to read images from them.<br>Credentials can be provided in several ways:</p><ul><li>Configuring Nodes to Authenticate to a Private Registry<ul><li>all pods can read any configured private registries</li><li>requires node configuration by cluster administrator</li></ul></li><li>Pre-pulled Images<ul><li>all pods can use any images cached on a node</li><li>requires root access to all nodes to set up</li></ul></li><li>Specifying ImagePullSecrets on a Pod<ul><li>only pods which provide own keys can access the private registry</li></ul></li><li>Vendor-specific or local extensions<ul><li>if you're using a custom node configuration, you (or your cloud
provider) can implement your mechanism for authenticating the node
to the container registry.</li></ul></li></ul><p>These options are explained in more detail below.</p><h3 id=configuring-nodes-to-authenticate-to-a-private-registry>Configuring nodes to authenticate to a private registry</h3><p>Specific instructions for setting credentials depends on the container runtime and registry you chose to use. You should refer to your solution's documentation for the most accurate information.</p><p>For an example of configuring a private container image registry, see the
<a href=/docs/tasks/configure-pod-container/pull-image-private-registry>Pull an Image from a Private Registry</a>
task. That example uses a private registry in Docker Hub.</p><h3 id=config-json>Interpretation of config.json</h3><p>The interpretation of <code>config.json</code> varies between the original Docker
implementation and the Kubernetes interpretation. In Docker, the <code>auths</code> keys
can only specify root URLs, whereas Kubernetes allows glob URLs as well as
prefix-matched paths. This means that a <code>config.json</code> like this is valid:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;auths&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;*my-registry.io/images&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:green;font-weight:700>&#34;auth&#34;</span>: <span style=color:#b44>&#34;…&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The root URL (<code>*my-registry.io</code>) is matched by using the following syntax:</p><pre tabindex=0><code>pattern:
    { term }

term:
    &#39;*&#39;         matches any sequence of non-Separator characters
    &#39;?&#39;         matches any single non-Separator character
    &#39;[&#39; [ &#39;^&#39; ] { character-range } &#39;]&#39;
                character class (must be non-empty)
    c           matches character c (c != &#39;*&#39;, &#39;?&#39;, &#39;\\&#39;, &#39;[&#39;)
    &#39;\\&#39; c      matches character c

character-range:
    c           matches character c (c != &#39;\\&#39;, &#39;-&#39;, &#39;]&#39;)
    &#39;\\&#39; c      matches character c
    lo &#39;-&#39; hi   matches character c for lo &lt;= c &lt;= hi
</code></pre><p>Image pull operations would now pass the credentials to the CRI container
runtime for every valid pattern. For example the following container image names
would match successfully:</p><ul><li><code>my-registry.io/images</code></li><li><code>my-registry.io/images/my-image</code></li><li><code>my-registry.io/images/another-image</code></li><li><code>sub.my-registry.io/images/my-image</code></li><li><code>a.sub.my-registry.io/images/my-image</code></li></ul><p>The kubelet performs image pulls sequentially for every found credential. This
means, that multiple entries in <code>config.json</code> are possible, too:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;auths&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;my-registry.io/images&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:green;font-weight:700>&#34;auth&#34;</span>: <span style=color:#b44>&#34;…&#34;</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;my-registry.io/images/subpath&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:green;font-weight:700>&#34;auth&#34;</span>: <span style=color:#b44>&#34;…&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If now a container specifies an image <code>my-registry.io/images/subpath/my-image</code>
to be pulled, then the kubelet will try to download them from both
authentication sources if one of them fails.</p><h3 id=pre-pulled-images>Pre-pulled images</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This approach is suitable if you can control node configuration. It
will not work reliably if your cloud provider manages nodes and replaces
them automatically.</div><p>By default, the kubelet tries to pull each image from the specified registry.
However, if the <code>imagePullPolicy</code> property of the container is set to <code>IfNotPresent</code> or <code>Never</code>,
then a local image is used (preferentially or exclusively, respectively).</p><p>If you want to rely on pre-pulled images as a substitute for registry authentication,
you must ensure all nodes in the cluster have the same pre-pulled images.</p><p>This can be used to preload certain images for speed or as an alternative to authenticating to a private registry.</p><p>All pods will have read access to any pre-pulled images.</p><h3 id=specifying-imagepullsecrets-on-a-pod>Specifying imagePullSecrets on a Pod</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This is the recommended approach to run containers based on images
in private registries.</div><p>Kubernetes supports specifying container image registry keys on a Pod.
<code>imagePullSecrets</code> must all be in the same namespace as the Pod. The referenced
Secrets must be of type <code>kubernetes.io/dockercfg</code> or <code>kubernetes.io/dockerconfigjson</code>.</p><h4 id=creating-a-secret-with-a-docker-config>Creating a Secret with a Docker config</h4><p>You need to know the username, registry password and client email address for authenticating
to the registry, as well as its hostname.
Run the following command, substituting the appropriate uppercase values:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret docker-registry &lt;name&gt; --docker-server<span style=color:#666>=</span>DOCKER_REGISTRY_SERVER --docker-username<span style=color:#666>=</span>DOCKER_USER --docker-password<span style=color:#666>=</span>DOCKER_PASSWORD --docker-email<span style=color:#666>=</span>DOCKER_EMAIL
</span></span></code></pre></div><p>If you already have a Docker credentials file then, rather than using the above
command, you can import the credentials file as a Kubernetes
<a class=glossary-tooltip title='Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secrets>Secrets</a>.<br><a href=/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials>Create a Secret based on existing Docker credentials</a> explains how to set this up.</p><p>This is particularly useful if you are using multiple private container
registries, as <code>kubectl create secret docker-registry</code> creates a Secret that
only works with a single private registry.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Pods can only reference image pull secrets in their own namespace,
so this process needs to be done one time per namespace.</div><h4 id=referring-to-an-imagepullsecrets-on-a-pod>Referring to an imagePullSecrets on a Pod</h4><p>Now, you can create pods which reference that secret by adding an <code>imagePullSecrets</code>
section to a Pod definition. Each item in the <code>imagePullSecrets</code> array can only
reference a Secret in the same namespace.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: Pod
</span></span></span><span style=display:flex><span><span style=color:#b44>metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>  name: foo
</span></span></span><span style=display:flex><span><span style=color:#b44>  namespace: awesomeapps
</span></span></span><span style=display:flex><span><span style=color:#b44>spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>  containers:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: foo
</span></span></span><span style=display:flex><span><span style=color:#b44>      image: janedoe/awesomeapp:v1
</span></span></span><span style=display:flex><span><span style=color:#b44>  imagePullSecrets:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: myregistrykey
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt;&gt; ./kustomization.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>resources:
</span></span></span><span style=display:flex><span><span style=color:#b44>- pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><p>This needs to be done for each pod that is using a private registry.</p><p>However, setting of this field can be automated by setting the imagePullSecrets
in a <a href=/docs/tasks/configure-pod-container/configure-service-account/>ServiceAccount</a> resource.</p><p>Check <a href=/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account>Add ImagePullSecrets to a Service Account</a> for detailed instructions.</p><p>You can use this in conjunction with a per-node <code>.docker/config.json</code>. The credentials
will be merged.</p><h2 id=use-cases>Use cases</h2><p>There are a number of solutions for configuring private registries. Here are some
common use cases and suggested solutions.</p><ol><li>Cluster running only non-proprietary (e.g. open-source) images. No need to hide images.<ul><li>Use public images from a public registry<ul><li>No configuration required.</li><li>Some cloud providers automatically cache or mirror public images, which improves availability and reduces the time to pull images.</li></ul></li></ul></li><li>Cluster running some proprietary images which should be hidden to those outside the company, but
visible to all cluster users.<ul><li>Use a hosted private registry<ul><li>Manual configuration may be required on the nodes that need to access to private registry</li></ul></li><li>Or, run an internal private registry behind your firewall with open read access.<ul><li>No Kubernetes configuration is required.</li></ul></li><li>Use a hosted container image registry service that controls image access<ul><li>It will work better with cluster autoscaling than manual node configuration.</li></ul></li><li>Or, on a cluster where changing the node configuration is inconvenient, use <code>imagePullSecrets</code>.</li></ul></li><li>Cluster with proprietary images, a few of which require stricter access control.<ul><li>Ensure <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>AlwaysPullImages admission controller</a> is active. Otherwise, all Pods potentially have access to all images.</li><li>Move sensitive data into a "Secret" resource, instead of packaging it in an image.</li></ul></li><li>A multi-tenant cluster where each tenant needs own private registry.<ul><li>Ensure <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>AlwaysPullImages admission controller</a> is active. Otherwise, all Pods of all tenants potentially have access to all images.</li><li>Run a private registry with authorization required.</li><li>Generate registry credential for each tenant, put into secret, and populate secret to each tenant namespace.</li><li>The tenant adds that secret to imagePullSecrets of each namespace.</li></ul></li></ol><p>If you need access to multiple registries, you can create one secret for each registry.</p><h2 id=what-s-next>What's next</h2><ul><li>Read the <a href=https://github.com/opencontainers/image-spec/blob/master/manifest.md>OCI Image Manifest Specification</a>.</li><li>Learn about <a href=/docs/concepts/architecture/garbage-collection/#container-image-garbage-collection>container image garbage collection</a>.</li><li>Learn more about <a href=/docs/tasks/configure-pod-container/pull-image-private-registry>pulling an Image from a Private Registry</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-643212488f778acf04bebed65ba34441>3.2 - Container Environment</h1><p>This page describes the resources available to Containers in the Container environment.</p><h2 id=container-environment>Container environment</h2><p>The Kubernetes Container environment provides several important resources to Containers:</p><ul><li>A filesystem, which is a combination of an <a href=/docs/concepts/containers/images/>image</a> and one or more <a href=/docs/concepts/storage/volumes/>volumes</a>.</li><li>Information about the Container itself.</li><li>Information about other objects in the cluster.</li></ul><h3 id=container-information>Container information</h3><p>The <em>hostname</em> of a Container is the name of the Pod in which the Container is running.
It is available through the <code>hostname</code> command or the
<a href=https://man7.org/linux/man-pages/man2/gethostname.2.html><code>gethostname</code></a>
function call in libc.</p><p>The Pod name and namespace are available as environment variables through the
<a href=/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/>downward API</a>.</p><p>User defined environment variables from the Pod definition are also available to the Container,
as are any environment variables specified statically in the container image.</p><h3 id=cluster-information>Cluster information</h3><p>A list of all services that were running when a Container was created is available to that Container as environment variables.
This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.</p><p>For a service named <em>foo</em> that maps to a Container named <em>bar</em>,
the following variables are defined:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_HOST</span><span style=color:#666>=</span>&lt;the host the service is running on&gt;
</span></span><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_PORT</span><span style=color:#666>=</span>&lt;the port the service is running on&gt;
</span></span></code></pre></div><p>Services have dedicated IP addresses and are available to the Container via DNS,
if <a href=https://releases.k8s.io/v1.25.0/cluster/addons/dns/>DNS addon</a> is enabled. </p><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=/docs/concepts/containers/container-lifecycle-hooks/>Container lifecycle hooks</a>.</li><li>Get hands-on experience
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>attaching handlers to Container lifecycle events</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a858027489648786a3b16264e451272b>3.3 - Runtime Class</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>This page describes the RuntimeClass resource and runtime selection mechanism.</p><p>RuntimeClass is a feature for selecting the container runtime configuration. The container runtime
configuration is used to run a Pod's containers.</p><h2 id=motivation>Motivation</h2><p>You can set a different RuntimeClass between different Pods to provide a balance of
performance versus security. For example, if part of your workload deserves a high
level of information security assurance, you might choose to schedule those Pods so
that they run in a container runtime that uses hardware virtualization. You'd then
benefit from the extra isolation of the alternative runtime, at the expense of some
additional overhead.</p><p>You can also use RuntimeClass to run different Pods with the same container runtime
but with different settings.</p><h2 id=setup>Setup</h2><ol><li>Configure the CRI implementation on nodes (runtime dependent)</li><li>Create the corresponding RuntimeClass resources</li></ol><h3 id=1-configure-the-cri-implementation-on-nodes>1. Configure the CRI implementation on nodes</h3><p>The configurations available through RuntimeClass are Container Runtime Interface (CRI)
implementation dependent. See the corresponding documentation (<a href=#cri-configuration>below</a>) for your
CRI implementation for how to configure.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> RuntimeClass assumes a homogeneous node configuration across the cluster by default (which means
that all nodes are configured the same way with respect to container runtimes). To support
heterogeneous node configurations, see <a href=#scheduling>Scheduling</a> below.</div><p>The configurations have a corresponding <code>handler</code> name, referenced by the RuntimeClass. The
handler must be a valid <a href=/docs/concepts/overview/working-with-objects/names/#dns-label-names>DNS label name</a>.</p><h3 id=2-create-the-corresponding-runtimeclass-resources>2. Create the corresponding RuntimeClass resources</h3><p>The configurations setup in step 1 should each have an associated <code>handler</code> name, which identifies
the configuration. For each handler, create a corresponding RuntimeClass object.</p><p>The RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name
(<code>metadata.name</code>) and the handler (<code>handler</code>). The object definition looks like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#080;font-style:italic># RuntimeClass is defined in the node.k8s.io API group</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># The name the RuntimeClass will be referenced by.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># RuntimeClass is a non-namespaced resource.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myclass <span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># The name of the corresponding CRI configuration</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>myconfiguration <span style=color:#bbb>
</span></span></span></code></pre></div><p>The name of a RuntimeClass object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> It is recommended that RuntimeClass write operations (create/update/patch/delete) be
restricted to the cluster administrator. This is typically the default. See
<a href=/docs/reference/access-authn-authz/authorization/>Authorization Overview</a> for more details.</div><h2 id=usage>Usage</h2><p>Once RuntimeClasses are configured for the cluster, you can specify a
<code>runtimeClassName</code> in the Pod spec to use it. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>myclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># ...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>This will instruct the kubelet to use the named RuntimeClass to run this pod. If the named
RuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will enter the
<code>Failed</code> terminal <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase>phase</a>. Look for a
corresponding <a href=/docs/tasks/debug/debug-application/debug-running-pod/>event</a> for an
error message.</p><p>If no <code>runtimeClassName</code> is specified, the default RuntimeHandler will be used, which is equivalent
to the behavior when the RuntimeClass feature is disabled.</p><h3 id=cri-configuration>CRI Configuration</h3><p>For more details on setting up CRI runtimes, see <a href=/docs/setup/production-environment/container-runtimes/>CRI installation</a>.</p><h4 id=hahahugoshortcode-s3-hbhb><a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></h4><p>Runtime handlers are configured through containerd's configuration at
<code>/etc/containerd/config.toml</code>. Valid handlers are configured under the runtimes section:</p><pre tabindex=0><code>[plugins.&#34;io.containerd.grpc.v1.cri&#34;.containerd.runtimes.${HANDLER_NAME}]
</code></pre><p>See containerd's <a href=https://github.com/containerd/containerd/blob/main/docs/cri/config.md>config documentation</a>
for more details:</p><h4 id=hahahugoshortcode-s4-hbhb><a class=glossary-tooltip title='A lightweight container runtime specifically for Kubernetes' data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></h4><p>Runtime handlers are configured through CRI-O's configuration at <code>/etc/crio/crio.conf</code>. Valid
handlers are configured under the
<a href=https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table>crio.runtime table</a>:</p><pre tabindex=0><code>[crio.runtime.runtimes.${HANDLER_NAME}]
  runtime_path = &#34;${PATH_TO_BINARY}&#34;
</code></pre><p>See CRI-O's <a href=https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md>config documentation</a> for more details.</p><h2 id=scheduling>Scheduling</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code></div><p>By specifying the <code>scheduling</code> field for a RuntimeClass, you can set constraints to
ensure that Pods running with this RuntimeClass are scheduled to nodes that support it.
If <code>scheduling</code> is not set, this RuntimeClass is assumed to be supported by all nodes.</p><p>To ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have a
common label which is then selected by the <code>runtimeclass.scheduling.nodeSelector</code> field. The
RuntimeClass's nodeSelector is merged with the pod's nodeSelector in admission, effectively taking
the intersection of the set of nodes selected by each. If there is a conflict, the pod will be
rejected.</p><p>If the supported nodes are tainted to prevent other RuntimeClass pods from running on the node, you
can add <code>tolerations</code> to the RuntimeClass. As with the <code>nodeSelector</code>, the tolerations are merged
with the pod's tolerations in admission, effectively taking the union of the set of nodes tolerated
by each.</p><p>To learn more about configuring the node selector and tolerations, see
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/>Assigning Pods to Nodes</a>.</p><h3 id=pod-overhead>Pod Overhead</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>You can specify <em>overhead</em> resources that are associated with running a Pod. Declaring overhead allows
the cluster (including the scheduler) to account for it when making decisions about Pods and resources.</p><p>Pod overhead is defined in RuntimeClass through the <code>overhead</code> field. Through the use of this field,
you can specify the overhead of running pods utilizing this RuntimeClass and ensure these overheads
are accounted for in Kubernetes.</p><h2 id=what-s-next>What's next</h2><ul><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md>RuntimeClass Design</a></li><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling>RuntimeClass Scheduling Design</a></li><li>Read about the <a href=/docs/concepts/scheduling-eviction/pod-overhead/>Pod Overhead</a> concept</li><li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>PodOverhead Feature Design</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e6941d969d81540208a3e78bc56f43bc>3.4 - Container Lifecycle Hooks</h1><p>This page describes how kubelet managed Containers can use the Container lifecycle hook framework
to run code triggered by events during their management lifecycle.</p><h2 id=overview>Overview</h2><p>Analogous to many programming language frameworks that have component lifecycle hooks, such as Angular,
Kubernetes provides Containers with lifecycle hooks.
The hooks enable Containers to be aware of events in their management lifecycle
and run code implemented in a handler when the corresponding lifecycle hook is executed.</p><h2 id=container-hooks>Container hooks</h2><p>There are two hooks that are exposed to Containers:</p><p><code>PostStart</code></p><p>This hook is executed immediately after a container is created.
However, there is no guarantee that the hook will execute before the container ENTRYPOINT.
No parameters are passed to the handler.</p><p><code>PreStop</code></p><p>This hook is called immediately before a container is terminated due to an API request or management
event such as a liveness/startup probe failure, preemption, resource contention and others. A call
to the <code>PreStop</code> hook fails if the container is already in a terminated or completed state and the
hook must complete before the TERM signal to stop the container can be sent. The Pod's termination
grace period countdown begins before the <code>PreStop</code> hook is executed, so regardless of the outcome of
the handler, the container will eventually terminate within the Pod's termination grace period. No
parameters are passed to the handler.</p><p>A more detailed description of the termination behavior can be found in
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>Termination of Pods</a>.</p><h3 id=hook-handler-implementations>Hook handler implementations</h3><p>Containers can access a hook by implementing and registering a handler for that hook.
There are two types of hook handlers that can be implemented for Containers:</p><ul><li>Exec - Executes a specific command, such as <code>pre-stop.sh</code>, inside the cgroups and namespaces of the Container.
Resources consumed by the command are counted against the Container.</li><li>HTTP - Executes an HTTP request against a specific endpoint on the Container.</li></ul><h3 id=hook-handler-execution>Hook handler execution</h3><p>When a Container lifecycle management hook is called,
the Kubernetes management system executes the handler according to the hook action,
<code>httpGet</code> and <code>tcpSocket</code> are executed by the kubelet process, and <code>exec</code> is executed in the container.</p><p>Hook handler calls are synchronous within the context of the Pod containing the Container.
This means that for a <code>PostStart</code> hook,
the Container ENTRYPOINT and hook fire asynchronously.
However, if the hook takes too long to run or hangs,
the Container cannot reach a <code>running</code> state.</p><p><code>PreStop</code> hooks are not executed asynchronously from the signal to stop the Container; the hook must
complete its execution before the TERM signal can be sent. If a <code>PreStop</code> hook hangs during
execution, the Pod's phase will be <code>Terminating</code> and remain there until the Pod is killed after its
<code>terminationGracePeriodSeconds</code> expires. This grace period applies to the total time it takes for
both the <code>PreStop</code> hook to execute and for the Container to stop normally. If, for example,
<code>terminationGracePeriodSeconds</code> is 60, and the hook takes 55 seconds to complete, and the Container
takes 10 seconds to stop normally after receiving the signal, then the Container will be killed
before it can stop normally, since <code>terminationGracePeriodSeconds</code> is less than the total time
(55+10) it takes for these two things to happen.</p><p>If either a <code>PostStart</code> or <code>PreStop</code> hook fails,
it kills the Container.</p><p>Users should make their hook handlers as lightweight as possible.
There are cases, however, when long running commands make sense,
such as when saving state prior to stopping a Container.</p><h3 id=hook-delivery-guarantees>Hook delivery guarantees</h3><p>Hook delivery is intended to be <em>at least once</em>,
which means that a hook may be called multiple times for any given event,
such as for <code>PostStart</code> or <code>PreStop</code>.
It is up to the hook implementation to handle this correctly.</p><p>Generally, only single deliveries are made.
If, for example, an HTTP hook receiver is down and is unable to take traffic,
there is no attempt to resend.
In some rare cases, however, double delivery may occur.
For instance, if a kubelet restarts in the middle of sending a hook,
the hook might be resent after the kubelet comes back up.</p><h3 id=debugging-hook-handlers>Debugging Hook handlers</h3><p>The logs for a Hook handler are not exposed in Pod events.
If a handler fails for some reason, it broadcasts an event.
For <code>PostStart</code>, this is the <code>FailedPostStartHook</code> event,
and for <code>PreStop</code>, this is the <code>FailedPreStopHook</code> event.
To generate a failed <code>FailedPostStartHook</code> event yourself, modify the <a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/lifecycle-events.yaml>lifecycle-events.yaml</a> file to change the postStart command to "badcommand" and apply it.
Here is some example output of the resulting events you see from running <code>kubectl describe pod lifecycle-demo</code>:</p><pre tabindex=0><code>Events:
  Type     Reason               Age              From               Message
  ----     ------               ----             ----               -------
  Normal   Scheduled            7s               default-scheduler  Successfully assigned default/lifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...
  Normal   Pulled               6s               kubelet            Successfully pulled image &#34;nginx&#34; in 229.604315ms
  Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image &#34;nginx&#34;
  Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-container
  Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-container
  Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook ([badcommand]) for Container &#34;lifecycle-demo-container&#34; in Pod &#34;lifecycle-demo_default(30229739-9651-4e5a-9a32-a8f1688862db)&#34; failed - error: command &#39;badcommand&#39; exited with 126: , message: &#34;OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \&#34;badcommand\&#34;: executable file not found in $PATH: unknown\r\n&#34;
  Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook
  Normal   Pulled               4s               kubelet            Successfully pulled image &#34;nginx&#34; in 215.66395ms
  Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container
</code></pre><h2 id=what-s-next>What's next</h2><ul><li>Learn more about the <a href=/docs/concepts/containers/container-environment/>Container environment</a>.</li><li>Get hands-on experience
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>attaching handlers to Container lifecycle events</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-05a1231ecbfe48ec554e6d078818aca4>4 - Windows in Kubernetes</h1></div><div class=td-content><h1 id=pg-849246a35c3de66980f66e1b0944ceb4>4.1 - Windows containers in Kubernetes</h1><p>Windows applications constitute a large portion of the services and applications that
run in many organizations. <a href=https://aka.ms/windowscontainers>Windows containers</a>
provide a way to encapsulate processes and package dependencies, making it easier
to use DevOps practices and follow cloud native patterns for Windows applications.</p><p>Organizations with investments in Windows-based applications and Linux-based
applications don't have to look for separate orchestrators to manage their workloads,
leading to increased operational efficiencies across their deployments, regardless
of operating system.</p><h2 id=windows-nodes-in-kubernetes>Windows nodes in Kubernetes</h2><p>To enable the orchestration of Windows containers in Kubernetes, include Windows nodes
in your existing Linux cluster. Scheduling Windows containers in
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> on Kubernetes is similar to
scheduling Linux-based containers.</p><p>In order to run Windows containers, your Kubernetes cluster must include
multiple operating systems.
While you can only run the <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> on Linux,
you can deploy worker nodes running either Windows or Linux.</p><p>Windows <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodes>nodes</a> are
<a href=#windows-os-version-support>supported</a> provided that the operating system is
Windows Server 2019.</p><p>This document uses the term <em>Windows containers</em> to mean Windows containers with
process isolation. Kubernetes does not support running Windows containers with
<a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/manage-containers/hyperv-container>Hyper-V isolation</a>.</p><h2 id=limitations>Compatibility and limitations</h2><p>Some node features are only available if you use a specific
<a href=#container-runtime>container runtime</a>; others are not available on Windows nodes,
including:</p><ul><li>HugePages: not supported for Windows containers</li><li>Privileged containers: not supported for Windows containers.
<a href=/docs/tasks/configure-pod-container/create-hostprocess-pod/>HostProcess Containers</a> offer similar functionality.</li><li>TerminationGracePeriod: requires containerD</li></ul><p>Not all features of shared namespaces are supported. See <a href=#api>API compatibility</a>
for more details.</p><p>See <a href=#windows-os-version-support>Windows OS version compatibility</a> for details on
the Windows versions that Kubernetes is tested against.</p><p>From an API and kubectl perspective, Windows containers behave in much the same
way as Linux-based containers. However, there are some notable differences in key
functionality which are outlined in this section.</p><h3 id=compatibility-linux-similarities>Comparison with Linux</h3><p>Key Kubernetes elements work the same way in Windows as they do in Linux. This
section refers to several key workload abstractions and how they map to Windows.</p><ul><li><p><a href=/docs/concepts/workloads/pods/>Pods</a></p><p>A Pod is the basic building block of Kubernetes–the smallest and simplest unit in
the Kubernetes object model that you create or deploy. You may not deploy Windows and
Linux containers in the same Pod. All containers in a Pod are scheduled onto a single
Node where each Node represents a specific platform and architecture. The following
Pod capabilities, properties and events are supported with Windows containers:</p><ul><li><p>Single or multiple containers per Pod with process isolation and volume sharing</p></li><li><p>Pod <code>status</code> fields</p></li><li><p>Readiness, liveness, and startup probes</p></li><li><p>postStart & preStop container lifecycle hooks</p></li><li><p>ConfigMap, Secrets: as environment variables or volumes</p></li><li><p><code>emptyDir</code> volumes</p></li><li><p>Named pipe host mounts</p></li><li><p>Resource limits</p></li><li><p>OS field:</p><p>The <code>.spec.os.name</code> field should be set to <code>windows</code> to indicate that the current Pod uses Windows containers.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Starting from 1.25, the <code>IdentifyPodOS</code> feature gate is in GA stage and defaults to be enabled.</div><p>If you set the <code>.spec.os.name</code> field to <code>windows</code>,
you must not set the following fields in the <code>.spec</code> of that Pod:</p><ul><li><code>spec.hostPID</code></li><li><code>spec.hostIPC</code></li><li><code>spec.securityContext.seLinuxOptions</code></li><li><code>spec.securityContext.seccompProfile</code></li><li><code>spec.securityContext.fsGroup</code></li><li><code>spec.securityContext.fsGroupChangePolicy</code></li><li><code>spec.securityContext.sysctls</code></li><li><code>spec.shareProcessNamespace</code></li><li><code>spec.securityContext.runAsUser</code></li><li><code>spec.securityContext.runAsGroup</code></li><li><code>spec.securityContext.supplementalGroups</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions</code></li><li><code>spec.containers[*].securityContext.seccompProfile</code></li><li><code>spec.containers[*].securityContext.capabilities</code></li><li><code>spec.containers[*].securityContext.readOnlyRootFilesystem</code></li><li><code>spec.containers[*].securityContext.privileged</code></li><li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.containers[*].securityContext.procMount</code></li><li><code>spec.containers[*].securityContext.runAsUser</code></li><li><code>spec.containers[*].securityContext.runAsGroup</code></li></ul><p>In the above list, wildcards (<code>*</code>) indicate all elements in a list.
For example, <code>spec.containers[*].securityContext</code> refers to the SecurityContext object
for all containers. If any of these fields is specified, the Pod will
not be admitted by the API server.</p></li></ul></li><li><p><a href=/docs/concepts/workloads/controllers/>Workload resources</a> including:</p><ul><li>ReplicaSet</li><li>Deployment</li><li>StatefulSet</li><li>DaemonSet</li><li>Job</li><li>CronJob</li><li>ReplicationController</li></ul></li><li><p><a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Services>Services</a>
See <a href=/docs/concepts/services-networking/windows-networking/#load-balancing-and-services>Load balancing and Services</a> for more details.</p></li></ul><p>Pods, workload resources, and Services are critical elements to managing Windows
workloads on Kubernetes. However, on their own they are not enough to enable
the proper lifecycle management of Windows workloads in a dynamic cloud native
environment.</p><ul><li><code>kubectl exec</code></li><li>Pod and container metrics</li><li><a class=glossary-tooltip title='An API resource that automatically scales the number of pod replicas based on targeted CPU utilization or custom metric targets.' data-toggle=tooltip data-placement=top href=/docs/tasks/run-application/horizontal-pod-autoscale/ target=_blank aria-label='Horizontal pod autoscaling'>Horizontal pod autoscaling</a></li><li><a class=glossary-tooltip title='Provides constraints that limit aggregate resource consumption per namespace.' data-toggle=tooltip data-placement=top href=/docs/concepts/policy/resource-quotas/ target=_blank aria-label='Resource quotas'>Resource quotas</a></li><li>Scheduler preemption</li></ul><h3 id=kubelet-compatibility>Command line options for the kubelet</h3><p>Some kubelet command line options behave differently on Windows, as described below:</p><ul><li>The <code>--windows-priorityclass</code> lets you set the scheduling priority of the kubelet process
(see <a href=/docs/concepts/configuration/windows-resource-management/#resource-management-cpu>CPU resource management</a>)</li><li>The <code>--kube-reserved</code>, <code>--system-reserved</code> , and <code>--eviction-hard</code> flags update
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>NodeAllocatable</a></li><li>Eviction by using <code>--enforce-node-allocable</code> is not implemented</li><li>Eviction by using <code>--eviction-hard</code> and <code>--eviction-soft</code> are not implemented</li><li>When running on a Windows node the kubelet does not have memory or CPU
restrictions. <code>--kube-reserved</code> and <code>--system-reserved</code> only subtract from <code>NodeAllocatable</code>
and do not guarantee resource provided for workloads.
See <a href=/docs/concepts/configuration/windows-resource-management/#resource-reservation>Resource Management for Windows nodes</a>
for more information.</li><li>The <code>MemoryPressure</code> Condition is not implemented</li><li>The kubelet does not take OOM eviction actions</li></ul><h3 id=api>API compatibility</h3><p>There are subtle differences in the way the Kubernetes APIs work for Windows due to the OS
and container runtime. Some workload properties were designed for Linux, and fail to run on Windows.</p><p>At a high level, these OS concepts are different:</p><ul><li>Identity - Linux uses userID (UID) and groupID (GID) which
are represented as integer types. User and group names
are not canonical - they are just an alias in <code>/etc/groups</code>
or <code>/etc/passwd</code> back to UID+GID. Windows uses a larger binary
<a href=https://docs.microsoft.com/en-us/windows/security/identity-protection/access-control/security-identifiers>security identifier</a> (SID)
which is stored in the Windows Security Access Manager (SAM) database. This
database is not shared between the host and containers, or between containers.</li><li>File permissions - Windows uses an access control list based on (SIDs), whereas
POSIX systems such as Linux use a bitmask based on object permissions and UID+GID,
plus <em>optional</em> access control lists.</li><li>File paths - the convention on Windows is to use <code>\</code> instead of <code>/</code>. The Go IO
libraries typically accept both and just make it work, but when you're setting a
path or command line that's interpreted inside a container, <code>\</code> may be needed.</li><li>Signals - Windows interactive apps handle termination differently, and can
implement one or more of these:<ul><li>A UI thread handles well-defined messages including <code>WM_CLOSE</code>.</li><li>Console apps handle Ctrl-C or Ctrl-break using a Control Handler.</li><li>Services register a Service Control Handler function that can accept
<code>SERVICE_CONTROL_STOP</code> control codes.</li></ul></li></ul><p>Container exit codes follow the same convention where 0 is success, and nonzero is failure.
The specific error codes may differ across Windows and Linux. However, exit codes
passed from the Kubernetes components (kubelet, kube-proxy) are unchanged.</p><h4 id=compatibility-v1-pod-spec-containers>Field compatibility for container specifications</h4><p>The following list documents differences between how Pod container specifications
work between Windows and Linux:</p><ul><li>Huge pages are not implemented in the Windows container
runtime, and are not available. They require <a href=https://docs.microsoft.com/en-us/windows/desktop/Memory/large-page-support>asserting a user
privilege</a>
that's not configurable for containers.</li><li><code>requests.cpu</code> and <code>requests.memory</code> - requests are subtracted
from node available resources, so they can be used to avoid overprovisioning a
node. However, they cannot be used to guarantee resources in an overprovisioned
node. They should be applied to all containers as a best practice if the operator
wants to avoid overprovisioning entirely.</li><li><code>securityContext.allowPrivilegeEscalation</code> -
not possible on Windows; none of the capabilities are hooked up</li><li><code>securityContext.capabilities</code> -
POSIX capabilities are not implemented on Windows</li><li><code>securityContext.privileged</code> -
Windows doesn't support privileged containers, use <a href=/docs/tasks/configure-pod-container/create-hostprocess-pod/>HostProcess Containers</a> instead</li><li><code>securityContext.procMount</code> -
Windows doesn't have a <code>/proc</code> filesystem</li><li><code>securityContext.readOnlyRootFilesystem</code> -
not possible on Windows; write access is required for registry & system
processes to run inside the container</li><li><code>securityContext.runAsGroup</code> -
not possible on Windows as there is no GID support</li><li><code>securityContext.runAsNonRoot</code> -
this setting will prevent containers from running as <code>ContainerAdministrator</code>
which is the closest equivalent to a root user on Windows.</li><li><code>securityContext.runAsUser</code> -
use <a href=/docs/tasks/configure-pod-container/configure-runasusername><code>runAsUserName</code></a>
instead</li><li><code>securityContext.seLinuxOptions</code> -
not possible on Windows as SELinux is Linux-specific</li><li><code>terminationMessagePath</code> -
this has some limitations in that Windows doesn't support mapping single files. The
default value is <code>/dev/termination-log</code>, which does work because it does not
exist on Windows by default.</li></ul><h4 id=compatibility-v1-pod>Field compatibility for Pod specifications</h4><p>The following list documents differences between how Pod specifications work between Windows and Linux:</p><ul><li><code>hostIPC</code> and <code>hostpid</code> - host namespace sharing is not possible on Windows</li><li><code>hostNetwork</code> - There is no Windows OS support to share the host network</li><li><code>dnsPolicy</code> - setting the Pod <code>dnsPolicy</code> to <code>ClusterFirstWithHostNet</code> is
not supported on Windows because host networking is not provided. Pods always
run with a container network.</li><li><code>podSecurityContext</code> (see below)</li><li><code>shareProcessNamespace</code> - this is a beta feature, and depends on Linux namespaces
which are not implemented on Windows. Windows cannot share process namespaces or
the container's root filesystem. Only the network can be shared.</li><li><code>terminationGracePeriodSeconds</code> - this is not fully implemented in Docker on Windows,
see the <a href=https://github.com/moby/moby/issues/25982>GitHub issue</a>.
The behavior today is that the ENTRYPOINT process is sent CTRL_SHUTDOWN_EVENT,
then Windows waits 5 seconds by default, and finally shuts down
all processes using the normal Windows shutdown behavior. The 5
second default is actually in the Windows registry
<a href=https://github.com/moby/moby/issues/25982#issuecomment-426441183>inside the container</a>,
so it can be overridden when the container is built.</li><li><code>volumeDevices</code> - this is a beta feature, and is not implemented on Windows.
Windows cannot attach raw block devices to pods.</li><li><code>volumes</code><ul><li>If you define an <code>emptyDir</code> volume, you cannot set its volume source to <code>memory</code>.</li></ul></li><li>You cannot enable <code>mountPropagation</code> for volume mounts as this is not
supported on Windows.</li></ul><h4 id=compatibility-v1-pod-spec-containers-securitycontext>Field compatibility for Pod security context</h4><p>None of the Pod <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context><code>securityContext</code></a> fields work on Windows.</p><h2 id=node-problem-detector>Node problem detector</h2><p>The node problem detector (see
<a href=/docs/tasks/debug/debug-cluster/monitor-node-health/>Monitor Node Health</a>)
has preliminary support for Windows.
For more information, visit the project's <a href=https://github.com/kubernetes/node-problem-detector#windows>GitHub page</a>.</p><h2 id=pause-container>Pause container</h2><p>In a Kubernetes Pod, an infrastructure or “pause” container is first created
to host the container. In Linux, the cgroups and namespaces that make up a pod
need a process to maintain their continued existence; the pause process provides
this. Containers that belong to the same pod, including infrastructure and worker
containers, share a common network endpoint (same IPv4 and / or IPv6 address, same
network port spaces). Kubernetes uses pause containers to allow for worker containers
crashing or restarting without losing any of the networking configuration.</p><p>Kubernetes maintains a multi-architecture image that includes support for Windows.
For Kubernetes v1.25 the recommended pause image is <code>registry.k8s.io/pause:3.6</code>.
The <a href=https://github.com/kubernetes/kubernetes/tree/master/build/pause>source code</a>
is available on GitHub.</p><p>Microsoft maintains a different multi-architecture image, with Linux and Windows
amd64 support, that you can find as <code>mcr.microsoft.com/oss/kubernetes/pause:3.6</code>.
This image is built from the same source as the Kubernetes maintained image but
all of the Windows binaries are <a href=https://docs.microsoft.com/en-us/windows-hardware/drivers/install/authenticode>authenticode signed</a> by Microsoft.
The Kubernetes project recommends using the Microsoft maintained image if you are
deploying to a production or production-like environment that requires signed
binaries.</p><h2 id=container-runtime>Container runtimes</h2><p>You need to install a
<a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>
into each node in the cluster so that Pods can run there.</p><p>The following container runtimes work with Windows:</p><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><h3 id=containerd>ContainerD</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>You can use <a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=ContainerD>ContainerD</a> 1.4.0+
as the container runtime for Kubernetes nodes that run Windows.</p><p>Learn how to <a href=/docs/setup/production-environment/container-runtimes/#install-containerd>install ContainerD on a Windows node</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> There is a <a href=/docs/tasks/configure-pod-container/configure-gmsa/#gmsa-limitations>known limitation</a>
when using GMSA with containerd to access Windows network shares, which requires a
kernel patch.</div><h3 id=mcr>Mirantis Container Runtime</h3><p><a href=https://docs.mirantis.com/mcr/20.10/overview.html>Mirantis Container Runtime</a> (MCR)
is available as a container runtime for all Windows Server 2019 and later versions.</p><p>See <a href=https://docs.mirantis.com/mcr/20.10/install/mcr-windows.html>Install MCR on Windows Servers</a> for more information.</p><h2 id=windows-os-version-support>Windows OS version compatibility</h2><p>On Windows nodes, strict compatibility rules apply where the host OS version must
match the container base image OS version. Only Windows containers with a container
operating system of Windows Server 2019 are fully supported.</p><p>For Kubernetes v1.25, operating system compatibility for Windows nodes (and Pods)
is as follows:</p><dl><dt>Windows Server LTSC release</dt><dd>Windows Server 2019</dd><dd>Windows Server 2022</dd><dt>Windows Server SAC release</dt><dd>Windows Server version 20H2</dd></dl><p>The Kubernetes <a href=/docs/setup/release/version-skew-policy/>version-skew policy</a> also applies.</p><h2 id=troubleshooting>Getting help and troubleshooting</h2><p>Your main source of help for troubleshooting your Kubernetes cluster should start
with the <a href=/docs/tasks/debug/>Troubleshooting</a>
page.</p><p>Some additional, Windows-specific troubleshooting help is included
in this section. Logs are an important element of troubleshooting
issues in Kubernetes. Make sure to include them any time you seek
troubleshooting assistance from other contributors. Follow the
instructions in the
SIG Windows <a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#gathering-logs>contributing guide on gathering logs</a>.</p><h3 id=reporting-issues-and-feature-requests>Reporting issues and feature requests</h3><p>If you have what looks like a bug, or you would like to
make a feature request, please follow the <a href=https://github.com/kubernetes/community/blob/master/sig-windows/CONTRIBUTING.md#reporting-issues-and-feature-requests>SIG Windows contributing guide</a> to create a new issue.
You should first search the list of issues in case it was
reported previously and comment with your experience on the issue and add additional
logs. SIG Windows channel on the Kubernetes Slack is also a great avenue to get some initial support and
troubleshooting ideas prior to creating a ticket.</p><h2 id=deployment-tools>Deployment tools</h2><p>The kubeadm tool helps you to deploy a Kubernetes cluster, providing the control
plane to manage the cluster it, and nodes to run your workloads.
<a href=/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/>Adding Windows nodes</a>
explains how to deploy Windows nodes to your cluster using kubeadm.</p><p>The Kubernetes <a href=https://cluster-api.sigs.k8s.io/>cluster API</a> project also provides means to automate deployment of Windows nodes.</p><h2 id=windows-distribution-channels>Windows distribution channels</h2><p>For a detailed explanation of Windows distribution channels see the
<a href=https://docs.microsoft.com/en-us/windows-server/get-started-19/servicing-channels-19>Microsoft documentation</a>.</p><p>Information on the different Windows Server servicing channels
including their support models can be found at
<a href=https://docs.microsoft.com/en-us/windows-server/get-started/servicing-channels-comparison>Windows Server servicing channels</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-0d8bfd3be43b3580681c56f6fec9d6dc>4.2 - Guide for scheduling Windows containers in Kubernetes</h1><p>Windows applications constitute a large portion of the services and applications that run in many organizations.
This guide walks you through the steps to configure and deploy Windows containers in Kubernetes.</p><h2 id=objectives>Objectives</h2><ul><li>Configure an example deployment to run Windows containers on the Windows node</li><li>Highlight Windows specific functionality in Kubernetes</li></ul><h2 id=before-you-begin>Before you begin</h2><ul><li>Create a Kubernetes cluster that includes a
control plane and a <a href=/docs/tasks/administer-cluster/kubeadm/adding-windows-nodes/>worker node running Windows Server</a></li><li>It is important to note that creating and deploying services and workloads on Kubernetes
behaves in much the same way for Linux and Windows containers.
<a href=/docs/reference/kubectl/>Kubectl commands</a> to interface with the cluster are identical.
The example in the section below is provided to jumpstart your experience with Windows containers.</li></ul><h2 id=getting-started-deploying-a-windows-container>Getting Started: Deploying a Windows container</h2><p>The example YAML file below deploys a simple webserver application running inside a Windows container.</p><p>Create a service spec named <code>win-webserver.yaml</code> with the contents below:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># the port that this service should serve on</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>NodePort<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>win-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windowswebserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore:ltsc2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- powershell.exe<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- -command<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:#b44>&#34;&lt;#code used from https://gist.github.com/19WAS85/5424431#&gt; ; $$listener = New-Object System.Net.HttpListener ; $$listener.Prefixes.Add(&#39;http://*:80/&#39;) ; $$listener.Start() ; $$callerCounts = @{} ; Write-Host(&#39;Listening at http://*:80/&#39;) ; while ($$listener.IsListening) { ;$$context = $$listener.GetContext() ;$$requestUrl = $$context.Request.Url ;$$clientIP = $$context.Request.RemoteEndPoint.Address ;$$response = $$context.Response ;Write-Host &#39;&#39; ;Write-Host(&#39;&gt; {0}&#39; -f $$requestUrl) ;  ;$$count = 1 ;$$k=$$callerCounts.Get_Item($$clientIP) ;if ($$k -ne $$null) { $$count += $$k } ;$$callerCounts.Set_Item($$clientIP, $$count) ;$$ip=(Get-NetAdapter | Get-NetIpAddress); $$header=&#39;&lt;html&gt;&lt;body&gt;&lt;H1&gt;Windows Container Web Server&lt;/H1&gt;&#39; ;$$callerCountsString=&#39;&#39; ;$$callerCounts.Keys | % { $$callerCountsString+=&#39;&lt;p&gt;IP {0} callerCount {1} &#39; -f $$ip[1].IPAddress,$$callerCounts.Item($$_) } ;$$footer=&#39;&lt;/body&gt;&lt;/html&gt;&#39; ;$$content=&#39;{0}{1}{2}&#39; -f $$header,$$callerCountsString,$$footer ;Write-Output $$content ;$$buffer = [System.Text.Encoding]::UTF8.GetBytes($$content) ;$$response.ContentLength64 = $$buffer.Length ;$$response.OutputStream.Write($$buffer, 0, $$buffer.Length) ;$$response.Close() ;$$responseStatus = $$response.StatusCode ;Write-Host(&#39;&lt; {0}&#39; -f $$responseStatus)  } ; &#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Port mapping is also supported, but for simplicity this example exposes
port 80 of the container directly to the Service.</div><ol><li><p>Check that all nodes are healthy:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get nodes
</span></span></code></pre></div></li><li><p>Deploy the service and watch for pod updates:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl apply -f win-webserver.yaml
</span></span><span style=display:flex><span>kubectl get pods -o wide -w
</span></span></code></pre></div><p>When the service is deployed correctly both Pods are marked as Ready. To exit the watch command, press Ctrl+C.</p></li><li><p>Check that the deployment succeeded. To verify:</p><ul><li>Two pods listed from the Linux control plane node, use <code>kubectl get pods</code></li><li>Node-to-pod communication across the network, <code>curl</code> port 80 of your pod IPs from the Linux control plane node
to check for a web server response</li><li>Pod-to-pod communication, ping between pods (and across hosts, if you have more than one Windows node)
using <code>docker exec</code> or <code>kubectl exec</code></li><li>Service-to-pod communication, <code>curl</code> the virtual service IP (seen under <code>kubectl get services</code>)
from the Linux control plane node and from individual pods</li><li>Service discovery, <code>curl</code> the service name with the Kubernetes <a href=/docs/concepts/services-networking/dns-pod-service/#services>default DNS suffix</a></li><li>Inbound connectivity, <code>curl</code> the NodePort from the Linux control plane node or machines outside of the cluster</li><li>Outbound connectivity, <code>curl</code> external IPs from inside the pod using <code>kubectl exec</code></li></ul></li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Windows container hosts are not able to access the IP of services scheduled on them due to current platform limitations of the Windows networking stack.
Only Windows pods are able to access service IPs.</div><h2 id=observability>Observability</h2><h3 id=capturing-logs-from-workloads>Capturing logs from workloads</h3><p>Logs are an important element of observability; they enable users to gain insights
into the operational aspect of workloads and are a key ingredient to troubleshooting issues.
Because Windows containers and workloads inside Windows containers behave differently from Linux containers,
users had a hard time collecting logs, limiting operational visibility.
Windows workloads for example are usually configured to log to ETW (Event Tracing for Windows)
or push entries to the application event log.
<a href=https://github.com/microsoft/windows-container-tools/tree/master/LogMonitor>LogMonitor</a>, an open source tool by Microsoft,
is the recommended way to monitor configured log sources inside a Windows container.
LogMonitor supports monitoring event logs, ETW providers, and custom application logs,
piping them to STDOUT for consumption by <code>kubectl logs &lt;pod></code>.</p><p>Follow the instructions in the LogMonitor GitHub page to copy its binaries and configuration files
to all your containers and add the necessary entrypoints for LogMonitor to push your logs to STDOUT.</p><h2 id=configuring-container-user>Configuring container user</h2><h3 id=using-configurable-container-usernames>Using configurable Container usernames</h3><p>Windows containers can be configured to run their entrypoints and processes
with different usernames than the image defaults.
Learn more about it <a href=/docs/tasks/configure-pod-container/configure-runasusername/>here</a>.</p><h3 id=managing-workload-identity-with-group-managed-service-accounts>Managing Workload Identity with Group Managed Service Accounts</h3><p>Windows container workloads can be configured to use Group Managed Service Accounts (GMSA).
Group Managed Service Accounts are a specific type of Active Directory account that provide automatic password management,
simplified service principal name (SPN) management, and the ability to delegate the management to other administrators across multiple servers.
Containers configured with a GMSA can access external Active Directory Domain resources while carrying the identity configured with the GMSA.
Learn more about configuring and using GMSA for Windows containers <a href=/docs/tasks/configure-pod-container/configure-gmsa/>here</a>.</p><h2 id=taints-and-tolerations>Taints and Tolerations</h2><p>Users need to use some combination of taints and node selectors in order to
schedule Linux and Windows workloads to their respective OS-specific nodes.
The recommended approach is outlined below,
with one of its main goals being that this approach should not break compatibility for existing Linux workloads.</p><p>Starting from 1.25, you can (and should) set <code>.spec.os.name</code> for each Pod, to indicate the operating system
that the containers in that Pod are designed for. For Pods that run Linux containers, set
<code>.spec.os.name</code> to <code>linux</code>. For Pods that run Windows containers, set <code>.spec.os.name</code>
to <code>windows</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Starting from 1.25, the <code>IdentifyPodOS</code> feature is in GA stage and defaults to be enabled.</div><p>The scheduler does not use the value of <code>.spec.os.name</code> when assigning Pods to nodes. You should
use normal Kubernetes mechanisms for
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/>assigning pods to nodes</a>
to ensure that the control plane for your cluster places pods onto nodes that are running the
appropriate operating system.</p><p>The <code>.spec.os.name</code> value has no effect on the scheduling of the Windows pods,
so taints and tolerations and node selectors are still required
to ensure that the Windows pods land onto appropriate Windows nodes.</p><h3 id=ensuring-os-specific-workloads-land-on-the-appropriate-container-host>Ensuring OS-specific workloads land on the appropriate container host</h3><p>Users can ensure Windows containers can be scheduled on the appropriate host using Taints and Tolerations.
All Kubernetes nodes today have the following default labels:</p><ul><li>kubernetes.io/os = [windows|linux]</li><li>kubernetes.io/arch = [amd64|arm64|...]</li></ul><p>If a Pod specification does not specify a nodeSelector like <code>"kubernetes.io/os": windows</code>,
it is possible the Pod can be scheduled on any host, Windows or Linux.
This can be problematic since a Windows container can only run on Windows and a Linux container can only run on Linux.
The best practice is to use a nodeSelector.</p><p>However, we understand that in many cases users have a pre-existing large number of deployments for Linux containers,
as well as an ecosystem of off-the-shelf configurations, such as community Helm charts, and programmatic Pod generation cases, such as with Operators.
In those situations, you may be hesitant to make the configuration change to add nodeSelectors.
The alternative is to use Taints. Because the kubelet can set Taints during registration,
it could easily be modified to automatically add a taint when running on Windows only.</p><p>For example: <code>--register-with-taints='os=windows:NoSchedule'</code></p><p>By adding a taint to all Windows nodes, nothing will be scheduled on them (that includes existing Linux Pods).
In order for a Windows Pod to be scheduled on a Windows node,
it would need both the nodeSelector and the appropriate matching toleration to choose Windows.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span>windows<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;os&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=handling-multiple-windows-versions-in-the-same-cluster>Handling multiple Windows versions in the same cluster</h3><p>The Windows Server version used by each pod must match that of the node. If you want to use multiple Windows
Server versions in the same cluster, then you should set additional node labels and nodeSelectors.</p><p>Kubernetes 1.17 automatically adds a new label <code>node.kubernetes.io/windows-build</code> to simplify this.
If you're running an older version, then it's recommended to add this label manually to Windows nodes.</p><p>This label reflects the Windows major, minor, and build number that need to match for compatibility.
Here are values used today for each Windows Server version.</p><table><thead><tr><th>Product Name</th><th>Build Number(s)</th></tr></thead><tbody><tr><td>Windows Server 2019</td><td>10.0.17763</td></tr><tr><td>Windows Server, Version 20H2</td><td>10.0.19042</td></tr><tr><td>Windows Server 2022</td><td>10.0.20348</td></tr></tbody></table><h3 id=simplifying-with-runtimeclass>Simplifying with RuntimeClass</h3><p><a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a> can be used to simplify the process of using taints and tolerations.
A cluster administrator can create a <code>RuntimeClass</code> object which is used to encapsulate these taints and tolerations.</p><ol><li>Save this file to <code>runtimeClasses.yml</code>. It includes the appropriate <code>nodeSelector</code>
for the Windows OS, architecture, and version.</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;docker&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>scheduling</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/os</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;windows&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/arch</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;amd64&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>node.kubernetes.io/windows-build</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;10.0.17763&#39;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>os<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Equal<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;windows&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><ol><li>Run <code>kubectl create -f runtimeClasses.yml</code> using as a cluster administrator</li><li>Add <code>runtimeClassName: windows-2019</code> as appropriate to Pod specs</li></ol><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>windows-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mcr.microsoft.com/windows/servercore/iis:windowsservercore-ltsc2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>800Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>.1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>300Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>iis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>iis-2019<span style=color:#bbb>
</span></span></span></code></pre></div></div><div class=td-content style=page-break-before:always><h1 id=pg-d52aadda80edd9f8c514cfe2321363c2>5 - Workloads</h1><div class=lead>Understand Pods, the smallest deployable compute object in Kubernetes, and the higher-level abstractions that help you to run them.</div><p>A workload is an application running on Kubernetes.
Whether your workload is a single component or several that work together, on Kubernetes you run
it inside a set of <a href=/docs/concepts/workloads/pods><em>pods</em></a>.
In Kubernetes, a <code>Pod</code> represents a set of running
<a class=glossary-tooltip title='A lightweight and portable executable image that contains software and all of its dependencies.' data-toggle=tooltip data-placement=top href=/docs/concepts/containers/ target=_blank aria-label=containers>containers</a> on your cluster.</p><p>Kubernetes pods have a <a href=/docs/concepts/workloads/pods/pod-lifecycle/>defined lifecycle</a>.
For example, once a pod is running in your cluster then a critical fault on the
<a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> where that pod is running means that
all the pods on that node fail. Kubernetes treats that level of failure as final: you
would need to create a new <code>Pod</code> to recover, even if the node later becomes healthy.</p><p>However, to make life considerably easier, you don't need to manage each <code>Pod</code> directly.
Instead, you can use <em>workload resources</em> that manage a set of pods on your behalf.
These resources configure <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controllers>controllers</a>
that make sure the right number of the right kind of pod are running, to match the state
you specified.</p><p>Kubernetes provides several built-in workload resources:</p><ul><li><a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> and <a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a>
(replacing the legacy resource
<a class=glossary-tooltip title='A (deprecated) API object that manages a replicated application.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-replication-controller' target=_blank aria-label=ReplicationController>ReplicationController</a>).
<code>Deployment</code> is a good fit for managing a stateless application workload on your cluster,
where any <code>Pod</code> in the <code>Deployment</code> is interchangeable and can be replaced if needed.</li><li><a href=/docs/concepts/workloads/controllers/statefulset/><code>StatefulSet</code></a> lets you
run one or more related Pods that do track state somehow. For example, if your workload
records data persistently, you can run a <code>StatefulSet</code> that matches each <code>Pod</code> with a
<a href=/docs/concepts/storage/persistent-volumes/><code>PersistentVolume</code></a>. Your code, running in the
<code>Pods</code> for that <code>StatefulSet</code>, can replicate data to other <code>Pods</code> in the same <code>StatefulSet</code>
to improve overall resilience.</li><li><a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> defines <code>Pods</code> that provide
node-local facilities. These might be fundamental to the operation of your cluster, such
as a networking helper tool, or be part of an
<a class=glossary-tooltip title='Resources that extend the functionality of Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/cluster-administration/addons/ target=_blank aria-label=add-on>add-on</a>.<br>Every time you add a node to your cluster that matches the specification in a <code>DaemonSet</code>,
the control plane schedules a <code>Pod</code> for that <code>DaemonSet</code> onto the new node.</li><li><a href=/docs/concepts/workloads/controllers/job/><code>Job</code></a> and
<a href=/docs/concepts/workloads/controllers/cron-jobs/><code>CronJob</code></a>
define tasks that run to completion and then stop. Jobs represent one-off tasks, whereas
<code>CronJobs</code> recur according to a schedule.</li></ul><p>In the wider Kubernetes ecosystem, you can find third-party workload resources that provide
additional behaviors. Using a
<a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>custom resource definition</a>,
you can add in a third-party workload resource if you want a specific behavior that's not part
of Kubernetes' core. For example, if you wanted to run a group of <code>Pods</code> for your application but
stop work unless <em>all</em> the Pods are available (perhaps for some high-throughput distributed task),
then you can implement or install an extension that does provide that feature.</p><h2 id=what-s-next>What's next</h2><p>As well as reading about each resource, you can learn about specific tasks that relate to them:</p><ul><li><a href=/docs/tasks/run-application/run-stateless-application-deployment/>Run a stateless application using a <code>Deployment</code></a></li><li>Run a stateful application either as a <a href=/docs/tasks/run-application/run-single-instance-stateful-application/>single instance</a>
or as a <a href=/docs/tasks/run-application/run-replicated-stateful-application/>replicated set</a></li><li><a href=/docs/tasks/job/automated-tasks-with-cron-jobs/>Run automated tasks with a <code>CronJob</code></a></li></ul><p>To learn about Kubernetes' mechanisms for separating code from configuration,
visit <a href=/docs/concepts/configuration/>Configuration</a>.</p><p>There are two supporting concepts that provide backgrounds about how Kubernetes manages pods
for applications:</p><ul><li><a href=/docs/concepts/architecture/garbage-collection/>Garbage collection</a> tidies up objects
from your cluster after their <em>owning resource</em> has been removed.</li><li>The <a href=/docs/concepts/workloads/controllers/ttlafterfinished/><em>time-to-live after finished</em> controller</a>
removes Jobs once a defined time has passed since they completed.</li></ul><p>Once your application is running, you might want to make it available on the internet as
a <a href=/docs/concepts/services-networking/service/><code>Service</code></a> or, for web application only,
using an <a href=/docs/concepts/services-networking/ingress><code>Ingress</code></a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4d68b0ccf9c683e6368ffdcc40c838d4>5.1 - Pods</h1><p><em>Pods</em> are the smallest deployable units of computing that you can create and manage in Kubernetes.</p><p>A <em>Pod</em> (as in a pod of whales or pea pod) is a group of one or more
<a class=glossary-tooltip title='A lightweight and portable executable image that contains software and all of its dependencies.' data-toggle=tooltip data-placement=top href=/docs/concepts/containers/ target=_blank aria-label=containers>containers</a>, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and
co-scheduled, and run in a shared context. A Pod models an
application-specific "logical host": it contains one or more application
containers which are relatively tightly coupled.
In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.</p><p>As well as application containers, a Pod can contain
<a href=/docs/concepts/workloads/pods/init-containers/>init containers</a> that run
during Pod startup. You can also inject
<a href=/docs/concepts/workloads/pods/ephemeral-containers/>ephemeral containers</a>
for debugging if your cluster offers this.</p><h2 id=what-is-a-pod>What is a Pod?</h2><div class="alert alert-info note callout" role=alert><strong>Note:</strong> While Kubernetes supports more
<a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtimes'>container runtimes</a>
than just Docker, <a href=https://www.docker.com/>Docker</a> is the most commonly known
runtime, and it helps to describe Pods using some terminology from Docker.</div><p>The shared context of a Pod is a set of Linux namespaces, cgroups, and
potentially other facets of isolation - the same things that isolate a <a class=glossary-tooltip title='A lightweight and portable executable image that contains software and all of its dependencies.' data-toggle=tooltip data-placement=top href=/docs/concepts/containers/ target=_blank aria-label=container>container</a>. Within a Pod's context, the individual applications may have
further sub-isolations applied.</p><p>A Pod is similar to a set of containers with shared namespaces and shared filesystem volumes.</p><h2 id=using-pods>Using Pods</h2><p>The following is an example of a Pod which consists of a container running the image <code>nginx:1.14.2</code>.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/simple-pod.yaml download=pods/simple-pod.yaml><code>pods/simple-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-simple-pod-yaml")' title="Copy pods/simple-pod.yaml to clipboard"></img></div><div class=includecode id=pods-simple-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>To create the Pod shown above, run the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/pods/simple-pod.yaml
</span></span></code></pre></div><p>Pods are generally not created directly and are created using workload resources.
See <a href=#working-with-pods>Working with Pods</a> for more information on how Pods are used
with workload resources.</p><h3 id=workload-resources-for-managing-pods>Workload resources for managing pods</h3><p>Usually you don't need to create Pods directly, even singleton Pods. Instead, create them using workload resources such as <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a> or <a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Job>Job</a>.
If your Pods need to track state, consider the
<a class=glossary-tooltip title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a> resource.</p><p>Pods in a Kubernetes cluster are used in two main ways:</p><ul><li><p><strong>Pods that run a single container</strong>. The "one-container-per-Pod" model is the
most common Kubernetes use case; in this case, you can think of a Pod as a
wrapper around a single container; Kubernetes manages Pods rather than managing
the containers directly.</p></li><li><p><strong>Pods that run multiple containers that need to work together</strong>. A Pod can
encapsulate an application composed of multiple co-located containers that are
tightly coupled and need to share resources. These co-located containers
form a single cohesive unit of service—for example, one container serving data
stored in a shared volume to the public, while a separate <em>sidecar</em> container
refreshes or updates those files.
The Pod wraps these containers, storage resources, and an ephemeral network
identity together as a single unit.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Grouping multiple co-located and co-managed containers in a single Pod is a
relatively advanced use case. You should use this pattern only in specific
instances in which your containers are tightly coupled.</div></li></ul><p>Each Pod is meant to run a single instance of a given application. If you want to
scale your application horizontally (to provide more overall resources by running
more instances), you should use multiple Pods, one for each instance. In
Kubernetes, this is typically referred to as <em>replication</em>.
Replicated Pods are usually created and managed as a group by a workload resource
and its <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>.</p><p>See <a href=#pods-and-controllers>Pods and controllers</a> for more information on how
Kubernetes uses workload resources, and their controllers, to implement application
scaling and auto-healing.</p><h3 id=how-pods-manage-multiple-containers>How Pods manage multiple containers</h3><p>Pods are designed to support multiple cooperating processes (as containers) that form
a cohesive unit of service. The containers in a Pod are automatically co-located and
co-scheduled on the same physical or virtual machine in the cluster. The containers
can share resources and dependencies, communicate with one another, and coordinate
when and how they are terminated.</p><p>For example, you might have a container that
acts as a web server for files in a shared volume, and a separate "sidecar" container
that updates those files from a remote source, as in the following diagram:</p><figure class=diagram-medium><img src=/images/docs/pod.svg alt="Pod creation diagram"></figure><p>Some Pods have <a class=glossary-tooltip title='One or more initialization containers that must run to completion before any app containers run.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-init-container' target=_blank aria-label='init containers'>init containers</a> as well as <a class=glossary-tooltip title='A container used to run part of a workload. Compare with init container.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-app-container' target=_blank aria-label='app containers'>app containers</a>. Init containers run and complete before the app containers are started.</p><p>Pods natively provide two kinds of shared resources for their constituent containers:
<a href=#pod-networking>networking</a> and <a href=#pod-storage>storage</a>.</p><h2 id=working-with-pods>Working with Pods</h2><p>You'll rarely create individual Pods directly in Kubernetes—even singleton Pods. This
is because Pods are designed as relatively ephemeral, disposable entities. When
a Pod gets created (directly by you, or indirectly by a
<a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>), the new Pod is
scheduled to run on a <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Node>Node</a> in your cluster.
The Pod remains on that node until the Pod finishes execution, the Pod object is deleted,
the Pod is <em>evicted</em> for lack of resources, or the node fails.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Restarting a container in a Pod should not be confused with restarting a Pod. A Pod
is not a process, but an environment for running container(s). A Pod persists until
it is deleted.</div><p>When you create the manifest for a Pod object, make sure the name specified is a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><h3 id=pod-os>Pod OS</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>You should set the <code>.spec.os.name</code> field to either <code>windows</code> or <code>linux</code> to indicate the OS on
which you want the pod to run. These two are the only operating systems supported for now by
Kubernetes. In future, this list may be expanded.</p><p>In Kubernetes v1.25, the value you set for this field has no
effect on <a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=scheduling>scheduling</a> of the pods.
Setting the <code>.spec.os.name</code> helps to identify the pod OS
authoratitively and is used for validation. The kubelet refuses to run a Pod where you have
specified a Pod OS, if this isn't the same as the operating system for the node where
that kubelet is running.
The <a href=/docs/concepts/security/pod-security-standards/>Pod security standards</a> also use this
field to avoid enforcing policies that aren't relevant to that operating system.</p><h3 id=pods-and-controllers>Pods and controllers</h3><p>You can use workload resources to create and manage multiple Pods for you. A controller
for the resource handles replication and rollout and automatic healing in case of
Pod failure. For example, if a Node fails, a controller notices that Pods on that
Node have stopped working and creates a replacement Pod. The scheduler places the
replacement Pod onto a healthy Node.</p><p>Here are some examples of workload resources that manage one or more Pods:</p><ul><li><a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a></li><li><a class=glossary-tooltip title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a></li><li><a class=glossary-tooltip title='Ensures a copy of a Pod is running across a set of nodes in a cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a></li></ul><h3 id=pod-templates>Pod templates</h3><p>Controllers for <a class=glossary-tooltip title='A workload is an application running on Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/ target=_blank aria-label=workload>workload</a> resources create Pods
from a <em>pod template</em> and manage those Pods on your behalf.</p><p>PodTemplates are specifications for creating Pods, and are included in workload resources such as
<a href=/docs/concepts/workloads/controllers/deployment/>Deployments</a>,
<a href=/docs/concepts/workloads/controllers/job/>Jobs</a>, and
<a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSets</a>.</p><p>Each controller for a workload resource uses the <code>PodTemplate</code> inside the workload
object to make actual Pods. The <code>PodTemplate</code> is part of the desired state of whatever
workload resource you used to run your app.</p><p>The sample below is a manifest for a simple Job with a <code>template</code> that starts one
container. The container in that Pod prints a message then pauses.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># This is the pod template</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;echo &#34;Hello, Kubernetes!&#34; &amp;&amp; sleep 3600&#39;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>OnFailure<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># The pod template ends here</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Modifying the pod template or switching to a new pod template has no direct effect
on the Pods that already exist. If you change the pod template for a workload
resource, that resource needs to create replacement Pods that use the updated template.</p><p>For example, the StatefulSet controller ensures that the running Pods match the current
pod template for each StatefulSet object. If you edit the StatefulSet to change its pod
template, the StatefulSet starts to create new Pods based on the updated template.
Eventually, all of the old Pods are replaced with new Pods, and the update is complete.</p><p>Each workload resource implements its own rules for handling changes to the Pod template.
If you want to read more about StatefulSet specifically, read
<a href=/docs/tutorials/stateful-application/basic-stateful-set/#updating-statefulsets>Update strategy</a> in the StatefulSet Basics tutorial.</p><p>On Nodes, the <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> does not
directly observe or manage any of the details around pod templates and updates; those
details are abstracted away. That abstraction and separation of concerns simplifies
system semantics, and makes it feasible to extend the cluster's behavior without
changing existing code.</p><h2 id=pod-update-and-replacement>Pod update and replacement</h2><p>As mentioned in the previous section, when the Pod template for a workload
resource is changed, the controller creates new Pods based on the updated
template instead of updating or patching the existing Pods.</p><p>Kubernetes doesn't prevent you from managing Pods directly. It is possible to
update some fields of a running Pod, in place. However, Pod update operations
like
<a href=/docs/reference/generated/kubernetes-api/v1.25/#patch-pod-v1-core><code>patch</code></a>, and
<a href=/docs/reference/generated/kubernetes-api/v1.25/#replace-pod-v1-core><code>replace</code></a>
have some limitations:</p><ul><li><p>Most of the metadata about a Pod is immutable. For example, you cannot
change the <code>namespace</code>, <code>name</code>, <code>uid</code>, or <code>creationTimestamp</code> fields;
the <code>generation</code> field is unique. It only accepts updates that increment the
field's current value.</p></li><li><p>If the <code>metadata.deletionTimestamp</code> is set, no new entry can be added to the
<code>metadata.finalizers</code> list.</p></li><li><p>Pod updates may not change fields other than <code>spec.containers[*].image</code>,
<code>spec.initContainers[*].image</code>, <code>spec.activeDeadlineSeconds</code> or
<code>spec.tolerations</code>. For <code>spec.tolerations</code>, you can only add new entries.</p></li><li><p>When updating the <code>spec.activeDeadlineSeconds</code> field, two types of updates
are allowed:</p><ol><li>setting the unassigned field to a positive number;</li><li>updating the field from a positive number to a smaller, non-negative
number.</li></ol></li></ul><h2 id=resource-sharing-and-communication>Resource sharing and communication</h2><p>Pods enable data sharing and communication among their constituent
containers.</p><h3 id=pod-storage>Storage in Pods</h3><p>A Pod can specify a set of shared storage
<a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volumes>volumes</a>. All containers
in the Pod can access the shared volumes, allowing those containers to
share data. Volumes also allow persistent data in a Pod to survive
in case one of the containers within needs to be restarted. See
<a href=/docs/concepts/storage/>Storage</a> for more information on how
Kubernetes implements shared storage and makes it available to Pods.</p><h3 id=pod-networking>Pod networking</h3><p>Each Pod is assigned a unique IP address for each address family. Every
container in a Pod shares the network namespace, including the IP address and
network ports. Inside a Pod (and <strong>only</strong> then), the containers that belong to the Pod
can communicate with one another using <code>localhost</code>. When containers in a Pod communicate
with entities <em>outside the Pod</em>,
they must coordinate how they use the shared network resources (such as ports).
Within a Pod, containers share an IP address and port space, and
can find each other via <code>localhost</code>. The containers in a Pod can also communicate
with each other using standard inter-process communications like SystemV semaphores
or POSIX shared memory. Containers in different Pods have distinct IP addresses
and can not communicate by OS-level IPC without special configuration.
Containers that want to interact with a container running in a different Pod can
use IP networking to communicate.</p><p>Containers within the Pod see the system hostname as being the same as the configured
<code>name</code> for the Pod. There's more about this in the <a href=/docs/concepts/cluster-administration/networking/>networking</a>
section.</p><h2 id=privileged-mode-for-containers>Privileged mode for containers</h2><p>In Linux, any container in a Pod can enable privileged mode using the <code>privileged</code> (Linux) flag on the <a href=/docs/tasks/configure-pod-container/security-context/>security context</a> of the container spec. This is useful for containers that want to use operating system administrative capabilities such as manipulating the network stack or accessing hardware devices.</p><p>If your cluster has the <code>WindowsHostProcessContainers</code> feature enabled, you can create a <a href=/docs/tasks/configure-pod-container/create-hostprocess-pod>Windows HostProcess pod</a> by setting the <code>windowsOptions.hostProcess</code> flag on the security context of the pod spec. All containers in these pods must run as Windows HostProcess containers. HostProcess pods run directly on the host and can also be used to perform administrative tasks as is done with Linux privileged containers.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Your <a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a> must support the concept of a privileged container for this setting to be relevant.</div><h2 id=static-pods>Static Pods</h2><p><em>Static Pods</em> are managed directly by the kubelet daemon on a specific node,
without the <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a>
observing them.
Whereas most Pods are managed by the control plane (for example, a
<a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>), for static
Pods, the kubelet directly supervises each static Pod (and restarts it if it fails).</p><p>Static Pods are always bound to one <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a> on a specific node.
The main use for static Pods is to run a self-hosted control plane: in other words,
using the kubelet to supervise the individual <a href=/docs/concepts/overview/components/#control-plane-components>control plane components</a>.</p><p>The kubelet automatically tries to create a <a class=glossary-tooltip title='An object in the API server that tracks a static pod on a kubelet.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-mirror-pod' target=_blank aria-label='mirror Pod'>mirror Pod</a>
on the Kubernetes API server for each static Pod.
This means that the Pods running on a node are visible on the API server,
but cannot be controlled from there.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>spec</code> of a static Pod cannot refer to other API objects
(e.g., <a class=glossary-tooltip title='Provides an identity for processes that run in a Pod.' data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/configure-service-account/ target=_blank aria-label=ServiceAccount>ServiceAccount</a>,
<a class=glossary-tooltip title='An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMap>ConfigMap</a>,
<a class=glossary-tooltip title='Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a>, etc).</div><h2 id=container-probes>Container probes</h2><p>A <em>probe</em> is a diagnostic performed periodically by the kubelet on a container. To perform a diagnostic, the kubelet can invoke different actions:</p><ul><li><code>ExecAction</code> (performed with the help of the container runtime)</li><li><code>TCPSocketAction</code> (checked directly by the kubelet)</li><li><code>HTTPGetAction</code> (checked directly by the kubelet)</li></ul><p>You can read more about <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>probes</a>
in the Pod Lifecycle documentation.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about the <a href=/docs/concepts/workloads/pods/pod-lifecycle/>lifecycle of a Pod</a>.</li><li>Learn about <a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a> and how you can use it to
configure different Pods with different container runtime configurations.</li><li>Read about <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> and how you can use it to manage application availability during disruptions.</li><li>Pod is a top-level resource in the Kubernetes REST API.
The
<a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/>Pod</a>
object definition describes the object in detail.</li><li><a href=/blog/2015/06/the-distributed-system-toolkit-patterns/>The Distributed System Toolkit: Patterns for Composite Containers</a> explains common layouts for Pods with more than one container.</li><li>Read about <a href=/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod topology spread constraints</a></li></ul><p>To understand the context for why Kubernetes wraps a common Pod API in other resources (such as <a class=glossary-tooltip title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSets>StatefulSets</a> or <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployments>Deployments</a>), you can read about the prior art, including:</p><ul><li><a href=https://aurora.apache.org/documentation/latest/reference/configuration/#job-schema>Aurora</a></li><li><a href=https://research.google.com/pubs/pub43438.html>Borg</a></li><li><a href=https://mesosphere.github.io/marathon/docs/rest-api.html>Marathon</a></li><li><a href=https://research.google/pubs/pub41684/>Omega</a></li><li><a href=https://engineering.fb.com/data-center-engineering/tupperware/>Tupperware</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c3c2b9cf30915ec9d46c147201da3332>5.1.1 - Pod Lifecycle</h1><p>This page describes the lifecycle of a Pod. Pods follow a defined lifecycle, starting
in the <code>Pending</code> <a href=#pod-phase>phase</a>, moving through <code>Running</code> if at least one
of its primary containers starts OK, and then through either the <code>Succeeded</code> or
<code>Failed</code> phases depending on whether any container in the Pod terminated in failure.</p><p>Whilst a Pod is running, the kubelet is able to restart containers to handle some
kind of faults. Within a Pod, Kubernetes tracks different container
<a href=#container-states>states</a> and determines what action to take to make the Pod
healthy again.</p><p>In the Kubernetes API, Pods have both a specification and an actual status. The
status for a Pod object consists of a set of <a href=#pod-conditions>Pod conditions</a>.
You can also inject <a href=#pod-readiness-gate>custom readiness information</a> into the
condition data for a Pod, if that is useful to your application.</p><p>Pods are only <a href=/docs/concepts/scheduling-eviction/>scheduled</a> once in their lifetime.
Once a Pod is scheduled (assigned) to a Node, the Pod runs on that Node until it stops
or is <a href=#pod-termination>terminated</a>.</p><h2 id=pod-lifetime>Pod lifetime</h2><p>Like individual application containers, Pods are considered to be relatively
ephemeral (rather than durable) entities. Pods are created, assigned a unique
ID (<a href=/docs/concepts/overview/working-with-objects/names/#uids>UID</a>), and scheduled
to nodes where they remain until termination (according to restart policy) or
deletion.
If a <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Node>Node</a> dies, the Pods scheduled to that node
are <a href=#pod-garbage-collection>scheduled for deletion</a> after a timeout period.</p><p>Pods do not, by themselves, self-heal. If a Pod is scheduled to a
<a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> that then fails, the Pod is deleted; likewise, a Pod won't
survive an eviction due to a lack of resources or Node maintenance. Kubernetes uses a
higher-level abstraction, called a
<a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>, that handles the work of
managing the relatively disposable Pod instances.</p><p>A given Pod (as defined by a UID) is never "rescheduled" to a different node; instead,
that Pod can be replaced by a new, near-identical Pod, with even the same name if
desired, but with a different UID.</p><p>When something is said to have the same lifetime as a Pod, such as a
<a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a>,
that means that the thing exists as long as that specific Pod (with that exact UID)
exists. If that Pod is deleted for any reason, and even if an identical replacement
is created, the related thing (a volume, in this example) is also destroyed and
created anew.</p><figure class=diagram-medium><img src=/images/docs/pod.svg><figcaption><h4>Pod diagram</h4></figcaption></figure><p><em>A multi-container Pod that contains a file puller and a
web server that uses a persistent volume for shared storage between the containers.</em></p><h2 id=pod-phase>Pod phase</h2><p>A Pod's <code>status</code> field is a
<a href=/docs/reference/generated/kubernetes-api/v1.25/#podstatus-v1-core>PodStatus</a>
object, which has a <code>phase</code> field.</p><p>The phase of a Pod is a simple, high-level summary of where the Pod is in its
lifecycle. The phase is not intended to be a comprehensive rollup of observations
of container or Pod state, nor is it intended to be a comprehensive state machine.</p><p>The number and meanings of Pod phase values are tightly guarded.
Other than what is documented here, nothing should be assumed about Pods that
have a given <code>phase</code> value.</p><p>Here are the possible values for <code>phase</code>:</p><table><thead><tr><th style=text-align:left>Value</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>Pending</code></td><td style=text-align:left>The Pod has been accepted by the Kubernetes cluster, but one or more of the containers has not been set up and made ready to run. This includes time a Pod spends waiting to be scheduled as well as the time spent downloading container images over the network.</td></tr><tr><td style=text-align:left><code>Running</code></td><td style=text-align:left>The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.</td></tr><tr><td style=text-align:left><code>Succeeded</code></td><td style=text-align:left>All containers in the Pod have terminated in success, and will not be restarted.</td></tr><tr><td style=text-align:left><code>Failed</code></td><td style=text-align:left>All containers in the Pod have terminated, and at least one container has terminated in failure. That is, the container either exited with non-zero status or was terminated by the system.</td></tr><tr><td style=text-align:left><code>Unknown</code></td><td style=text-align:left>For some reason the state of the Pod could not be obtained. This phase typically occurs due to an error in communicating with the node where the Pod should be running.</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> When a Pod is being deleted, it is shown as <code>Terminating</code> by some kubectl commands.
This <code>Terminating</code> status is not one of the Pod phases.
A Pod is granted a term to terminate gracefully, which defaults to 30 seconds.
You can use the flag <code>--force</code> to <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination-forced>terminate a Pod by force</a>.</div><p>If a node dies or is disconnected from the rest of the cluster, Kubernetes
applies a policy for setting the <code>phase</code> of all Pods on the lost node to Failed.</p><h2 id=container-states>Container states</h2><p>As well as the <a href=#pod-phase>phase</a> of the Pod overall, Kubernetes tracks the state of
each container inside a Pod. You can use
<a href=/docs/concepts/containers/container-lifecycle-hooks/>container lifecycle hooks</a> to
trigger events to run at certain points in a container's lifecycle.</p><p>Once the <a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a>
assigns a Pod to a Node, the kubelet starts creating containers for that Pod
using a <a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>.
There are three possible container states: <code>Waiting</code>, <code>Running</code>, and <code>Terminated</code>.</p><p>To check the state of a Pod's containers, you can use
<code>kubectl describe pod &lt;name-of-pod></code>. The output shows the state for each container
within that Pod.</p><p>Each state has a specific meaning:</p><h3 id=container-state-waiting><code>Waiting</code></h3><p>If a container is not in either the <code>Running</code> or <code>Terminated</code> state, it is <code>Waiting</code>.
A container in the <code>Waiting</code> state is still running the operations it requires in
order to complete start up: for example, pulling the container image from a container
image registry, or applying <a class=glossary-tooltip title='Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a>
data.
When you use <code>kubectl</code> to query a Pod with a container that is <code>Waiting</code>, you also see
a Reason field to summarize why the container is in that state.</p><h3 id=container-state-running><code>Running</code></h3><p>The <code>Running</code> status indicates that a container is executing without issues. If there
was a <code>postStart</code> hook configured, it has already executed and finished. When you use
<code>kubectl</code> to query a Pod with a container that is <code>Running</code>, you also see information
about when the container entered the <code>Running</code> state.</p><h3 id=container-state-terminated><code>Terminated</code></h3><p>A container in the <code>Terminated</code> state began execution and then either ran to
completion or failed for some reason. When you use <code>kubectl</code> to query a Pod with
a container that is <code>Terminated</code>, you see a reason, an exit code, and the start and
finish time for that container's period of execution.</p><p>If a container has a <code>preStop</code> hook configured, this hook runs before the container enters
the <code>Terminated</code> state.</p><h2 id=restart-policy>Container restart policy</h2><p>The <code>spec</code> of a Pod has a <code>restartPolicy</code> field with possible values Always, OnFailure,
and Never. The default value is Always.</p><p>The <code>restartPolicy</code> applies to all containers in the Pod. <code>restartPolicy</code> only
refers to restarts of the containers by the kubelet on the same node. After containers
in a Pod exit, the kubelet restarts them with an exponential back-off delay (10s, 20s,
40s, …), that is capped at five minutes. Once a container has executed for 10 minutes
without any problems, the kubelet resets the restart backoff timer for that container.</p><h2 id=pod-conditions>Pod conditions</h2><p>A Pod has a PodStatus, which has an array of
<a href=/docs/reference/generated/kubernetes-api/v1.25/#podcondition-v1-core>PodConditions</a>
through which the Pod has or has not passed. Kubelet manages the following
PodConditions:</p><ul><li><code>PodScheduled</code>: the Pod has been scheduled to a node.</li><li><code>PodHasNetwork</code>: (alpha feature; must be <a href=#pod-has-network>enabled explicitly</a>) the
Pod sandbox has been successfully created and networking configured.</li><li><code>ContainersReady</code>: all containers in the Pod are ready.</li><li><code>Initialized</code>: all <a href=/docs/concepts/workloads/pods/init-containers/>init containers</a>
have completed successfully.</li><li><code>Ready</code>: the Pod is able to serve requests and should be added to the load
balancing pools of all matching Services.</li></ul><table><thead><tr><th style=text-align:left>Field name</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><code>type</code></td><td style=text-align:left>Name of this Pod condition.</td></tr><tr><td style=text-align:left><code>status</code></td><td style=text-align:left>Indicates whether that condition is applicable, with possible values "<code>True</code>", "<code>False</code>", or "<code>Unknown</code>".</td></tr><tr><td style=text-align:left><code>lastProbeTime</code></td><td style=text-align:left>Timestamp of when the Pod condition was last probed.</td></tr><tr><td style=text-align:left><code>lastTransitionTime</code></td><td style=text-align:left>Timestamp for when the Pod last transitioned from one status to another.</td></tr><tr><td style=text-align:left><code>reason</code></td><td style=text-align:left>Machine-readable, UpperCamelCase text indicating the reason for the condition's last transition.</td></tr><tr><td style=text-align:left><code>message</code></td><td style=text-align:left>Human-readable message indicating details about the last status transition.</td></tr></tbody></table><h3 id=pod-readiness-gate>Pod readiness</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><p>Your application can inject extra feedback or signals into PodStatus:
<em>Pod readiness</em>. To use this, set <code>readinessGates</code> in the Pod's <code>spec</code> to
specify a list of additional conditions that the kubelet evaluates for Pod readiness.</p><p>Readiness gates are determined by the current state of <code>status.condition</code>
fields for the Pod. If Kubernetes cannot find such a condition in the
<code>status.conditions</code> field of a Pod, the status of the condition
is defaulted to "<code>False</code>".</p><p>Here is an example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>readinessGates</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>conditionType</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;www.example.com/feature-1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>conditions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Ready                             <span style=color:#bbb> </span><span style=color:#080;font-style:italic># a built in PodCondition</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;False&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastProbeTime</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>null</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span>2018-01-01T00:00:00Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;www.example.com/feature-1&#34;</span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># an extra PodCondition</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;False&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastProbeTime</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>null</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span>2018-01-01T00:00:00Z<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containerStatuses</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>containerID</span>:<span style=color:#bbb> </span>docker://abcd...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ready</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The Pod conditions you add must have names that meet the Kubernetes <a href=/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set>label key format</a>.</p><h3 id=pod-readiness-status>Status for Pod readiness</h3><p>The <code>kubectl patch</code> command does not support patching object status.
To set these <code>status.conditions</code> for the pod, applications and
<a class=glossary-tooltip title='A specialized controller used to manage a custom resource' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/operator/ target=_blank aria-label=operators>operators</a> should use
the <code>PATCH</code> action.
You can use a <a href=/docs/reference/using-api/client-libraries/>Kubernetes client library</a> to
write code that sets custom Pod conditions for Pod readiness.</p><p>For a Pod that uses custom conditions, that Pod is evaluated to be ready <strong>only</strong>
when both the following statements apply:</p><ul><li>All containers in the Pod are ready.</li><li>All conditions specified in <code>readinessGates</code> are <code>True</code>.</li></ul><p>When a Pod's containers are Ready but at least one custom condition is missing or
<code>False</code>, the kubelet sets the Pod's <a href=#pod-conditions>condition</a> to <code>ContainersReady</code>.</p><h3 id=pod-has-network>Pod network readiness</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><p>After a Pod gets scheduled on a node, it needs to be admitted by the Kubelet and
have any volumes mounted. Once these phases are complete, the Kubelet works with
a container runtime (using <a class=glossary-tooltip title='An API for container runtimes to integrate with kubelet' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#container-runtime target=_blank aria-label='Container runtime interface (CRI)'>Container runtime interface (CRI)</a>) to set up a
runtime sandbox and configure networking for the Pod. If the
<code>PodHasNetworkCondition</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> is enabled,
Kubelet reports whether a pod has reached this initialization milestone through
the <code>PodHasNetwork</code> condition in the <code>status.conditions</code> field of a Pod.</p><p>The <code>PodHasNetwork</code> condition is set to <code>False</code> by the Kubelet when it detects a
Pod does not have a runtime sandbox with networking configured. This occurs in
the following scenarios:</p><ul><li>Early in the lifecycle of the Pod, when the kubelet has not yet begun to set up a sandbox for the Pod using the container runtime.</li><li>Later in the lifecycle of the Pod, when the Pod sandbox has been destroyed due
to either:<ul><li>the node rebooting, without the Pod getting evicted</li><li>for container runtimes that use virtual machines for isolation, the Pod
sandbox virtual machine rebooting, which then requires creating a new sandbox and fresh container network configuration.</li></ul></li></ul><p>The <code>PodHasNetwork</code> condition is set to <code>True</code> by the kubelet after the
successful completion of sandbox creation and network configuration for the Pod
by the runtime plugin. The kubelet can start pulling container images and create
containers after <code>PodHasNetwork</code> condition has been set to <code>True</code>.</p><p>For a Pod with init containers, the kubelet sets the <code>Initialized</code> condition to
<code>True</code> after the init containers have successfully completed (which happens
after successful sandbox creation and network configuration by the runtime
plugin). For a Pod without init containers, the kubelet sets the <code>Initialized</code>
condition to <code>True</code> before sandbox creation and network configuration starts.</p><h2 id=container-probes>Container probes</h2><p>A <em>probe</em> is a diagnostic
performed periodically by the
<a href=/docs/reference/command-line-tools-reference/kubelet/>kubelet</a>
on a container. To perform a diagnostic,
the kubelet either executes code within the container, or makes
a network request.</p><h3 id=probe-check-methods>Check mechanisms</h3><p>There are four different ways to check a container using a probe.
Each probe must define exactly one of these four mechanisms:</p><dl><dt><code>exec</code></dt><dd>Executes a specified command inside the container. The diagnostic
is considered successful if the command exits with a status code of 0.</dd><dt><code>grpc</code></dt><dd>Performs a remote procedure call using <a href=https://grpc.io/>gRPC</a>.
The target should implement
<a href=https://grpc.io/grpc/core/md_doc_health-checking.html>gRPC health checks</a>.
The diagnostic is considered successful if the <code>status</code>
of the response is <code>SERVING</code>.<br>gRPC probes are an alpha feature and are only available if you
enable the <code>GRPCContainerProbe</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>.</dd><dt><code>httpGet</code></dt><dd>Performs an HTTP <code>GET</code> request against the Pod's IP
address on a specified port and path. The diagnostic is
considered successful if the response has a status code
greater than or equal to 200 and less than 400.</dd><dt><code>tcpSocket</code></dt><dd>Performs a TCP check against the Pod's IP address on
a specified port. The diagnostic is considered successful if
the port is open. If the remote system (the container) closes
the connection immediately after it opens, this counts as healthy.</dd></dl><h3 id=probe-outcome>Probe outcome</h3><p>Each probe has one of three results:</p><dl><dt><code>Success</code></dt><dd>The container passed the diagnostic.</dd><dt><code>Failure</code></dt><dd>The container failed the diagnostic.</dd><dt><code>Unknown</code></dt><dd>The diagnostic failed (no action should be taken, and the kubelet
will make further checks).</dd></dl><h3 id=types-of-probe>Types of probe</h3><p>The kubelet can optionally perform and react to three kinds of probes on running
containers:</p><dl><dt><code>livenessProbe</code></dt><dd>Indicates whether the container is running. If
the liveness probe fails, the kubelet kills the container, and the container
is subjected to its <a href=#restart-policy>restart policy</a>. If a container does not
provide a liveness probe, the default state is <code>Success</code>.</dd><dt><code>readinessProbe</code></dt><dd>Indicates whether the container is ready to respond to requests.
If the readiness probe fails, the endpoints controller removes the Pod's IP
address from the endpoints of all Services that match the Pod. The default
state of readiness before the initial delay is <code>Failure</code>. If a container does
not provide a readiness probe, the default state is <code>Success</code>.</dd><dt><code>startupProbe</code></dt><dd>Indicates whether the application within the container is started.
All other probes are disabled if a startup probe is provided, until it succeeds.
If the startup probe fails, the kubelet kills the container, and the container
is subjected to its <a href=#restart-policy>restart policy</a>. If a container does not
provide a startup probe, the default state is <code>Success</code>.</dd></dl><p>For more information about how to set up a liveness, readiness, or startup probe,
see <a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/>Configure Liveness, Readiness and Startup Probes</a>.</p><h4 id=when-should-you-use-a-liveness-probe>When should you use a liveness probe?</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.0 [stable]</code></div><p>If the process in your container is able to crash on its own whenever it
encounters an issue or becomes unhealthy, you do not necessarily need a liveness
probe; the kubelet will automatically perform the correct action in accordance
with the Pod's <code>restartPolicy</code>.</p><p>If you'd like your container to be killed and restarted if a probe fails, then
specify a liveness probe, and specify a <code>restartPolicy</code> of Always or OnFailure.</p><h4 id=when-should-you-use-a-readiness-probe>When should you use a readiness probe?</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.0 [stable]</code></div><p>If you'd like to start sending traffic to a Pod only when a probe succeeds,
specify a readiness probe. In this case, the readiness probe might be the same
as the liveness probe, but the existence of the readiness probe in the spec means
that the Pod will start without receiving any traffic and only start receiving
traffic after the probe starts succeeding.</p><p>If you want your container to be able to take itself down for maintenance, you
can specify a readiness probe that checks an endpoint specific to readiness that
is different from the liveness probe.</p><p>If your app has a strict dependency on back-end services, you can implement both
a liveness and a readiness probe. The liveness probe passes when the app itself
is healthy, but the readiness probe additionally checks that each required
back-end service is available. This helps you avoid directing traffic to Pods
that can only respond with error messages.</p><p>If your container needs to work on loading large data, configuration files, or
migrations during startup, you can use a
<a href=#when-should-you-use-a-startup-probe>startup probe</a>. However, if you want to
detect the difference between an app that has failed and an app that is still
processing its startup data, you might prefer a readiness probe.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you want to be able to drain requests when the Pod is deleted, you do not
necessarily need a readiness probe; on deletion, the Pod automatically puts itself
into an unready state regardless of whether the readiness probe exists.
The Pod remains in the unready state while it waits for the containers in the Pod
to stop.</div><h4 id=when-should-you-use-a-startup-probe>When should you use a startup probe?</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>Startup probes are useful for Pods that have containers that take a long time to
come into service. Rather than set a long liveness interval, you can configure
a separate configuration for probing the container as it starts up, allowing
a time longer than the liveness interval would allow.</p><p>If your container usually starts in more than
<code>initialDelaySeconds + failureThreshold × periodSeconds</code>, you should specify a
startup probe that checks the same endpoint as the liveness probe. The default for
<code>periodSeconds</code> is 10s. You should then set its <code>failureThreshold</code> high enough to
allow the container to start, without changing the default values of the liveness
probe. This helps to protect against deadlocks.</p><h2 id=pod-termination>Termination of Pods</h2><p>Because Pods represent processes running on nodes in the cluster, it is important to
allow those processes to gracefully terminate when they are no longer needed (rather
than being abruptly stopped with a <code>KILL</code> signal and having no chance to clean up).</p><p>The design aim is for you to be able to request deletion and know when processes
terminate, but also be able to ensure that deletes eventually complete.
When you request deletion of a Pod, the cluster records and tracks the intended grace period
before the Pod is allowed to be forcefully killed. With that forceful shutdown tracking in
place, the <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> attempts graceful
shutdown.</p><p>Typically, the container runtime sends a TERM signal to the main process in each
container. Many container runtimes respect the <code>STOPSIGNAL</code> value defined in the container
image and send this instead of TERM.
Once the grace period has expired, the KILL signal is sent to any remaining
processes, and the Pod is then deleted from the
<a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API Server'>API Server</a>. If the kubelet or the
container runtime's management service is restarted while waiting for processes to terminate, the
cluster retries from the start including the full original grace period.</p><p>An example flow:</p><ol><li>You use the <code>kubectl</code> tool to manually delete a specific Pod, with the default grace period
(30 seconds).</li><li>The Pod in the API server is updated with the time beyond which the Pod is considered "dead"
along with the grace period.
If you use <code>kubectl describe</code> to check on the Pod you're deleting, that Pod shows up as
"Terminating".
On the node where the Pod is running: as soon as the kubelet sees that a Pod has been marked
as terminating (a graceful shutdown duration has been set), the kubelet begins the local Pod
shutdown process.<ol><li>If one of the Pod's containers has defined a <code>preStop</code>
<a href=/docs/concepts/containers/container-lifecycle-hooks>hook</a>, the kubelet
runs that hook inside of the container. If the <code>preStop</code> hook is still running after the
grace period expires, the kubelet requests a small, one-off grace period extension of 2
seconds.<div class="alert alert-info note callout" role=alert><strong>Note:</strong> If the <code>preStop</code> hook needs longer to complete than the default grace period allows,
you must modify <code>terminationGracePeriodSeconds</code> to suit this.</div></li><li>The kubelet triggers the container runtime to send a TERM signal to process 1 inside each
container.<div class="alert alert-info note callout" role=alert><strong>Note:</strong> The containers in the Pod receive the TERM signal at different times and in an arbitrary
order. If the order of shutdowns matters, consider using a <code>preStop</code> hook to synchronize.</div></li></ol></li><li>At the same time as the kubelet is starting graceful shutdown, the control plane removes that
shutting-down Pod from EndpointSlice (and Endpoints) objects where these represent
a <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> with a configured
<a class=glossary-tooltip title='Allows users to filter a list of resources based on labels.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=selector>selector</a>.
<a class=glossary-tooltip title='ReplicaSet ensures that a specified number of Pod replicas are running at one time' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/replicaset/ target=_blank aria-label=ReplicaSets>ReplicaSets</a> and other workload resources
no longer treat the shutting-down Pod as a valid, in-service replica. Pods that shut down slowly
cannot continue to serve traffic as load balancers (like the service proxy) remove the Pod from
the list of endpoints as soon as the termination grace period <em>begins</em>.</li><li>When the grace period expires, the kubelet triggers forcible shutdown. The container runtime sends
<code>SIGKILL</code> to any processes still running in any container in the Pod.
The kubelet also cleans up a hidden <code>pause</code> container if that container runtime uses one.</li><li>The kubelet triggers forcible removal of Pod object from the API server, by setting grace period
to 0 (immediate deletion).</li><li>The API server deletes the Pod's API object, which is then no longer visible from any client.</li></ol><h3 id=pod-termination-forced>Forced Pod termination</h3><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Forced deletions can be potentially disruptive for some workloads and their Pods.</div><p>By default, all deletes are graceful within 30 seconds. The <code>kubectl delete</code> command supports
the <code>--grace-period=&lt;seconds></code> option which allows you to override the default and specify your
own value.</p><p>Setting the grace period to <code>0</code> forcibly and immediately deletes the Pod from the API
server. If the pod was still running on a node, that forcible deletion triggers the kubelet to
begin immediate cleanup.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must specify an additional flag <code>--force</code> along with <code>--grace-period=0</code> in order to perform force deletions.</div><p>When a force deletion is performed, the API server does not wait for confirmation
from the kubelet that the Pod has been terminated on the node it was running on. It
removes the Pod in the API immediately so a new Pod can be created with the same
name. On the node, Pods that are set to terminate immediately will still be given
a small grace period before being force killed.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.</div><p>If you need to force-delete Pods that are part of a StatefulSet, refer to the task
documentation for
<a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>deleting Pods from a StatefulSet</a>.</p><h3 id=pod-garbage-collection>Garbage collection of terminated Pods</h3><p>For failed Pods, the API objects remain in the cluster's API until a human or
<a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> process
explicitly removes them.</p><p>The control plane cleans up terminated Pods (with a phase of <code>Succeeded</code> or
<code>Failed</code>), when the number of Pods exceeds the configured threshold
(determined by <code>terminated-pod-gc-threshold</code> in the kube-controller-manager).
This avoids a resource leak as Pods are created and terminated over time.</p><h2 id=what-s-next>What's next</h2><ul><li><p>Get hands-on experience
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>attaching handlers to container lifecycle events</a>.</p></li><li><p>Get hands-on experience
<a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/>configuring Liveness, Readiness and Startup Probes</a>.</p></li><li><p>Learn more about <a href=/docs/concepts/containers/container-lifecycle-hooks/>container lifecycle hooks</a>.</p></li><li><p>For detailed information about Pod and container status in the API, see
the API reference documentation covering
<a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodStatus><code>.status</code></a> for Pod.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1ccbd4eeded6ab138d98b59175bd557e>5.1.2 - Init Containers</h1><p>This page provides an overview of init containers: specialized containers that run
before app containers in a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>.
Init containers can contain utilities or setup scripts not present in an app image.</p><p>You can specify init containers in the Pod specification alongside the <code>containers</code>
array (which describes app containers).</p><h2 id=understanding-init-containers>Understanding init containers</h2><p>A <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> can have multiple containers
running apps within it, but it can also have one or more init containers, which are run
before the app containers are started.</p><p>Init containers are exactly like regular containers, except:</p><ul><li>Init containers always run to completion.</li><li>Each init container must complete successfully before the next one starts.</li></ul><p>If a Pod's init container fails, the kubelet repeatedly restarts that init container until it succeeds.
However, if the Pod has a <code>restartPolicy</code> of Never, and an init container fails during startup of that Pod, Kubernetes treats the overall Pod as failed.</p><p>To specify an init container for a Pod, add the <code>initContainers</code> field into
the <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec>Pod specification</a>,
as an array of <code>container</code> items (similar to the app <code>containers</code> field and its contents).
See <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container>Container</a> in the
API reference for more details.</p><p>The status of the init containers is returned in <code>.status.initContainerStatuses</code>
field as an array of the container statuses (similar to the <code>.status.containerStatuses</code>
field).</p><h3 id=differences-from-regular-containers>Differences from regular containers</h3><p>Init containers support all the fields and features of app containers,
including resource limits, volumes, and security settings. However, the
resource requests and limits for an init container are handled differently,
as documented in <a href=#resources>Resources</a>.</p><p>Also, init containers do not support <code>lifecycle</code>, <code>livenessProbe</code>, <code>readinessProbe</code>, or
<code>startupProbe</code> because they must run to completion before the Pod can be ready.</p><p>If you specify multiple init containers for a Pod, kubelet runs each init
container sequentially. Each init container must succeed before the next can run.
When all of the init containers have run to completion, kubelet initializes
the application containers for the Pod and runs them as usual.</p><h2 id=using-init-containers>Using init containers</h2><p>Because init containers have separate images from app containers, they
have some advantages for start-up related code:</p><ul><li>Init containers can contain utilities or custom code for setup that are not present in an app
image. For example, there is no need to make an image <code>FROM</code> another image just to use a tool like
<code>sed</code>, <code>awk</code>, <code>python</code>, or <code>dig</code> during setup.</li><li>The application image builder and deployer roles can work independently without
the need to jointly build a single app image.</li><li>Init containers can run with a different view of the filesystem than app containers in the
same Pod. Consequently, they can be given access to
<a class=glossary-tooltip title='Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secrets>Secrets</a> that app containers cannot access.</li><li>Because init containers run to completion before any app containers start, init containers offer
a mechanism to block or delay app container startup until a set of preconditions are met. Once
preconditions are met, all of the app containers in a Pod can start in parallel.</li><li>Init containers can securely run utilities or custom code that would otherwise make an app
container image less secure. By keeping unnecessary tools separate you can limit the attack
surface of your app container image.</li></ul><h3 id=examples>Examples</h3><p>Here are some ideas for how to use init containers:</p><ul><li><p>Wait for a <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> to
be created, using a shell one-line command like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f;font-weight:700>for</span> i in <span style=color:#666>{</span>1..100<span style=color:#666>}</span>; <span style=color:#a2f;font-weight:700>do</span> sleep 1; <span style=color:#a2f;font-weight:700>if</span> dig myservice; <span style=color:#a2f;font-weight:700>then</span> <span style=color:#a2f>exit</span> 0; <span style=color:#a2f;font-weight:700>fi</span>; <span style=color:#a2f;font-weight:700>done</span>; <span style=color:#a2f>exit</span> <span style=color:#666>1</span>
</span></span></code></pre></div></li><li><p>Register this Pod with a remote server from the downward API with a command like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl -X POST http://<span style=color:#b8860b>$MANAGEMENT_SERVICE_HOST</span>:<span style=color:#b8860b>$MANAGEMENT_SERVICE_PORT</span>/register -d <span style=color:#b44>&#39;instance=$(&lt;POD_NAME&gt;)&amp;ip=$(&lt;POD_IP&gt;)&#39;</span>
</span></span></code></pre></div></li><li><p>Wait for some time before starting the app container with a command like</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sleep <span style=color:#666>60</span>
</span></span></code></pre></div></li><li><p>Clone a Git repository into a <a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volume>Volume</a></p></li><li><p>Place values into a configuration file and run a template tool to dynamically
generate a configuration file for the main app container. For example,
place the <code>POD_IP</code> value in a configuration and generate the main app
configuration file using Jinja.</p></li></ul><h4 id=init-containers-in-use>Init containers in use</h4><p>This example defines a simple Pod that has two init containers.
The first waits for <code>myservice</code>, and the second waits for <code>mydb</code>. Once both
init containers complete, the Pod runs the app container from its <code>spec</code> section.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myapp-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;echo The app is running! &amp;&amp; sleep 3600&#39;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>initContainers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>init-mydb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#39;sh&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#39;-c&#39;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done&#34;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div><p>You can start this Pod by running:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>pod/myapp-pod created
</code></pre><p>And check on its status with:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME        READY     STATUS     RESTARTS   AGE
myapp-pod   0/1       Init:0/2   0          6m
</code></pre><p>or for more details:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Name:          myapp-pod
Namespace:     default
[...]
Labels:        app.kubernetes.io/name=MyApp
Status:        Pending
[...]
Init Containers:
  init-myservice:
[...]
    State:         Running
[...]
  init-mydb:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Containers:
  myapp-container:
[...]
    State:         Waiting
      Reason:      PodInitializing
    Ready:         False
[...]
Events:
  FirstSeen    LastSeen    Count    From                      SubObjectPath                           Type          Reason        Message
  ---------    --------    -----    ----                      -------------                           --------      ------        -------
  16s          16s         1        {default-scheduler }                                              Normal        Scheduled     Successfully assigned myapp-pod to 172.17.4.201
  16s          16s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulling       pulling image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Pulled        Successfully pulled image &#34;busybox&#34;
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Created       Created container init-myservice
  13s          13s         1        {kubelet 172.17.4.201}    spec.initContainers{init-myservice}     Normal        Started       Started container init-myservice
</code></pre><p>To see logs for the init containers in this Pod, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs myapp-pod -c init-myservice <span style=color:#080;font-style:italic># Inspect the first init container</span>
</span></span><span style=display:flex><span>kubectl logs myapp-pod -c init-mydb      <span style=color:#080;font-style:italic># Inspect the second init container</span>
</span></span></code></pre></div><p>At this point, those init containers will be waiting to discover Services named
<code>mydb</code> and <code>myservice</code>.</p><p>Here's a configuration you can use to make those Services appear:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myservice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9377</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>To create the <code>mydb</code> and <code>myservice</code> services:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f services.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>service/myservice created
service/mydb created
</code></pre><p>You'll then see that those init containers complete, and that the <code>myapp-pod</code>
Pod moves into the Running state:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get -f myapp.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME        READY     STATUS    RESTARTS   AGE
myapp-pod   1/1       Running   0          9m
</code></pre><p>This simple example should provide some inspiration for you to create your own
init containers. <a href=#what-s-next>What's next</a> contains a link to a more detailed example.</p><h2 id=detailed-behavior>Detailed behavior</h2><p>During Pod startup, the kubelet delays running init containers until the networking
and storage are ready. Then the kubelet runs the Pod's init containers in the order
they appear in the Pod's spec.</p><p>Each init container must exit successfully before
the next container starts. If a container fails to start due to the runtime or
exits with failure, it is retried according to the Pod <code>restartPolicy</code>. However,
if the Pod <code>restartPolicy</code> is set to Always, the init containers use
<code>restartPolicy</code> OnFailure.</p><p>A Pod cannot be <code>Ready</code> until all init containers have succeeded. The ports on an
init container are not aggregated under a Service. A Pod that is initializing
is in the <code>Pending</code> state but should have a condition <code>Initialized</code> set to false.</p><p>If the Pod <a href=#pod-restart-reasons>restarts</a>, or is restarted, all init containers
must execute again.</p><p>Changes to the init container spec are limited to the container image field.
Altering an init container image field is equivalent to restarting the Pod.</p><p>Because init containers can be restarted, retried, or re-executed, init container
code should be idempotent. In particular, code that writes to files on <code>EmptyDirs</code>
should be prepared for the possibility that an output file already exists.</p><p>Init containers have all of the fields of an app container. However, Kubernetes
prohibits <code>readinessProbe</code> from being used because init containers cannot
define readiness distinct from completion. This is enforced during validation.</p><p>Use <code>activeDeadlineSeconds</code> on the Pod to prevent init containers from failing forever.
The active deadline includes init containers.
However it is recommended to use <code>activeDeadlineSeconds</code> only if teams deploy their application
as a Job, because <code>activeDeadlineSeconds</code> has an effect even after initContainer finished.
The Pod which is already running correctly would be killed by <code>activeDeadlineSeconds</code> if you set.</p><p>The name of each app and init container in a Pod must be unique; a
validation error is thrown for any container sharing a name with another.</p><h3 id=resources>Resources</h3><p>Given the ordering and execution for init containers, the following rules
for resource usage apply:</p><ul><li>The highest of any particular resource request or limit defined on all init
containers is the <em>effective init request/limit</em>. If any resource has no
resource limit specified this is considered as the highest limit.</li><li>The Pod's <em>effective request/limit</em> for a resource is the higher of:<ul><li>the sum of all app containers request/limit for a resource</li><li>the effective init request/limit for a resource</li></ul></li><li>Scheduling is done based on effective requests/limits, which means
init containers can reserve resources for initialization that are not used
during the life of the Pod.</li><li>The QoS (quality of service) tier of the Pod's <em>effective QoS tier</em> is the
QoS tier for init containers and app containers alike.</li></ul><p>Quota and limits are applied based on the effective Pod request and
limit.</p><p>Pod level control groups (cgroups) are based on the effective Pod request and
limit, the same as the scheduler.</p><h3 id=pod-restart-reasons>Pod restart reasons</h3><p>A Pod can restart, causing re-execution of init containers, for the following
reasons:</p><ul><li>The Pod infrastructure container is restarted. This is uncommon and would
have to be done by someone with root access to nodes.</li><li>All containers in a Pod are terminated while <code>restartPolicy</code> is set to Always,
forcing a restart, and the init container completion record has been lost due
to garbage collection.</li></ul><p>The Pod will not be restarted when the init container image is changed, or the
init container completion record has been lost due to garbage collection. This
applies for Kubernetes v1.20 and later. If you are using an earlier version of
Kubernetes, consult the documentation for the version you are using.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/tasks/configure-pod-container/configure-pod-initialization/#create-a-pod-that-has-an-init-container>creating a Pod that has an init container</a></li><li>Learn how to <a href=/docs/tasks/debug/debug-application/debug-init-containers/>debug init containers</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4aaf43c715cd764bc8ed4436f3537e68>5.1.3 - Disruptions</h1><p>This guide is for application owners who want to build
highly available applications, and thus need to understand
what types of disruptions can happen to Pods.</p><p>It is also for cluster administrators who want to perform automated
cluster actions, like upgrading and autoscaling clusters.</p><h2 id=voluntary-and-involuntary-disruptions>Voluntary and involuntary disruptions</h2><p>Pods do not disappear until someone (a person or a controller) destroys them, or
there is an unavoidable hardware or system software error.</p><p>We call these unavoidable cases <em>involuntary disruptions</em> to
an application. Examples are:</p><ul><li>a hardware failure of the physical machine backing the node</li><li>cluster administrator deletes VM (instance) by mistake</li><li>cloud provider or hypervisor failure makes VM disappear</li><li>a kernel panic</li><li>the node disappears from the cluster due to cluster network partition</li><li>eviction of a pod due to the node being <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>out-of-resources</a>.</li></ul><p>Except for the out-of-resources condition, all these conditions
should be familiar to most users; they are not specific
to Kubernetes.</p><p>We call other cases <em>voluntary disruptions</em>. These include both
actions initiated by the application owner and those initiated by a Cluster
Administrator. Typical application owner actions include:</p><ul><li>deleting the deployment or other controller that manages the pod</li><li>updating a deployment's pod template causing a restart</li><li>directly deleting a pod (e.g. by accident)</li></ul><p>Cluster administrator actions include:</p><ul><li><a href=/docs/tasks/administer-cluster/safely-drain-node/>Draining a node</a> for repair or upgrade.</li><li>Draining a node from a cluster to scale the cluster down (learn about
<a href=https://github.com/kubernetes/autoscaler/#readme>Cluster Autoscaling</a>
).</li><li>Removing a pod from a node to permit something else to fit on that node.</li></ul><p>These actions might be taken directly by the cluster administrator, or by automation
run by the cluster administrator, or by your cluster hosting provider.</p><p>Ask your cluster administrator or consult your cloud provider or distribution documentation
to determine if any sources of voluntary disruptions are enabled for your cluster.
If none are enabled, you can skip creating Pod Disruption Budgets.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Not all voluntary disruptions are constrained by Pod Disruption Budgets. For example,
deleting deployments or pods bypasses Pod Disruption Budgets.</div><h2 id=dealing-with-disruptions>Dealing with disruptions</h2><p>Here are some ways to mitigate involuntary disruptions:</p><ul><li>Ensure your pod <a href=/docs/tasks/configure-pod-container/assign-memory-resource>requests the resources</a> it needs.</li><li>Replicate your application if you need higher availability. (Learn about running replicated
<a href=/docs/tasks/run-application/run-stateless-application-deployment/>stateless</a>
and <a href=/docs/tasks/run-application/run-replicated-stateful-application/>stateful</a> applications.)</li><li>For even higher availability when running replicated applications,
spread applications across racks (using
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>anti-affinity</a>)
or across zones (if using a
<a href=/docs/setup/multiple-zones>multi-zone cluster</a>.)</li></ul><p>The frequency of voluntary disruptions varies. On a basic Kubernetes cluster, there are
no automated voluntary disruptions (only user-triggered ones). However, your cluster administrator or hosting provider
may run some additional services which cause voluntary disruptions. For example,
rolling out node software updates can cause voluntary disruptions. Also, some implementations
of cluster (node) autoscaling may cause voluntary disruptions to defragment and compact nodes.
Your cluster administrator or hosting provider should have documented what level of voluntary
disruptions, if any, to expect. Certain configuration options, such as
<a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>using PriorityClasses</a>
in your pod spec can also cause voluntary (and involuntary) disruptions.</p><h2 id=pod-disruption-budgets>Pod disruption budgets</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>Kubernetes offers features to help you run highly available applications even when you
introduce frequent voluntary disruptions.</p><p>As an application owner, you can create a PodDisruptionBudget (PDB) for each application.
A PDB limits the number of Pods of a replicated application that are down simultaneously from
voluntary disruptions. For example, a quorum-based application would
like to ensure that the number of replicas running is never brought below the
number needed for a quorum. A web front end might want to
ensure that the number of replicas serving load never falls below a certain
percentage of the total.</p><p>Cluster managers and hosting providers should use tools which
respect PodDisruptionBudgets by calling the <a href=/docs/tasks/administer-cluster/safely-drain-node/#eviction-api>Eviction API</a>
instead of directly deleting pods or deployments.</p><p>For example, the <code>kubectl drain</code> subcommand lets you mark a node as going out of
service. When you run <code>kubectl drain</code>, the tool tries to evict all of the Pods on
the Node you're taking out of service. The eviction request that <code>kubectl</code> submits on
your behalf may be temporarily rejected, so the tool periodically retries all failed
requests until all Pods on the target node are terminated, or until a configurable timeout
is reached.</p><p>A PDB specifies the number of replicas that an application can tolerate having, relative to how
many it is intended to have. For example, a Deployment which has a <code>.spec.replicas: 5</code> is
supposed to have 5 pods at any given time. If its PDB allows for there to be 4 at a time,
then the Eviction API will allow voluntary disruption of one (but not two) pods at a time.</p><p>The group of pods that comprise the application is specified using a label selector, the same
as the one used by the application's controller (deployment, stateful-set, etc).</p><p>The "intended" number of pods is computed from the <code>.spec.replicas</code> of the workload resource
that is managing those pods. The control plane discovers the owning workload resource by
examining the <code>.metadata.ownerReferences</code> of the Pod.</p><p><a href=#voluntary-and-involuntary-disruptions>Involuntary disruptions</a> cannot be prevented by PDBs; however they
do count against the budget.</p><p>Pods which are deleted or unavailable due to a rolling upgrade to an application do count
against the disruption budget, but workload resources (such as Deployment and StatefulSet)
are not limited by PDBs when doing rolling upgrades. Instead, the handling of failures
during application updates is configured in the spec for the specific workload resource.</p><p>When a pod is evicted using the eviction API, it is gracefully
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>terminated</a>, honoring the
<code>terminationGracePeriodSeconds</code> setting in its <a href=/docs/reference/generated/kubernetes-api/v1.25/#podspec-v1-core>PodSpec</a>.</p><h2 id=pdb-example>PodDisruptionBudget example</h2><p>Consider a cluster with 3 nodes, <code>node-1</code> through <code>node-3</code>.
The cluster is running several applications. One of them has 3 replicas initially called
<code>pod-a</code>, <code>pod-b</code>, and <code>pod-c</code>. Another, unrelated pod without a PDB, called <code>pod-x</code>, is also shown.
Initially, the pods are laid out as follows:</p><table><thead><tr><th style=text-align:center>node-1</th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th></tr></thead><tbody><tr><td style=text-align:center>pod-a <em>available</em></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center>pod-x <em>available</em></td><td style=text-align:center></td><td style=text-align:center></td></tr></tbody></table><p>All 3 pods are part of a deployment, and they collectively have a PDB which requires
there be at least 2 of the 3 pods to be available at all times.</p><p>For example, assume the cluster administrator wants to reboot into a new kernel version to fix a bug in the kernel.
The cluster administrator first tries to drain <code>node-1</code> using the <code>kubectl drain</code> command.
That tool tries to evict <code>pod-a</code> and <code>pod-x</code>. This succeeds immediately.
Both pods go into the <code>terminating</code> state at the same time.
This puts the cluster in this state:</p><table><thead><tr><th style=text-align:center>node-1 <em>draining</em></th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th></tr></thead><tbody><tr><td style=text-align:center>pod-a <em>terminating</em></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center>pod-x <em>terminating</em></td><td style=text-align:center></td><td style=text-align:center></td></tr></tbody></table><p>The deployment notices that one of the pods is terminating, so it creates a replacement
called <code>pod-d</code>. Since <code>node-1</code> is cordoned, it lands on another node. Something has
also created <code>pod-y</code> as a replacement for <code>pod-x</code>.</p><p>(Note: for a StatefulSet, <code>pod-a</code>, which would be called something like <code>pod-0</code>, would need
to terminate completely before its replacement, which is also called <code>pod-0</code> but has a
different UID, could be created. Otherwise, the example applies to a StatefulSet as well.)</p><p>Now the cluster is in this state:</p><table><thead><tr><th style=text-align:center>node-1 <em>draining</em></th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th></tr></thead><tbody><tr><td style=text-align:center>pod-a <em>terminating</em></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center>pod-x <em>terminating</em></td><td style=text-align:center>pod-d <em>starting</em></td><td style=text-align:center>pod-y</td></tr></tbody></table><p>At some point, the pods terminate, and the cluster looks like this:</p><table><thead><tr><th style=text-align:center>node-1 <em>drained</em></th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center></td><td style=text-align:center>pod-d <em>starting</em></td><td style=text-align:center>pod-y</td></tr></tbody></table><p>At this point, if an impatient cluster administrator tries to drain <code>node-2</code> or
<code>node-3</code>, the drain command will block, because there are only 2 available
pods for the deployment, and its PDB requires at least 2. After some time passes, <code>pod-d</code> becomes available.</p><p>The cluster state now looks like this:</p><table><thead><tr><th style=text-align:center>node-1 <em>drained</em></th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>pod-b <em>available</em></td><td style=text-align:center>pod-c <em>available</em></td></tr><tr><td style=text-align:center></td><td style=text-align:center>pod-d <em>available</em></td><td style=text-align:center>pod-y</td></tr></tbody></table><p>Now, the cluster administrator tries to drain <code>node-2</code>.
The drain command will try to evict the two pods in some order, say
<code>pod-b</code> first and then <code>pod-d</code>. It will succeed at evicting <code>pod-b</code>.
But, when it tries to evict <code>pod-d</code>, it will be refused because that would leave only
one pod available for the deployment.</p><p>The deployment creates a replacement for <code>pod-b</code> called <code>pod-e</code>.
Because there are not enough resources in the cluster to schedule
<code>pod-e</code> the drain will again block. The cluster may end up in this
state:</p><table><thead><tr><th style=text-align:center>node-1 <em>drained</em></th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th><th style=text-align:center><em>no node</em></th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center>pod-b <em>terminating</em></td><td style=text-align:center>pod-c <em>available</em></td><td style=text-align:center>pod-e <em>pending</em></td></tr><tr><td style=text-align:center></td><td style=text-align:center>pod-d <em>available</em></td><td style=text-align:center>pod-y</td><td style=text-align:center></td></tr></tbody></table><p>At this point, the cluster administrator needs to
add a node back to the cluster to proceed with the upgrade.</p><p>You can see how Kubernetes varies the rate at which disruptions
can happen, according to:</p><ul><li>how many replicas an application needs</li><li>how long it takes to gracefully shutdown an instance</li><li>how long it takes a new instance to start up</li><li>the type of controller</li><li>the cluster's resource capacity</li></ul><h2 id=pod-disruption-conditions>Pod disruption conditions</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In order to use this behavior, you must enable the <code>PodDisruptionConditions</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
in your cluster.</div><p>When enabled, a dedicated Pod <code>DisruptionTarget</code> <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-conditions>condition</a> is added to indicate
that the Pod is about to be deleted due to a <a class=glossary-tooltip title='An event that leads to Pod(s) going out of service' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/disruptions/ target=_blank aria-label=disruption>disruption</a>.
The <code>reason</code> field of the condition additionally
indicates one of the following reasons for the Pod termination:</p><dl><dt><code>PreemptionByKubeScheduler</code></dt><dd>Pod is due to be <a class=glossary-tooltip title='Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption target=_blank aria-label=preempted>preempted</a> by a scheduler in order to accommodate a new Pod with a higher priority. For more information, see <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod priority preemption</a>.</dd><dt><code>DeletionByTaintManager</code></dt><dd>Pod is due to be deleted by Taint Manager (which is part of the node lifecycle controller within <code>kube-controller-manager</code>) due to a <code>NoExecute</code> taint that the Pod does not tolerate; see <a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=taint>taint</a>-based evictions.</dd><dt><code>EvictionByEvictionAPI</code></dt><dd>Pod has been marked for <a class=glossary-tooltip title='API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/api-eviction/ target=_blank aria-label='eviction using the Kubernetes API'>eviction using the Kubernetes API</a> .</dd><dt><code>DeletionByPodGC</code></dt><dd>Pod, that is bound to a no longer existing Node, is due to be deleted by <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection>Pod garbage collection</a>.</dd></dl><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A Pod disruption might be interrupted. The control plane might re-attempt to
continue the disruption of the same Pod, but it is not guaranteed. As a result,
the <code>DisruptionTarget</code> condition might be added to a Pod, but that Pod might then not actually be
deleted. In such a situation, after some time, the
Pod disruption condition will be cleared.</div><p>When using a Job (or CronJob), you may want to use these Pod disruption conditions as part of your Job's
<a href=/docs/concepts/workloads/controllers/job#pod-failure-policy>Pod failure policy</a>.</p><h2 id=separating-cluster-owner-and-application-owner-roles>Separating Cluster Owner and Application Owner Roles</h2><p>Often, it is useful to think of the Cluster Manager
and Application Owner as separate roles with limited knowledge
of each other. This separation of responsibilities
may make sense in these scenarios:</p><ul><li>when there are many application teams sharing a Kubernetes cluster, and
there is natural specialization of roles</li><li>when third-party tools or services are used to automate cluster management</li></ul><p>Pod Disruption Budgets support this separation of roles by providing an
interface between the roles.</p><p>If you do not have such a separation of responsibilities in your organization,
you may not need to use Pod Disruption Budgets.</p><h2 id=how-to-perform-disruptive-actions-on-your-cluster>How to perform Disruptive Actions on your Cluster</h2><p>If you are a Cluster Administrator, and you need to perform a disruptive action on all
the nodes in your cluster, such as a node or system software upgrade, here are some options:</p><ul><li>Accept downtime during the upgrade.</li><li>Failover to another complete replica cluster.<ul><li>No downtime, but may be costly both for the duplicated nodes
and for human effort to orchestrate the switchover.</li></ul></li><li>Write disruption tolerant applications and use PDBs.<ul><li>No downtime.</li><li>Minimal resource duplication.</li><li>Allows more automation of cluster administration.</li><li>Writing disruption-tolerant applications is tricky, but the work to tolerate voluntary
disruptions largely overlaps with work to support autoscaling and tolerating
involuntary disruptions.</li></ul></li></ul><h2 id=what-s-next>What's next</h2><ul><li><p>Follow steps to protect your application by <a href=/docs/tasks/run-application/configure-pdb/>configuring a Pod Disruption Budget</a>.</p></li><li><p>Learn more about <a href=/docs/tasks/administer-cluster/safely-drain-node/>draining nodes</a></p></li><li><p>Learn about <a href=/docs/concepts/workloads/controllers/deployment/#updating-a-deployment>updating a deployment</a>
including steps to maintain its availability during the rollout.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-53a1005011e1bda2ce81819aad7c8b32>5.1.4 - Ephemeral Containers</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>This page provides an overview of ephemeral containers: a special type of container
that runs temporarily in an existing <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> to
accomplish user-initiated actions such as troubleshooting. You use ephemeral
containers to inspect services rather than to build applications.</p><h2 id=understanding-ephemeral-containers>Understanding ephemeral containers</h2><p><a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> are the fundamental building
block of Kubernetes applications. Since Pods are intended to be disposable and
replaceable, you cannot add a container to a Pod once it has been created.
Instead, you usually delete and replace Pods in a controlled fashion using
<a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=deployments>deployments</a>.</p><p>Sometimes it's necessary to inspect the state of an existing Pod, however, for
example to troubleshoot a hard-to-reproduce bug. In these cases you can run
an ephemeral container in an existing Pod to inspect its state and run
arbitrary commands.</p><h3 id=what-is-an-ephemeral-container>What is an ephemeral container?</h3><p>Ephemeral containers differ from other containers in that they lack guarantees
for resources or execution, and they will never be automatically restarted, so
they are not appropriate for building applications. Ephemeral containers are
described using the same <code>ContainerSpec</code> as regular containers, but many fields
are incompatible and disallowed for ephemeral containers.</p><ul><li>Ephemeral containers may not have ports, so fields such as <code>ports</code>,
<code>livenessProbe</code>, <code>readinessProbe</code> are disallowed.</li><li>Pod resource allocations are immutable, so setting <code>resources</code> is disallowed.</li><li>For a complete list of allowed fields, see the <a href=/docs/reference/generated/kubernetes-api/v1.25/#ephemeralcontainer-v1-core>EphemeralContainer reference
documentation</a>.</li></ul><p>Ephemeral containers are created using a special <code>ephemeralcontainers</code> handler
in the API rather than by adding them directly to <code>pod.spec</code>, so it's not
possible to add an ephemeral container using <code>kubectl edit</code>.</p><p>Like regular containers, you may not change or remove an ephemeral container
after you have added it to a Pod.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Ephemeral containers are not supported by <a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a>.</div><h2 id=uses-for-ephemeral-containers>Uses for ephemeral containers</h2><p>Ephemeral containers are useful for interactive troubleshooting when <code>kubectl exec</code> is insufficient because a container has crashed or a container image
doesn't include debugging utilities.</p><p>In particular, <a href=https://github.com/GoogleContainerTools/distroless>distroless images</a>
enable you to deploy minimal container images that reduce attack surface
and exposure to bugs and vulnerabilities. Since distroless images do not include a
shell or any debugging utilities, it's difficult to troubleshoot distroless
images using <code>kubectl exec</code> alone.</p><p>When using ephemeral containers, it's helpful to enable <a href=/docs/tasks/configure-pod-container/share-process-namespace/>process namespace
sharing</a> so
you can view processes in other containers.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn how to <a href=/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container>debug pods using ephemeral containers</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-868be91dc02aab6dc768102e4abf5eff>5.1.5 - User Namespaces</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><p>This page explains how user namespaces are used in Kubernetes pods. A user
namespace allows to isolate the user running inside the container from the one
in the host.</p><p>A process running as root in a container can run as a different (non-root) user
in the host; in other words, the process has full privileges for operations
inside the user namespace, but is unprivileged for operations outside the
namespace.</p><p>You can use this feature to reduce the damage a compromised container can do to
the host or other pods in the same node. There are <a href=https://github.com/kubernetes/enhancements/tree/217d790720c5aef09b8bd4d6ca96284a0affe6c2/keps/sig-node/127-user-namespaces#motivation>several security
vulnerabilities</a> rated either <strong>HIGH</strong> or <strong>CRITICAL</strong> that were not
exploitable when user namespaces is active. It is expected user namespace will
mitigate some future vulnerabilities too.</p><h2 id=before-you-begin>Before you begin</h2><div class="alert alert-secondary callout third-party-content" role=alert>&#128711; This item links to a third party project or product that is not part of Kubernetes itself. <a class=alert-more-info href=#third-party-content-disclaimer>More information</a></div><p>This is a Linux only feature. In addition, support is needed in the
<a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>
to use this feature with Kubernetes stateless pods:</p><ul><li><p>CRI-O: v1.25 has support for user namespaces.</p></li><li><p>containerd: support is planned for the 1.7 release. See containerd
issue <a href=https://github.com/containerd/containerd/issues/7063>#7063</a> for more details.</p></li></ul><p>Support for this in <a href=https://github.com/Mirantis/cri-dockerd/issues/74>cri-dockerd is not planned</a> yet.</p><h2 id=introduction>Introduction</h2><p>User namespaces is a Linux feature that allows to map users in the container to
different users in the host. Furthermore, the capabilities granted to a pod in
a user namespace are valid only in the namespace and void outside of it.</p><p>A pod can opt-in to use user namespaces by setting the <code>pod.spec.hostUsers</code> field
to <code>false</code>.</p><p>The kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way
to guarantee that no two stateless pods on the same node use the same mapping.</p><p>The <code>runAsUser</code>, <code>runAsGroup</code>, <code>fsGroup</code>, etc. fields in the <code>pod.spec</code> always
refer to the user inside the container.</p><p>The valid UIDs/GIDs when this feature is enabled is the range 0-65535. This
applies to files and processes (<code>runAsUser</code>, <code>runAsGroup</code>, etc.).</p><p>Files using a UID/GID outside this range will be seen as belonging to the
overflow ID, usually 65534 (configured in <code>/proc/sys/kernel/overflowuid</code> and
<code>/proc/sys/kernel/overflowgid</code>). However, it is not possible to modify those
files, even by running as the 65534 user/group.</p><p>Most applications that need to run as root but don't access other host
namespaces or resources, should continue to run fine without any changes needed
if user namespaces is activated.</p><h2 id=understanding-user-namespaces-for-stateless-pods>Understanding user namespaces for stateless pods</h2><p>Several container runtimes with their default configuration (like Docker Engine,
containerd, CRI-O) use Linux namespaces for isolation. Other technologies exist
and can be used with those runtimes too (e.g. Kata Containers uses VMs instead of
Linux namespaces). This page is applicable for container runtimes using Linux
namespaces for isolation.</p><p>When creating a pod, by default, several new namespaces are used for isolation:
a network namespace to isolate the network of the container, a PID namespace to
isolate the view of processes, etc. If a user namespace is used, this will
isolate the users in the container from the users in the node.</p><p>This means containers can run as root and be mapped to a non-root user on the
host. Inside the container the process will think it is running as root (and
therefore tools like <code>apt</code>, <code>yum</code>, etc. work fine), while in reality the process
doesn't have privileges on the host. You can verify this, for example, if you
check which user the container process is running by executing <code>ps aux</code> from
the host. The user <code>ps</code> shows is not the same as the user you see if you
execute inside the container the command <code>id</code>.</p><p>This abstraction limits what can happen, for example, if the container manages
to escape to the host. Given that the container is running as a non-privileged
user on the host, it is limited what it can do to the host.</p><p>Furthermore, as users on each pod will be mapped to different non-overlapping
users in the host, it is limited what they can do to other pods too.</p><p>Capabilities granted to a pod are also limited to the pod user namespace and
mostly invalid out of it, some are even completely void. Here are two examples:</p><ul><li><code>CAP_SYS_MODULE</code> does not have any effect if granted to a pod using user
namespaces, the pod isn't able to load kernel modules.</li><li><code>CAP_SYS_ADMIN</code> is limited to the pod's user namespace and invalid outside
of it.</li></ul><p>Without using a user namespace a container running as root, in the case of a
container breakout, has root privileges on the node. And if some capability were
granted to the container, the capabilities are valid on the host too. None of
this is true when we use user namespaces.</p><p>If you want to know more details about what changes when user namespaces are in
use, see <code>man 7 user_namespaces</code>.</p><h2 id=set-up-a-node-to-support-user-namespaces>Set up a node to support user namespaces</h2><p>It is recommended that the host's files and host's processes use UIDs/GIDs in
the range of 0-65535.</p><p>The kubelet will assign UIDs/GIDs higher than that to pods. Therefore, to
guarantee as much isolation as possible, the UIDs/GIDs used by the host's files
and host's processes should be in the range 0-65535.</p><p>Note that this recommendation is important to mitigate the impact of CVEs like
<a href=https://github.com/kubernetes/kubernetes/issues/104980>CVE-2021-25741</a>, where a pod can potentially read arbitrary
files in the hosts. If the UIDs/GIDs of the pod and the host don't overlap, it
is limited what a pod would be able to do: the pod UID/GID won't match the
host's file owner/group.</p><h2 id=limitations>Limitations</h2><p>When using a user namespace for the pod, it is disallowed to use other host
namespaces. In particular, if you set <code>hostUsers: false</code> then you are not
allowed to set any of:</p><ul><li><code>hostNetwork: true</code></li><li><code>hostIPC: true</code></li><li><code>hostPID: true</code></li></ul><p>The pod is allowed to use no volumes at all or, if using volumes, only these
volume types are allowed:</p><ul><li>configmap</li><li>secret</li><li>projected</li><li>downwardAPI</li><li>emptyDir</li></ul><p>To guarantee that the pod can read the files of such volumes, volumes are
created as if you specified <code>.spec.securityContext.fsGroup</code> as <code>0</code> for the Pod.
If it is specified to a different value, this other value will of course be
honored instead.</p><p>As a by-product of this, folders and files for these volumes will have
permissions for the group, even if <code>defaultMode</code> or <code>mode</code> to specific items of
the volumes were specified without permissions to groups. For example, it is not
possible to mount these volumes in a way that its files have permissions only
for the owner.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-420713565efe2f940e277f6b4824ad9a>5.1.6 - Downward API</h1><div class=lead>There are two ways to expose Pod and container fields to a running container: environment variables, and as files that are populated by a special volume type. Together, these two ways of exposing Pod and container fields are called the downward API.</div><p>It is sometimes useful for a container to have information about itself, without
being overly coupled to Kubernetes. The <em>downward API</em> allows containers to consume
information about themselves or the cluster without using the Kubernetes client
or API server.</p><p>An example is an existing application that assumes a particular well-known
environment variable holds a unique identifier. One possibility is to wrap the
application, but that is tedious and error-prone, and it violates the goal of low
coupling. A better option would be to use the Pod's name as an identifier, and
inject the Pod's name into the well-known environment variable.</p><p>In Kubernetes, there are two ways to expose Pod and container fields to a running container:</p><ul><li>as <a href=/docs/tasks/inject-data-application/environment-variable-expose-pod-information/>environment variables</a></li><li>as <a href=/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/>files in a <code>downwardAPI</code> volume</a></li></ul><p>Together, these two ways of exposing Pod and container fields are called the
<em>downward API</em>.</p><h2 id=available-fields>Available fields</h2><p>Only some Kubernetes API fields are available through the downward API. This
section lists which fields you can make available.</p><p>You can pass information from available Pod-level fields using <code>fieldRef</code>.
At the API level, the <code>spec</code> for a Pod always defines at least one
<a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container>Container</a>.
You can pass information from available Container-level fields using
<code>resourceFieldRef</code>.</p><h3 id=downwardapi-fieldRef>Information available via <code>fieldRef</code></h3><p>For most Pod-level fields, you can provide them to a container either as
an environment variable or using a <code>downwardAPI</code> volume. The fields available
via either mechanism are:</p><dl><dt><code>metadata.name</code></dt><dd>the pod's name</dd><dt><code>metadata.namespace</code></dt><dd>the pod's <a class=glossary-tooltip title='An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a></dd><dt><code>metadata.uid</code></dt><dd>the pod's unique ID</dd><dt><code>metadata.annotations['&lt;KEY>']</code></dt><dd>the value of the pod's <a class=glossary-tooltip title='A key-value pair that is used to attach arbitrary non-identifying metadata to objects.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/annotations target=_blank aria-label=annotation>annotation</a> named <code>&lt;KEY></code> (for example, <code>metadata.annotations['myannotation']</code>)</dd><dt><code>metadata.labels['&lt;KEY>']</code></dt><dd>the text value of the pod's <a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=label>label</a> named <code>&lt;KEY></code> (for example, <code>metadata.labels['mylabel']</code>)</dd><dt><code>spec.serviceAccountName</code></dt><dd>the name of the pod's <a class=glossary-tooltip title='Provides an identity for processes that run in a Pod.' data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/configure-service-account/ target=_blank aria-label='service account'>service account</a></dd><dt><code>spec.nodeName</code></dt><dd>the name of the <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> where the Pod is executing</dd><dt><code>status.hostIP</code></dt><dd>the primary IP address of the node to which the Pod is assigned</dd><dt><code>status.podIP</code></dt><dd>the pod's primary IP address (usually, its IPv4 address)</dd></dl><p>In addition, the following information is available through
a <code>downwardAPI</code> volume <code>fieldRef</code>, but <strong>not as environment variables</strong>:</p><dl><dt><code>metadata.labels</code></dt><dd>all of the pod's labels, formatted as <code>label-key="escaped-label-value"</code> with one label per line</dd><dt><code>metadata.annotations</code></dt><dd>all of the pod's annotations, formatted as <code>annotation-key="escaped-annotation-value"</code> with one annotation per line</dd></dl><h3 id=downwardapi-resourceFieldRef>Information available via <code>resourceFieldRef</code></h3><p>These container-level fields allow you to provide information about
<a href=/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>requests and limits</a>
for resources such as CPU and memory.</p><dl><dt><code>resource: limits.cpu</code></dt><dd>A container's CPU limit</dd><dt><code>resource: requests.cpu</code></dt><dd>A container's CPU request</dd><dt><code>resource: limits.memory</code></dt><dd>A container's memory limit</dd><dt><code>resource: requests.memory</code></dt><dd>A container's memory request</dd><dt><code>resource: limits.hugepages-*</code></dt><dd>A container's hugepages limit (provided that the <code>DownwardAPIHugePages</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> is enabled)</dd><dt><code>resource: requests.hugepages-*</code></dt><dd>A container's hugepages request (provided that the <code>DownwardAPIHugePages</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> is enabled)</dd><dt><code>resource: limits.ephemeral-storage</code></dt><dd>A container's ephemeral-storage limit</dd><dt><code>resource: requests.ephemeral-storage</code></dt><dd>A container's ephemeral-storage request</dd></dl><h4 id=fallback-information-for-resource-limits>Fallback information for resource limits</h4><p>If CPU and memory limits are not specified for a container, and you use the
downward API to try to expose that information, then the
kubelet defaults to exposing the maximum allocatable value for CPU and memory
based on the <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>node allocatable</a>
calculation.</p><h2 id=what-s-next>What's next</h2><p>You can read about <a href=/docs/concepts/storage/volumes/#downwardapi><code>downwardAPI</code> volumes</a>.</p><p>You can try using the downward API to expose container- or Pod-level information:</p><ul><li>as <a href=/docs/tasks/inject-data-application/environment-variable-expose-pod-information/>environment variables</a></li><li>as <a href=/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/>files in <code>downwardAPI</code> volume</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-89637410cacae45a36ab1cc278c482eb>5.2 - Workload Resources</h1></div><div class=td-content><h1 id=pg-a2dc0393e0c4079e1c504b6429844e86>5.2.1 - Deployments</h1><p>A <em>Deployment</em> provides declarative updates for <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> and
<a class=glossary-tooltip title='ReplicaSet ensures that a specified number of Pod replicas are running at one time' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/replicaset/ target=_blank aria-label=ReplicaSets>ReplicaSets</a>.</p><p>You describe a <em>desired state</em> in a Deployment, and the Deployment <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a> changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.</div><h2 id=use-case>Use Case</h2><p>The following are typical use cases for Deployments:</p><ul><li><a href=#creating-a-deployment>Create a Deployment to rollout a ReplicaSet</a>. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.</li><li><a href=#updating-a-deployment>Declare the new state of the Pods</a> by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.</li><li><a href=#rolling-back-a-deployment>Rollback to an earlier Deployment revision</a> if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.</li><li><a href=#scaling-a-deployment>Scale up the Deployment to facilitate more load</a>.</li><li><a href=#pausing-and-resuming-a-deployment>Pause the rollout of a Deployment</a> to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.</li><li><a href=#deployment-status>Use the status of the Deployment</a> as an indicator that a rollout has stuck.</li><li><a href=#clean-up-policy>Clean up older ReplicaSets</a> that you don't need anymore.</li></ul><h2 id=creating-a-deployment>Creating a Deployment</h2><p>The following is an example of a Deployment. It creates a ReplicaSet to bring up three <code>nginx</code> Pods:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/nginx-deployment.yaml download=controllers/nginx-deployment.yaml><code>controllers/nginx-deployment.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-nginx-deployment-yaml")' title="Copy controllers/nginx-deployment.yaml to clipboard"></img></div><div class=includecode id=controllers-nginx-deployment-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>In this example:</p><ul><li><p>A Deployment named <code>nginx-deployment</code> is created, indicated by the <code>.metadata.name</code> field.</p></li><li><p>The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the <code>.spec.replicas</code> field.</p></li><li><p>The <code>.spec.selector</code> field defines how the created ReplicaSet finds which Pods to manage.
In this case, you select a label that is defined in the Pod template (<code>app: nginx</code>).
However, more sophisticated selection rules are possible,
as long as the Pod template itself satisfies the rule.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>.spec.selector.matchLabels</code> field is a map of {key,value} pairs.
A single {key,value} in the <code>matchLabels</code> map is equivalent to an element of <code>matchExpressions</code>,
whose <code>key</code> field is "key", the <code>operator</code> is "In", and the <code>values</code> array contains only "value".
All of the requirements, from both <code>matchLabels</code> and <code>matchExpressions</code>, must be satisfied in order to match.</div></li><li><p>The <code>template</code> field contains the following sub-fields:</p><ul><li>The Pods are labeled <code>app: nginx</code>using the <code>.metadata.labels</code> field.</li><li>The Pod template's specification, or <code>.template.spec</code> field, indicates that
the Pods run one container, <code>nginx</code>, which runs the <code>nginx</code>
<a href=https://hub.docker.com/>Docker Hub</a> image at version 1.14.2.</li><li>Create one container and name it <code>nginx</code> using the <code>.spec.template.spec.containers[0].name</code> field.</li></ul></li></ul><p>Before you begin, make sure your Kubernetes cluster is up and running.
Follow the steps given below to create the above Deployment:</p><ol><li><p>Create the Deployment by running the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
</span></span></code></pre></div></li><li><p>Run <code>kubectl get deployments</code> to check if the Deployment was created.</p><p>If the Deployment is still being created, the output is similar to the following:</p><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s
</code></pre><p>When you inspect the Deployments in your cluster, the following fields are displayed:</p><ul><li><code>NAME</code> lists the names of the Deployments in the namespace.</li><li><code>READY</code> displays how many replicas of the application are available to your users. It follows the pattern ready/desired.</li><li><code>UP-TO-DATE</code> displays the number of replicas that have been updated to achieve the desired state.</li><li><code>AVAILABLE</code> displays how many replicas of the application are available to your users.</li><li><code>AGE</code> displays the amount of time that the application has been running.</li></ul><p>Notice how the number of desired replicas is 3 according to <code>.spec.replicas</code> field.</p></li><li><p>To see the Deployment rollout status, run <code>kubectl rollout status deployment/nginx-deployment</code>.</p><p>The output is similar to:</p><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment &#34;nginx-deployment&#34; successfully rolled out
</code></pre></li><li><p>Run the <code>kubectl get deployments</code> again a few seconds later.
The output is similar to this:</p><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s
</code></pre><p>Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.</p></li><li><p>To see the ReplicaSet (<code>rs</code>) created by the Deployment, run <code>kubectl get rs</code>. The output is similar to this:</p><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s
</code></pre><p>ReplicaSet output shows the following fields:</p><ul><li><code>NAME</code> lists the names of the ReplicaSets in the namespace.</li><li><code>DESIRED</code> displays the desired number of <em>replicas</em> of the application, which you define when you create the Deployment. This is the <em>desired state</em>.</li><li><code>CURRENT</code> displays how many replicas are currently running.</li><li><code>READY</code> displays how many replicas of the application are available to your users.</li><li><code>AGE</code> displays the amount of time that the application has been running.</li></ul><p>Notice that the name of the ReplicaSet is always formatted as <code>[DEPLOYMENT-NAME]-[HASH]</code>.
The <code>HASH</code> string is the same as the <code>pod-template-hash</code> label on the ReplicaSet.</p></li><li><p>To see the labels automatically generated for each Pod, run <code>kubectl get pods --show-labels</code>.
The output is similar to:</p><pre tabindex=0><code>NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
</code></pre><p>The created ReplicaSet ensures that there are three <code>nginx</code> Pods.</p></li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>You must specify an appropriate selector and Pod template labels in a Deployment
(in this case, <code>app: nginx</code>).</p><p>Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.</p></div><h3 id=pod-template-hash-label>Pod-template-hash label</h3><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Do not change this label.</div><p>The <code>pod-template-hash</code> label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.</p><p>This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the <code>PodTemplate</code> of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,
and in any existing Pods that the ReplicaSet might have.</p><h2 id=updating-a-deployment>Updating a Deployment</h2><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, <code>.spec.template</code>)
is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.</div><p>Follow the steps given below to update your Deployment:</p><ol><li><p>Let's update the nginx Pods to use the <code>nginx:1.16.1</code> image instead of the <code>nginx:1.14.2</code> image.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.16.1
</span></span></code></pre></div><p>or use the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.16.1
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre><p>Alternatively, you can <code>edit</code> the Deployment and change <code>.spec.template.spec.containers[0].image</code> from <code>nginx:1.14.2</code> to <code>nginx:1.16.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>deployment.apps/nginx-deployment edited
</code></pre></li><li><p>To see the rollout status, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
</code></pre><p>or</p><pre tabindex=0><code>deployment &#34;nginx-deployment&#34; successfully rolled out
</code></pre></li></ol><p>Get more details on your updated Deployment:</p><ul><li><p>After the rollout succeeds, you can view the Deployment by running <code>kubectl get deployments</code>.
The output is similar to this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#b44>NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span>
</span></span><span style=display:flex><span><span style=color:#b44>nginx-deployment   3/3     3            3           36s</span>
</span></span></code></pre></div></li><li><p>Run <code>kubectl get rs</code> to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it
up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
</code></pre></li><li><p>Running <code>get pods</code> should now show only the new Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</code></pre><p>Next time you want to update these Pods, you only need to update the Deployment's Pod template again.</p><p>Deployment ensures that only a certain number of Pods are down while they are being updated. By default,
it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).</p><p>Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.
By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).</p><p>For example, if you look at the above Deployment closely, you will see that it first creates a new Pod,
then deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of
new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.
It makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of
a Deployment with 4 replicas, the number of Pods would be between 3 and 5.</p></li><li><p>Get details of your Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployments
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
   Containers:
    nginx:
      Image:        nginx:1.16.1
      Port:         80/TCP
      Environment:  &lt;none&gt;
      Mounts:       &lt;none&gt;
    Volumes:        &lt;none&gt;
  Conditions:
    Type           Status  Reason
    ----           ------  ------
    Available      True    MinimumReplicasAvailable
    Progressing    True    NewReplicaSetAvailable
  OldReplicaSets:  &lt;none&gt;
  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
  Events:
    Type    Reason             Age   From                   Message
    ----    ------             ----  ----                   -------
    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
</code></pre><p>Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)
and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet
(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet
to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.
It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.
Finally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.</p></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes doesn't count terminating Pods when calculating the number of <code>availableReplicas</code>, which must be between
<code>replicas - maxUnavailable</code> and <code>replicas + maxSurge</code>. As a result, you might notice that there are more Pods than
expected during a rollout, and that the total resources consumed by the Deployment is more than <code>replicas + maxSurge</code>
until the <code>terminationGracePeriodSeconds</code> of the terminating Pods expires.</div><h3 id=rollover-aka-multiple-updates-in-flight>Rollover (aka multiple updates in-flight)</h3><p>Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up
the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels
match <code>.spec.selector</code> but whose template does not match <code>.spec.template</code> are scaled down. Eventually, the new
ReplicaSet is scaled to <code>.spec.replicas</code> and all old ReplicaSets is scaled to 0.</p><p>If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet
as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously
-- it will add it to its list of old ReplicaSets and start scaling it down.</p><p>For example, suppose you create a Deployment to create 5 replicas of <code>nginx:1.14.2</code>,
but then update the Deployment to create 5 replicas of <code>nginx:1.16.1</code>, when only 3
replicas of <code>nginx:1.14.2</code> had been created. In that case, the Deployment immediately starts
killing the 3 <code>nginx:1.14.2</code> Pods that it had created, and starts creating
<code>nginx:1.16.1</code> Pods. It does not wait for the 5 replicas of <code>nginx:1.14.2</code> to be created
before changing course.</p><h3 id=label-selector-updates>Label selector updates</h3><p>It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.
In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped
all of the implications.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In API version <code>apps/v1</code>, a Deployment's label selector is immutable after it gets created.</div><ul><li>Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,
otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does
not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and
creating a new ReplicaSet.</li><li>Selector updates changes the existing value in a selector key -- result in the same behavior as additions.</li><li>Selector removals removes an existing key from the Deployment selector -- do not require any changes in the
Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the
removed label still exists in any existing Pods and ReplicaSets.</li></ul><h2 id=rolling-back-a-deployment>Rolling Back a Deployment</h2><p>Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.
By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want
(you can change that by modifying revision history limit).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A Deployment's revision is created when a Deployment's rollout is triggered. This means that the
new revision is created if and only if the Deployment's Pod template (<code>.spec.template</code>) is changed,
for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,
do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.
This means that when you roll back to an earlier revision, only the Deployment's Pod template part is
rolled back.</div><ul><li><p>Suppose that you made a typo while updating the Deployment, by putting the image name as <code>nginx:1.161</code> instead of <code>nginx:1.16.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.161 
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>The rollout gets stuck. You can verify it by checking the rollout status:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
</code></pre></li><li><p>Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,
<a href=#deployment-status>read more here</a>.</p></li><li><p>You see that the number of old replicas (<code>nginx-deployment-1564180365</code> and <code>nginx-deployment-2035384211</code>) is 2, and new replicas (nginx-deployment-3066724191) is 1.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s
</code></pre></li><li><p>Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
</code></pre><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (<code>maxUnavailable</code> specifically) that you have specified. Kubernetes by default sets the value to 25%.</div></li><li><p>Get the description of the Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.161
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
</code></pre><p>To fix this, you need to rollback to a previous revision of Deployment that is stable.</p></li></ul><h3 id=checking-rollout-history-of-a-deployment>Checking Rollout History of a Deployment</h3><p>Follow the steps given below to check the rollout history:</p><ol><li><p>First, check the revisions of this Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployments &#34;nginx-deployment&#34;
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml
2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161
</code></pre><p><code>CHANGE-CAUSE</code> is copied from the Deployment annotation <code>kubernetes.io/change-cause</code> to its revisions upon creation. You can specify the<code>CHANGE-CAUSE</code> message by:</p><ul><li>Annotating the Deployment with <code>kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.16.1"</code></li><li>Manually editing the manifest of the resource.</li></ul></li><li><p>To see the details of each revision, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment/nginx-deployment --revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployments &#34;nginx-deployment&#34; revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
  Containers:
   nginx:
    Image:      nginx:1.16.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &lt;none&gt;
  No volumes.
</code></pre></li></ol><h3 id=rolling-back-to-a-previous-revision>Rolling Back to a Previous Revision</h3><p>Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.</p><ol><li><p>Now you've decided to undo the current rollout and rollback to the previous revision:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment rolled back
</code></pre><p>Alternatively, you can rollback to a specific revision by specifying it with <code>--to-revision</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment/nginx-deployment --to-revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment rolled back
</code></pre><p>For more details about rollout related commands, read <a href=/docs/reference/generated/kubectl/kubectl-commands#rollout><code>kubectl rollout</code></a>.</p><p>The Deployment is now rolled back to a previous stable revision. As you can see, a <code>DeploymentRollback</code> event
for rolling back to revision 2 is generated from Deployment controller.</p></li><li><p>Check if the rollback was successful and the Deployment is running as expected, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m
</code></pre></li><li><p>Get the description of the Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.16.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &#34;nginx-deployment&#34; to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
</code></pre></li></ol><h2 id=scaling-a-deployment>Scaling a Deployment</h2><p>You can scale a Deployment by using the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment/nginx-deployment --replicas<span style=color:#666>=</span><span style=color:#666>10</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment scaled
</code></pre><p>Assuming <a href=/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/>horizontal Pod autoscaling</a> is enabled
in your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of
Pods you want to run based on the CPU utilization of your existing Pods.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale deployment/nginx-deployment --min<span style=color:#666>=</span><span style=color:#666>10</span> --max<span style=color:#666>=</span><span style=color:#666>15</span> --cpu-percent<span style=color:#666>=</span><span style=color:#666>80</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment scaled
</code></pre><h3 id=proportional-scaling>Proportional scaling</h3><p>RollingUpdate Deployments support running multiple versions of an application at the same time. When you
or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress
or paused), the Deployment controller balances the additional replicas in the existing active
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called <em>proportional scaling</em>.</p><p>For example, you are running a Deployment with 10 replicas, <a href=#max-surge>maxSurge</a>=3, and <a href=#max-unavailable>maxUnavailable</a>=2.</p><ul><li><p>Ensure that the 10 replicas in your Deployment are running.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s
</code></pre></li><li><p>You update to a new image which happens to be unresolvable from inside the cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:sometag
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the
<code>maxUnavailable</code> requirement that you mentioned above. Check out the rollout status:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre><code>The output is similar to this:
</code></pre><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
</code></pre></li><li><p>Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas
to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using
proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you
spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the
most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.</p></li></ul><p>In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the
new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming
the new replicas become healthy. To confirm this, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
</code></pre><p>The rollout status confirms how the replicas were added to each ReplicaSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
</code></pre><h2 id=pausing-and-resuming-a-deployment>Pausing and Resuming a rollout of a Deployment</h2><p>When you update a Deployment, or plan to, you can pause rollouts
for that Deployment before you trigger one or more updates. When
you're ready to apply those changes, you resume rollouts for the
Deployment. This approach allows you to
apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.</p><ul><li><p>For example, with a Deployment that was created:</p><p>Get the Deployment details:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m
</code></pre><p>Get the rollout status:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
</code></pre></li><li><p>Pause by running the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout pause deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment paused
</code></pre></li><li><p>Then update the image of the Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.16.1
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>Notice that no new rollout started:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployments &#34;nginx&#34;
REVISION  CHANGE-CAUSE
1   &lt;none&gt;
</code></pre></li><li><p>Get the rollout status to verify that the existing ReplicaSet has not changed:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m
</code></pre></li><li><p>You can make as many updates as you wish, for example, update the resources that will be used:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> resources deployment/nginx-deployment -c<span style=color:#666>=</span>nginx --limits<span style=color:#666>=</span><span style=color:#b8860b>cpu</span><span style=color:#666>=</span>200m,memory<span style=color:#666>=</span>512Mi
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment resource requirements updated
</code></pre><p>The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to
the Deployment will not have any effect as long as the Deployment rollout is paused.</p></li><li><p>Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout resume deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment resumed
</code></pre></li><li><p>Watch the status of the rollout until it's done.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs -w
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
</code></pre></li><li><p>Get the status of the latest rollout:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
</code></pre></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You cannot rollback a paused Deployment until you resume it.</div><h2 id=deployment-status>Deployment status</h2><p>A Deployment enters various states during its lifecycle. It can be <a href=#progressing-deployment>progressing</a> while
rolling out a new ReplicaSet, it can be <a href=#complete-deployment>complete</a>, or it can <a href=#failed-deployment>fail to progress</a>.</p><h3 id=progressing-deployment>Progressing Deployment</h3><p>Kubernetes marks a Deployment as <em>progressing</em> when one of the following tasks is performed:</p><ul><li>The Deployment creates a new ReplicaSet.</li><li>The Deployment is scaling up its newest ReplicaSet.</li><li>The Deployment is scaling down its older ReplicaSet(s).</li><li>New Pods become ready or available (ready for at least <a href=#min-ready-seconds>MinReadySeconds</a>).</li></ul><p>When the rollout becomes “progressing”, the Deployment controller adds a condition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "True"</code></li><li><code>reason: NewReplicaSetCreated</code> | <code>reason: FoundNewReplicaSet</code> | <code>reason: ReplicaSetUpdated</code></li></ul><p>You can monitor the progress for a Deployment by using <code>kubectl rollout status</code>.</p><h3 id=complete-deployment>Complete Deployment</h3><p>Kubernetes marks a Deployment as <em>complete</em> when it has the following characteristics:</p><ul><li>All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any
updates you've requested have been completed.</li><li>All of the replicas associated with the Deployment are available.</li><li>No old replicas for the Deployment are running.</li></ul><p>When the rollout becomes “complete”, the Deployment controller sets a condition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "True"</code></li><li><code>reason: NewReplicaSetAvailable</code></li></ul><p>This <code>Progressing</code> condition will retain a status value of <code>"True"</code> until a new rollout
is initiated. The condition holds even when availability of replicas changes (which
does instead affect the <code>Available</code> condition).</p><p>You can check if a Deployment has completed by using <code>kubectl rollout status</code>. If the rollout completed
successfully, <code>kubectl rollout status</code> returns a zero exit code.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment &#34;nginx-deployment&#34; successfully rolled out
</code></pre><p>and the exit status from <code>kubectl rollout</code> is 0 (success):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$?</span>
</span></span></code></pre></div><pre tabindex=0><code>0
</code></pre><h3 id=failed-deployment>Failed Deployment</h3><p>Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur
due to some of the following factors:</p><ul><li>Insufficient quota</li><li>Readiness probe failures</li><li>Image pull errors</li><li>Insufficient permissions</li><li>Limit ranges</li><li>Application runtime misconfiguration</li></ul><p>One way you can detect this condition is to specify a deadline parameter in your Deployment spec:
(<a href=#progress-deadline-seconds><code>.spec.progressDeadlineSeconds</code></a>). <code>.spec.progressDeadlineSeconds</code> denotes the
number of seconds the Deployment controller waits before indicating (in the Deployment status) that the
Deployment progress has stalled.</p><p>The following <code>kubectl</code> command sets the spec with <code>progressDeadlineSeconds</code> to make the controller report
lack of progress of a rollout for a Deployment after 10 minutes:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl patch deployment/nginx-deployment -p <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;progressDeadlineSeconds&#34;:600}}&#39;</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment patched
</code></pre><p>Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "False"</code></li><li><code>reason: ProgressDeadlineExceeded</code></li></ul><p>This condition can also fail early and is then set to status value of <code>"False"</code> due to reasons as <code>ReplicaSetCreateError</code>.
Also, the deadline is not taken into account anymore once the Deployment rollout completes.</p><p>See the <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties>Kubernetes API conventions</a> for more information on status conditions.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes takes no action on a stalled Deployment other than to report a status condition with
<code>reason: ProgressDeadlineExceeded</code>. Higher level orchestrators can take advantage of it and act accordingly, for
example, rollback the Deployment to its previous version.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.
You can safely pause a Deployment rollout in the middle of a rollout and resume without triggering
the condition for exceeding the deadline.</div><p>You may experience transient errors with your Deployments, either due to a low timeout that you have set or
due to any other kind of error that can be treated as transient. For example, let's suppose you have
insufficient quota. If you describe the Deployment you will notice the following section:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
</code></pre><p>If you run <code>kubectl get deployment nginx-deployment -o yaml</code>, the Deployment status is similar to this:</p><pre tabindex=0><code>status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &#34;nginx-deployment-4262182780&#34; is progressing.
    reason: ReplicaSetUpdated
    status: &#34;True&#34;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &#34;True&#34;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: &#39;Error creating: pods &#34;nginx-deployment-4262182780-&#34; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2&#39;
    reason: FailedCreate
    status: &#34;True&#34;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
</code></pre><p>Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the
reason for the Progressing condition:</p><pre tabindex=0><code>Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
</code></pre><p>You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other
controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota
conditions and the Deployment controller then completes the Deployment rollout, you'll see the
Deployment's status update with a successful condition (<code>status: "True"</code> and <code>reason: NewReplicaSetAvailable</code>).</p><pre tabindex=0><code>Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
</code></pre><p><code>type: Available</code> with <code>status: "True"</code> means that your Deployment has minimum availability. Minimum availability is dictated
by the parameters specified in the deployment strategy. <code>type: Progressing</code> with <code>status: "True"</code> means that your Deployment
is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum
required new replicas are available (see the Reason of the condition for the particulars - in our case
<code>reason: NewReplicaSetAvailable</code> means that the Deployment is complete).</p><p>You can check if a Deployment has failed to progress by using <code>kubectl rollout status</code>. <code>kubectl rollout status</code>
returns a non-zero exit code if the Deployment has exceeded the progression deadline.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &#34;nginx&#34; exceeded its progress deadline
</code></pre><p>and the exit status from <code>kubectl rollout</code> is 1 (indicating an error):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$?</span>
</span></span></code></pre></div><pre tabindex=0><code>1
</code></pre><h3 id=operating-on-a-failed-deployment>Operating on a failed deployment</h3><p>All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back
to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.</p><h2 id=clean-up-policy>Clean up Policy</h2><p>You can set <code>.spec.revisionHistoryLimit</code> field in a Deployment to specify how many old ReplicaSets for
this Deployment you want to retain. The rest will be garbage-collected in the background. By default,
it is 10.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment
thus that Deployment will not be able to roll back.</div><h2 id=canary-deployment>Canary Deployment</h2><p>If you want to roll out releases to a subset of users or servers using the Deployment, you
can create multiple Deployments, one for each release, following the canary pattern described in
<a href=/docs/concepts/cluster-administration/manage-deployment/#canary-deployments>managing resources</a>.</p><h2 id=writing-a-deployment-spec>Writing a Deployment Spec</h2><p>As with all other Kubernetes configs, a Deployment needs <code>.apiVersion</code>, <code>.kind</code>, and <code>.metadata</code> fields.
For general information about working with config files, see
<a href=/docs/tasks/run-application/run-stateless-application-deployment/>deploying applications</a>,
configuring containers, and <a href=/docs/concepts/overview/working-with-objects/object-management/>using kubectl to manage resources</a> documents.
The name of a Deployment object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>A Deployment also needs a <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> and <code>.spec.selector</code> are the only required fields of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>Pod template</a>. It has exactly the same schema as a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href=#selector>selector</a>.</p><p>Only a <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>.spec.template.spec.restartPolicy</code></a> equal to <code>Always</code> is
allowed, which is the default if not specified.</p><h3 id=replicas>Replicas</h3><p><code>.spec.replicas</code> is an optional field that specifies the number of desired Pods. It defaults to 1.</p><p>Should you manually scale a Deployment, example via <code>kubectl scale deployment deployment --replicas=X</code>, and then you update that Deployment based on a manifest
(for example: by running <code>kubectl apply -f deployment.yaml</code>),
then applying that manifest overwrites the manual scaling that you previously did.</p><p>If a <a href=/docs/tasks/run-application/horizontal-pod-autoscale/>HorizontalPodAutoscaler</a> (or any
similar API for horizontal scaling) is managing scaling for a Deployment, don't set <code>.spec.replicas</code>.</p><p>Instead, allow the Kubernetes
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> to manage the
<code>.spec.replicas</code> field automatically.</p><h3 id=selector>Selector</h3><p><code>.spec.selector</code> is a required field that specifies a <a href=/docs/concepts/overview/working-with-objects/labels/>label selector</a>
for the Pods targeted by this Deployment.</p><p><code>.spec.selector</code> must match <code>.spec.template.metadata.labels</code>, or it will be rejected by the API.</p><p>In API version <code>apps/v1</code>, <code>.spec.selector</code> and <code>.metadata.labels</code> do not default to <code>.spec.template.metadata.labels</code> if not set. So they must be set explicitly. Also note that <code>.spec.selector</code> is immutable after creation of the Deployment in <code>apps/v1</code>.</p><p>A Deployment may terminate Pods whose labels match the selector if their template is different
from <code>.spec.template</code> or if the total number of such Pods exceeds <code>.spec.replicas</code>. It brings up new
Pods with <code>.spec.template</code> if the number of Pods is less than the desired number.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You should not create other Pods whose labels match this selector, either directly, by creating
another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you
do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.</div><p>If you have multiple controllers that have overlapping selectors, the controllers will fight with each
other and won't behave correctly.</p><h3 id=strategy>Strategy</h3><p><code>.spec.strategy</code> specifies the strategy used to replace old Pods by new ones.
<code>.spec.strategy.type</code> can be "Recreate" or "RollingUpdate". "RollingUpdate" is
the default value.</p><h4 id=recreate-deployment>Recreate Deployment</h4><p>All existing Pods are killed before new ones are created when <code>.spec.strategy.type==Recreate</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods
of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new
revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the
replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an
"at most" guarantee for your Pods, you should consider using a
<a href=/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>.</div><h4 id=rolling-update-deployment>Rolling Update Deployment</h4><p>The Deployment updates Pods in a rolling update
fashion when <code>.spec.strategy.type==RollingUpdate</code>. You can specify <code>maxUnavailable</code> and <code>maxSurge</code> to control
the rolling update process.</p><h5 id=max-unavailable>Max Unavailable</h5><p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> is an optional field that specifies the maximum number
of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)
or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by
rounding down. The value cannot be 0 if <code>.spec.strategy.rollingUpdate.maxSurge</code> is 0. The default value is 25%.</p><p>For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired
Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled
down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available
at all times during the update is at least 70% of the desired Pods.</p><h5 id=max-surge>Max Surge</h5><p><code>.spec.strategy.rollingUpdate.maxSurge</code> is an optional field that specifies the maximum number of Pods
that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a
percentage of desired Pods (for example, 10%). The value cannot be 0 if <code>MaxUnavailable</code> is 0. The absolute number
is calculated from the percentage by rounding up. The default value is 25%.</p><p>For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the
rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired
Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the
total number of Pods running at any time during the update is at most 130% of desired Pods.</p><h3 id=progress-deadline-seconds>Progress Deadline Seconds</h3><p><code>.spec.progressDeadlineSeconds</code> is an optional field that specifies the number of seconds you want
to wait for your Deployment to progress before the system reports back that the Deployment has
<a href=#failed-deployment>failed progressing</a> - surfaced as a condition with <code>type: Progressing</code>, <code>status: "False"</code>.
and <code>reason: ProgressDeadlineExceeded</code> in the status of the resource. The Deployment controller will keep
retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment
controller will roll back a Deployment as soon as it observes such a condition.</p><p>If specified, this field needs to be greater than <code>.spec.minReadySeconds</code>.</p><h3 id=min-ready-seconds>Min Ready Seconds</h3><p><code>.spec.minReadySeconds</code> is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>Container Probes</a>.</p><h3 id=revision-history-limit>Revision History Limit</h3><p>A Deployment's revision history is stored in the ReplicaSets it controls.</p><p><code>.spec.revisionHistoryLimit</code> is an optional field that specifies the number of old ReplicaSets to retain
to allow rollback. These old ReplicaSets consume resources in <code>etcd</code> and crowd the output of <code>kubectl get rs</code>. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.</p><p>More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.
In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.</p><h3 id=paused>Paused</h3><p><code>.spec.paused</code> is an optional boolean field for pausing and resuming a Deployment. The only difference between
a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused
Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when
it is created.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li><a href=/docs/tasks/run-application/run-stateless-application-deployment/>Run a Stateless Application Using a Deployment</a>.</li><li><code>Deployment</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/deployment-v1/>Deployment</a>
object definition to understand the API for deployments.</li><li>Read about <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d459b930218774655fa7fd1620625539>5.2.2 - ReplicaSet</h1><p>A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often
used to guarantee the availability of a specified number of identical Pods.</p><h2 id=how-a-replicaset-works>How a ReplicaSet works</h2><p>A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number
of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods
it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating
and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod
template.</p><p>A ReplicaSet is linked to its Pods via the Pods' <a href=/docs/concepts/architecture/garbage-collection/#owners-and-dependents>metadata.ownerReferences</a>
field, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning
ReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet
knows of the state of the Pods it is maintaining and plans accordingly.</p><p>A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no
OwnerReference or the OwnerReference is not a <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a> and it
matches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.</p><h2 id=when-to-use-a-replicaset>When to use a ReplicaSet</h2><p>A ReplicaSet ensures that a specified number of pod replicas are running at any given
time. However, a Deployment is a higher-level concept that manages ReplicaSets and
provides declarative updates to Pods along with a lot of other useful features.
Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless
you require custom update orchestration or don't require updates at all.</p><p>This actually means that you may never need to manipulate ReplicaSet objects:
use a Deployment instead, and define your application in the spec section.</p><h2 id=example>Example</h2><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/frontend.yaml download=controllers/frontend.yaml><code>controllers/frontend.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-frontend-yaml")' title="Copy controllers/frontend.yaml to clipboard"></img></div><div class=includecode id=controllers-frontend-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># modify replicas according to your case</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>php-redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google_samples/gb-frontend:v3<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Saving this manifest into <code>frontend.yaml</code> and submitting it to a Kubernetes cluster will
create the defined ReplicaSet and the Pods that it manages.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>You can then get the current ReplicaSets deployed:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>And see the frontend one you created:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME       DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>frontend   <span style=color:#666>3</span>         <span style=color:#666>3</span>         <span style=color:#666>3</span>       6s
</span></span></code></pre></div><p>You can also check on the state of the ReplicaSet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe rs/frontend
</span></span></code></pre></div><p>And you will see output similar to:</p><pre tabindex=0><code>Name:         frontend
Namespace:    default
Selector:     tier=frontend
Labels:       app=guestbook
              tier=frontend
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;ReplicaSet&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;labels&#34;:{&#34;app&#34;:&#34;guestbook&#34;,&#34;tier&#34;:&#34;frontend&#34;},&#34;name&#34;:&#34;frontend&#34;,...
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  tier=frontend
  Containers:
   php-redis:
    Image:        gcr.io/google_samples/gb-frontend:v3
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts
</code></pre><p>And lastly you can check for the Pods brought up:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>You should see Pod information similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-b2zdv   1/1     Running   <span style=color:#666>0</span>          6m36s
</span></span><span style=display:flex><span>frontend-vcmts   1/1     Running   <span style=color:#666>0</span>          6m36s
</span></span><span style=display:flex><span>frontend-wtsmm   1/1     Running   <span style=color:#666>0</span>          6m36s
</span></span></code></pre></div><p>You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.
To do this, get the yaml of one of the Pods running:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods frontend-b2zdv -o yaml
</span></span></code></pre></div><p>The output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>creationTimestamp</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2020-02-12T07:06:16Z&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>generateName</span>:<span style=color:#bbb> </span>frontend-<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-b2zdv<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ownerReferences</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>blockOwnerDeletion</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>controller</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>uid</span>:<span style=color:#bbb> </span>f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=non-template-pod-acquisitions>Non-Template Pod acquisitions</h2><p>While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have
labels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited
to owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.</p><p>Take the previous frontend ReplicaSet example, and the Pods specified in the following manifest:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-rs.yaml download=pods/pod-rs.yaml><code>pods/pod-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-rs-yaml")' title="Copy pods/pod-rs.yaml to clipboard"></img></div><div class=includecode id=pods-pod-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:2.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:1.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend
ReplicaSet, they will immediately be acquired by it.</p><p>Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to
fulfill its replica count requirement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over
its desired count.</p><p>Fetching the Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>The output shows that the new Pods are either already terminated, or in the process of being terminated:</p><pre tabindex=0><code>NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       0          10m
frontend-vcmts   1/1     Running       0          10m
frontend-wtsmm   1/1     Running       0          10m
pod1             0/1     Terminating   0          1s
pod2             0/1     Terminating   0          1s
</code></pre><p>If you create the Pods first:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>And then create the ReplicaSet however:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the
number of its new Pods and the original matches its desired count. As fetching the Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>Will reveal in its output:</p><pre tabindex=0><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   0          9s
pod1             1/1     Running   0          36s
pod2             1/1     Running   0          36s
</code></pre><p>In this manner, a ReplicaSet can own a non-homogenous set of Pods</p><h2 id=writing-a-replicaset-manifest>Writing a ReplicaSet manifest</h2><p>As with all other Kubernetes API objects, a ReplicaSet needs the <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.
For ReplicaSets, the <code>kind</code> is always a ReplicaSet.</p><p>The name of a ReplicaSet object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>A ReplicaSet also needs a <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>pod template</a> which is also
required to have labels in place. In our <code>frontend.yaml</code> example we had one label: <code>tier: frontend</code>.
Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.</p><p>For the template's <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy>restart policy</a> field,
<code>.spec.template.spec.restartPolicy</code>, the only allowed value is <code>Always</code>, which is the default.</p><h3 id=pod-selector>Pod Selector</h3><p>The <code>.spec.selector</code> field is a <a href=/docs/concepts/overview/working-with-objects/labels/>label selector</a>. As discussed
<a href=#how-a-replicaset-works>earlier</a> these are the labels used to identify potential Pods to acquire. In our
<code>frontend.yaml</code> example, the selector was:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span></code></pre></div><p>In the ReplicaSet, <code>.spec.template.metadata.labels</code> must match <code>spec.selector</code>, or it will
be rejected by the API.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> For 2 ReplicaSets specifying the same <code>.spec.selector</code> but different
<code>.spec.template.metadata.labels</code> and <code>.spec.template.spec</code> fields, each ReplicaSet ignores the
Pods created by the other ReplicaSet.</div><h3 id=replicas>Replicas</h3><p>You can specify how many Pods should run concurrently by setting <code>.spec.replicas</code>. The ReplicaSet will create/delete
its Pods to match this number.</p><p>If you do not specify <code>.spec.replicas</code>, then it defaults to 1.</p><h2 id=working-with-replicasets>Working with ReplicaSets</h2><h3 id=deleting-a-replicaset-and-its-pods>Deleting a ReplicaSet and its Pods</h3><p>To delete a ReplicaSet and all of its Pods, use
<a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>. The
<a href=/docs/concepts/architecture/garbage-collection/>Garbage collector</a> automatically deletes all of
the dependent Pods by default.</p><p>When using the REST API or the <code>client-go</code> library, you must set <code>propagationPolicy</code> to
<code>Background</code> or <code>Foreground</code> in the <code>-d</code> option. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><h3 id=deleting-just-a-replicaset>Deleting just a ReplicaSet</h3><p>You can delete a ReplicaSet without affecting any of its Pods using
<a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>
with the <code>--cascade=orphan</code> option.
When using the REST API or the <code>client-go</code> library, you must set <code>propagationPolicy</code> to <code>Orphan</code>.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>Once the original is deleted, you can create a new ReplicaSet to replace it. As long
as the old and new <code>.spec.selector</code> are the same, then the new one will adopt the old Pods.
However, it will not make any effort to make existing Pods match a new, different pod template.
To update Pods to a new spec in a controlled way, use a
<a href=/docs/concepts/workloads/controllers/deployment/#creating-a-deployment>Deployment</a>, as
ReplicaSets do not support a rolling update directly.</p><h3 id=isolating-pods-from-a-replicaset>Isolating Pods from a ReplicaSet</h3><p>You can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods
from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (
assuming that the number of replicas is not also changed).</p><h3 id=scaling-a-replicaset>Scaling a ReplicaSet</h3><p>A ReplicaSet can be easily scaled up or down by simply updating the <code>.spec.replicas</code> field. The ReplicaSet controller
ensures that a desired number of Pods with a matching label selector are available and operational.</p><p>When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to
prioritize scaling down pods based on the following general algorithm:</p><ol><li>Pending (and unschedulable) pods are scaled down first</li><li>If <code>controller.kubernetes.io/pod-deletion-cost</code> annotation is set, then
the pod with the lower value will come first.</li><li>Pods on nodes with more replicas come before pods on nodes with fewer replicas.</li><li>If the pods' creation times differ, the pod that was created more recently
comes before the older pod (the creation times are bucketed on an integer log scale
when the <code>LogarithmicScaleDown</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> is enabled)</li></ol><p>If all of the above match, then selection is random.</p><h3 id=pod-deletion-cost>Pod deletion cost</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code></div><p>Using the <a href=/docs/reference/labels-annotations-taints/#pod-deletion-cost><code>controller.kubernetes.io/pod-deletion-cost</code></a>
annotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.</p><p>The annotation should be set on the pod, the range is [-2147483647, 2147483647]. It represents the cost of
deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion
cost are preferred to be deleted before pods with higher deletion cost.</p><p>The implicit value for this annotation for pods that don't set it is 0; negative values are permitted.
Invalid values will be rejected by the API server.</p><p>This feature is beta and enabled by default. You can disable it using the
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
<code>PodDeletionCost</code> in both kube-apiserver and kube-controller-manager.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><ul><li>This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.</li><li>Users should avoid updating the annotation frequently, such as updating it based on a metric value,
because doing so will generate a significant number of pod updates on the apiserver.</li></ul></div><h4 id=example-use-case>Example Use Case</h4><p>The different pods of an application could have different utilization levels. On scale down, the application
may prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application
should update <code>controller.kubernetes.io/pod-deletion-cost</code> once before issuing a scale down (setting the
annotation to a value proportional to pod utilization level). This works if the application itself controls
the down scaling; for example, the driver pod of a Spark deployment.</p><h3 id=replicaset-as-a-horizontal-pod-autoscaler-target>ReplicaSet as a Horizontal Pod Autoscaler Target</h3><p>A ReplicaSet can also be a target for
<a href=/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscalers (HPA)</a>. That is,
a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting
the ReplicaSet we created in the previous example.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/hpa-rs.yaml download=controllers/hpa-rs.yaml><code>controllers/hpa-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-hpa-rs-yaml")' title="Copy controllers/hpa-rs.yaml to clipboard"></img></div><div class=includecode id=controllers-hpa-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>autoscaling/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>HorizontalPodAutoscaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-scaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scaleTargetRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>minReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>maxReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>targetCPUUtilizationPercentage</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Saving this manifest into <code>hpa-rs.yaml</code> and submitting it to a Kubernetes cluster should
create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage
of the replicated Pods.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
</span></span></code></pre></div><p>Alternatively, you can use the <code>kubectl autoscale</code> command to accomplish the same
(and it's easier!)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale rs frontend --max<span style=color:#666>=</span><span style=color:#666>10</span> --min<span style=color:#666>=</span><span style=color:#666>3</span> --cpu-percent<span style=color:#666>=</span><span style=color:#666>50</span>
</span></span></code></pre></div><h2 id=alternatives-to-replicaset>Alternatives to ReplicaSet</h2><h3 id=deployment-recommended>Deployment (recommended)</h3><p><a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> is an object which can own ReplicaSets and update
them and their Pods via declarative, server-side rolling updates.
While ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod
creation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that
they create. Deployments own and manage their ReplicaSets.
As such, it is recommended to use Deployments when you want ReplicaSets.</p><h3 id=bare-pods>Bare Pods</h3><p>Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or
terminated for any reason, such as in the case of node failure or disruptive node maintenance,
such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your
application requires only a single Pod. Think of it similarly to a process supervisor, only it
supervises multiple Pods across multiple nodes instead of individual processes on a single node. A
ReplicaSet delegates local container restarts to some agent on the node such as Kubelet.</p><h3 id=job>Job</h3><p>Use a <a href=/docs/concepts/workloads/controllers/job/><code>Job</code></a> instead of a ReplicaSet for Pods that are
expected to terminate on their own (that is, batch jobs).</p><h3 id=daemonset>DaemonSet</h3><p>Use a <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> instead of a ReplicaSet for Pods that provide a
machine-level function, such as machine monitoring or machine logging. These Pods have a lifetime that is tied
to a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h3 id=replicationcontroller>ReplicationController</h3><p>ReplicaSets are the successors to <a href=/docs/concepts/workloads/controllers/replicationcontroller/>ReplicationControllers</a>.
The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based
selector requirements as described in the <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>labels user guide</a>.
As such, ReplicaSets are preferred over ReplicationControllers</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li>Learn about <a href=/docs/concepts/workloads/controllers/deployment/>Deployments</a>.</li><li><a href=/docs/tasks/run-application/run-stateless-application-deployment/>Run a Stateless Application Using a Deployment</a>,
which relies on ReplicaSets to work.</li><li><code>ReplicaSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/replica-set-v1/>ReplicaSet</a>
object definition to understand the API for replica sets.</li><li>Read about <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6d72299952c37ca8cc61b416e5bdbcd4>5.2.3 - StatefulSets</h1><p>StatefulSet is the workload API object used to manage stateful applications.</p><p>Manages the deployment and scaling of a set of <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>, <em>and provides guarantees about the ordering and uniqueness</em> of these Pods.</p><p>Like a <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p><p>If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.</p><h2 id=using-statefulsets>Using StatefulSets</h2><p>StatefulSets are valuable for applications that require one or more of the
following.</p><ul><li>Stable, unique network identifiers.</li><li>Stable, persistent storage.</li><li>Ordered, graceful deployment and scaling.</li><li>Ordered, automated rolling updates.</li></ul><p>In the above, stable is synonymous with persistence across Pod (re)scheduling.
If an application doesn't require any stable identifiers or ordered deployment,
deletion, or scaling, you should deploy your application using a workload object
that provides a set of stateless replicas.
<a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a> or
<a href=/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a> may be better suited to your stateless needs.</p><h2 id=limitations>Limitations</h2><ul><li>The storage for a given Pod must either be provisioned by a
<a href=https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md>PersistentVolume Provisioner</a>
based on the requested <code>storage class</code>, or pre-provisioned by an admin.</li><li>Deleting and/or scaling a StatefulSet down will <em>not</em> delete the volumes associated with the
StatefulSet. This is done to ensure data safety, which is generally more valuable than an
automatic purge of all related StatefulSet resources.</li><li>StatefulSets currently require a <a href=/docs/concepts/services-networking/service/#headless-services>Headless Service</a>
to be responsible for the network identity of the Pods. You are responsible for creating this
Service.</li><li>StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is
deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is
possible to scale the StatefulSet down to 0 prior to deletion.</li><li>When using <a href=#rolling-updates>Rolling Updates</a> with the default
<a href=#pod-management-policies>Pod Management Policy</a> (<code>OrderedReady</code>),
it's possible to get into a broken state that requires
<a href=#forced-rollback>manual intervention to repair</a>.</li></ul><h2 id=components>Components</h2><p>The example below demonstrates the components of a StatefulSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span>None<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># has to match .spec.template.metadata.labels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;nginx&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># by default is 1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>minReadySeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># by default is 0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># has to match .spec.selector.matchLabels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/nginx-slim:0.8<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/usr/share/nginx/html<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeClaimTemplates</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;ReadWriteOnce&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;my-storage-class&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></span></span></code></pre></div><p>In the above example:</p><ul><li>A Headless Service, named <code>nginx</code>, is used to control the network domain.</li><li>The StatefulSet, named <code>web</code>, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.</li><li>The <code>volumeClaimTemplates</code> will provide stable storage using
<a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a> provisioned by a
PersistentVolume Provisioner.</li></ul><p>The name of a StatefulSet object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><h3 id=pod-selector>Pod Selector</h3><p>You must set the <code>.spec.selector</code> field of a StatefulSet to match the labels of its
<code>.spec.template.metadata.labels</code>. Failing to specify a matching Pod Selector will result in a
validation error during StatefulSet creation.</p><h3 id=volume-claim-templates>Volume Claim Templates</h3><p>You can set the <code>.spec.volumeClaimTemplates</code> which can provide stable storage using
<a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a> provisioned by a PersistentVolume
Provisioner.</p><h3 id=minimum-ready-seconds>Minimum ready seconds</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p><code>.spec.minReadySeconds</code> is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be running and ready without any of its containers crashing, for it to be considered available.
This is used to check progression of a rollout when using a <a href=#rolling-updates>Rolling Update</a> strategy.
This field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>Container Probes</a>.</p><h2 id=pod-identity>Pod Identity</h2><p>StatefulSet Pods have a unique identity that consists of an ordinal, a
stable network identity, and stable storage. The identity sticks to the Pod,
regardless of which node it's (re)scheduled on.</p><h3 id=ordinal-index>Ordinal Index</h3><p>For a StatefulSet with N replicas, each Pod in the StatefulSet will be
assigned an integer ordinal, from 0 up through N-1, that is unique over the Set.</p><h3 id=stable-network-id>Stable Network ID</h3><p>Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet
and the ordinal of the Pod. The pattern for the constructed hostname
is <code>$(statefulset name)-$(ordinal)</code>. The example above will create three Pods
named <code>web-0,web-1,web-2</code>.
A StatefulSet can use a <a href=/docs/concepts/services-networking/service/#headless-services>Headless Service</a>
to control the domain of its Pods. The domain managed by this Service takes the form:
<code>$(service name).$(namespace).svc.cluster.local</code>, where "cluster.local" is the
cluster domain.
As each Pod is created, it gets a matching DNS subdomain, taking the form:
<code>$(podname).$(governing service domain)</code>, where the governing service is defined
by the <code>serviceName</code> field on the StatefulSet.</p><p>Depending on how DNS is configured in your cluster, you may not be able to look up the DNS
name for a newly-run Pod immediately. This behavior can occur when other clients in the
cluster have already sent queries for the hostname of the Pod before it was created.
Negative caching (normal in DNS) means that the results of previous failed lookups are
remembered and reused, even after the Pod is running, for at least a few seconds.</p><p>If you need to discover Pods promptly after they are created, you have a few options:</p><ul><li>Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.</li><li>Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the
config map for CoreDNS, which currently caches for 30 seconds).</li></ul><p>As mentioned in the <a href=#limitations>limitations</a> section, you are responsible for
creating the <a href=/docs/concepts/services-networking/service/#headless-services>Headless Service</a>
responsible for the network identity of the pods.</p><p>Here are some examples of choices for Cluster Domain, Service name,
StatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.</p><table><thead><tr><th>Cluster Domain</th><th>Service (ns/name)</th><th>StatefulSet (ns/name)</th><th>StatefulSet Domain</th><th>Pod DNS</th><th>Pod Hostname</th></tr></thead><tbody><tr><td>cluster.local</td><td>default/nginx</td><td>default/web</td><td>nginx.default.svc.cluster.local</td><td>web-{0..N-1}.nginx.default.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>cluster.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.cluster.local</td><td>web-{0..N-1}.nginx.foo.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>kube.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.kube.local</td><td>web-{0..N-1}.nginx.foo.svc.kube.local</td><td>web-{0..N-1}</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Cluster Domain will be set to <code>cluster.local</code> unless
<a href=/docs/concepts/services-networking/dns-pod-service/>otherwise configured</a>.</div><h3 id=stable-storage>Stable Storage</h3><p>For each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one
PersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume
with a StorageClass of <code>my-storage-class</code> and 1 Gib of provisioned storage. If no StorageClass
is specified, then the default StorageClass will be used. When a Pod is (re)scheduled
onto a node, its <code>volumeMounts</code> mount the PersistentVolumes associated with its
PersistentVolume Claims. Note that, the PersistentVolumes associated with the
Pods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.
This must be done manually.</p><h3 id=pod-name-label>Pod Name Label</h3><p>When the StatefulSet <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> creates a Pod,
it adds a label, <code>statefulset.kubernetes.io/pod-name</code>, that is set to the name of
the Pod. This label allows you to attach a Service to a specific Pod in
the StatefulSet.</p><h2 id=deployment-and-scaling-guarantees>Deployment and Scaling Guarantees</h2><ul><li>For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.</li><li>When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.</li><li>Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.</li><li>Before a Pod is terminated, all of its successors must be completely shutdown.</li></ul><p>The StatefulSet should not specify a <code>pod.Spec.TerminationGracePeriodSeconds</code> of 0. This practice
is unsafe and strongly discouraged. For further explanation, please refer to
<a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>force deleting StatefulSet Pods</a>.</p><p>When the nginx example above is created, three Pods will be deployed in the order
web-0, web-1, web-2. web-1 will not be deployed before web-0 is
<a href=/docs/concepts/workloads/pods/pod-lifecycle/>Running and Ready</a>, and web-2 will not be deployed until
web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before
web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and
becomes Running and Ready.</p><p>If a user were to scale the deployed example by patching the StatefulSet such that
<code>replicas=1</code>, web-2 would be terminated first. web-1 would not be terminated until web-2
is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and
is completely shutdown, but prior to web-1's termination, web-1 would not be terminated
until web-0 is Running and Ready.</p><h3 id=pod-management-policies>Pod Management Policies</h3><p>StatefulSet allows you to relax its ordering guarantees while
preserving its uniqueness and identity guarantees via its <code>.spec.podManagementPolicy</code> field.</p><h4 id=orderedready-pod-management>OrderedReady Pod Management</h4><p><code>OrderedReady</code> pod management is the default for StatefulSets. It implements the behavior
described <a href=#deployment-and-scaling-guarantees>above</a>.</p><h4 id=parallel-pod-management>Parallel Pod Management</h4><p><code>Parallel</code> pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and to not wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not
affected.</p><h2 id=update-strategies>Update strategies</h2><p>A StatefulSet's <code>.spec.updateStrategy</code> field allows you to configure
and disable automated rolling updates for containers, labels, resource request/limits, and
annotations for the Pods in a StatefulSet. There are two possible values:</p><dl><dt><code>OnDelete</code></dt><dd>When a StatefulSet's <code>.spec.updateStrategy.type</code> is set to <code>OnDelete</code>,
the StatefulSet controller will not automatically update the Pods in a
StatefulSet. Users must manually delete Pods to cause the controller to
create new Pods that reflect modifications made to a StatefulSet's <code>.spec.template</code>.</dd><dt><code>RollingUpdate</code></dt><dd>The <code>RollingUpdate</code> update strategy implements automated, rolling updates for the Pods in a
StatefulSet. This is the default update strategy.</dd></dl><h2 id=rolling-updates>Rolling Updates</h2><p>When a StatefulSet's <code>.spec.updateStrategy.type</code> is set to <code>RollingUpdate</code>, the
StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed
in the same order as Pod termination (from the largest ordinal to the smallest), updating
each Pod one at a time.</p><p>The Kubernetes control plane waits until an updated Pod is Running and Ready prior
to updating its predecessor. If you have set <code>.spec.minReadySeconds</code> (see
<a href=#minimum-ready-seconds>Minimum Ready Seconds</a>), the control plane additionally waits that
amount of time after the Pod turns ready, before moving on.</p><h3 id=partitions>Partitioned rolling updates</h3><p>The <code>RollingUpdate</code> update strategy can be partitioned, by specifying a
<code>.spec.updateStrategy.rollingUpdate.partition</code>. If a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the StatefulSet's
<code>.spec.template</code> is updated. All Pods with an ordinal that is less than the partition will not
be updated, and, even if they are deleted, they will be recreated at the previous version. If a
StatefulSet's <code>.spec.updateStrategy.rollingUpdate.partition</code> is greater than its <code>.spec.replicas</code>,
updates to its <code>.spec.template</code> will not be propagated to its Pods.
In most cases you will not need to use a partition, but they are useful if you want to stage an
update, roll out a canary, or perform a phased roll out.</p><h3 id=maximum-unavailable-pods>Maximum unavailable Pods</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [alpha]</code></div><p>You can control the maximum number of Pods that can be unavailable during an update
by specifying the <code>.spec.updateStrategy.rollingUpdate.maxUnavailable</code> field.
The value can be an absolute number (for example, <code>5</code>) or a percentage of desired
Pods (for example, <code>10%</code>). Absolute number is calculated from the percentage value
by rounding it up. This field cannot be 0. The default setting is 1.</p><p>This field applies to all Pods in the range <code>0</code> to <code>replicas - 1</code>. If there is any
unavailable Pod in the range <code>0</code> to <code>replicas - 1</code>, it will be counted towards
<code>maxUnavailable</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>maxUnavailable</code> field is in Alpha stage and it is honored only by API servers
that are running with the <code>MaxUnavailableStatefulSet</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
enabled.</div><h3 id=forced-rollback>Forced rollback</h3><p>When using <a href=#rolling-updates>Rolling Updates</a> with the default
<a href=#pod-management-policies>Pod Management Policy</a> (<code>OrderedReady</code>),
it's possible to get into a broken state that requires manual intervention to repair.</p><p>If you update the Pod template to a configuration that never becomes Running and
Ready (for example, due to a bad binary or application-level configuration error),
StatefulSet will stop the rollout and wait.</p><p>In this state, it's not enough to revert the Pod template to a good configuration.
Due to a <a href=https://github.com/kubernetes/kubernetes/issues/67250>known issue</a>,
StatefulSet will continue to wait for the broken Pod to become Ready
(which never happens) before it will attempt to revert it back to the working
configuration.</p><p>After reverting the template, you must also delete any Pods that StatefulSet had
already attempted to run with the bad configuration.
StatefulSet will then begin to recreate the Pods using the reverted template.</p><h2 id=persistentvolumeclaim-retention>PersistentVolumeClaim retention</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code></div><p>The optional <code>.spec.persistentVolumeClaimRetentionPolicy</code> field controls if
and how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the
<code>StatefulSetAutoDeletePVC</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the API server and the controller manager to use this field.
Once enabled, there are two policies you can configure for each StatefulSet:</p><dl><dt><code>whenDeleted</code></dt><dd>configures the volume retention behavior that applies when the StatefulSet is deleted</dd><dt><code>whenScaled</code></dt><dd>configures the volume retention behavior that applies when the replica count of
the StatefulSet is reduced; for example, when scaling down the set.</dd></dl><p>For each policy that you can configure, you can set the value to either <code>Delete</code> or <code>Retain</code>.</p><dl><dt><code>Delete</code></dt><dd>The PVCs created from the StatefulSet <code>volumeClaimTemplate</code> are deleted for each Pod
affected by the policy. With the <code>whenDeleted</code> policy all PVCs from the
<code>volumeClaimTemplate</code> are deleted after their Pods have been deleted. With the
<code>whenScaled</code> policy, only PVCs corresponding to Pod replicas being scaled down are
deleted, after their Pods have been deleted.</dd><dt><code>Retain</code> (default)</dt><dd>PVCs from the <code>volumeClaimTemplate</code> are not affected when their Pod is
deleted. This is the behavior before this new feature.</dd></dl><p>Bear in mind that these policies <strong>only</strong> apply when Pods are being removed due to the
StatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet
fails due to node failure, and the control plane creates a replacement Pod, the StatefulSet
retains the existing PVC. The existing volume is unaffected, and the cluster will attach it to
the node where the new Pod is about to launch.</p><p>The default for policies is <code>Retain</code>, matching the StatefulSet behavior before this new feature.</p><p>Here is an example policy.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>persistentVolumeClaimRetentionPolicy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenDeleted</span>:<span style=color:#bbb> </span>Retain<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenScaled</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The StatefulSet <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> adds
<a href=/docs/concepts/overview/working-with-objects/owners-dependents/#owner-references-in-object-specifications>owner references</a>
to its PVCs, which are then deleted by the <a class=glossary-tooltip title='A collective term for the various mechanisms Kubernetes uses to clean up cluster resources.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/garbage-collection/ target=_blank aria-label='garbage collector'>garbage collector</a> after the Pod is terminated. This enables the Pod to
cleanly unmount all volumes before the PVCs are deleted (and before the backing PV and
volume are deleted, depending on the retain policy). When you set the <code>whenDeleted</code>
policy to <code>Delete</code>, an owner reference to the StatefulSet instance is placed on all PVCs
associated with that StatefulSet.</p><p>The <code>whenScaled</code> policy must delete PVCs only when a Pod is scaled down, and not when a
Pod is deleted for another reason. When reconciling, the StatefulSet controller compares
its desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod
whose id greater than the replica count is condemned and marked for deletion. If the
<code>whenScaled</code> policy is <code>Delete</code>, the condemned Pods are first set as owners to the
associated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs
to be garbage collected after only the condemned Pods have terminated.</p><p>This means that if the controller crashes and restarts, no Pod will be deleted before its
owner reference has been updated appropriate to the policy. If a condemned Pod is
force-deleted while the controller is down, the owner reference may or may not have been
set up, depending on when the controller crashed. It may take several reconcile loops to
update the owner references, so some condemned Pods may have set up owner references and
others may not. For this reason we recommend waiting for the controller to come back up,
which will verify owner references before terminating Pods. If that is not possible, the
operator should verify the owner references on PVCs to ensure the expected objects are
deleted when Pods are force-deleted.</p><h3 id=replicas>Replicas</h3><p><code>.spec.replicas</code> is an optional field that specifies the number of desired Pods. It defaults to 1.</p><p>Should you manually scale a deployment, example via <code>kubectl scale statefulset statefulset --replicas=X</code>, and then you update that StatefulSet
based on a manifest (for example: by running <code>kubectl apply -f statefulset.yaml</code>), then applying that manifest overwrites the manual scaling
that you previously did.</p><p>If a <a href=/docs/tasks/run-application/horizontal-pod-autoscale/>HorizontalPodAutoscaler</a>
(or any similar API for horizontal scaling) is managing scaling for a
Statefulset, don't set <code>.spec.replicas</code>. Instead, allow the Kubernetes
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> to manage
the <code>.spec.replicas</code> field automatically.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li>Find out how to use StatefulSets<ul><li>Follow an example of <a href=/docs/tutorials/stateful-application/basic-stateful-set/>deploying a stateful application</a>.</li><li>Follow an example of <a href=/docs/tutorials/stateful-application/cassandra/>deploying Cassandra with Stateful Sets</a>.</li><li>Follow an example of <a href=/docs/tasks/run-application/run-replicated-stateful-application/>running a replicated stateful application</a>.</li><li>Learn how to <a href=/docs/tasks/run-application/scale-stateful-set/>scale a StatefulSet</a>.</li><li>Learn what's involved when you <a href=/docs/tasks/run-application/delete-stateful-set/>delete a StatefulSet</a>.</li><li>Learn how to <a href=/docs/tasks/configure-pod-container/configure-volume-storage/>configure a Pod to use a volume for storage</a>.</li><li>Learn how to <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/>configure a Pod to use a PersistentVolume for storage</a>.</li></ul></li><li><code>StatefulSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/>StatefulSet</a>
object definition to understand the API for stateful sets.</li><li>Read about <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-41600eb8b6631c88848156f381e9d588>5.2.4 - DaemonSet</h1><p>A <em>DaemonSet</em> ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the
cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage
collected. Deleting a DaemonSet will clean up the Pods it created.</p><p>Some typical uses of a DaemonSet are:</p><ul><li>running a cluster storage daemon on every node</li><li>running a logs collection daemon on every node</li><li>running a node monitoring daemon on every node</li></ul><p>In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.
A more complex setup might use multiple DaemonSets for a single type of daemon, but with
different flags and/or different memory and cpu requests for different hardware types.</p><h2 id=writing-a-daemonset-spec>Writing a DaemonSet Spec</h2><h3 id=create-a-daemonset>Create a DaemonSet</h3><p>You can describe a DaemonSet in a YAML file. For example, the <code>daemonset.yaml</code> file below
describes a DaemonSet that runs the fluentd-elasticsearch Docker image:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/daemonset.yaml download=controllers/daemonset.yaml><code>controllers/daemonset.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-daemonset-yaml")' title="Copy controllers/daemonset.yaml to clipboard"></img></div><div class=includecode id=controllers-daemonset-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DaemonSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>fluentd-logging<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># these tolerations are to have the daemonset runnable on control plane nodes</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># remove them if your control plane nodes should not run pods</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node-role.kubernetes.io/control-plane<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Exists<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node-role.kubernetes.io/master<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Exists<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>quay.io/fluentd_elasticsearch/fluentd:v2.5.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>30</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Create a DaemonSet based on the YAML file:</p><pre tabindex=0><code>kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
</code></pre><h3 id=required-fields>Required Fields</h3><p>As with all other Kubernetes config, a DaemonSet needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields. For
general information about working with config files, see
<a href=/docs/tasks/run-application/run-stateless-application-deployment/>running stateless applications</a>
and <a href=/docs/concepts/overview/working-with-objects/object-management/>object management using kubectl</a>.</p><p>The name of a DaemonSet object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>A DaemonSet also needs a
<a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code></a>
section.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> is one of the required fields in <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>pod template</a>.
It has exactly the same schema as a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>,
except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate
labels (see <a href=#pod-selector>pod selector</a>).</p><p>A Pod Template in a DaemonSet must have a <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>RestartPolicy</code></a>
equal to <code>Always</code>, or be unspecified, which defaults to <code>Always</code>.</p><h3 id=pod-selector>Pod Selector</h3><p>The <code>.spec.selector</code> field is a pod selector. It works the same as the <code>.spec.selector</code> of
a <a href=/docs/concepts/workloads/controllers/job/>Job</a>.</p><p>You must specify a pod selector that matches the labels of the
<code>.spec.template</code>.
Also, once a DaemonSet is created,
its <code>.spec.selector</code> can not be mutated. Mutating the pod selector can lead to the
unintentional orphaning of Pods, and it was found to be confusing to users.</p><p>The <code>.spec.selector</code> is an object consisting of two fields:</p><ul><li><code>matchLabels</code> - works the same as the <code>.spec.selector</code> of a
<a href=/docs/concepts/workloads/controllers/replicationcontroller/>ReplicationController</a>.</li><li><code>matchExpressions</code> - allows to build more sophisticated selectors by specifying key,
list of values and an operator that relates the key and values.</li></ul><p>When the two are specified the result is ANDed.</p><p>The <code>.spec.selector</code> must match the <code>.spec.template.metadata.labels</code>.
Config with these two not matching will be rejected by the API.</p><h3 id=running-pods-on-select-nodes>Running Pods on select Nodes</h3><p>If you specify a <code>.spec.template.spec.nodeSelector</code>, then the DaemonSet controller will
create Pods on nodes which match that <a href=/docs/concepts/scheduling-eviction/assign-pod-node/>node selector</a>.
Likewise if you specify a <code>.spec.template.spec.affinity</code>,
then DaemonSet controller will create Pods on nodes which match that
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/>node affinity</a>.
If you do not specify either, then the DaemonSet controller will create Pods on all nodes.</p><h2 id=how-daemon-pods-are-scheduled>How Daemon Pods are scheduled</h2><h3 id=scheduled-by-default-scheduler>Scheduled by default scheduler</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.17 [stable]</code></div><p>A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the
node that a Pod runs on is selected by the Kubernetes scheduler. However,
DaemonSet pods are created and scheduled by the DaemonSet controller instead.
That introduces the following issues:</p><ul><li>Inconsistent Pod behavior: Normal Pods waiting to be scheduled are created
and in <code>Pending</code> state, but DaemonSet pods are not created in <code>Pending</code>
state. This is confusing to the user.</li><li><a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod preemption</a>
is handled by default scheduler. When preemption is enabled, the DaemonSet controller
will make scheduling decisions without considering pod priority and preemption.</li></ul><p><code>ScheduleDaemonSetPods</code> allows you to schedule DaemonSets using the default
scheduler instead of the DaemonSet controller, by adding the <code>NodeAffinity</code> term
to the DaemonSet pods, instead of the <code>.spec.nodeName</code> term. The default
scheduler is then used to bind the pod to the target host. If node affinity of
the DaemonSet pod already exists, it is replaced (the original node affinity was
taken into account before selecting the target host). The DaemonSet controller only
performs these operations when creating or modifying DaemonSet pods, and no
changes are made to the <code>spec.template</code> of the DaemonSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>matchFields</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>metadata.name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- target-host-name<span style=color:#bbb>
</span></span></span></code></pre></div><p>In addition, <code>node.kubernetes.io/unschedulable:NoSchedule</code> toleration is added
automatically to DaemonSet Pods. The default scheduler ignores
<code>unschedulable</code> Nodes when scheduling DaemonSet Pods.</p><h3 id=taints-and-tolerations>Taints and Tolerations</h3><p>Although Daemon Pods respect
<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>taints and tolerations</a>,
the following tolerations are added to DaemonSet Pods automatically according to
the related features.</p><table><thead><tr><th>Toleration Key</th><th>Effect</th><th>Version</th><th>Description</th></tr></thead><tbody><tr><td><code>node.kubernetes.io/not-ready</code></td><td>NoExecute</td><td>1.13+</td><td>DaemonSet pods will not be evicted when there are node problems such as a network partition.</td></tr><tr><td><code>node.kubernetes.io/unreachable</code></td><td>NoExecute</td><td>1.13+</td><td>DaemonSet pods will not be evicted when there are node problems such as a network partition.</td></tr><tr><td><code>node.kubernetes.io/disk-pressure</code></td><td>NoSchedule</td><td>1.8+</td><td>DaemonSet pods tolerate disk-pressure attributes by default scheduler.</td></tr><tr><td><code>node.kubernetes.io/memory-pressure</code></td><td>NoSchedule</td><td>1.8+</td><td>DaemonSet pods tolerate memory-pressure attributes by default scheduler.</td></tr><tr><td><code>node.kubernetes.io/unschedulable</code></td><td>NoSchedule</td><td>1.12+</td><td>DaemonSet pods tolerate unschedulable attributes by default scheduler.</td></tr><tr><td><code>node.kubernetes.io/network-unavailable</code></td><td>NoSchedule</td><td>1.12+</td><td>DaemonSet pods, who uses host network, tolerate network-unavailable attributes by default scheduler.</td></tr></tbody></table><h2 id=communicating-with-daemon-pods>Communicating with Daemon Pods</h2><p>Some possible patterns for communicating with Pods in a DaemonSet are:</p><ul><li><strong>Push</strong>: Pods in the DaemonSet are configured to send updates to another service, such
as a stats database. They do not have clients.</li><li><strong>NodeIP and Known Port</strong>: Pods in the DaemonSet can use a <code>hostPort</code>, so that the pods
are reachable via the node IPs.
Clients know the list of node IPs somehow, and know the port by convention.</li><li><strong>DNS</strong>: Create a <a href=/docs/concepts/services-networking/service/#headless-services>headless service</a>
with the same pod selector, and then discover DaemonSets using the <code>endpoints</code>
resource or retrieve multiple A records from DNS.</li><li><strong>Service</strong>: Create a service with the same Pod selector, and use the service to reach a
daemon on a random node. (No way to reach specific node.)</li></ul><h2 id=updating-a-daemonset>Updating a DaemonSet</h2><p>If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete
Pods from newly not-matching nodes.</p><p>You can modify the Pods that a DaemonSet creates. However, Pods do not allow all
fields to be updated. Also, the DaemonSet controller will use the original template the next
time a node (even with the same name) is created.</p><p>You can delete a DaemonSet. If you specify <code>--cascade=orphan</code> with <code>kubectl</code>, then the Pods
will be left on the nodes. If you subsequently create a new DaemonSet with the same selector,
the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces
them according to its <code>updateStrategy</code>.</p><p>You can <a href=/docs/tasks/manage-daemon/update-daemon-set/>perform a rolling update</a> on a DaemonSet.</p><h2 id=alternatives-to-daemonset>Alternatives to DaemonSet</h2><h3 id=init-scripts>Init scripts</h3><p>It is certainly possible to run daemon processes by directly starting them on a node (e.g. using
<code>init</code>, <code>upstartd</code>, or <code>systemd</code>). This is perfectly fine. However, there are several advantages to
running such processes via a DaemonSet:</p><ul><li>Ability to monitor and manage logs for daemons in the same way as applications.</li><li>Same config language and tools (e.g. Pod templates, <code>kubectl</code>) for daemons and applications.</li><li>Running daemons in containers with resource limits increases isolation between daemons from app
containers. However, this can also be accomplished by running the daemons in a container but not in a Pod.</li></ul><h3 id=bare-pods>Bare Pods</h3><p>It is possible to create Pods directly which specify a particular node to run on. However,
a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of
node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should
use a DaemonSet rather than creating individual Pods.</p><h3 id=static-pods>Static Pods</h3><p>It is possible to create Pods by writing a file to a certain directory watched by Kubelet. These
are called <a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a>.
Unlike DaemonSet, static Pods cannot be managed with kubectl
or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful
in cluster bootstrapping cases. Also, static Pods may be deprecated in the future.</p><h3 id=deployments>Deployments</h3><p>DaemonSets are similar to <a href=/docs/concepts/workloads/controllers/deployment/>Deployments</a> in that
they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,
storage servers).</p><p>Use a Deployment for stateless services, like frontends, where scaling up and down the
number of replicas and rolling out updates are more important than controlling exactly which host
the Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on
all or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.</p><p>For example, <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>network plugins</a> often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.<ul><li>Learn about <a href=#static-pods>static Pods</a>, which are useful for running Kubernetes
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> components.</li></ul></li><li>Find out how to use DaemonSets<ul><li><a href=/docs/tasks/manage-daemon/update-daemon-set/>Perform a rolling update on a DaemonSet</a></li><li><a href=/docs/tasks/manage-daemon/rollback-daemon-set/>Perform a rollback on a DaemonSet</a>
(for example, if a roll out didn't work how you expected).</li></ul></li><li>Understand <a href=/docs/concepts/scheduling-eviction/assign-pod-node/>how Kubernetes assigns Pods to Nodes</a>.</li><li>Learn about <a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>device plugins</a> and
<a href=/docs/concepts/cluster-administration/addons/>add ons</a>, which often run as DaemonSets.</li><li><code>DaemonSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/daemon-set-v1/>DaemonSet</a>
object definition to understand the API for daemon sets.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cc7cc3c4907039d9f863162e20bfbbef>5.2.5 - Jobs</h1><p>A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.
As pods successfully complete, the Job tracks the successful completions. When a specified number
of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up
the Pods it created. Suspending a Job will delete its active Pods until the Job
is resumed again.</p><p>A simple case is to create one Job object in order to reliably run one Pod to completion.
The Job object will start a new Pod if the first Pod fails or is deleted (for example
due to a node hardware failure or a node reboot).</p><p>You can also use a Job to run multiple Pods in parallel.</p><p>If you want to run a Job (either a single task, or several in parallel) on a schedule,
see <a href=/docs/concepts/workloads/controllers/cron-jobs/>CronJob</a>.</p><h2 id=running-an-example-job>Running an example Job</h2><p>Here is an example Job config. It computes π to 2000 places and prints it out.
It takes around 10s to complete.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/job.yaml download=controllers/job.yaml><code>controllers/job.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-job-yaml")' title="Copy controllers/job.yaml to clipboard"></img></div><div class=includecode id=controllers-job-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl:5.34.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>4</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>You can run the example with this command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>job.batch/pi created
</code></pre><p>Check on the status of the Job with <code>kubectl</code>:</p><ul class="nav nav-tabs" id=check-status-of-job role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#check-status-of-job-0 role=tab aria-controls=check-status-of-job-0 aria-selected=true>kubectl describe job pi</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#check-status-of-job-1 role=tab aria-controls=check-status-of-job-1>kubectl get job pi -o yaml</a></li></ul><div class=tab-content id=check-status-of-job><div id=check-status-of-job-0 class="tab-pane show active" role=tabpanel aria-labelledby=check-status-of-job-0><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>Name:           pi
</span></span><span style=display:flex><span>Namespace:      default
</span></span><span style=display:flex><span>Selector:       controller-uid<span style=color:#666>=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span style=display:flex><span>Labels:         controller-uid<span style=color:#666>=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span style=display:flex><span>                job-name<span style=color:#666>=</span>pi
</span></span><span style=display:flex><span>Annotations:    kubectl.kubernetes.io/last-applied-configuration:
</span></span><span style=display:flex><span>                  <span style=color:#666>{</span><span style=color:#b44>&#34;apiVersion&#34;</span>:<span style=color:#b44>&#34;batch/v1&#34;</span>,<span style=color:#b44>&#34;kind&#34;</span>:<span style=color:#b44>&#34;Job&#34;</span>,<span style=color:#b44>&#34;metadata&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;annotations&#34;</span>:<span style=color:#666>{}</span>,<span style=color:#b44>&#34;name&#34;</span>:<span style=color:#b44>&#34;pi&#34;</span>,<span style=color:#b44>&#34;namespace&#34;</span>:<span style=color:#b44>&#34;default&#34;</span><span style=color:#666>}</span>,<span style=color:#b44>&#34;spec&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;backoffLimit&#34;</span>:4,<span style=color:#b44>&#34;template&#34;</span>:...
</span></span><span style=display:flex><span>Parallelism:    <span style=color:#666>1</span>
</span></span><span style=display:flex><span>Completions:    <span style=color:#666>1</span>
</span></span><span style=display:flex><span>Start Time:     Mon, <span style=color:#666>02</span> Dec <span style=color:#666>2019</span> 15:20:11 +0200
</span></span><span style=display:flex><span>Completed At:   Mon, <span style=color:#666>02</span> Dec <span style=color:#666>2019</span> 15:21:16 +0200
</span></span><span style=display:flex><span>Duration:       65s
</span></span><span style=display:flex><span>Pods Statuses:  <span style=color:#666>0</span> Running / <span style=color:#666>1</span> Succeeded / <span style=color:#666>0</span> Failed
</span></span><span style=display:flex><span>Pod Template:
</span></span><span style=display:flex><span>  Labels:  controller-uid<span style=color:#666>=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span style=display:flex><span>           job-name<span style=color:#666>=</span>pi
</span></span><span style=display:flex><span>  Containers:
</span></span><span style=display:flex><span>   pi:
</span></span><span style=display:flex><span>    Image:      perl:5.34.0
</span></span><span style=display:flex><span>    Port:       &lt;none&gt;
</span></span><span style=display:flex><span>    Host Port:  &lt;none&gt;
</span></span><span style=display:flex><span>    Command:
</span></span><span style=display:flex><span>      perl
</span></span><span style=display:flex><span>      -Mbignum<span style=color:#666>=</span>bpi
</span></span><span style=display:flex><span>      -wle
</span></span><span style=display:flex><span>      print bpi<span style=color:#666>(</span>2000<span style=color:#666>)</span>
</span></span><span style=display:flex><span>    Environment:  &lt;none&gt;
</span></span><span style=display:flex><span>    Mounts:       &lt;none&gt;
</span></span><span style=display:flex><span>  Volumes:        &lt;none&gt;
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type    Reason            Age   From            Message
</span></span><span style=display:flex><span>  ----    ------            ----  ----            -------
</span></span><span style=display:flex><span>  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7
</span></span></code></pre></div></div><div id=check-status-of-job-1 class=tab-pane role=tabpanel aria-labelledby=check-status-of-job-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>apiVersion: batch/v1
</span></span><span style=display:flex><span>kind: Job
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |
</span></span><span style=display:flex><span>      <span style=color:#666>{</span><span style=color:#b44>&#34;apiVersion&#34;</span>:<span style=color:#b44>&#34;batch/v1&#34;</span>,<span style=color:#b44>&#34;kind&#34;</span>:<span style=color:#b44>&#34;Job&#34;</span>,<span style=color:#b44>&#34;metadata&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;annotations&#34;</span>:<span style=color:#666>{}</span>,<span style=color:#b44>&#34;name&#34;</span>:<span style=color:#b44>&#34;pi&#34;</span>,<span style=color:#b44>&#34;namespace&#34;</span>:<span style=color:#b44>&#34;default&#34;</span><span style=color:#666>}</span>,<span style=color:#b44>&#34;spec&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;backoffLimit&#34;</span>:4,<span style=color:#b44>&#34;template&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;spec&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;containers&#34;</span>:<span style=color:#666>[{</span><span style=color:#b44>&#34;command&#34;</span>:<span style=color:#666>[</span><span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#b44>&#34;print bpi(2000)&#34;</span><span style=color:#666>]</span>,<span style=color:#b44>&#34;image&#34;</span>:<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#b44>&#34;name&#34;</span>:<span style=color:#b44>&#34;pi&#34;</span><span style=color:#666>}]</span>,<span style=color:#b44>&#34;restartPolicy&#34;</span>:<span style=color:#b44>&#34;Never&#34;</span><span style=color:#666>}}}}</span>
</span></span><span style=display:flex><span>  creationTimestamp: <span style=color:#b44>&#34;2022-06-15T08:40:15Z&#34;</span>
</span></span><span style=display:flex><span>  generation: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style=display:flex><span>    job-name: pi
</span></span><span style=display:flex><span>  name: pi
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#b44>&#34;987&#34;</span>
</span></span><span style=display:flex><span>  uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  backoffLimit: <span style=color:#666>4</span>
</span></span><span style=display:flex><span>  completionMode: NonIndexed
</span></span><span style=display:flex><span>  completions: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  parallelism: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style=display:flex><span>  suspend: <span style=color:#a2f>false</span>
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      creationTimestamp: null
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style=display:flex><span>        job-name: pi
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - command:
</span></span><span style=display:flex><span>        - perl
</span></span><span style=display:flex><span>        - -Mbignum<span style=color:#666>=</span>bpi
</span></span><span style=display:flex><span>        - -wle
</span></span><span style=display:flex><span>        - print bpi<span style=color:#666>(</span>2000<span style=color:#666>)</span>
</span></span><span style=display:flex><span>        image: perl:5.34.0
</span></span><span style=display:flex><span>        imagePullPolicy: Always
</span></span><span style=display:flex><span>        name: pi
</span></span><span style=display:flex><span>        resources: <span style=color:#666>{}</span>
</span></span><span style=display:flex><span>        terminationMessagePath: /dev/termination-log
</span></span><span style=display:flex><span>        terminationMessagePolicy: File
</span></span><span style=display:flex><span>      dnsPolicy: ClusterFirst
</span></span><span style=display:flex><span>      restartPolicy: Never
</span></span><span style=display:flex><span>      schedulerName: default-scheduler
</span></span><span style=display:flex><span>      securityContext: <span style=color:#666>{}</span>
</span></span><span style=display:flex><span>      terminationGracePeriodSeconds: <span style=color:#666>30</span>
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  active: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  ready: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  startTime: <span style=color:#b44>&#34;2022-06-15T08:40:15Z&#34;</span>
</span></span></code></pre></div></div></div><p>To view completed Pods of a Job, use <code>kubectl get pods</code>.</p><p>To list all the Pods that belong to a Job in a machine readable form, you can use a command like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>pods</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl get pods --selector<span style=color:#666>=</span>job-name<span style=color:#666>=</span>pi --output<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.items[*].metadata.name}&#39;</span><span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>pi-5rwd7
</code></pre><p>Here, the selector is the same as the selector for the Job. The <code>--output=jsonpath</code> option specifies an expression
with the name from each Pod in the returned list.</p><p>View the standard output of one of the pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
</code></pre><h2 id=writing-a-job-spec>Writing a Job spec</h2><p>As with all other Kubernetes config, a Job needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.
Its name must be a valid <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>A Job also needs a <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> is the only required field of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>pod template</a>. It has exactly the same schema as a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a pod template in a Job must specify appropriate
labels (see <a href=#pod-selector>pod selector</a>) and an appropriate restart policy.</p><p>Only a <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>RestartPolicy</code></a> equal to <code>Never</code> or <code>OnFailure</code> is allowed.</p><h3 id=pod-selector>Pod selector</h3><p>The <code>.spec.selector</code> field is optional. In almost all cases you should not specify it.
See section <a href=#specifying-your-own-pod-selector>specifying your own pod selector</a>.</p><h3 id=parallel-jobs>Parallel execution for Jobs</h3><p>There are three main types of task suitable to run as a Job:</p><ol><li>Non-parallel Jobs<ul><li>normally, only one Pod is started, unless the Pod fails.</li><li>the Job is complete as soon as its Pod terminates successfully.</li></ul></li><li>Parallel Jobs with a <em>fixed completion count</em>:<ul><li>specify a non-zero positive value for <code>.spec.completions</code>.</li><li>the Job represents the overall task, and is complete when there are <code>.spec.completions</code> successful Pods.</li><li>when using <code>.spec.completionMode="Indexed"</code>, each Pod gets a different index in the range 0 to <code>.spec.completions-1</code>.</li></ul></li><li>Parallel Jobs with a <em>work queue</em>:<ul><li>do not specify <code>.spec.completions</code>, default to <code>.spec.parallelism</code>.</li><li>the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.</li><li>each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done.</li><li>when <em>any</em> Pod from the Job terminates with success, no new Pods are created.</li><li>once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.</li><li>once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They should all be in the process of exiting.</li></ul></li></ol><p>For a <em>non-parallel</em> Job, you can leave both <code>.spec.completions</code> and <code>.spec.parallelism</code> unset. When both are
unset, both are defaulted to 1.</p><p>For a <em>fixed completion count</em> Job, you should set <code>.spec.completions</code> to the number of completions needed.
You can set <code>.spec.parallelism</code>, or leave it unset and it will default to 1.</p><p>For a <em>work queue</em> Job, you must leave <code>.spec.completions</code> unset, and set <code>.spec.parallelism</code> to
a non-negative integer.</p><p>For more information about how to make use of the different types of job, see the <a href=#job-patterns>job patterns</a> section.</p><h4 id=controlling-parallelism>Controlling parallelism</h4><p>The requested parallelism (<code>.spec.parallelism</code>) can be set to any non-negative value.
If it is unspecified, it defaults to 1.
If it is specified as 0, then the Job is effectively paused until it is increased.</p><p>Actual parallelism (number of pods running at any instant) may be more or less than requested
parallelism, for a variety of reasons:</p><ul><li>For <em>fixed completion count</em> Jobs, the actual number of pods running in parallel will not exceed the number of
remaining completions. Higher values of <code>.spec.parallelism</code> are effectively ignored.</li><li>For <em>work queue</em> Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.</li><li>If the Job <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a> has not had time to react.</li><li>If the Job controller failed to create Pods for any reason (lack of <code>ResourceQuota</code>, lack of permission, etc.),
then there may be fewer pods than requested.</li><li>The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.</li><li>When a Pod is gracefully shut down, it takes time to stop.</li></ul><h3 id=completion-mode>Completion mode</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>Jobs with <em>fixed completion count</em> - that is, jobs that have non null
<code>.spec.completions</code> - can have a completion mode that is specified in <code>.spec.completionMode</code>:</p><ul><li><p><code>NonIndexed</code> (default): the Job is considered complete when there have been
<code>.spec.completions</code> successfully completed Pods. In other words, each Pod
completion is homologous to each other. Note that Jobs that have null
<code>.spec.completions</code> are implicitly <code>NonIndexed</code>.</p></li><li><p><code>Indexed</code>: the Pods of a Job get an associated completion index from 0 to
<code>.spec.completions-1</code>. The index is available through three mechanisms:</p><ul><li>The Pod annotation <code>batch.kubernetes.io/job-completion-index</code>.</li><li>As part of the Pod hostname, following the pattern <code>$(job-name)-$(index)</code>.
When you use an Indexed Job in combination with a
<a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>, Pods within the Job can use
the deterministic hostnames to address each other via DNS. For more information about
how to configure this, see <a href=/docs/tasks/job/job-with-pod-to-pod-communication/>Job with Pod-to-Pod Communication</a>.</li><li>From the containerized task, in the environment variable <code>JOB_COMPLETION_INDEX</code>.</li></ul><p>The Job is considered complete when there is one successfully completed Pod
for each index. For more information about how to use this mode, see
<a href=/docs/tasks/job/indexed-parallel-processing-static/>Indexed Job for Parallel Processing with Static Work Assignment</a>.
Note that, although rare, more than one Pod could be started for the same
index, but only one of them will count towards the completion count.</p></li></ul><h2 id=handling-pod-and-container-failures>Handling Pod and container failures</h2><p>A container in a Pod may fail for a number of reasons, such as because the process in it exited with
a non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this
happens, and the <code>.spec.template.spec.restartPolicy = "OnFailure"</code>, then the Pod stays
on the node, but the container is re-run. Therefore, your program needs to handle the case when it is
restarted locally, or else specify <code>.spec.template.spec.restartPolicy = "Never"</code>.
See <a href=/docs/concepts/workloads/pods/pod-lifecycle/#example-states>pod lifecycle</a> for more information on <code>restartPolicy</code>.</p><p>An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node
(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the
<code>.spec.template.spec.restartPolicy = "Never"</code>. When a Pod fails, then the Job controller
starts a new Pod. This means that your application needs to handle the case when it is restarted in a new
pod. In particular, it needs to handle temporary files, locks, incomplete output and the like
caused by previous runs.</p><p>Note that even if you specify <code>.spec.parallelism = 1</code> and <code>.spec.completions = 1</code> and
<code>.spec.template.spec.restartPolicy = "Never"</code>, the same program may
sometimes be started twice.</p><p>If you do specify <code>.spec.parallelism</code> and <code>.spec.completions</code> both greater than 1, then there may be
multiple pods running at once. Therefore, your pods must also be tolerant of concurrency.</p><h3 id=pod-backoff-failure-policy>Pod backoff failure policy</h3><p>There are situations where you want to fail a Job after some amount of retries
due to a logical error in configuration etc.
To do so, set <code>.spec.backoffLimit</code> to specify the number of retries before
considering a Job as failed. The back-off limit is set by default to 6. Failed
Pods associated with the Job are recreated by the Job controller with an
exponential back-off delay (10s, 20s, 40s ...) capped at six minutes.</p><p>The number of retries is calculated in two ways:</p><ul><li>The number of Pods with <code>.status.phase = "Failed"</code>.</li><li>When using <code>restartPolicy = "OnFailure"</code>, the number of retries in all the
containers of Pods with <code>.status.phase</code> equal to <code>Pending</code> or <code>Running</code>.</li></ul><p>If either of the calculations reaches the <code>.spec.backoffLimit</code>, the Job is
considered failed.</p><p>When the <a href=#job-tracking-with-finalizers><code>JobTrackingWithFinalizers</code></a> feature is
disabled, the number of failed Pods is only based on Pods that are still present
in the API.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If your job has <code>restartPolicy = "OnFailure"</code>, keep in mind that your Pod running the Job
will be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult. We suggest setting
<code>restartPolicy = "Never"</code> when debugging the Job or using a logging system to ensure output
from failed Jobs is not lost inadvertently.</div><h2 id=job-termination-and-cleanup>Job termination and cleanup</h2><p>When a Job completes, no more Pods are created, but the Pods are <a href=#pod-backoff-failure-policy>usually</a> not deleted either.
Keeping them around
allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.
The job object also remains after it is completed so that you can view its status. It is up to the user to delete
old jobs after noting their status. Delete the job with <code>kubectl</code> (e.g. <code>kubectl delete jobs/pi</code> or <code>kubectl delete -f ./job.yaml</code>). When you delete the job using <code>kubectl</code>, all the pods it created are deleted too.</p><p>By default, a Job will run uninterrupted unless a Pod fails (<code>restartPolicy=Never</code>) or a Container exits in error (<code>restartPolicy=OnFailure</code>), at which point the Job defers to the
<code>.spec.backoffLimit</code> described above. Once <code>.spec.backoffLimit</code> has been reached the Job will be marked as failed and any running Pods will be terminated.</p><p>Another way to terminate a Job is by setting an active deadline.
Do this by setting the <code>.spec.activeDeadlineSeconds</code> field of the Job to a number of seconds.
The <code>activeDeadlineSeconds</code> applies to the duration of the job, no matter how many Pods are created.
Once a Job reaches <code>activeDeadlineSeconds</code>, all of its running Pods are terminated and the Job status will become <code>type: Failed</code> with <code>reason: DeadlineExceeded</code>.</p><p>Note that a Job's <code>.spec.activeDeadlineSeconds</code> takes precedence over its <code>.spec.backoffLimit</code>. Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it reaches the time limit specified by <code>activeDeadlineSeconds</code>, even if the <code>backoffLimit</code> is not yet reached.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi-with-timeout<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>activeDeadlineSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl:5.34.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><p>Note that both the Job spec and the <a href=/docs/concepts/workloads/pods/init-containers/#detailed-behavior>Pod template spec</a> within the Job have an <code>activeDeadlineSeconds</code> field. Ensure that you set this field at the proper level.</p><p>Keep in mind that the <code>restartPolicy</code> applies to the Pod, and not to the Job itself: there is no automatic Job restart once the Job status is <code>type: Failed</code>.
That is, the Job termination mechanisms activated with <code>.spec.activeDeadlineSeconds</code> and <code>.spec.backoffLimit</code> result in a permanent Job failure that requires manual intervention to resolve.</p><h2 id=clean-up-finished-jobs-automatically>Clean up finished jobs automatically</h2><p>Finished Jobs are usually no longer needed in the system. Keeping them around in
the system will put pressure on the API server. If the Jobs are managed directly
by a higher level controller, such as
<a href=/docs/concepts/workloads/controllers/cron-jobs/>CronJobs</a>, the Jobs can be
cleaned up by CronJobs based on the specified capacity-based cleanup policy.</p><h3 id=ttl-mechanism-for-finished-jobs>TTL mechanism for finished Jobs</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code></div><p>Another way to clean up finished Jobs (either <code>Complete</code> or <code>Failed</code>)
automatically is to use a TTL mechanism provided by a
<a href=/docs/concepts/workloads/controllers/ttlafterfinished/>TTL controller</a> for
finished resources, by specifying the <code>.spec.ttlSecondsAfterFinished</code> field of
the Job.</p><p>When the TTL controller cleans up the Job, it will delete the Job cascadingly,
i.e. delete its dependent objects, such as Pods, together with the Job. Note
that when the Job is deleted, its lifecycle guarantees, such as finalizers, will
be honored.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi-with-ttl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ttlSecondsAfterFinished</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl:5.34.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><p>The Job <code>pi-with-ttl</code> will be eligible to be automatically deleted, <code>100</code>
seconds after it finishes.</p><p>If the field is set to <code>0</code>, the Job will be eligible to be automatically deleted
immediately after it finishes. If the field is unset, this Job won't be cleaned
up by the TTL controller after it finishes.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>It is recommended to set <code>ttlSecondsAfterFinished</code> field because unmanaged jobs
(Jobs that you created directly, and not indirectly through other workload APIs
such as CronJob) have a default deletion
policy of <code>orphanDependents</code> causing Pods created by an unmanaged Job to be left around
after that Job is fully deleted.
Even though the <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> eventually
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection>garbage collects</a>
the Pods from a deleted Job after they either fail or complete, sometimes those
lingering pods may cause cluster performance degradation or in worst case cause the
cluster to go offline due to this degradation.</p><p>You can use <a href=/docs/concepts/policy/limit-range/>LimitRanges</a> and
<a href=/docs/concepts/policy/resource-quotas/>ResourceQuotas</a> to place a
cap on the amount of resources that a particular namespace can
consume.</p></div><h2 id=job-patterns>Job patterns</h2><p>The Job object can be used to support reliable parallel execution of Pods. The Job object is not
designed to support closely-communicating parallel processes, as commonly found in scientific
computing. It does support parallel processing of a set of independent but related <em>work items</em>.
These might be emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a
NoSQL database to scan, and so on.</p><p>In a complex system, there may be multiple different sets of work items. Here we are just
considering one set of work items that the user wants to manage together — a <em>batch job</em>.</p><p>There are several different patterns for parallel computation, each with strengths and weaknesses.
The tradeoffs are:</p><ul><li>One Job object for each work item, vs. a single Job object for all work items. The latter is
better for large numbers of work items. The former creates some overhead for the user and for the
system to manage large numbers of Job objects.</li><li>Number of pods created equals number of work items, vs. each Pod can process multiple work items.
The former typically requires less modification to existing code and containers. The latter
is better for large numbers of work items, for similar reasons to the previous bullet.</li><li>Several approaches use a work queue. This requires running a queue service,
and modifications to the existing program or container to make it use the work queue.
Other approaches are easier to adapt to an existing containerised application.</li></ul><p>The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.
The pattern names are also links to examples and more detailed description.</p><table><thead><tr><th>Pattern</th><th style=text-align:center>Single Job object</th><th style=text-align:center>Fewer pods than work items?</th><th style=text-align:center>Use app unmodified?</th></tr></thead><tbody><tr><td><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Queue with Pod Per Work Item</a></td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>sometimes</td></tr><tr><td><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Queue with Variable Pod Count</a></td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center></td></tr><tr><td><a href=/docs/tasks/job/indexed-parallel-processing-static/>Indexed Job with Static Work Assignment</a></td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>✓</td></tr><tr><td><a href=/docs/tasks/job/parallel-processing-expansion/>Job Template Expansion</a></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>✓</td></tr><tr><td><a href=/docs/tasks/job/job-with-pod-to-pod-communication/>Job with Pod-to-Pod Communication</a></td><td style=text-align:center>✓</td><td style=text-align:center>sometimes</td><td style=text-align:center>sometimes</td></tr></tbody></table><p>When you specify completions with <code>.spec.completions</code>, each Pod created by the Job controller
has an identical <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>spec</code></a>. This means that
all pods for a task will have the same command line and the same
image, the same volumes, and (almost) the same environment variables. These patterns
are different ways to arrange for pods to work on different things.</p><p>This table shows the required settings for <code>.spec.parallelism</code> and <code>.spec.completions</code> for each of the patterns.
Here, <code>W</code> is the number of work items.</p><table><thead><tr><th>Pattern</th><th style=text-align:center><code>.spec.completions</code></th><th style=text-align:center><code>.spec.parallelism</code></th></tr></thead><tbody><tr><td><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Queue with Pod Per Work Item</a></td><td style=text-align:center>W</td><td style=text-align:center>any</td></tr><tr><td><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Queue with Variable Pod Count</a></td><td style=text-align:center>null</td><td style=text-align:center>any</td></tr><tr><td><a href=/docs/tasks/job/indexed-parallel-processing-static/>Indexed Job with Static Work Assignment</a></td><td style=text-align:center>W</td><td style=text-align:center>any</td></tr><tr><td><a href=/docs/tasks/job/parallel-processing-expansion/>Job Template Expansion</a></td><td style=text-align:center>1</td><td style=text-align:center>should be 1</td></tr><tr><td><a href=/docs/tasks/job/job-with-pod-to-pod-communication/>Job with Pod-to-Pod Communication</a></td><td style=text-align:center>W</td><td style=text-align:center>W</td></tr></tbody></table><h2 id=advanced-usage>Advanced usage</h2><h3 id=suspending-a-job>Suspending a Job</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>When a Job is created, the Job controller will immediately begin creating Pods
to satisfy the Job's requirements and will continue to do so until the Job is
complete. However, you may want to temporarily suspend a Job's execution and
resume it later, or start Jobs in suspended state and have a custom controller
decide later when to start them.</p><p>To suspend a Job, you can update the <code>.spec.suspend</code> field of
the Job to true; later, when you want to resume it again, update it to false.
Creating a Job with <code>.spec.suspend</code> set to true will create it in the suspended
state.</p><p>When a Job is resumed from suspension, its <code>.status.startTime</code> field will be
reset to the current time. This means that the <code>.spec.activeDeadlineSeconds</code>
timer will be stopped and reset when a Job is suspended and resumed.</p><p>When you suspend a Job, any running Pods that don't have a status of <code>Completed</code> will be <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>terminated</a>.
with a SIGTERM signal. The Pod's graceful termination period will be honored and
your Pod must handle this signal in this period. This may involve saving
progress for later or undoing changes. Pods terminated this way will not count
towards the Job's <code>completions</code> count.</p><p>An example Job definition in the suspended state can be like so:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get job myjob -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myjob<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>suspend</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parallelism</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>completions</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>You can also toggle Job suspension by patching the Job using the command line.</p><p>Suspend an active Job:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl patch job/myjob --type<span style=color:#666>=</span>strategic --patch <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;suspend&#34;:true}}&#39;</span>
</span></span></code></pre></div><p>Resume a suspended Job:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl patch job/myjob --type<span style=color:#666>=</span>strategic --patch <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;suspend&#34;:false}}&#39;</span>
</span></span></code></pre></div><p>The Job's status can be used to determine if a Job is suspended or has been
suspended in the past:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get jobs/myjob -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># .metadata and .spec omitted</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>conditions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>lastProbeTime</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2021-02-05T13:14:33Z&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2021-02-05T13:14:33Z&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;True&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Suspended<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>startTime</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2021-02-05T13:13:48Z&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The Job condition of type "Suspended" with status "True" means the Job is
suspended; the <code>lastTransitionTime</code> field can be used to determine how long the
Job has been suspended for. If the status of that condition is "False", then the
Job was previously suspended and is now running. If such a condition does not
exist in the Job's status, the Job has never been stopped.</p><p>Events are also created when the Job is suspended and resumed:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe jobs/myjob
</span></span></code></pre></div><pre tabindex=0><code>Name:           myjob
...
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
  Normal  Suspended         11m   job-controller  Job suspended
  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
  Normal  Resumed           3s    job-controller  Job resumed
</code></pre><p>The last four events, particularly the "Suspended" and "Resumed" events, are
directly a result of toggling the <code>.spec.suspend</code> field. In the time between
these two events, we see that no Pods were created, but Pod creation restarted
as soon as the Job was resumed.</p><h3 id=mutable-scheduling-directives>Mutable Scheduling Directives</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In order to use this behavior, you must enable the <code>JobMutableNodeSchedulingDirectives</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>API server</a>.
It is enabled by default.</div><p>In most cases a parallel job will want the pods to run with constraints,
like all in the same zone, or all either on GPU model x or y but not a mix of both.</p><p>The <a href=#suspending-a-job>suspend</a> field is the first step towards achieving those semantics. Suspend allows a
custom queue controller to decide when a job should start; However, once a job is unsuspended,
a custom queue controller has no influence on where the pods of a job will actually land.</p><p>This feature allows updating a Job's scheduling directives before it starts, which gives custom queue
controllers the ability to influence pod placement while at the same time offloading actual
pod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never
been unsuspended before.</p><p>The fields in a Job's pod template that can be updated are node affinity, node selector,
tolerations, labels and annotations.</p><h3 id=specifying-your-own-pod-selector>Specifying your own Pod selector</h3><p>Normally, when you create a Job object, you do not specify <code>.spec.selector</code>.
The system defaulting logic adds this field when the Job is created.
It picks a selector value that will not overlap with any other jobs.</p><p>However, in some cases, you might need to override this automatically set selector.
To do this, you can specify the <code>.spec.selector</code> of the Job.</p><p>Be very careful when doing this. If you specify a label selector which is not
unique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated
job may be deleted, or this Job may count other Pods as completing it, or one or both
Jobs may refuse to create Pods or run to completion. If a non-unique selector is
chosen, then other controllers (e.g. ReplicationController) and their Pods may behave
in unpredictable ways too. Kubernetes will not stop you from making a mistake when
specifying <code>.spec.selector</code>.</p><p>Here is an example of a case when you might want to use this feature.</p><p>Say Job <code>old</code> is already running. You want existing Pods
to keep running, but you want the rest of the Pods it creates
to use a different pod template and for the Job to have a new name.
You cannot update the Job because these fields are not updatable.
Therefore, you delete Job <code>old</code> but <em>leave its pods
running</em>, using <code>kubectl delete jobs/old --cascade=orphan</code>.
Before deleting it, you make a note of what selector it uses:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get job old -o yaml
</span></span></code></pre></div><p>The output is similar to this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>old<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>controller-uid</span>:<span style=color:#bbb> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>Then you create a new Job with name <code>new</code> and you explicitly specify the same selector.
Since the existing Pods have label <code>controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002</code>,
they are controlled by Job <code>new</code> as well.</p><p>You need to specify <code>manualSelector: true</code> in the new Job since you are not using
the selector that the system normally generates for you automatically.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>manualSelector</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>controller-uid</span>:<span style=color:#bbb> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>The new Job itself will have a different uid from <code>a8f3d00d-c6d2-11e5-9f87-42010af00002</code>. Setting
<code>manualSelector: true</code> tells the system that you know what you are doing and to allow this
mismatch.</p><h3 id=pod-failure-policy>Pod failure policy</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You can only configure a Pod failure policy for a Job if you have the
<code>JobPodFailurePolicy</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
enabled in your cluster. Additionally, it is recommended
to enable the <code>PodDisruptionConditions</code> feature gate in order to be able to detect and handle
Pod disruption conditions in the Pod failure policy (see also:
<a href=/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions>Pod disruption conditions</a>). Both feature gates are
available in Kubernetes v1.25.</div><p>A Pod failure policy, defined with the <code>.spec.podFailurePolicy</code> field, enables
your cluster to handle Pod failures based on the container exit codes and the
Pod conditions.</p><p>In some situations, you may want to have a better control when handling Pod
failures than the control provided by the <a href=#pod-backoff-failure-policy>Pod backoff failure policy</a>,
which is based on the Job's <code>.spec.backoffLimit</code>. These are some examples of use cases:</p><ul><li>To optimize costs of running workloads by avoiding unnecessary Pod restarts,
you can terminate a Job as soon as one of its Pods fails with an exit code
indicating a software bug.</li><li>To guarantee that your Job finishes even if there are disruptions, you can
ignore Pod failures caused by disruptions (such <a class=glossary-tooltip title='Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption target=_blank aria-label=preemption>preemption</a>,
<a class=glossary-tooltip title='API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/api-eviction/ target=_blank aria-label='API-initiated eviction'>API-initiated eviction</a>
or <a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=taint>taint</a>-based eviction) so
that they don't count towards the <code>.spec.backoffLimit</code> limit of retries.</li></ul><p>You can configure a Pod failure policy, in the <code>.spec.podFailurePolicy</code> field,
to meet the above use cases. This policy can handle Pod failures based on the
container exit codes and the Pod conditions.</p><p>Here is a manifest for a Job that defines a <code>podFailurePolicy</code>:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-pod-failure-policy-example.yaml download=/controllers/job-pod-failure-policy-example.yaml><code>/controllers/job-pod-failure-policy-example.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-job-pod-failure-policy-example-yaml")' title="Copy /controllers/job-pod-failure-policy-example.yaml to clipboard"></img></div><div class=includecode id=controllers-job-pod-failure-policy-example-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>job-pod-failure-policy-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>completions</span>:<span style=color:#bbb> </span><span style=color:#666>12</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parallelism</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>main<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>docker.io/library/bash:5<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;bash&#34;</span>]<span style=color:#bbb>        </span><span style=color:#080;font-style:italic># example command simulating a bug which triggers the FailJob action</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- echo &#34;Hello world!&#34; &amp;&amp; sleep 5 &amp;&amp; exit 42<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>6</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podFailurePolicy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>action</span>:<span style=color:#bbb> </span>FailJob<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>onExitCodes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>containerName</span>:<span style=color:#bbb> </span>main     <span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator: In             # one of</span>:<span style=color:#bbb> </span>In, NotIn<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#666>42</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>action: Ignore             # one of</span>:<span style=color:#bbb> </span>Ignore, FailJob, Count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>onPodConditions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>DisruptionTarget  <span style=color:#bbb> </span><span style=color:#080;font-style:italic># indicates Pod disruption</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>In the example above, the first rule of the Pod failure policy specifies that
the Job should be marked failed if the <code>main</code> container fails with the 42 exit
code. The following are the rules for the <code>main</code> container specifically:</p><ul><li>an exit code of 0 means that the container succeeded</li><li>an exit code of 42 means that the <strong>entire Job</strong> failed</li><li>any other exit code represents that the container failed, and hence the entire
Pod. The Pod will be re-created if the total number of restarts is
below <code>backoffLimit</code>. If the <code>backoffLimit</code> is reached the <strong>entire Job</strong> failed.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Because the Pod template specifies a <code>restartPolicy: Never</code>,
the kubelet does not restart the <code>main</code> container in that particular Pod.</div><p>The second rule of the Pod failure policy, specifying the <code>Ignore</code> action for
failed Pods with condition <code>DisruptionTarget</code> excludes Pod disruptions from
being counted towards the <code>.spec.backoffLimit</code> limit of retries.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If the Job failed, either by the Pod failure policy or Pod backoff
failure policy, and the Job is running multiple Pods, Kubernetes terminates all
the Pods in that Job that are still Pending or Running.</div><p>These are some requirements and semantics of the API:</p><ul><li>if you want to use a <code>.spec.podFailurePolicy</code> field for a Job, you must
also define that Job's pod template with <code>.spec.restartPolicy</code> set to <code>Never</code>.</li><li>the Pod failure policy rules you specify under <code>spec.podFailurePolicy.rules</code>
are evaluated in order. Once a rule matches a Pod failure, the remaining rules
are ignored. When no rule matches the Pod failure, the default
handling applies.</li><li>you may want to restrict a rule to a specific container by specifing its name
in<code>spec.podFailurePolicy.rules[*].containerName</code>. When not specified the rule
applies to all containers. When specified, it should match one the container
or <code>initContainer</code> names in the Pod template.</li><li>you may specify the action taken when a Pod failure policy is matched by
<code>spec.podFailurePolicy.rules[*].action</code>. Possible values are:<ul><li><code>FailJob</code>: use to indicate that the Pod's job should be marked as Failed and
all running Pods should be terminated.</li><li><code>Ignore</code>: use to indicate that the counter towards the <code>.spec.backoffLimit</code>
should not be incremented and a replacement Pod should be created.</li><li><code>Count</code>: use to indicate that the Pod should be handled in the default way.
The counter towards the <code>.spec.backoffLimit</code> should be incremented.</li></ul></li></ul><h3 id=job-tracking-with-finalizers>Job tracking with finalizers</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>In order to use this behavior, you must enable the <code>JobTrackingWithFinalizers</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>API server</a>
and the <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>controller manager</a>.</p><p>When enabled, the control plane tracks new Jobs using the behavior described
below. Jobs created before the feature was enabled are unaffected. As a user,
the only difference you would see is that the control plane tracking of Job
completion is more accurate.</p></div><p>When this feature isn't enabled, the Job <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a>
relies on counting the Pods that exist in the cluster to track the Job status,
that is, to keep the counters for <code>succeeded</code> and <code>failed</code> Pods.
However, Pods can be removed for a number of reasons, including:</p><ul><li>The garbage collector that removes orphan Pods when a Node goes down.</li><li>The garbage collector that removes finished Pods (in <code>Succeeded</code> or <code>Failed</code>
phase) after a threshold.</li><li>Human intervention to delete Pods belonging to a Job.</li><li>An external controller (not provided as part of Kubernetes) that removes or
replaces Pods.</li></ul><p>If you enable the <code>JobTrackingWithFinalizers</code> feature for your cluster, the
control plane keeps track of the Pods that belong to any Job and notices if any
such Pod is removed from the API server. To do that, the Job controller creates Pods with
the finalizer <code>batch.kubernetes.io/job-tracking</code>. The controller removes the
finalizer only after the Pod has been accounted for in the Job status, allowing
the Pod to be removed by other controllers or users.</p><p>The Job controller uses the new algorithm for new Jobs only. Jobs created
before the feature is enabled are unaffected. You can determine if the Job
controller is tracking a Job using Pod finalizers by checking if the Job has the
annotation <code>batch.kubernetes.io/job-tracking</code>. You should <strong>not</strong> manually add
or remove this annotation from Jobs.</p><h2 id=alternatives>Alternatives</h2><h3 id=bare-pods>Bare Pods</h3><p>When the node that a Pod is running on reboots or fails, the pod is terminated
and will not be restarted. However, a Job will create new Pods to replace terminated ones.
For this reason, we recommend that you use a Job rather than a bare Pod, even if your application
requires only a single Pod.</p><h3 id=replication-controller>Replication Controller</h3><p>Jobs are complementary to <a href=/docs/concepts/workloads/controllers/replicationcontroller/>Replication Controllers</a>.
A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job
manages Pods that are expected to terminate (e.g. batch tasks).</p><p>As discussed in <a href=/docs/concepts/workloads/pods/pod-lifecycle/>Pod Lifecycle</a>, <code>Job</code> is <em>only</em> appropriate
for pods with <code>RestartPolicy</code> equal to <code>OnFailure</code> or <code>Never</code>.
(Note: If <code>RestartPolicy</code> is not set, the default value is <code>Always</code>.)</p><h3 id=single-job-starts-controller-pod>Single Job starts controller Pod</h3><p>Another pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort
of custom controller for those Pods. This allows the most flexibility, but may be somewhat
complicated to get started with and offers less integration with Kubernetes.</p><p>One example of this pattern would be a Job which starts a Pod which runs a script that in turn
starts a Spark master controller (see <a href=https://github.com/kubernetes/examples/tree/master/staging/spark/README.md>spark example</a>), runs a spark
driver, and then cleans up.</p><p>An advantage of this approach is that the overall process gets the completion guarantee of a Job
object, but maintains complete control over what Pods are created and how work is assigned to them.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li>Read about different ways of running Jobs:<ul><li><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Coarse Parallel Processing Using a Work Queue</a></li><li><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Fine Parallel Processing Using a Work Queue</a></li><li>Use an <a href=/docs/tasks/job/indexed-parallel-processing-static/>indexed Job for parallel processing with static work assignment</a></li><li>Create multiple Jobs based on a template: <a href=/docs/tasks/job/parallel-processing-expansion/>Parallel Processing using Expansions</a></li></ul></li><li>Follow the links within <a href=#clean-up-finished-jobs-automatically>Clean up finished jobs automatically</a>
to learn more about how your cluster can clean up completed and / or failed tasks.</li><li><code>Job</code> is part of the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/job-v1/>Job</a>
object definition to understand the API for jobs.</li><li>Read about <a href=/docs/concepts/workloads/controllers/cron-jobs/><code>CronJob</code></a>, which you
can use to define a series of Jobs that will run based on a schedule, similar to
the UNIX tool <code>cron</code>.</li><li>Practice how to configure handling of retriable and non-retriable pod failures
using <code>podFailurePolicy</code>, based on the step-by-step <a href=/docs/tasks/job/pod-failure-policy/>examples</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4de50a37ebb6f2340484192126cb7a04>5.2.6 - Automatic Clean-up for Finished Jobs</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code></div><p>TTL-after-finished <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> provides a
TTL (time to live) mechanism to limit the lifetime of resource objects that
have finished execution. TTL controller only handles
<a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Jobs>Jobs</a>.</p><h2 id=ttl-after-finished-controller>TTL-after-finished Controller</h2><p>The TTL-after-finished controller is only supported for Jobs. A cluster operator can use this feature to clean
up finished Jobs (either <code>Complete</code> or <code>Failed</code>) automatically by specifying the
<code>.spec.ttlSecondsAfterFinished</code> field of a Job, as in this
<a href=/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically>example</a>.
The TTL-after-finished controller will assume that a job is eligible to be cleaned up
TTL seconds after the job has finished, in other words, when the TTL has expired. When the
TTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete
its dependent objects together with it. Note that when the job is deleted,
its lifecycle guarantees, such as finalizers, will be honored.</p><p>The TTL seconds can be set at any time. Here are some examples for setting the
<code>.spec.ttlSecondsAfterFinished</code> field of a Job:</p><ul><li>Specify this field in the job manifest, so that a Job can be cleaned up
automatically some time after it finishes.</li><li>Set this field of existing, already finished jobs, to adopt this new
feature.</li><li>Use a
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>mutating admission webhook</a>
to set this field dynamically at job creation time. Cluster administrators can
use this to enforce a TTL policy for finished jobs.</li><li>Use a
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>mutating admission webhook</a>
to set this field dynamically after the job has finished, and choose
different TTL values based on job status, labels, etc.</li></ul><h2 id=caveat>Caveat</h2><h3 id=updating-ttl-seconds>Updating TTL Seconds</h3><p>Note that the TTL period, e.g. <code>.spec.ttlSecondsAfterFinished</code> field of Jobs,
can be modified after the job is created or has finished. However, once the
Job becomes eligible to be deleted (when the TTL has expired), the system won't
guarantee that the Jobs will be kept, even if an update to extend the TTL
returns a successful API response.</p><h3 id=time-skew>Time Skew</h3><p>Because TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to
determine whether the TTL has expired or not, this feature is sensitive to time
skew in the cluster, which may cause TTL-after-finish controller to clean up job objects
at the wrong time.</p><p>Clocks aren't always correct, but the difference should be
very small. Please be aware of this risk when setting a non-zero TTL.</p><h2 id=what-s-next>What's next</h2><ul><li><p><a href=/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically>Clean up Jobs automatically</a></p></li><li><p><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md>Design doc</a></p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2e4cec01c525b45eccd6010e21cc76d9>5.2.7 - CronJob</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>A <em>CronJob</em> creates <a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Jobs>Jobs</a> on a repeating schedule.</p><p>One CronJob object is like one line of a <em>crontab</em> (cron table) file. It runs a job periodically
on a given schedule, written in <a href=https://en.wikipedia.org/wiki/Cron>Cron</a> format.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>All <strong>CronJob</strong> <code>schedule:</code> times are based on the timezone of the
<a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>.</p><p>If your control plane runs the kube-controller-manager in Pods or bare
containers, the timezone set for the kube-controller-manager container determines the timezone
that the cron job controller uses.</p></div><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>The <a href=/docs/reference/kubernetes-api/workload-resources/cron-job-v1/>v1 CronJob API</a>
does not officially support setting timezone as explained above.</p><p>Setting variables such as <code>CRON_TZ</code> or <code>TZ</code> is not officially supported by the Kubernetes project.
<code>CRON_TZ</code> or <code>TZ</code> is an implementation detail of the internal library being used
for parsing and calculating the next Job creation time. Any usage of it is not
recommended in a production cluster.</p></div><p>When creating the manifest for a CronJob resource, make sure the name you provide
is a valid <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.
The name must be no longer than 52 characters. This is because the CronJob controller will automatically
append 11 characters to the job name provided and there is a constraint that the
maximum length of a Job name is no more than 63 characters.</p><h2 id=cronjob>CronJob</h2><p>CronJobs are meant for performing regular scheduled actions such as backups,
report generation, and so on. Each of those tasks should be configured to recur
indefinitely (for example: once a day / week / month); you can define the point
in time within that interval when the job should start.</p><h3 id=example>Example</h3><p>This example CronJob manifest prints the current time and a hello message every minute:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/cronjob.yaml download=application/job/cronjob.yaml><code>application/job/cronjob.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("application-job-cronjob-yaml")' title="Copy application/job/cronjob.yaml to clipboard"></img></div><div class=includecode id=application-job-cronjob-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>CronJob<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>schedule</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;* * * * *&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>jobTemplate</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- date; echo Hello from the Kubernetes cluster<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>OnFailure<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>(<a href=/docs/tasks/job/automated-tasks-with-cron-jobs/>Running Automated Tasks with a CronJob</a>
takes you through this example in more detail).</p><h3 id=cron-schedule-syntax>Cron schedule syntax</h3><pre tabindex=0><code># ┌───────────── minute (0 - 59)
# │ ┌───────────── hour (0 - 23)
# │ │ ┌───────────── day of the month (1 - 31)
# │ │ │ ┌───────────── month (1 - 12)
# │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;
# │ │ │ │ │                                   7 is also Sunday on some systems)
# │ │ │ │ │                                   OR sun, mon, tue, wed, thu, fri, sat
# │ │ │ │ │
# * * * * *
</code></pre><table><thead><tr><th>Entry</th><th>Description</th><th>Equivalent to</th></tr></thead><tbody><tr><td>@yearly (or @annually)</td><td>Run once a year at midnight of 1 January</td><td>0 0 1 1 *</td></tr><tr><td>@monthly</td><td>Run once a month at midnight of the first day of the month</td><td>0 0 1 * *</td></tr><tr><td>@weekly</td><td>Run once a week at midnight on Sunday morning</td><td>0 0 * * 0</td></tr><tr><td>@daily (or @midnight)</td><td>Run once a day at midnight</td><td>0 0 * * *</td></tr><tr><td>@hourly</td><td>Run once an hour at the beginning of the hour</td><td>0 * * * *</td></tr></tbody></table><p>For example, the line below states that the task must be started every Friday at midnight, as well as on the 13th of each month at midnight:</p><p><code>0 0 13 * 5</code></p><p>To generate CronJob schedule expressions, you can also use web tools like <a href=https://crontab.guru/>crontab.guru</a>.</p><h2 id=time-zones>Time zones</h2><p>For CronJobs with no time zone specified, the kube-controller-manager interprets schedules relative to its local time zone.</p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [beta]</code></div><p>If you enable the <code>CronJobTimeZone</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>,
you can specify a time zone for a CronJob (if you don't enable that feature gate, or if you are using a version of
Kubernetes that does not have experimental time zone support, all CronJobs in your cluster have an unspecified
timezone).</p><p>When you have the feature enabled, you can set <code>spec.timeZone</code> to the name of a valid <a href=https://en.wikipedia.org/wiki/List_of_tz_database_time_zones>time zone</a>. For example, setting
<code>spec.timeZone: "Etc/UTC"</code> instructs Kubernetes to interpret the schedule relative to Coordinated Universal Time.</p><p>A time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.</p><h2 id=cron-job-limitations>CronJob limitations</h2><p>A cron job creates a job object <em>about</em> once per execution time of its schedule. We say "about" because there
are certain circumstances where two jobs might be created, or no job might be created. We attempt to make these rare,
but do not completely prevent them. Therefore, jobs should be <em>idempotent</em>.</p><p>If <code>startingDeadlineSeconds</code> is set to a large value or left unset (the default)
and if <code>concurrencyPolicy</code> is set to <code>Allow</code>, the jobs will always run
at least once.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> If <code>startingDeadlineSeconds</code> is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.</div><p>For every CronJob, the CronJob <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a> checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs the error.</p><pre tabindex=0><code>Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
</code></pre><p>It is important to note that if the <code>startingDeadlineSeconds</code> field is set (not <code>nil</code>), the controller counts how many missed jobs occurred from the value of <code>startingDeadlineSeconds</code> until now rather than from the last scheduled time until now. For example, if <code>startingDeadlineSeconds</code> is <code>200</code>, the controller counts how many missed jobs occurred in the last 200 seconds.</p><p>A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if <code>concurrencyPolicy</code> is set to <code>Forbid</code> and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.</p><p>For example, suppose a CronJob is set to schedule a new Job every one minute beginning at <code>08:30:00</code>, and its
<code>startingDeadlineSeconds</code> field is not set. If the CronJob controller happens to
be down from <code>08:29:00</code> to <code>10:21:00</code>, the job will not start as the number of missed jobs which missed their schedule is greater than 100.</p><p>To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at <code>08:30:00</code>, and its
<code>startingDeadlineSeconds</code> is set to 200 seconds. If the CronJob controller happens to
be down for the same period as the previous example (<code>08:29:00</code> to <code>10:21:00</code>,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.</p><p>The CronJob is only responsible for creating Jobs that match its schedule, and
the Job in turn is responsible for the management of the Pods it represents.</p><h2 id=new-controller>Controller version</h2><p>Starting with Kubernetes v1.21 the second version of the CronJob controller
is the default implementation. To disable the default CronJob controller
and use the original CronJob controller instead, pass the <code>CronJobControllerV2</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
flag to the <a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>,
and set this flag to <code>false</code>. For example:</p><pre tabindex=0><code>--feature-gates=&#34;CronJobControllerV2=false&#34;
</code></pre><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods/>Pods</a> and
<a href=/docs/concepts/workloads/controllers/job/>Jobs</a>, two concepts
that CronJobs rely upon.</li><li>Read about the <a href=https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format>format</a>
of CronJob <code>.spec.schedule</code> fields.</li><li>For instructions on creating and working with CronJobs, and for an example
of a CronJob manifest,
see <a href=/docs/tasks/job/automated-tasks-with-cron-jobs/>Running automated tasks with CronJobs</a>.</li><li>For instructions to clean up failed or completed jobs automatically,
see <a href=/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically>Clean up Jobs automatically</a></li><li><code>CronJob</code> is part of the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/cron-job-v1/>CronJob</a>
object definition to understand the API for Kubernetes cron jobs.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-27f1331d515d95f76aa1156088b4ad91>5.2.8 - ReplicationController</h1><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> that configures a <a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a> is now the recommended way to set up replication.</div><p>A <em>ReplicationController</em> ensures that a specified number of pod replicas are running at any one
time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is
always up and available.</p><h2 id=how-a-replicationcontroller-works>How a ReplicationController Works</h2><p>If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the
ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a
ReplicationController are automatically replaced if they fail, are deleted, or are terminated.
For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.
For this reason, you should use a ReplicationController even if your application requires
only a single pod. A ReplicationController is similar to a process supervisor,
but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods
across multiple nodes.</p><p>ReplicationController is often abbreviated to "rc" in discussion, and as a shortcut in
kubectl commands.</p><p>A simple case is to create one ReplicationController object to reliably run one instance of
a Pod indefinitely. A more complex use case is to run several identical replicas of a replicated
service, such as web servers.</p><h2 id=running-an-example-replicationcontroller>Running an example ReplicationController</h2><p>This example ReplicationController config runs three copies of the nginx web server.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/replication.yaml download=controllers/replication.yaml><code>controllers/replication.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-replication-yaml")' title="Copy controllers/replication.yaml to clipboard"></img></div><div class=includecode id=controllers-replication-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicationController<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Run the example job by downloading the example file and then running this command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>replicationcontroller/nginx created
</code></pre><p>Check on the status of the ReplicationController using this command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe replicationcontrollers/nginx
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
</code></pre><p>Here, three pods are created, but none is running yet, perhaps because the image is being pulled.
A little later, the same command may show:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Pods Status:    <span style=color:#666>3</span> Running / <span style=color:#666>0</span> Waiting / <span style=color:#666>0</span> Succeeded / <span style=color:#666>0</span> Failed
</span></span></code></pre></div><p>To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>pods</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl get pods --selector<span style=color:#666>=</span><span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx --output<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>={</span>.items..metadata.name<span style=color:#666>}</span><span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>nginx-3ntk0 nginx-4ok8v nginx-qrm3m
</code></pre><p>Here, the selector is the same as the selector for the ReplicationController (seen in the
<code>kubectl describe</code> output), and in a different form in <code>replication.yaml</code>. The <code>--output=jsonpath</code> option
specifies an expression with the name from each pod in the returned list.</p><h2 id=writing-a-replicationcontroller-spec>Writing a ReplicationController Spec</h2><p>As with all other Kubernetes config, a ReplicationController needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.
The name of a ReplicationController object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.
For general information about working with configuration files, see <a href=/docs/concepts/overview/working-with-objects/object-management/>object management</a>.</p><p>A ReplicationController also needs a <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> is the only required field of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>pod template</a>. It has exactly the same schema as a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href=#pod-selector>pod selector</a>.</p><p>Only a <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>.spec.template.spec.restartPolicy</code></a> equal to <code>Always</code> is allowed, which is the default if not specified.</p><p>For local container restarts, ReplicationControllers delegate to an agent on the node,
for example the <a href=/docs/reference/command-line-tools-reference/kubelet/>Kubelet</a>.</p><h3 id=labels-on-the-replicationcontroller>Labels on the ReplicationController</h3><p>The ReplicationController can itself have labels (<code>.metadata.labels</code>). Typically, you
would set these the same as the <code>.spec.template.metadata.labels</code>; if <code>.metadata.labels</code> is not specified
then it defaults to <code>.spec.template.metadata.labels</code>. However, they are allowed to be
different, and the <code>.metadata.labels</code> do not affect the behavior of the ReplicationController.</p><h3 id=pod-selector>Pod Selector</h3><p>The <code>.spec.selector</code> field is a <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>label selector</a>. A ReplicationController
manages all the pods with labels that match the selector. It does not distinguish
between pods that it created or deleted and pods that another person or process created or
deleted. This allows the ReplicationController to be replaced without affecting the running pods.</p><p>If specified, the <code>.spec.template.metadata.labels</code> must be equal to the <code>.spec.selector</code>, or it will
be rejected by the API. If <code>.spec.selector</code> is unspecified, it will be defaulted to
<code>.spec.template.metadata.labels</code>.</p><p>Also you should not normally create any pods whose labels match this selector, either directly, with
another ReplicationController, or with another controller such as Job. If you do so, the
ReplicationController thinks that it created the other pods. Kubernetes does not stop you
from doing this.</p><p>If you do end up with multiple controllers that have overlapping selectors, you
will have to manage the deletion yourself (see <a href=#working-with-replicationcontrollers>below</a>).</p><h3 id=multiple-replicas>Multiple Replicas</h3><p>You can specify how many pods should run concurrently by setting <code>.spec.replicas</code> to the number
of pods you would like to have running concurrently. The number running at any time may be higher
or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully
shutdown, and a replacement starts early.</p><p>If you do not specify <code>.spec.replicas</code>, then it defaults to 1.</p><h2 id=working-with-replicationcontrollers>Working with ReplicationControllers</h2><h3 id=deleting-a-replicationcontroller-and-its-pods>Deleting a ReplicationController and its Pods</h3><p>To delete a ReplicationController and all its pods, use <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>. Kubectl will scale the ReplicationController to zero and wait
for it to delete each pod before deleting the ReplicationController itself. If this kubectl
command is interrupted, it can be restarted.</p><p>When using the REST API or <a href=/docs/reference/using-api/client-libraries>client library</a>, you need to do the steps explicitly (scale replicas to
0, wait for pod deletions, then delete the ReplicationController).</p><h3 id=deleting-only-a-replicationcontroller>Deleting only a ReplicationController</h3><p>You can delete a ReplicationController without affecting any of its pods.</p><p>Using kubectl, specify the <code>--cascade=orphan</code> option to <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>.</p><p>When using the REST API or <a href=/docs/reference/using-api/client-libraries>client library</a>, you can delete the ReplicationController object.</p><p>Once the original is deleted, you can create a new ReplicationController to replace it. As long
as the old and new <code>.spec.selector</code> are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update pods to a new spec in a controlled way, use a <a href=#rolling-updates>rolling update</a>.</p><h3 id=isolating-pods-from-a-replicationcontroller>Isolating pods from a ReplicationController</h3><p>Pods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).</p><h2 id=common-usage-patterns>Common usage patterns</h2><h3 id=rescheduling>Rescheduling</h3><p>As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).</p><h3 id=scaling>Scaling</h3><p>The ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the <code>replicas</code> field.</p><h3 id=rolling-updates>Rolling updates</h3><p>The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.</p><p>As explained in <a href=https://issue.k8s.io/1353>#1353</a>, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.</p><p>Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.</p><p>The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.</p><h3 id=multiple-release-tracks>Multiple release tracks</h3><p>In addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.</p><p>For instance, a service might target all pods with <code>tier in (frontend), environment in (prod)</code>. Now say you have 10 replicated pods that make up this tier. But you want to be able to 'canary' a new version of this component. You could set up a ReplicationController with <code>replicas</code> set to 9 for the bulk of the replicas, with labels <code>tier=frontend, environment=prod, track=stable</code>, and another ReplicationController with <code>replicas</code> set to 1 for the canary, with labels <code>tier=frontend, environment=prod, track=canary</code>. Now the service is covering both the canary and non-canary pods. But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.</p><h3 id=using-replicationcontrollers-with-services>Using ReplicationControllers with Services</h3><p>Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.</p><p>A ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.</p><h2 id=writing-programs-for-replication>Writing programs for Replication</h2><p>Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the <a href=https://www.rabbitmq.com/tutorials/tutorial-two-python.html>RabbitMQ work queues</a>, as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.</p><h2 id=responsibilities-of-the-replicationcontroller>Responsibilities of the ReplicationController</h2><p>The ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, <a href=https://issue.k8s.io/620>readiness</a> and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.</p><p>The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in <a href=https://issue.k8s.io/492>#492</a>), which would change its <code>replicas</code> field. We will not add scheduling policies (for example, <a href=https://issue.k8s.io/367#issuecomment-48428019>spreading</a>) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation (<a href=https://issue.k8s.io/170>#170</a>).</p><p>The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The "macro" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like <a href=https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1>Asgard</a> managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.</p><h2 id=api-object>API Object</h2><p>Replication controller is a top-level resource in the Kubernetes REST API. More details about the
API object can be found at:
<a href=/docs/reference/generated/kubernetes-api/v1.25/#replicationcontroller-v1-core>ReplicationController API object</a>.</p><h2 id=alternatives-to-replicationcontroller>Alternatives to ReplicationController</h2><h3 id=replicaset>ReplicaSet</h3><p><a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a> is the next-generation ReplicationController that supports the new <a href=/docs/concepts/overview/working-with-objects/labels/#set-based-requirement>set-based label selector</a>.
It's mainly used by <a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a> as a mechanism to orchestrate pod creation, deletion and updates.
Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.</p><h3 id=deployment-recommended>Deployment (Recommended)</h3><p><a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want the rolling update functionality, because they are declarative, server-side, and have additional features.</p><h3 id=bare-pods>Bare Pods</h3><p>Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.</p><h3 id=job>Job</h3><p>Use a <a href=/docs/concepts/workloads/controllers/job/><code>Job</code></a> instead of a ReplicationController for pods that are expected to terminate on their own
(that is, batch jobs).</p><h3 id=daemonset>DaemonSet</h3><p>Use a <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li>Learn about <a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a>, the replacement
for ReplicationController.</li><li><code>ReplicationController</code> is part of the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/replication-controller-v1/>ReplicationController</a>
object definition to understand the API for replication controllers.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0a0a7eca3e302a3c08f8c85e15d337fd>6 - Services, Load Balancing, and Networking</h1><div class=lead>Concepts and resources behind networking in Kubernetes.</div><h2 id=the-kubernetes-network-model>The Kubernetes network model</h2><p>Every <a href=/docs/concepts/workloads/pods/><code>Pod</code></a> in a cluster gets its own unique cluster-wide IP address.
This means you do not need to explicitly create links between <code>Pods</code> and you
almost never need to deal with mapping container ports to host ports.<br>This creates a clean, backwards-compatible model where <code>Pods</code> can be treated
much like VMs or physical hosts from the perspectives of port allocation,
naming, service discovery, <a href=/docs/concepts/services-networking/ingress/#load-balancing>load balancing</a>,
application configuration, and migration.</p><p>Kubernetes imposes the following fundamental requirements on any networking
implementation (barring any intentional network segmentation policies):</p><ul><li>pods can communicate with all other pods on any other <a href=/docs/concepts/architecture/nodes/>node</a>
without NAT</li><li>agents on a node (e.g. system daemons, kubelet) can communicate with all
pods on that node</li></ul><p>Note: For those platforms that support <code>Pods</code> running in the host network (e.g.
Linux), when pods are attached to the host network of a node they can still communicate
with all pods on all nodes without NAT.</p><p>This model is not only less complex overall, but it is principally compatible
with the desire for Kubernetes to enable low-friction porting of apps from VMs
to containers. If your job previously ran in a VM, your VM had an IP and could
talk to other VMs in your project. This is the same basic model.</p><p>Kubernetes IP addresses exist at the <code>Pod</code> scope - containers within a <code>Pod</code>
share their network namespaces - including their IP address and MAC address.
This means that containers within a <code>Pod</code> can all reach each other's ports on
<code>localhost</code>. This also means that containers within a <code>Pod</code> must coordinate port
usage, but this is no different from processes in a VM. This is called the
"IP-per-pod" model.</p><p>How this is implemented is a detail of the particular container runtime in use.</p><p>It is possible to request ports on the <code>Node</code> itself which forward to your <code>Pod</code>
(called host ports), but this is a very niche operation. How that forwarding is
implemented is also a detail of the container runtime. The <code>Pod</code> itself is
blind to the existence or non-existence of host ports.</p><p>Kubernetes networking addresses four concerns:</p><ul><li>Containers within a Pod <a href=/docs/concepts/services-networking/dns-pod-service/>use networking to communicate</a> via loopback.</li><li>Cluster networking provides communication between different Pods.</li><li>The <a href=/docs/concepts/services-networking/service/>Service</a> API lets you
<a href=/docs/tutorials/services/connect-applications-service/>expose an application running in Pods</a>
to be reachable from outside your cluster.<ul><li><a href=/docs/concepts/services-networking/ingress/>Ingress</a> provides extra functionality
specifically for exposing HTTP applications, websites and APIs.</li></ul></li><li>You can also use Services to
<a href=/docs/concepts/services-networking/service-traffic-policy/>publish services only for consumption inside your cluster</a>.</li></ul><p>The <a href=/docs/tutorials/services/connect-applications-service/>Connecting Applications with Services</a> tutorial lets you learn about Services and Kubernetes networking with a hands-on example.</p><p><a href=/docs/concepts/cluster-administration/networking/>Cluster Networking</a> explains how to set
up networking for your cluster, and also provides an overview of the technologies involved.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-5701136fd2ce258047b6ddc389112352>6.1 - Service</h1><div class=lead>Expose an application running in your cluster behind a single outward-facing endpoint, even when the workload is split across multiple backends.</div>An abstract way to expose an application running on a set of <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> as a network service.<p>With Kubernetes you don't need to modify your application to use an unfamiliar service discovery mechanism.
Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods,
and can load-balance across them.</p><h2 id=motivation>Motivation</h2><p>Kubernetes <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> are created and destroyed
to match the desired state of your cluster. Pods are nonpermanent resources.
If you use a <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a> to run your app,
it can create and destroy Pods dynamically.</p><p>Each Pod gets its own IP address, however in a Deployment, the set of Pods
running in one moment in time could be different from
the set of Pods running that application a moment later.</p><p>This leads to a problem: if some set of Pods (call them "backends") provides
functionality to other Pods (call them "frontends") inside your cluster,
how do the frontends find out and keep track of which IP address to connect
to, so that the frontend can use the backend part of the workload?</p><p>Enter <em>Services</em>.</p><h2 id=service-resource>Service resources</h2><p>In Kubernetes, a Service is an abstraction which defines a logical set of Pods
and a policy by which to access them (sometimes this pattern is called
a micro-service). The set of Pods targeted by a Service is usually determined
by a <a class=glossary-tooltip title='Allows users to filter a list of resources based on labels.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=selector>selector</a>.
To learn about other ways to define Service endpoints,
see <a href=#services-without-selectors>Services <em>without</em> selectors</a>.</p><p>For example, consider a stateless image-processing backend which is running with
3 replicas. Those replicas are fungible—frontends do not care which backend
they use. While the actual Pods that compose the backend set may change, the
frontend clients should not need to be aware of that, nor should they need to keep
track of the set of backends themselves.</p><p>The Service abstraction enables this decoupling.</p><h3 id=cloud-native-service-discovery>Cloud-native service discovery</h3><p>If you're able to use Kubernetes APIs for service discovery in your application,
you can query the <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a>
for matching EndpointSlices. Kubernetes updates the EndpointSlices for a Service
whenever the set of Pods in a Service changes.</p><p>For non-native applications, Kubernetes offers ways to place a network port or load
balancer in between your application and the backend Pods.</p><h2 id=defining-a-service>Defining a Service</h2><p>A Service in Kubernetes is a REST object, similar to a Pod. Like all of the
REST objects, you can <code>POST</code> a Service definition to the API server to create
a new instance.
The name of a Service object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#rfc-1035-label-names>RFC 1035 label name</a>.</p><p>For example, suppose you have a set of Pods where each listens on TCP port 9376
and contains a label <code>app.kubernetes.io/name=MyApp</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>This specification creates a new Service object named "my-service", which
targets TCP port 9376 on any Pod with the <code>app.kubernetes.io/name=MyApp</code> label.</p><p>Kubernetes assigns this Service an IP address (sometimes called the "cluster IP"),
which is used by the Service proxies
(see <a href=#virtual-ips-and-service-proxies>Virtual IPs and service proxies</a> below).</p><p>The controller for the Service selector continuously scans for Pods that
match its selector, and then POSTs any updates to an Endpoint object
also named "my-service".</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A Service can map <em>any</em> incoming <code>port</code> to a <code>targetPort</code>. By default and
for convenience, the <code>targetPort</code> is set to the same value as the <code>port</code>
field.</div><p>Port definitions in Pods have names, and you can reference these names in the
<code>targetPort</code> attribute of a Service. For example, we can bind the <code>targetPort</code>
of the Service to the Pod port in the following way:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>proxy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:stable<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>http-web-svc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>proxy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>name-of-service-port<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span>http-web-svc<span style=color:#bbb>
</span></span></span></code></pre></div><p>This works even if there is a mixture of Pods in the Service using a single
configured name, with the same network protocol available via different
port numbers. This offers a lot of flexibility for deploying and evolving
your Services. For example, you can change the port numbers that Pods expose
in the next version of your backend software, without breaking clients.</p><p>The default protocol for Services is
<a href=/docs/reference/networking/service-protocols/#protocol-tcp>TCP</a>; you can also
use any other <a href=/docs/reference/networking/service-protocols/>supported protocol</a>.</p><p>As many Services need to expose more than one port, Kubernetes supports multiple
port definitions on a Service object.
Each port definition can have the same <code>protocol</code>, or a different one.</p><h3 id=services-without-selectors>Services without selectors</h3><p>Services most commonly abstract access to Kubernetes Pods thanks to the selector,
but when used with a corresponding set of
<a class=glossary-tooltip title='A way to group network endpoints together with Kubernetes resources.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/endpoint-slices/ target=_blank aria-label=EndpointSlices>EndpointSlices</a>
objects and without a selector, the Service can abstract other kinds of backends,
including ones that run outside the cluster.</p><p>For example:</p><ul><li>You want to have an external database cluster in production, but in your
test environment you use your own databases.</li><li>You want to point your Service to a Service in a different
<a class=glossary-tooltip title='An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=Namespace>Namespace</a> or on another cluster.</li><li>You are migrating a workload to Kubernetes. While evaluating the approach,
you run only a portion of your backends in Kubernetes.</li></ul><p>In any of these scenarios you can define a Service <em>without</em> a Pod selector.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Because this Service has no selector, the corresponding EndpointSlice (and
legacy Endpoints) objects are not created automatically. You can manually map the Service
to the network address and port where it's running, by adding an EndpointSlice
object manually. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>discovery.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>EndpointSlice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service-1<span style=color:#bbb> </span><span style=color:#080;font-style:italic># by convention, use the name of the Service</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                     </span><span style=color:#080;font-style:italic># as a prefix for the name of the EndpointSlice</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># You should set the &#34;kubernetes.io/service-name&#34; label.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># Set its value to match the name of the Service</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/service-name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>addressType</span>:<span style=color:#bbb> </span>IPv4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#39;&#39;</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># empty because port 9376 is not assigned as a well-known</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>             </span><span style=color:#080;font-style:italic># port (by IANA)</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>appProtocol</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>endpoints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>addresses</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;10.4.5.6&#34;</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># the IP addresses in this list can appear in any order</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;10.1.2.3&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=custom-endpointslices>Custom EndpointSlices</h4><p>When you create an <a href=#endpointslices>EndpointSlice</a> object for a Service, you can
use any name for the EndpointSlice. Each EndpointSlice in a namespace must have a
unique name. You link an EndpointSlice to a Service by setting the
<code>kubernetes.io/service-name</code> <a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=label>label</a>
on that EndpointSlice.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>The endpoint IPs <em>must not</em> be: loopback (127.0.0.0/8 for IPv4, ::1/128 for IPv6), or
link-local (169.254.0.0/16 and 224.0.0.0/24 for IPv4, fe80::/64 for IPv6).</p><p>The endpoint IP addresses cannot be the cluster IPs of other Kubernetes Services,
because <a class=glossary-tooltip title='kube-proxy is a network proxy that runs on each node in the cluster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a> doesn't support virtual IPs
as a destination.</p></div><p>For an EndpointSlice that you create yourself, or in your own code,
you should also pick a value to use for the <a href=/docs/reference/labels-annotations-taints/#endpointslicekubernetesiomanaged-by><code>endpointslice.kubernetes.io/managed-by</code></a> label.
If you create your own controller code to manage EndpointSlices, consider using a
value similar to <code>"my-domain.example/name-of-controller"</code>. If you are using a third
party tool, use the name of the tool in all-lowercase and change spaces and other
punctuation to dashes (<code>-</code>).
If people are directly using a tool such as <code>kubectl</code> to manage EndpointSlices,
use a name that describes this manual management, such as <code>"staff"</code> or
<code>"cluster-admins"</code>. You should
avoid using the reserved value <code>"controller"</code>, which identifies EndpointSlices
managed by Kubernetes' own control plane.</p><h4 id=service-no-selector-access>Accessing a Service without a selector</h4><p>Accessing a Service without a selector works the same as if it had a selector.
In the <a href=#services-without-selectors>example</a> for a Service without a selector, traffic is routed to one of the two endpoints defined in
the EndpointSlice manifest: a TCP connection to 10.1.2.3 or 10.4.5.6, on port 9376.</p><p>An ExternalName Service is a special case of Service that does not have
selectors and uses DNS names instead. For more information, see the
<a href=#externalname>ExternalName</a> section later in this document.</p><h3 id=endpointslices>EndpointSlices</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p><a href=/docs/concepts/services-networking/endpoint-slices/>EndpointSlices</a> are objects that
represent a subset (a <em>slice</em>) of the backing network endpoints for a Service.</p><p>Your Kubernetes cluster tracks how many endpoints each EndpointSlice represents.
If there are so many endpoints for a Service that a threshold is reached, then
Kubernetes adds another empty EndpointSlice and stores new endpoint information
there.
By default, Kubernetes makes a new EndpointSlice once the existing EndpointSlices
all contain at least 100 endpoints. Kubernetes does not make the new EndpointSlice
until an extra endpoint needs to be added.</p><p>See <a href=/docs/concepts/services-networking/endpoint-slices/>EndpointSlices</a> for more
information about this API.</p><h3 id=endpoints>Endpoints</h3><p>In the Kubernetes API, an
<a href=/docs/reference/kubernetes-api/service-resources/endpoints-v1/>Endpoints</a>
(the resource kind is plural) defines a list of network endpoints, typically
referenced by a Service to define which Pods the traffic can be sent to.</p><p>The EndpointSlice API is the recommended replacement for Endpoints.</p><h4 id=over-capacity-endpoints>Over-capacity endpoints</h4><p>Kubernetes limits the number of endpoints that can fit in a single Endpoints
object. When there are over 1000 backing endpoints for a Service, Kubernetes
truncates the data in the Endpoints object. Because a Service can be linked
with more than one EndpointSlice, the 1000 backing endpoint limit only
affects the legacy Endpoints API.</p><p>In that case, Kubernetes selects at most 1000 possible backend endpoints to store
into the Endpoints object, and sets an
<a class=glossary-tooltip title='A key-value pair that is used to attach arbitrary non-identifying metadata to objects.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/annotations target=_blank aria-label=annotation>annotation</a> on the
Endpoints:
<a href=/docs/reference/labels-annotations-taints/#endpoints-kubernetes-io-over-capacity><code>endpoints.kubernetes.io/over-capacity: truncated</code></a>.
The control plane also removes that annotation if the number of backend Pods drops below 1000.</p><p>Traffic is still sent to backends, but any load balancing mechanism that relies on the
legacy Endpoints API only sends traffic to at most 1000 of the available backing endpoints.</p><p>The same API limit means that you cannot manually update an Endpoints to have more than 1000 endpoints.</p><h3 id=application-protocol>Application protocol</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>The <code>appProtocol</code> field provides a way to specify an application protocol for
each Service port. The value of this field is mirrored by the corresponding
Endpoints and EndpointSlice objects.</p><p>This field follows standard Kubernetes label syntax. Values should either be
<a href=https://www.iana.org/assignments/service-names>IANA standard service names</a> or
domain prefixed names such as <code>mycompany.com/my-custom-protocol</code>.</p><h2 id=multi-port-services>Multi-Port Services</h2><p>For some Services, you need to expose more than one port.
Kubernetes lets you configure multiple port definitions on a Service object.
When using multiple ports for a Service, you must give all of your ports names
so that these are unambiguous.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>https<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>443</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9377</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>As with Kubernetes <a class=glossary-tooltip title='A client-provided string that refers to an object in a resource URL, such as /api/v1/pods/some-name.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/names target=_blank aria-label=names>names</a> in general, names for ports
must only contain lowercase alphanumeric characters and <code>-</code>. Port names must
also start and end with an alphanumeric character.</p><p>For example, the names <code>123-abc</code> and <code>web</code> are valid, but <code>123_abc</code> and <code>-web</code> are not.</p></div><h2 id=choosing-your-own-ip-address>Choosing your own IP address</h2><p>You can specify your own cluster IP address as part of a <code>Service</code> creation
request. To do this, set the <code>.spec.clusterIP</code> field. For example, if you
already have an existing DNS entry that you wish to reuse, or legacy systems
that are configured for a specific IP address and difficult to re-configure.</p><p>The IP address that you choose must be a valid IPv4 or IPv6 address from within the
<code>service-cluster-ip-range</code> CIDR range that is configured for the API server.
If you try to create a Service with an invalid clusterIP address value, the API
server will return a 422 HTTP status code to indicate that there's a problem.</p><h2 id=discovering-services>Discovering services</h2><p>Kubernetes supports 2 primary modes of finding a Service - environment
variables and DNS.</p><h3 id=environment-variables>Environment variables</h3><p>When a Pod is run on a Node, the kubelet adds a set of environment variables
for each active Service. It adds <code>{SVCNAME}_SERVICE_HOST</code> and <code>{SVCNAME}_SERVICE_PORT</code> variables,
where the Service name is upper-cased and dashes are converted to underscores.
It also supports variables (see <a href=https://github.com/kubernetes/kubernetes/blob/dd2d12f6dc0e654c15d5db57a5f9f6ba61192726/pkg/kubelet/envvars/envvars.go#L72>makeLinkVariables</a>)
that are compatible with Docker Engine's
"<em><a href=https://docs.docker.com/network/links/>legacy container links</a></em>" feature.</p><p>For example, the Service <code>redis-primary</code> which exposes TCP port 6379 and has been
allocated cluster IP address 10.0.0.11, produces the following environment
variables:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>REDIS_PRIMARY_SERVICE_HOST</span><span style=color:#666>=</span>10.0.0.11
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_PRIMARY_SERVICE_PORT</span><span style=color:#666>=</span><span style=color:#666>6379</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_PRIMARY_PORT</span><span style=color:#666>=</span>tcp://10.0.0.11:6379
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_PRIMARY_PORT_6379_TCP</span><span style=color:#666>=</span>tcp://10.0.0.11:6379
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_PRIMARY_PORT_6379_TCP_PROTO</span><span style=color:#666>=</span>tcp
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_PRIMARY_PORT_6379_TCP_PORT</span><span style=color:#666>=</span><span style=color:#666>6379</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>REDIS_PRIMARY_PORT_6379_TCP_ADDR</span><span style=color:#666>=</span>10.0.0.11
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>When you have a Pod that needs to access a Service, and you are using
the environment variable method to publish the port and cluster IP to the client
Pods, you must create the Service <em>before</em> the client Pods come into existence.
Otherwise, those client Pods won't have their environment variables populated.</p><p>If you only use DNS to discover the cluster IP for a Service, you don't need to
worry about this ordering issue.</p></div><h3 id=dns>DNS</h3><p>You can (and almost always should) set up a DNS service for your Kubernetes
cluster using an <a href=/docs/concepts/cluster-administration/addons/>add-on</a>.</p><p>A cluster-aware DNS server, such as CoreDNS, watches the Kubernetes API for new
Services and creates a set of DNS records for each one. If DNS has been enabled
throughout your cluster then all Pods should automatically be able to resolve
Services by their DNS name.</p><p>For example, if you have a Service called <code>my-service</code> in a Kubernetes
namespace <code>my-ns</code>, the control plane and the DNS Service acting together
create a DNS record for <code>my-service.my-ns</code>. Pods in the <code>my-ns</code> namespace
should be able to find the service by doing a name lookup for <code>my-service</code>
(<code>my-service.my-ns</code> would also work).</p><p>Pods in other namespaces must qualify the name as <code>my-service.my-ns</code>. These names
will resolve to the cluster IP assigned for the Service.</p><p>Kubernetes also supports DNS SRV (Service) records for named ports. If the
<code>my-service.my-ns</code> Service has a port named <code>http</code> with the protocol set to
<code>TCP</code>, you can do a DNS SRV query for <code>_http._tcp.my-service.my-ns</code> to discover
the port number for <code>http</code>, as well as the IP address.</p><p>The Kubernetes DNS server is the only way to access <code>ExternalName</code> Services.
You can find more information about <code>ExternalName</code> resolution in
<a href=/docs/concepts/services-networking/dns-pod-service/>DNS Pods and Services</a>.</p><h2 id=headless-services>Headless Services</h2><p>Sometimes you don't need load-balancing and a single Service IP. In
this case, you can create what are termed "headless" Services, by explicitly
specifying <code>"None"</code> for the cluster IP (<code>.spec.clusterIP</code>).</p><p>You can use a headless Service to interface with other service discovery mechanisms,
without being tied to Kubernetes' implementation.</p><p>For headless <code>Services</code>, a cluster IP is not allocated, kube-proxy does not handle
these Services, and there is no load balancing or proxying done by the platform
for them. How DNS is automatically configured depends on whether the Service has
selectors defined:</p><h3 id=with-selectors>With selectors</h3><p>For headless Services that define selectors, the Kubernetes control plane creates
EndpointSlice objects in the Kubernetes API, and modifies the DNS configuration to return
A or AAAA records (IPv4 or IPv6 addresses) that point directly to the Pods backing
the Service.</p><h3 id=without-selectors>Without selectors</h3><p>For headless Services that do not define selectors, the control plane does
not create EndpointSlice objects. However, the DNS system looks for and configures
either:</p><ul><li>DNS CNAME records for <a href=#externalname><code>type: ExternalName</code></a> Services.</li><li>DNS A / AAAA records for all IP addresses of the Service's ready endpoints,
for all Service types other than <code>ExternalName</code>.<ul><li>For IPv4 endpoints, the DNS system creates A records.</li><li>For IPv6 endpoints, the DNS system creates AAAA records.</li></ul></li></ul><h2 id=publishing-services-service-types>Publishing Services (ServiceTypes)</h2><p>For some parts of your application (for example, frontends) you may want to expose a
Service onto an external IP address, that's outside of your cluster.</p><p>Kubernetes <code>ServiceTypes</code> allow you to specify what kind of Service you want.</p><p><code>Type</code> values and their behaviors are:</p><ul><li><code>ClusterIP</code>: Exposes the Service on a cluster-internal IP. Choosing this value
makes the Service only reachable from within the cluster. This is the
default that is used if you don't explicitly specify a <code>type</code> for a Service.</li><li><a href=#type-nodeport><code>NodePort</code></a>: Exposes the Service on each Node's IP at a static port
(the <code>NodePort</code>).
To make the node port available, Kubernetes sets up a cluster IP address,
the same as if you had requested a Service of <code>type: ClusterIP</code>.</li><li><a href=#loadbalancer><code>LoadBalancer</code></a>: Exposes the Service externally using a cloud
provider's load balancer.</li><li><a href=#externalname><code>ExternalName</code></a>: Maps the Service to the contents of the
<code>externalName</code> field (e.g. <code>foo.bar.example.com</code>), by returning a <code>CNAME</code> record
with its value. No proxying of any kind is set up.<div class="alert alert-info note callout" role=alert><strong>Note:</strong> You need either <code>kube-dns</code> version 1.7 or CoreDNS version 0.0.8 or higher
to use the <code>ExternalName</code> type.</div></li></ul><p>The <code>type</code> field was designed as nested functionality - each level adds to the
previous. This is not strictly required on all cloud providers (for example: Google
Compute Engine does not need to allocate a node port to make <code>type: LoadBalancer</code> work,
but another cloud provider integration might do). Although strict nesting is not required,
but the Kubernetes API design for Service requires it anyway.</p><p>You can also use <a href=/docs/concepts/services-networking/ingress/>Ingress</a> to expose your Service.
Ingress is not a Service type, but it acts as the entry point for your cluster.
It lets you consolidate your routing rules into a single resource as it can expose multiple
services under the same IP address.</p><h3 id=type-nodeport>Type NodePort</h3><p>If you set the <code>type</code> field to <code>NodePort</code>, the Kubernetes control plane
allocates a port from a range specified by <code>--service-node-port-range</code> flag (default: 30000-32767).
Each node proxies that port (the same port number on every Node) into your Service.
Your Service reports the allocated port in its <code>.spec.ports[*].nodePort</code> field.</p><p>Using a NodePort gives you the freedom to set up your own load balancing solution,
to configure environments that are not fully supported by Kubernetes, or even
to expose one or more nodes' IP addresses directly.</p><p>For a node port Service, Kubernetes additionally allocates a port (TCP, UDP or
SCTP to match the protocol of the Service). Every node in the cluster configures
itself to listen on that assigned port and to forward traffic to one of the ready
endpoints associated with that Service. You'll be able to contact the <code>type: NodePort</code>
Service, from outside the cluster, by connecting to any node using the appropriate
protocol (for example: TCP), and the appropriate port (as assigned to that Service).</p><h4 id=nodeport-custom-port>Choosing your own port</h4><p>If you want a specific port number, you can specify a value in the <code>nodePort</code>
field. The control plane will either allocate you that port or report that
the API transaction failed.
This means that you need to take care of possible port collisions yourself.
You also have to use a valid port number, one that's inside the range configured
for NodePort use.</p><p>Here is an example manifest for a Service of <code>type: NodePort</code> that specifies
a NodePort value (30007, in this example).</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>NodePort<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># By default and for convenience, the `targetPort` is set to the same value as the `port` field.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Optional field</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodePort</span>:<span style=color:#bbb> </span><span style=color:#666>30007</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=service-nodeport-custom-listen-address>Custom IP address configuration for <code>type: NodePort</code> Services</h4><p>You can set up nodes in your cluster to use a particular IP address for serving node port
services. You might want to do this if each node is connected to multiple networks (for example:
one network for application traffic, and another network for traffic between nodes and the
control plane).</p><p>If you want to specify particular IP address(es) to proxy the port, you can set the
<code>--nodeport-addresses</code> flag for kube-proxy or the equivalent <code>nodePortAddresses</code>
field of the
<a href=/docs/reference/config-api/kube-proxy-config.v1alpha1/>kube-proxy configuration file</a>
to particular IP block(s).</p><p>This flag takes a comma-delimited list of IP blocks (e.g. <code>10.0.0.0/8</code>, <code>192.0.2.0/25</code>)
to specify IP address ranges that kube-proxy should consider as local to this node.</p><p>For example, if you start kube-proxy with the <code>--nodeport-addresses=127.0.0.0/8</code> flag,
kube-proxy only selects the loopback interface for NodePort Services.
The default for <code>--nodeport-addresses</code> is an empty list.
This means that kube-proxy should consider all available network interfaces for NodePort.
(That's also compatible with earlier Kubernetes releases.)<div class="alert alert-info note callout" role=alert><strong>Note:</strong> This Service is visible as <code>&lt;NodeIP>:spec.ports[*].nodePort</code> and <code>.spec.clusterIP:spec.ports[*].port</code>.
If the <code>--nodeport-addresses</code> flag for kube-proxy or the equivalent field
in the kube-proxy configuration file is set, <code>&lt;NodeIP></code> would be a filtered node IP address (or possibly IP addresses).</div></p><h3 id=loadbalancer>Type LoadBalancer</h3><p>On cloud providers which support external load balancers, setting the <code>type</code>
field to <code>LoadBalancer</code> provisions a load balancer for your Service.
The actual creation of the load balancer happens asynchronously, and
information about the provisioned balancer is published in the Service's
<code>.status.loadBalancer</code> field.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span><span style=color:#666>10.0.171.239</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>loadBalancer</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>ip</span>:<span style=color:#bbb> </span><span style=color:#666>192.0.2.127</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Traffic from the external load balancer is directed at the backend Pods.
The cloud provider decides how it is load balanced.</p><p>Some cloud providers allow you to specify the <code>loadBalancerIP</code>. In those cases, the load-balancer is created
with the user-specified <code>loadBalancerIP</code>. If the <code>loadBalancerIP</code> field is not specified,
the loadBalancer is set up with an ephemeral IP address. If you specify a <code>loadBalancerIP</code>
but your cloud provider does not support the feature, the <code>loadbalancerIP</code> field that you
set is ignored.</p><p>To implement a Service of <code>type: LoadBalancer</code>, Kubernetes typically starts off
by making the changes that are equivalent to you requesting a Service of
<code>type: NodePort</code>. The cloud-controller-manager component then configures the external load balancer to
forward traffic to that assigned node port.</p><p><em>As an alpha feature</em>, you can configure a load balanced Service to
<a href=#load-balancer-nodeport-allocation>omit</a> assigning a node port, provided that the
cloud provider implementation supports this.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>On <strong>Azure</strong>, if you want to use a user-specified public type <code>loadBalancerIP</code>, you first need
to create a static type public IP address resource. This public IP address resource should
be in the same resource group of the other automatically created resources of the cluster.
For example, <code>MC_myResourceGroup_myAKSCluster_eastus</code>.</p><p>Specify the assigned IP address as loadBalancerIP. Ensure that you have updated the
<code>securityGroupName</code> in the cloud provider configuration file.
For information about troubleshooting <code>CreatingLoadBalancerFailed</code> permission issues see,
<a href=https://docs.microsoft.com/en-us/azure/aks/static-ip>Use a static IP address with the Azure Kubernetes Service (AKS) load balancer</a>
or <a href=https://github.com/Azure/AKS/issues/357>CreatingLoadBalancerFailed on AKS cluster with advanced networking</a>.</p></div><h4 id=load-balancers-with-mixed-protocol-types>Load balancers with mixed protocol types</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [beta]</code></div><p>By default, for LoadBalancer type of Services, when there is more than one port defined, all
ports must have the same protocol, and the protocol must be one which is supported
by the cloud provider.</p><p>The feature gate <code>MixedProtocolLBService</code> (enabled by default for the kube-apiserver as of v1.24) allows the use of
different protocols for LoadBalancer type of Services, when there is more than one port defined.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The set of protocols that can be used for LoadBalancer type of Services is still defined by the cloud provider. If a
cloud provider does not support mixed protocols they will provide only a single protocol.</div><h4 id=load-balancer-nodeport-allocation>Disabling load balancer NodePort allocation</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>You can optionally disable node port allocation for a Service of <code>type=LoadBalancer</code>, by setting
the field <code>spec.allocateLoadBalancerNodePorts</code> to <code>false</code>. This should only be used for load balancer implementations
that route traffic directly to pods as opposed to using node ports. By default, <code>spec.allocateLoadBalancerNodePorts</code>
is <code>true</code> and type LoadBalancer Services will continue to allocate node ports. If <code>spec.allocateLoadBalancerNodePorts</code>
is set to <code>false</code> on an existing Service with allocated node ports, those node ports will <strong>not</strong> be de-allocated automatically.
You must explicitly remove the <code>nodePorts</code> entry in every Service port to de-allocate those node ports.</p><h4 id=load-balancer-class>Specifying class of load balancer implementation</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p><code>spec.loadBalancerClass</code> enables you to use a load balancer implementation other than the cloud provider default.
By default, <code>spec.loadBalancerClass</code> is <code>nil</code> and a <code>LoadBalancer</code> type of Service uses
the cloud provider's default load balancer implementation if the cluster is configured with
a cloud provider using the <code>--cloud-provider</code> component flag.
If <code>spec.loadBalancerClass</code> is specified, it is assumed that a load balancer
implementation that matches the specified class is watching for Services.
Any default load balancer implementation (for example, the one provided by
the cloud provider) will ignore Services that have this field set.
<code>spec.loadBalancerClass</code> can be set on a Service of type <code>LoadBalancer</code> only.
Once set, it cannot be changed.
The value of <code>spec.loadBalancerClass</code> must be a label-style identifier,
with an optional prefix such as "<code>internal-vip</code>" or "<code>example.com/internal-vip</code>".
Unprefixed names are reserved for end-users.</p><h4 id=internal-load-balancer>Internal load balancer</h4><p>In a mixed environment it is sometimes necessary to route traffic from Services inside the same
(virtual) network address block.</p><p>In a split-horizon DNS environment you would need two Services to be able to route both external
and internal traffic to your endpoints.</p><p>To set an internal load balancer, add one of the following annotations to your Service
depending on the cloud Service provider you're using.</p><ul class="nav nav-tabs" id=service-tabs role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#service-tabs-0 role=tab aria-controls=service-tabs-0 aria-selected=true>Default</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-1 role=tab aria-controls=service-tabs-1>GCP</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-2 role=tab aria-controls=service-tabs-2>AWS</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-3 role=tab aria-controls=service-tabs-3>Azure</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-4 role=tab aria-controls=service-tabs-4>IBM Cloud</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-5 role=tab aria-controls=service-tabs-5>OpenStack</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-6 role=tab aria-controls=service-tabs-6>Baidu Cloud</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-7 role=tab aria-controls=service-tabs-7>Tencent Cloud</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-8 role=tab aria-controls=service-tabs-8>Alibaba Cloud</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#service-tabs-9 role=tab aria-controls=service-tabs-9>OCI</a></li></ul><div class=tab-content id=service-tabs><div id=service-tabs-0 class="tab-pane show active" role=tabpanel aria-labelledby=service-tabs-0><p><p>Select one of the tabs.</p></div><div id=service-tabs-1 class=tab-pane role=tabpanel aria-labelledby=service-tabs-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cloud.google.com/load-balancer-type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Internal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-2 class=tab-pane role=tabpanel aria-labelledby=service-tabs-2><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-internal</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-3 class=tab-pane role=tabpanel aria-labelledby=service-tabs-3><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/azure-load-balancer-internal</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-4 class=tab-pane role=tabpanel aria-labelledby=service-tabs-4><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/ibm-load-balancer-cloud-provider-ip-type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;private&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-5 class=tab-pane role=tabpanel aria-labelledby=service-tabs-5><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/openstack-internal-load-balancer</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-6 class=tab-pane role=tabpanel aria-labelledby=service-tabs-6><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/cce-load-balancer-internal-vpc</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-7 class=tab-pane role=tabpanel aria-labelledby=service-tabs-7><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.kubernetes.io/qcloud-loadbalancer-internal-subnetid</span>:<span style=color:#bbb> </span>subnet-xxxxx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-8 class=tab-pane role=tabpanel aria-labelledby=service-tabs-8><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/alibaba-cloud-loadbalancer-address-type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;intranet&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=service-tabs-9 class=tab-pane role=tabpanel aria-labelledby=service-tabs-9><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[...]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/oci-load-balancer-internal</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>[...]<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h4 id=ssl-support-on-aws>TLS support on AWS</h4><p>For partial TLS / SSL support on clusters running on AWS, you can add three
annotations to a <code>LoadBalancer</code> service:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-ssl-cert</span>:<span style=color:#bbb> </span>arn:aws:acm:us-east-1:123456789012:certificate/12345678-1234-1234-1234-123456789012<span style=color:#bbb>
</span></span></span></code></pre></div><p>The first specifies the ARN of the certificate to use. It can be either a
certificate from a third party issuer that was uploaded to IAM or one created
within AWS Certificate Manager.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span>:<span style=color:#bbb> </span>(https|http|ssl|tcp)<span style=color:#bbb>
</span></span></span></code></pre></div><p>The second annotation specifies which protocol a Pod speaks. For HTTPS and
SSL, the ELB expects the Pod to authenticate itself over the encrypted
connection, using a certificate.</p><p>HTTP and HTTPS selects layer 7 proxying: the ELB terminates
the connection with the user, parses headers, and injects the <code>X-Forwarded-For</code>
header with the user's IP address (Pods only see the IP address of the
ELB at the other end of its connection) when forwarding requests.</p><p>TCP and SSL selects layer 4 proxying: the ELB forwards traffic without
modifying the headers.</p><p>In a mixed-use environment where some ports are secured and others are left unencrypted,
you can use the following annotations:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-ssl-ports</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;443,8443&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>In the above example, if the Service contained three ports, <code>80</code>, <code>443</code>, and
<code>8443</code>, then <code>443</code> and <code>8443</code> would use the SSL certificate, but <code>80</code> would be proxied HTTP.</p><p>From Kubernetes v1.9 onwards you can use
<a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-security-policy-table.html>predefined AWS SSL policies</a>
with HTTPS or SSL listeners for your Services.
To see which policies are available for use, you can use the <code>aws</code> command line tool:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>aws elb describe-load-balancer-policies --query <span style=color:#b44>&#39;PolicyDescriptions[].PolicyName&#39;</span>
</span></span></code></pre></div><p>You can then specify any one of those policies using the
"<code>service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy</code>"
annotation; for example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;ELBSecurityPolicy-TLS-1-2-2017-01&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=proxy-protocol-support-on-aws>PROXY protocol support on AWS</h4><p>To enable <a href=https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt>PROXY protocol</a>
support for clusters running on AWS, you can use the following service
annotation:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-proxy-protocol</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Since version 1.3.0, the use of this annotation applies to all ports proxied by the ELB
and cannot be configured otherwise.</p><h4 id=elb-access-logs-on-aws>ELB Access Logs on AWS</h4><p>There are several annotations to manage access logs for ELB Services on AWS.</p><p>The annotation <code>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</code>
controls whether access logs are enabled.</p><p>The annotation <code>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</code>
controls the interval in minutes for publishing the access logs. You can specify
an interval of either 5 or 60 minutes.</p><p>The annotation <code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</code>
controls the name of the Amazon S3 bucket where load balancer access logs are
stored.</p><p>The annotation <code>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</code>
specifies the logical hierarchy you created for your Amazon S3 bucket.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Specifies whether access logs are enabled for the load balancer</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># The interval for publishing the access logs. You can specify an interval of either 5 or 60 (minutes).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-access-log-emit-interval</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;60&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># The name of the Amazon S3 bucket where the access logs are stored</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;my-bucket&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># The logical hierarchy you created for your Amazon S3 bucket, for example `my-bucket-prefix/prod`</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;my-bucket-prefix/prod&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=connection-draining-on-aws>Connection Draining on AWS</h4><p>Connection draining for Classic ELBs can be managed with the annotation
<code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</code> set
to the value of <code>"true"</code>. The annotation
<code>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</code> can
also be used to set maximum time, in seconds, to keep the existing connections open before
deregistering the instances.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;60&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=other-elb-annotations>Other ELB annotations</h4><p>There are other annotations to manage Classic Elastic Load Balancers that are described below.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># The time, in seconds, that the connection is allowed to be idle (no data has been sent</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># over the connection) before it is closed by the load balancer</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;60&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Specifies whether cross-zone load balancing is enabled for the load balancer</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># A comma-separated list of key-value pairs which will be recorded as</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># additional tags in the ELB.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;environment=prod,owner=devops&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># The number of successive successful health checks required for a backend to</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># be considered healthy for traffic. Defaults to 2, must be between 2 and 10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># The number of unsuccessful health checks required for a backend to be</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># considered unhealthy for traffic. Defaults to 6, must be between 2 and 10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;3&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># The approximate interval, in seconds, between health checks of an</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># individual instance. Defaults to 10, must be between 5 and 300</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;20&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># The amount of time, in seconds, during which no response means a failed</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># health check. This value must be less than the service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># value. Defaults to 5, must be between 2 and 60</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># A list of existing security groups to be configured on the ELB created. Unlike the annotation</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># service.beta.kubernetes.io/aws-load-balancer-extra-security-groups, this replaces all other</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># security groups previously assigned to the ELB and also overrides the creation</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># of a uniquely generated security group for this ELB.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># The first security group ID on this list is used as a source to permit incoming traffic to</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># target worker nodes (service traffic and health checks).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># If multiple ELBs are configured with the same security group ID, only a single permit line</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># will be added to the worker node security groups, that means if you delete any</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># of those ELBs it will remove the single permit line and block access for all ELBs that shared the same security group ID.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># This can cause a cross-service outage if not used properly</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-security-groups</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;sg-53fae93f&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># A list of additional security groups to be added to the created ELB, this leaves the uniquely</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># generated security group in place, this ensures that every ELB</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># has a unique security group ID and a matching permit line to allow traffic to the target worker nodes</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># (service traffic and health checks).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Security groups defined here can be shared between services.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-extra-security-groups</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;sg-53fae93f,sg-42efd82e&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># A comma separated list of key-value pairs which are used</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># to select the target nodes for the load balancer</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-target-node-labels</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;ingress-gw,gw-name=public-api&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=aws-nlb-support>Network Load Balancer support on AWS</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code></div><p>To use a Network Load Balancer on AWS, use the annotation <code>service.beta.kubernetes.io/aws-load-balancer-type</code> with the value set to <code>nlb</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.beta.kubernetes.io/aws-load-balancer-type</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;nlb&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> NLB only works with certain instance classes; see the
<a href=https://docs.aws.amazon.com/elasticloadbalancing/latest/network/target-group-register-targets.html#register-deregister-targets>AWS documentation</a>
on Elastic Load Balancing for a list of supported instance types.</div><p>Unlike Classic Elastic Load Balancers, Network Load Balancers (NLBs) forward the
client's IP address through to the node. If a Service's <code>.spec.externalTrafficPolicy</code>
is set to <code>Cluster</code>, the client's IP address is not propagated to the end
Pods.</p><p>By setting <code>.spec.externalTrafficPolicy</code> to <code>Local</code>, the client IP addresses is
propagated to the end Pods, but this could result in uneven distribution of
traffic. Nodes without any Pods for a particular LoadBalancer Service will fail
the NLB Target Group's health check on the auto-assigned
<code>.spec.healthCheckNodePort</code> and not receive any traffic.</p><p>In order to achieve even traffic, either use a DaemonSet or specify a
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>pod anti-affinity</a>
to not locate on the same node.</p><p>You can also use NLB Services with the <a href=/docs/concepts/services-networking/service/#internal-load-balancer>internal load balancer</a>
annotation.</p><p>In order for client traffic to reach instances behind an NLB, the Node security
groups are modified with the following IP rules:</p><table><thead><tr><th>Rule</th><th>Protocol</th><th>Port(s)</th><th>IpRange(s)</th><th>IpRange Description</th></tr></thead><tbody><tr><td>Health Check</td><td>TCP</td><td>NodePort(s) (<code>.spec.healthCheckNodePort</code> for <code>.spec.externalTrafficPolicy = Local</code>)</td><td>Subnet CIDR</td><td>kubernetes.io/rule/nlb/health=&lt;loadBalancerName></td></tr><tr><td>Client Traffic</td><td>TCP</td><td>NodePort(s)</td><td><code>.spec.loadBalancerSourceRanges</code> (defaults to <code>0.0.0.0/0</code>)</td><td>kubernetes.io/rule/nlb/client=&lt;loadBalancerName></td></tr><tr><td>MTU Discovery</td><td>ICMP</td><td>3,4</td><td><code>.spec.loadBalancerSourceRanges</code> (defaults to <code>0.0.0.0/0</code>)</td><td>kubernetes.io/rule/nlb/mtu=&lt;loadBalancerName></td></tr></tbody></table><p>In order to limit which client IP's can access the Network Load Balancer,
specify <code>loadBalancerSourceRanges</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>loadBalancerSourceRanges</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;143.231.0.0/16&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If <code>.spec.loadBalancerSourceRanges</code> is not set, Kubernetes
allows traffic from <code>0.0.0.0/0</code> to the Node Security Group(s). If nodes have
public IP addresses, be aware that non-NLB traffic can also reach all instances
in those modified security groups.</div><p>Further documentation on annotations for Elastic IPs and other common use-cases may be found
in the <a href=https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/>AWS Load Balancer Controller documentation</a>.</p><h4 id=other-clb-annotations-on-tencent-kubernetes-engine-tke>Other CLB annotations on Tencent Kubernetes Engine (TKE)</h4><p>There are other annotations for managing Cloud Load Balancers on TKE as shown below.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Bind Loadbalancers with specified nodes</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/qcloud-loadbalancer-backends-label</span>:<span style=color:#bbb> </span>key in (value1, value2)<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># ID of an existing load balancer</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>service.kubernetes.io/tke-existed-lbid：lb-6swtxxxx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Custom parameters for the load balancer (LB), does not support modification of LB type yet</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/service.extensiveParameters</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Custom parameters for the LB listener</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/service.listenerParameters</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Specifies the type of Load balancer;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># valid values: classic (Classic Cloud Load Balancer) or application (Application Cloud Load Balancer)</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/loadbalance-type</span>:<span style=color:#bbb> </span>xxxxx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Specifies the public network bandwidth billing method;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># valid values: TRAFFIC_POSTPAID_BY_HOUR(bill-by-traffic) and BANDWIDTH_POSTPAID_BY_HOUR (bill-by-bandwidth).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/qcloud-loadbalancer-internet-charge-type</span>:<span style=color:#bbb> </span>xxxxxx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Specifies the bandwidth value (value range: [1,2000] Mbps).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/qcloud-loadbalancer-internet-max-bandwidth-out</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># When this annotation is set，the loadbalancers will only register nodes</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># with pod running on it, otherwise all nodes will be registered.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>service.kubernetes.io/local-svc-only-bind-node-with-pod</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=externalname>Type ExternalName</h3><p>Services of type ExternalName map a Service to a DNS name, not to a typical selector such as
<code>my-service</code> or <code>cassandra</code>. You specify these Services with the <code>spec.externalName</code> parameter.</p><p>This Service definition, for example, maps
the <code>my-service</code> Service in the <code>prod</code> namespace to <code>my.database.example.com</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>prod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>ExternalName<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>externalName</span>:<span style=color:#bbb> </span>my.database.example.com<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> ExternalName accepts an IPv4 address string, but as a DNS name comprised of digits, not as an IP address.
ExternalNames that resemble IPv4 addresses are not resolved by CoreDNS or ingress-nginx because ExternalName
is intended to specify a canonical DNS name. To hardcode an IP address, consider using
<a href=#headless-services>headless Services</a>.</div><p>When looking up the host <code>my-service.prod.svc.cluster.local</code>, the cluster DNS Service
returns a <code>CNAME</code> record with the value <code>my.database.example.com</code>. Accessing
<code>my-service</code> works in the same way as other Services but with the crucial
difference that redirection happens at the DNS level rather than via proxying or
forwarding. Should you later decide to move your database into your cluster, you
can start its Pods, add appropriate selectors or endpoints, and change the
Service's <code>type</code>.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>You may have trouble using ExternalName for some common protocols, including HTTP and HTTPS.
If you use ExternalName then the hostname used by clients inside your cluster is different from
the name that the ExternalName references.</p><p>For protocols that use hostnames this difference may lead to errors or unexpected responses.
HTTP requests will have a <code>Host:</code> header that the origin server does not recognize;
TLS servers will not be able to provide a certificate matching the hostname that the client connected to.</p></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This section is indebted to the <a href=https://akomljen.com/kubernetes-tips-part-1/>Kubernetes Tips - Part
1</a> blog post from <a href=https://akomljen.com/>Alen Komljen</a>.</div><h3 id=external-ips>External IPs</h3><p>If there are external IPs that route to one or more cluster nodes, Kubernetes Services can be exposed on those
<code>externalIPs</code>. Traffic that ingresses into the cluster with the external IP (as destination IP), on the Service port,
will be routed to one of the Service endpoints. <code>externalIPs</code> are not managed by Kubernetes and are the responsibility
of the cluster administrator.</p><p>In the Service spec, <code>externalIPs</code> can be specified along with any of the <code>ServiceTypes</code>.
In the example below, "<code>my-service</code>" can be accessed by clients on "<code>80.11.12.10:80</code>" (<code>externalIP:port</code>)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>externalIPs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#666>80.11.12.10</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=session-stickiness>Session stickiness</h2><p>If you want to make sure that connections from a particular client are passed to
the same Pod each time, you can configure session affinity based on the client's
IP address. Read <a href=/docs/reference/networking/virtual-ips/#session-affinity>session affinity</a>
to learn more.</p><h2 id=api-object>API Object</h2><p>Service is a top-level resource in the Kubernetes REST API. You can find more details
about the <a href=/docs/reference/generated/kubernetes-api/v1.25/#service-v1-core>Service API object</a>.</p><p><a id=shortcomings><a id=#the-gory-details-of-virtual-ips></p><h2 id=virtual-ip-addressing-mechanism>Virtual IP addressing mechanism</h2><p>Read <a href=/docs/reference/networking/virtual-ips/>Virtual IPs and Service Proxies</a> to learn about the
mechanism Kubernetes provides to expose a Service with a virtual IP address.</p><h2 id=what-s-next>What's next</h2><ul><li>Follow the <a href=/docs/tutorials/services/connect-applications-service/>Connecting Applications with Services</a> tutorial</li><li>Read about <a href=/docs/concepts/services-networking/ingress/>Ingress</a></li><li>Read about <a href=/docs/concepts/services-networking/endpoint-slices/>EndpointSlices</a></li></ul><p>For more context:</p><ul><li>Read <a href=/docs/reference/networking/virtual-ips/>Virtual IPs and Service Proxies</a></li><li>Read the <a href=/docs/reference/kubernetes-api/service-resources/service-v1/>API reference</a> for the Service API</li><li>Read the <a href=/docs/reference/kubernetes-api/service-resources/endpoints-v1/>API reference</a> for the Endpoints API</li><li>Read the <a href=/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/>API reference</a> for the EndpointSlice API</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-199bcc92443dbc9bed44819467d7eb75>6.2 - Ingress</h1><div class=lead>Make your HTTP (or HTTPS) network service available using a protocol-aware configuration mechanism, that understands web concepts like URIs, hostnames, paths, and more. The Ingress concept lets you map traffic to different backends based on rules you define via the Kubernetes API.</div><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code></div><p>An API object that manages external access to the services in a cluster, typically HTTP.</p><p>Ingress may provide load balancing, SSL termination and name-based virtual hosting.</p></p><h2 id=terminology>Terminology</h2><p>For clarity, this guide defines the following terms:</p><ul><li>Node: A worker machine in Kubernetes, part of a cluster.</li><li>Cluster: A set of Nodes that run containerized applications managed by Kubernetes. For this example, and in most common Kubernetes deployments, nodes in the cluster are not part of the public internet.</li><li>Edge router: A router that enforces the firewall policy for your cluster. This could be a gateway managed by a cloud provider or a physical piece of hardware.</li><li>Cluster network: A set of links, logical or physical, that facilitate communication within a cluster according to the Kubernetes <a href=/docs/concepts/cluster-administration/networking/>networking model</a>.</li><li>Service: A Kubernetes <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> that identifies a set of Pods using <a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=label>label</a> selectors. Unless mentioned otherwise, Services are assumed to have virtual IPs only routable within the cluster network.</li></ul><h2 id=what-is-ingress>What is Ingress?</h2><p><a href=/docs/reference/generated/kubernetes-api/v1.25/#ingress-v1-networking-k8s-io>Ingress</a> exposes HTTP and HTTPS routes from outside the cluster to
<a href=/docs/concepts/services-networking/service/ target=_blank>services</a> within the cluster.
Traffic routing is controlled by rules defined on the Ingress resource.</p><p>Here is a simple example where an Ingress sends all its traffic to one Service:</p><figure class=diagram-large><a href=https://mermaid.live/edit#pako:eNqNkstuwyAQRX8F4U0r2VHqPlSRKqt0UamLqlnaWWAYJygYLB59KMm_Fxcix-qmGwbuXA7DwAEzzQETXKutof0Ovb4vaoUQkwKUu6pi3FwXM_QSHGBt0VFFt8DRU2OWSGrKUUMlVQwMmhVLEV1Vcm9-aUksiuXRaO_CEhkv4WjBfAgG1TrGaLa-iaUw6a0DcwGI-WgOsF7zm-pN881fvRx1UDzeiFq7ghb1kgqFWiElyTjnuXVG74FkbdumefEpuNuRu_4rZ1pqQ7L5fL6YQPaPNiFuywcG9_-ihNyUkm6YSONWkjVNM8WUIyaeOJLO3clTB_KhL8NQDmVe-OJjxgZM5FhFiiFTK5zjDkxHBQ9_4zB4a-x20EGNSZhyaKmXrg7f5hSsvufUwTMXThtMWiot5Jh6p9ffimHijIezaSVoeN0uiqcfMJvf7w><img src=/docs/images/ingress.svg alt=ingress-diagram></a><figcaption><p>Figure. Ingress</p></figcaption></figure><p>An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting. An <a href=/docs/concepts/services-networking/ingress-controllers>Ingress controller</a> is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic.</p><p>An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically
uses a service of type <a href=/docs/concepts/services-networking/service/#type-nodeport>Service.Type=NodePort</a> or
<a href=/docs/concepts/services-networking/service/#loadbalancer>Service.Type=LoadBalancer</a>.</p><h2 id=prerequisites>Prerequisites</h2><p>You must have an <a href=/docs/concepts/services-networking/ingress-controllers>Ingress controller</a> to satisfy an Ingress. Only creating an Ingress resource has no effect.</p><p>You may need to deploy an Ingress controller such as <a href=https://kubernetes.github.io/ingress-nginx/deploy/>ingress-nginx</a>. You can choose from a number of
<a href=/docs/concepts/services-networking/ingress-controllers>Ingress controllers</a>.</p><p>Ideally, all Ingress controllers should fit the reference specification. In reality, the various Ingress
controllers operate slightly differently.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Make sure you review your Ingress controller's documentation to understand the caveats of choosing it.</div><h2 id=the-ingress-resource>The Ingress resource</h2><p>A minimal Ingress resource example:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/minimal-ingress.yaml download=service/networking/minimal-ingress.yaml><code>service/networking/minimal-ingress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-minimal-ingress-yaml")' title="Copy service/networking/minimal-ingress.yaml to clipboard"></img></div><div class=includecode id=service-networking-minimal-ingress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>minimal-ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nginx.ingress.kubernetes.io/rewrite-target</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingressClassName</span>:<span style=color:#bbb> </span>nginx-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/testpath<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>An Ingress needs <code>apiVersion</code>, <code>kind</code>, <code>metadata</code> and <code>spec</code> fields.
The name of an Ingress object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.
For general information about working with config files, see <a href=/docs/tasks/run-application/run-stateless-application-deployment/>deploying applications</a>, <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>configuring containers</a>, <a href=/docs/concepts/cluster-administration/manage-deployment/>managing resources</a>.
Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which
is the <a href=https://github.com/kubernetes/ingress-nginx/blob/master/docs/examples/rewrite/README.md>rewrite-target annotation</a>.
Different <a href=/docs/concepts/services-networking/ingress-controllers>Ingress controllers</a> support different annotations. Review the documentation for
your choice of Ingress controller to learn which annotations are supported.</p><p>The Ingress <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>spec</a>
has all the information needed to configure a load balancer or proxy server. Most importantly, it
contains a list of rules matched against all incoming requests. Ingress resource only supports rules
for directing HTTP(S) traffic.</p><p>If the <code>ingressClassName</code> is omitted, a <a href=#default-ingress-class>default Ingress class</a>
should be defined.</p><p>There are some ingress controllers, that work without the definition of a
default <code>IngressClass</code>. For example, the Ingress-NGINX controller can be
configured with a <a href=https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class>flag</a>
<code>--watch-ingress-without-class</code>. It is <a href=https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do>recommended</a> though, to specify the
default <code>IngressClass</code> as shown <a href=#default-ingress-class>below</a>.</p><h3 id=ingress-rules>Ingress rules</h3><p>Each HTTP rule contains the following information:</p><ul><li>An optional host. In this example, no host is specified, so the rule applies to all inbound
HTTP traffic through the IP address specified. If a host is provided (for example,
foo.bar.com), the rules apply to that host.</li><li>A list of paths (for example, <code>/testpath</code>), each of which has an associated
backend defined with a <code>service.name</code> and a <code>service.port.name</code> or
<code>service.port.number</code>. Both the host and path must match the content of an
incoming request before the load balancer directs traffic to the referenced
Service.</li><li>A backend is a combination of Service and port names as described in the
<a href=/docs/concepts/services-networking/service/>Service doc</a> or a <a href=#resource-backend>custom resource backend</a> by way of a <a class=glossary-tooltip title='Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.' data-toggle=tooltip data-placement=top href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ target=_blank aria-label=CRD>CRD</a>. HTTP (and HTTPS) requests to the
Ingress that matches the host and path of the rule are sent to the listed backend.</li></ul><p>A <code>defaultBackend</code> is often configured in an Ingress controller to service any requests that do not
match a path in the spec.</p><h3 id=default-backend>DefaultBackend</h3><p>An Ingress with no rules sends all traffic to a single default backend and <code>.spec.defaultBackend</code>
is the backend that should handle requests in that case.
The <code>defaultBackend</code> is conventionally a configuration option of the
<a href=/docs/concepts/services-networking/ingress-controllers>Ingress controller</a> and
is not specified in your Ingress resources.
If no <code>.spec.rules</code> are specified, <code>.spec.defaultBackend</code> must be specified.
If <code>defaultBackend</code> is not set, the handling of requests that do not match any of the rules will be up to the
ingress controller (consult the documentation for your ingress controller to find out how it handles this case).</p><p>If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is
routed to your default backend.</p><h3 id=resource-backend>Resource backends</h3><p>A <code>Resource</code> backend is an ObjectRef to another Kubernetes resource within the
same namespace as the Ingress object. A <code>Resource</code> is a mutually exclusive
setting with Service, and will fail validation if both are specified. A common
usage for a <code>Resource</code> backend is to ingress data to an object storage backend
with static assets.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/ingress-resource-backend.yaml download=service/networking/ingress-resource-backend.yaml><code>service/networking/ingress-resource-backend.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-ingress-resource-backend-yaml")' title="Copy service/networking/ingress-resource-backend.yaml to clipboard"></img></div><div class=includecode id=service-networking-ingress-resource-backend-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ingress-resource-backend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>defaultBackend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resource</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>k8s.example.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageBucket<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>static-assets<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/icons<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>ImplementationSpecific<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>resource</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>k8s.example.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageBucket<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>icon-assets<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>After creating the Ingress above, you can view it with the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe ingress ingress-resource-backend
</span></span></code></pre></div><pre tabindex=0><code>Name:             ingress-resource-backend
Namespace:        default
Address:
Default backend:  APIGroup: k8s.example.com, Kind: StorageBucket, Name: static-assets
Rules:
  Host        Path  Backends
  ----        ----  --------
  *
              /icons   APIGroup: k8s.example.com, Kind: StorageBucket, Name: icon-assets
Annotations:  &lt;none&gt;
Events:       &lt;none&gt;
</code></pre><h3 id=path-types>Path types</h3><p>Each path in an Ingress is required to have a corresponding path type. Paths
that do not include an explicit <code>pathType</code> will fail validation. There are three
supported path types:</p><ul><li><p><code>ImplementationSpecific</code>: With this path type, matching is up to the
IngressClass. Implementations can treat this as a separate <code>pathType</code> or treat
it identically to <code>Prefix</code> or <code>Exact</code> path types.</p></li><li><p><code>Exact</code>: Matches the URL path exactly and with case sensitivity.</p></li><li><p><code>Prefix</code>: Matches based on a URL path prefix split by <code>/</code>. Matching is case
sensitive and done on a path element by element basis. A path element refers
to the list of labels in the path split by the <code>/</code> separator. A request is a
match for path <em>p</em> if every <em>p</em> is an element-wise prefix of <em>p</em> of the
request path.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If the last element of the path is a substring of the last
element in request path, it is not a match (for example: <code>/foo/bar</code>
matches<code>/foo/bar/baz</code>, but does not match <code>/foo/barbaz</code>).</div></li></ul><h3 id=examples>Examples</h3><table><thead><tr><th>Kind</th><th>Path(s)</th><th>Request path(s)</th><th>Matches?</th></tr></thead><tbody><tr><td>Prefix</td><td><code>/</code></td><td>(all paths)</td><td>Yes</td></tr><tr><td>Exact</td><td><code>/foo</code></td><td><code>/foo</code></td><td>Yes</td></tr><tr><td>Exact</td><td><code>/foo</code></td><td><code>/bar</code></td><td>No</td></tr><tr><td>Exact</td><td><code>/foo</code></td><td><code>/foo/</code></td><td>No</td></tr><tr><td>Exact</td><td><code>/foo/</code></td><td><code>/foo</code></td><td>No</td></tr><tr><td>Prefix</td><td><code>/foo</code></td><td><code>/foo</code>, <code>/foo/</code></td><td>Yes</td></tr><tr><td>Prefix</td><td><code>/foo/</code></td><td><code>/foo</code>, <code>/foo/</code></td><td>Yes</td></tr><tr><td>Prefix</td><td><code>/aaa/bb</code></td><td><code>/aaa/bbb</code></td><td>No</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbb</code></td><td>Yes</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb/</code></td><td><code>/aaa/bbb</code></td><td>Yes, ignores trailing slash</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbb/</code></td><td>Yes, matches trailing slash</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbb/ccc</code></td><td>Yes, matches subpath</td></tr><tr><td>Prefix</td><td><code>/aaa/bbb</code></td><td><code>/aaa/bbbxyz</code></td><td>No, does not match string prefix</td></tr><tr><td>Prefix</td><td><code>/</code>, <code>/aaa</code></td><td><code>/aaa/ccc</code></td><td>Yes, matches <code>/aaa</code> prefix</td></tr><tr><td>Prefix</td><td><code>/</code>, <code>/aaa</code>, <code>/aaa/bbb</code></td><td><code>/aaa/bbb</code></td><td>Yes, matches <code>/aaa/bbb</code> prefix</td></tr><tr><td>Prefix</td><td><code>/</code>, <code>/aaa</code>, <code>/aaa/bbb</code></td><td><code>/ccc</code></td><td>Yes, matches <code>/</code> prefix</td></tr><tr><td>Prefix</td><td><code>/aaa</code></td><td><code>/ccc</code></td><td>No, uses default backend</td></tr><tr><td>Mixed</td><td><code>/foo</code> (Prefix), <code>/foo</code> (Exact)</td><td><code>/foo</code></td><td>Yes, prefers Exact</td></tr></tbody></table><h4 id=multiple-matches>Multiple matches</h4><p>In some cases, multiple paths within an Ingress will match a request. In those
cases precedence will be given first to the longest matching path. If two paths
are still equally matched, precedence will be given to paths with an exact path
type over prefix path type.</p><h2 id=hostname-wildcards>Hostname wildcards</h2><p>Hosts can be precise matches (for example “<code>foo.bar.com</code>”) or a wildcard (for
example “<code>*.foo.com</code>”). Precise matches require that the HTTP <code>host</code> header
matches the <code>host</code> field. Wildcard matches require the HTTP <code>host</code> header is
equal to the suffix of the wildcard rule.</p><table><thead><tr><th>Host</th><th>Host header</th><th>Match?</th></tr></thead><tbody><tr><td><code>*.foo.com</code></td><td><code>bar.foo.com</code></td><td>Matches based on shared suffix</td></tr><tr><td><code>*.foo.com</code></td><td><code>baz.bar.foo.com</code></td><td>No match, wildcard only covers a single DNS label</td></tr><tr><td><code>*.foo.com</code></td><td><code>foo.com</code></td><td>No match, wildcard only covers a single DNS label</td></tr></tbody></table><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/ingress-wildcard-host.yaml download=service/networking/ingress-wildcard-host.yaml><code>service/networking/ingress-wildcard-host.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-ingress-wildcard-host-yaml")' title="Copy service/networking/ingress-wildcard-host.yaml to clipboard"></img></div><div class=includecode id=service-networking-ingress-wildcard-host-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ingress-wildcard-host<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;foo.bar.com&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/bar&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;*.foo.com&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h2 id=ingress-class>Ingress class</h2><p>Ingresses can be implemented by different controllers, often with different
configuration. Each Ingress should specify a class, a reference to an
IngressClass resource that contains additional configuration including the name
of the controller that should implement the class.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/external-lb.yaml download=service/networking/external-lb.yaml><code>service/networking/external-lb.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-external-lb-yaml")' title="Copy service/networking/external-lb.yaml to clipboard"></img></div><div class=includecode id=service-networking-external-lb-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>IngressClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>external-lb<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>controller</span>:<span style=color:#bbb> </span>example.com/ingress-controller<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>k8s.example.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>IngressParameters<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>external-lb<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>The <code>.spec.parameters</code> field of an IngressClass lets you reference another
resource that provides configuration related to that IngressClass.</p><p>The specific type of parameters to use depends on the ingress controller
that you specify in the <code>.spec.controller</code> field of the IngressClass.</p><h3 id=ingressclass-scope>IngressClass scope</h3><p>Depending on your ingress controller, you may be able to use parameters
that you set cluster-wide, or just for one namespace.</p><ul class="nav nav-tabs" id=tabs-ingressclass-parameter-scope role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#tabs-ingressclass-parameter-scope-0 role=tab aria-controls=tabs-ingressclass-parameter-scope-0 aria-selected=true>Cluster</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#tabs-ingressclass-parameter-scope-1 role=tab aria-controls=tabs-ingressclass-parameter-scope-1>Namespaced</a></li></ul><div class=tab-content id=tabs-ingressclass-parameter-scope><div id=tabs-ingressclass-parameter-scope-0 class="tab-pane show active" role=tabpanel aria-labelledby=tabs-ingressclass-parameter-scope-0><p><p>The default scope for IngressClass parameters is cluster-wide.</p><p>If you set the <code>.spec.parameters</code> field and don't set
<code>.spec.parameters.scope</code>, or if you set <code>.spec.parameters.scope</code> to
<code>Cluster</code>, then the IngressClass refers to a cluster-scoped resource.
The <code>kind</code> (in combination the <code>apiGroup</code>) of the parameters
refers to a cluster-scoped API (possibly a custom resource), and
the <code>name</code> of the parameters identifies a specific cluster scoped
resource for that API.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>IngressClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>external-lb-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>controller</span>:<span style=color:#bbb> </span>example.com/ingress-controller<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># The parameters for this IngressClass are specified in a</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># ClusterIngressParameter (API group k8s.example.net) named</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># &#34;external-config-1&#34;. This definition tells Kubernetes to</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># look for a cluster-scoped parameter resource.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>scope</span>:<span style=color:#bbb> </span>Cluster<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>k8s.example.net<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterIngressParameter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>external-config-1<span style=color:#bbb>
</span></span></span></code></pre></div></div><div id=tabs-ingressclass-parameter-scope-1 class=tab-pane role=tabpanel aria-labelledby=tabs-ingressclass-parameter-scope-1><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code></div><p>If you set the <code>.spec.parameters</code> field and set
<code>.spec.parameters.scope</code> to <code>Namespace</code>, then the IngressClass refers
to a namespaced-scoped resource. You must also set the <code>namespace</code>
field within <code>.spec.parameters</code> to the namespace that contains
the parameters you want to use.</p><p>The <code>kind</code> (in combination the <code>apiGroup</code>) of the parameters
refers to a namespaced API (for example: ConfigMap), and
the <code>name</code> of the parameters identifies a specific resource
in the namespace you specified in <code>namespace</code>.</p><p>Namespace-scoped parameters help the cluster operator delegate control over the
configuration (for example: load balancer settings, API gateway definition)
that is used for a workload. If you used a cluster-scoped parameter then either:</p><ul><li>the cluster operator team needs to approve a different team's changes every
time there's a new configuration change being applied.</li><li>the cluster operator must define specific access controls, such as
<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a> roles and bindings, that let
the application team make changes to the cluster-scoped parameters resource.</li></ul><p>The IngressClass API itself is always cluster-scoped.</p><p>Here is an example of an IngressClass that refers to parameters that are
namespaced:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>IngressClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>external-lb-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>controller</span>:<span style=color:#bbb> </span>example.com/ingress-controller<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># The parameters for this IngressClass are specified in an</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># IngressParameter (API group k8s.example.com) named &#34;external-config&#34;,</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># that&#39;s in the &#34;external-configuration&#34; namespace.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>scope</span>:<span style=color:#bbb> </span>Namespace<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>k8s.example.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>IngressParameter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>external-configuration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>external-config<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h3 id=deprecated-annotation>Deprecated annotation</h3><p>Before the IngressClass resource and <code>ingressClassName</code> field were added in
Kubernetes 1.18, Ingress classes were specified with a
<code>kubernetes.io/ingress.class</code> annotation on the Ingress. This annotation was
never formally defined, but was widely supported by Ingress controllers.</p><p>The newer <code>ingressClassName</code> field on Ingresses is a replacement for that
annotation, but is not a direct equivalent. While the annotation was generally
used to reference the name of the Ingress controller that should implement the
Ingress, the field is a reference to an IngressClass resource that contains
additional Ingress configuration, including the name of the Ingress controller.</p><h3 id=default-ingress-class>Default IngressClass</h3><p>You can mark a particular IngressClass as default for your cluster. Setting the
<code>ingressclass.kubernetes.io/is-default-class</code> annotation to <code>true</code> on an
IngressClass resource will ensure that new Ingresses without an
<code>ingressClassName</code> field specified will be assigned this default IngressClass.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> If you have more than one IngressClass marked as the default for your cluster,
the admission controller prevents creating new Ingress objects that don't have
an <code>ingressClassName</code> specified. You can resolve this by ensuring that at most 1
IngressClass is marked as default in your cluster.</div><p>There are some ingress controllers, that work without the definition of a
default <code>IngressClass</code>. For example, the Ingress-NGINX controller can be
configured with a <a href=https://kubernetes.github.io/ingress-nginx/#what-is-the-flag-watch-ingress-without-class>flag</a>
<code>--watch-ingress-without-class</code>. It is <a href=https://kubernetes.github.io/ingress-nginx/#i-have-only-one-instance-of-the-ingresss-nginx-controller-in-my-cluster-what-should-i-do>recommended</a> though, to specify the
default <code>IngressClass</code>:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/default-ingressclass.yaml download=service/networking/default-ingressclass.yaml><code>service/networking/default-ingressclass.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-default-ingressclass-yaml")' title="Copy service/networking/default-ingressclass.yaml to clipboard"></img></div><div class=includecode id=service-networking-default-ingressclass-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>IngressClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/component</span>:<span style=color:#bbb> </span>controller<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ingressclass.kubernetes.io/is-default-class</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>controller</span>:<span style=color:#bbb> </span>k8s.io/ingress-nginx<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h2 id=types-of-ingress>Types of Ingress</h2><h3 id=single-service-ingress>Ingress backed by a single Service</h3><p>There are existing Kubernetes concepts that allow you to expose a single Service
(see <a href=#alternatives>alternatives</a>). You can also do this with an Ingress by specifying a
<em>default backend</em> with no rules.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/test-ingress.yaml download=service/networking/test-ingress.yaml><code>service/networking/test-ingress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-test-ingress-yaml")' title="Copy service/networking/test-ingress.yaml to clipboard"></img></div><div class=includecode id=service-networking-test-ingress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>defaultBackend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>If you create it using <code>kubectl apply -f</code> you should be able to view the state
of the Ingress you added:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get ingress test-ingress
</span></span></code></pre></div><pre tabindex=0><code>NAME           CLASS         HOSTS   ADDRESS         PORTS   AGE
test-ingress   external-lb   *       203.0.113.123   80      59s
</code></pre><p>Where <code>203.0.113.123</code> is the IP allocated by the Ingress controller to satisfy
this Ingress.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Ingress controllers and load balancers may take a minute or two to allocate an IP address.
Until that time, you often see the address listed as <code>&lt;pending></code>.</div><h3 id=simple-fanout>Simple fanout</h3><p>A fanout configuration routes traffic from a single IP address to more than one Service,
based on the HTTP URI being requested. An Ingress allows you to keep the number of load balancers
down to a minimum. For example, a setup like:</p><figure class=diagram-large><a href=https://mermaid.live/edit#pako:eNqNUslOwzAQ_RXLvYCUhMQpUFzUUzkgcUBwbHpw4klr4diR7bCo8O8k2FFbFomLPZq3jP00O1xpDpjijWHtFt09zAuFUCUFKHey8vf6NE7QrdoYsDZumGIb4Oi6NAskNeOoZJKpCgxK4oXwrFVgRyi7nCVXWZKRPMlysv5yD6Q4Xryf1Vq_WzDPooJs9egLNDbolKTpT03JzKgh3zWEztJZ0Niu9L-qZGcdmAMfj4cxvWmreba613z9C0B-AMQD-V_AdA-A4j5QZu0SatRKJhSqhZR0wjmPrDP6CeikrutQxy-Cuy2dtq9RpaU2dJKm6fzI5Glmg0VOLio4_5dLjx27hFSC015KJ2VZHtuQvY2fuHcaE43G0MaCREOow_FV5cMxHZ5-oPX75UM5avuXhXuOI9yAaZjg_aLuBl6B3RYaKDDtSw4166QrcKE-emrXcubghgunDaY1kxYizDqnH99UhakzHYykpWD9hjS--fEJoIELqQ><img src=/docs/images/ingressFanOut.svg alt=ingress-fanout-diagram></a><figcaption><p>Figure. Ingress Fan Out</p></figcaption></figure><p>would require an Ingress such as:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/simple-fanout-example.yaml download=service/networking/simple-fanout-example.yaml><code>service/networking/simple-fanout-example.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-simple-fanout-example-yaml")' title="Copy service/networking/simple-fanout-example.yaml to clipboard"></img></div><div class=includecode id=service-networking-simple-fanout-example-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>simple-fanout-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span>foo.bar.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>4200</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>8080</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>When you create the Ingress with <code>kubectl apply -f</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe ingress simple-fanout-example
</span></span></code></pre></div><pre tabindex=0><code>Name:             simple-fanout-example
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:4200 (10.8.0.90:4200)
               /bar   service2:8080 (10.8.0.91:8080)
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     22s                loadbalancer-controller  default/test
</code></pre><p>The Ingress controller provisions an implementation-specific load balancer
that satisfies the Ingress, as long as the Services (<code>service1</code>, <code>service2</code>) exist.
When it has done so, you can see the address of the load balancer at the
Address field.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Depending on the <a href=/docs/concepts/services-networking/ingress-controllers/>Ingress controller</a>
you are using, you may need to create a default-http-backend
<a href=/docs/concepts/services-networking/service/>Service</a>.</div><h3 id=name-based-virtual-hosting>Name based virtual hosting</h3><p>Name-based virtual hosts support routing HTTP traffic to multiple host names at the same IP address.</p><figure class=diagram-large><a href=https://mermaid.live/edit#pako:eNqNkl9PwyAUxb8KYS-atM1Kp05m9qSJJj4Y97jugcLtRqTQAPVPdN_dVlq3qUt8gZt7zvkBN7xjbgRgiteW1Rt0_zjLNUJcSdD-ZBn21WmcoDu9tuBcXDHN1iDQVWHnSBkmUMEU0xwsSuK5DK5l745QejFNLtMkJVmSZmT1Re9NcTz_uDXOU1QakxTMJtxUHw7ss-SQLhehQEODTsdH4l20Q-zFyc84-Y67pghv5apxHuweMuj9eS2_NiJdPhix-kMgvwQShOyYMNkJoEUYM3PuGkpUKyY1KqVSdCSEiJy35gnoqCzLvo5fpPAbOqlfI26UsXQ0Ho9nB5CnqesRGTnncPYvSqsdUvqp9KRdlI6KojjEkB0mnLgjDRONhqENBYm6oXbLV5V1y6S7-l42_LowlIN2uFm_twqOcAW2YlK0H_i9c-bYb6CCHNO2FFCyRvkc53rbWptaMA83QnpjMS2ZchBh1nizeNMcU28bGEzXkrV_pArN7Sc0rBTu><img src=/docs/images/ingressNameBased.svg alt=ingress-namebase-diagram></a><figcaption><p>Figure. Ingress Name Based Virtual hosting</p></figcaption></figure><p>The following Ingress tells the backing load balancer to route requests based on
the <a href=https://tools.ietf.org/html/rfc7230#section-5.4>Host header</a>.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/name-virtual-host-ingress.yaml download=service/networking/name-virtual-host-ingress.yaml><code>service/networking/name-virtual-host-ingress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-name-virtual-host-ingress-yaml")' title="Copy service/networking/name-virtual-host-ingress.yaml to clipboard"></img></div><div class=includecode id=service-networking-name-virtual-host-ingress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>name-virtual-host-ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span>foo.bar.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span>bar.foo.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>If you create an Ingress resource without any hosts defined in the rules, then any
web traffic to the IP address of your Ingress controller can be matched without a name based
virtual host being required.</p><p>For example, the following Ingress routes traffic
requested for <code>first.bar.com</code> to <code>service1</code>, <code>second.bar.com</code> to <code>service2</code>, and any traffic whose request host header doesn't match <code>first.bar.com</code> and <code>second.bar.com</code> to <code>service3</code>.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/name-virtual-host-ingress-no-third-host.yaml download=service/networking/name-virtual-host-ingress-no-third-host.yaml><code>service/networking/name-virtual-host-ingress-no-third-host.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-name-virtual-host-ingress-no-third-host-yaml")' title="Copy service/networking/name-virtual-host-ingress-no-third-host.yaml to clipboard"></img></div><div class=includecode id=service-networking-name-virtual-host-ingress-no-third-host-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>name-virtual-host-ingress-no-third-host<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span>first.bar.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span>second.bar.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h3 id=tls>TLS</h3><p>You can secure an Ingress by specifying a <a class=glossary-tooltip title='Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a>
that contains a TLS private key and certificate. The Ingress resource only
supports a single TLS port, 443, and assumes TLS termination at the ingress point
(traffic to the Service and its Pods is in plaintext).
If the TLS configuration section in an Ingress specifies different hosts, they are
multiplexed on the same port according to the hostname specified through the
SNI TLS extension (provided the Ingress controller supports SNI). The TLS secret
must contain keys named <code>tls.crt</code> and <code>tls.key</code> that contain the certificate
and private key to use for TLS. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>testsecret-tls<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tls.crt</span>:<span style=color:#bbb> </span>base64 encoded cert<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tls.key</span>:<span style=color:#bbb> </span>base64 encoded key<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/tls<span style=color:#bbb>
</span></span></span></code></pre></div><p>Referencing this secret in an Ingress tells the Ingress controller to
secure the channel from the client to the load balancer using TLS. You need to make
sure the TLS secret you created came from a certificate that contains a Common
Name (CN), also known as a Fully Qualified Domain Name (FQDN) for <code>https-example.foo.com</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Keep in mind that TLS will not work on the default rule because the
certificates would have to be issued for all the possible sub-domains. Therefore,
<code>hosts</code> in the <code>tls</code> section need to explicitly match the <code>host</code> in the <code>rules</code>
section.</div><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/tls-example-ingress.yaml download=service/networking/tls-example-ingress.yaml><code>service/networking/tls-example-ingress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-tls-example-ingress-yaml")' title="Copy service/networking/tls-example-ingress.yaml to clipboard"></img></div><div class=includecode id=service-networking-tls-example-ingress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>tls-example-ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tls</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>hosts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- https-example.foo.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>testsecret-tls<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span>https-example.foo.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> There is a gap between TLS features supported by various Ingress
controllers. Please refer to documentation on
<a href=https://kubernetes.github.io/ingress-nginx/user-guide/tls/>nginx</a>,
<a href=https://git.k8s.io/ingress-gce/README.md#frontend-https>GCE</a>, or any other
platform specific Ingress controller to understand how TLS works in your environment.</div><h3 id=load-balancing>Load balancing</h3><p>An Ingress controller is bootstrapped with some load balancing policy settings
that it applies to all Ingress, such as the load balancing algorithm, backend
weight scheme, and others. More advanced load balancing concepts
(e.g. persistent sessions, dynamic weights) are not yet exposed through the
Ingress. You can instead get these features through the load balancer used for
a Service.</p><p>It's also worth noting that even though health checks are not exposed directly
through the Ingress, there exist parallel concepts in Kubernetes such as
<a href=/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/>readiness probes</a>
that allow you to achieve the same end result. Please review the controller
specific documentation to see how they handle health checks (for example:
<a href=https://git.k8s.io/ingress-nginx/README.md>nginx</a>, or
<a href=https://git.k8s.io/ingress-gce/README.md#health-checks>GCE</a>).</p><h2 id=updating-an-ingress>Updating an Ingress</h2><p>To update an existing Ingress to add a new Host, you can update it by editing the resource:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe ingress <span style=color:#a2f>test</span>
</span></span></code></pre></div><pre tabindex=0><code>Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     35s                loadbalancer-controller  default/test
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit ingress <span style=color:#a2f>test</span>
</span></span></code></pre></div><p>This pops up an editor with the existing configuration in YAML format.
Modify it to include the new Host:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span>foo.bar.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>host</span>:<span style=color:#bbb> </span>bar.baz.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>http</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>paths</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>backend</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>service2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>number</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>pathType</span>:<span style=color:#bbb> </span>Prefix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>..<span style=color:#bbb>
</span></span></span></code></pre></div><p>After you save your changes, kubectl updates the resource in the API server, which tells the
Ingress controller to reconfigure the load balancer.</p><p>Verify this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe ingress <span style=color:#a2f>test</span>
</span></span></code></pre></div><pre tabindex=0><code>Name:             test
Namespace:        default
Address:          178.91.123.132
Default backend:  default-http-backend:80 (10.8.2.3:8080)
Rules:
  Host         Path  Backends
  ----         ----  --------
  foo.bar.com
               /foo   service1:80 (10.8.0.90:80)
  bar.baz.com
               /foo   service2:80 (10.8.0.91:80)
Annotations:
  nginx.ingress.kubernetes.io/rewrite-target:  /
Events:
  Type     Reason  Age                From                     Message
  ----     ------  ----               ----                     -------
  Normal   ADD     45s                loadbalancer-controller  default/test
</code></pre><p>You can achieve the same outcome by invoking <code>kubectl replace -f</code> on a modified Ingress YAML file.</p><h2 id=failing-across-availability-zones>Failing across availability zones</h2><p>Techniques for spreading traffic across failure domains differ between cloud providers.
Please check the documentation of the relevant <a href=/docs/concepts/services-networking/ingress-controllers>Ingress controller</a> for details.</p><h2 id=alternatives>Alternatives</h2><p>You can expose a Service in multiple ways that don't directly involve the Ingress resource:</p><ul><li>Use <a href=/docs/concepts/services-networking/service/#loadbalancer>Service.Type=LoadBalancer</a></li><li>Use <a href=/docs/concepts/services-networking/service/#nodeport>Service.Type=NodePort</a></li></ul><h2 id=what-s-next>What's next</h2><ul><li>Learn about the <a href=/docs/reference/kubernetes-api/service-resources/ingress-v1/>Ingress</a> API</li><li>Learn about <a href=/docs/concepts/services-networking/ingress-controllers/>Ingress controllers</a></li><li><a href=/docs/tasks/access-application-cluster/ingress-minikube/>Set up Ingress on Minikube with the NGINX Controller</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5a8edeb1f2dc8e38cd6d561bb08b0d78>6.3 - Ingress Controllers</h1><div class=lead>In order for an <a href=/docs/concepts/services-networking/ingress/>Ingress</a> to work in your cluster, there must be an <em>ingress controller</em> running. You need to select at least one ingress controller and make sure it is set up in your cluster. This page lists common ingress controllers that you can deploy.</div><p>In order for the Ingress resource to work, the cluster must have an ingress controller running.</p><p>Unlike other types of controllers which run as part of the <code>kube-controller-manager</code> binary, Ingress controllers
are not started automatically with a cluster. Use this page to choose the ingress controller implementation
that best fits your cluster.</p><p>Kubernetes as a project supports and maintains <a href=https://github.com/kubernetes-sigs/aws-load-balancer-controller#readme>AWS</a>, <a href=https://git.k8s.io/ingress-gce/README.md#readme>GCE</a>, and
<a href=https://git.k8s.io/ingress-nginx/README.md#readme>nginx</a> ingress controllers.</p><h2 id=additional-controllers>Additional controllers</h2><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><ul><li><a href="https://docs.microsoft.com/azure/application-gateway/tutorial-ingress-controller-add-on-existing?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Faks%2Ftoc.json&bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json">AKS Application Gateway Ingress Controller</a> is an ingress controller that configures the <a href=https://docs.microsoft.com/azure/application-gateway/overview>Azure Application Gateway</a>.</li><li><a href=https://www.getambassador.io/>Ambassador</a> API Gateway is an <a href=https://www.envoyproxy.io>Envoy</a>-based ingress
controller.</li><li><a href=https://github.com/apache/apisix-ingress-controller>Apache APISIX ingress controller</a> is an <a href=https://github.com/apache/apisix>Apache APISIX</a>-based ingress controller.</li><li><a href=https://github.com/vmware/load-balancer-and-ingress-services-for-kubernetes>Avi Kubernetes Operator</a> provides L4-L7 load-balancing using <a href=https://avinetworks.com/>VMware NSX Advanced Load Balancer</a>.</li><li><a href=https://github.com/bfenetworks/ingress-bfe>BFE Ingress Controller</a> is a <a href=https://www.bfe-networks.net>BFE</a>-based ingress controller.</li><li>The <a href=https://github.com/citrix/citrix-k8s-ingress-controller#readme>Citrix ingress controller</a> works with
Citrix Application Delivery Controller.</li><li><a href=https://projectcontour.io/>Contour</a> is an <a href=https://www.envoyproxy.io/>Envoy</a> based ingress controller.</li><li><a href=https://getenroute.io/>EnRoute</a> is an <a href=https://www.envoyproxy.io>Envoy</a> based API gateway that can run as an ingress controller.</li><li><a href=https://github.com/megaease/easegress/blob/main/doc/reference/ingresscontroller.md>Easegress IngressController</a> is an <a href=https://megaease.com/easegress/>Easegress</a> based API gateway that can run as an ingress controller.</li><li>F5 BIG-IP <a href=https://clouddocs.f5.com/containers/latest/userguide/kubernetes/>Container Ingress Services for Kubernetes</a>
lets you use an Ingress to configure F5 BIG-IP virtual servers.</li><li><a href=https://gloo.solo.io>Gloo</a> is an open-source ingress controller based on <a href=https://www.envoyproxy.io>Envoy</a>,
which offers API gateway functionality.</li><li><a href=https://haproxy-ingress.github.io/>HAProxy Ingress</a> is an ingress controller for
<a href=https://www.haproxy.org/#desc>HAProxy</a>.</li><li>The <a href=https://github.com/haproxytech/kubernetes-ingress#readme>HAProxy Ingress Controller for Kubernetes</a>
is also an ingress controller for <a href=https://www.haproxy.org/#desc>HAProxy</a>.</li><li><a href=https://istio.io/latest/docs/tasks/traffic-management/ingress/kubernetes-ingress/>Istio Ingress</a>
is an <a href=https://istio.io/>Istio</a> based ingress controller.</li><li>The <a href=https://github.com/Kong/kubernetes-ingress-controller#readme>Kong Ingress Controller for Kubernetes</a>
is an ingress controller driving <a href=https://konghq.com/kong/>Kong Gateway</a>.</li><li><a href=https://kusk.kubeshop.io/>Kusk Gateway</a> is an OpenAPI-driven ingress controller based on <a href=https://www.envoyproxy.io>Envoy</a>.</li><li>The <a href=https://www.nginx.com/products/nginx-ingress-controller/>NGINX Ingress Controller for Kubernetes</a>
works with the <a href=https://www.nginx.com/resources/glossary/nginx/>NGINX</a> webserver (as a proxy).</li><li>The <a href=https://www.pomerium.com/docs/k8s/ingress.html>Pomerium Ingress Controller</a> is based on <a href=https://pomerium.com/>Pomerium</a>, which offers context-aware access policy.</li><li><a href=https://opensource.zalando.com/skipper/kubernetes/ingress-controller/>Skipper</a> HTTP router and reverse proxy for service composition, including use cases like Kubernetes Ingress, designed as a library to build your custom proxy.</li><li>The <a href=https://doc.traefik.io/traefik/providers/kubernetes-ingress/>Traefik Kubernetes Ingress provider</a> is an
ingress controller for the <a href=https://traefik.io/traefik/>Traefik</a> proxy.</li><li><a href=https://github.com/TykTechnologies/tyk-operator>Tyk Operator</a> extends Ingress with Custom Resources to bring API Management capabilities to Ingress. Tyk Operator works with the Open Source Tyk Gateway & Tyk Cloud control plane.</li><li><a href=https://appscode.com/products/voyager>Voyager</a> is an ingress controller for
<a href=https://www.haproxy.org/#desc>HAProxy</a>.</li></ul><h2 id=using-multiple-ingress-controllers>Using multiple Ingress controllers</h2><p>You may deploy any number of ingress controllers using <a href=/docs/concepts/services-networking/ingress/#ingress-class>ingress class</a>
within a cluster. Note the <code>.metadata.name</code> of your ingress class resource. When you create an ingress you would need that name to specify the <code>ingressClassName</code> field on your Ingress object (refer to <a href=/docs/reference/kubernetes-api/service-resources/ingress-v1/#IngressSpec>IngressSpec v1 reference</a>. <code>ingressClassName</code> is a replacement of the older <a href=/docs/concepts/services-networking/ingress/#deprecated-annotation>annotation method</a>.</p><p>If you do not specify an IngressClass for an Ingress, and your cluster has exactly one IngressClass marked as default, then Kubernetes <a href=/docs/concepts/services-networking/ingress/#default-ingress-class>applies</a> the cluster's default IngressClass to the Ingress.
You mark an IngressClass as default by setting the <a href=/docs/reference/labels-annotations-taints/#ingressclass-kubernetes-io-is-default-class><code>ingressclass.kubernetes.io/is-default-class</code> annotation</a> on that IngressClass, with the string value <code>"true"</code>.</p><p>Ideally, all ingress controllers should fulfill this specification, but the various ingress
controllers operate slightly differently.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Make sure you review your ingress controller's documentation to understand the caveats of choosing it.</div><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=/docs/concepts/services-networking/ingress/>Ingress</a>.</li><li><a href=/docs/tasks/access-application-cluster/ingress-minikube>Set up Ingress on Minikube with the NGINX Controller</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f51db1097575de8072afe1f5b156a70c>6.4 - EndpointSlices</h1><div class=lead>The EndpointSlice API is the mechanism that Kubernetes uses to let your Service scale to handle large numbers of backends, and allows the cluster to update its list of healthy backends efficiently.</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>Kubernetes' <em>EndpointSlice</em> API provides a way to track network endpoints
within a Kubernetes cluster. EndpointSlices offer a more scalable and extensible
alternative to <a href=/docs/concepts/services-networking/service/#endpoints>Endpoints</a>.</p><h2 id=endpointslice-resource>EndpointSlice API</h2><p>In Kubernetes, an EndpointSlice contains references to a set of network
endpoints. The control plane automatically creates EndpointSlices
for any Kubernetes Service that has a <a class=glossary-tooltip title='Allows users to filter a list of resources based on labels.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=selector>selector</a> specified. These EndpointSlices include
references to all the Pods that match the Service selector. EndpointSlices group
network endpoints together by unique combinations of protocol, port number, and
Service name.
The name of a EndpointSlice object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>As an example, here's a sample EndpointSlice object, that's owned by the <code>example</code>
Kubernetes Service.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>discovery.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>EndpointSlice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-abc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/service-name</span>:<span style=color:#bbb> </span>example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>addressType</span>:<span style=color:#bbb> </span>IPv4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>endpoints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>addresses</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;10.1.2.3&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>conditions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ready</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostname</span>:<span style=color:#bbb> </span>pod-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeName</span>:<span style=color:#bbb> </span>node-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>zone</span>:<span style=color:#bbb> </span>us-west2-a<span style=color:#bbb>
</span></span></span></code></pre></div><p>By default, the control plane creates and manages EndpointSlices to have no
more than 100 endpoints each. You can configure this with the
<code>--max-endpoints-per-slice</code>
<a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>
flag, up to a maximum of 1000.</p><p>EndpointSlices can act as the source of truth for
<a class=glossary-tooltip title='kube-proxy is a network proxy that runs on each node in the cluster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a> when it comes to
how to route internal traffic.</p><h3 id=address-types>Address types</h3><p>EndpointSlices support three address types:</p><ul><li>IPv4</li><li>IPv6</li><li>FQDN (Fully Qualified Domain Name)</li></ul><p>Each <code>EndpointSlice</code> object represents a specific IP address type. If you have
a Service that is available via IPv4 and IPv6, there will be at least two
<code>EndpointSlice</code> objects (one for IPv4, and one for IPv6).</p><h3 id=conditions>Conditions</h3><p>The EndpointSlice API stores conditions about endpoints that may be useful for consumers.
The three conditions are <code>ready</code>, <code>serving</code>, and <code>terminating</code>.</p><h4 id=ready>Ready</h4><p><code>ready</code> is a condition that maps to a Pod's <code>Ready</code> condition. A running Pod with the <code>Ready</code>
condition set to <code>True</code> should have this EndpointSlice condition also set to <code>true</code>. For
compatibility reasons, <code>ready</code> is NEVER <code>true</code> when a Pod is terminating. Consumers should refer
to the <code>serving</code> condition to inspect the readiness of terminating Pods. The only exception to
this rule is for Services with <code>spec.publishNotReadyAddresses</code> set to <code>true</code>. Endpoints for these
Services will always have the <code>ready</code> condition set to <code>true</code>.</p><h4 id=serving>Serving</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code></div><p><code>serving</code> is identical to the <code>ready</code> condition, except it does not account for terminating states.
Consumers of the EndpointSlice API should check this condition if they care about pod readiness while
the pod is also terminating.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Although <code>serving</code> is almost identical to <code>ready</code>, it was added to prevent break the existing meaning
of <code>ready</code>. It may be unexpected for existing clients if <code>ready</code> could be <code>true</code> for terminating
endpoints, since historically terminating endpoints were never included in the Endpoints or
EndpointSlice API to begin with. For this reason, <code>ready</code> is <em>always</em> <code>false</code> for terminating
endpoints, and a new condition <code>serving</code> was added in v1.20 so that clients can track readiness
for terminating pods independent of the existing semantics for <code>ready</code>.</div><h4 id=terminating>Terminating</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code></div><p><code>Terminating</code> is a condition that indicates whether an endpoint is terminating.
For pods, this is any pod that has a deletion timestamp set.</p><h3 id=topology>Topology information</h3><p>Each endpoint within an EndpointSlice can contain relevant topology information.
The topology information includes the location of the endpoint and information
about the corresponding Node and zone. These are available in the following
per endpoint fields on EndpointSlices:</p><ul><li><code>nodeName</code> - The name of the Node this endpoint is on.</li><li><code>zone</code> - The zone this endpoint is in.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>In the v1 API, the per endpoint <code>topology</code> was effectively removed in favor of
the dedicated fields <code>nodeName</code> and <code>zone</code>.</p><p>Setting arbitrary topology fields on the <code>endpoint</code> field of an <code>EndpointSlice</code>
resource has been deprecated and is not supported in the v1 API.
Instead, the v1 API supports setting individual <code>nodeName</code> and <code>zone</code> fields.
These fields are automatically translated between API versions. For example, the
value of the <code>"topology.kubernetes.io/zone"</code> key in the <code>topology</code> field in
the v1beta1 API is accessible as the <code>zone</code> field in the v1 API.</p></div><h3 id=management>Management</h3><p>Most often, the control plane (specifically, the endpoint slice
<a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>) creates and
manages EndpointSlice objects. There are a variety of other use cases for
EndpointSlices, such as service mesh implementations, that could result in other
entities or controllers managing additional sets of EndpointSlices.</p><p>To ensure that multiple entities can manage EndpointSlices without interfering
with each other, Kubernetes defines the
<a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=label>label</a>
<code>endpointslice.kubernetes.io/managed-by</code>, which indicates the entity managing
an EndpointSlice.
The endpoint slice controller sets <code>endpointslice-controller.k8s.io</code> as the value
for this label on all EndpointSlices it manages. Other entities managing
EndpointSlices should also set a unique value for this label.</p><h3 id=ownership>Ownership</h3><p>In most use cases, EndpointSlices are owned by the Service that the endpoint
slice object tracks endpoints for. This ownership is indicated by an owner
reference on each EndpointSlice as well as a <code>kubernetes.io/service-name</code>
label that enables simple lookups of all EndpointSlices belonging to a Service.</p><h3 id=endpointslice-mirroring>EndpointSlice mirroring</h3><p>In some cases, applications create custom Endpoints resources. To ensure that
these applications do not need to concurrently write to both Endpoints and
EndpointSlice resources, the cluster's control plane mirrors most Endpoints
resources to corresponding EndpointSlices.</p><p>The control plane mirrors Endpoints resources unless:</p><ul><li>the Endpoints resource has a <code>endpointslice.kubernetes.io/skip-mirror</code> label
set to <code>true</code>.</li><li>the Endpoints resource has a <code>control-plane.alpha.kubernetes.io/leader</code>
annotation.</li><li>the corresponding Service resource does not exist.</li><li>the corresponding Service resource has a non-nil selector.</li></ul><p>Individual Endpoints resources may translate into multiple EndpointSlices. This
will occur if an Endpoints resource has multiple subsets or includes endpoints
with multiple IP families (IPv4 and IPv6). A maximum of 1000 addresses per
subset will be mirrored to EndpointSlices.</p><h3 id=distribution-of-endpointslices>Distribution of EndpointSlices</h3><p>Each EndpointSlice has a set of ports that applies to all endpoints within the
resource. When named ports are used for a Service, Pods may end up with
different target port numbers for the same named port, requiring different
EndpointSlices. This is similar to the logic behind how subsets are grouped
with Endpoints.</p><p>The control plane tries to fill EndpointSlices as full as possible, but does not
actively rebalance them. The logic is fairly straightforward:</p><ol><li>Iterate through existing EndpointSlices, remove endpoints that are no longer
desired and update matching endpoints that have changed.</li><li>Iterate through EndpointSlices that have been modified in the first step and
fill them up with any new endpoints needed.</li><li>If there's still new endpoints left to add, try to fit them into a previously
unchanged slice and/or create new ones.</li></ol><p>Importantly, the third step prioritizes limiting EndpointSlice updates over a
perfectly full distribution of EndpointSlices. As an example, if there are 10
new endpoints to add and 2 EndpointSlices with room for 5 more endpoints each,
this approach will create a new EndpointSlice instead of filling up the 2
existing EndpointSlices. In other words, a single EndpointSlice creation is
preferrable to multiple EndpointSlice updates.</p><p>With kube-proxy running on each Node and watching EndpointSlices, every change
to an EndpointSlice becomes relatively expensive since it will be transmitted to
every Node in the cluster. This approach is intended to limit the number of
changes that need to be sent to every Node, even if it may result with multiple
EndpointSlices that are not full.</p><p>In practice, this less than ideal distribution should be rare. Most changes
processed by the EndpointSlice controller will be small enough to fit in an
existing EndpointSlice, and if not, a new EndpointSlice is likely going to be
necessary soon anyway. Rolling updates of Deployments also provide a natural
repacking of EndpointSlices with all Pods and their corresponding endpoints
getting replaced.</p><h3 id=duplicate-endpoints>Duplicate endpoints</h3><p>Due to the nature of EndpointSlice changes, endpoints may be represented in more
than one EndpointSlice at the same time. This naturally occurs as changes to
different EndpointSlice objects can arrive at the Kubernetes client watch / cache
at different times.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Clients of the EndpointSlice API must iterate through all the existing EndpointSlices
associated to a Service and build a complete list of unique network endpoints. It is
important to mention that endpoints may be duplicated in different EndointSlices.</p><p>You can find a reference implementation for how to perform this endpoint aggregation
and deduplication as part of the <code>EndpointSliceCache</code> code within <code>kube-proxy</code>.</p></div><h2 id=motivation>Comparison with Endpoints</h2><p>The original Endpoints API provided a simple and straightforward way of
tracking network endpoints in Kubernetes. As Kubernetes clusters
and <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Services>Services</a> grew to handle
more traffic and to send more traffic to more backend Pods, the
limitations of that original API became more visible.
Most notably, those included challenges with scaling to larger numbers of
network endpoints.</p><p>Since all network endpoints for a Service were stored in a single Endpoints
object, those Endpoints objects could get quite large. For Services that stayed
stable (the same set of endpoints over a long period of time) the impact was
less noticeable; even then, some use cases of Kubernetes weren't well served.</p><p>When a Service had a lot of backend endpoints and the workload was either
scaling frequently, or rolling out new changes frequently, each update to
the single Endpoints object for that Service meant a lot of traffic between
Kubernetes cluster components (within the control plane, and also between
nodes and the API server). This extra traffic also had a cost in terms of
CPU use.</p><p>With EndpointSlices, adding or removing a single Pod triggers the same <em>number</em>
of updates to clients that are watching for changes, but the size of those
update message is much smaller at large scale.</p><p>EndpointSlices also enabled innovation around new features such dual-stack
networking and topology-aware routing.</p><h2 id=what-s-next>What's next</h2><ul><li>Follow the <a href=/docs/tutorials/services/connect-applications-service/>Connecting Applications with Services</a> tutorial</li><li>Read the <a href=/docs/reference/kubernetes-api/service-resources/endpoint-slice-v1/>API reference</a> for the EndpointSlice API</li><li>Read the <a href=/docs/reference/kubernetes-api/service-resources/endpoints-v1/>API reference</a> for the Endpoints API</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ded1daafdcd293023ee333728007ca61>6.5 - Network Policies</h1><div class=lead>If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), NetworkPolicies allow you to specify rules for traffic flow within your cluster, and also between Pods and the outside world. Your cluster must use a network plugin that supports NetworkPolicy enforcement.</div><p>If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster. NetworkPolicies are an application-centric construct which allow you to specify how a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=pod>pod</a> is allowed to communicate with various network "entities" (we use the word "entity" here to avoid overloading the more common terms such as "endpoints" and "services", which have specific Kubernetes connotations) over the network. NetworkPolicies apply to a connection with a pod on one or both ends, and are not relevant to other connections.</p><p>The entities that a Pod can communicate with are identified through a combination of the following 3 identifiers:</p><ol><li>Other pods that are allowed (exception: a pod cannot block access to itself)</li><li>Namespaces that are allowed</li><li>IP blocks (exception: traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the Pod or the node)</li></ol><p>When defining a pod- or namespace- based NetworkPolicy, you use a <a class=glossary-tooltip title='Allows users to filter a list of resources based on labels.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels/ target=_blank aria-label=selector>selector</a> to specify what traffic is allowed to and from the Pod(s) that match the selector.</p><p>Meanwhile, when IP based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).</p><h2 id=prerequisites>Prerequisites</h2><p>Network policies are implemented by the <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>network plugin</a>. To use network policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.</p><h2 id=the-two-sorts-of-pod-isolation>The Two Sorts of Pod Isolation</h2><p>There are two sorts of isolation for a pod: isolation for egress, and isolation for ingress. They concern what connections may be established. "Isolation" here is not absolute, rather it means "some restrictions apply". The alternative, "non-isolated for $direction", means that no restrictions apply in the stated direction. The two sorts of isolation (or not) are declared independently, and are both relevant for a connection from one pod to another.</p><p>By default, a pod is non-isolated for egress; all outbound connections are allowed. A pod is isolated for egress if there is any NetworkPolicy that both selects the pod and has "Egress" in its <code>policyTypes</code>; we say that such a policy applies to the pod for egress. When a pod is isolated for egress, the only allowed connections from the pod are those allowed by the <code>egress</code> list of some NetworkPolicy that applies to the pod for egress. The effects of those <code>egress</code> lists combine additively.</p><p>By default, a pod is non-isolated for ingress; all inbound connections are allowed. A pod is isolated for ingress if there is any NetworkPolicy that both selects the pod and has "Ingress" in its <code>policyTypes</code>; we say that such a policy applies to the pod for ingress. When a pod is isolated for ingress, the only allowed connections into the pod are those from the pod's node and those allowed by the <code>ingress</code> list of some NetworkPolicy that applies to the pod for ingress. The effects of those <code>ingress</code> lists combine additively.</p><p>Network policies do not conflict; they are additive. If any policy or policies apply to a given pod for a given direction, the connections allowed in that direction from that pod is the union of what the applicable policies allow. Thus, order of evaluation does not affect the policy result.</p><p>For a connection from a source pod to a destination pod to be allowed, both the egress policy on the source pod and the ingress policy on the destination pod need to allow the connection. If either side does not allow the connection, it will not happen.</p><h2 id=networkpolicy-resource>The NetworkPolicy resource</h2><p>See the <a href=/docs/reference/generated/kubernetes-api/v1.25/#networkpolicy-v1-networking-k8s-io>NetworkPolicy</a> reference for a full definition of the resource.</p><p>An example NetworkPolicy might look like this:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/networkpolicy.yaml download=service/networking/networkpolicy.yaml><code>service/networking/networkpolicy.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-networkpolicy-yaml")' title="Copy service/networking/networkpolicy.yaml to clipboard"></img></div><div class=includecode id=service-networking-networkpolicy-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-network-policy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>db<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- Egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>ipBlock</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cidr</span>:<span style=color:#bbb> </span><span style=color:#666>172.17.0.0</span>/16<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>except</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:#666>172.17.1.0</span>/24<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>project</span>:<span style=color:#bbb> </span>myproject<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>6379</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>to</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>ipBlock</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cidr</span>:<span style=color:#bbb> </span><span style=color:#666>10.0.0.0</span>/24<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>5978</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> POSTing this to the API server for your cluster will have no effect unless your chosen networking solution supports network policy.</div><p><strong>Mandatory Fields</strong>: As with all other Kubernetes config, a NetworkPolicy
needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields. For general information
about working with config files, see
<a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>Configure a Pod to Use a ConfigMap</a>,
and <a href=/docs/concepts/overview/working-with-objects/object-management>Object Management</a>.</p><p><strong>spec</strong>: NetworkPolicy <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status>spec</a> has all the information needed to define a particular network policy in the given namespace.</p><p><strong>podSelector</strong>: Each NetworkPolicy includes a <code>podSelector</code> which selects the grouping of pods to which the policy applies. The example policy selects pods with the label "role=db". An empty <code>podSelector</code> selects all pods in the namespace.</p><p><strong>policyTypes</strong>: Each NetworkPolicy includes a <code>policyTypes</code> list which may include either <code>Ingress</code>, <code>Egress</code>, or both. The <code>policyTypes</code> field indicates whether or not the given policy applies to ingress traffic to selected pod, egress traffic from selected pods, or both. If no <code>policyTypes</code> are specified on a NetworkPolicy then by default <code>Ingress</code> will always be set and <code>Egress</code> will be set if the NetworkPolicy has any egress rules.</p><p><strong>ingress</strong>: Each NetworkPolicy may include a list of allowed <code>ingress</code> rules. Each rule allows traffic which matches both the <code>from</code> and <code>ports</code> sections. The example policy contains a single rule, which matches traffic on a single port, from one of three sources, the first specified via an <code>ipBlock</code>, the second via a <code>namespaceSelector</code> and the third via a <code>podSelector</code>.</p><p><strong>egress</strong>: Each NetworkPolicy may include a list of allowed <code>egress</code> rules. Each rule allows traffic which matches both the <code>to</code> and <code>ports</code> sections. The example policy contains a single rule, which matches traffic on a single port to any destination in <code>10.0.0.0/24</code>.</p><p>So, the example NetworkPolicy:</p><ol><li><p>isolates "role=db" pods in the "default" namespace for both ingress and egress traffic (if they weren't already isolated)</p></li><li><p>(Ingress rules) allows connections to all pods in the "default" namespace with the label "role=db" on TCP port 6379 from:</p><ul><li>any pod in the "default" namespace with the label "role=frontend"</li><li>any pod in a namespace with the label "project=myproject"</li><li>IP addresses in the ranges 172.17.0.0–172.17.0.255 and 172.17.2.0–172.17.255.255 (ie, all of 172.17.0.0/16 except 172.17.1.0/24)</li></ul></li><li><p>(Egress rules) allows connections from any pod in the "default" namespace with the label "role=db" to CIDR 10.0.0.0/24 on TCP port 5978</p></li></ol><p>See the <a href=/docs/tasks/administer-cluster/declare-network-policy/>Declare Network Policy</a> walkthrough for further examples.</p><h2 id=behavior-of-to-and-from-selectors>Behavior of <code>to</code> and <code>from</code> selectors</h2><p>There are four kinds of selectors that can be specified in an <code>ingress</code> <code>from</code> section or <code>egress</code> <code>to</code> section:</p><p><strong>podSelector</strong>: This selects particular Pods in the same namespace as the NetworkPolicy which should be allowed as ingress sources or egress destinations.</p><p><strong>namespaceSelector</strong>: This selects particular namespaces for which all Pods should be allowed as ingress sources or egress destinations.</p><p><strong>namespaceSelector</strong> <em>and</em> <strong>podSelector</strong>: A single <code>to</code>/<code>from</code> entry that specifies both <code>namespaceSelector</code> and <code>podSelector</code> selects particular Pods within particular namespaces. Be careful to use correct YAML syntax; this policy:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>alice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>client<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>contains a single <code>from</code> element allowing connections from Pods with the label <code>role=client</code> in namespaces with the label <code>user=alice</code>. But <em>this</em> policy:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>from</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>namespaceSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>user</span>:<span style=color:#bbb> </span>alice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>client<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>contains two elements in the <code>from</code> array, and allows connections from Pods in the local Namespace with the label <code>role=client</code>, <em>or</em> from any Pod in any namespace with the label <code>user=alice</code>.</p><p>When in doubt, use <code>kubectl describe</code> to see how Kubernetes has interpreted the policy.</p><p><a name=behavior-of-ipblock-selectors></a>
<strong>ipBlock</strong>: This selects particular IP CIDR ranges to allow as ingress sources or egress destinations. These should be cluster-external IPs, since Pod IPs are ephemeral and unpredictable.</p><p>Cluster ingress and egress mechanisms often require rewriting the source or destination IP
of packets. In cases where this happens, it is not defined whether this happens before or
after NetworkPolicy processing, and the behavior may be different for different
combinations of network plugin, cloud provider, <code>Service</code> implementation, etc.</p><p>In the case of ingress, this means that in some cases you may be able to filter incoming
packets based on the actual original source IP, while in other cases, the "source IP" that
the NetworkPolicy acts on may be the IP of a <code>LoadBalancer</code> or of the Pod's node, etc.</p><p>For egress, this means that connections from pods to <code>Service</code> IPs that get rewritten to
cluster-external IPs may or may not be subject to <code>ipBlock</code>-based policies.</p><h2 id=default-policies>Default policies</h2><p>By default, if no policies exist in a namespace, then all ingress and egress traffic is allowed to and from pods in that namespace. The following examples let you change the default behavior
in that namespace.</p><h3 id=default-deny-all-ingress-traffic>Default deny all ingress traffic</h3><p>You can create a "default" ingress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any ingress traffic to those pods.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-default-deny-ingress.yaml download=service/networking/network-policy-default-deny-ingress.yaml><code>service/networking/network-policy-default-deny-ingress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-default-deny-ingress-yaml")' title="Copy service/networking/network-policy-default-deny-ingress.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-default-deny-ingress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny-ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>This ensures that even pods that aren't selected by any other NetworkPolicy will still be isolated for ingress. This policy does not affect isolation for egress from any pod.</p><h3 id=allow-all-ingress-traffic>Allow all ingress traffic</h3><p>If you want to allow all incoming connections to all pods in a namespace, you can create a policy that explicitly allows that.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-allow-all-ingress.yaml download=service/networking/network-policy-allow-all-ingress.yaml><code>service/networking/network-policy-allow-all-ingress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-allow-all-ingress-yaml")' title="Copy service/networking/network-policy-allow-all-ingress.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-allow-all-ingress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>allow-all-ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ingress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- {}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>With this policy in place, no additional policy or policies can cause any incoming connection to those pods to be denied. This policy has no effect on isolation for egress from any pod.</p><h3 id=default-deny-all-egress-traffic>Default deny all egress traffic</h3><p>You can create a "default" egress isolation policy for a namespace by creating a NetworkPolicy that selects all pods but does not allow any egress traffic from those pods.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-default-deny-egress.yaml download=service/networking/network-policy-default-deny-egress.yaml><code>service/networking/network-policy-default-deny-egress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-default-deny-egress-yaml")' title="Copy service/networking/network-policy-default-deny-egress.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-default-deny-egress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny-egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed egress traffic. This policy does not
change the ingress isolation behavior of any pod.</p><h3 id=allow-all-egress-traffic>Allow all egress traffic</h3><p>If you want to allow all connections from all pods in a namespace, you can create a policy that explicitly allows all outgoing connections from pods in that namespace.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-allow-all-egress.yaml download=service/networking/network-policy-allow-all-egress.yaml><code>service/networking/network-policy-allow-all-egress.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-allow-all-egress-yaml")' title="Copy service/networking/network-policy-allow-all-egress.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-allow-all-egress-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>allow-all-egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- {}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>With this policy in place, no additional policy or policies can cause any outgoing connection from those pods to be denied. This policy has no effect on isolation for ingress to any pod.</p><h3 id=default-deny-all-ingress-and-all-egress-traffic>Default deny all ingress and all egress traffic</h3><p>You can create a "default" policy for a namespace which prevents all ingress AND egress traffic by creating the following NetworkPolicy in that namespace.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/network-policy-default-deny-all.yaml download=service/networking/network-policy-default-deny-all.yaml><code>service/networking/network-policy-default-deny-all.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-network-policy-default-deny-all-yaml")' title="Copy service/networking/network-policy-default-deny-all.yaml to clipboard"></img></div><div class=includecode id=service-networking-network-policy-default-deny-all-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-deny-all<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Ingress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>This ensures that even pods that aren't selected by any other NetworkPolicy will not be allowed ingress or egress traffic.</p><h2 id=sctp-support>SCTP support</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>As a stable feature, this is enabled by default. To disable SCTP at a cluster level, you (or your cluster administrator) will need to disable the <code>SCTPSupport</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> for the API server with <code>--feature-gates=SCTPSupport=false,…</code>.
When the feature gate is enabled, you can set the <code>protocol</code> field of a NetworkPolicy to <code>SCTP</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must be using a <a class=glossary-tooltip title='Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label=CNI>CNI</a> plugin that supports SCTP protocol NetworkPolicies.</div><h2 id=targeting-a-range-of-ports>Targeting a range of ports</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>When writing a NetworkPolicy, you can target a range of ports instead of a single port.</p><p>This is achievable with the usage of the <code>endPort</code> field, as the following example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>networking.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>NetworkPolicy<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>multi-port-egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>db<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>policyTypes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- Egress<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>egress</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>to</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>ipBlock</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cidr</span>:<span style=color:#bbb> </span><span style=color:#666>10.0.0.0</span>/24<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>32000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>endPort</span>:<span style=color:#bbb> </span><span style=color:#666>32768</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The above rule allows any Pod with label <code>role=db</code> on the namespace <code>default</code> to communicate
with any IP within the range <code>10.0.0.0/24</code> over TCP, provided that the target
port is between the range 32000 and 32768.</p><p>The following restrictions apply when using this field:</p><ul><li>The <code>endPort</code> field must be equal to or greater than the <code>port</code> field.</li><li><code>endPort</code> can only be defined if <code>port</code> is also defined.</li><li>Both ports must be numeric.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Your cluster must be using a <a class=glossary-tooltip title='Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label=CNI>CNI</a> plugin that
supports the <code>endPort</code> field in NetworkPolicy specifications.
If your <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>network plugin</a>
does not support the <code>endPort</code> field and you specify a NetworkPolicy with that,
the policy will be applied only for the single <code>port</code> field.</div><h2 id=targeting-a-namespace-by-its-name>Targeting a Namespace by its name</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.22 [stable]</code></div><p>The Kubernetes control plane sets an immutable label <code>kubernetes.io/metadata.name</code> on all
namespaces, provided that the <code>NamespaceDefaultLabelName</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> is enabled.
The value of the label is the namespace name.</p><p>While NetworkPolicy cannot target a namespace by its name with some object field, you can use the
standardized label to target a specific namespace.</p><h2 id=what-you-can-t-do-with-network-policies-at-least-not-yet>What you can't do with network policies (at least, not yet)</h2><p>As of Kubernetes 1.25, the following functionality does not exist in the NetworkPolicy API, but you might be able to implement workarounds using Operating System components (such as SELinux, OpenVSwitch, IPTables, and so on) or Layer 7 technologies (Ingress controllers, Service Mesh implementations) or admission controllers. In case you are new to network security in Kubernetes, its worth noting that the following User Stories cannot (yet) be implemented using the NetworkPolicy API.</p><ul><li>Forcing internal cluster traffic to go through a common gateway (this might be best served with a service mesh or other proxy).</li><li>Anything TLS related (use a service mesh or ingress controller for this).</li><li>Node specific policies (you can use CIDR notation for these, but you cannot target nodes by their Kubernetes identities specifically).</li><li>Targeting of services by name (you can, however, target pods or namespaces by their <a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a>, which is often a viable workaround).</li><li>Creation or management of "Policy requests" that are fulfilled by a third party.</li><li>Default policies which are applied to all namespaces or pods (there are some third party Kubernetes distributions and projects which can do this).</li><li>Advanced policy querying and reachability tooling.</li><li>The ability to log network security events (for example connections that are blocked or accepted).</li><li>The ability to explicitly deny policies (currently the model for NetworkPolicies are deny by default, with only the ability to add allow rules).</li><li>The ability to prevent loopback or incoming host traffic (Pods cannot currently block localhost access, nor do they have the ability to block access from their resident node).</li></ul><h2 id=what-s-next>What's next</h2><ul><li>See the <a href=/docs/tasks/administer-cluster/declare-network-policy/>Declare Network Policy</a>
walkthrough for further examples.</li><li>See more <a href=https://github.com/ahmetb/kubernetes-network-policy-recipes>recipes</a> for common scenarios enabled by the NetworkPolicy resource.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-91cb8a4438b003df11bc1c426a81b756>6.6 - DNS for Services and Pods</h1><div class=lead>Your workload can discover Services within your cluster using DNS; this page explains how that works.</div><p>Kubernetes creates DNS records for Services and Pods. You can contact
Services with consistent DNS names instead of IP addresses.</p><p>Kubernetes DNS schedules a DNS Pod and Service on the cluster, and configures
the kubelets to tell individual containers to use the DNS Service's IP to
resolve DNS names.</p><p>Every Service defined in the cluster (including the DNS server itself) is
assigned a DNS name. By default, a client Pod's DNS search list includes the
Pod's own namespace and the cluster's default domain.</p><h3 id=namespaces-of-services>Namespaces of Services</h3><p>A DNS query may return different results based on the namespace of the Pod making
it. DNS queries that don't specify a namespace are limited to the Pod's
namespace. Access Services in other namespaces by specifying it in the DNS query.</p><p>For example, consider a Pod in a <code>test</code> namespace. A <code>data</code> Service is in
the <code>prod</code> namespace.</p><p>A query for <code>data</code> returns no results, because it uses the Pod's <code>test</code> namespace.</p><p>A query for <code>data.prod</code> returns the intended result, because it specifies the
namespace.</p><p>DNS queries may be expanded using the Pod's <code>/etc/resolv.conf</code>. Kubelet
sets this file for each Pod. For example, a query for just <code>data</code> may be
expanded to <code>data.test.svc.cluster.local</code>. The values of the <code>search</code> option
are used to expand queries. To learn more about DNS queries, see
<a href=https://www.man7.org/linux/man-pages/man5/resolv.conf.5.html>the <code>resolv.conf</code> manual page.</a></p><pre tabindex=0><code>nameserver 10.32.0.10
search &lt;namespace&gt;.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
</code></pre><p>In summary, a Pod in the <em>test</em> namespace can successfully resolve either
<code>data.prod</code> or <code>data.prod.svc.cluster.local</code>.</p><h3 id=dns-records>DNS Records</h3><p>What objects get DNS records?</p><ol><li>Services</li><li>Pods</li></ol><p>The following sections detail the supported DNS record types and layout that is
supported. Any other layout or names or queries that happen to work are
considered implementation details and are subject to change without warning.
For more up-to-date specification, see
<a href=https://github.com/kubernetes/dns/blob/master/docs/specification.md>Kubernetes DNS-Based Service Discovery</a>.</p><h2 id=services>Services</h2><h3 id=a-aaaa-records>A/AAAA records</h3><p>"Normal" (not headless) Services are assigned a DNS A or AAAA record,
depending on the IP family of the Service, for a name of the form
<code>my-svc.my-namespace.svc.cluster-domain.example</code>. This resolves to the cluster IP
of the Service.</p><p>"Headless" (without a cluster IP) Services are also assigned a DNS A or AAAA record,
depending on the IP family of the Service, for a name of the form
<code>my-svc.my-namespace.svc.cluster-domain.example</code>. Unlike normal
Services, this resolves to the set of IPs of the Pods selected by the Service.
Clients are expected to consume the set or else use standard round-robin
selection from the set.</p><h3 id=srv-records>SRV records</h3><p>SRV Records are created for named ports that are part of normal or <a href=/docs/concepts/services-networking/service/#headless-services>Headless
Services</a>.
For each named port, the SRV record would have the form
<code>_my-port-name._my-port-protocol.my-svc.my-namespace.svc.cluster-domain.example</code>.
For a regular Service, this resolves to the port number and the domain name:
<code>my-svc.my-namespace.svc.cluster-domain.example</code>.
For a headless Service, this resolves to multiple answers, one for each Pod
that is backing the Service, and contains the port number and the domain name of the Pod
of the form <code>auto-generated-name.my-svc.my-namespace.svc.cluster-domain.example</code>.</p><h2 id=pods>Pods</h2><h3 id=a-aaaa-records-1>A/AAAA records</h3><p>In general a Pod has the following DNS resolution:</p><p><code>pod-ip-address.my-namespace.pod.cluster-domain.example</code>.</p><p>For example, if a Pod in the <code>default</code> namespace has the IP address 172.17.0.3,
and the domain name for your cluster is <code>cluster.local</code>, then the Pod has a DNS name:</p><p><code>172-17-0-3.default.pod.cluster.local</code>.</p><p>Any Pods exposed by a Service have the following DNS resolution available:</p><p><code>pod-ip-address.service-name.my-namespace.svc.cluster-domain.example</code>.</p><h3 id=pod-s-hostname-and-subdomain-fields>Pod's hostname and subdomain fields</h3><p>Currently when a Pod is created, its hostname is the Pod's <code>metadata.name</code> value.</p><p>The Pod spec has an optional <code>hostname</code> field, which can be used to specify the
Pod's hostname. When specified, it takes precedence over the Pod's name to be
the hostname of the Pod. For example, given a Pod with <code>hostname</code> set to
"<code>my-host</code>", the Pod will have its hostname set to "<code>my-host</code>".</p><p>The Pod spec also has an optional <code>subdomain</code> field which can be used to specify
its subdomain. For example, a Pod with <code>hostname</code> set to "<code>foo</code>", and <code>subdomain</code>
set to "<code>bar</code>", in namespace "<code>my-namespace</code>", will have the fully qualified
domain name (FQDN) "<code>foo.bar.my-namespace.svc.cluster-domain.example</code>".</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>default-subdomain<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span>None<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb> </span><span style=color:#080;font-style:italic># Actually, no port is needed.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>1234</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>1234</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hostname</span>:<span style=color:#bbb> </span>busybox-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>subdomain</span>:<span style=color:#bbb> </span>default-subdomain<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- sleep<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;3600&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hostname</span>:<span style=color:#bbb> </span>busybox-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>subdomain</span>:<span style=color:#bbb> </span>default-subdomain<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- sleep<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;3600&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span></code></pre></div><p>If there exists a headless Service in the same namespace as the Pod and with
the same name as the subdomain, the cluster's DNS Server also returns an A or AAAA
record for the Pod's fully qualified hostname.
For example, given a Pod with the hostname set to "<code>busybox-1</code>" and the subdomain set to
"<code>default-subdomain</code>", and a headless Service named "<code>default-subdomain</code>" in
the same namespace, the Pod will see its own FQDN as
"<code>busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example</code>". DNS serves an
A or AAAA record at that name, pointing to the Pod's IP. Both Pods "<code>busybox1</code>" and
"<code>busybox2</code>" can have their distinct A or AAAA records.</p><p>An <a class=glossary-tooltip title='A way to group network endpoints together with Kubernetes resources.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/endpoint-slices/ target=_blank aria-label=EndpointSlice>EndpointSlice</a> can specify
the DNS hostname for any endpoint addresses, along with its IP.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Because A or AAAA records are not created for Pod names, <code>hostname</code> is required for the Pod's A or AAAA
record to be created. A Pod with no <code>hostname</code> but with <code>subdomain</code> will only create the
A or AAAA record for the headless Service (<code>default-subdomain.my-namespace.svc.cluster-domain.example</code>),
pointing to the Pod's IP address. Also, Pod needs to become ready in order to have a
record unless <code>publishNotReadyAddresses=True</code> is set on the Service.</div><h3 id=pod-sethostnameasfqdn-field>Pod's setHostnameAsFQDN field</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [stable]</code></div><p>When a Pod is configured to have fully qualified domain name (FQDN), its hostname is the short hostname. For example, if you have a Pod with the fully qualified domain name <code>busybox-1.default-subdomain.my-namespace.svc.cluster-domain.example</code>, then by default the <code>hostname</code> command inside that Pod returns <code>busybox-1</code> and the <code>hostname --fqdn</code> command returns the FQDN.</p><p>When you set <code>setHostnameAsFQDN: true</code> in the Pod spec, the kubelet writes the Pod's FQDN into the hostname for that Pod's namespace. In this case, both <code>hostname</code> and <code>hostname --fqdn</code> return the Pod's FQDN.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>In Linux, the hostname field of the kernel (the <code>nodename</code> field of <code>struct utsname</code>) is limited to 64 characters.</p><p>If a Pod enables this feature and its FQDN is longer than 64 character, it will fail to start. The Pod will remain in <code>Pending</code> status (<code>ContainerCreating</code> as seen by <code>kubectl</code>) generating error events, such as Failed to construct FQDN from Pod hostname and cluster domain, FQDN <code>long-FQDN</code> is too long (64 characters is the max, 70 characters requested). One way of improving user experience for this scenario is to create an <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>admission webhook controller</a> to control FQDN size when users create top level objects, for example, Deployment.</p></div><h3 id=pod-s-dns-policy>Pod's DNS Policy</h3><p>DNS policies can be set on a per-Pod basis. Currently Kubernetes supports the
following Pod-specific DNS policies. These policies are specified in the
<code>dnsPolicy</code> field of a Pod Spec.</p><ul><li>"<code>Default</code>": The Pod inherits the name resolution configuration from the node
that the Pods run on.
See <a href=/docs/tasks/administer-cluster/dns-custom-nameservers>related discussion</a>
for more details.</li><li>"<code>ClusterFirst</code>": Any DNS query that does not match the configured cluster
domain suffix, such as "<code>www.kubernetes.io</code>", is forwarded to an upstream
nameserver by the DNS server. Cluster administrators may have extra
stub-domain and upstream DNS servers configured.
See <a href=/docs/tasks/administer-cluster/dns-custom-nameservers>related discussion</a>
for details on how DNS queries are handled in those cases.</li><li>"<code>ClusterFirstWithHostNet</code>": For Pods running with hostNetwork, you should
explicitly set its DNS policy to "<code>ClusterFirstWithHostNet</code>". Otherwise, Pods
running with hostNetwork and <code>"ClusterFirst"</code> will fallback to the behavior
of the <code>"Default"</code> policy.<ul><li>Note: This is not supported on Windows. See <a href=#dns-windows>below</a> for details</li></ul></li><li>"<code>None</code>": It allows a Pod to ignore DNS settings from the Kubernetes
environment. All DNS settings are supposed to be provided using the
<code>dnsConfig</code> field in the Pod Spec.
See <a href=#pod-dns-config>Pod's DNS config</a> subsection below.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> "Default" is not the default DNS policy. If <code>dnsPolicy</code> is not
explicitly specified, then "ClusterFirst" is used.</div><p>The example below shows a Pod with its DNS policy set to
"<code>ClusterFirstWithHostNet</code>" because it has <code>hostNetwork</code> set to <code>true</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- sleep<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;3600&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Always<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hostNetwork</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dnsPolicy</span>:<span style=color:#bbb> </span>ClusterFirstWithHostNet<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=pod-dns-config>Pod's DNS Config</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><p>Pod's DNS Config allows users more control on the DNS settings for a Pod.</p><p>The <code>dnsConfig</code> field is optional and it can work with any <code>dnsPolicy</code> settings.
However, when a Pod's <code>dnsPolicy</code> is set to "<code>None</code>", the <code>dnsConfig</code> field has
to be specified.</p><p>Below are the properties a user can specify in the <code>dnsConfig</code> field:</p><ul><li><code>nameservers</code>: a list of IP addresses that will be used as DNS servers for the
Pod. There can be at most 3 IP addresses specified. When the Pod's <code>dnsPolicy</code>
is set to "<code>None</code>", the list must contain at least one IP address, otherwise
this property is optional.
The servers listed will be combined to the base nameservers generated from the
specified DNS policy with duplicate addresses removed.</li><li><code>searches</code>: a list of DNS search domains for hostname lookup in the Pod.
This property is optional. When specified, the provided list will be merged
into the base search domain names generated from the chosen DNS policy.
Duplicate domain names are removed.
Kubernetes allows for at most 6 search domains.</li><li><code>options</code>: an optional list of objects where each object may have a <code>name</code>
property (required) and a <code>value</code> property (optional). The contents in this
property will be merged to the options generated from the specified DNS policy.
Duplicate entries are removed.</li></ul><p>The following is an example Pod with custom DNS settings:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/custom-dns.yaml download=service/networking/custom-dns.yaml><code>service/networking/custom-dns.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-custom-dns-yaml")' title="Copy service/networking/custom-dns.yaml to clipboard"></img></div><div class=includecode id=service-networking-custom-dns-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dns-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dnsPolicy</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;None&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dnsConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nameservers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#666>1.2.3.4</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>searches</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- ns1.svc.cluster-domain.example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- my.dns.search.suffix<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>options</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ndots<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>edns0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>When the Pod above is created, the container <code>test</code> gets the following contents
in its <code>/etc/resolv.conf</code> file:</p><pre tabindex=0><code>nameserver 1.2.3.4
search ns1.svc.cluster-domain.example my.dns.search.suffix
options ndots:2 edns0
</code></pre><p>For IPv6 setup, search path and name server should be set up like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>exec</span> -it dns-example -- cat /etc/resolv.conf
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>nameserver 2001:db8:30::a
search default.svc.cluster-domain.example svc.cluster-domain.example cluster-domain.example
options ndots:5
</code></pre><h4 id=expanded-dns-configuration>Expanded DNS Configuration</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.22 [alpha]</code></div><p>By default, for Pod's DNS Config, Kubernetes allows at most 6 search domains and
a list of search domains of up to 256 characters.</p><p>If the feature gate <code>ExpandedDNSConfig</code> is enabled for the kube-apiserver and
the kubelet, it is allowed for Kubernetes to have at most 32 search domains and
a list of search domains of up to 2048 characters.</p><h2 id=dns-windows>DNS resolution on Windows nodes</h2><ul><li>ClusterFirstWithHostNet is not supported for Pods that run on Windows nodes.
Windows treats all names with a <code>.</code> as a FQDN and skips FQDN resolution.</li><li>On Windows, there are multiple DNS resolvers that can be used. As these come with
slightly different behaviors, using the
<a href=https://docs.microsoft.com/powershell/module/dnsclient/resolve-dnsname><code>Resolve-DNSName</code></a>
powershell cmdlet for name query resolutions is recommended.</li><li>On Linux, you have a DNS suffix list, which is used after resolution of a name as fully
qualified has failed.
On Windows, you can only have 1 DNS suffix, which is the DNS suffix associated with that
Pod's namespace (example: <code>mydns.svc.cluster.local</code>). Windows can resolve FQDNs, Services,
or network name which can be resolved with this single suffix. For example, a Pod spawned
in the <code>default</code> namespace, will have the DNS suffix <code>default.svc.cluster.local</code>.
Inside a Windows Pod, you can resolve both <code>kubernetes.default.svc.cluster.local</code>
and <code>kubernetes</code>, but not the partially qualified names (<code>kubernetes.default</code> or
<code>kubernetes.default.svc</code>).</li></ul><h2 id=what-s-next>What's next</h2><p>For guidance on administering DNS configurations, check
<a href=/docs/tasks/administer-cluster/dns-custom-nameservers/>Configure DNS Service</a></p></div><div class=td-content style=page-break-before:always><h1 id=pg-21f8d19c60c33914baab66224c3d46a7>6.7 - IPv4/IPv6 dual-stack</h1><div class=lead>Kubernetes lets you configure single-stack IPv4 networking, single-stack IPv6 networking, or dual stack networking with both network families active. This page explains how.</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code></div><p>IPv4/IPv6 dual-stack networking enables the allocation of both IPv4 and IPv6 addresses to
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> and <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Services>Services</a>.</p><p>IPv4/IPv6 dual-stack networking is enabled by default for your Kubernetes cluster starting in
1.21, allowing the simultaneous assignment of both IPv4 and IPv6 addresses.</p><h2 id=supported-features>Supported Features</h2><p>IPv4/IPv6 dual-stack on your Kubernetes cluster provides the following features:</p><ul><li>Dual-stack Pod networking (a single IPv4 and IPv6 address assignment per Pod)</li><li>IPv4 and IPv6 enabled Services</li><li>Pod off-cluster egress routing (eg. the Internet) via both IPv4 and IPv6 interfaces</li></ul><h2 id=prerequisites>Prerequisites</h2><p>The following prerequisites are needed in order to utilize IPv4/IPv6 dual-stack Kubernetes clusters:</p><ul><li><p>Kubernetes 1.20 or later</p><p>For information about using dual-stack services with earlier
Kubernetes versions, refer to the documentation for that version
of Kubernetes.</p></li><li><p>Provider support for dual-stack networking (Cloud provider or otherwise must be able to provide
Kubernetes nodes with routable IPv4/IPv6 network interfaces)</p></li><li><p>A <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>network plugin</a> that
supports dual-stack networking.</p></li></ul><h2 id=configure-ipv4-ipv6-dual-stack>Configure IPv4/IPv6 dual-stack</h2><p>To configure IPv4/IPv6 dual-stack, set dual-stack cluster network assignments:</p><ul><li>kube-apiserver:<ul><li><code>--service-cluster-ip-range=&lt;IPv4 CIDR>,&lt;IPv6 CIDR></code></li></ul></li><li>kube-controller-manager:<ul><li><code>--cluster-cidr=&lt;IPv4 CIDR>,&lt;IPv6 CIDR></code></li><li><code>--service-cluster-ip-range=&lt;IPv4 CIDR>,&lt;IPv6 CIDR></code></li><li><code>--node-cidr-mask-size-ipv4|--node-cidr-mask-size-ipv6</code> defaults to /24 for IPv4 and /64 for IPv6</li></ul></li><li>kube-proxy:<ul><li><code>--cluster-cidr=&lt;IPv4 CIDR>,&lt;IPv6 CIDR></code></li></ul></li><li>kubelet:<ul><li>when there is no <code>--cloud-provider</code> the administrator can pass a comma-separated pair of IP
addresses via <code>--node-ip</code> to manually configure dual-stack <code>.status.addresses</code> for that Node.
If a Pod runs on that node in HostNetwork mode, the Pod reports these IP addresses in its
<code>.status.podIPs</code> field.
All <code>podIPs</code> in a node match the IP family preference defined by the <code>.status.addresses</code>
field for that Node.</li></ul></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>An example of an IPv4 CIDR: <code>10.244.0.0/16</code> (though you would supply your own address range)</p><p>An example of an IPv6 CIDR: <code>fdXY:IJKL:MNOP:15::/64</code> (this shows the format but is not a valid
address - see <a href=https://tools.ietf.org/html/rfc4193>RFC 4193</a>)</p></div><h2 id=services>Services</h2><p>You can create <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Services>Services</a> which can use IPv4, IPv6, or both.</p><p>The address family of a Service defaults to the address family of the first service cluster IP
range (configured via the <code>--service-cluster-ip-range</code> flag to the kube-apiserver).</p><p>When you define a Service you can optionally configure it as dual stack. To specify the behavior you want, you
set the <code>.spec.ipFamilyPolicy</code> field to one of the following values:</p><ul><li><code>SingleStack</code>: Single-stack service. The control plane allocates a cluster IP for the Service,
using the first configured service cluster IP range.</li><li><code>PreferDualStack</code>:<ul><li>Allocates IPv4 and IPv6 cluster IPs for the Service.</li></ul></li><li><code>RequireDualStack</code>: Allocates Service <code>.spec.ClusterIPs</code> from both IPv4 and IPv6 address ranges.<ul><li>Selects the <code>.spec.ClusterIP</code> from the list of <code>.spec.ClusterIPs</code> based on the address family
of the first element in the <code>.spec.ipFamilies</code> array.</li></ul></li></ul><p>If you would like to define which IP family to use for single stack or define the order of IP
families for dual-stack, you can choose the address families by setting an optional field,
<code>.spec.ipFamilies</code>, on the Service.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>.spec.ipFamilies</code> field is immutable because the <code>.spec.ClusterIP</code> cannot be reallocated on a
Service that already exists. If you want to change <code>.spec.ipFamilies</code>, delete and recreate the
Service.</div><p>You can set <code>.spec.ipFamilies</code> to any of the following array values:</p><ul><li><code>["IPv4"]</code></li><li><code>["IPv6"]</code></li><li><code>["IPv4","IPv6"]</code> (dual stack)</li><li><code>["IPv6","IPv4"]</code> (dual stack)</li></ul><p>The first family you list is used for the legacy <code>.spec.ClusterIP</code> field.</p><h3 id=dual-stack-service-configuration-scenarios>Dual-stack Service configuration scenarios</h3><p>These examples demonstrate the behavior of various dual-stack Service configuration scenarios.</p><h4 id=dual-stack-options-on-new-services>Dual-stack options on new Services</h4><ol><li><p>This Service specification does not explicitly define <code>.spec.ipFamilyPolicy</code>. When you create
this Service, Kubernetes assigns a cluster IP for the Service from the first configured
<code>service-cluster-ip-range</code> and sets the <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code>. (<a href=/docs/concepts/services-networking/service/#services-without-selectors>Services
without selectors</a> and
<a href=/docs/concepts/services-networking/service/#headless-services>headless Services</a> with selectors
will behave in this same way.)</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml download=service/networking/dual-stack-default-svc.yaml><code>service/networking/dual-stack-default-svc.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-dual-stack-default-svc-yaml")' title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"></img></div><div class=includecode id=service-networking-dual-stack-default-svc-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
   </span></span></span></code></pre></div></div></div></li><li><p>This Service specification explicitly defines <code>PreferDualStack</code> in <code>.spec.ipFamilyPolicy</code>. When
you create this Service on a dual-stack cluster, Kubernetes assigns both IPv4 and IPv6
addresses for the service. The control plane updates the <code>.spec</code> for the Service to record the IP
address assignments. The field <code>.spec.ClusterIPs</code> is the primary field, and contains both assigned
IP addresses; <code>.spec.ClusterIP</code> is a secondary field with its value calculated from
<code>.spec.ClusterIPs</code>.</p><ul><li>For the <code>.spec.ClusterIP</code> field, the control plane records the IP address that is from the
same address family as the first service cluster IP range.</li><li>On a single-stack cluster, the <code>.spec.ClusterIPs</code> and <code>.spec.ClusterIP</code> fields both only list
one address.</li><li>On a cluster with dual-stack enabled, specifying <code>RequireDualStack</code> in <code>.spec.ipFamilyPolicy</code>
behaves the same as <code>PreferDualStack</code>.</li></ul><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-preferred-svc.yaml download=service/networking/dual-stack-preferred-svc.yaml><code>service/networking/dual-stack-preferred-svc.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-dual-stack-preferred-svc-yaml")' title="Copy service/networking/dual-stack-preferred-svc.yaml to clipboard"></img></div><div class=includecode id=service-networking-dual-stack-preferred-svc-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ipFamilyPolicy</span>:<span style=color:#bbb> </span>PreferDualStack<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
   </span></span></span></code></pre></div></div></div></li><li><p>This Service specification explicitly defines <code>IPv6</code> and <code>IPv4</code> in <code>.spec.ipFamilies</code> as well
as defining <code>PreferDualStack</code> in <code>.spec.ipFamilyPolicy</code>. When Kubernetes assigns an IPv6 and
IPv4 address in <code>.spec.ClusterIPs</code>, <code>.spec.ClusterIP</code> is set to the IPv6 address because that is
the first element in the <code>.spec.ClusterIPs</code> array, overriding the default.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-preferred-ipfamilies-svc.yaml download=service/networking/dual-stack-preferred-ipfamilies-svc.yaml><code>service/networking/dual-stack-preferred-ipfamilies-svc.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-dual-stack-preferred-ipfamilies-svc-yaml")' title="Copy service/networking/dual-stack-preferred-ipfamilies-svc.yaml to clipboard"></img></div><div class=includecode id=service-networking-dual-stack-preferred-ipfamilies-svc-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ipFamilyPolicy</span>:<span style=color:#bbb> </span>PreferDualStack<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ipFamilies</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- IPv6<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- IPv4<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
   </span></span></span></code></pre></div></div></div></li></ol><h4 id=dual-stack-defaults-on-existing-services>Dual-stack defaults on existing Services</h4><p>These examples demonstrate the default behavior when dual-stack is newly enabled on a cluster
where Services already exist. (Upgrading an existing cluster to 1.21 or beyond will enable
dual-stack.)</p><ol><li><p>When dual-stack is enabled on a cluster, existing Services (whether <code>IPv4</code> or <code>IPv6</code>) are
configured by the control plane to set <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code> and set
<code>.spec.ipFamilies</code> to the address family of the existing Service. The existing Service cluster IP
will be stored in <code>.spec.ClusterIPs</code>.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml download=service/networking/dual-stack-default-svc.yaml><code>service/networking/dual-stack-default-svc.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-dual-stack-default-svc-yaml")' title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"></img></div><div class=includecode id=service-networking-dual-stack-default-svc-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
   </span></span></span></code></pre></div></div></div><p>You can validate this behavior by using kubectl to inspect an existing service.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get svc my-service -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span><span style=color:#666>10.0.197.123</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIPs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:#666>10.0.197.123</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ipFamilies</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- IPv4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ipFamilyPolicy</span>:<span style=color:#bbb> </span>SingleStack<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>ClusterIP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>loadBalancer</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></li><li><p>When dual-stack is enabled on a cluster, existing
<a href=/docs/concepts/services-networking/service/#headless-services>headless Services</a> with selectors are
configured by the control plane to set <code>.spec.ipFamilyPolicy</code> to <code>SingleStack</code> and set
<code>.spec.ipFamilies</code> to the address family of the first service cluster IP range (configured via the
<code>--service-cluster-ip-range</code> flag to the kube-apiserver) even though <code>.spec.ClusterIP</code> is set to
<code>None</code>.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/service/networking/dual-stack-default-svc.yaml download=service/networking/dual-stack-default-svc.yaml><code>service/networking/dual-stack-default-svc.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("service-networking-dual-stack-default-svc-yaml")' title="Copy service/networking/dual-stack-default-svc.yaml to clipboard"></img></div><div class=includecode id=service-networking-dual-stack-default-svc-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
   </span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
   </span></span></span></code></pre></div></div></div><p>You can validate this behavior by using kubectl to inspect an existing headless service with selectors.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get svc my-service -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span>None<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIPs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- None<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ipFamilies</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- IPv4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ipFamilyPolicy</span>:<span style=color:#bbb> </span>SingleStack<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span></code></pre></div></li></ol><h4 id=switching-services-between-single-stack-and-dual-stack>Switching Services between single-stack and dual-stack</h4><p>Services can be changed from single-stack to dual-stack and from dual-stack to single-stack.</p><ol><li><p>To change a Service from single-stack to dual-stack, change <code>.spec.ipFamilyPolicy</code> from
<code>SingleStack</code> to <code>PreferDualStack</code> or <code>RequireDualStack</code> as desired. When you change this
Service from single-stack to dual-stack, Kubernetes assigns the missing address family so that the
Service now has IPv4 and IPv6 addresses.</p><p>Edit the Service specification updating the <code>.spec.ipFamilyPolicy</code> from <code>SingleStack</code> to <code>PreferDualStack</code>.</p><p>Before:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ipFamilyPolicy</span>:<span style=color:#bbb> </span>SingleStack<span style=color:#bbb>
</span></span></span></code></pre></div><p>After:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ipFamilyPolicy</span>:<span style=color:#bbb> </span>PreferDualStack<span style=color:#bbb>
</span></span></span></code></pre></div></li><li><p>To change a Service from dual-stack to single-stack, change <code>.spec.ipFamilyPolicy</code> from
<code>PreferDualStack</code> or <code>RequireDualStack</code> to <code>SingleStack</code>. When you change this Service from
dual-stack to single-stack, Kubernetes retains only the first element in the <code>.spec.ClusterIPs</code>
array, and sets <code>.spec.ClusterIP</code> to that IP address and sets <code>.spec.ipFamilies</code> to the address
family of <code>.spec.ClusterIPs</code>.</p></li></ol><h3 id=headless-services-without-selector>Headless Services without selector</h3><p>For <a href=/docs/concepts/services-networking/service/#without-selectors>Headless Services without selectors</a>
and without <code>.spec.ipFamilyPolicy</code> explicitly set, the <code>.spec.ipFamilyPolicy</code> field defaults to
<code>RequireDualStack</code>.</p><h3 id=service-type-loadbalancer>Service type LoadBalancer</h3><p>To provision a dual-stack load balancer for your Service:</p><ul><li>Set the <code>.spec.type</code> field to <code>LoadBalancer</code></li><li>Set <code>.spec.ipFamilyPolicy</code> field to <code>PreferDualStack</code> or <code>RequireDualStack</code></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> To use a dual-stack <code>LoadBalancer</code> type Service, your cloud provider must support IPv4 and IPv6
load balancers.</div><h2 id=egress-traffic>Egress traffic</h2><p>If you want to enable egress traffic in order to reach off-cluster destinations (eg. the public
Internet) from a Pod that uses non-publicly routable IPv6 addresses, you need to enable the Pod to
use a publicly routed IPv6 address via a mechanism such as transparent proxying or IP
masquerading. The <a href=https://github.com/kubernetes-sigs/ip-masq-agent>ip-masq-agent</a> project
supports IP masquerading on dual-stack clusters.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Ensure your <a class=glossary-tooltip title='Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label=CNI>CNI</a> provider supports IPv6.</div><h2 id=windows-support>Windows support</h2><p>Kubernetes on Windows does not support single-stack "IPv6-only" networking. However,
dual-stack IPv4/IPv6 networking for pods and nodes with single-family services
is supported.</p><p>You can use IPv4/IPv6 dual-stack networking with <code>l2bridge</code> networks.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Overlay (VXLAN) networks on Windows <strong>do not</strong> support dual-stack networking.</div><p>You can read more about the different network modes for Windows within the
<a href=/docs/concepts/services-networking/windows-networking#network-modes>Networking on Windows</a> topic.</p><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/tasks/network/validate-dual-stack>Validate IPv4/IPv6 dual-stack</a> networking</li><li><a href=/docs/setup/production-environment/tools/kubeadm/dual-stack-support/>Enable dual-stack networking using kubeadm</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-374e5c954990aec58a0797adc70a5039>6.8 - Topology Aware Hints</h1><div class=lead><em>Topology Aware Hints</em> provides a mechanism to help keep network traffic within the zone where it originated. Preferring same-zone traffic between Pods in your cluster can help with reliability, performance (network latency and throughput), or cost.</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div><p><em>Topology Aware Hints</em> enable topology aware routing by including suggestions
for how clients should consume endpoints. This approach adds metadata to enable
consumers of EndpointSlice (or Endpoints) objects, so that traffic to
those network endpoints can be routed closer to where it originated.</p><p>For example, you can route traffic within a locality to reduce
costs, or to improve network performance.</p><h2 id=motivation>Motivation</h2><p>Kubernetes clusters are increasingly deployed in multi-zone environments.
<em>Topology Aware Hints</em> provides a mechanism to help keep traffic within the zone
it originated from. This concept is commonly referred to as "Topology Aware
Routing". When calculating the endpoints for a <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>,
the EndpointSlice controller considers the topology (region and zone) of each endpoint
and populates the hints field to allocate it to a zone.
Cluster components such as the <a class=glossary-tooltip title='kube-proxy is a network proxy that runs on each node in the cluster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a>
can then consume those hints, and use them to influence how the traffic is routed
(favoring topologically closer endpoints).</p><h2 id=using-topology-aware-hints>Using Topology Aware Hints</h2><p>You can activate Topology Aware Hints for a Service by setting the
<code>service.kubernetes.io/topology-aware-hints</code> annotation to <code>auto</code>. This tells
the EndpointSlice controller to set topology hints if it is deemed safe.
Importantly, this does not guarantee that hints will always be set.</p><h2 id=implementation>How it works</h2><p>The functionality enabling this feature is split into two components: The
EndpointSlice controller and the kube-proxy. This section provides a high level overview
of how each component implements this feature.</p><h3 id=implementation-control-plane>EndpointSlice controller</h3><p>The EndpointSlice controller is responsible for setting hints on EndpointSlices
when this feature is enabled. The controller allocates a proportional amount of
endpoints to each zone. This proportion is based on the
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>allocatable</a>
CPU cores for nodes running in that zone. For example, if one zone had 2 CPU
cores and another zone only had 1 CPU core, the controller would allocate twice
as many endpoints to the zone with 2 CPU cores.</p><p>The following example shows what an EndpointSlice looks like when hints have
been populated:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>discovery.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>EndpointSlice<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-hints<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/service-name</span>:<span style=color:#bbb> </span>example-svc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>addressType</span>:<span style=color:#bbb> </span>IPv4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>http<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>endpoints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>addresses</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;10.1.2.3&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>conditions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ready</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostname</span>:<span style=color:#bbb> </span>pod-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>zone</span>:<span style=color:#bbb> </span>zone-a<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>forZones</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;zone-a&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=implementation-kube-proxy>kube-proxy</h3><p>The kube-proxy component filters the endpoints it routes to based on the hints set by
the EndpointSlice controller. In most cases, this means that the kube-proxy is able
to route traffic to endpoints in the same zone. Sometimes the controller allocates endpoints
from a different zone to ensure more even distribution of endpoints between zones.
This would result in some traffic being routed to other zones.</p><h2 id=safeguards>Safeguards</h2><p>The Kubernetes control plane and the kube-proxy on each node apply some
safeguard rules before using Topology Aware Hints. If these don't check out,
the kube-proxy selects endpoints from anywhere in your cluster, regardless of the
zone.</p><ol><li><p><strong>Insufficient number of endpoints:</strong> If there are less endpoints than zones
in a cluster, the controller will not assign any hints.</p></li><li><p><strong>Impossible to achieve balanced allocation:</strong> In some cases, it will be
impossible to achieve a balanced allocation of endpoints among zones. For
example, if zone-a is twice as large as zone-b, but there are only 2
endpoints, an endpoint allocated to zone-a may receive twice as much traffic
as zone-b. The controller does not assign hints if it can't get this "expected
overload" value below an acceptable threshold for each zone. Importantly this
is not based on real-time feedback. It is still possible for individual
endpoints to become overloaded.</p></li><li><p><strong>One or more Nodes has insufficient information:</strong> If any node does not have
a <code>topology.kubernetes.io/zone</code> label or is not reporting a value for
allocatable CPU, the control plane does not set any topology-aware endpoint
hints and so kube-proxy does not filter endpoints by zone.</p></li><li><p><strong>One or more endpoints does not have a zone hint:</strong> When this happens,
the kube-proxy assumes that a transition from or to Topology Aware Hints is
underway. Filtering endpoints for a Service in this state would be dangerous
so the kube-proxy falls back to using all endpoints.</p></li><li><p><strong>A zone is not represented in hints:</strong> If the kube-proxy is unable to find
at least one endpoint with a hint targeting the zone it is running in, it falls
to using endpoints from all zones. This is most likely to happen as you add
a new zone into your existing cluster.</p></li></ol><h2 id=constraints>Constraints</h2><ul><li><p>Topology Aware Hints are not used when either <code>externalTrafficPolicy</code> or
<code>internalTrafficPolicy</code> is set to <code>Local</code> on a Service. It is possible to use
both features in the same cluster on different Services, just not on the same
Service.</p></li><li><p>This approach will not work well for Services that have a large proportion of
traffic originating from a subset of zones. Instead this assumes that incoming
traffic will be roughly proportional to the capacity of the Nodes in each
zone.</p></li><li><p>The EndpointSlice controller ignores unready nodes as it calculates the
proportions of each zone. This could have unintended consequences if a large
portion of nodes are unready.</p></li><li><p>The EndpointSlice controller does not take into account <a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Tolerations enable the scheduling of pods on nodes or node groups that have a matching taint.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=tolerations>tolerations</a> when deploying or calculating the
proportions of each zone. If the Pods backing a Service are limited to a
subset of Nodes in the cluster, this will not be taken into account.</p></li><li><p>This may not work well with autoscaling. For example, if a lot of traffic is
originating from a single zone, only the endpoints allocated to that zone will
be handling that traffic. That could result in <a class=glossary-tooltip title='An API resource that automatically scales the number of pod replicas based on targeted CPU utilization or custom metric targets.' data-toggle=tooltip data-placement=top href=/docs/tasks/run-application/horizontal-pod-autoscale/ target=_blank aria-label='Horizontal Pod Autoscaler'>Horizontal Pod Autoscaler</a>
either not picking up on this event, or newly added pods starting in a
different zone.</p></li></ul><h2 id=what-s-next>What's next</h2><ul><li>Follow the <a href=/docs/tutorials/services/connect-applications-service/>Connecting Applications with Services</a> tutorial</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-9092684b3a27432bc9041d56b7a4a8ba>6.9 - Networking on Windows</h1><p>Kubernetes supports running nodes on either Linux or Windows. You can mix both kinds of node
within a single cluster.
This page provides an overview to networking specific to the Windows operating system.</p><h2 id=networking>Container networking on Windows</h2><p>Networking for Windows containers is exposed through
<a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>CNI plugins</a>.
Windows containers function similarly to virtual machines in regards to
networking. Each container has a virtual network adapter (vNIC) which is connected
to a Hyper-V virtual switch (vSwitch). The Host Networking Service (HNS) and the
Host Compute Service (HCS) work together to create containers and attach container
vNICs to networks. HCS is responsible for the management of containers whereas HNS
is responsible for the management of networking resources such as:</p><ul><li>Virtual networks (including creation of vSwitches)</li><li>Endpoints / vNICs</li><li>Namespaces</li><li>Policies including packet encapsulations, load-balancing rules, ACLs, and NAT rules.</li></ul><p>The Windows HNS and vSwitch implement namespacing and can
create virtual NICs as needed for a pod or container. However, many configurations such
as DNS, routes, and metrics are stored in the Windows registry database rather than as
files inside <code>/etc</code>, which is how Linux stores those configurations. The Windows registry for the container
is separate from that of the host, so concepts like mapping <code>/etc/resolv.conf</code> from
the host into a container don't have the same effect they would on Linux. These must
be configured using Windows APIs run in the context of that container. Therefore
CNI implementations need to call the HNS instead of relying on file mappings to pass
network details into the pod or container.</p><h2 id=network-modes>Network modes</h2><p>Windows supports five different networking drivers/modes: L2bridge, L2tunnel,
Overlay (Beta), Transparent, and NAT. In a heterogeneous cluster with Windows and Linux
worker nodes, you need to select a networking solution that is compatible on both
Windows and Linux. The following table lists the out-of-tree plugins are supported on Windows,
with recommendations on when to use each CNI:</p><table><thead><tr><th>Network Driver</th><th>Description</th><th>Container Packet Modifications</th><th>Network Plugins</th><th>Network Plugin Characteristics</th></tr></thead><tbody><tr><td>L2bridge</td><td>Containers are attached to an external vSwitch. Containers are attached to the underlay network, although the physical network doesn't need to learn the container MACs because they are rewritten on ingress/egress.</td><td>MAC is rewritten to host MAC, IP may be rewritten to host IP using HNS OutboundNAT policy.</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-bridge>win-bridge</a>, <a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a>, Flannel host-gateway uses win-bridge</td><td>win-bridge uses L2bridge network mode, connects containers to the underlay of hosts, offering best performance. Requires user-defined routes (UDR) for inter-node connectivity.</td></tr><tr><td>L2Tunnel</td><td>This is a special case of l2bridge, but only used on Azure. All packets are sent to the virtualization host where SDN policy is applied.</td><td>MAC rewritten, IP visible on the underlay network</td><td><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md>Azure-CNI</a></td><td>Azure-CNI allows integration of containers with Azure vNET, and allows them to leverage the set of capabilities that <a href=https://azure.microsoft.com/en-us/services/virtual-network/>Azure Virtual Network provides</a>. For example, securely connect to Azure services or use Azure NSGs. See <a href=https://docs.microsoft.com/azure/aks/concepts-network#azure-cni-advanced-networking>azure-cni for some examples</a></td></tr><tr><td>Overlay</td><td>Containers are given a vNIC connected to an external vSwitch. Each overlay network gets its own IP subnet, defined by a custom IP prefix.The overlay network driver uses VXLAN encapsulation.</td><td>Encapsulated with an outer header.</td><td><a href=https://github.com/containernetworking/plugins/tree/master/plugins/main/windows/win-overlay>win-overlay</a>, Flannel VXLAN (uses win-overlay)</td><td>win-overlay should be used when virtual container networks are desired to be isolated from underlay of hosts (e.g. for security reasons). Allows for IPs to be re-used for different overlay networks (which have different VNID tags) if you are restricted on IPs in your datacenter. This option requires <a href=https://support.microsoft.com/help/4489899>KB4489899</a> on Windows Server 2019.</td></tr><tr><td>Transparent (special use case for <a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a>)</td><td>Requires an external vSwitch. Containers are attached to an external vSwitch which enables intra-pod communication via logical networks (logical switches and routers).</td><td>Packet is encapsulated either via <a href=https://datatracker.ietf.org/doc/draft-gross-geneve/>GENEVE</a> or <a href=https://datatracker.ietf.org/doc/draft-davie-stt/>STT</a> tunneling to reach pods which are not on the same host.<br>Packets are forwarded or dropped via the tunnel metadata information supplied by the ovn network controller.<br>NAT is done for north-south communication.</td><td><a href=https://github.com/openvswitch/ovn-kubernetes>ovn-kubernetes</a></td><td><a href=https://github.com/openvswitch/ovn-kubernetes/tree/master/contrib>Deploy via ansible</a>. Distributed ACLs can be applied via Kubernetes policies. IPAM support. Load-balancing can be achieved without kube-proxy. NATing is done without using iptables/netsh.</td></tr><tr><td>NAT (<em>not used in Kubernetes</em>)</td><td>Containers are given a vNIC connected to an internal vSwitch. DNS/DHCP is provided using an internal component called <a href=https://techcommunity.microsoft.com/t5/virtualization/windows-nat-winnat-capabilities-and-limitations/ba-p/382303>WinNAT</a></td><td>MAC and IP is rewritten to host MAC/IP.</td><td><a href=https://github.com/Microsoft/windows-container-networking/tree/master/plugins/nat>nat</a></td><td>Included here for completeness</td></tr></tbody></table><p>As outlined above, the <a href=https://github.com/coreos/flannel>Flannel</a>
<a href=https://github.com/flannel-io/cni-plugin>CNI plugin</a>
is also <a href=https://github.com/flannel-io/cni-plugin#windows-support-experimental>supported</a> on Windows via the
<a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>VXLAN network backend</a> (<strong>Beta support</strong> ; delegates to win-overlay)
and <a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#host-gw>host-gateway network backend</a> (stable support; delegates to win-bridge).</p><p>This plugin supports delegating to one of the reference CNI plugins (win-overlay,
win-bridge), to work in conjunction with Flannel daemon on Windows (Flanneld) for
automatic node subnet lease assignment and HNS network creation. This plugin reads
in its own configuration file (cni.conf), and aggregates it with the environment
variables from the FlannelD generated subnet.env file. It then delegates to one of
the reference CNI plugins for network plumbing, and sends the correct configuration
containing the node-assigned subnet to the IPAM plugin (for example: <code>host-local</code>).</p><p>For Node, Pod, and Service objects, the following network flows are supported for
TCP/UDP traffic:</p><ul><li>Pod → Pod (IP)</li><li>Pod → Pod (Name)</li><li>Pod → Service (Cluster IP)</li><li>Pod → Service (PQDN, but only if there are no ".")</li><li>Pod → Service (FQDN)</li><li>Pod → external (IP)</li><li>Pod → external (DNS)</li><li>Node → Pod</li><li>Pod → Node</li></ul><h2 id=ipam>IP address management (IPAM)</h2><p>The following IPAM options are supported on Windows:</p><ul><li><a href=https://github.com/containernetworking/plugins/tree/master/plugins/ipam/host-local>host-local</a></li><li><a href=https://github.com/Azure/azure-container-networking/blob/master/docs/ipam.md>azure-vnet-ipam</a> (for azure-cni only)</li><li><a href=https://docs.microsoft.com/windows-server/networking/technologies/ipam/ipam-top>Windows Server IPAM</a> (fallback option if no IPAM is set)</li></ul><h2 id=load-balancing-and-services>Load balancing and Services</h2><p>A Kubernetes <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a> is an abstraction
that defines a logical set of Pods and a means to access them over a network.
In a cluster that includes Windows nodes, you can use the following types of Service:</p><ul><li><code>NodePort</code></li><li><code>ClusterIP</code></li><li><code>LoadBalancer</code></li><li><code>ExternalName</code></li></ul><p>Windows container networking differs in some important ways from Linux networking.
The <a href=https://docs.microsoft.com/en-us/virtualization/windowscontainers/container-networking/architecture>Microsoft documentation for Windows Container Networking</a>
provides additional details and background.</p><p>On Windows, you can use the following settings to configure Services and load
balancing behavior:</p><table><caption style=display:none>Windows Service Settings</caption><thead><tr><th>Feature</th><th>Description</th><th>Minimum Supported Windows OS build</th><th>How to enable</th></tr></thead><tbody><tr><td>Session affinity</td><td>Ensures that connections from a particular client are passed to the same Pod each time.</td><td>Windows Server 2022</td><td>Set <code>service.spec.sessionAffinity</code> to "ClientIP"</td></tr><tr><td>Direct Server Return (DSR)</td><td>Load balancing mode where the IP address fixups and the LBNAT occurs at the container vSwitch port directly; service traffic arrives with the source IP set as the originating pod IP.</td><td>Windows Server 2019</td><td>Set the following flags in kube-proxy: <code>--feature-gates="WinDSR=true" --enable-dsr=true</code></td></tr><tr><td>Preserve-Destination</td><td>Skips DNAT of service traffic, thereby preserving the virtual IP of the target service in packets reaching the backend Pod. Also disables node-node forwarding.</td><td>Windows Server, version 1903</td><td>Set <code>"preserve-destination": "true"</code> in service annotations and enable DSR in kube-proxy.</td></tr><tr><td>IPv4/IPv6 dual-stack networking</td><td>Native IPv4-to-IPv4 in parallel with IPv6-to-IPv6 communications to, from, and within a cluster</td><td>Windows Server 2019</td><td>See <a href=#ipv4ipv6-dual-stack>IPv4/IPv6 dual-stack</a></td></tr><tr><td>Client IP preservation</td><td>Ensures that source IP of incoming ingress traffic gets preserved. Also disables node-node forwarding.</td><td>Windows Server 2019</td><td>Set <code>service.spec.externalTrafficPolicy</code> to "Local" and enable DSR in kube-proxy</td></tr></tbody></table><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>There are known issue with NodePort Services on overlay networking, if the destination node is running Windows Server 2022.
To avoid the issue entirely, you can configure the service with <code>externalTrafficPolicy: Local</code>.</p><p>There are known issues with Pod to Pod connectivity on l2bridge network on Windows Server 2022 with KB5005619 or higher installed.
To workaround the issue and restore Pod to Pod connectivity, you can disable the WinDSR feature in kube-proxy.</p><p>These issues require OS fixes.
Please follow <a href=https://github.com/microsoft/Windows-Containers/issues/204>https://github.com/microsoft/Windows-Containers/issues/204</a> for updates.</p></div><h2 id=limitations>Limitations</h2><p>The following networking functionality is <em>not</em> supported on Windows nodes:</p><ul><li>Host networking mode</li><li>Local NodePort access from the node itself (works for other nodes or external clients)</li><li>More than 64 backend pods (or unique destination addresses) for a single Service</li><li>IPv6 communication between Windows pods connected to overlay networks</li><li>Local Traffic Policy in non-DSR mode</li><li>Outbound communication using the ICMP protocol via the <code>win-overlay</code>, <code>win-bridge</code>, or using the Azure-CNI plugin.<br>Specifically, the Windows data plane (<a href=https://www.microsoft.com/research/project/azure-virtual-filtering-platform/>VFP</a>)
doesn't support ICMP packet transpositions, and this means:<ul><li>ICMP packets directed to destinations within the same network (such as pod to pod communication via ping)
work as expected;</li><li>TCP/UDP packets work as expected;</li><li>ICMP packets directed to pass through a remote network (e.g. pod to external internet communication via ping)
cannot be transposed and thus will not be routed back to their source;</li><li>Since TCP/UDP packets can still be transposed, you can substitute <code>ping &lt;destination></code> with
<code>curl &lt;destination></code> when debugging connectivity with the outside world.</li></ul></li></ul><p>Other limitations:</p><ul><li>Windows reference network plugins win-bridge and win-overlay do not implement
<a href=https://github.com/containernetworking/cni/blob/master/SPEC.md>CNI spec</a> v0.4.0,
due to a missing <code>CHECK</code> implementation.</li><li>The Flannel VXLAN CNI plugin has the following limitations on Windows:<ul><li>Node-pod connectivity is only possible for local pods with Flannel v0.12.0 (or higher).</li><li>Flannel is restricted to using VNI 4096 and UDP port 4789. See the official
<a href=https://github.com/coreos/flannel/blob/master/Documentation/backends.md#vxlan>Flannel VXLAN</a>
backend docs for more details on these parameters.</li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cd7657b1056ad32451974db57a951ba5>6.10 - Service Internal Traffic Policy</h1><div class=lead>If two Pods in your cluster want to communicate, and both Pods are actually running on the same node, use <em>Service Internal Traffic Policy</em> to keep network traffic within that node. Avoiding a round trip via the cluster network can help with reliability, performance (network latency and throughput), or cost.</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div><p><em>Service Internal Traffic Policy</em> enables internal traffic restrictions to only route
internal traffic to endpoints within the node the traffic originated from. The
"internal" traffic here refers to traffic originated from Pods in the current
cluster. This can help to reduce costs and improve performance.</p><h2 id=using-service-internal-traffic-policy>Using Service Internal Traffic Policy</h2><p>The <code>ServiceInternalTrafficPolicy</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
is a Beta feature and enabled by default.
When the feature is enabled, you can enable the internal-only traffic policy for a
<a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>, by setting its
<code>.spec.internalTrafficPolicy</code> to <code>Local</code>.
This tells kube-proxy to only use node local endpoints for cluster internal traffic.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> For pods on nodes with no endpoints for a given Service, the Service
behaves as if it has zero endpoints (for Pods on this node) even if the service
does have endpoints on other nodes.</div><p>The following example shows what a Service looks like when you set
<code>.spec.internalTrafficPolicy</code> to <code>Local</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app.kubernetes.io/name</span>:<span style=color:#bbb> </span>MyApp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>internalTrafficPolicy</span>:<span style=color:#bbb> </span>Local<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=how-it-works>How it works</h2><p>The kube-proxy filters the endpoints it routes to based on the
<code>spec.internalTrafficPolicy</code> setting. When it's set to <code>Local</code>, only node local
endpoints are considered. When it's <code>Cluster</code> or missing, all endpoints are
considered.
When the <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
<code>ServiceInternalTrafficPolicy</code> is enabled, <code>spec.internalTrafficPolicy</code> defaults to "Cluster".</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/services-networking/topology-aware-hints>Topology Aware Hints</a></li><li>Read about <a href=/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip>Service External Traffic Policy</a></li><li>Follow the <a href=/docs/tutorials/services/connect-applications-service/>Connecting Applications with Services</a> tutorial</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3a38878244d862dfdb8d7adb32f77584>6.11 - Topology-aware traffic routing with topology keys</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This feature, specifically the alpha <code>topologyKeys</code> API, is deprecated since
Kubernetes v1.21.
<a href=/docs/concepts/services-networking/topology-aware-hints/>Topology Aware Hints</a>,
introduced in Kubernetes v1.21, provide similar functionality.</div><p><em>Service Topology</em> enables a service to route traffic based upon the Node
topology of the cluster. For example, a service can specify that traffic be
preferentially routed to endpoints that are on the same Node as the client, or
in the same availability zone.</p><h2 id=topology-aware-traffic-routing>Topology-aware traffic routing</h2><p>By default, traffic sent to a <code>ClusterIP</code> or <code>NodePort</code> Service may be routed to
any backend address for the Service. Kubernetes 1.7 made it possible to
route "external" traffic to the Pods running on the same Node that received the
traffic. For <code>ClusterIP</code> Services, the equivalent same-node preference for
routing wasn't possible; nor could you configure your cluster to favor routing
to endpoints within the same zone.
By setting <code>topologyKeys</code> on a Service, you're able to define a policy for routing
traffic based upon the Node labels for the originating and destination Nodes.</p><p>The label matching between the source and destination lets you, as a cluster
operator, designate sets of Nodes that are "closer" and "farther" from one another.
You can define labels to represent whatever metric makes sense for your own
requirements.
In public clouds, for example, you might prefer to keep network traffic within the
same zone, because interzonal traffic has a cost associated with it (and intrazonal
traffic typically does not). Other common needs include being able to route traffic
to a local Pod managed by a DaemonSet, or directing traffic to Nodes connected to the
same top-of-rack switch for the lowest latency.</p><h2 id=using-service-topology>Using Service Topology</h2><p>If your cluster has the <code>ServiceTopology</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> enabled, you can control Service traffic
routing by specifying the <code>topologyKeys</code> field on the Service spec. This field
is a preference-order list of Node labels which will be used to sort endpoints
when accessing this Service. Traffic will be directed to a Node whose value for
the first label matches the originating Node's value for that label. If there is
no backend for the Service on a matching Node, then the second label will be
considered, and so forth, until no labels remain.</p><p>If no match is found, the traffic will be rejected, as if there were no
backends for the Service at all. That is, endpoints are chosen based on the first
topology key with available backends. If this field is specified and all entries
have no backends that match the topology of the client, the service has no
backends for that client and connections should fail. The special value <code>"*"</code> may
be used to mean "any topology". This catch-all value, if used, only makes sense
as the last value in the list.</p><p>If <code>topologyKeys</code> is not specified or empty, no topology constraints will be applied.</p><p>Consider a cluster with Nodes that are labeled with their hostname, zone name,
and region name. Then you can set the <code>topologyKeys</code> values of a service to direct
traffic as follows.</p><ul><li>Only to endpoints on the same node, failing if no endpoint exists on the node:
<code>["kubernetes.io/hostname"]</code>.</li><li>Preferentially to endpoints on the same node, falling back to endpoints in the
same zone, followed by the same region, and failing otherwise: <code>["kubernetes.io/hostname", "topology.kubernetes.io/zone", "topology.kubernetes.io/region"]</code>.
This may be useful, for example, in cases where data locality is critical.</li><li>Preferentially to the same zone, but fallback on any available endpoint if
none are available within this zone:
<code>["topology.kubernetes.io/zone", "*"]</code>.</li></ul><h2 id=constraints>Constraints</h2><ul><li><p>Service topology is not compatible with <code>externalTrafficPolicy=Local</code>, and
therefore a Service cannot use both of these features. It is possible to use
both features in the same cluster on different Services, only not on the same
Service.</p></li><li><p>Valid topology keys are currently limited to <code>kubernetes.io/hostname</code>,
<code>topology.kubernetes.io/zone</code>, and <code>topology.kubernetes.io/region</code>, but will
be generalized to other node labels in the future.</p></li><li><p>Topology keys must be valid label keys and at most 16 keys may be specified.</p></li><li><p>The catch-all value, <code>"*"</code>, must be the last value in the topology keys, if
it is used.</p></li></ul><h2 id=examples>Examples</h2><p>The following are common examples of using the Service Topology feature.</p><h3 id=only-node-local-endpoints>Only Node Local Endpoints</h3><p>A Service that only routes to node local endpoints. If no endpoints exist on the node, traffic is dropped:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>my-app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologyKeys</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=prefer-node-local-endpoints>Prefer Node Local Endpoints</h3><p>A Service that prefers node local Endpoints but falls back to cluster wide endpoints if node local endpoints do not exist:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>my-app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologyKeys</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=only-zonal-or-regional-endpoints>Only Zonal or Regional Endpoints</h3><p>A Service that prefers zonal then regional endpoints. If no endpoints exist in either, traffic is dropped.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>my-app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologyKeys</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;topology.kubernetes.io/zone&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;topology.kubernetes.io/region&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=prefer-node-local-zonal-then-regional-endpoints>Prefer Node Local, Zonal, then Regional Endpoints</h3><p>A Service that prefers node local, zonal, then regional endpoints but falls back to cluster wide endpoints.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>my-app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>protocol</span>:<span style=color:#bbb> </span>TCP<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>targetPort</span>:<span style=color:#bbb> </span><span style=color:#666>9376</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologyKeys</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;topology.kubernetes.io/zone&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;topology.kubernetes.io/region&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/services-networking/topology-aware-hints/>Topology Aware Hints</a></li><li>Read <a href=/docs/tutorials/services/connect-applications-service/>Connecting Applications with Services</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f018f568c6723865753f150c3c59bdda>7 - Storage</h1><div class=lead>Ways to provide both long-term and temporary storage to Pods in your cluster.</div></div><div class=td-content><h1 id=pg-27795584640a03bd2024f1fe3b3ab754>7.1 - Volumes</h1><p>On-disk files in a container are ephemeral, which presents some problems for
non-trivial applications when running in containers. One problem
is the loss of files when a container crashes. The kubelet restarts the container
but with a clean state. A second problem occurs when sharing files
between containers running together in a <code>Pod</code>.
The Kubernetes <a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a> abstraction
solves both of these problems.
Familiarity with <a href=/docs/concepts/workloads/pods/>Pods</a> is suggested.</p><h2 id=background>Background</h2><p>Docker has a concept of
<a href=https://docs.docker.com/storage/>volumes</a>, though it is
somewhat looser and less managed. A Docker volume is a directory on
disk or in another container. Docker provides volume
drivers, but the functionality is somewhat limited.</p><p>Kubernetes supports many types of volumes. A <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>
can use any number of volume types simultaneously.
Ephemeral volume types have a lifetime of a pod, but persistent volumes exist beyond
the lifetime of a pod. When a pod ceases to exist, Kubernetes destroys ephemeral volumes;
however, Kubernetes does not destroy persistent volumes.
For any kind of volume in a given pod, data is preserved across container restarts.</p><p>At its core, a volume is a directory, possibly with some data in it, which
is accessible to the containers in a pod. How that directory comes to be, the
medium that backs it, and the contents of it are determined by the particular
volume type used.</p><p>To use a volume, specify the volumes to provide for the Pod in <code>.spec.volumes</code>
and declare where to mount those volumes into containers in <code>.spec.containers[*].volumeMounts</code>.
A process in a container sees a filesystem view composed from the initial contents of
the <a class=glossary-tooltip title='Stored instance of a container that holds a set of software needed to run an application.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-image' target=_blank aria-label='container image'>container image</a>, plus volumes
(if defined) mounted inside the container.
The process sees a root filesystem that initially matches the contents of the container
image.
Any writes to within that filesystem hierarchy, if allowed, affect what that process views
when it performs a subsequent filesystem access.
Volumes mount at the <a href=#using-subpath>specified paths</a> within
the image.
For each container defined within a Pod, you must independently specify where
to mount each volume that the container uses.</p><p>Volumes cannot mount within other volumes (but see <a href=#using-subpath>Using subPath</a>
for a related mechanism). Also, a volume cannot contain a hard link to anything in
a different volume.</p><h2 id=volume-types>Types of volumes</h2><p>Kubernetes supports several types of volumes.</p><h3 id=awselasticblockstore>awsElasticBlockStore (deprecated)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [deprecated]</code></div><p>An <code>awsElasticBlockStore</code> volume mounts an Amazon Web Services (AWS)
<a href=https://aws.amazon.com/ebs/>EBS volume</a> into your pod. Unlike
<code>emptyDir</code>, which is erased when a pod is removed, the contents of an EBS
volume are persisted and the volume is unmounted. This means that an
EBS volume can be pre-populated with data, and that data can be shared between pods.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must create an EBS volume by using <code>aws ec2 create-volume</code> or the AWS API before you can use it.</div><p>There are some restrictions when using an <code>awsElasticBlockStore</code> volume:</p><ul><li>the nodes on which pods are running must be AWS EC2 instances</li><li>those instances need to be in the same region and availability zone as the EBS volume</li><li>EBS only supports a single EC2 instance mounting a volume</li></ul><h4 id=creating-an-aws-ebs-volume>Creating an AWS EBS volume</h4><p>Before you can use an EBS volume with a pod, you need to create it.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>aws ec2 create-volume --availability-zone<span style=color:#666>=</span>eu-west-1a --size<span style=color:#666>=</span><span style=color:#666>10</span> --volume-type<span style=color:#666>=</span>gp2
</span></span></code></pre></div><p>Make sure the zone matches the zone you brought up your cluster in. Check that the size and EBS volume
type are suitable for your use.</p><h4 id=aws-ebs-configuration-example>AWS EBS configuration example</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-ebs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-ebs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># This AWS EBS volume must already exist.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>awsElasticBlockStore</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeID</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&lt;volume id&gt;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span></code></pre></div><p>If the EBS volume is partitioned, you can supply the optional field <code>partition: "&lt;partition number>"</code> to specify which partition to mount on.</p><h4 id=aws-ebs-csi-migration>AWS EBS CSI migration</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>The <code>CSIMigration</code> feature for <code>awsElasticBlockStore</code>, when enabled, redirects
all plugin operations from the existing in-tree plugin to the <code>ebs.csi.aws.com</code> Container
Storage Interface (CSI) driver. In order to use this feature, the <a href=https://github.com/kubernetes-sigs/aws-ebs-csi-driver>AWS EBS CSI
driver</a>
must be installed on the cluster.</p><h4 id=aws-ebs-csi-migration-complete>AWS EBS CSI migration complete</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [alpha]</code></div><p>To disable the <code>awsElasticBlockStore</code> storage plugin from being loaded by the controller manager
and the kubelet, set the <code>InTreePluginAWSUnregister</code> flag to <code>true</code>.</p><h3 id=azuredisk>azureDisk (deprecated)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [deprecated]</code></div><p>The <code>azureDisk</code> volume type mounts a Microsoft Azure <a href=https://docs.microsoft.com/en-us/azure/aks/csi-storage-drivers>Data Disk</a> into a pod.</p><p>For more details, see the <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_disk/README.md><code>azureDisk</code> volume plugin</a>.</p><h4 id=azuredisk-csi-migration>azureDisk CSI migration</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>The <code>CSIMigration</code> feature for <code>azureDisk</code>, when enabled, redirects all plugin operations
from the existing in-tree plugin to the <code>disk.csi.azure.com</code> Container
Storage Interface (CSI) Driver. In order to use this feature, the
<a href=https://github.com/kubernetes-sigs/azuredisk-csi-driver>Azure Disk CSI Driver</a>
must be installed on the cluster.</p><h4 id=azuredisk-csi-migration-complete>azureDisk CSI migration complete</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code></div><p>To disable the <code>azureDisk</code> storage plugin from being loaded by the controller manager
and the kubelet, set the <code>InTreePluginAzureDiskUnregister</code> flag to <code>true</code>.</p><h3 id=azurefile>azureFile (deprecated)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [deprecated]</code></div><p>The <code>azureFile</code> volume type mounts a Microsoft Azure File volume (SMB 2.1 and 3.0)
into a pod.</p><p>For more details, see the <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/azure_file/README.md><code>azureFile</code> volume plugin</a>.</p><h4 id=azurefile-csi-migration>azureFile CSI migration</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code></div><p>The <code>CSIMigration</code> feature for <code>azureFile</code>, when enabled, redirects all plugin operations
from the existing in-tree plugin to the <code>file.csi.azure.com</code> Container
Storage Interface (CSI) Driver. In order to use this feature, the <a href=https://github.com/kubernetes-sigs/azurefile-csi-driver>Azure File CSI
Driver</a>
must be installed on the cluster and the <code>CSIMigrationAzureFile</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gates</a> must be enabled.</p><p>Azure File CSI driver does not support using same volume with different fsgroups. If
<code>CSIMigrationAzureFile</code> is enabled, using same volume with different fsgroups won't be supported at all.</p><h4 id=azurefile-csi-migration-complete>azureFile CSI migration complete</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code></div><p>To disable the <code>azureFile</code> storage plugin from being loaded by the controller manager
and the kubelet, set the <code>InTreePluginAzureFileUnregister</code> flag to <code>true</code>.</p><h3 id=cephfs>cephfs</h3><p>A <code>cephfs</code> volume allows an existing CephFS volume to be
mounted into your Pod. Unlike <code>emptyDir</code>, which is erased when a pod is
removed, the contents of a <code>cephfs</code> volume are preserved and the volume is merely
unmounted. This means that a <code>cephfs</code> volume can be pre-populated with data, and
that data can be shared between pods. The <code>cephfs</code> volume can be mounted by multiple
writers simultaneously.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must have your own Ceph server running with the share exported before you can use it.</div><p>See the <a href=https://github.com/kubernetes/examples/tree/master/volumes/cephfs/>CephFS example</a> for more details.</p><h3 id=cinder>cinder (deprecated)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [deprecated]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes must be configured with the OpenStack cloud provider.</div><p>The <code>cinder</code> volume type is used to mount the OpenStack Cinder volume into your pod.</p><h4 id=cinder-volume-configuration-example>Cinder volume configuration example</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-cinder<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-cinder-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-cinder<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># This OpenStack volume must already exist.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cinder</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeID</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&lt;volume id&gt;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=openstack-csi-migration>OpenStack CSI migration</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>The <code>CSIMigration</code> feature for Cinder is enabled by default since Kubernetes 1.21.
It redirects all plugin operations from the existing in-tree plugin to the
<code>cinder.csi.openstack.org</code> Container Storage Interface (CSI) Driver.
<a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/using-cinder-csi-plugin.md>OpenStack Cinder CSI Driver</a>
must be installed on the cluster.</p><p>To disable the in-tree Cinder plugin from being loaded by the controller manager
and the kubelet, you can enable the <code>InTreePluginOpenStackUnregister</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>.</p><h3 id=configmap>configMap</h3><p>A <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMap</a>
provides a way to inject configuration data into pods.
The data stored in a ConfigMap can be referenced in a volume of type
<code>configMap</code> and then consumed by containerized applications running in a pod.</p><p>When referencing a ConfigMap, you provide the name of the ConfigMap in the
volume. You can customize the path to use for a specific
entry in the ConfigMap. The following configuration shows how to mount
the <code>log-config</code> ConfigMap onto a Pod called <code>configmap-pod</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>configmap-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>log-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>log_level<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>log_level<span style=color:#bbb>
</span></span></span></code></pre></div><p>The <code>log-config</code> ConfigMap is mounted as a volume, and all contents stored in
its <code>log_level</code> entry are mounted into the Pod at path <code>/etc/config/log_level</code>.
Note that this path is derived from the volume's <code>mountPath</code> and the <code>path</code>
keyed with <code>log_level</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><ul><li><p>You must create a <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMap</a>
before you can use it.</p></li><li><p>A container using a ConfigMap as a <a href=#using-subpath><code>subPath</code></a> volume mount will not
receive ConfigMap updates.</p></li><li><p>Text data is exposed as files using the UTF-8 character encoding. For other character encodings, use <code>binaryData</code>.</p></li></ul></div><h3 id=downwardapi>downwardAPI</h3><p>A <code>downwardAPI</code> volume makes <a class=glossary-tooltip title='A mechanism to expose Pod and container field values to code running in a container.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/downward-api/ target=_blank aria-label='downward API'>downward API</a>
data available to applications. Within the volume, you can find the exposed
data as read-only files in plain text format.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A container using the downward API as a <a href=#using-subpath><code>subPath</code></a> volume mount does not
receive updates when field values change.</div><p>See <a href=/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/>Expose Pod Information to Containers Through Files</a>
to learn more.</p><h3 id=emptydir>emptyDir</h3><p>An <code>emptyDir</code> volume is first created when a Pod is assigned to a node, and
exists as long as that Pod is running on that node. As the name says, the
<code>emptyDir</code> volume is initially empty. All containers in the Pod can read and write the same
files in the <code>emptyDir</code> volume, though that volume can be mounted at the same
or different paths in each container. When a Pod is removed from a node for
any reason, the data in the <code>emptyDir</code> is deleted permanently.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A container crashing does <em>not</em> remove a Pod from a node. The data in an <code>emptyDir</code> volume
is safe across container crashes.</div><p>Some uses for an <code>emptyDir</code> are:</p><ul><li>scratch space, such as for a disk-based merge sort</li><li>checkpointing a long computation for recovery from crashes</li><li>holding files that a content-manager container fetches while a webserver
container serves the data</li></ul><p>The <code>emptyDir.medium</code> field controls where <code>emptyDir</code> volumes are stored. By
default <code>emptyDir</code> volumes are stored on whatever medium that backs the node
such as disk, SSD, or network storage, depending on your environment. If you set
the <code>emptyDir.medium</code> field to <code>"Memory"</code>, Kubernetes mounts a tmpfs (RAM-backed
filesystem) for you instead. While tmpfs is very fast, be aware that unlike
disks, tmpfs is cleared on node reboot and any files you write count against
your container's memory limit.</p><p>A size limit can be specified for the default medium, which limits the capacity
of the <code>emptyDir</code> volume. The storage is allocated from <a href=/docs/concepts/configuration/manage-resources-containers/#setting-requests-and-limits-for-local-ephemeral-storage>node ephemeral
storage</a>.
If that is filled up from another source (for example, log files or image
overlays), the <code>emptyDir</code> may run out of capacity before this limit.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If the <code>SizeMemoryBackedVolumes</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> is enabled,
you can specify a size for memory backed volumes. If no size is specified, memory
backed volumes are sized to 50% of the memory on a Linux host.</div><h4 id=emptydir-configuration-example>emptyDir configuration example</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/cache<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cache-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cache-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>sizeLimit</span>:<span style=color:#bbb> </span>500Mi<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=fc>fc (fibre channel)</h3><p>An <code>fc</code> volume type allows an existing fibre channel block storage volume
to mount in a Pod. You can specify single or multiple target world wide names (WWNs)
using the parameter <code>targetWWNs</code> in your Volume configuration. If multiple WWNs are specified,
targetWWNs expect that those WWNs are from multi-path connections.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must configure FC SAN Zoning to allocate and mask those LUNs (volumes) to the target WWNs
beforehand so that Kubernetes hosts can access them.</div><p>See the <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/fibre_channel>fibre channel example</a> for more details.</p><h3 id=gcepersistentdisk>gcePersistentDisk (deprecated)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [deprecated]</code></div><p>A <code>gcePersistentDisk</code> volume mounts a Google Compute Engine (GCE)
<a href=https://cloud.google.com/compute/docs/disks>persistent disk</a> (PD) into your Pod.
Unlike <code>emptyDir</code>, which is erased when a pod is removed, the contents of a PD are
preserved and the volume is merely unmounted. This means that a PD can be
pre-populated with data, and that data can be shared between pods.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must create a PD using <code>gcloud</code> or the GCE API or UI before you can use it.</div><p>There are some restrictions when using a <code>gcePersistentDisk</code>:</p><ul><li>the nodes on which Pods are running must be GCE VMs</li><li>those VMs need to be in the same GCE project and zone as the persistent disk</li></ul><p>One feature of GCE persistent disk is concurrent read-only access to a persistent disk.
A <code>gcePersistentDisk</code> volume permits multiple consumers to simultaneously
mount a persistent disk as read-only. This means that you can pre-populate a PD with your dataset
and then serve it in parallel from as many Pods as you need. Unfortunately,
PDs can only be mounted by a single consumer in read-write mode. Simultaneous
writers are not allowed.</p><p>Using a GCE persistent disk with a Pod controlled by a ReplicaSet will fail unless
the PD is read-only or the replica count is 0 or 1.</p><h4 id=gce-create-persistent-disk>Creating a GCE persistent disk</h4><p>Before you can use a GCE persistent disk with a Pod, you need to create it.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>gcloud compute disks create --size<span style=color:#666>=</span>500GB --zone<span style=color:#666>=</span>us-central1-a my-data-disk
</span></span></code></pre></div><h4 id=gce-persistent-disk-configuration-example>GCE persistent disk configuration example</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># This GCE PD must already exist.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>gcePersistentDisk</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pdName</span>:<span style=color:#bbb> </span>my-data-disk<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=regional-persistent-disks>Regional persistent disks</h4><p>The <a href=https://cloud.google.com/compute/docs/disks/#repds>Regional persistent disks</a>
feature allows the creation of persistent disks that are available in two zones
within the same region. In order to use this feature, the volume must be provisioned
as a PersistentVolume; referencing the volume directly from a pod is not supported.</p><h4 id=manually-provisioning-a-regional-pd-persistentvolume>Manually provisioning a Regional PD PersistentVolume</h4><p>Dynamic provisioning is possible using a
<a href=/docs/concepts/storage/storage-classes/#gce-pd>StorageClass for GCE PD</a>.
Before creating a PersistentVolume, you must create the persistent disk:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>gcloud compute disks create --size<span style=color:#666>=</span>500GB my-data-disk
</span></span><span style=display:flex><span>  --region us-central1
</span></span><span style=display:flex><span>  --replica-zones us-central1-a,us-central1-b
</span></span></code></pre></div><h4 id=regional-persistent-disk-configuration-example>Regional persistent disk configuration example</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>capacity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>400Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>gcePersistentDisk</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pdName</span>:<span style=color:#bbb> </span>my-data-disk<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>required</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># failure-domain.beta.kubernetes.io/zone should be used prior to 1.21</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- us-central1-a<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- us-central1-b<span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=gce-csi-migration>GCE CSI migration</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>The <code>CSIMigration</code> feature for GCE PD, when enabled, redirects all plugin operations
from the existing in-tree plugin to the <code>pd.csi.storage.gke.io</code> Container
Storage Interface (CSI) Driver. In order to use this feature, the <a href=https://github.com/kubernetes-sigs/gcp-compute-persistent-disk-csi-driver>GCE PD CSI
Driver</a>
must be installed on the cluster.</p><h4 id=gce-csi-migration-complete>GCE CSI migration complete</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code></div><p>To disable the <code>gcePersistentDisk</code> storage plugin from being loaded by the controller manager
and the kubelet, set the <code>InTreePluginGCEUnregister</code> flag to <code>true</code>.</p><h3 id=gitrepo>gitRepo (deprecated)</h3><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> The <code>gitRepo</code> volume type is deprecated. To provision a container with a git repo, mount an <a href=#emptydir>EmptyDir</a> into an InitContainer that clones the repo using git, then mount the <a href=#emptydir>EmptyDir</a> into the Pod's container.</div><p>A <code>gitRepo</code> volume is an example of a volume plugin. This plugin
mounts an empty directory and clones a git repository into this directory
for your Pod to use.</p><p>Here is an example of a <code>gitRepo</code> volume:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/mypath<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>git-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>git-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>gitRepo</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>repository</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;git@somewhere:me/my-git-repository.git&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>revision</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;22f1d8406d464b0c0874075539c1f2e96c253775&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=glusterfs>glusterfs (deprecated)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [deprecated]</code></div><p>A <code>glusterfs</code> volume allows a <a href=https://www.gluster.org>Glusterfs</a> (an open
source networked filesystem) volume to be mounted into your Pod. Unlike
<code>emptyDir</code>, which is erased when a Pod is removed, the contents of a
<code>glusterfs</code> volume are preserved and the volume is merely unmounted. This
means that a <code>glusterfs</code> volume can be pre-populated with data, and that data can
be shared between pods. GlusterFS can be mounted by multiple writers
simultaneously.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must have your own GlusterFS installation running before you can use it.</div><p>See the <a href=https://github.com/kubernetes/examples/tree/master/volumes/glusterfs>GlusterFS example</a> for more details.</p><h3 id=hostpath>hostPath</h3><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>HostPath volumes present many security risks, and it is a best practice to avoid the use of
HostPaths when possible. When a HostPath volume must be used, it should be scoped to only the
required file or directory, and mounted as ReadOnly.</p><p>If restricting HostPath access to specific directories through AdmissionPolicy, <code>volumeMounts</code> MUST
be required to use <code>readOnly</code> mounts for the policy to be effective.</p></div><p>A <code>hostPath</code> volume mounts a file or directory from the host node's filesystem
into your Pod. This is not something that most Pods will need, but it offers a
powerful escape hatch for some applications.</p><p>For example, some uses for a <code>hostPath</code> are:</p><ul><li>running a container that needs access to Docker internals; use a <code>hostPath</code>
of <code>/var/lib/docker</code></li><li>running cAdvisor in a container; use a <code>hostPath</code> of <code>/sys</code></li><li>allowing a Pod to specify whether a given <code>hostPath</code> should exist prior to the
Pod running, whether it should be created, and what it should exist as</li></ul><p>In addition to the required <code>path</code> property, you can optionally specify a <code>type</code> for a <code>hostPath</code> volume.</p><p>The supported values for field <code>type</code> are:</p><table><thead><tr><th style=text-align:left>Value</th><th style=text-align:left>Behavior</th></tr></thead><tbody><tr><td style=text-align:left></td><td style=text-align:left>Empty string (default) is for backward compatibility, which means that no checks will be performed before mounting the hostPath volume.</td></tr><tr><td style=text-align:left><code>DirectoryOrCreate</code></td><td style=text-align:left>If nothing exists at the given path, an empty directory will be created there as needed with permission set to 0755, having the same group and ownership with Kubelet.</td></tr><tr><td style=text-align:left><code>Directory</code></td><td style=text-align:left>A directory must exist at the given path</td></tr><tr><td style=text-align:left><code>FileOrCreate</code></td><td style=text-align:left>If nothing exists at the given path, an empty file will be created there as needed with permission set to 0644, having the same group and ownership with Kubelet.</td></tr><tr><td style=text-align:left><code>File</code></td><td style=text-align:left>A file must exist at the given path</td></tr><tr><td style=text-align:left><code>Socket</code></td><td style=text-align:left>A UNIX socket must exist at the given path</td></tr><tr><td style=text-align:left><code>CharDevice</code></td><td style=text-align:left>A character device must exist at the given path</td></tr><tr><td style=text-align:left><code>BlockDevice</code></td><td style=text-align:left>A block device must exist at the given path</td></tr></tbody></table><p>Watch out when using this type of volume, because:</p><ul><li>HostPaths can expose privileged system credentials (such as for the Kubelet) or privileged APIs
(such as container runtime socket), which can be used for container escape or to attack other
parts of the cluster.</li><li>Pods with identical configuration (such as created from a PodTemplate) may
behave differently on different nodes due to different files on the nodes</li><li>The files or directories created on the underlying hosts are only writable by root. You
either need to run your process as root in a
<a href=/docs/tasks/configure-pod-container/security-context/>privileged Container</a> or modify the file
permissions on the host to be able to write to a <code>hostPath</code> volume</li></ul><h4 id=hostpath-configuration-example>hostPath configuration example</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># directory location on host</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># this field is optional</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Directory<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> The <code>FileOrCreate</code> mode does not create the parent directory of the file. If the parent directory
of the mounted file does not exist, the pod fails to start. To ensure that this mode works,
you can try to mount directories and files separately, as shown in the
<a href=#hostpath-fileorcreate-example><code>FileOrCreate</code>configuration</a>.</div><h4 id=hostpath-fileorcreate-example>hostPath FileOrCreate configuration example</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/test-webserver:latest<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/local/aaa<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydir<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/local/aaa/1.txt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myfile<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mydir<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Ensure the file directory is created.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/local/aaa<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>DirectoryOrCreate<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myfile<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/local/aaa/1.txt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>FileOrCreate<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=iscsi>iscsi</h3><p>An <code>iscsi</code> volume allows an existing iSCSI (SCSI over IP) volume to be mounted
into your Pod. Unlike <code>emptyDir</code>, which is erased when a Pod is removed, the
contents of an <code>iscsi</code> volume are preserved and the volume is merely
unmounted. This means that an iscsi volume can be pre-populated with data, and
that data can be shared between pods.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must have your own iSCSI server running with the volume created before you can use it.</div><p>A feature of iSCSI is that it can be mounted as read-only by multiple consumers
simultaneously. This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many Pods as you need. Unfortunately,
iSCSI volumes can only be mounted by a single consumer in read-write mode.
Simultaneous writers are not allowed.</p><p>See the <a href=https://github.com/kubernetes/examples/tree/master/volumes/iscsi>iSCSI example</a> for more details.</p><h3 id=local>local</h3><p>A <code>local</code> volume represents a mounted local storage device such as a disk,
partition or directory.</p><p>Local volumes can only be used as a statically created PersistentVolume. Dynamic
provisioning is not supported.</p><p>Compared to <code>hostPath</code> volumes, <code>local</code> volumes are used in a durable and
portable manner without manually scheduling pods to nodes. The system is aware
of the volume's node constraints by looking at the node affinity on the PersistentVolume.</p><p>However, <code>local</code> volumes are subject to the availability of the underlying
node and are not suitable for all applications. If a node becomes unhealthy,
then the <code>local</code> volume becomes inaccessible by the pod. The pod using this volume
is unable to run. Applications using <code>local</code> volumes must be able to tolerate this
reduced availability, as well as potential data loss, depending on the
durability characteristics of the underlying disk.</p><p>The following example shows a PersistentVolume using a <code>local</code> volume and
<code>nodeAffinity</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-pv<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>capacity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>100Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Filesystem<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>persistentVolumeReclaimPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>local-storage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>local</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/mnt/disks/ssd1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>required</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>kubernetes.io/hostname<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- example-node<span style=color:#bbb>
</span></span></span></code></pre></div><p>You must set a PersistentVolume <code>nodeAffinity</code> when using <code>local</code> volumes.
The Kubernetes scheduler uses the PersistentVolume <code>nodeAffinity</code> to schedule
these Pods to the correct node.</p><p>PersistentVolume <code>volumeMode</code> can be set to "Block" (instead of the default
value "Filesystem") to expose the local volume as a raw block device.</p><p>When using local volumes, it is recommended to create a StorageClass with
<code>volumeBindingMode</code> set to <code>WaitForFirstConsumer</code>. For more details, see the
local <a href=/docs/concepts/storage/storage-classes/#local>StorageClass</a> example.
Delaying volume binding ensures that the PersistentVolumeClaim binding decision
will also be evaluated with any other node constraints the Pod may have,
such as node resource requirements, node selectors, Pod affinity, and Pod anti-affinity.</p><p>An external static provisioner can be run separately for improved management of
the local volume lifecycle. Note that this provisioner does not support dynamic
provisioning yet. For an example on how to run an external local provisioner,
see the <a href=https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner>local volume provisioner user
guide</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The local PersistentVolume requires manual cleanup and deletion by the
user if the external static provisioner is not used to manage the volume
lifecycle.</div><h3 id=nfs>nfs</h3><p>An <code>nfs</code> volume allows an existing NFS (Network File System) share to be
mounted into a Pod. Unlike <code>emptyDir</code>, which is erased when a Pod is
removed, the contents of an <code>nfs</code> volume are preserved and the volume is merely
unmounted. This means that an NFS volume can be pre-populated with data, and
that data can be shared between pods. NFS can be mounted by multiple
writers simultaneously.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/my-nfs-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nfs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>server</span>:<span style=color:#bbb> </span>my-nfs-server.example.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/my-nfs-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>You must have your own NFS server running with the share exported before you can use it.</p><p>Also note that you can't specify NFS mount options in a Pod spec. You can either set mount options server-side or use <a href=https://man7.org/linux/man-pages/man5/nfsmount.conf.5.html>/etc/nfsmount.conf</a>. You can also mount NFS volumes via PersistentVolumes which do allow you to set mount options.</p></div><p>See the <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/nfs>NFS example</a> for an example of mounting NFS volumes with PersistentVolumes.</p><h3 id=persistentvolumeclaim>persistentVolumeClaim</h3><p>A <code>persistentVolumeClaim</code> volume is used to mount a
<a href=/docs/concepts/storage/persistent-volumes/>PersistentVolume</a> into a Pod. PersistentVolumeClaims
are a way for users to "claim" durable storage (such as a GCE PersistentDisk or an
iSCSI volume) without knowing the details of the particular cloud environment.</p><p>See the information about <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a> for more
details.</p><h3 id=portworxvolume>portworxVolume (deprecated)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [deprecated]</code></div><p>A <code>portworxVolume</code> is an elastic block storage layer that runs hyperconverged with
Kubernetes. <a href=https://portworx.com/use-case/kubernetes-storage/>Portworx</a> fingerprints storage
in a server, tiers based on capabilities, and aggregates capacity across multiple servers.
Portworx runs in-guest in virtual machines or on bare metal Linux nodes.</p><p>A <code>portworxVolume</code> can be dynamically created through Kubernetes or it can also
be pre-provisioned and referenced inside a Pod.
Here is an example Pod referencing a pre-provisioned Portworx volume:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-portworx-volume-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/test-webserver<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/mnt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pxvol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pxvol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:#080;font-style:italic># This Portworx volume must already exist.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>portworxVolume</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeID</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;pxvol&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&lt;fs-type&gt;&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Make sure you have an existing PortworxVolume with name <code>pxvol</code>
before using it in the Pod.</div><p>For more details, see the <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/portworx/README.md>Portworx volume</a> examples.</p><h4 id=portworx-csi-migration>Portworx CSI migration</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [beta]</code></div><p>The <code>CSIMigration</code> feature for Portworx has been added but disabled by default in Kubernetes 1.23 since it's in alpha state.
It has been beta now since v1.25 but it is still turned off by default.
It redirects all plugin operations from the existing in-tree plugin to the
<code>pxd.portworx.com</code> Container Storage Interface (CSI) Driver.
<a href=https://docs.portworx.com/portworx-install-with-kubernetes/storage-operations/csi/>Portworx CSI Driver</a>
must be installed on the cluster.
To enable the feature, set <code>CSIMigrationPortworx=true</code> in kube-controller-manager and kubelet.</p><h3 id=projected>projected</h3><p>A projected volume maps several existing volume sources into the same
directory. For more details, see <a href=/docs/concepts/storage/projected-volumes/>projected volumes</a>.</p><h3 id=rbd>rbd</h3><p>An <code>rbd</code> volume allows a
<a href=https://docs.ceph.com/en/latest/rbd/>Rados Block Device</a> (RBD) volume to mount
into your Pod. Unlike <code>emptyDir</code>, which is erased when a pod is removed, the
contents of an <code>rbd</code> volume are preserved and the volume is unmounted. This
means that a RBD volume can be pre-populated with data, and that data can be
shared between pods.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must have a Ceph installation running before you can use RBD.</div><p>A feature of RBD is that it can be mounted as read-only by multiple consumers
simultaneously. This means that you can pre-populate a volume with your dataset
and then serve it in parallel from as many pods as you need. Unfortunately,
RBD volumes can only be mounted by a single consumer in read-write mode.
Simultaneous writers are not allowed.</p><p>See the <a href=https://github.com/kubernetes/examples/tree/master/volumes/rbd>RBD example</a>
for more details.</p><h4 id=rbd-csi-migration>RBD CSI migration</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code></div><p>The <code>CSIMigration</code> feature for <code>RBD</code>, when enabled, redirects all plugin
operations from the existing in-tree plugin to the <code>rbd.csi.ceph.com</code> <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> driver. In order to use this
feature, the
<a href=https://github.com/ceph/ceph-csi>Ceph CSI driver</a>
must be installed on the cluster and the <code>CSIMigrationRBD</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
must be enabled. (Note that the <code>csiMigrationRBD</code> flag has been removed and
replaced with <code>CSIMigrationRBD</code> in release v1.24)</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>As a Kubernetes cluster operator that administers storage, here are the
prerequisites that you must complete before you attempt migration to the
RBD CSI driver:</p><ul><li>You must install the Ceph CSI driver (<code>rbd.csi.ceph.com</code>), v3.5.0 or above,
into your Kubernetes cluster.</li><li>considering the <code>clusterID</code> field is a required parameter for CSI driver for
its operations, but in-tree StorageClass has <code>monitors</code> field as a required
parameter, a Kubernetes storage admin has to create a clusterID based on the
monitors hash ( ex:<code>#echo -n '&lt;monitors_string>' | md5sum</code>) in the CSI config map and keep the monitors
under this clusterID configuration.</li><li>Also, if the value of <code>adminId</code> in the in-tree Storageclass is different from
<code>admin</code>, the <code>adminSecretName</code> mentioned in the in-tree Storageclass has to be
patched with the base64 value of the <code>adminId</code> parameter value, otherwise this
step can be skipped.</li></ul></div><h3 id=secret>secret</h3><p>A <code>secret</code> volume is used to pass sensitive information, such as passwords, to
Pods. You can store secrets in the Kubernetes API and mount them as files for
use by pods without coupling to Kubernetes directly. <code>secret</code> volumes are
backed by tmpfs (a RAM-backed filesystem) so they are never written to
non-volatile storage.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must create a Secret in the Kubernetes API before you can use it.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A container using a Secret as a <a href=#using-subpath><code>subPath</code></a> volume mount will not
receive Secret updates.</div><p>For more details, see <a href=/docs/concepts/configuration/secret/>Configuring Secrets</a>.</p><h3 id=vspherevolume>vsphereVolume (deprecated)</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> We recommend to use vSphere CSI out-of-tree driver instead.</div><p>A <code>vsphereVolume</code> is used to mount a vSphere VMDK volume into your Pod. The contents
of a volume are preserved when it is unmounted. It supports both VMFS and VSAN datastore.</p><p>For more information, see the <a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere>vSphere volume</a> examples.</p><h4 id=vsphere-csi-migration>vSphere CSI migration</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code></div><p>The <code>CSIMigrationvSphere</code> feature for <code>vsphereVolume</code> is enabled by default as of Kubernetes v1.25.
All plugin operations from the in-tree <code>vspherevolume</code> will be redirected to the <code>csi.vsphere.vmware.com</code> <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> driver unless <code>CSIMigrationvSphere</code> feature gate is disabled.</p><p><a href=https://github.com/kubernetes-sigs/vsphere-csi-driver>vSphere CSI driver</a>
must be installed on the cluster. You can find additional advice on how to migrate in-tree <code>vsphereVolume</code> in VMware's documentation page
<a href=https://docs.vmware.com/en/VMware-vSphere-Container-Storage-Plug-in/2.0/vmware-vsphere-csp-getting-started/GUID-968D421F-D464-4E22-8127-6CB9FF54423F.html>Migrating In-Tree vSphere Volumes to vSphere Container Storage Plug-in</a>.</p><p>As of Kubernetes v1.25, vSphere releases less than 7.0u2 are not supported for the
(deprecated) in-tree vSphere storage driver. You must run vSphere 7.0u2 or later
in order to either continue using the deprecated driver, or to migrate to
the replacement CSI driver.</p><p>If you are running a version of Kubernetes other than v1.25, consult
the documentation for that version of Kubernetes.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>The following StorageClass parameters from the built-in <code>vsphereVolume</code> plugin are not supported by the vSphere CSI driver:</p><ul><li><code>diskformat</code></li><li><code>hostfailurestotolerate</code></li><li><code>forceprovisioning</code></li><li><code>cachereservation</code></li><li><code>diskstripes</code></li><li><code>objectspacereservation</code></li><li><code>iopslimit</code></li></ul><p>Existing volumes created using these parameters will be migrated to the vSphere CSI driver,
but new volumes created by the vSphere CSI driver will not be honoring these parameters.</p></div><h4 id=vsphere-csi-migration-complete>vSphere CSI migration complete</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [beta]</code></div><p>To turn off the <code>vsphereVolume</code> plugin from being loaded by the controller manager and the kubelet, you need to set <code>InTreePluginvSphereUnregister</code> feature flag to <code>true</code>. You must install a <code>csi.vsphere.vmware.com</code> <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> driver on all worker nodes.</p><h2 id=using-subpath>Using subPath</h2><p>Sometimes, it is useful to share one volume for multiple uses in a single pod.
The <code>volumeMounts.subPath</code> property specifies a sub-path inside the referenced volume
instead of its root.</p><p>The following example shows how to configure a Pod with a LAMP stack (Linux Apache MySQL PHP)
using a single, shared volume. This sample <code>subPath</code> configuration is not recommended
for production use.</p><p>The PHP application's code and assets map to the volume's <code>html</code> folder and
the MySQL database is stored in the volume's <code>mysql</code> folder. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-lamp-site<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>MYSQL_ROOT_PASSWORD<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;rootpasswd&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/lib/mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>site-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>subPath</span>:<span style=color:#bbb> </span>mysql<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>php<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>php:7.0-apache<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/www/html<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>site-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>subPath</span>:<span style=color:#bbb> </span>html<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>site-data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>my-lamp-site-data<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=using-subpath-expanded-environment>Using subPath with expanded environment variables</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code></div><p>Use the <code>subPathExpr</code> field to construct <code>subPath</code> directory names from
downward API environment variables.
The <code>subPath</code> and <code>subPathExpr</code> properties are mutually exclusive.</p><p>In this example, a <code>Pod</code> uses <code>subPathExpr</code> to create a directory <code>pod1</code> within
the <code>hostPath</code> volume <code>/var/log/pods</code>.
The <code>hostPath</code> volume takes the <code>Pod</code> name from the <code>downwardAPI</code>.
The host directory <code>/var/log/pods/pod1</code> is mounted at <code>/logs</code> in the container.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>container1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>POD_NAME<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>fieldRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>fieldPath</span>:<span style=color:#bbb> </span>metadata.name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;sh&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;while [ true ]; do echo &#39;Hello&#39;; sleep 10; done | tee -a /logs/hello.txt&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>workdir1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/logs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># The variable expansion uses round brackets (not curly brackets).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>subPathExpr</span>:<span style=color:#bbb> </span>$(POD_NAME)<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>workdir1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/log/pods<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=resources>Resources</h2><p>The storage media (such as Disk or SSD) of an <code>emptyDir</code> volume is determined by the
medium of the filesystem holding the kubelet root dir (typically
<code>/var/lib/kubelet</code>). There is no limit on how much space an <code>emptyDir</code> or
<code>hostPath</code> volume can consume, and no isolation between containers or between
pods.</p><p>To learn about requesting space using a resource specification, see
<a href=/docs/concepts/configuration/manage-resources-containers/>how to manage resources</a>.</p><h2 id=out-of-tree-volume-plugins>Out-of-tree volume plugins</h2><p>The out-of-tree volume plugins include
<a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label='Container Storage Interface'>Container Storage Interface</a> (CSI), and also FlexVolume (which is deprecated). These plugins enable storage vendors to create custom storage plugins
without adding their plugin source code to the Kubernetes repository.</p><p>Previously, all volume plugins were "in-tree". The "in-tree" plugins were built, linked, compiled,
and shipped with the core Kubernetes binaries. This meant that adding a new storage system to
Kubernetes (a volume plugin) required checking code into the core Kubernetes code repository.</p><p>Both CSI and FlexVolume allow volume plugins to be developed independent of
the Kubernetes code base, and deployed (installed) on Kubernetes clusters as
extensions.</p><p>For storage vendors looking to create an out-of-tree volume plugin, please refer
to the <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md>volume plugin FAQ</a>.</p><h3 id=csi>csi</h3><p><a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>Container Storage Interface</a>
(CSI) defines a standard interface for container orchestration systems (like
Kubernetes) to expose arbitrary storage systems to their container workloads.</p><p>Please read the <a href=https://git.k8s.io/design-proposals-archive/storage/container-storage-interface.md>CSI design proposal</a> for more information.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Support for CSI spec versions 0.2 and 0.3 are deprecated in Kubernetes
v1.13 and will be removed in a future release.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> CSI drivers may not be compatible across all Kubernetes releases.
Please check the specific CSI driver's documentation for supported
deployments steps for each Kubernetes release and a compatibility matrix.</div><p>Once a CSI compatible volume driver is deployed on a Kubernetes cluster, users
may use the <code>csi</code> volume type to attach or mount the volumes exposed by the
CSI driver.</p><p>A <code>csi</code> volume can be used in a Pod in three different ways:</p><ul><li>through a reference to a <a href=#persistentvolumeclaim>PersistentVolumeClaim</a></li><li>with a <a href=/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes>generic ephemeral volume</a></li><li>with a <a href=/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes>CSI ephemeral volume</a> if the driver supports that</li></ul><p>The following fields are available to storage administrators to configure a CSI
persistent volume:</p><ul><li><code>driver</code>: A string value that specifies the name of the volume driver to use.
This value must correspond to the value returned in the <code>GetPluginInfoResponse</code>
by the CSI driver as defined in the <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md#getplugininfo>CSI spec</a>.
It is used by Kubernetes to identify which CSI driver to call out to, and by
CSI driver components to identify which PV objects belong to the CSI driver.</li><li><code>volumeHandle</code>: A string value that uniquely identifies the volume. This value
must correspond to the value returned in the <code>volume.id</code> field of the
<code>CreateVolumeResponse</code> by the CSI driver as defined in the <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume>CSI spec</a>.
The value is passed as <code>volume_id</code> on all calls to the CSI volume driver when
referencing the volume.</li><li><code>readOnly</code>: An optional boolean value indicating whether the volume is to be
"ControllerPublished" (attached) as read only. Default is false. This value is
passed to the CSI driver via the <code>readonly</code> field in the
<code>ControllerPublishVolumeRequest</code>.</li><li><code>fsType</code>: If the PV's <code>VolumeMode</code> is <code>Filesystem</code> then this field may be used
to specify the filesystem that should be used to mount the volume. If the
volume has not been formatted and formatting is supported, this value will be
used to format the volume.
This value is passed to the CSI driver via the <code>VolumeCapability</code> field of
<code>ControllerPublishVolumeRequest</code>, <code>NodeStageVolumeRequest</code>, and
<code>NodePublishVolumeRequest</code>.</li><li><code>volumeAttributes</code>: A map of string to string that specifies static properties
of a volume. This map must correspond to the map returned in the
<code>volume.attributes</code> field of the <code>CreateVolumeResponse</code> by the CSI driver as
defined in the <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md#createvolume>CSI spec</a>.
The map is passed to the CSI driver via the <code>volume_context</code> field in the
<code>ControllerPublishVolumeRequest</code>, <code>NodeStageVolumeRequest</code>, and
<code>NodePublishVolumeRequest</code>.</li><li><code>controllerPublishSecretRef</code>: A reference to the secret object containing
sensitive information to pass to the CSI driver to complete the CSI
<code>ControllerPublishVolume</code> and <code>ControllerUnpublishVolume</code> calls. This field is
optional, and may be empty if no secret is required. If the Secret
contains more than one secret, all secrets are passed.
<code>nodeExpandSecretRef</code>: A reference to the secret containing sensitive
information to pass to the CSI driver to complete the CSI
<code>NodeExpandVolume</code> call. This field is optional, and may be empty if no
secret is required. If the object contains more than one secret, all
secrets are passed. When you have configured secret data for node-initiated
volume expansion, the kubelet passes that data via the <code>NodeExpandVolume()</code>
call to the CSI driver. In order to use the <code>nodeExpandSecretRef</code> field, your
cluster should be running Kubernetes version 1.25 or later and you must enable
the <a href=https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
named <code>CSINodeExpandSecret</code> for each kube-apiserver and for the kubelet on every
node. You must also be using a CSI driver that supports or requires secret data during
node-initiated storage resize operations.</li><li><code>nodePublishSecretRef</code>: A reference to the secret object containing
sensitive information to pass to the CSI driver to complete the CSI
<code>NodePublishVolume</code> call. This field is optional, and may be empty if no
secret is required. If the secret object contains more than one secret, all
secrets are passed.</li><li><code>nodeStageSecretRef</code>: A reference to the secret object containing
sensitive information to pass to the CSI driver to complete the CSI
<code>NodeStageVolume</code> call. This field is optional, and may be empty if no secret
is required. If the Secret contains more than one secret, all secrets
are passed.</li></ul><h4 id=csi-raw-block-volume-support>CSI raw block volume support</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code></div><p>Vendors with external CSI drivers can implement raw block volume support
in Kubernetes workloads.</p><p>You can set up your
<a href=/docs/concepts/storage/persistent-volumes/#raw-block-volume-support>PersistentVolume/PersistentVolumeClaim with raw block volume support</a> as usual, without any CSI specific changes.</p><h4 id=csi-ephemeral-volumes>CSI ephemeral volumes</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>You can directly configure CSI volumes within the Pod
specification. Volumes specified in this way are ephemeral and do not
persist across pod restarts. See <a href=/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes>Ephemeral
Volumes</a>
for more information.</p><p>For more information on how to develop a CSI driver, refer to the
<a href=https://kubernetes-csi.github.io/docs/>kubernetes-csi documentation</a></p><h4 id=windows-csi-proxy>Windows CSI proxy</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [stable]</code></div><p>CSI node plugins need to perform various privileged
operations like scanning of disk devices and mounting of file systems. These operations
differ for each host operating system. For Linux worker nodes, containerized CSI node
node plugins are typically deployed as privileged containers. For Windows worker nodes,
privileged operations for containerized CSI node plugins is supported using
<a href=https://github.com/kubernetes-csi/csi-proxy>csi-proxy</a>, a community-managed,
stand-alone binary that needs to be pre-installed on each Windows node.</p><p>For more details, refer to the deployment guide of the CSI plugin you wish to deploy.</p><h4 id=migrating-to-csi-drivers-from-in-tree-plugins>Migrating to CSI drivers from in-tree plugins</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>The <code>CSIMigration</code> feature directs operations against existing in-tree
plugins to corresponding CSI plugins (which are expected to be installed and configured).
As a result, operators do not have to make any
configuration changes to existing Storage Classes, PersistentVolumes or PersistentVolumeClaims
(referring to in-tree plugins) when transitioning to a CSI driver that supersedes an in-tree plugin.</p><p>The operations and features that are supported include:
provisioning/delete, attach/detach, mount/unmount and resizing of volumes.</p><p>In-tree plugins that support <code>CSIMigration</code> and have a corresponding CSI driver implemented
are listed in <a href=#volume-types>Types of Volumes</a>.</p><p>The following in-tree plugins support persistent storage on Windows nodes:</p><ul><li><a href=#awselasticblockstore><code>awsElasticBlockStore</code></a></li><li><a href=#azuredisk><code>azureDisk</code></a></li><li><a href=#azurefile><code>azureFile</code></a></li><li><a href=#gcepersistentdisk><code>gcePersistentDisk</code></a></li><li><a href=#vspherevolume><code>vsphereVolume</code></a></li></ul><h3 id=flexvolume>flexVolume (deprecated)</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [deprecated]</code></div><p>FlexVolume is an out-of-tree plugin interface that uses an exec-based model to interface
with storage drivers. The FlexVolume driver binaries must be installed in a pre-defined
volume plugin path on each node and in some cases the control plane nodes as well.</p><p>Pods interact with FlexVolume drivers through the <code>flexVolume</code> in-tree volume plugin.
For more details, see the FlexVolume <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-storage/flexvolume.md#readme>README</a> document.</p><p>The following FlexVolume <a href=https://github.com/Microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows>plugins</a>,
deployed as PowerShell scripts on the host, support Windows nodes:</p><ul><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~smb.cmd>SMB</a></li><li><a href=https://github.com/microsoft/K8s-Storage-Plugins/tree/master/flexvolume/windows/plugins/microsoft.com~iscsi.cmd>iSCSI</a></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>FlexVolume is deprecated. Using an out-of-tree CSI driver is the recommended way to integrate external storage with Kubernetes.</p><p>Maintainers of FlexVolume driver should implement a CSI Driver and help to migrate users of FlexVolume drivers to CSI.
Users of FlexVolume should move their workloads to use the equivalent CSI Driver.</p></div><h2 id=mount-propagation>Mount propagation</h2><p>Mount propagation allows for sharing volumes mounted by a container to
other containers in the same pod, or even to other pods on the same node.</p><p>Mount propagation of a volume is controlled by the <code>mountPropagation</code> field
in <code>Container.volumeMounts</code>. Its values are:</p><ul><li><p><code>None</code> - This volume mount will not receive any subsequent mounts
that are mounted to this volume or any of its subdirectories by the host.
In similar fashion, no mounts created by the container will be visible on
the host. This is the default mode.</p><p>This mode is equal to <code>private</code> mount propagation as described in the
<a href=https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt>Linux kernel documentation</a></p></li><li><p><code>HostToContainer</code> - This volume mount will receive all subsequent mounts
that are mounted to this volume or any of its subdirectories.</p><p>In other words, if the host mounts anything inside the volume mount, the
container will see it mounted there.</p><p>Similarly, if any Pod with <code>Bidirectional</code> mount propagation to the same
volume mounts anything there, the container with <code>HostToContainer</code> mount
propagation will see it.</p><p>This mode is equal to <code>rslave</code> mount propagation as described in the
<a href=https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt>Linux kernel documentation</a></p></li><li><p><code>Bidirectional</code> - This volume mount behaves the same the <code>HostToContainer</code> mount.
In addition, all volume mounts created by the container will be propagated
back to the host and to all containers of all pods that use the same volume.</p><p>A typical use case for this mode is a Pod with a FlexVolume or CSI driver or
a Pod that needs to mount something on the host using a <code>hostPath</code> volume.</p><p>This mode is equal to <code>rshared</code> mount propagation as described in the
<a href=https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt>Linux kernel documentation</a></p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> <code>Bidirectional</code> mount propagation can be dangerous. It can damage
the host operating system and therefore it is allowed only in privileged
containers. Familiarity with Linux kernel behavior is strongly recommended.
In addition, any volume mounts created by containers in pods must be destroyed
(unmounted) by the containers on termination.</div></li></ul><h3 id=configuration>Configuration</h3><p>Before mount propagation can work properly on some deployments (CoreOS,
RedHat/Centos, Ubuntu) mount share must be configured correctly in
Docker as shown below.</p><p>Edit your Docker's <code>systemd</code> service file. Set <code>MountFlags</code> as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>MountFlags</span><span style=color:#666>=</span>shared
</span></span></code></pre></div><p>Or, remove <code>MountFlags=slave</code> if present. Then restart the Docker daemon:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sudo systemctl daemon-reload
</span></span><span style=display:flex><span>sudo systemctl restart docker
</span></span></code></pre></div><h2 id=what-s-next>What's next</h2><p>Follow an example of <a href=/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/>deploying WordPress and MySQL with Persistent Volumes</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-ffd12528a12882b282e1bd19e29f9e75>7.2 - Persistent Volumes</h1><p>This document describes <em>persistent volumes</em> in Kubernetes. Familiarity with <a href=/docs/concepts/storage/volumes/>volumes</a> is suggested.</p><h2 id=introduction>Introduction</h2><p>Managing storage is a distinct problem from managing compute instances. The PersistentVolume subsystem provides an API for users and administrators that abstracts details of how storage is provided from how it is consumed. To do this, we introduce two new API resources: PersistentVolume and PersistentVolumeClaim.</p><p>A <em>PersistentVolume</em> (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using <a href=/docs/concepts/storage/storage-classes/>Storage Classes</a>. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV. This API object captures the details of the implementation of the storage, be that NFS, iSCSI, or a cloud-provider-specific storage system.</p><p>A <em>PersistentVolumeClaim</em> (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany, see <a href=#access-modes>AccessModes</a>).</p><p>While PersistentVolumeClaims allow a user to consume abstract storage resources, it is common that users need PersistentVolumes with varying properties, such as performance, for different problems. Cluster administrators need to be able to offer a variety of PersistentVolumes that differ in more ways than size and access modes, without exposing users to the details of how those volumes are implemented. For these needs, there is the <em>StorageClass</em> resource.</p><p>See the <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/>detailed walkthrough with working examples</a>.</p><h2 id=lifecycle-of-a-volume-and-claim>Lifecycle of a volume and claim</h2><p>PVs are resources in the cluster. PVCs are requests for those resources and also act as claim checks to the resource. The interaction between PVs and PVCs follows this lifecycle:</p><h3 id=provisioning>Provisioning</h3><p>There are two ways PVs may be provisioned: statically or dynamically.</p><h4 id=static>Static</h4><p>A cluster administrator creates a number of PVs. They carry the details of the real storage, which is available for use by cluster users. They exist in the Kubernetes API and are available for consumption.</p><h4 id=dynamic>Dynamic</h4><p>When none of the static PVs the administrator created match a user's PersistentVolumeClaim,
the cluster may try to dynamically provision a volume specially for the PVC.
This provisioning is based on StorageClasses: the PVC must request a
<a href=/docs/concepts/storage/storage-classes/>storage class</a> and
the administrator must have created and configured that class for dynamic
provisioning to occur. Claims that request the class <code>""</code> effectively disable
dynamic provisioning for themselves.</p><p>To enable dynamic storage provisioning based on storage class, the cluster administrator
needs to enable the <code>DefaultStorageClass</code> <a href=/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass>admission controller</a>
on the API server. This can be done, for example, by ensuring that <code>DefaultStorageClass</code> is
among the comma-delimited, ordered list of values for the <code>--enable-admission-plugins</code> flag of
the API server component. For more information on API server command-line flags,
check <a href=/docs/admin/kube-apiserver/>kube-apiserver</a> documentation.</p><h3 id=binding>Binding</h3><p>A user creates, or in the case of dynamic provisioning, has already created, a PersistentVolumeClaim with a specific amount of storage requested and with certain access modes. A control loop in the master watches for new PVCs, finds a matching PV (if possible), and binds them together. If a PV was dynamically provisioned for a new PVC, the loop will always bind that PV to the PVC. Otherwise, the user will always get at least what they asked for, but the volume may be in excess of what was requested. Once bound, PersistentVolumeClaim binds are exclusive, regardless of how they were bound. A PVC to PV binding is a one-to-one mapping, using a ClaimRef which is a bi-directional binding between the PersistentVolume and the PersistentVolumeClaim.</p><p>Claims will remain unbound indefinitely if a matching volume does not exist. Claims will be bound as matching volumes become available. For example, a cluster provisioned with many 50Gi PVs would not match a PVC requesting 100Gi. The PVC can be bound when a 100Gi PV is added to the cluster.</p><h3 id=using>Using</h3><p>Pods use claims as volumes. The cluster inspects the claim to find the bound volume and mounts that volume for a Pod. For volumes that support multiple access modes, the user specifies which mode is desired when using their claim as a volume in a Pod.</p><p>Once a user has a claim and that claim is bound, the bound PV belongs to the user for as long as they need it. Users schedule Pods and access their claimed PVs by including a <code>persistentVolumeClaim</code> section in a Pod's <code>volumes</code> block. See <a href=#claims-as-volumes>Claims As Volumes</a> for more details on this.</p><h3 id=storage-object-in-use-protection>Storage Object in Use Protection</h3><p>The purpose of the Storage Object in Use Protection feature is to ensure that PersistentVolumeClaims (PVCs) in active use by a Pod and PersistentVolume (PVs) that are bound to PVCs are not removed from the system, as this may result in data loss.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> PVC is in active use by a Pod when a Pod object exists that is using the PVC.</div><p>If a user deletes a PVC in active use by a Pod, the PVC is not removed immediately. PVC removal is postponed until the PVC is no longer actively used by any Pods. Also, if an admin deletes a PV that is bound to a PVC, the PV is not removed immediately. PV removal is postponed until the PV is no longer bound to a PVC.</p><p>You can see that a PVC is protected when the PVC's status is <code>Terminating</code> and the <code>Finalizers</code> list includes <code>kubernetes.io/pvc-protection</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pvc hostpath
</span></span><span style=display:flex><span>Name:          hostpath
</span></span><span style=display:flex><span>Namespace:     default
</span></span><span style=display:flex><span>StorageClass:  example-hostpath
</span></span><span style=display:flex><span>Status:        Terminating
</span></span><span style=display:flex><span>Volume:
</span></span><span style=display:flex><span>Labels:        &lt;none&gt;
</span></span><span style=display:flex><span>Annotations:   volume.beta.kubernetes.io/storage-class<span style=color:#666>=</span>example-hostpath
</span></span><span style=display:flex><span>               volume.beta.kubernetes.io/storage-provisioner<span style=color:#666>=</span>example.com/hostpath
</span></span><span style=display:flex><span>Finalizers:    <span style=color:#666>[</span>kubernetes.io/pvc-protection<span style=color:#666>]</span>
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>You can see that a PV is protected when the PV's status is <code>Terminating</code> and the <code>Finalizers</code> list includes <code>kubernetes.io/pv-protection</code> too:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pv task-pv-volume
</span></span><span style=display:flex><span>Name:            task-pv-volume
</span></span><span style=display:flex><span>Labels:          <span style=color:#b8860b>type</span><span style=color:#666>=</span><span style=color:#a2f>local</span>
</span></span><span style=display:flex><span>Annotations:     &lt;none&gt;
</span></span><span style=display:flex><span>Finalizers:      <span style=color:#666>[</span>kubernetes.io/pv-protection<span style=color:#666>]</span>
</span></span><span style=display:flex><span>StorageClass:    standard
</span></span><span style=display:flex><span>Status:          Terminating
</span></span><span style=display:flex><span>Claim:
</span></span><span style=display:flex><span>Reclaim Policy:  Delete
</span></span><span style=display:flex><span>Access Modes:    RWO
</span></span><span style=display:flex><span>Capacity:        1Gi
</span></span><span style=display:flex><span>Message:
</span></span><span style=display:flex><span>Source:
</span></span><span style=display:flex><span>    Type:          HostPath <span style=color:#666>(</span>bare host directory volume<span style=color:#666>)</span>
</span></span><span style=display:flex><span>    Path:          /tmp/data
</span></span><span style=display:flex><span>    HostPathType:
</span></span><span style=display:flex><span>Events:            &lt;none&gt;
</span></span></code></pre></div><h3 id=reclaiming>Reclaiming</h3><p>When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted.</p><h4 id=retain>Retain</h4><p>The <code>Retain</code> reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered "released". But it is not yet available for another claim because the previous claimant's data remains on the volume. An administrator can manually reclaim the volume with the following steps.</p><ol><li>Delete the PersistentVolume. The associated storage asset in external infrastructure (such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume) still exists after the PV is deleted.</li><li>Manually clean up the data on the associated storage asset accordingly.</li><li>Manually delete the associated storage asset.</li></ol><p>If you want to reuse the same storage asset, create a new PersistentVolume with the same storage asset definition.</p><h4 id=delete>Delete</h4><p>For volume plugins that support the <code>Delete</code> reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS, GCE PD, Azure Disk, or Cinder volume. Volumes that were dynamically provisioned inherit the <a href=#reclaim-policy>reclaim policy of their StorageClass</a>, which defaults to <code>Delete</code>. The administrator should configure the StorageClass according to users' expectations; otherwise, the PV must be edited or patched after it is created. See <a href=/docs/tasks/administer-cluster/change-pv-reclaim-policy/>Change the Reclaim Policy of a PersistentVolume</a>.</p><h4 id=recycle>Recycle</h4><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> The <code>Recycle</code> reclaim policy is deprecated. Instead, the recommended approach is to use dynamic provisioning.</div><p>If supported by the underlying volume plugin, the <code>Recycle</code> reclaim policy performs a basic scrub (<code>rm -rf /thevolume/*</code>) on the volume and makes it available again for a new claim.</p><p>However, an administrator can configure a custom recycler Pod template using
the Kubernetes controller manager command line arguments as described in the
<a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>reference</a>.
The custom recycler Pod template must contain a <code>volumes</code> specification, as
shown in the example below:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pv-recycler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/any/path/it/will/be/replaced<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pv-recycler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;registry.k8s.io/busybox&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;/bin/sh&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \&#34;$(ls -A /scrub)\&#34; || exit 1&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/scrub<span style=color:#bbb>
</span></span></span></code></pre></div><p>However, the particular path specified in the custom recycler Pod template in the <code>volumes</code> part is replaced with the particular path of the volume that is being recycled.</p><h3 id=persistentvolume-deletion-protection-finalizer>PersistentVolume deletion protection finalizer</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code></div><p>Finalizers can be added on a PersistentVolume to ensure that PersistentVolumes
having <code>Delete</code> reclaim policy are deleted only after the backing storage are deleted.</p><p>The newly introduced finalizers <code>kubernetes.io/pv-controller</code> and <code>external-provisioner.volume.kubernetes.io/finalizer</code>
are only added to dynamically provisioned volumes.</p><p>The finalizer <code>kubernetes.io/pv-controller</code> is added to in-tree plugin volumes. The following is an example</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pv pvc-74a498d6-3929-47e8-8c02-078c1ece4d78
</span></span><span style=display:flex><span>Name:            pvc-74a498d6-3929-47e8-8c02-078c1ece4d78
</span></span><span style=display:flex><span>Labels:          &lt;none&gt;
</span></span><span style=display:flex><span>Annotations:     kubernetes.io/createdby: vsphere-volume-dynamic-provisioner
</span></span><span style=display:flex><span>                 pv.kubernetes.io/bound-by-controller: yes
</span></span><span style=display:flex><span>                 pv.kubernetes.io/provisioned-by: kubernetes.io/vsphere-volume
</span></span><span style=display:flex><span>Finalizers:      <span style=color:#666>[</span>kubernetes.io/pv-protection kubernetes.io/pv-controller<span style=color:#666>]</span>
</span></span><span style=display:flex><span>StorageClass:    vcp-sc
</span></span><span style=display:flex><span>Status:          Bound
</span></span><span style=display:flex><span>Claim:           default/vcp-pvc-1
</span></span><span style=display:flex><span>Reclaim Policy:  Delete
</span></span><span style=display:flex><span>Access Modes:    RWO
</span></span><span style=display:flex><span>VolumeMode:      Filesystem
</span></span><span style=display:flex><span>Capacity:        1Gi
</span></span><span style=display:flex><span>Node Affinity:   &lt;none&gt;
</span></span><span style=display:flex><span>Message:         
</span></span><span style=display:flex><span>Source:
</span></span><span style=display:flex><span>    Type:               vSphereVolume <span style=color:#666>(</span>a Persistent Disk resource in vSphere<span style=color:#666>)</span>
</span></span><span style=display:flex><span>    VolumePath:         <span style=color:#666>[</span>vsanDatastore<span style=color:#666>]</span> d49c4a62-166f-ce12-c464-020077ba5d46/kubernetes-dynamic-pvc-74a498d6-3929-47e8-8c02-078c1ece4d78.vmdk
</span></span><span style=display:flex><span>    FSType:             ext4
</span></span><span style=display:flex><span>    StoragePolicyName:  vSAN Default Storage Policy
</span></span><span style=display:flex><span>Events:                 &lt;none&gt;
</span></span></code></pre></div><p>The finalizer <code>external-provisioner.volume.kubernetes.io/finalizer</code> is added for CSI volumes.
The following is an example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Name:            pvc-2f0bab97-85a8-4552-8044-eb8be45cf48d
</span></span><span style=display:flex><span>Labels:          &lt;none&gt;
</span></span><span style=display:flex><span>Annotations:     pv.kubernetes.io/provisioned-by: csi.vsphere.vmware.com
</span></span><span style=display:flex><span>Finalizers:      <span style=color:#666>[</span>kubernetes.io/pv-protection external-provisioner.volume.kubernetes.io/finalizer<span style=color:#666>]</span>
</span></span><span style=display:flex><span>StorageClass:    fast
</span></span><span style=display:flex><span>Status:          Bound
</span></span><span style=display:flex><span>Claim:           demo-app/nginx-logs
</span></span><span style=display:flex><span>Reclaim Policy:  Delete
</span></span><span style=display:flex><span>Access Modes:    RWO
</span></span><span style=display:flex><span>VolumeMode:      Filesystem
</span></span><span style=display:flex><span>Capacity:        200Mi
</span></span><span style=display:flex><span>Node Affinity:   &lt;none&gt;
</span></span><span style=display:flex><span>Message:         
</span></span><span style=display:flex><span>Source:
</span></span><span style=display:flex><span>    Type:              CSI <span style=color:#666>(</span>a Container Storage Interface <span style=color:#666>(</span>CSI<span style=color:#666>)</span> volume <span style=color:#a2f>source</span><span style=color:#666>)</span>
</span></span><span style=display:flex><span>    Driver:            csi.vsphere.vmware.com
</span></span><span style=display:flex><span>    FSType:            ext4
</span></span><span style=display:flex><span>    VolumeHandle:      44830fa8-79b4-406b-8b58-621ba25353fd
</span></span><span style=display:flex><span>    ReadOnly:          <span style=color:#a2f>false</span>
</span></span><span style=display:flex><span>    VolumeAttributes:      storage.kubernetes.io/csiProvisionerIdentity<span style=color:#666>=</span>1648442357185-8081-csi.vsphere.vmware.com
</span></span><span style=display:flex><span>                           <span style=color:#b8860b>type</span><span style=color:#666>=</span>vSphere CNS Block Volume
</span></span><span style=display:flex><span>Events:                &lt;none&gt;
</span></span></code></pre></div><p>When the <code>CSIMigration{provider}</code> feature flag is enabled for a specific in-tree volume plugin,
the <code>kubernetes.io/pv-controller</code> finalizer is replaced by the
<code>external-provisioner.volume.kubernetes.io/finalizer</code> finalizer.</p><h3 id=reserving-a-persistentvolume>Reserving a PersistentVolume</h3><p>The control plane can <a href=#binding>bind PersistentVolumeClaims to matching PersistentVolumes</a> in the
cluster. However, if you want a PVC to bind to a specific PV, you need to pre-bind them.</p><p>By specifying a PersistentVolume in a PersistentVolumeClaim, you declare a binding between that specific PV and PVC.
If the PersistentVolume exists and has not reserved PersistentVolumeClaims through its <code>claimRef</code> field, then the PersistentVolume and PersistentVolumeClaim will be bound.</p><p>The binding happens regardless of some volume matching criteria, including node affinity.
The control plane still checks that <a href=/docs/concepts/storage/storage-classes/>storage class</a>, access modes, and requested storage size are valid.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo-pvc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># Empty string must be explicitly set otherwise default StorageClass will be set</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeName</span>:<span style=color:#bbb> </span>foo-pv<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>This method does not guarantee any binding privileges to the PersistentVolume. If other PersistentVolumeClaims could use the PV that you specify, you first need to reserve that storage volume. Specify the relevant PersistentVolumeClaim in the <code>claimRef</code> field of the PV so that other PVCs can not bind to it.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo-pv<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>claimRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo-pvc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>This is useful if you want to consume PersistentVolumes that have their <code>claimPolicy</code> set
to <code>Retain</code>, including cases where you are reusing an existing PV.</p><h3 id=expanding-persistent-volumes-claims>Expanding Persistent Volumes Claims</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>Support for expanding PersistentVolumeClaims (PVCs) is enabled by default. You can expand
the following types of volumes:</p><ul><li>azureDisk</li><li>azureFile</li><li>awsElasticBlockStore</li><li>cinder (deprecated)</li><li><a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=csi>csi</a></li><li>flexVolume (deprecated)</li><li>gcePersistentDisk</li><li>glusterfs (deprecated)</li><li>rbd</li><li>portworxVolume</li></ul><p>You can only expand a PVC if its storage class's <code>allowVolumeExpansion</code> field is set to true.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-vol-default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>vendor-name.example/magicstorage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resturl</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;http://192.168.10.100:8080&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restuser</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>secretNamespace</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>allowVolumeExpansion</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>To request a larger volume for a PVC, edit the PVC object and specify a larger
size. This triggers expansion of the volume that backs the underlying PersistentVolume. A
new PersistentVolume is never created to satisfy the claim. Instead, an existing volume is resized.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> Directly editing the size of a PersistentVolume can prevent an automatic resize of that volume.
If you edit the capacity of a PersistentVolume, and then edit the <code>.spec</code> of a matching
PersistentVolumeClaim to make the size of the PersistentVolumeClaim match the PersistentVolume,
then no storage resize happens.
The Kubernetes control plane will see that the desired state of both resources matches,
conclude that the backing volume size has been manually
increased and that no resize is necessary.</div><h4 id=csi-volume-expansion>CSI Volume expansion</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>Support for expanding CSI volumes is enabled by default but it also requires a specific CSI driver to support volume expansion. Refer to documentation of the specific CSI driver for more information.</p><h4 id=resizing-a-volume-containing-a-file-system>Resizing a volume containing a file system</h4><p>You can only resize volumes containing a file system if the file system is XFS, Ext3, or Ext4.</p><p>When a volume contains a file system, the file system is only resized when a new Pod is using
the PersistentVolumeClaim in <code>ReadWrite</code> mode. File system expansion is either done when a Pod is starting up
or when a Pod is running and the underlying file system supports online expansion.</p><p>FlexVolumes (deprecated since Kubernetes v1.23) allow resize if the driver is configured with the
<code>RequiresFSResize</code> capability to <code>true</code>. The FlexVolume can be resized on Pod restart.</p><h4 id=resizing-an-in-use-persistentvolumeclaim>Resizing an in-use PersistentVolumeClaim</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>In this case, you don't need to delete and recreate a Pod or deployment that is using an existing PVC.
Any in-use PVC automatically becomes available to its Pod as soon as its file system has been expanded.
This feature has no effect on PVCs that are not in use by a Pod or deployment. You must create a Pod that
uses the PVC before the expansion can complete.</p><p>Similar to other volume types - FlexVolume volumes can also be expanded when in-use by a Pod.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> FlexVolume resize is possible only when the underlying driver supports resize.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Expanding EBS volumes is a time-consuming operation. Also, there is a per-volume quota of one modification every 6 hours.</div><h4 id=recovering-from-failure-when-expanding-volumes>Recovering from Failure when Expanding Volumes</h4><p>If a user specifies a new size that is too big to be satisfied by underlying storage system, expansion of PVC will be continuously retried until user or cluster administrator takes some action. This can be undesirable and hence Kubernetes provides following methods of recovering from such failures.</p><ul class="nav nav-tabs" id=recovery-methods role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#recovery-methods-0 role=tab aria-controls=recovery-methods-0 aria-selected=true>Manually with Cluster Administrator access</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#recovery-methods-1 role=tab aria-controls=recovery-methods-1>By requesting expansion to smaller size</a></li></ul><div class=tab-content id=recovery-methods><div id=recovery-methods-0 class="tab-pane show active" role=tabpanel aria-labelledby=recovery-methods-0><p><p>If expanding underlying storage fails, the cluster administrator can manually recover the Persistent Volume Claim (PVC) state and cancel the resize requests. Otherwise, the resize requests are continuously retried by the controller without administrator intervention.</p><ol><li>Mark the PersistentVolume(PV) that is bound to the PersistentVolumeClaim(PVC) with <code>Retain</code> reclaim policy.</li><li>Delete the PVC. Since PV has <code>Retain</code> reclaim policy - we will not lose any data when we recreate the PVC.</li><li>Delete the <code>claimRef</code> entry from PV specs, so as new PVC can bind to it. This should make the PV <code>Available</code>.</li><li>Re-create the PVC with smaller size than PV and set <code>volumeName</code> field of the PVC to the name of the PV. This should bind new PVC to existing PV.</li><li>Don't forget to restore the reclaim policy of the PV.</li></ol></div><div id=recovery-methods-1 class=tab-pane role=tabpanel aria-labelledby=recovery-methods-1><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Recovery from failing PVC expansion by users is available as an alpha feature since Kubernetes 1.23. The <code>RecoverVolumeExpansionFailure</code> feature must be enabled for this feature to work. Refer to the <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> documentation for more information.</div><p>If the feature gates <code>RecoverVolumeExpansionFailure</code> is
enabled in your cluster, and expansion has failed for a PVC, you can retry expansion with a
smaller size than the previously requested value. To request a new expansion attempt with a
smaller proposed size, edit <code>.spec.resources</code> for that PVC and choose a value that is less than the
value you previously tried.
This is useful if expansion to a higher value did not succeed because of capacity constraint.
If that has happened, or you suspect that it might have, you can retry expansion by specifying a
size that is within the capacity limits of underlying storage provider. You can monitor status of resize operation by watching <code>.status.resizeStatus</code> and events on the PVC.</p><p>Note that,
although you can specify a lower amount of storage than what was requested previously,
the new value must still be higher than <code>.status.capacity</code>.
Kubernetes does not support shrinking a PVC to less than its current size.</p></div></div><h2 id=types-of-persistent-volumes>Types of Persistent Volumes</h2><p>PersistentVolume types are implemented as plugins. Kubernetes currently supports the following plugins:</p><ul><li><a href=/docs/concepts/storage/volumes/#cephfs><code>cephfs</code></a> - CephFS volume</li><li><a href=/docs/concepts/storage/volumes/#csi><code>csi</code></a> - Container Storage Interface (CSI)</li><li><a href=/docs/concepts/storage/volumes/#fc><code>fc</code></a> - Fibre Channel (FC) storage</li><li><a href=/docs/concepts/storage/volumes/#hostpath><code>hostPath</code></a> - HostPath volume
(for single node testing only; WILL NOT WORK in a multi-node cluster;
consider using <code>local</code> volume instead)</li><li><a href=/docs/concepts/storage/volumes/#iscsi><code>iscsi</code></a> - iSCSI (SCSI over IP) storage</li><li><a href=/docs/concepts/storage/volumes/#local><code>local</code></a> - local storage devices
mounted on nodes.</li><li><a href=/docs/concepts/storage/volumes/#nfs><code>nfs</code></a> - Network File System (NFS) storage</li><li><a href=/docs/concepts/storage/volumes/#rbd><code>rbd</code></a> - Rados Block Device (RBD) volume</li></ul><p>The following types of PersistentVolume are deprecated. This means that support is still available but will be removed in a future Kubernetes release.</p><ul><li><a href=/docs/concepts/storage/volumes/#awselasticblockstore><code>awsElasticBlockStore</code></a> - AWS Elastic Block Store (EBS)
(<strong>deprecated</strong> in v1.17)</li><li><a href=/docs/concepts/storage/volumes/#azuredisk><code>azureDisk</code></a> - Azure Disk
(<strong>deprecated</strong> in v1.19)</li><li><a href=/docs/concepts/storage/volumes/#azurefile><code>azureFile</code></a> - Azure File
(<strong>deprecated</strong> in v1.21)</li><li><a href=/docs/concepts/storage/volumes/#cinder><code>cinder</code></a> - Cinder (OpenStack block storage)
(<strong>deprecated</strong> in v1.18)</li><li><a href=/docs/concepts/storage/volumes/#flexvolume><code>flexVolume</code></a> - FlexVolume
(<strong>deprecated</strong> in v1.23)</li><li><a href=/docs/concepts/storage/volumes/#gcepersistentdisk><code>gcePersistentDisk</code></a> - GCE Persistent Disk
(<strong>deprecated</strong> in v1.17)</li><li><a href=/docs/concepts/storage/volumes/#glusterfs><code>glusterfs</code></a> - Glusterfs volume
(<strong>deprecated</strong> in v1.25)</li><li><a href=/docs/concepts/storage/volumes/#portworxvolume><code>portworxVolume</code></a> - Portworx volume
(<strong>deprecated</strong> in v1.25)</li><li><a href=/docs/concepts/storage/volumes/#vspherevolume><code>vsphereVolume</code></a> - vSphere VMDK volume
(<strong>deprecated</strong> in v1.19)</li></ul><p>Older versions of Kubernetes also supported the following in-tree PersistentVolume types:</p><ul><li><code>photonPersistentDisk</code> - Photon controller persistent disk.
(<strong>not available</strong> starting v1.15)</li><li><a href=/docs/concepts/storage/volumes/#scaleio><code>scaleIO</code></a> - ScaleIO volume
(<strong>not available</strong> starting v1.21)</li><li><a href=/docs/concepts/storage/volumes/#flocker><code>flocker</code></a> - Flocker storage
(<strong>not available</strong> starting v1.25)</li><li><a href=/docs/concepts/storage/volumes/#quobyte><code>quobyte</code></a> - Quobyte volume
(<strong>not available</strong> starting v1.25)</li><li><a href=/docs/concepts/storage/volumes/#storageos><code>storageos</code></a> - StorageOS volume
(<strong>not available</strong> starting v1.25)</li></ul><h2 id=persistent-volumes>Persistent Volumes</h2><p>Each PV contains a spec and status, which is the specification and status of the volume.
The name of a PersistentVolume object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pv0003<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>capacity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>5Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Filesystem<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>persistentVolumeReclaimPolicy</span>:<span style=color:#bbb> </span>Recycle<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>mountOptions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- hard<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- nfsvers=4.1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nfs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/tmp<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>server</span>:<span style=color:#bbb> </span><span style=color:#666>172.17.0.2</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Helper programs relating to the volume type may be required for consumption of a PersistentVolume within a cluster. In this example, the PersistentVolume is of type NFS and the helper program /sbin/mount.nfs is required to support the mounting of NFS filesystems.</div><h3 id=capacity>Capacity</h3><p>Generally, a PV will have a specific storage capacity. This is set using the PV's <code>capacity</code> attribute. Read the glossary term <a href="/docs/reference/glossary/?all=true#term-quantity">Quantity</a> to understand the units expected by <code>capacity</code>.</p><p>Currently, storage size is the only resource that can be set or requested. Future attributes may include IOPS, throughput, etc.</p><h3 id=volume-mode>Volume Mode</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code></div><p>Kubernetes supports two <code>volumeModes</code> of PersistentVolumes: <code>Filesystem</code> and <code>Block</code>.</p><p><code>volumeMode</code> is an optional API parameter.
<code>Filesystem</code> is the default mode used when <code>volumeMode</code> parameter is omitted.</p><p>A volume with <code>volumeMode: Filesystem</code> is <em>mounted</em> into Pods into a directory. If the volume
is backed by a block device and the device is empty, Kubernetes creates a filesystem
on the device before mounting it for the first time.</p><p>You can set the value of <code>volumeMode</code> to <code>Block</code> to use a volume as a raw block device.
Such volume is presented into a Pod as a block device, without any filesystem on it.
This mode is useful to provide a Pod the fastest possible way to access a volume, without
any filesystem layer between the Pod and the volume. On the other hand, the application
running in the Pod must know how to handle a raw block device.
See <a href=#raw-block-volume-support>Raw Block Volume Support</a>
for an example on how to use a volume with <code>volumeMode: Block</code> in a Pod.</p><h3 id=access-modes>Access Modes</h3><p>A PersistentVolume can be mounted on a host in any way supported by the resource provider. As shown in the table below, providers will have different capabilities and each PV's access modes are set to the specific modes supported by that particular volume. For example, NFS can support multiple read/write clients, but a specific NFS PV might be exported on the server as read-only. Each PV gets its own set of access modes describing that specific PV's capabilities.</p><p>The access modes are:</p><dl><dt><code>ReadWriteOnce</code></dt><dd>the volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple pods to access the volume when the pods are running on the same node.</dd><dt><code>ReadOnlyMany</code></dt><dd>the volume can be mounted as read-only by many nodes.</dd><dt><code>ReadWriteMany</code></dt><dd>the volume can be mounted as read-write by many nodes.</dd><dt><code>ReadWriteOncePod</code></dt><dd>the volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to ensure that only one pod across whole cluster can read that PVC or write to it. This is only supported for CSI volumes and Kubernetes version 1.22+.</dd></dl><p>The blog article <a href=/blog/2021/09/13/read-write-once-pod-access-mode-alpha/>Introducing Single Pod Access Mode for PersistentVolumes</a> covers this in more detail.</p><p>In the CLI, the access modes are abbreviated to:</p><ul><li>RWO - ReadWriteOnce</li><li>ROX - ReadOnlyMany</li><li>RWX - ReadWriteMany</li><li>RWOP - ReadWriteOncePod</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes uses volume access modes to match PersistentVolumeClaims and PersistentVolumes.
In some cases, the volume access modes also constrain where the PersistentVolume can be mounted.
Volume access modes do <strong>not</strong> enforce write protection once the storage has been mounted.
Even if the access modes are specified as ReadWriteOnce, ReadOnlyMany, or ReadWriteMany, they don't set any constraints on the volume.
For example, even if a PersistentVolume is created as ReadOnlyMany, it is no guarantee that it will be read-only.
If the access modes are specified as ReadWriteOncePod, the volume is constrained and can be mounted on only a single Pod.</div><blockquote><p><strong>Important!</strong> A volume can only be mounted using one access mode at a time, even if it supports many. For example, a GCEPersistentDisk can be mounted as ReadWriteOnce by a single node or ReadOnlyMany by many nodes, but not at the same time.</p></blockquote><table><thead><tr><th style=text-align:left>Volume Plugin</th><th style=text-align:center>ReadWriteOnce</th><th style=text-align:center>ReadOnlyMany</th><th style=text-align:center>ReadWriteMany</th><th>ReadWriteOncePod</th></tr></thead><tbody><tr><td style=text-align:left>AWSElasticBlockStore</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td>-</td></tr><tr><td style=text-align:left>AzureFile</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td>-</td></tr><tr><td style=text-align:left>AzureDisk</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td>-</td></tr><tr><td style=text-align:left>CephFS</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td>-</td></tr><tr><td style=text-align:left>Cinder</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td style=text-align:center>(<a href=https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/cinder-csi-plugin/features.md#multi-attach-volumes>if multi-attach volumes are available</a>)</td><td>-</td></tr><tr><td style=text-align:left>CSI</td><td style=text-align:center>depends on the driver</td><td style=text-align:center>depends on the driver</td><td style=text-align:center>depends on the driver</td><td>depends on the driver</td></tr><tr><td style=text-align:left>FC</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td>-</td></tr><tr><td style=text-align:left>FlexVolume</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center>depends on the driver</td><td>-</td></tr><tr><td style=text-align:left>GCEPersistentDisk</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td>-</td></tr><tr><td style=text-align:left>Glusterfs</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td>-</td></tr><tr><td style=text-align:left>HostPath</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td>-</td></tr><tr><td style=text-align:left>iSCSI</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td>-</td></tr><tr><td style=text-align:left>NFS</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td>-</td></tr><tr><td style=text-align:left>RBD</td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td>-</td></tr><tr><td style=text-align:left>VsphereVolume</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td style=text-align:center>- (works when Pods are collocated)</td><td>-</td></tr><tr><td style=text-align:left>PortworxVolume</td><td style=text-align:center>✓</td><td style=text-align:center>-</td><td style=text-align:center>✓</td><td>-</td></tr></tbody></table><h3 id=class>Class</h3><p>A PV can have a class, which is specified by setting the
<code>storageClassName</code> attribute to the name of a
<a href=/docs/concepts/storage/storage-classes/>StorageClass</a>.
A PV of a particular class can only be bound to PVCs requesting
that class. A PV with no <code>storageClassName</code> has no class and can only be bound
to PVCs that request no particular class.</p><p>In the past, the annotation <code>volume.beta.kubernetes.io/storage-class</code> was used instead
of the <code>storageClassName</code> attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.</p><h3 id=reclaim-policy>Reclaim Policy</h3><p>Current reclaim policies are:</p><ul><li>Retain -- manual reclamation</li><li>Recycle -- basic scrub (<code>rm -rf /thevolume/*</code>)</li><li>Delete -- associated storage asset such as AWS EBS, GCE PD, Azure Disk, or OpenStack Cinder volume is deleted</li></ul><p>Currently, only NFS and HostPath support recycling. AWS EBS, GCE PD, Azure Disk, and Cinder volumes support deletion.</p><h3 id=mount-options>Mount Options</h3><p>A Kubernetes administrator can specify additional mount options for when a Persistent Volume is mounted on a node.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Not all Persistent Volume types support mount options.</div><p>The following volume types support mount options:</p><ul><li><code>awsElasticBlockStore</code></li><li><code>azureDisk</code></li><li><code>azureFile</code></li><li><code>cephfs</code></li><li><code>cinder</code> (<strong>deprecated</strong> in v1.18)</li><li><code>gcePersistentDisk</code></li><li><code>glusterfs</code> (<strong>deprecated</strong> in v1.25)</li><li><code>iscsi</code></li><li><code>nfs</code></li><li><code>rbd</code></li><li><code>vsphereVolume</code></li></ul><p>Mount options are not validated. If a mount option is invalid, the mount fails.</p><p>In the past, the annotation <code>volume.beta.kubernetes.io/mount-options</code> was used instead
of the <code>mountOptions</code> attribute. This annotation is still working; however,
it will become fully deprecated in a future Kubernetes release.</p><h3 id=node-affinity>Node Affinity</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> For most volume types, you do not need to set this field. It is automatically populated for <a href=/docs/concepts/storage/volumes/#awselasticblockstore>AWS EBS</a>, <a href=/docs/concepts/storage/volumes/#gcepersistentdisk>GCE PD</a> and <a href=/docs/concepts/storage/volumes/#azuredisk>Azure Disk</a> volume block types. You need to explicitly set this for <a href=/docs/concepts/storage/volumes/#local>local</a> volumes.</div><p>A PV can specify node affinity to define constraints that limit what nodes this volume can be accessed from. Pods that use a PV will only be scheduled to nodes that are selected by the node affinity. To specify node affinity, set <code>nodeAffinity</code> in the <code>.spec</code> of a PV. The <a href=/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/#PersistentVolumeSpec>PersistentVolume</a> API reference has more details on this field.</p><h3 id=phase>Phase</h3><p>A volume will be in one of the following phases:</p><ul><li>Available -- a free resource that is not yet bound to a claim</li><li>Bound -- the volume is bound to a claim</li><li>Released -- the claim has been deleted, but the resource is not yet reclaimed by the cluster</li><li>Failed -- the volume has failed its automatic reclamation</li></ul><p>The CLI will show the name of the PVC bound to the PV.</p><h2 id=persistentvolumeclaims>PersistentVolumeClaims</h2><p>Each PVC contains a spec and status, which is the specification and status of the claim.
The name of a PersistentVolumeClaim object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myclaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Filesystem<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>8Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>release</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;stable&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- {<span style=color:green;font-weight:700>key: environment, operator: In, values</span>:<span style=color:#bbb> </span>[dev]}<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=access-modes-1>Access Modes</h3><p>Claims use <a href=#access-modes>the same conventions as volumes</a> when requesting storage with specific access modes.</p><h3 id=volume-modes>Volume Modes</h3><p>Claims use <a href=#volume-mode>the same convention as volumes</a> to indicate the consumption of the volume as either a filesystem or block device.</p><h3 id=resources>Resources</h3><p>Claims, like Pods, can request specific quantities of a resource. In this case, the request is for storage. The same <a href=https://git.k8s.io/design-proposals-archive/scheduling/resources.md>resource model</a> applies to both volumes and claims.</p><h3 id=selector>Selector</h3><p>Claims can specify a <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>label selector</a> to further filter the set of volumes. Only the volumes whose labels match the selector can be bound to the claim. The selector can consist of two fields:</p><ul><li><code>matchLabels</code> - the volume must have a label with this value</li><li><code>matchExpressions</code> - a list of requirements made by specifying key, list of values, and operator that relates the key and values. Valid operators include In, NotIn, Exists, and DoesNotExist.</li></ul><p>All of the requirements, from both <code>matchLabels</code> and <code>matchExpressions</code>, are ANDed together – they must all be satisfied in order to match.</p><h3 id=class-1>Class</h3><p>A claim can request a particular class by specifying the name of a
<a href=/docs/concepts/storage/storage-classes/>StorageClass</a>
using the attribute <code>storageClassName</code>.
Only PVs of the requested class, ones with the same <code>storageClassName</code> as the PVC, can
be bound to the PVC.</p><p>PVCs don't necessarily have to request a class. A PVC with its <code>storageClassName</code> set
equal to <code>""</code> is always interpreted to be requesting a PV with no class, so it
can only be bound to PVs with no class (no annotation or one set equal to
<code>""</code>). A PVC with no <code>storageClassName</code> is not quite the same and is treated differently
by the cluster, depending on whether the
<a href=/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass><code>DefaultStorageClass</code> admission plugin</a>
is turned on.</p><ul><li>If the admission plugin is turned on, the administrator may specify a
default StorageClass. All PVCs that have no <code>storageClassName</code> can be bound only to
PVs of that default. Specifying a default StorageClass is done by setting the
annotation <code>storageclass.kubernetes.io/is-default-class</code> equal to <code>true</code> in
a StorageClass object. If the administrator does not specify a default, the
cluster responds to PVC creation as if the admission plugin were turned off. If
more than one default is specified, the admission plugin forbids the creation of
all PVCs.</li><li>If the admission plugin is turned off, there is no notion of a default
StorageClass. All PVCs that have <code>storageClassName</code> set to <code>""</code> can be
bound only to PVs that have <code>storageClassName</code> also set to <code>""</code>.
However, PVCs with missing <code>storageClassName</code> can be updated later once
default StorageClass becomes available. If the PVC gets updated it will no
longer bind to PVs that have <code>storageClassName</code> also set to <code>""</code>.</li></ul><p>See <a href=#retroactive-default-storageclass-assignment>retroactive default StorageClass assignment</a> for more details.</p><p>Depending on installation method, a default StorageClass may be deployed
to a Kubernetes cluster by addon manager during installation.</p><p>When a PVC specifies a <code>selector</code> in addition to requesting a StorageClass,
the requirements are ANDed together: only a PV of the requested class and with
the requested labels may be bound to the PVC.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Currently, a PVC with a non-empty <code>selector</code> can't have a PV dynamically provisioned for it.</div><p>In the past, the annotation <code>volume.beta.kubernetes.io/storage-class</code> was used instead
of <code>storageClassName</code> attribute. This annotation is still working; however,
it won't be supported in a future Kubernetes release.</p><h4 id=retroactive-default-storageclass-assignment>Retroactive default StorageClass assignment</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><p>You can create a PersistentVolumeClaim without specifying a <code>storageClassName</code> for the new PVC, and you can do so even when no default StorageClass exists in your cluster. In this case, the new PVC creates as you defined it, and the <code>storageClassName</code> of that PVC remains unset until default becomes available.
However, if you enable the <a href=/docs/reference/command-line-tools-reference/feature-gates/><code>RetroactiveDefaultStorageClass</code> feature gate</a> then Kubernetes behaves differently: existing PVCs without <code>storageClassName</code> update to use the new default StorageClass.</p><p>When a default StorageClass becomes available, the control plane identifies any existing PVCs without <code>storageClassName</code>. For the PVCs that either have an empty value for <code>storageClassName</code> or do not have this key, the control plane then updates those PVCs to set <code>storageClassName</code> to match the new default StorageClass. If you have an existing PVC where the <code>storageClassName</code> is <code>""</code>, and you configure a default StorageClass, then this PVC will not get updated.</p><p>In order to keep binding to PVs with <code>storageClassName</code> set to <code>""</code> (while a default StorageClass is present), you need to set the <code>storageClassName</code> of the associated PVC to <code>""</code>.</p><p>This behavior helps administrators change default StorageClass by removing the old one first and then creating or setting another one. This brief window while there is no default causes PVCs without <code>storageClassName</code> created at that time to not have any default, but due to the retroactive default StorageClass assignment this way of changing defaults is safe.</p><h2 id=claims-as-volumes>Claims As Volumes</h2><p>Pods access storage by using the claim as a volume. Claims must exist in the same namespace as the Pod using the claim. The cluster finds the claim in the Pod's namespace and uses it to get the PersistentVolume backing the claim. The volume is then mounted to the host and into the Pod.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myfrontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/var/www/html&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>myclaim<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=a-note-on-namespaces>A Note on Namespaces</h3><p>PersistentVolumes binds are exclusive, and since PersistentVolumeClaims are namespaced objects, mounting claims with "Many" modes (<code>ROX</code>, <code>RWX</code>) is only possible within one namespace.</p><h3 id=persistentvolumes-typed-hostpath>PersistentVolumes typed <code>hostPath</code></h3><p>A <code>hostPath</code> PersistentVolume uses a file or directory on the Node to emulate network-attached storage.
See <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume>an example of <code>hostPath</code> typed volume</a>.</p><h2 id=raw-block-volume-support>Raw Block Volume Support</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code></div><p>The following volume plugins support raw block volumes, including dynamic provisioning where
applicable:</p><ul><li>AWSElasticBlockStore</li><li>AzureDisk</li><li>CSI</li><li>FC (Fibre Channel)</li><li>GCEPersistentDisk</li><li>iSCSI</li><li>Local volume</li><li>OpenStack Cinder</li><li>RBD (Ceph Block Device)</li><li>VsphereVolume</li></ul><h3 id=persistent-volume-using-a-raw-block-volume>PersistentVolume using a Raw Block Volume</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>block-pv<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>capacity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Block<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>persistentVolumeReclaimPolicy</span>:<span style=color:#bbb> </span>Retain<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fc</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>targetWWNs</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;50060e801049cfd1&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>lun</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=persistent-volume-claim-requesting-a-raw-block-volume>PersistentVolumeClaim requesting a Raw Block Volume</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>block-pvc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeMode</span>:<span style=color:#bbb> </span>Block<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=pod-specification-adding-raw-block-device-path-in-container>Pod specification adding Raw Block Device path in container</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod-with-block-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fc-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>fedora:26<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;/bin/sh&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-c&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;tail -f /dev/null&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeDevices</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>devicePath</span>:<span style=color:#bbb> </span>/dev/xvda<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>data<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>block-pvc<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> When adding a raw block device for a Pod, you specify the device path in the container instead of a mount path.</div><h3 id=binding-block-volumes>Binding Block Volumes</h3><p>If a user requests a raw block volume by indicating this using the <code>volumeMode</code> field in the PersistentVolumeClaim spec, the binding rules differ slightly from previous releases that didn't consider this mode as part of the spec.
Listed is a table of possible combinations the user and admin might specify for requesting a raw block device. The table indicates if the volume will be bound or not given the combinations:
Volume binding matrix for statically provisioned volumes:</p><table><thead><tr><th>PV volumeMode</th><th style=text-align:center>PVC volumeMode</th><th style=text-align:right>Result</th></tr></thead><tbody><tr><td>unspecified</td><td style=text-align:center>unspecified</td><td style=text-align:right>BIND</td></tr><tr><td>unspecified</td><td style=text-align:center>Block</td><td style=text-align:right>NO BIND</td></tr><tr><td>unspecified</td><td style=text-align:center>Filesystem</td><td style=text-align:right>BIND</td></tr><tr><td>Block</td><td style=text-align:center>unspecified</td><td style=text-align:right>NO BIND</td></tr><tr><td>Block</td><td style=text-align:center>Block</td><td style=text-align:right>BIND</td></tr><tr><td>Block</td><td style=text-align:center>Filesystem</td><td style=text-align:right>NO BIND</td></tr><tr><td>Filesystem</td><td style=text-align:center>Filesystem</td><td style=text-align:right>BIND</td></tr><tr><td>Filesystem</td><td style=text-align:center>Block</td><td style=text-align:right>NO BIND</td></tr><tr><td>Filesystem</td><td style=text-align:center>unspecified</td><td style=text-align:right>BIND</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Only statically provisioned volumes are supported for alpha release. Administrators should take care to consider these values when working with raw block devices.</div><h2 id=volume-snapshot-and-restore-volume-from-snapshot-support>Volume Snapshot and Restore Volume from Snapshot Support</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>Volume snapshots only support the out-of-tree CSI volume plugins. For details, see <a href=/docs/concepts/storage/volume-snapshots/>Volume Snapshots</a>.
In-tree volume plugins are deprecated. You can read about the deprecated volume plugins in the <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md>Volume Plugin FAQ</a>.</p><h3 id=create-persistent-volume-claim-from-volume-snapshot>Create a PersistentVolumeClaim from a Volume Snapshot</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>restore-pvc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>csi-hostpath-sc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dataSource</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=volume-cloning>Volume Cloning</h2><p><a href=/docs/concepts/storage/volume-pvc-datasource/>Volume Cloning</a> only available for CSI volume plugins.</p><h3 id=create-persistent-volume-claim-from-an-existing-pvc>Create PersistentVolumeClaim from an existing PVC</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cloned-pvc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>my-csi-plugin<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dataSource</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>existing-src-pvc-name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=volume-populators-and-data-sources>Volume populators and data sources</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [beta]</code></div><p>Kubernetes supports custom volume populators.
To use custom volume populators, you must enable the <code>AnyVolumeDataSource</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> for
the kube-apiserver and kube-controller-manager.</p><p>Volume populators take advantage of a PVC spec field called <code>dataSourceRef</code>. Unlike the
<code>dataSource</code> field, which can only contain either a reference to another PersistentVolumeClaim
or to a VolumeSnapshot, the <code>dataSourceRef</code> field can contain a reference to any object in the
same namespace, except for core objects other than PVCs. For clusters that have the feature
gate enabled, use of the <code>dataSourceRef</code> is preferred over <code>dataSource</code>.</p><h2 id=data-source-references>Data source references</h2><p>The <code>dataSourceRef</code> field behaves almost the same as the <code>dataSource</code> field. If either one is
specified while the other is not, the API server will give both fields the same value. Neither
field can be changed after creation, and attempting to specify different values for the two
fields will result in a validation error. Therefore the two fields will always have the same
contents.</p><p>There are two differences between the <code>dataSourceRef</code> field and the <code>dataSource</code> field that
users should be aware of:</p><ul><li>The <code>dataSource</code> field ignores invalid values (as if the field was blank) while the
<code>dataSourceRef</code> field never ignores values and will cause an error if an invalid value is
used. Invalid values are any core object (objects with no apiGroup) except for PVCs.</li><li>The <code>dataSourceRef</code> field may contain different types of objects, while the <code>dataSource</code> field
only allows PVCs and VolumeSnapshots.</li></ul><p>Users should always use <code>dataSourceRef</code> on clusters that have the feature gate enabled, and
fall back to <code>dataSource</code> on clusters that do not. It is not necessary to look at both fields
under any circumstance. The duplicated values with slightly different semantics exist only for
backwards compatibility. In particular, a mixture of older and newer controllers are able to
interoperate because the fields are the same.</p><h3 id=using-volume-populators>Using volume populators</h3><p>Volume populators are <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controllers>controllers</a> that can
create non-empty volumes, where the contents of the volume are determined by a Custom Resource.
Users create a populated volume by referring to a Custom Resource using the <code>dataSourceRef</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>populated-pvc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dataSourceRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ExampleDataSource<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiGroup</span>:<span style=color:#bbb> </span>example.storage.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span></span></span></code></pre></div><p>Because volume populators are external components, attempts to create a PVC that uses one
can fail if not all the correct components are installed. External controllers should generate
events on the PVC to provide feedback on the status of the creation, including warnings if
the PVC cannot be created due to some missing component.</p><p>You can install the alpha <a href=https://github.com/kubernetes-csi/volume-data-source-validator>volume data source validator</a>
controller into your cluster. That controller generates warning Events on a PVC in the case that no populator
is registered to handle that kind of data source. When a suitable populator is installed for a PVC, it's the
responsibility of that populator controller to report Events that relate to volume creation and issues during
the process.</p><h2 id=writing-portable-configuration>Writing Portable Configuration</h2><p>If you're writing configuration templates or examples that run on a wide range of clusters
and need persistent storage, it is recommended that you use the following pattern:</p><ul><li>Include PersistentVolumeClaim objects in your bundle of config (alongside
Deployments, ConfigMaps, etc).</li><li>Do not include PersistentVolume objects in the config, since the user instantiating
the config may not have permission to create PersistentVolumes.</li><li>Give the user the option of providing a storage class name when instantiating
the template.<ul><li>If the user provides a storage class name, put that value into the
<code>persistentVolumeClaim.storageClassName</code> field.
This will cause the PVC to match the right storage
class if the cluster has StorageClasses enabled by the admin.</li><li>If the user does not provide a storage class name, leave the
<code>persistentVolumeClaim.storageClassName</code> field as nil. This will cause a
PV to be automatically provisioned for the user with the default StorageClass
in the cluster. Many cluster environments have a default StorageClass installed,
or administrators can create their own default StorageClass.</li></ul></li><li>In your tooling, watch for PVCs that are not getting bound after some time
and surface this to the user, as this may indicate that the cluster has no
dynamic storage support (in which case the user should create a matching PV)
or the cluster has no storage system (in which case the user cannot deploy
config requiring PVCs).</li></ul><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolume>Creating a PersistentVolume</a>.</li><li>Learn more about <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/#create-a-persistentvolumeclaim>Creating a PersistentVolumeClaim</a>.</li><li>Read the <a href=https://git.k8s.io/design-proposals-archive/storage/persistent-storage.md>Persistent Storage design document</a>.</li></ul><h3 id=reference>API references</h3><p>Read about the APIs described in this page:</p><ul><li><a href=/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/><code>PersistentVolume</code></a></li><li><a href=/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-claim-v1/><code>PersistentVolumeClaim</code></a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2db414b26d4daec3ebed19dd837830c3>7.3 - Projected Volumes</h1><p>This document describes <em>projected volumes</em> in Kubernetes. Familiarity with <a href=/docs/concepts/storage/volumes/>volumes</a> is suggested.</p><h2 id=introduction>Introduction</h2><p>A <code>projected</code> volume maps several existing volume sources into the same directory.</p><p>Currently, the following types of volume sources can be projected:</p><ul><li><a href=/docs/concepts/storage/volumes/#secret><code>secret</code></a></li><li><a href=/docs/concepts/storage/volumes/#downwardapi><code>downwardAPI</code></a></li><li><a href=/docs/concepts/storage/volumes/#configmap><code>configMap</code></a></li><li><a href=#serviceaccounttoken><code>serviceAccountToken</code></a></li></ul><p>All sources are required to be in the same namespace as the Pod. For more details,
see the <a href=https://git.k8s.io/design-proposals-archive/node/all-in-one-volume.md>all-in-one volume</a> design document.</p><h3 id=example-configuration-secret-downwardapi-configmap>Example configuration with a secret, a downwardAPI, and a configMap</h3><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-secret-downwardapi-configmap.yaml download=pods/storage/projected-secret-downwardapi-configmap.yaml><code>pods/storage/projected-secret-downwardapi-configmap.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-storage-projected-secret-downwardapi-configmap-yaml")' title="Copy pods/storage/projected-secret-downwardapi-configmap.yaml to clipboard"></img></div><div class=includecode id=pods-storage-projected-secret-downwardapi-configmap-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>volume-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>container-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>all-in-one<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/projected-volume&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>all-in-one<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>projected</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>sources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>downwardAPI</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;labels&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>fieldRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>fieldPath</span>:<span style=color:#bbb> </span>metadata.labels<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;cpu_limit&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>resourceFieldRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>containerName</span>:<span style=color:#bbb> </span>container-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>resource</span>:<span style=color:#bbb> </span>limits.cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myconfigmap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-config<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h3 id=example-configuration-secrets-nondefault-permission-mode>Example configuration: secrets with a non-default permission mode set</h3><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-secrets-nondefault-permission-mode.yaml download=pods/storage/projected-secrets-nondefault-permission-mode.yaml><code>pods/storage/projected-secrets-nondefault-permission-mode.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-storage-projected-secrets-nondefault-permission-mode-yaml")' title="Copy pods/storage/projected-secrets-nondefault-permission-mode.yaml to clipboard"></img></div><div class=includecode id=pods-storage-projected-secrets-nondefault-permission-mode-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>volume-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>container-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>all-in-one<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/projected-volume&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>all-in-one<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>projected</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>sources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>password<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-password<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>mode</span>:<span style=color:#bbb> </span><span style=color:#666>511</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Each projected volume source is listed in the spec under <code>sources</code>. The
parameters are nearly the same with two exceptions:</p><ul><li>For secrets, the <code>secretName</code> field has been changed to <code>name</code> to be consistent
with ConfigMap naming.</li><li>The <code>defaultMode</code> can only be specified at the projected level and not for each
volume source. However, as illustrated above, you can explicitly set the <code>mode</code>
for each individual projection.</li></ul><h2 id=serviceaccounttoken>serviceAccountToken projected volumes</h2><p>You can inject the token for the current <a href=/docs/reference/access-authn-authz/authentication/#service-account-tokens>service account</a>
into a Pod at a specified path. For example:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/storage/projected-service-account-token.yaml download=pods/storage/projected-service-account-token.yaml><code>pods/storage/projected-service-account-token.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-storage-projected-service-account-token-yaml")' title="Copy pods/storage/projected-service-account-token.yaml to clipboard"></img></div><div class=includecode id=pods-storage-projected-service-account-token-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>sa-token-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>container-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>token-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/service-account&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceAccountName</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>token-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>projected</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>sources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>serviceAccountToken</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>audience</span>:<span style=color:#bbb> </span>api<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>expirationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>3600</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>token<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>The example Pod has a projected volume containing the injected service account
token. Containers in this Pod can use that token to access the Kubernetes API
server, authenticating with the identity of <a href=/docs/tasks/configure-pod-container/configure-service-account/>the pod's ServiceAccount</a>.
The <code>audience</code> field contains the intended audience of the
token. A recipient of the token must identify itself with an identifier specified
in the audience of the token, and otherwise should reject the token. This field
is optional and it defaults to the identifier of the API server.</p><p>The <code>expirationSeconds</code> is the expected duration of validity of the service account
token. It defaults to 1 hour and must be at least 10 minutes (600 seconds). An administrator
can also limit its maximum value by specifying the <code>--service-account-max-token-expiration</code>
option for the API server. The <code>path</code> field specifies a relative path to the mount point
of the projected volume.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A container using a projected volume source as a <a href=/docs/concepts/storage/volumes/#using-subpath><code>subPath</code></a>
volume mount will not receive updates for those volume sources.</div><h2 id=securitycontext-interactions>SecurityContext interactions</h2><p>The <a href=https://git.k8s.io/enhancements/keps/sig-storage/2451-service-account-token-volumes#proposal>proposal</a> for file permission handling in projected service account volume enhancement introduced the projected files having the correct owner permissions set.</p><h3 id=linux>Linux</h3><p>In Linux pods that have a projected volume and <code>RunAsUser</code> set in the Pod
<a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context><code>SecurityContext</code></a>,
the projected files have the correct ownership set including container user
ownership.</p><p>When all containers in a pod have the same <code>runAsUser</code> set in their
<a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context><code>PodSecurityContext</code></a>
or container
<a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#security-context-1><code>SecurityContext</code></a>,
then the kubelet ensures that the contents of the <code>serviceAccountToken</code> volume are owned by that user,
and the token file has its permission mode set to <code>0600</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p><a class=glossary-tooltip title='A type of container type that you can temporarily run inside a Pod' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ephemeral-containers/ target=_blank aria-label='Ephemeral containers'>Ephemeral containers</a>
added to a Pod after it is created do <em>not</em> change volume permissions that were
set when the pod was created.</p><p>If a Pod's <code>serviceAccountToken</code> volume permissions were set to <code>0600</code> because
all other containers in the Pod have the same <code>runAsUser</code>, ephemeral
containers must use the same <code>runAsUser</code> to be able to read the token.</p></div><h3 id=windows>Windows</h3><p>In Windows pods that have a projected volume and <code>RunAsUsername</code> set in the
Pod <code>SecurityContext</code>, the ownership is not enforced due to the way user
accounts are managed in Windows. Windows stores and manages local user and group
accounts in a database file called Security Account Manager (SAM). Each
container maintains its own instance of the SAM database, to which the host has
no visibility into while the container is running. Windows containers are
designed to run the user mode portion of the OS in isolation from the host,
hence the maintenance of a virtual SAM database. As a result, the kubelet running
on the host does not have the ability to dynamically configure host file
ownership for virtualized container accounts. It is recommended that if files on
the host machine are to be shared with the container then they should be placed
into their own volume mount outside of <code>C:\</code>.</p><p>By default, the projected files will have the following ownership as shown for
an example projected volume file:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-powershell data-lang=powershell><span style=display:flex><span><span style=color:#a2f>PS </span>C:\&gt; <span style=color:#a2f>Get-Acl</span> C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.<span style=color:#666>318230061</span>\ca.crt | <span style=color:#a2f>Format-List</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Path   <span>:</span> Microsoft.PowerShell.Core\FileSystem::C:\var\run\secrets\kubernetes.io\serviceaccount\..2021_08_31_22_22_18.<span style=color:#666>318230061</span>\ca.crt
</span></span><span style=display:flex><span>Owner  <span>:</span> BUILTIN\Administrators
</span></span><span style=display:flex><span><span style=color:#a2f>Group </span> <span>:</span> NT AUTHORITY\SYSTEM
</span></span><span style=display:flex><span>Access <span>:</span> NT AUTHORITY\SYSTEM Allow  FullControl
</span></span><span style=display:flex><span>         BUILTIN\Administrators Allow  FullControl
</span></span><span style=display:flex><span>         BUILTIN\Users Allow  ReadAndExecute, Synchronize
</span></span><span style=display:flex><span>Audit  <span>:</span>
</span></span><span style=display:flex><span>Sddl   <span>:</span> O:BAG<span>:</span>SYD<span>:</span>AI(A;ID;FA;;;SY)(A;ID;FA;;;BA)(A;ID;0x1200a9;;;BU)
</span></span></code></pre></div><p>This implies all administrator users like <code>ContainerAdministrator</code> will have
read, write and execute access while, non-administrator users will have read and
execute access.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>In general, granting the container access to the host is discouraged as it can
open the door for potential security exploits.</p><p>Creating a Windows Pod with <code>RunAsUser</code> in it's <code>SecurityContext</code> will result in
the Pod being stuck at <code>ContainerCreating</code> forever. So it is advised to not use
the Linux only <code>RunAsUser</code> option with Windows Pods.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-df33eab51202c17bb0fe551d1d5cc5d2>7.4 - Ephemeral Volumes</h1><p>This document describes <em>ephemeral volumes</em> in Kubernetes. Familiarity
with <a href=/docs/concepts/storage/volumes/>volumes</a> is suggested, in
particular PersistentVolumeClaim and PersistentVolume.</p><p>Some application need additional storage but don't care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance.</p><p>Other applications expect some read-only input data to be present in
files, like configuration data or secret keys.</p><p><em>Ephemeral volumes</em> are designed for these use cases. Because volumes
follow the Pod's lifetime and get created and deleted along with the
Pod, Pods can be stopped and restarted without being limited to where
some persistent volume is available.</p><p>Ephemeral volumes are specified <em>inline</em> in the Pod spec, which
simplifies application deployment and management.</p><h3 id=types-of-ephemeral-volumes>Types of ephemeral volumes</h3><p>Kubernetes supports several different kinds of ephemeral volumes for
different purposes:</p><ul><li><a href=/docs/concepts/storage/volumes/#emptydir>emptyDir</a>: empty at Pod startup,
with storage coming locally from the kubelet base directory (usually
the root disk) or RAM</li><li><a href=/docs/concepts/storage/volumes/#configmap>configMap</a>,
<a href=/docs/concepts/storage/volumes/#downwardapi>downwardAPI</a>,
<a href=/docs/concepts/storage/volumes/#secret>secret</a>: inject different
kinds of Kubernetes data into a Pod</li><li><a href=#csi-ephemeral-volumes>CSI ephemeral volumes</a>:
similar to the previous volume kinds, but provided by special
<a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>CSI drivers</a>
which specifically <a href=https://kubernetes-csi.github.io/docs/drivers.html>support this feature</a></li><li><a href=#generic-ephemeral-volumes>generic ephemeral volumes</a>, which
can be provided by all storage drivers that also support persistent volumes</li></ul><p><code>emptyDir</code>, <code>configMap</code>, <code>downwardAPI</code>, <code>secret</code> are provided as
<a href=/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage>local ephemeral
storage</a>.
They are managed by kubelet on each node.</p><p>CSI ephemeral volumes <em>must</em> be provided by third-party CSI storage
drivers.</p><p>Generic ephemeral volumes <em>can</em> be provided by third-party CSI storage
drivers, but also by any other storage driver that supports dynamic
provisioning. Some CSI drivers are written specifically for CSI
ephemeral volumes and do not support dynamic provisioning: those then
cannot be used for generic ephemeral volumes.</p><p>The advantage of using third-party drivers is that they can offer
functionality that Kubernetes itself does not support, for example
storage with different performance characteristics than the disk that
is managed by kubelet, or injecting different data.</p><h3 id=csi-ephemeral-volumes>CSI ephemeral volumes</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> CSI ephemeral volumes are only supported by a subset of CSI drivers.
The Kubernetes CSI <a href=https://kubernetes-csi.github.io/docs/drivers.html>Drivers list</a>
shows which drivers support ephemeral volumes.</div><p>Conceptually, CSI ephemeral volumes are similar to <code>configMap</code>,
<code>downwardAPI</code> and <code>secret</code> volume types: the storage is managed locally on each
node and is created together with other local resources after a Pod has been
scheduled onto a node. Kubernetes has no concept of rescheduling Pods
anymore at this stage. Volume creation has to be unlikely to fail,
otherwise Pod startup gets stuck. In particular, <a href=/docs/concepts/storage/storage-capacity/>storage capacity
aware Pod scheduling</a> is <em>not</em>
supported for these volumes. They are currently also not covered by
the storage resource usage limits of a Pod, because that is something
that kubelet can only enforce for storage that it manages itself.</p><p>Here's an example manifest for a Pod that uses CSI ephemeral storage:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-csi-app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/data&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-csi-inline-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;sleep&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;1000000&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-csi-inline-vol<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>csi</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>inline.storage.kubernetes.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeAttributes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span></code></pre></div><p>The <code>volumeAttributes</code> determine what volume is prepared by the
driver. These attributes are specific to each driver and not
standardized. See the documentation of each CSI driver for further
instructions.</p><h3 id=csi-driver-restrictions>CSI driver restrictions</h3><p>CSI ephemeral volumes allow users to provide <code>volumeAttributes</code>
directly to the CSI driver as part of the Pod spec. A CSI driver
allowing <code>volumeAttributes</code> that are typically restricted to
administrators is NOT suitable for use in an inline ephemeral volume.
For example, parameters that are normally defined in the StorageClass
should not be exposed to users through the use of inline ephemeral volumes.</p><p>Cluster administrators who need to restrict the CSI drivers that are
allowed to be used as inline volumes within a Pod spec may do so by:</p><ul><li>Removing <code>Ephemeral</code> from <code>volumeLifecycleModes</code> in the CSIDriver spec, which prevents the
driver from being used as an inline ephemeral volume.</li><li>Using an <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/>admission webhook</a>
to restrict how this driver is used.</li></ul><h3 id=generic-ephemeral-volumes>Generic ephemeral volumes</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code></div><p>Generic ephemeral volumes are similar to <code>emptyDir</code> volumes in the
sense that they provide a per-pod directory for scratch data that is
usually empty after provisioning. But they may also have additional
features:</p><ul><li>Storage can be local or network-attached.</li><li>Volumes can have a fixed size that Pods are not able to exceed.</li><li>Volumes may have some initial data, depending on the driver and
parameters.</li><li>Typical operations on volumes are supported assuming that the driver
supports them, including
<a href=/docs/concepts/storage/volume-snapshots/>snapshotting</a>,
<a href=/docs/concepts/storage/volume-pvc-datasource/>cloning</a>,
<a href=/docs/concepts/storage/persistent-volumes/#expanding-persistent-volumes-claims>resizing</a>,
and <a href=/docs/concepts/storage/storage-capacity/>storage capacity tracking</a>.</li></ul><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/scratch&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>scratch-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;sleep&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;1000000&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>scratch-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ephemeral</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeClaimTemplate</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>my-frontend-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;ReadWriteOnce&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;scratch-storage-class&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=lifecycle-and-persistentvolumeclaim>Lifecycle and PersistentVolumeClaim</h3><p>The key design idea is that the
<a href=/docs/reference/generated/kubernetes-api/v1.25/#ephemeralvolumesource-v1alpha1-core>parameters for a volume claim</a>
are allowed inside a volume source of the Pod. Labels, annotations and
the whole set of fields for a PersistentVolumeClaim are supported. When such a Pod gets
created, the ephemeral volume controller then creates an actual PersistentVolumeClaim
object in the same namespace as the Pod and ensures that the PersistentVolumeClaim
gets deleted when the Pod gets deleted.</p><p>That triggers volume binding and/or provisioning, either immediately if
the <a class=glossary-tooltip title='A StorageClass provides a way for administrators to describe different available storage types.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/storage-classes target=_blank aria-label=StorageClass>StorageClass</a> uses immediate volume binding or when the Pod is
tentatively scheduled onto a node (<code>WaitForFirstConsumer</code> volume
binding mode). The latter is recommended for generic ephemeral volumes
because then the scheduler is free to choose a suitable node for
the Pod. With immediate binding, the scheduler is forced to select a node that has
access to the volume once it is available.</p><p>In terms of <a href=/docs/concepts/architecture/garbage-collection/#owners-dependents>resource ownership</a>,
a Pod that has generic ephemeral storage is the owner of the PersistentVolumeClaim(s)
that provide that ephemeral storage. When the Pod is deleted,
the Kubernetes garbage collector deletes the PVC, which then usually
triggers deletion of the volume because the default reclaim policy of
storage classes is to delete volumes. You can create quasi-ephemeral local storage
using a StorageClass with a reclaim policy of <code>retain</code>: the storage outlives the Pod,
and in this case you need to ensure that volume clean up happens separately.</p><p>While these PVCs exist, they can be used like any other PVC. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.</p><h3 id=persistentvolumeclaim-naming>PersistentVolumeClaim naming</h3><p>Naming of the automatically created PVCs is deterministic: the name is
a combination of Pod name and volume name, with a hyphen (<code>-</code>) in the
middle. In the example above, the PVC name will be
<code>my-app-scratch-volume</code>. This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known.</p><p>The deterministic naming also introduces a potential conflict between different
Pods (a Pod "pod-a" with volume "scratch" and another Pod with name
"pod" and volume "a-scratch" both end up with the same PVC name
"pod-a-scratch") and between Pods and manually created PVCs.</p><p>Such conflicts are detected: a PVC is only used for an ephemeral
volume if it was created for the Pod. This check is based on the
ownership relationship. An existing PVC is not overwritten or
modified. But this does not resolve the conflict because without the
right PVC, the Pod cannot start.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Take care when naming Pods and volumes inside the
same namespace, so that these conflicts can't occur.</div><h3 id=security>Security</h3><p>Enabling the GenericEphemeralVolume feature allows users to create
PVCs indirectly if they can create Pods, even if they do not have
permission to create PVCs directly. Cluster administrators must be
aware of this. If this does not fit their security model, they should
use an <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/>admission webhook</a>
that rejects objects like Pods that have a generic ephemeral volume.</p><p>The normal <a href=/docs/concepts/policy/resource-quotas/#storage-resource-quota>namespace quota for PVCs</a>
still applies, so even if users are allowed to use this new mechanism, they cannot use
it to circumvent other policies.</p><h2 id=what-s-next>What's next</h2><h3 id=ephemeral-volumes-managed-by-kubelet>Ephemeral volumes managed by kubelet</h3><p>See <a href=/docs/concepts/configuration/manage-resources-containers/#local-ephemeral-storage>local ephemeral storage</a>.</p><h3 id=csi-ephemeral-volumes-1>CSI ephemeral volumes</h3><ul><li>For more information on the design, see the
<a href=https://github.com/kubernetes/enhancements/blob/ad6021b3d61a49040a3f835e12c8bb5424db2bbb/keps/sig-storage/20190122-csi-inline-volumes.md>Ephemeral Inline CSI volumes KEP</a>.</li><li>For more information on further development of this feature, see the
<a href=https://github.com/kubernetes/enhancements/issues/596>enhancement tracking issue #596</a>.</li></ul><h3 id=generic-ephemeral-volumes-1>Generic ephemeral volumes</h3><ul><li>For more information on the design, see the
<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1698-generic-ephemeral-volumes/README.md>Generic ephemeral inline volumes KEP</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-f0276d05eef111249272a1c932a91e2c>7.5 - Storage Classes</h1><p>This document describes the concept of a StorageClass in Kubernetes. Familiarity
with <a href=/docs/concepts/storage/volumes/>volumes</a> and
<a href=/docs/concepts/storage/persistent-volumes>persistent volumes</a> is suggested.</p><h2 id=introduction>Introduction</h2><p>A StorageClass provides a way for administrators to describe the "classes" of
storage they offer. Different classes might map to quality-of-service levels,
or to backup policies, or to arbitrary policies determined by the cluster
administrators. Kubernetes itself is unopinionated about what classes
represent. This concept is sometimes called "profiles" in other storage
systems.</p><h2 id=the-storageclass-resource>The StorageClass Resource</h2><p>Each StorageClass contains the fields <code>provisioner</code>, <code>parameters</code>, and
<code>reclaimPolicy</code>, which are used when a PersistentVolume belonging to the
class needs to be dynamically provisioned.</p><p>The name of a StorageClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating StorageClass objects, and the objects cannot
be updated once they are created.</p><p>Administrators can specify a default StorageClass only for PVCs that don't
request any particular class to bind to: see the
<a href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>PersistentVolumeClaim section</a>
for details.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>standard<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/aws-ebs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>gp2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>reclaimPolicy</span>:<span style=color:#bbb> </span>Retain<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>allowVolumeExpansion</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>mountOptions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- debug<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>volumeBindingMode</span>:<span style=color:#bbb> </span>Immediate<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=provisioner>Provisioner</h3><p>Each StorageClass has a provisioner that determines what volume plugin is used
for provisioning PVs. This field must be specified.</p><table><thead><tr><th style=text-align:left>Volume Plugin</th><th style=text-align:center>Internal Provisioner</th><th style=text-align:center>Config Example</th></tr></thead><tbody><tr><td style=text-align:left>AWSElasticBlockStore</td><td style=text-align:center>✓</td><td style=text-align:center><a href=#aws-ebs>AWS EBS</a></td></tr><tr><td style=text-align:left>AzureFile</td><td style=text-align:center>✓</td><td style=text-align:center><a href=#azure-file>Azure File</a></td></tr><tr><td style=text-align:left>AzureDisk</td><td style=text-align:center>✓</td><td style=text-align:center><a href=#azure-disk>Azure Disk</a></td></tr><tr><td style=text-align:left>CephFS</td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>Cinder</td><td style=text-align:center>✓</td><td style=text-align:center><a href=#openstack-cinder>OpenStack Cinder</a></td></tr><tr><td style=text-align:left>FC</td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>FlexVolume</td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>GCEPersistentDisk</td><td style=text-align:center>✓</td><td style=text-align:center><a href=#gce-pd>GCE PD</a></td></tr><tr><td style=text-align:left>Glusterfs</td><td style=text-align:center>✓</td><td style=text-align:center><a href=#glusterfs>Glusterfs</a></td></tr><tr><td style=text-align:left>iSCSI</td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>NFS</td><td style=text-align:center>-</td><td style=text-align:center><a href=#nfs>NFS</a></td></tr><tr><td style=text-align:left>RBD</td><td style=text-align:center>✓</td><td style=text-align:center><a href=#ceph-rbd>Ceph RBD</a></td></tr><tr><td style=text-align:left>VsphereVolume</td><td style=text-align:center>✓</td><td style=text-align:center><a href=#vsphere>vSphere</a></td></tr><tr><td style=text-align:left>PortworxVolume</td><td style=text-align:center>✓</td><td style=text-align:center><a href=#portworx-volume>Portworx Volume</a></td></tr><tr><td style=text-align:left>Local</td><td style=text-align:center>-</td><td style=text-align:center><a href=#local>Local</a></td></tr></tbody></table><p>You are not restricted to specifying the "internal" provisioners
listed here (whose names are prefixed with "kubernetes.io" and shipped
alongside Kubernetes). You can also run and specify external provisioners,
which are independent programs that follow a <a href=https://git.k8s.io/design-proposals-archive/storage/volume-provisioning.md>specification</a>
defined by Kubernetes. Authors of external provisioners have full discretion
over where their code lives, how the provisioner is shipped, how it needs to be
run, what volume plugin it uses (including Flex), etc. The repository
<a href=https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner>kubernetes-sigs/sig-storage-lib-external-provisioner</a>
houses a library for writing external provisioners that implements the bulk of
the specification. Some external provisioners are listed under the repository
<a href=https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner>kubernetes-sigs/sig-storage-lib-external-provisioner</a>.</p><p>For example, NFS doesn't provide an internal provisioner, but an external
provisioner can be used. There are also cases when 3rd party storage
vendors provide their own external provisioner.</p><h3 id=reclaim-policy>Reclaim Policy</h3><p>PersistentVolumes that are dynamically created by a StorageClass will have the
reclaim policy specified in the <code>reclaimPolicy</code> field of the class, which can be
either <code>Delete</code> or <code>Retain</code>. If no <code>reclaimPolicy</code> is specified when a
StorageClass object is created, it will default to <code>Delete</code>.</p><p>PersistentVolumes that are created manually and managed via a StorageClass will have
whatever reclaim policy they were assigned at creation.</p><h3 id=allow-volume-expansion>Allow Volume Expansion</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.11 [beta]</code></div><p>PersistentVolumes can be configured to be expandable. This feature when set to <code>true</code>,
allows the users to resize the volume by editing the corresponding PVC object.</p><p>The following types of volumes support volume expansion, when the underlying
StorageClass has the field <code>allowVolumeExpansion</code> set to true.</p><table><caption style=display:none>Table of Volume types and the version of Kubernetes they require</caption><thead><tr><th style=text-align:left>Volume type</th><th style=text-align:left>Required Kubernetes version</th></tr></thead><tbody><tr><td style=text-align:left>gcePersistentDisk</td><td style=text-align:left>1.11</td></tr><tr><td style=text-align:left>awsElasticBlockStore</td><td style=text-align:left>1.11</td></tr><tr><td style=text-align:left>Cinder</td><td style=text-align:left>1.11</td></tr><tr><td style=text-align:left>glusterfs</td><td style=text-align:left>1.11</td></tr><tr><td style=text-align:left>rbd</td><td style=text-align:left>1.11</td></tr><tr><td style=text-align:left>Azure File</td><td style=text-align:left>1.11</td></tr><tr><td style=text-align:left>Azure Disk</td><td style=text-align:left>1.11</td></tr><tr><td style=text-align:left>Portworx</td><td style=text-align:left>1.11</td></tr><tr><td style=text-align:left>FlexVolume</td><td style=text-align:left>1.13</td></tr><tr><td style=text-align:left>CSI</td><td style=text-align:left>1.14 (alpha), 1.16 (beta)</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You can only use the volume expansion feature to grow a Volume, not to shrink it.</div><h3 id=mount-options>Mount Options</h3><p>PersistentVolumes that are dynamically created by a StorageClass will have the
mount options specified in the <code>mountOptions</code> field of the class.</p><p>If the volume plugin does not support mount options but mount options are
specified, provisioning will fail. Mount options are not validated on either
the class or PV. If a mount option is invalid, the PV mount fails.</p><h3 id=volume-binding-mode>Volume Binding Mode</h3><p>The <code>volumeBindingMode</code> field controls when <a href=/docs/concepts/storage/persistent-volumes/#provisioning>volume binding and dynamic
provisioning</a> should occur. When unset, "Immediate" mode is used by default.</p><p>The <code>Immediate</code> mode indicates that volume binding and dynamic
provisioning occurs once the PersistentVolumeClaim is created. For storage
backends that are topology-constrained and not globally accessible from all Nodes
in the cluster, PersistentVolumes will be bound or provisioned without knowledge of the Pod's scheduling
requirements. This may result in unschedulable Pods.</p><p>A cluster administrator can address this issue by specifying the <code>WaitForFirstConsumer</code> mode which
will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.
PersistentVolumes will be selected or provisioned conforming to the topology that is
specified by the Pod's scheduling constraints. These include, but are not limited to, <a href=/docs/concepts/configuration/manage-resources-containers/>resource
requirements</a>,
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector>node selectors</a>,
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>pod affinity and
anti-affinity</a>,
and <a href=/docs/concepts/scheduling-eviction/taint-and-toleration>taints and tolerations</a>.</p><p>The following plugins support <code>WaitForFirstConsumer</code> with dynamic provisioning:</p><ul><li><a href=#aws-ebs>AWSElasticBlockStore</a></li><li><a href=#gce-pd>GCEPersistentDisk</a></li><li><a href=#azure-disk>AzureDisk</a></li></ul><p>The following plugins support <code>WaitForFirstConsumer</code> with pre-created PersistentVolume binding:</p><ul><li>All of the above</li><li><a href=#local>Local</a></li></ul><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code></div><a href=/docs/concepts/storage/volumes/#csi>CSI volumes</a> are also supported with dynamic provisioning
and pre-created PVs, but you'll need to look at the documentation for a specific CSI driver
to see its supported topology keys and examples.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>If you choose to use <code>WaitForFirstConsumer</code>, do not use <code>nodeName</code> in the Pod spec
to specify node affinity. If <code>nodeName</code> is used in this case, the scheduler will be bypassed and PVC will remain in <code>pending</code> state.</p><p>Instead, you can use node selector for hostname in this case as shown below.</p></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>task-pv-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/hostname</span>:<span style=color:#bbb> </span>kube-01<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>task-pv-storage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>persistentVolumeClaim</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>claimName</span>:<span style=color:#bbb> </span>task-pv-claim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>task-pv-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;http-server&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/usr/share/nginx/html&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>task-pv-storage<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=allowed-topologies>Allowed Topologies</h3><p>When a cluster operator specifies the <code>WaitForFirstConsumer</code> volume binding mode, it is no longer necessary
to restrict provisioning to specific topologies in most situations. However,
if still required, <code>allowedTopologies</code> can be specified.</p><p>This example demonstrates how to restrict the topology of provisioned volumes to specific
zones and should be used as a replacement for the <code>zone</code> and <code>zones</code> parameters for the
supported plugins.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>standard<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/gce-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>pd-standard<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>volumeBindingMode</span>:<span style=color:#bbb> </span>WaitForFirstConsumer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>allowedTopologies</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>matchLabelExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>failure-domain.beta.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- us-central-1a<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- us-central-1b<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=parameters>Parameters</h2><p>Storage Classes have parameters that describe volumes belonging to the storage
class. Different parameters may be accepted depending on the <code>provisioner</code>. For
example, the value <code>io1</code>, for the parameter <code>type</code>, and the parameter
<code>iopsPerGB</code> are specific to EBS. When a parameter is omitted, some default is
used.</p><p>There can be at most 512 parameters defined for a StorageClass.
The total length of the parameters object including its keys and values cannot
exceed 256 KiB.</p><h3 id=aws-ebs>AWS EBS</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/aws-ebs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>io1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>iopsPerGB</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><code>type</code>: <code>io1</code>, <code>gp2</code>, <code>sc1</code>, <code>st1</code>. See
<a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>AWS docs</a>
for details. Default: <code>gp2</code>.</li><li><code>zone</code> (Deprecated): AWS zone. If neither <code>zone</code> nor <code>zones</code> is specified, volumes are
generally round-robin-ed across all active zones where Kubernetes cluster
has a node. <code>zone</code> and <code>zones</code> parameters must not be used at the same time.</li><li><code>zones</code> (Deprecated): A comma separated list of AWS zone(s). If neither <code>zone</code> nor <code>zones</code>
is specified, volumes are generally round-robin-ed across all active zones
where Kubernetes cluster has a node. <code>zone</code> and <code>zones</code> parameters must not
be used at the same time.</li><li><code>iopsPerGB</code>: only for <code>io1</code> volumes. I/O operations per second per GiB. AWS
volume plugin multiplies this with size of requested volume to compute IOPS
of the volume and caps it at 20 000 IOPS (maximum supported by AWS, see
<a href=https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html>AWS docs</a>).
A string is expected here, i.e. <code>"10"</code>, not <code>10</code>.</li><li><code>fsType</code>: fsType that is supported by kubernetes. Default: <code>"ext4"</code>.</li><li><code>encrypted</code>: denotes whether the EBS volume should be encrypted or not.
Valid values are <code>"true"</code> or <code>"false"</code>. A string is expected here,
i.e. <code>"true"</code>, not <code>true</code>.</li><li><code>kmsKeyId</code>: optional. The full Amazon Resource Name of the key to use when
encrypting the volume. If none is supplied but <code>encrypted</code> is true, a key is
generated by AWS. See AWS docs for valid ARN value.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> <code>zone</code> and <code>zones</code> parameters are deprecated and replaced with
<a href=#allowed-topologies>allowedTopologies</a></div><h3 id=gce-pd>GCE PD</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/gce-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>pd-standard<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fstype</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replication-type</span>:<span style=color:#bbb> </span>none<span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><p><code>type</code>: <code>pd-standard</code> or <code>pd-ssd</code>. Default: <code>pd-standard</code></p></li><li><p><code>zone</code> (Deprecated): GCE zone. If neither <code>zone</code> nor <code>zones</code> is specified, volumes are
generally round-robin-ed across all active zones where Kubernetes cluster has
a node. <code>zone</code> and <code>zones</code> parameters must not be used at the same time.</p></li><li><p><code>zones</code> (Deprecated): A comma separated list of GCE zone(s). If neither <code>zone</code> nor <code>zones</code>
is specified, volumes are generally round-robin-ed across all active zones
where Kubernetes cluster has a node. <code>zone</code> and <code>zones</code> parameters must not
be used at the same time.</p></li><li><p><code>fstype</code>: <code>ext4</code> or <code>xfs</code>. Default: <code>ext4</code>. The defined filesystem type must be supported by the host operating system.</p></li><li><p><code>replication-type</code>: <code>none</code> or <code>regional-pd</code>. Default: <code>none</code>.</p></li></ul><p>If <code>replication-type</code> is set to <code>none</code>, a regular (zonal) PD will be provisioned.</p><p>If <code>replication-type</code> is set to <code>regional-pd</code>, a
<a href=https://cloud.google.com/compute/docs/disks/#repds>Regional Persistent Disk</a>
will be provisioned. It's highly recommended to have
<code>volumeBindingMode: WaitForFirstConsumer</code> set, in which case when you create
a Pod that consumes a PersistentVolumeClaim which uses this StorageClass, a
Regional Persistent Disk is provisioned with two zones. One zone is the same
as the zone that the Pod is scheduled in. The other zone is randomly picked
from the zones available to the cluster. Disk zones can be further constrained
using <code>allowedTopologies</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> <code>zone</code> and <code>zones</code> parameters are deprecated and replaced with
<a href=#allowed-topologies>allowedTopologies</a></div><h3 id=glusterfs>Glusterfs (deprecated)</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/glusterfs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resturl</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;http://127.0.0.1:8081&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterid</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;630372ccdc720a92c681fb928f27b53f&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restauthenabled</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restuser</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;admin&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>secretNamespace</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;default&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;heketi-secret&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>gidMin</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;40000&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>gidMax</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;50000&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumetype</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;replicate:3&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><p><code>resturl</code>: Gluster REST service/Heketi service url which provision gluster
volumes on demand. The general format should be <code>IPaddress:Port</code> and this is
a mandatory parameter for GlusterFS dynamic provisioner. If Heketi service is
exposed as a routable service in openshift/kubernetes setup, this can have a
format similar to <code>http://heketi-storage-project.cloudapps.mystorage.com</code>
where the fqdn is a resolvable Heketi service url.</p></li><li><p><code>restauthenabled</code> : Gluster REST service authentication boolean that enables
authentication to the REST server. If this value is <code>"true"</code>, <code>restuser</code> and
<code>restuserkey</code> or <code>secretNamespace</code> + <code>secretName</code> have to be filled. This
option is deprecated, authentication is enabled when any of <code>restuser</code>,
<code>restuserkey</code>, <code>secretName</code> or <code>secretNamespace</code> is specified.</p></li><li><p><code>restuser</code> : Gluster REST service/Heketi user who has access to create volumes
in the Gluster Trusted Pool.</p></li><li><p><code>restuserkey</code> : Gluster REST service/Heketi user's password which will be used
for authentication to the REST server. This parameter is deprecated in favor
of <code>secretNamespace</code> + <code>secretName</code>.</p></li><li><p><code>secretNamespace</code>, <code>secretName</code> : Identification of Secret instance that
contains user password to use when talking to Gluster REST service. These
parameters are optional, empty password will be used when both
<code>secretNamespace</code> and <code>secretName</code> are omitted. The provided secret must have
type <code>"kubernetes.io/glusterfs"</code>, for example created in this way:</p><pre tabindex=0><code>kubectl create secret generic heketi-secret \
  --type=&#34;kubernetes.io/glusterfs&#34; --from-literal=key=&#39;opensesame&#39; \
  --namespace=default
</code></pre><p>Example of a secret can be found in
<a href=https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/glusterfs/glusterfs-secret.yaml>glusterfs-provisioning-secret.yaml</a>.</p></li><li><p><code>clusterid</code>: <code>630372ccdc720a92c681fb928f27b53f</code> is the ID of the cluster
which will be used by Heketi when provisioning the volume. It can also be a
list of clusterids, for example:
<code>"8452344e2becec931ece4e33c4674e4e,42982310de6c63381718ccfa6d8cf397"</code>. This
is an optional parameter.</p></li><li><p><code>gidMin</code>, <code>gidMax</code> : The minimum and maximum value of GID range for the
StorageClass. A unique value (GID) in this range ( gidMin-gidMax ) will be
used for dynamically provisioned volumes. These are optional values. If not
specified, the volume will be provisioned with a value between 2000-2147483647
which are defaults for gidMin and gidMax respectively.</p></li><li><p><code>volumetype</code> : The volume type and its parameters can be configured with this
optional value. If the volume type is not mentioned, it's up to the provisioner
to decide the volume type.</p><p>For example:</p><ul><li>Replica volume: <code>volumetype: replicate:3</code> where '3' is replica count.</li><li>Disperse/EC volume: <code>volumetype: disperse:4:2</code> where '4' is data and '2' is the redundancy count.</li><li>Distribute volume: <code>volumetype: none</code></li></ul><p>For available volume types and administration options, refer to the
<a href=https://access.redhat.com/documentation/en-us/red_hat_gluster_storage/>Administration Guide</a>.</p><p>For further reference information, see
<a href=https://github.com/heketi/heketi/wiki/Setting-up-the-topology>How to configure Heketi</a>.</p><p>When persistent volumes are dynamically provisioned, the Gluster plugin
automatically creates an endpoint and a headless service in the name
<code>gluster-dynamic-&lt;claimname></code>. The dynamic endpoint and service are automatically
deleted when the persistent volume claim is deleted.</p></li></ul><h3 id=nfs>NFS</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-nfs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>example.com/external-nfs<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>server</span>:<span style=color:#bbb> </span>nfs-server.example.com<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/share<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;false&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><code>server</code>: Server is the hostname or IP address of the NFS server.</li><li><code>path</code>: Path that is exported by the NFS server.</li><li><code>readOnly</code>: A flag indicating whether the storage will be mounted as read only (default false).</li></ul><p>Kubernetes doesn't include an internal NFS provisioner. You need to use an external provisioner to create a StorageClass for NFS.
Here are some examples:</p><ul><li><a href=https://github.com/kubernetes-sigs/nfs-ganesha-server-and-external-provisioner>NFS Ganesha server and external provisioner</a></li><li><a href=https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner>NFS subdir external provisioner</a></li></ul><h3 id=openstack-cinder>OpenStack Cinder</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>gold<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/cinder<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>availability</span>:<span style=color:#bbb> </span>nova<span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><code>availability</code>: Availability Zone. If not specified, volumes are generally
round-robin-ed across all active zones where Kubernetes cluster has a node.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.11 [deprecated]</code></div><p>This internal provisioner of OpenStack is deprecated. Please use <a href=https://github.com/kubernetes/cloud-provider-openstack>the external cloud provider for OpenStack</a>.</div><h3 id=vsphere>vSphere</h3><p>There are two types of provisioners for vSphere storage classes:</p><ul><li><a href=#vsphere-provisioner-csi>CSI provisioner</a>: <code>csi.vsphere.vmware.com</code></li><li><a href=#vcp-provisioner>vCP provisioner</a>: <code>kubernetes.io/vsphere-volume</code></li></ul><p>In-tree provisioners are <a href=/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/#why-are-we-migrating-in-tree-plugins-to-csi>deprecated</a>. For more information on the CSI provisioner, see <a href=https://vsphere-csi-driver.sigs.k8s.io/>Kubernetes vSphere CSI Driver</a> and <a href=/docs/concepts/storage/volumes/#vsphere-csi-migration>vSphereVolume CSI migration</a>.</p><h4 id=vsphere-provisioner-csi>CSI Provisioner</h4><p>The vSphere CSI StorageClass provisioner works with Tanzu Kubernetes clusters. For an example, refer to the <a href=https://github.com/kubernetes-sigs/vsphere-csi-driver/blob/master/example/vanilla-k8s-RWM-filesystem-volumes/example-sc.yaml>vSphere CSI repository</a>.</p><h4 id=vcp-provisioner>vCP Provisioner</h4><p>The following examples use the VMware Cloud Provider (vCP) StorageClass provisioner.</p><ol><li><p>Create a StorageClass with a user specified disk format.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fast<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/vsphere-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>diskformat</span>:<span style=color:#bbb> </span>zeroedthick<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>diskformat</code>: <code>thin</code>, <code>zeroedthick</code> and <code>eagerzeroedthick</code>. Default: <code>"thin"</code>.</p></li><li><p>Create a StorageClass with a disk format on a user specified datastore.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fast<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/vsphere-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>diskformat</span>:<span style=color:#bbb> </span>zeroedthick<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>datastore</span>:<span style=color:#bbb> </span>VSANDatastore<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>datastore</code>: The user can also specify the datastore in the StorageClass.
The volume will be created on the datastore specified in the StorageClass,
which in this case is <code>VSANDatastore</code>. This field is optional. If the
datastore is not specified, then the volume will be created on the datastore
specified in the vSphere config file used to initialize the vSphere Cloud
Provider.</p></li><li><p>Storage Policy Management inside kubernetes</p><ul><li><p>Using existing vCenter SPBM policy</p><p>One of the most important features of vSphere for Storage Management is
policy based Management. Storage Policy Based Management (SPBM) is a
storage policy framework that provides a single unified control plane
across a broad range of data services and storage solutions. SPBM enables
vSphere administrators to overcome upfront storage provisioning challenges,
such as capacity planning, differentiated service levels and managing
capacity headroom.</p><p>The SPBM policies can be specified in the StorageClass using the
<code>storagePolicyName</code> parameter.</p></li><li><p>Virtual SAN policy support inside Kubernetes</p><p>Vsphere Infrastructure (VI) Admins will have the ability to specify custom
Virtual SAN Storage Capabilities during dynamic volume provisioning. You
can now define storage requirements, such as performance and availability,
in the form of storage capabilities during dynamic volume provisioning.
The storage capability requirements are converted into a Virtual SAN
policy which are then pushed down to the Virtual SAN layer when a
persistent volume (virtual disk) is being created. The virtual disk is
distributed across the Virtual SAN datastore to meet the requirements.</p><p>You can see <a href=https://github.com/vmware-archive/vsphere-storage-for-kubernetes/blob/fa4c8b8ad46a85b6555d715dd9d27ff69839df53/documentation/policy-based-mgmt.md>Storage Policy Based Management for dynamic provisioning of volumes</a>
for more details on how to use storage policies for persistent volumes
management.</p></li></ul></li></ol><p>There are few
<a href=https://github.com/kubernetes/examples/tree/master/staging/volumes/vsphere>vSphere examples</a>
which you try out for persistent volume management inside Kubernetes for vSphere.</p><h3 id=ceph-rbd>Ceph RBD</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fast<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/rbd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>monitors</span>:<span style=color:#bbb> </span><span style=color:#666>10.16.153.105</span>:<span style=color:#666>6789</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>adminId</span>:<span style=color:#bbb> </span>kube<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>adminSecretName</span>:<span style=color:#bbb> </span>ceph-secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>adminSecretNamespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>pool</span>:<span style=color:#bbb> </span>kube<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>userId</span>:<span style=color:#bbb> </span>kube<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>userSecretName</span>:<span style=color:#bbb> </span>ceph-secret-user<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>userSecretNamespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fsType</span>:<span style=color:#bbb> </span>ext4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imageFormat</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imageFeatures</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;layering&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><p><code>monitors</code>: Ceph monitors, comma delimited. This parameter is required.</p></li><li><p><code>adminId</code>: Ceph client ID that is capable of creating images in the pool.
Default is "admin".</p></li><li><p><code>adminSecretName</code>: Secret Name for <code>adminId</code>. This parameter is required.
The provided secret must have type "kubernetes.io/rbd".</p></li><li><p><code>adminSecretNamespace</code>: The namespace for <code>adminSecretName</code>. Default is "default".</p></li><li><p><code>pool</code>: Ceph RBD pool. Default is "rbd".</p></li><li><p><code>userId</code>: Ceph client ID that is used to map the RBD image. Default is the
same as <code>adminId</code>.</p></li><li><p><code>userSecretName</code>: The name of Ceph Secret for <code>userId</code> to map RBD image. It
must exist in the same namespace as PVCs. This parameter is required.
The provided secret must have type "kubernetes.io/rbd", for example created in this
way:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic ceph-secret --type<span style=color:#666>=</span><span style=color:#b44>&#34;kubernetes.io/rbd&#34;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --from-literal<span style=color:#666>=</span><span style=color:#b8860b>key</span><span style=color:#666>=</span><span style=color:#b44>&#39;QVFEQ1pMdFhPUnQrSmhBQUFYaERWNHJsZ3BsMmNjcDR6RFZST0E9PQ==&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --namespace<span style=color:#666>=</span>kube-system
</span></span></code></pre></div></li><li><p><code>userSecretNamespace</code>: The namespace for <code>userSecretName</code>.</p></li><li><p><code>fsType</code>: fsType that is supported by kubernetes. Default: <code>"ext4"</code>.</p></li><li><p><code>imageFormat</code>: Ceph RBD image format, "1" or "2". Default is "2".</p></li><li><p><code>imageFeatures</code>: This parameter is optional and should only be used if you
set <code>imageFormat</code> to "2". Currently supported features are <code>layering</code> only.
Default is "", and no features are turned on.</p></li></ul><h3 id=azure-disk>Azure Disk</h3><h4 id=azure-unmanaged-disk-storage-class>Azure Unmanaged Disk storage class</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/azure-disk<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>skuName</span>:<span style=color:#bbb> </span>Standard_LRS<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>location</span>:<span style=color:#bbb> </span>eastus<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageAccount</span>:<span style=color:#bbb> </span>azure_storage_account_name<span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><code>skuName</code>: Azure storage account Sku tier. Default is empty.</li><li><code>location</code>: Azure storage account location. Default is empty.</li><li><code>storageAccount</code>: Azure storage account name. If a storage account is provided,
it must reside in the same resource group as the cluster, and <code>location</code> is
ignored. If a storage account is not provided, a new storage account will be
created in the same resource group as the cluster.</li></ul><h4 id=azure-disk-storage-class>Azure Disk storage class (starting from v1.7.2)</h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/azure-disk<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageaccounttype</span>:<span style=color:#bbb> </span>Standard_LRS<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>managed<span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><code>storageaccounttype</code>: Azure storage account Sku tier. Default is empty.</li><li><code>kind</code>: Possible values are <code>shared</code>, <code>dedicated</code>, and <code>managed</code> (default).
When <code>kind</code> is <code>shared</code>, all unmanaged disks are created in a few shared
storage accounts in the same resource group as the cluster. When <code>kind</code> is
<code>dedicated</code>, a new dedicated storage account will be created for the new
unmanaged disk in the same resource group as the cluster. When <code>kind</code> is
<code>managed</code>, all managed disks are created in the same resource group as
the cluster.</li><li><code>resourceGroup</code>: Specify the resource group in which the Azure disk will be created.
It must be an existing resource group name. If it is unspecified, the disk will be
placed in the same resource group as the current Kubernetes cluster.</li></ul><ul><li>Premium VM can attach both Standard_LRS and Premium_LRS disks, while Standard
VM can only attach Standard_LRS disks.</li><li>Managed VM can only attach managed disks and unmanaged VM can only attach
unmanaged disks.</li></ul><h3 id=azure-file>Azure File</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>azurefile<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/azure-file<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>skuName</span>:<span style=color:#bbb> </span>Standard_LRS<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>location</span>:<span style=color:#bbb> </span>eastus<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageAccount</span>:<span style=color:#bbb> </span>azure_storage_account_name<span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><code>skuName</code>: Azure storage account Sku tier. Default is empty.</li><li><code>location</code>: Azure storage account location. Default is empty.</li><li><code>storageAccount</code>: Azure storage account name. Default is empty. If a storage
account is not provided, all storage accounts associated with the resource
group are searched to find one that matches <code>skuName</code> and <code>location</code>. If a
storage account is provided, it must reside in the same resource group as the
cluster, and <code>skuName</code> and <code>location</code> are ignored.</li><li><code>secretNamespace</code>: the namespace of the secret that contains the Azure Storage
Account Name and Key. Default is the same as the Pod.</li><li><code>secretName</code>: the name of the secret that contains the Azure Storage Account Name and
Key. Default is <code>azure-storage-account-&lt;accountName>-secret</code></li><li><code>readOnly</code>: a flag indicating whether the storage will be mounted as read only.
Defaults to false which means a read/write mount. This setting will impact the
<code>ReadOnly</code> setting in VolumeMounts as well.</li></ul><p>During storage provisioning, a secret named by <code>secretName</code> is created for the
mounting credentials. If the cluster has enabled both
<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a> and
<a href=/docs/reference/access-authn-authz/rbac/#controller-roles>Controller Roles</a>,
add the <code>create</code> permission of resource <code>secret</code> for clusterrole
<code>system:controller:persistent-volume-binder</code>.</p><p>In a multi-tenancy context, it is strongly recommended to set the value for
<code>secretNamespace</code> explicitly, otherwise the storage account credentials may
be read by other users.</p><h3 id=portworx-volume>Portworx Volume</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>portworx-io-priority-high<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/portworx-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>repl</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>snap_interval</span>:<span style=color:#bbb>   </span><span style=color:#b44>&#34;70&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priority_io</span>:<span style=color:#bbb>  </span><span style=color:#b44>&#34;high&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><ul><li><code>fs</code>: filesystem to be laid out: <code>none/xfs/ext4</code> (default: <code>ext4</code>).</li><li><code>block_size</code>: block size in Kbytes (default: <code>32</code>).</li><li><code>repl</code>: number of synchronous replicas to be provided in the form of
replication factor <code>1..3</code> (default: <code>1</code>) A string is expected here i.e.
<code>"1"</code> and not <code>1</code>.</li><li><code>priority_io</code>: determines whether the volume will be created from higher
performance or a lower priority storage <code>high/medium/low</code> (default: <code>low</code>).</li><li><code>snap_interval</code>: clock/time interval in minutes for when to trigger snapshots.
Snapshots are incremental based on difference with the prior snapshot, 0
disables snaps (default: <code>0</code>). A string is expected here i.e.
<code>"70"</code> and not <code>70</code>.</li><li><code>aggregation_level</code>: specifies the number of chunks the volume would be
distributed into, 0 indicates a non-aggregated volume (default: <code>0</code>). A string
is expected here i.e. <code>"0"</code> and not <code>0</code></li><li><code>ephemeral</code>: specifies whether the volume should be cleaned-up after unmount
or should be persistent. <code>emptyDir</code> use case can set this value to true and
<code>persistent volumes</code> use case such as for databases like Cassandra should set
to false, <code>true/false</code> (default <code>false</code>). A string is expected here i.e.
<code>"true"</code> and not <code>true</code>.</li></ul><h3 id=local>Local</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>local-storage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/no-provisioner<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>volumeBindingMode</span>:<span style=color:#bbb> </span>WaitForFirstConsumer<span style=color:#bbb>
</span></span></span></code></pre></div><p>Local volumes do not currently support dynamic provisioning, however a StorageClass
should still be created to delay volume binding until Pod scheduling. This is
specified by the <code>WaitForFirstConsumer</code> volume binding mode.</p><p>Delaying volume binding allows the scheduler to consider all of a Pod's
scheduling constraints when choosing an appropriate PersistentVolume for a
PersistentVolumeClaim.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-018f0a7fc6e2f6d16da37702fc39b4f3>7.6 - Dynamic Volume Provisioning</h1><p>Dynamic volume provisioning allows storage volumes to be created on-demand.
Without dynamic provisioning, cluster administrators have to manually make
calls to their cloud or storage provider to create new storage volumes, and
then create <a href=/docs/concepts/storage/persistent-volumes/><code>PersistentVolume</code> objects</a>
to represent them in Kubernetes. The dynamic provisioning feature eliminates
the need for cluster administrators to pre-provision storage. Instead, it
automatically provisions storage when it is requested by users.</p><h2 id=background>Background</h2><p>The implementation of dynamic volume provisioning is based on the API object <code>StorageClass</code>
from the API group <code>storage.k8s.io</code>. A cluster administrator can define as many
<code>StorageClass</code> objects as needed, each specifying a <em>volume plugin</em> (aka
<em>provisioner</em>) that provisions a volume and the set of parameters to pass to
that provisioner when provisioning.
A cluster administrator can define and expose multiple flavors of storage (from
the same or different storage systems) within a cluster, each with a custom set
of parameters. This design also ensures that end users don't have to worry
about the complexity and nuances of how storage is provisioned, but still
have the ability to select from multiple storage options.</p><p>More information on storage classes can be found
<a href=/docs/concepts/storage/storage-classes/>here</a>.</p><h2 id=enabling-dynamic-provisioning>Enabling Dynamic Provisioning</h2><p>To enable dynamic provisioning, a cluster administrator needs to pre-create
one or more StorageClass objects for users.
StorageClass objects define which provisioner should be used and what parameters
should be passed to that provisioner when dynamic provisioning is invoked.
The name of a StorageClass object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>The following manifest creates a storage class "slow" which provisions standard
disk-like persistent disks.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>slow<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/gce-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>pd-standard<span style=color:#bbb>
</span></span></span></code></pre></div><p>The following manifest creates a storage class "fast" which provisions
SSD-like persistent disks.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StorageClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fast<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>provisioner</span>:<span style=color:#bbb> </span>kubernetes.io/gce-pd<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>pd-ssd<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=using-dynamic-provisioning>Using Dynamic Provisioning</h2><p>Users request dynamically provisioned storage by including a storage class in
their <code>PersistentVolumeClaim</code>. Before Kubernetes v1.6, this was done via the
<code>volume.beta.kubernetes.io/storage-class</code> annotation. However, this annotation
is deprecated since v1.9. Users now can and should instead use the
<code>storageClassName</code> field of the <code>PersistentVolumeClaim</code> object. The value of
this field must match the name of a <code>StorageClass</code> configured by the
administrator (see <a href=#enabling-dynamic-provisioning>below</a>).</p><p>To select the "fast" storage class, for example, a user would create the
following PersistentVolumeClaim:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>claim1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>fast<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>30Gi<span style=color:#bbb>
</span></span></span></code></pre></div><p>This claim results in an SSD-like Persistent Disk being automatically
provisioned. When the claim is deleted, the volume is destroyed.</p><h2 id=defaulting-behavior>Defaulting Behavior</h2><p>Dynamic provisioning can be enabled on a cluster such that all claims are
dynamically provisioned if no storage class is specified. A cluster administrator
can enable this behavior by:</p><ul><li>Marking one <code>StorageClass</code> object as <em>default</em>;</li><li>Making sure that the <a href=/docs/reference/access-authn-authz/admission-controllers/#defaultstorageclass><code>DefaultStorageClass</code> admission controller</a>
is enabled on the API server.</li></ul><p>An administrator can mark a specific <code>StorageClass</code> as default by adding the
<a href=/docs/reference/labels-annotations-taints/#storageclass-kubernetes-io-is-default-class><code>storageclass.kubernetes.io/is-default-class</code> annotation</a> to it.
When a default <code>StorageClass</code> exists in a cluster and a user creates a
<code>PersistentVolumeClaim</code> with <code>storageClassName</code> unspecified, the
<code>DefaultStorageClass</code> admission controller automatically adds the
<code>storageClassName</code> field pointing to the default storage class.</p><p>Note that there can be at most one <em>default</em> storage class on a cluster, or
a <code>PersistentVolumeClaim</code> without <code>storageClassName</code> explicitly specified cannot
be created.</p><h2 id=topology-awareness>Topology Awareness</h2><p>In <a href=/docs/setup/best-practices/multiple-zones/>Multi-Zone</a> clusters, Pods can be spread across
Zones in a Region. Single-Zone storage backends should be provisioned in the Zones where
Pods are scheduled. This can be accomplished by setting the
<a href=/docs/concepts/storage/storage-classes/#volume-binding-mode>Volume Binding Mode</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c262af210c6828dec445d2f55a1d877a>7.7 - Volume Snapshots</h1><p>In Kubernetes, a <em>VolumeSnapshot</em> represents a snapshot of a volume on a storage
system. This document assumes that you are already familiar with Kubernetes
<a href=/docs/concepts/storage/persistent-volumes/>persistent volumes</a>.</p><h2 id=introduction>Introduction</h2><p>Similar to how API resources <code>PersistentVolume</code> and <code>PersistentVolumeClaim</code> are
used to provision volumes for users and administrators, <code>VolumeSnapshotContent</code>
and <code>VolumeSnapshot</code> API resources are provided to create volume snapshots for
users and administrators.</p><p>A <code>VolumeSnapshotContent</code> is a snapshot taken from a volume in the cluster that
has been provisioned by an administrator. It is a resource in the cluster just
like a PersistentVolume is a cluster resource.</p><p>A <code>VolumeSnapshot</code> is a request for snapshot of a volume by a user. It is similar
to a PersistentVolumeClaim.</p><p><code>VolumeSnapshotClass</code> allows you to specify different attributes belonging to a
<code>VolumeSnapshot</code>. These attributes may differ among snapshots taken from the same
volume on the storage system and therefore cannot be expressed by using the same
<code>StorageClass</code> of a <code>PersistentVolumeClaim</code>.</p><p>Volume snapshots provide Kubernetes users with a standardized way to copy a volume's
contents at a particular point in time without creating an entirely new volume. This
functionality enables, for example, database administrators to backup databases before
performing edit or delete modifications.</p><p>Users need to be aware of the following when using this feature:</p><ul><li>API Objects <code>VolumeSnapshot</code>, <code>VolumeSnapshotContent</code>, and <code>VolumeSnapshotClass</code>
are <a class=glossary-tooltip title='Custom code that defines a resource to add to your Kubernetes API server without building a complete custom server.' data-toggle=tooltip data-placement=top href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/ target=_blank aria-label=CRDs>CRDs</a>, not
part of the core API.</li><li><code>VolumeSnapshot</code> support is only available for CSI drivers.</li><li>As part of the deployment process of <code>VolumeSnapshot</code>, the Kubernetes team provides
a snapshot controller to be deployed into the control plane, and a sidecar helper
container called csi-snapshotter to be deployed together with the CSI driver.
The snapshot controller watches <code>VolumeSnapshot</code> and <code>VolumeSnapshotContent</code> objects
and is responsible for the creation and deletion of <code>VolumeSnapshotContent</code> object.
The sidecar csi-snapshotter watches <code>VolumeSnapshotContent</code> objects and triggers
<code>CreateSnapshot</code> and <code>DeleteSnapshot</code> operations against a CSI endpoint.</li><li>There is also a validating webhook server which provides tightened validation on
snapshot objects. This should be installed by the Kubernetes distros along with
the snapshot controller and CRDs, not CSI drivers. It should be installed in all
Kubernetes clusters that has the snapshot feature enabled.</li><li>CSI drivers may or may not have implemented the volume snapshot functionality.
The CSI drivers that have provided support for volume snapshot will likely use
the csi-snapshotter. See <a href=https://kubernetes-csi.github.io/docs/>CSI Driver documentation</a> for details.</li><li>The CRDs and snapshot controller installations are the responsibility of the Kubernetes distribution.</li></ul><h2 id=lifecycle-of-a-volume-snapshot-and-volume-snapshot-content>Lifecycle of a volume snapshot and volume snapshot content</h2><p><code>VolumeSnapshotContents</code> are resources in the cluster. <code>VolumeSnapshots</code> are requests
for those resources. The interaction between <code>VolumeSnapshotContents</code> and <code>VolumeSnapshots</code>
follow this lifecycle:</p><h3 id=provisioning-volume-snapshot>Provisioning Volume Snapshot</h3><p>There are two ways snapshots may be provisioned: pre-provisioned or dynamically provisioned.</p><h4 id=static>Pre-provisioned</h4><p>A cluster administrator creates a number of <code>VolumeSnapshotContents</code>. They carry the details
of the real volume snapshot on the storage system which is available for use by cluster users.
They exist in the Kubernetes API and are available for consumption.</p><h4 id=dynamic>Dynamic</h4><p>Instead of using a pre-existing snapshot, you can request that a snapshot to be dynamically
taken from a PersistentVolumeClaim. The <a href=/docs/concepts/storage/volume-snapshot-classes/>VolumeSnapshotClass</a>
specifies storage provider-specific parameters to use when taking a snapshot.</p><h3 id=binding>Binding</h3><p>The snapshot controller handles the binding of a <code>VolumeSnapshot</code> object with an appropriate
<code>VolumeSnapshotContent</code> object, in both pre-provisioned and dynamically provisioned scenarios.
The binding is a one-to-one mapping.</p><p>In the case of pre-provisioned binding, the VolumeSnapshot will remain unbound until the
requested VolumeSnapshotContent object is created.</p><h3 id=persistent-volume-claim-as-snapshot-source-protection>Persistent Volume Claim as Snapshot Source Protection</h3><p>The purpose of this protection is to ensure that in-use
<a class=glossary-tooltip title='Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PersistentVolumeClaim>PersistentVolumeClaim</a>
API objects are not removed from the system while a snapshot is being taken from it
(as this may result in data loss).</p><p>While a snapshot is being taken of a PersistentVolumeClaim, that PersistentVolumeClaim
is in-use. If you delete a PersistentVolumeClaim API object in active use as a snapshot
source, the PersistentVolumeClaim object is not removed immediately. Instead, removal of
the PersistentVolumeClaim object is postponed until the snapshot is readyToUse or aborted.</p><h3 id=delete>Delete</h3><p>Deletion is triggered by deleting the <code>VolumeSnapshot</code> object, and the <code>DeletionPolicy</code>
will be followed. If the <code>DeletionPolicy</code> is <code>Delete</code>, then the underlying storage snapshot
will be deleted along with the <code>VolumeSnapshotContent</code> object. If the <code>DeletionPolicy</code> is
<code>Retain</code>, then both the underlying snapshot and <code>VolumeSnapshotContent</code> remain.</p><h2 id=volumesnapshots>VolumeSnapshots</h2><p>Each VolumeSnapshot contains a spec and a status.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotClassName</span>:<span style=color:#bbb> </span>csi-hostpath-snapclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>persistentVolumeClaimName</span>:<span style=color:#bbb> </span>pvc-test<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>persistentVolumeClaimName</code> is the name of the PersistentVolumeClaim data source
for the snapshot. This field is required for dynamically provisioning a snapshot.</p><p>A volume snapshot can request a particular class by specifying the name of a
<a href=/docs/concepts/storage/volume-snapshot-classes/>VolumeSnapshotClass</a>
using the attribute <code>volumeSnapshotClassName</code>. If nothing is set, then the
default class is used if available.</p><p>For pre-provisioned snapshots, you need to specify a <code>volumeSnapshotContentName</code>
as the source for the snapshot as shown in the following example. The
<code>volumeSnapshotContentName</code> source field is required for pre-provisioned snapshots.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshot<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-snapshot<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeSnapshotContentName</span>:<span style=color:#bbb> </span>test-content<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=volume-snapshot-contents>Volume Snapshot Contents</h2><p>Each VolumeSnapshotContent contains a spec and status. In dynamic provisioning,
the snapshot common controller creates <code>VolumeSnapshotContent</code> objects. Here is an example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotContent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>snapcontent-72d9a349-aacd-42d2-a240-d775650d2455<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>hostpath.csi.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeHandle</span>:<span style=color:#bbb> </span>ee0cfb94-f8d4-11e9-b2d8-0242ac110002<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>sourceVolumeMode</span>:<span style=color:#bbb> </span>Filesystem<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotClassName</span>:<span style=color:#bbb> </span>csi-hostpath-snapclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>uid</span>:<span style=color:#bbb> </span>72d9a349-aacd-42d2-a240-d775650d2455<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>volumeHandle</code> is the unique identifier of the volume created on the storage
backend and returned by the CSI driver during the volume creation. This field
is required for dynamically provisioning a snapshot.
It specifies the volume source of the snapshot.</p><p>For pre-provisioned snapshots, you (as cluster administrator) are responsible
for creating the <code>VolumeSnapshotContent</code> object as follows.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotContent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-content-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>hostpath.csi.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>snapshotHandle</span>:<span style=color:#bbb> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>sourceVolumeMode</span>:<span style=color:#bbb> </span>Filesystem<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span></code></pre></div><p><code>snapshotHandle</code> is the unique identifier of the volume snapshot created on
the storage backend. This field is required for the pre-provisioned snapshots.
It specifies the CSI snapshot id on the storage system that this
<code>VolumeSnapshotContent</code> represents.</p><p><code>sourceVolumeMode</code> is the mode of the volume whose snapshot is taken. The value
of the <code>sourceVolumeMode</code> field can be either <code>Filesystem</code> or <code>Block</code>. If the
source volume mode is not specified, Kubernetes treats the snapshot as if the
source volume's mode is unknown.</p><p><code>volumeSnapshotRef</code> is the reference of the corresponding <code>VolumeSnapshot</code>. Note that
when the <code>VolumeSnapshotContent</code> is being created as a pre-provisioned snapshot, the
<code>VolumeSnapshot</code> referenced in <code>volumeSnapshotRef</code> might not exist yet.</p><h2 id=convert-volume-mode>Converting the volume mode of a Snapshot</h2><p>If the <code>VolumeSnapshots</code> API installed on your cluster supports the <code>sourceVolumeMode</code>
field, then the API has the capability to prevent unauthorized users from converting
the mode of a volume.</p><p>To check if your cluster has capability for this feature, run the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>$ kubectl get crd volumesnapshotcontent -o yaml<span style=color:#bbb>
</span></span></span></code></pre></div><p>If you want to allow users to create a <code>PersistentVolumeClaim</code> from an existing
<code>VolumeSnapshot</code>, but with a different volume mode than the source, the annotation
<code>snapshot.storage.kubernetes.io/allowVolumeModeChange: "true"</code>needs to be added to
the <code>VolumeSnapshotContent</code> that corresponds to the <code>VolumeSnapshot</code>.</p><p>For pre-provisioned snapshots, <code>Spec.SourceVolumeMode</code> needs to be populated
by the cluster administrator.</p><p>An example <code>VolumeSnapshotContent</code> resource with this feature enabled would look like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotContent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-content-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>snapshot.storage.kubernetes.io/allowVolumeModeChange</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>hostpath.csi.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>source</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>snapshotHandle</span>:<span style=color:#bbb> </span>7bdd0de3-aaeb-11e8-9aae-0242ac110002<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>sourceVolumeMode</span>:<span style=color:#bbb> </span>Filesystem<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeSnapshotRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new-snapshot-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=provisioning-volumes-from-snapshots>Provisioning Volumes from Snapshots</h2><p>You can provision a new volume, pre-populated with data from a snapshot, by using
the <em>dataSource</em> field in the <code>PersistentVolumeClaim</code> object.</p><p>For more details, see
<a href=/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support>Volume Snapshot and Restore Volume from Snapshot</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4d00116c86dade62bdd5be7dc2afa1ca>7.8 - Volume Snapshot Classes</h1><p>This document describes the concept of VolumeSnapshotClass in Kubernetes. Familiarity
with <a href=/docs/concepts/storage/volume-snapshots/>volume snapshots</a> and
<a href=/docs/concepts/storage/storage-classes>storage classes</a> is suggested.</p><h2 id=introduction>Introduction</h2><p>Just like StorageClass provides a way for administrators to describe the "classes"
of storage they offer when provisioning a volume, VolumeSnapshotClass provides a
way to describe the "classes" of storage when provisioning a volume snapshot.</p><h2 id=the-volumesnapshotclass-resource>The VolumeSnapshotClass Resource</h2><p>Each VolumeSnapshotClass contains the fields <code>driver</code>, <code>deletionPolicy</code>, and <code>parameters</code>,
which are used when a VolumeSnapshot belonging to the class needs to be
dynamically provisioned.</p><p>The name of a VolumeSnapshotClass object is significant, and is how users can
request a particular class. Administrators set the name and other parameters
of a class when first creating VolumeSnapshotClass objects, and the objects cannot
be updated once they are created.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Installation of the CRDs is the responsibility of the Kubernetes distribution. Without the required CRDs present, the creation of a VolumeSnapshotClass fails.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-hostpath-snapclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>hostpath.csi.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span></code></pre></div><p>Administrators can specify a default VolumeSnapshotClass for VolumeSnapshots
that don't request any particular class to bind to by adding the
<code>snapshot.storage.kubernetes.io/is-default-class: "true"</code> annotation:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>snapshot.storage.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>VolumeSnapshotClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>csi-hostpath-snapclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>snapshot.storage.kubernetes.io/is-default-class</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>driver</span>:<span style=color:#bbb> </span>hostpath.csi.k8s.io<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>deletionPolicy</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>parameters</span>:<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=driver>Driver</h3><p>Volume snapshot classes have a driver that determines what CSI volume plugin is
used for provisioning VolumeSnapshots. This field must be specified.</p><h3 id=deletionpolicy>DeletionPolicy</h3><p>Volume snapshot classes have a deletionPolicy. It enables you to configure what happens to a VolumeSnapshotContent when the VolumeSnapshot object it is bound to is to be deleted. The deletionPolicy of a volume snapshot class can either be <code>Retain</code> or <code>Delete</code>. This field must be specified.</p><p>If the deletionPolicy is <code>Delete</code>, then the underlying storage snapshot will be deleted along with the VolumeSnapshotContent object. If the deletionPolicy is <code>Retain</code>, then both the underlying snapshot and VolumeSnapshotContent remain.</p><h2 id=parameters>Parameters</h2><p>Volume snapshot classes have parameters that describe volume snapshots belonging to
the volume snapshot class. Different parameters may be accepted depending on the
<code>driver</code>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-707ca81a34eb1ca202f34692e9917d1e>7.9 - CSI Volume Cloning</h1><p>This document describes the concept of cloning existing CSI Volumes in Kubernetes. Familiarity with <a href=/docs/concepts/storage/volumes>Volumes</a> is suggested.</p><h2 id=introduction>Introduction</h2><p>The <a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> Volume Cloning feature adds support for specifying existing <a class=glossary-tooltip title='Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PVC>PVC</a>s in the <code>dataSource</code> field to indicate a user would like to clone a <a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volume>Volume</a>.</p><p>A Clone is defined as a duplicate of an existing Kubernetes Volume that can be consumed as any standard Volume would be. The only difference is that upon provisioning, rather than creating a "new" empty Volume, the back end device creates an exact duplicate of the specified Volume.</p><p>The implementation of cloning, from the perspective of the Kubernetes API, adds the ability to specify an existing PVC as a dataSource during new PVC creation. The source PVC must be bound and available (not in use).</p><p>Users need to be aware of the following when using this feature:</p><ul><li>Cloning support (<code>VolumePVCDataSource</code>) is only available for CSI drivers.</li><li>Cloning support is only available for dynamic provisioners.</li><li>CSI drivers may or may not have implemented the volume cloning functionality.</li><li>You can only clone a PVC when it exists in the same namespace as the destination PVC (source and destination must be in the same namespace).</li><li>Cloning is supported with a different Storage Class.<ul><li>Destination volume can be the same or a different storage class as the source.</li><li>Default storage class can be used and storageClassName omitted in the spec.</li></ul></li><li>Cloning can only be performed between two volumes that use the same VolumeMode setting (if you request a block mode volume, the source MUST also be block mode)</li></ul><h2 id=provisioning>Provisioning</h2><p>Clones are provisioned like any other PVC with the exception of adding a dataSource that references an existing PVC in the same namespace.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>clone-of-pvc-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>myns<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- ReadWriteOnce<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span>cloning<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>5Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>dataSource</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PersistentVolumeClaim<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pvc-1<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You must specify a capacity value for <code>spec.resources.requests.storage</code>, and the value you specify must be the same or larger than the capacity of the source volume.</div><p>The result is a new PVC with the name <code>clone-of-pvc-1</code> that has the exact same content as the specified source <code>pvc-1</code>.</p><h2 id=usage>Usage</h2><p>Upon availability of the new PVC, the cloned PVC is consumed the same as other PVC. It's also expected at this point that the newly created PVC is an independent object. It can be consumed, cloned, snapshotted, or deleted independently and without consideration for it's original dataSource PVC. This also implies that the source is not linked in any way to the newly created clone, it may also be modified or deleted without affecting the newly created clone.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-00cd24f4570b7acaac75c2551c948bc7>7.10 - Storage Capacity</h1><p>Storage capacity is limited and may vary depending on the node on
which a pod runs: network-attached storage might not be accessible by
all nodes, or storage is local to a node to begin with.</p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>This page describes how Kubernetes keeps track of storage capacity and
how the scheduler uses that information to <a href=/docs/concepts/scheduling-eviction/>schedule Pods</a> onto nodes
that have access to enough storage capacity for the remaining missing
volumes. Without storage capacity tracking, the scheduler may choose a
node that doesn't have enough capacity to provision a volume and
multiple scheduling retries will be needed.</p><h2 id=before-you-begin>Before you begin</h2><p>Kubernetes v1.25 includes cluster-level API support for
storage capacity tracking. To use this you must also be using a CSI driver that
supports capacity tracking. Consult the documentation for the CSI drivers that
you use to find out whether this support is available and, if so, how to use
it. If you are not running Kubernetes v1.25, check the
documentation for that version of Kubernetes.</p><h2 id=api>API</h2><p>There are two API extensions for this feature:</p><ul><li><a href=/docs/reference/kubernetes-api/config-and-storage-resources/csi-storage-capacity-v1/>CSIStorageCapacity</a> objects:
these get produced by a CSI driver in the namespace
where the driver is installed. Each object contains capacity
information for one storage class and defines which nodes have
access to that storage.</li><li><a href=/docs/reference/kubernetes-api/config-and-storage-resources/csi-driver-v1/#CSIDriverSpec>The <code>CSIDriverSpec.StorageCapacity</code> field</a>:
when set to <code>true</code>, the Kubernetes scheduler will consider storage
capacity for volumes that use the CSI driver.</li></ul><h2 id=scheduling>Scheduling</h2><p>Storage capacity information is used by the Kubernetes scheduler if:</p><ul><li>a Pod uses a volume that has not been created yet,</li><li>that volume uses a <a class=glossary-tooltip title='A StorageClass provides a way for administrators to describe different available storage types.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/storage-classes target=_blank aria-label=StorageClass>StorageClass</a> which references a CSI driver and
uses <code>WaitForFirstConsumer</code> <a href=/docs/concepts/storage/storage-classes/#volume-binding-mode>volume binding
mode</a>,
and</li><li>the <code>CSIDriver</code> object for the driver has <code>StorageCapacity</code> set to
true.</li></ul><p>In that case, the scheduler only considers nodes for the Pod which
have enough storage available to them. This check is very
simplistic and only compares the size of the volume against the
capacity listed in <code>CSIStorageCapacity</code> objects with a topology that
includes the node.</p><p>For volumes with <code>Immediate</code> volume binding mode, the storage driver
decides where to create the volume, independently of Pods that will
use the volume. The scheduler then schedules Pods onto nodes where the
volume is available after the volume has been created.</p><p>For <a href=/docs/concepts/storage/ephemeral-volumes/#csi-ephemeral-volumes>CSI ephemeral volumes</a>,
scheduling always happens without considering storage capacity. This
is based on the assumption that this volume type is only used by
special CSI drivers which are local to a node and do not need
significant resources there.</p><h2 id=rescheduling>Rescheduling</h2><p>When a node has been selected for a Pod with <code>WaitForFirstConsumer</code>
volumes, that decision is still tentative. The next step is that the
CSI storage driver gets asked to create the volume with a hint that the
volume is supposed to be available on the selected node.</p><p>Because Kubernetes might have chosen a node based on out-dated
capacity information, it is possible that the volume cannot really be
created. The node selection is then reset and the Kubernetes scheduler
tries again to find a node for the Pod.</p><h2 id=limitations>Limitations</h2><p>Storage capacity tracking increases the chance that scheduling works
on the first try, but cannot guarantee this because the scheduler has
to decide based on potentially out-dated information. Usually, the
same retry mechanism as for scheduling without any storage capacity
information handles scheduling failures.</p><p>One situation where scheduling can fail permanently is when a Pod uses
multiple volumes: one volume might have been created already in a
topology segment which then does not have enough capacity left for
another volume. Manual intervention is necessary to recover from this,
for example by increasing capacity or deleting the volume that was
already created.</p><h2 id=what-s-next>What's next</h2><ul><li>For more information on the design, see the
<a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/1472-storage-capacity-tracking/README.md>Storage Capacity Constraints for Pod Scheduling KEP</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b2e4b16ac37988c678a3312a4a6639f8>7.11 - Node-specific Volume Limits</h1><p>This page describes the maximum number of volumes that can be attached
to a Node for various cloud providers.</p><p>Cloud providers like Google, Amazon, and Microsoft typically have a limit on
how many volumes can be attached to a Node. It is important for Kubernetes to
respect those limits. Otherwise, Pods scheduled on a Node could get stuck
waiting for volumes to attach.</p><h2 id=kubernetes-default-limits>Kubernetes default limits</h2><p>The Kubernetes scheduler has default limits on the number of volumes
that can be attached to a Node:</p><table><tr><th>Cloud service</th><th>Maximum volumes per Node</th></tr><tr><td><a href=https://aws.amazon.com/ebs/>Amazon Elastic Block Store (EBS)</a></td><td>39</td></tr><tr><td><a href=https://cloud.google.com/persistent-disk/>Google Persistent Disk</a></td><td>16</td></tr><tr><td><a href=https://azure.microsoft.com/en-us/services/storage/main-disks/>Microsoft Azure Disk Storage</a></td><td>16</td></tr></table><h2 id=custom-limits>Custom limits</h2><p>You can change these limits by setting the value of the
<code>KUBE_MAX_PD_VOLS</code> environment variable, and then starting the scheduler.
CSI drivers might have a different procedure, see their documentation
on how to customize their limits.</p><p>Use caution if you set a limit that is higher than the default limit. Consult
the cloud provider's documentation to make sure that Nodes can actually support
the limit you set.</p><p>The limit applies to the entire cluster, so it affects all Nodes.</p><h2 id=dynamic-volume-limits>Dynamic volume limits</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code></div><p>Dynamic volume limits are supported for following volume types.</p><ul><li>Amazon EBS</li><li>Google Persistent Disk</li><li>Azure Disk</li><li>CSI</li></ul><p>For volumes managed by in-tree volume plugins, Kubernetes automatically determines the Node
type and enforces the appropriate maximum number of volumes for the node. For example:</p><ul><li><p>On
<a href=https://cloud.google.com/compute/>Google Compute Engine</a>,
up to 127 volumes can be attached to a node, <a href=https://cloud.google.com/compute/docs/disks/#pdnumberlimits>depending on the node
type</a>.</p></li><li><p>For Amazon EBS disks on M5,C5,R5,T3 and Z1D instance types, Kubernetes allows only 25
volumes to be attached to a Node. For other instance types on
<a href=https://aws.amazon.com/ec2/>Amazon Elastic Compute Cloud (EC2)</a>,
Kubernetes allows 39 volumes to be attached to a Node.</p></li><li><p>On Azure, up to 64 disks can be attached to a node, depending on the node type. For more details, refer to <a href=https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes>Sizes for virtual machines in Azure</a>.</p></li><li><p>If a CSI storage driver advertises a maximum number of volumes for a Node (using <code>NodeGetInfo</code>), the <a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a> honors that limit.
Refer to the <a href=https://github.com/container-storage-interface/spec/blob/master/spec.md#nodegetinfo>CSI specifications</a> for details.</p></li><li><p>For volumes managed by in-tree plugins that have been migrated to a CSI driver, the maximum number of volumes will be the one reported by the CSI driver.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4f40cb95a671e51b4f0156a409d95c6d>7.12 - Volume Health Monitoring</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [alpha]</code></div><p><a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label=CSI>CSI</a> volume health monitoring allows CSI Drivers to detect abnormal volume conditions from the underlying storage systems and report them as events on <a class=glossary-tooltip title='Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PVCs>PVCs</a> or <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>.</p><h2 id=volume-health-monitoring>Volume health monitoring</h2><p>Kubernetes <em>volume health monitoring</em> is part of how Kubernetes implements the Container Storage Interface (CSI). Volume health monitoring feature is implemented in two components: an External Health Monitor controller, and the <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>.</p><p>If a CSI Driver supports Volume Health Monitoring feature from the controller side, an event will be reported on the related <a class=glossary-tooltip title='Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PersistentVolumeClaim>PersistentVolumeClaim</a> (PVC) when an abnormal volume condition is detected on a CSI volume.</p><p>The External Health Monitor <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> also watches for node failure events. You can enable node failure monitoring by setting the <code>enable-node-watcher</code> flag to true. When the external health monitor detects a node failure event, the controller reports an Event will be reported on the PVC to indicate that pods using this PVC are on a failed node.</p><p>If a CSI Driver supports Volume Health Monitoring feature from the node side, an Event will be reported on every Pod using the PVC when an abnormal volume condition is detected on a CSI volume. In addition, Volume Health information is exposed as Kubelet VolumeStats metrics. A new metric kubelet_volume_stats_health_status_abnormal is added. This metric includes two labels: <code>namespace</code> and <code>persistentvolumeclaim</code>. The count is either 1 or 0. 1 indicates the volume is unhealthy, 0 indicates volume is healthy. For more information, please check <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-storage/1432-volume-health-monitor#kubelet-metrics-changes>KEP</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You need to enable the <code>CSIVolumeHealth</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> to use this feature from the node side.</div><h2 id=what-s-next>What's next</h2><p>See the <a href=https://kubernetes-csi.github.io/docs/drivers.html>CSI driver documentation</a> to find out which CSI drivers have implemented this feature.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-055a8df536f8ba8f3aa0217bd2db5437>7.13 - Windows Storage</h1><p>This page provides an storage overview specific to the Windows operating system.</p><h2 id=storage>Persistent storage</h2><p>Windows has a layered filesystem driver to mount container layers and create a copy
filesystem based on NTFS. All file paths in the container are resolved only within
the context of that container.</p><ul><li>With Docker, volume mounts can only target a directory in the container, and not
an individual file. This limitation does not apply to containerd.</li><li>Volume mounts cannot project files or directories back to the host filesystem.</li><li>Read-only filesystems are not supported because write access is always required
for the Windows registry and SAM database. However, read-only volumes are supported.</li><li>Volume user-masks and permissions are not available. Because the SAM is not shared
between the host & container, there's no mapping between them. All permissions are
resolved within the context of the container.</li></ul><p>As a result, the following storage functionality is not supported on Windows nodes:</p><ul><li>Volume subpath mounts: only the entire volume can be mounted in a Windows container</li><li>Subpath volume mounting for Secrets</li><li>Host mount projection</li><li>Read-only root filesystem (mapped volumes still support <code>readOnly</code>)</li><li>Block device mapping</li><li>Memory as the storage medium (for example, <code>emptyDir.medium</code> set to <code>Memory</code>)</li><li>File system features like uid/gid; per-user Linux filesystem permissions</li><li>Setting <a href=/docs/concepts/configuration/secret/#secret-files-permissions>secret permissions with DefaultMode</a> (due to UID/GID dependency)</li><li>NFS based storage/volume support</li><li>Expanding the mounted volume (resizefs)</li></ul><p>Kubernetes <a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volumes>volumes</a> enable complex
applications, with data persistence and Pod volume sharing requirements, to be deployed
on Kubernetes. Management of persistent volumes associated with a specific storage
back-end or protocol includes actions such as provisioning/de-provisioning/resizing
of volumes, attaching/detaching a volume to/from a Kubernetes node and
mounting/dismounting a volume to/from individual containers in a pod that needs to
persist data.</p><p>Volume management components are shipped as Kubernetes volume
<a href=/docs/concepts/storage/volumes/#types-of-volumes>plugin</a>.
The following broad classes of Kubernetes volume plugins are supported on Windows:</p><ul><li><a href=/docs/concepts/storage/volumes/#flexvolume><code>FlexVolume plugins</code></a><ul><li>Please note that FlexVolumes have been deprecated as of 1.23</li></ul></li><li><a href=/docs/concepts/storage/volumes/#csi><code>CSI Plugins</code></a></li></ul><h5 id=in-tree-volume-plugins>In-tree volume plugins</h5><p>The following in-tree plugins support persistent storage on Windows nodes:</p><ul><li><a href=/docs/concepts/storage/volumes/#awselasticblockstore><code>awsElasticBlockStore</code></a></li><li><a href=/docs/concepts/storage/volumes/#azuredisk><code>azureDisk</code></a></li><li><a href=/docs/concepts/storage/volumes/#azurefile><code>azureFile</code></a></li><li><a href=/docs/concepts/storage/volumes/#gcepersistentdisk><code>gcePersistentDisk</code></a></li><li><a href=/docs/concepts/storage/volumes/#vspherevolume><code>vsphereVolume</code></a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-275bea454e1cf4c5adeca4058b5af988>8 - Configuration</h1><div class=lead>Resources that Kubernetes provides for configuring Pods.</div></div><div class=td-content><h1 id=pg-ddef6fd0e47bb51c6f05e8e7fb11d2dd>8.1 - Configuration Best Practices</h1><p>This document highlights and consolidates configuration best practices that are introduced
throughout the user guide, Getting Started documentation, and examples.</p><p>This is a living document. If you think of something that is not on this list but might be useful
to others, please don't hesitate to file an issue or submit a PR.</p><h2 id=general-configuration-tips>General Configuration Tips</h2><ul><li><p>When defining configurations, specify the latest stable API version.</p></li><li><p>Configuration files should be stored in version control before being pushed to the cluster. This
allows you to quickly roll back a configuration change if necessary. It also aids cluster
re-creation and restoration.</p></li><li><p>Write your configuration files using YAML rather than JSON. Though these formats can be used
interchangeably in almost all scenarios, YAML tends to be more user-friendly.</p></li><li><p>Group related objects into a single file whenever it makes sense. One file is often easier to
manage than several. See the
<a href=https://github.com/kubernetes/examples/tree/master/guestbook/all-in-one/guestbook-all-in-one.yaml>guestbook-all-in-one.yaml</a>
file as an example of this syntax.</p></li><li><p>Note also that many <code>kubectl</code> commands can be called on a directory. For example, you can call
<code>kubectl apply</code> on a directory of config files.</p></li><li><p>Don't specify default values unnecessarily: simple, minimal configuration will make errors less likely.</p></li><li><p>Put object descriptions in annotations, to allow better introspection.</p></li></ul><h2 id=naked-pods-vs-replicasets-deployments-and-jobs>"Naked" Pods versus ReplicaSets, Deployments, and Jobs</h2><ul><li><p>Don't use naked Pods (that is, Pods not bound to a <a href=/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a> or
<a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a>) if you can avoid it. Naked Pods
will not be rescheduled in the event of a node failure.</p><p>A Deployment, which both creates a ReplicaSet to ensure that the desired number of Pods is
always available, and specifies a strategy to replace Pods (such as
<a href=/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment>RollingUpdate</a>), is
almost always preferable to creating Pods directly, except for some explicit
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>restartPolicy: Never</code></a> scenarios.
A <a href=/docs/concepts/workloads/controllers/job/>Job</a> may also be appropriate.</p></li></ul><h2 id=services>Services</h2><ul><li><p>Create a <a href=/docs/concepts/services-networking/service/>Service</a> before its corresponding backend
workloads (Deployments or ReplicaSets), and before any workloads that need to access it.
When Kubernetes starts a container, it provides environment variables pointing to all the Services
which were running when the container was started. For example, if a Service named <code>foo</code> exists,
all containers will get the following variables in their initial environment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_HOST</span><span style=color:#666>=</span>&lt;the host the Service is running on&gt;
</span></span><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_PORT</span><span style=color:#666>=</span>&lt;the port the Service is running on&gt;
</span></span></code></pre></div><p><em>This does imply an ordering requirement</em> - any <code>Service</code> that a <code>Pod</code> wants to access must be
created before the <code>Pod</code> itself, or else the environment variables will not be populated.
DNS does not have this restriction.</p></li><li><p>An optional (though strongly recommended) <a href=/docs/concepts/cluster-administration/addons/>cluster add-on</a>
is a DNS server. The DNS server watches the Kubernetes API for new <code>Services</code> and creates a set
of DNS records for each. If DNS has been enabled throughout the cluster then all <code>Pods</code> should be
able to do name resolution of <code>Services</code> automatically.</p></li><li><p>Don't specify a <code>hostPort</code> for a Pod unless it is absolutely necessary. When you bind a Pod to a
<code>hostPort</code>, it limits the number of places the Pod can be scheduled, because each &lt;<code>hostIP</code>,
<code>hostPort</code>, <code>protocol</code>> combination must be unique. If you don't specify the <code>hostIP</code> and
<code>protocol</code> explicitly, Kubernetes will use <code>0.0.0.0</code> as the default <code>hostIP</code> and <code>TCP</code> as the
default <code>protocol</code>.</p><p>If you only need access to the port for debugging purposes, you can use the
<a href=/docs/tasks/access-application-cluster/access-cluster/#manually-constructing-apiserver-proxy-urls>apiserver proxy</a>
or <a href=/docs/tasks/access-application-cluster/port-forward-access-application-cluster/><code>kubectl port-forward</code></a>.</p><p>If you explicitly need to expose a Pod's port on the node, consider using a
<a href=/docs/concepts/services-networking/service/#type-nodeport>NodePort</a> Service before resorting to
<code>hostPort</code>.</p></li><li><p>Avoid using <code>hostNetwork</code>, for the same reasons as <code>hostPort</code>.</p></li><li><p>Use <a href=/docs/concepts/services-networking/service/#headless-services>headless Services</a>
(which have a <code>ClusterIP</code> of <code>None</code>) for service discovery when you don't need <code>kube-proxy</code>
load balancing.</p></li></ul><h2 id=using-labels>Using Labels</h2><ul><li>Define and use <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a> that identify
<strong>semantic attributes</strong> of your application or Deployment, such as <code>{ app.kubernetes.io/name: MyApp, tier: frontend, phase: test, deployment: v3 }</code>. You can use these labels to select the
appropriate Pods for other resources; for example, a Service that selects all <code>tier: frontend</code>
Pods, or all <code>phase: test</code> components of <code>app.kubernetes.io/name: MyApp</code>.
See the <a href=https://github.com/kubernetes/examples/tree/master/guestbook/>guestbook</a> app
for examples of this approach.</li></ul><p>A Service can be made to span multiple Deployments by omitting release-specific labels from its
selector. When you need to update a running service without downtime, use a
<a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a>.</p><p>A desired state of an object is described by a Deployment, and if changes to that spec are
<em>applied</em>, the deployment controller changes the actual state to the desired state at a controlled
rate.</p><ul><li><p>Use the <a href=/docs/concepts/overview/working-with-objects/common-labels/>Kubernetes common labels</a>
for common use cases. These standardized labels enrich the metadata in a way that allows tools,
including <code>kubectl</code> and <a href=/docs/tasks/access-application-cluster/web-ui-dashboard>dashboard</a>, to
work in an interoperable way.</p></li><li><p>You can manipulate labels for debugging. Because Kubernetes controllers (such as ReplicaSet) and
Services match to Pods using selector labels, removing the relevant labels from a Pod will stop
it from being considered by a controller or from being served traffic by a Service. If you remove
the labels of an existing Pod, its controller will create a new Pod to take its place. This is a
useful way to debug a previously "live" Pod in a "quarantine" environment. To interactively remove
or add labels, use <a href=/docs/reference/generated/kubectl/kubectl-commands#label><code>kubectl label</code></a>.</p></li></ul><h2 id=using-kubectl>Using kubectl</h2><ul><li><p>Use <code>kubectl apply -f &lt;directory></code>. This looks for Kubernetes configuration in all <code>.yaml</code>,
<code>.yml</code>, and <code>.json</code> files in <code>&lt;directory></code> and passes it to <code>apply</code>.</p></li><li><p>Use label selectors for <code>get</code> and <code>delete</code> operations instead of specific object names. See the
sections on <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>label selectors</a>
and <a href=/docs/concepts/cluster-administration/manage-deployment/#using-labels-effectively>using labels effectively</a>.</p></li><li><p>Use <code>kubectl create deployment</code> and <code>kubectl expose</code> to quickly create single-container
Deployments and Services.
See <a href=/docs/tasks/access-application-cluster/service-access-application-cluster/>Use a Service to Access an Application in a Cluster</a>
for an example.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6b5ccadd699df0904e8e9917c5450c4b>8.2 - ConfigMaps</h1><p><p>A ConfigMap is an API object used to store non-confidential data in key-value pairs.
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> can consume ConfigMaps as
environment variables, command-line arguments, or as configuration files in a
<a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a>.</p></p><p>A ConfigMap allows you to decouple environment-specific configuration from your <a class=glossary-tooltip title='Stored instance of a container that holds a set of software needed to run an application.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-image' target=_blank aria-label='container images'>container images</a>, so that your applications are easily portable.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> ConfigMap does not provide secrecy or encryption.
If the data you want to store are confidential, use a
<a class=glossary-tooltip title='Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a> rather than a ConfigMap,
or use additional (third party) tools to keep your data private.</div><h2 id=motivation>Motivation</h2><p>Use a ConfigMap for setting configuration data separately from application code.</p><p>For example, imagine that you are developing an application that you can run on your
own computer (for development) and in the cloud (to handle real traffic).
You write the code to look in an environment variable named <code>DATABASE_HOST</code>.
Locally, you set that variable to <code>localhost</code>. In the cloud, you set it to
refer to a Kubernetes <a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>
that exposes the database component to your cluster.
This lets you fetch a container image running in the cloud and
debug the exact same code locally if needed.</p><p>A ConfigMap is not designed to hold large chunks of data. The data stored in a
ConfigMap cannot exceed 1 MiB. If you need to store settings that are
larger than this limit, you may want to consider mounting a volume or use a
separate database or file service.</p><h2 id=configmap-object>ConfigMap object</h2><p>A ConfigMap is an API <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/>object</a>
that lets you store configuration for other objects to use. Unlike most
Kubernetes objects that have a <code>spec</code>, a ConfigMap has <code>data</code> and <code>binaryData</code>
fields. These fields accept key-value pairs as their values. Both the <code>data</code>
field and the <code>binaryData</code> are optional. The <code>data</code> field is designed to
contain UTF-8 strings while the <code>binaryData</code> field is designed to
contain binary data as base64-encoded strings.</p><p>The name of a ConfigMap must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>Each key under the <code>data</code> or the <code>binaryData</code> field must consist of
alphanumeric characters, <code>-</code>, <code>_</code> or <code>.</code>. The keys stored in <code>data</code> must not
overlap with the keys in the <code>binaryData</code> field.</p><p>Starting from v1.19, you can add an <code>immutable</code> field to a ConfigMap
definition to create an <a href=#configmap-immutable>immutable ConfigMap</a>.</p><h2 id=configmaps-and-pods>ConfigMaps and Pods</h2><p>You can write a Pod <code>spec</code> that refers to a ConfigMap and configures the container(s)
in that Pod based on the data in the ConfigMap. The Pod and the ConfigMap must be in
the same <a class=glossary-tooltip title='An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>spec</code> of a <a class=glossary-tooltip title='A pod managed directly by the kubelet daemon on a specific node.' data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/static-pod/ target=_blank aria-label='static Pod'>static Pod</a> cannot refer to a ConfigMap
or any other API objects.</div><p>Here's an example ConfigMap that has some keys with single values,
and other keys where the value looks like a fragment of a configuration
format.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># property-like keys; each key maps to a simple value</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>player_initial_lives</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;3&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ui_properties_file_name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user-interface.properties&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># file-like keys</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>game.properties</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    enemy.types=aliens,monsters
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    player.maximum-lives=5</span><span style=color:#bbb>    
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>user-interface.properties</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    color.good=purple
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    color.bad=yellow
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    allow.textmode=true</span><span style=color:#bbb>    
</span></span></span></code></pre></div><p>There are four different ways that you can use a ConfigMap to configure
a container inside a Pod:</p><ol><li>Inside a container command and args</li><li>Environment variables for a container</li><li>Add a file in read-only volume, for the application to read</li><li>Write code to run inside the Pod that uses the Kubernetes API to read a ConfigMap</li></ol><p>These different methods lend themselves to different ways of modeling
the data being consumed.
For the first three methods, the
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> uses the data from
the ConfigMap when it launches container(s) for a Pod.</p><p>The fourth method means you have to write code to read the ConfigMap and its data.
However, because you're using the Kubernetes API directly, your application can
subscribe to get updates whenever the ConfigMap changes, and react
when that happens. By accessing the Kubernetes API directly, this
technique also lets you access a ConfigMap in a different namespace.</p><p>Here's an example Pod that uses values from <code>game-demo</code> to configure a Pod:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/configmap/configure-pod.yaml download=configmap/configure-pod.yaml><code>configmap/configure-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("configmap-configure-pod-yaml")' title="Copy configmap/configure-pod.yaml to clipboard"></img></div><div class=includecode id=configmap-configure-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>configmap-demo-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>alpine<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;sleep&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;3600&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:#080;font-style:italic># Define the environment variable</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PLAYER_INITIAL_LIVES<span style=color:#bbb> </span><span style=color:#080;font-style:italic># Notice that the case is different here</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                                     </span><span style=color:#080;font-style:italic># from the key name in the ConfigMap.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>configMapKeyRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo          <span style=color:#bbb> </span><span style=color:#080;font-style:italic># The ConfigMap this value comes from.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>player_initial_lives<span style=color:#bbb> </span><span style=color:#080;font-style:italic># The key to fetch.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>UI_PROPERTIES_FILE_NAME<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>configMapKeyRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>ui_properties_file_name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/config&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># You set volumes at the Pod level, then mount them into containers inside that Pod</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># Provide the name of the ConfigMap you want to mount.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>game-demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># An array of keys from the ConfigMap to create as files</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;game.properties&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;game.properties&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user-interface.properties&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;user-interface.properties&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span></span></span></code></pre></div></div></div><p>A ConfigMap doesn't differentiate between single line property values and
multi-line file-like values.
What matters is how Pods and other objects consume those values.</p><p>For this example, defining a volume and mounting it inside the <code>demo</code>
container as <code>/config</code> creates two files,
<code>/config/game.properties</code> and <code>/config/user-interface.properties</code>,
even though there are four keys in the ConfigMap. This is because the Pod
definition specifies an <code>items</code> array in the <code>volumes</code> section.
If you omit the <code>items</code> array entirely, every key in the ConfigMap becomes
a file with the same name as the key, and you get 4 files.</p><h2 id=using-configmaps>Using ConfigMaps</h2><p>ConfigMaps can be mounted as data volumes. ConfigMaps can also be used by other
parts of the system, without being directly exposed to the Pod. For example,
ConfigMaps can hold data that other parts of the system should use for configuration.</p><p>The most common way to use ConfigMaps is to configure settings for
containers running in a Pod in the same namespace. You can also use a
ConfigMap separately.</p><p>For example, you
might encounter <a class=glossary-tooltip title='Resources that extend the functionality of Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/cluster-administration/addons/ target=_blank aria-label=addons>addons</a>
or <a class=glossary-tooltip title='A specialized controller used to manage a custom resource' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/operator/ target=_blank aria-label=operators>operators</a> that
adjust their behavior based on a ConfigMap.</p><h3 id=using-configmaps-as-files-from-a-pod>Using ConfigMaps as files from a Pod</h3><p>To consume a ConfigMap in a volume in a Pod:</p><ol><li>Create a ConfigMap or use an existing one. Multiple Pods can reference the
same ConfigMap.</li><li>Modify your Pod definition to add a volume under <code>.spec.volumes[]</code>. Name
the volume anything, and have a <code>.spec.volumes[].configMap.name</code> field set
to reference your ConfigMap object.</li><li>Add a <code>.spec.containers[].volumeMounts[]</code> to each container that needs the
ConfigMap. Specify <code>.spec.containers[].volumeMounts[].readOnly = true</code> and
<code>.spec.containers[].volumeMounts[].mountPath</code> to an unused directory name
where you would like the ConfigMap to appear.</li><li>Modify your image or command line so that the program looks for files in
that directory. Each key in the ConfigMap <code>data</code> map becomes the filename
under <code>mountPath</code>.</li></ol><p>This is an example of a Pod that mounts a ConfigMap in a volume:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myconfigmap<span style=color:#bbb>
</span></span></span></code></pre></div><p>Each ConfigMap you want to use needs to be referred to in <code>.spec.volumes</code>.</p><p>If there are multiple containers in the Pod, then each container needs its
own <code>volumeMounts</code> block, but only one <code>.spec.volumes</code> is needed per ConfigMap.</p><h4 id=mounted-configmaps-are-updated-automatically>Mounted ConfigMaps are updated automatically</h4><p>When a ConfigMap currently consumed in a volume is updated, projected keys are eventually updated as well.
The kubelet checks whether the mounted ConfigMap is fresh on every periodic sync.
However, the kubelet uses its local cache for getting the current value of the ConfigMap.
The type of the cache is configurable using the <code>ConfigMapAndSecretChangeDetectionStrategy</code> field in
the <a href=/docs/reference/config-api/kubelet-config.v1beta1/>KubeletConfiguration struct</a>.
A ConfigMap can be either propagated by watch (default), ttl-based, or by redirecting
all requests directly to the API server.
As a result, the total delay from the moment when the ConfigMap is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(it equals to watch propagation delay, ttl of cache, or zero correspondingly).</p><p>ConfigMaps consumed as environment variables are not updated automatically and require a pod restart.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A container using a ConfigMap as a <a href=/docs/concepts/storage/volumes#using-subpath>subPath</a> volume mount will not receive ConfigMap updates.</div><h2 id=configmap-immutable>Immutable ConfigMaps</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>The Kubernetes feature <em>Immutable Secrets and ConfigMaps</em> provides an option to set
individual Secrets and ConfigMaps as immutable. For clusters that extensively use ConfigMaps
(at least tens of thousands of unique ConfigMap to Pod mounts), preventing changes to their
data has the following advantages:</p><ul><li>protects you from accidental (or unwanted) updates that could cause applications outages</li><li>improves performance of your cluster by significantly reducing load on kube-apiserver, by
closing watches for ConfigMaps marked as immutable.</li></ul><p>This feature is controlled by the <code>ImmutableEphemeralVolumes</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>.
You can create an immutable ConfigMap by setting the <code>immutable</code> field to <code>true</code>.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>immutable</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Once a ConfigMap is marked as immutable, it is <em>not</em> possible to revert this change
nor to mutate the contents of the <code>data</code> or the <code>binaryData</code> field. You can
only delete and recreate the ConfigMap. Because existing Pods maintain a mount point
to the deleted ConfigMap, it is recommended to recreate these pods.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/configuration/secret/>Secrets</a>.</li><li>Read <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>Configure a Pod to Use a ConfigMap</a>.</li><li>Read about <a href=/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/>changing a ConfigMap (or any other Kubernetes object)</a></li><li>Read <a href=https://12factor.net/>The Twelve-Factor App</a> to understand the motivation for
separating code from configuration.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e511ed821ada65d0053341dbd8ad2bb5>8.3 - Secrets</h1><p>A Secret is an object that contains a small amount of sensitive data such as
a password, a token, or a key. Such information might otherwise be put in a
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> specification or in a
<a class=glossary-tooltip title='Stored instance of a container that holds a set of software needed to run an application.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-image' target=_blank aria-label='container image'>container image</a>. Using a
Secret means that you don't need to include confidential data in your
application code.</p><p>Because Secrets can be created independently of the Pods that use them, there
is less risk of the Secret (and its data) being exposed during the workflow of
creating, viewing, and editing Pods. Kubernetes, and applications that run in
your cluster, can also take additional precautions with Secrets, such as avoiding
writing secret data to nonvolatile storage.</p><p>Secrets are similar to <a class=glossary-tooltip title='An API object used to store non-confidential data in key-value pairs. Can be consumed as environment variables, command-line arguments, or configuration files in a volume.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/configmap/ target=_blank aria-label=ConfigMaps>ConfigMaps</a>
but are specifically intended to hold confidential data.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>Kubernetes Secrets are, by default, stored unencrypted in the API server's underlying data store
(etcd). Anyone with API access can retrieve or modify a Secret, and so can anyone with access to etcd.
Additionally, anyone who is authorized to create a Pod in a namespace can use that access to read
any Secret in that namespace; this includes indirect access such as the ability to create a
Deployment.</p><p>In order to safely use Secrets, take at least the following steps:</p><ol><li><a href=/docs/tasks/administer-cluster/encrypt-data/>Enable Encryption at Rest</a> for Secrets.</li><li><a href=/docs/reference/access-authn-authz/authorization/>Enable or configure RBAC rules</a> with
least-privilege access to Secrets.</li><li>Restrict Secret access to specific containers.</li><li><a href=https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver>Consider using external Secret store providers</a>.</li></ol><p>For more guidelines to manage and improve the security of your Secrets, refer to
<a href=/docs/concepts/security/secrets-good-practices>Good practices for Kubernetes Secrets</a>.</p></div><p>See <a href=#information-security-for-secrets>Information security for Secrets</a> for more details.</p><h2 id=uses-for-secrets>Uses for Secrets</h2><p>There are three main ways for a Pod to use a Secret:</p><ul><li>As <a href=#using-secrets-as-files-from-a-pod>files</a> in a
<a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a> mounted on one or more of
its containers.</li><li>As <a href=#using-secrets-as-environment-variables>container environment variable</a>.</li><li>By the <a href=#using-imagepullsecrets>kubelet when pulling images</a> for the Pod.</li></ul><p>The Kubernetes control plane also uses Secrets; for example,
<a href=#bootstrap-token-secrets>bootstrap token Secrets</a> are a mechanism to
help automate node registration.</p><h3 id=alternatives-to-secrets>Alternatives to Secrets</h3><p>Rather than using a Secret to protect confidential data, you can pick from alternatives.</p><p>Here are some of your options:</p><ul><li>if your cloud-native component needs to authenticate to another application that you
know is running within the same Kubernetes cluster, you can use a
<a href=/docs/reference/access-authn-authz/authentication/#service-account-tokens>ServiceAccount</a>
and its tokens to identify your client.</li><li>there are third-party tools that you can run, either within or outside your cluster,
that provide secrets management. For example, a service that Pods access over HTTPS,
that reveals a secret if the client correctly authenticates (for example, with a ServiceAccount
token).</li><li>for authentication, you can implement a custom signer for X.509 certificates, and use
<a href=/docs/reference/access-authn-authz/certificate-signing-requests/>CertificateSigningRequests</a>
to let that custom signer issue certificates to Pods that need them.</li><li>you can use a <a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>device plugin</a>
to expose node-local encryption hardware to a specific Pod. For example, you can schedule
trusted Pods onto nodes that provide a Trusted Platform Module, configured out-of-band.</li></ul><p>You can also combine two or more of those options, including the option to use Secret objects themselves.</p><p>For example: implement (or deploy) an <a class=glossary-tooltip title='A specialized controller used to manage a custom resource' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/operator/ target=_blank aria-label=operator>operator</a>
that fetches short-lived session tokens from an external service, and then creates Secrets based
on those short-lived session tokens. Pods running in your cluster can make use of the session tokens,
and operator ensures they are valid. This separation means that you can run Pods that are unaware of
the exact mechanisms for issuing and refreshing those session tokens.</p><h2 id=working-with-secrets>Working with Secrets</h2><h3 id=creating-a-secret>Creating a Secret</h3><p>There are several options to create a Secret:</p><ul><li><a href=/docs/tasks/configmap-secret/managing-secret-using-kubectl/>Use <code>kubectl</code></a></li><li><a href=/docs/tasks/configmap-secret/managing-secret-using-config-file/>Use a configuration file</a></li><li><a href=/docs/tasks/configmap-secret/managing-secret-using-kustomize/>Use the Kustomize tool</a></li></ul><h4 id=restriction-names-data>Constraints on Secret names and data</h4><p>The name of a Secret object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>You can specify the <code>data</code> and/or the <code>stringData</code> field when creating a
configuration file for a Secret. The <code>data</code> and the <code>stringData</code> fields are optional.
The values for all keys in the <code>data</code> field have to be base64-encoded strings.
If the conversion to base64 string is not desirable, you can choose to specify
the <code>stringData</code> field instead, which accepts arbitrary strings as values.</p><p>The keys of <code>data</code> and <code>stringData</code> must consist of alphanumeric characters,
<code>-</code>, <code>_</code> or <code>.</code>. All key-value pairs in the <code>stringData</code> field are internally
merged into the <code>data</code> field. If a key appears in both the <code>data</code> and the
<code>stringData</code> field, the value specified in the <code>stringData</code> field takes
precedence.</p><h4 id=restriction-data-size>Size limit</h4><p>Individual secrets are limited to 1MiB in size. This is to discourage creation
of very large secrets that could exhaust the API server and kubelet memory.
However, creation of many smaller secrets could also exhaust memory. You can
use a <a href=/docs/concepts/policy/resource-quotas/>resource quota</a> to limit the
number of Secrets (or other resources) in a namespace.</p><h3 id=editing-a-secret>Editing a Secret</h3><p>You can edit an existing Secret unless it is <a href=#secret-immutable>immutable</a>. To
edit a Secret, use one of the following methods:</p><ul><li><a href=/docs/tasks/configmap-secret/managing-secret-using-kubectl/#edit-secret>Use <code>kubectl</code></a></li><li><a href=/docs/tasks/configmap-secret/managing-secret-using-config-file/#edit-secret>Use a configuration file</a></li></ul><p>You can also edit the data in a Secret using the <a href=/docs/tasks/configmap-secret/managing-secret-using-kustomize/#edit-secret>Kustomize tool</a>. However, this
method creates a new <code>Secret</code> object with the edited data.</p><p>Depending on how you created the Secret, as well as how the Secret is used in
your Pods, updates to existing <code>Secret</code> objects are propagated automatically to
Pods that use the data. For more information, refer to <a href=#mounted-secrets-are-updated-automatically>Mounted Secrets are updated automatically</a>.</p><h3 id=using-a-secret>Using a Secret</h3><p>Secrets can be mounted as data volumes or exposed as
<a class=glossary-tooltip title='Container environment variables are name=value pairs that provide useful information into containers running in a Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/containers/container-environment/ target=_blank aria-label='environment variables'>environment variables</a>
to be used by a container in a Pod. Secrets can also be used by other parts of the
system, without being directly exposed to the Pod. For example, Secrets can hold
credentials that other parts of the system should use to interact with external
systems on your behalf.</p><p>Secret volume sources are validated to ensure that the specified object
reference actually points to an object of type Secret. Therefore, a Secret
needs to be created before any Pods that depend on it.</p><p>If the Secret cannot be fetched (perhaps because it does not exist, or
due to a temporary lack of connection to the API server) the kubelet
periodically retries running that Pod. The kubelet also reports an Event
for that Pod, including details of the problem fetching the Secret.</p><h4 id=restriction-secret-must-exist>Optional Secrets</h4><p>When you define a container environment variable based on a Secret,
you can mark it as <em>optional</em>. The default is for the Secret to be
required.</p><p>None of a Pod's containers will start until all non-optional Secrets are
available.</p><p>If a Pod references a specific key in a Secret and that Secret does exist, but
is missing the named key, the Pod fails during startup.</p><h3 id=using-secrets-as-files-from-a-pod>Using Secrets as files from a Pod</h3><p>If you want to access data from a Secret in a Pod, one way to do that is to
have Kubernetes make the value of that Secret be available as a file inside
the filesystem of one or more of the Pod's containers.</p><p>To configure that, you:</p><ol><li>Create a secret or use an existing one. Multiple Pods can reference the same secret.</li><li>Modify your Pod definition to add a volume under <code>.spec.volumes[]</code>. Name the volume anything,
and have a <code>.spec.volumes[].secret.secretName</code> field equal to the name of the Secret object.</li><li>Add a <code>.spec.containers[].volumeMounts[]</code> to each container that needs the secret. Specify
<code>.spec.containers[].volumeMounts[].readOnly = true</code> and
<code>.spec.containers[].volumeMounts[].mountPath</code> to an unused directory name where you would like the
secrets to appear.</li><li>Modify your image or command line so that the program looks for files in that directory. Each
key in the secret <code>data</code> map becomes the filename under <code>mountPath</code>.</li></ol><p>This is an example of a Pod that mounts a Secret named <code>mysecret</code> in a volume:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>optional</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># default setting; &#34;mysecret&#34; must exist</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Each Secret you want to use needs to be referred to in <code>.spec.volumes</code>.</p><p>If there are multiple containers in the Pod, then each container needs its
own <code>volumeMounts</code> block, but only one <code>.spec.volumes</code> is needed per Secret.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Versions of Kubernetes before v1.22 automatically created credentials for accessing
the Kubernetes API. This older mechanism was based on creating token Secrets that
could then be mounted into running Pods.
In more recent versions, including Kubernetes v1.25, API credentials
are obtained directly by using the <a href=/docs/reference/kubernetes-api/authentication-resources/token-request-v1/>TokenRequest</a> API,
and are mounted into Pods using a <a href=/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume>projected volume</a>.
The tokens obtained using this method have bounded lifetimes, and are automatically
invalidated when the Pod they are mounted into is deleted.</p><p>You can still <a href=/docs/tasks/configure-pod-container/configure-service-account/#manually-create-a-service-account-api-token>manually create</a>
a service account token Secret; for example, if you need a token that never expires.
However, using the <a href=/docs/reference/kubernetes-api/authentication-resources/token-request-v1/>TokenRequest</a>
subresource to obtain a token to access the API is recommended instead.
You can use the <a href=/docs/reference/generated/kubectl/kubectl-commands#-em-token-em-><code>kubectl create token</code></a>
command to obtain a token from the <code>TokenRequest</code> API.</p></div><h4 id=projection-of-secret-keys-to-specific-paths>Projection of Secret keys to specific paths</h4><p>You can also control the paths within the volume where Secret keys are projected.
You can use the <code>.spec.volumes[].secret.items</code> field to change the target path of each key:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>my-group/my-username<span style=color:#bbb>
</span></span></span></code></pre></div><p>What will happen:</p><ul><li>the <code>username</code> key from <code>mysecret</code> is available to the container at the path
<code>/etc/foo/my-group/my-username</code> instead of at <code>/etc/foo/username</code>.</li><li>the <code>password</code> key from that Secret object is not projected.</li></ul><p>If <code>.spec.volumes[].secret.items</code> is used, only keys specified in <code>items</code> are projected.
To consume all keys from the Secret, all of them must be listed in the <code>items</code> field.</p><p>If you list keys explicitly, then all listed keys must exist in the corresponding Secret.
Otherwise, the volume is not created.</p><h4 id=secret-files-permissions>Secret files permissions</h4><p>You can set the POSIX file access permission bits for a single Secret key.
If you don't specify any permissions, <code>0644</code> is used by default.
You can also set a default mode for the entire Secret volume and override per key if needed.</p><p>For example, you can specify a default mode like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/foo&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>defaultMode</span>:<span style=color:#bbb> </span><span style=color:#666>0400</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The secret is mounted on <code>/etc/foo</code>; all the files created by the
secret volume mount have permission <code>0400</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you're defining a Pod or a Pod template using JSON, beware that the JSON
specification doesn't support octal notation. You can use the decimal value
for the <code>defaultMode</code> (for example, 0400 in octal is 256 in decimal) instead.
If you're writing YAML, you can write the <code>defaultMode</code> in octal.</div><h4 id=consuming-secret-values-from-volumes>Consuming Secret values from volumes</h4><p>Inside the container that mounts a secret volume, the secret keys appear as
files. The secret values are base64 decoded and stored inside these files.</p><p>This is the result of commands executed inside the container from the example above:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>ls /etc/foo/
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>username
password
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat /etc/foo/username
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>admin
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat /etc/foo/password
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>1f2d1e2e67df
</code></pre><p>The program in a container is responsible for reading the secret data from these
files, as needed.</p><h4 id=mounted-secrets-are-updated-automatically>Mounted Secrets are updated automatically</h4><p>When a volume contains data from a Secret, and that Secret is updated, Kubernetes tracks
this and updates the data in the volume, using an eventually-consistent approach.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A container using a Secret as a
<a href=/docs/concepts/storage/volumes#using-subpath>subPath</a> volume mount does not receive
automated Secret updates.</div><p>The kubelet keeps a cache of the current keys and values for the Secrets that are used in
volumes for pods on that node.
You can configure the way that the kubelet detects changes from the cached values. The
<code>configMapAndSecretChangeDetectionStrategy</code> field in the
<a href=/docs/reference/config-api/kubelet-config.v1beta1/>kubelet configuration</a> controls
which strategy the kubelet uses. The default strategy is <code>Watch</code>.</p><p>Updates to Secrets can be either propagated by an API watch mechanism (the default), based on
a cache with a defined time-to-live, or polled from the cluster API server on each kubelet
synchronisation loop.</p><p>As a result, the total delay from the moment when the Secret is updated to the moment
when new keys are projected to the Pod can be as long as the kubelet sync period + cache
propagation delay, where the cache propagation delay depends on the chosen cache type
(following the same order listed in the previous paragraph, these are:
watch propagation delay, the configured cache TTL, or zero for direct polling).</p><h3 id=using-secrets-as-environment-variables>Using Secrets as environment variables</h3><p>To use a Secret in an <a class=glossary-tooltip title='Container environment variables are name=value pairs that provide useful information into containers running in a Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/containers/container-environment/ target=_blank aria-label='environment variable'>environment variable</a>
in a Pod:</p><ol><li>Create a Secret (or use an existing one). Multiple Pods can reference the same Secret.</li><li>Modify your Pod definition in each container that you wish to consume the value of a secret
key to add an environment variable for each secret key you wish to consume. The environment
variable that consumes the secret key should populate the secret's name and key in <code>env[].valueFrom.secretKeyRef</code>.</li><li>Modify your image and/or command line so that the program looks for values in the specified
environment variables.</li></ol><p>This is an example of a Pod that uses a Secret via environment variables:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-env-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mycontainer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>SECRET_USERNAME<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secretKeyRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>username<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>optional</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># same as default; &#34;mysecret&#34; must exist</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                            </span><span style=color:#080;font-style:italic># and include a key named &#34;username&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>SECRET_PASSWORD<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>secretKeyRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>password<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>optional</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># same as default; &#34;mysecret&#34; must exist</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                            </span><span style=color:#080;font-style:italic># and include a key named &#34;password&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=restriction-env-from-invalid>Invalid environment variables</h4><p>Secrets used to populate environment variables by the <code>envFrom</code> field that have keys
that are considered invalid environment variable names will have those keys
skipped. The Pod is allowed to start.</p><p>If you define a Pod with an invalid variable name, the failed Pod startup includes
an event with the reason set to <code>InvalidVariableNames</code> and a message that lists the
skipped invalid keys. The following example shows a Pod that refers to a Secret
named <code>mysecret</code>, where <code>mysecret</code> contains 2 invalid keys: <code>1badkey</code> and <code>2alsobad</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get events
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>LASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         TYPE      REASON
0s         0s          1         dapi-test-pod   Pod                                         Warning   InvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.
</code></pre><h4 id=consuming-secret-values-from-environment-variables>Consuming Secret values from environment variables</h4><p>Inside a container that consumes a Secret using environment variables, the secret keys appear
as normal environment variables. The values of those variables are the base64 decoded values
of the secret data.</p><p>This is the result of commands executed inside the container from the example above:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SECRET_USERNAME</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>admin
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b44>&#34;</span><span style=color:#b8860b>$SECRET_PASSWORD</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>1f2d1e2e67df
</code></pre><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If a container already consumes a Secret in an environment variable,
a Secret update will not be seen by the container unless it is
restarted. There are third party solutions for triggering restarts when
secrets change.</div><h3 id=using-imagepullsecrets>Container image pull secrets</h3><p>If you want to fetch container images from a private repository, you need a way for
the kubelet on each node to authenticate to that repository. You can configure
<em>image pull secrets</em> to make this possible. These secrets are configured at the Pod
level.</p><p>The <code>imagePullSecrets</code> field for a Pod is a list of references to Secrets in the same namespace
as the Pod.
You can use an <code>imagePullSecrets</code> to pass image registry access credentials to
the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.
See <code>PodSpec</code> in the <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodSpec>Pod API reference</a>
for more information about the <code>imagePullSecrets</code> field.</p><h4 id=using-imagepullsecrets-1>Using imagePullSecrets</h4><p>The <code>imagePullSecrets</code> field is a list of references to secrets in the same namespace.
You can use an <code>imagePullSecrets</code> to pass a secret that contains a Docker (or other) image registry
password to the kubelet. The kubelet uses this information to pull a private image on behalf of your Pod.
See the <a href=/docs/reference/generated/kubernetes-api/v1.25/#podspec-v1-core>PodSpec API</a>
for more information about the <code>imagePullSecrets</code> field.</p><h5 id=manually-specifying-an-imagepullsecret>Manually specifying an imagePullSecret</h5><p>You can learn how to specify <code>imagePullSecrets</code> from the
<a href=/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod>container images</a>
documentation.</p><h5 id=arranging-for-imagepullsecrets-to-be-automatically-attached>Arranging for imagePullSecrets to be automatically attached</h5><p>You can manually create <code>imagePullSecrets</code>, and reference these from a ServiceAccount. Any Pods
created with that ServiceAccount or created with that ServiceAccount by default, will get their
<code>imagePullSecrets</code> field set to that of the service account.
See <a href=/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account>Add ImagePullSecrets to a service account</a>
for a detailed explanation of that process.</p><h3 id=restriction-static-pod>Using Secrets with static Pods</h3><p>You cannot use ConfigMaps or Secrets with <a class=glossary-tooltip title='A pod managed directly by the kubelet daemon on a specific node.' data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/static-pod/ target=_blank aria-label='static Pods'>static Pods</a>.</p><h2 id=use-cases>Use cases</h2><h3 id=use-case-as-container-environment-variables>Use case: As container environment variables</h3><p>Create a secret</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Opaque<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>USER_NAME</span>:<span style=color:#bbb> </span>YWRtaW4=<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>PASSWORD</span>:<span style=color:#bbb> </span>MWYyZDFlMmU2N2Rm<span style=color:#bbb>
</span></span></span></code></pre></div><p>Create the Secret:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f mysecret.yaml
</span></span></code></pre></div><p>Use <code>envFrom</code> to define all of the Secret's data as container environment variables. The key from
the Secret becomes the environment variable name in the Pod.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-test-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;/bin/sh&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;env&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>envFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>secretRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mysecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=use-case-pod-with-ssh-keys>Use case: Pod with SSH keys</h3><p>Create a Secret containing some SSH keys:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic ssh-key-secret --from-file<span style=color:#666>=</span>ssh-privatekey<span style=color:#666>=</span>/path/to/.ssh/id_rsa --from-file<span style=color:#666>=</span>ssh-publickey<span style=color:#666>=</span>/path/to/.ssh/id_rsa.pub
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>secret &#34;ssh-key-secret&#34; created
</code></pre><p>You can also create a <code>kustomization.yaml</code> with a <code>secretGenerator</code> field containing ssh keys.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>Think carefully before sending your own SSH keys: other users of the cluster may have access
to the Secret.</p><p>You could instead create an SSH private key representing a service identity that you want to be
accessible to all the users with whom you share the Kubernetes cluster, and that you can revoke
if the credentials are compromised.</p></div><p>Now you can create a Pod which references the secret with the SSH key and
consumes it in a volume:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-test-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>ssh-key-secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ssh-test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>mySshImage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/secret-volume&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>When the container's command runs, the pieces of the key will be available in:</p><pre tabindex=0><code>/etc/secret-volume/ssh-publickey
/etc/secret-volume/ssh-privatekey
</code></pre><p>The container is then free to use the secret data to establish an SSH connection.</p><h3 id=use-case-pods-with-prod-test-credentials>Use case: Pods with prod / test credentials</h3><p>This example illustrates a Pod which consumes a secret containing production credentials and
another Pod which consumes a secret with test environment credentials.</p><p>You can create a <code>kustomization.yaml</code> with a <code>secretGenerator</code> field or run
<code>kubectl create secret</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic prod-db-secret --from-literal<span style=color:#666>=</span><span style=color:#b8860b>username</span><span style=color:#666>=</span>produser --from-literal<span style=color:#666>=</span><span style=color:#b8860b>password</span><span style=color:#666>=</span>Y4nys7f11
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>secret &#34;prod-db-secret&#34; created
</code></pre><p>You can also create a secret for test environment credentials.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic test-db-secret --from-literal<span style=color:#666>=</span><span style=color:#b8860b>username</span><span style=color:#666>=</span>testuser --from-literal<span style=color:#666>=</span><span style=color:#b8860b>password</span><span style=color:#666>=</span>iluvtests
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>secret &#34;test-db-secret&#34; created
</code></pre><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Special characters such as <code>$</code>, <code>\</code>, <code>*</code>, <code>=</code>, and <code>!</code> will be interpreted by your
<a href=https://en.wikipedia.org/wiki/Shell_(computing)>shell</a> and require escaping.</p><p>In most shells, the easiest way to escape the password is to surround it with single quotes (<code>'</code>).
For example, if your actual password is <code>S!B\*d$zDsb=</code>, you should execute the command this way:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic dev-db-secret --from-literal<span style=color:#666>=</span><span style=color:#b8860b>username</span><span style=color:#666>=</span>devuser --from-literal<span style=color:#666>=</span><span style=color:#b8860b>password</span><span style=color:#666>=</span><span style=color:#b44>&#39;S!B\*d$zDsb=&#39;</span>
</span></span></code></pre></div><p>You do not need to escape special characters in passwords from files (<code>--from-file</code>).</p></div><p>Now make the Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: List
</span></span></span><span style=display:flex><span><span style=color:#b44>items:
</span></span></span><span style=display:flex><span><span style=color:#b44>- kind: Pod
</span></span></span><span style=display:flex><span><span style=color:#b44>  apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>  metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>    name: prod-db-client-pod
</span></span></span><span style=display:flex><span><span style=color:#b44>    labels:
</span></span></span><span style=display:flex><span><span style=color:#b44>      name: prod-db-client
</span></span></span><span style=display:flex><span><span style=color:#b44>  spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>    volumes:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: secret-volume
</span></span></span><span style=display:flex><span><span style=color:#b44>      secret:
</span></span></span><span style=display:flex><span><span style=color:#b44>        secretName: prod-db-secret
</span></span></span><span style=display:flex><span><span style=color:#b44>    containers:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: db-client-container
</span></span></span><span style=display:flex><span><span style=color:#b44>      image: myClientImage
</span></span></span><span style=display:flex><span><span style=color:#b44>      volumeMounts:
</span></span></span><span style=display:flex><span><span style=color:#b44>      - name: secret-volume
</span></span></span><span style=display:flex><span><span style=color:#b44>        readOnly: true
</span></span></span><span style=display:flex><span><span style=color:#b44>        mountPath: &#34;/etc/secret-volume&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>- kind: Pod
</span></span></span><span style=display:flex><span><span style=color:#b44>  apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>  metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>    name: test-db-client-pod
</span></span></span><span style=display:flex><span><span style=color:#b44>    labels:
</span></span></span><span style=display:flex><span><span style=color:#b44>      name: test-db-client
</span></span></span><span style=display:flex><span><span style=color:#b44>  spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>    volumes:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: secret-volume
</span></span></span><span style=display:flex><span><span style=color:#b44>      secret:
</span></span></span><span style=display:flex><span><span style=color:#b44>        secretName: test-db-secret
</span></span></span><span style=display:flex><span><span style=color:#b44>    containers:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: db-client-container
</span></span></span><span style=display:flex><span><span style=color:#b44>      image: myClientImage
</span></span></span><span style=display:flex><span><span style=color:#b44>      volumeMounts:
</span></span></span><span style=display:flex><span><span style=color:#b44>      - name: secret-volume
</span></span></span><span style=display:flex><span><span style=color:#b44>        readOnly: true
</span></span></span><span style=display:flex><span><span style=color:#b44>        mountPath: &#34;/etc/secret-volume&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><p>Add the pods to the same <code>kustomization.yaml</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt;&gt; kustomization.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>resources:
</span></span></span><span style=display:flex><span><span style=color:#b44>- pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><p>Apply all those objects on the API server by running:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -k .
</span></span></code></pre></div><p>Both containers will have the following files present on their filesystems with the values
for each container's environment:</p><pre tabindex=0><code>/etc/secret-volume/username
/etc/secret-volume/password
</code></pre><p>Note how the specs for the two Pods differ only in one field; this facilitates
creating Pods with different capabilities from a common Pod template.</p><p>You could further simplify the base Pod specification by using two service accounts:</p><ol><li><code>prod-user</code> with the <code>prod-db-secret</code></li><li><code>test-user</code> with the <code>test-db-secret</code></li></ol><p>The Pod specification is shortened to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prod-db-client-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prod-db-client<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceAccount</span>:<span style=color:#bbb> </span>prod-db-client<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>db-client-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>myClientImage<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=use-case-dotfiles-in-a-secret-volume>Use case: dotfiles in a secret volume</h3><p>You can make your data "hidden" by defining a key that begins with a dot.
This key represents a dotfile or "hidden" file. For example, when the following secret
is mounted into a volume, <code>secret-volume</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dotfile-secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>.secret-file</span>:<span style=color:#bbb> </span>dmFsdWUtMg0KDQo=<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-dotfiles-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>secret</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>secretName</span>:<span style=color:#bbb> </span>dotfile-secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>dotfile-test-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/busybox<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- ls<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;-l&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:#b44>&#34;/etc/secret-volume&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>readOnly</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/etc/secret-volume&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The volume will contain a single file, called <code>.secret-file</code>, and
the <code>dotfile-test-container</code> will have this file present at the path
<code>/etc/secret-volume/.secret-file</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Files beginning with dot characters are hidden from the output of <code>ls -l</code>;
you must use <code>ls -la</code> to see them when listing directory contents.</div><h3 id=use-case-secret-visible-to-one-container-in-a-pod>Use case: Secret visible to one container in a Pod</h3><p>Consider a program that needs to handle HTTP requests, do some complex business
logic, and then sign some messages with an HMAC. Because it has complex
application logic, there might be an unnoticed remote file reading exploit in
the server, which could expose the private key to an attacker.</p><p>This could be divided into two processes in two containers: a frontend container
which handles user interaction and business logic, but which cannot see the
private key; and a signer container that can see the private key, and responds
to simple signing requests from the frontend (for example, over localhost networking).</p><p>With this partitioned approach, an attacker now has to trick the application
server into doing something rather arbitrary, which may be harder than getting
it to read a file.</p><h2 id=secret-types>Types of Secret</h2><p>When creating a Secret, you can specify its type using the <code>type</code> field of
the <a href=/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/>Secret</a>
resource, or certain equivalent <code>kubectl</code> command line flags (if available).
The Secret type is used to facilitate programmatic handling of the Secret data.</p><p>Kubernetes provides several built-in types for some common usage scenarios.
These types vary in terms of the validations performed and the constraints
Kubernetes imposes on them.</p><table><thead><tr><th>Built-in Type</th><th>Usage</th></tr></thead><tbody><tr><td><code>Opaque</code></td><td>arbitrary user-defined data</td></tr><tr><td><code>kubernetes.io/service-account-token</code></td><td>ServiceAccount token</td></tr><tr><td><code>kubernetes.io/dockercfg</code></td><td>serialized <code>~/.dockercfg</code> file</td></tr><tr><td><code>kubernetes.io/dockerconfigjson</code></td><td>serialized <code>~/.docker/config.json</code> file</td></tr><tr><td><code>kubernetes.io/basic-auth</code></td><td>credentials for basic authentication</td></tr><tr><td><code>kubernetes.io/ssh-auth</code></td><td>credentials for SSH authentication</td></tr><tr><td><code>kubernetes.io/tls</code></td><td>data for a TLS client or server</td></tr><tr><td><code>bootstrap.kubernetes.io/token</code></td><td>bootstrap token data</td></tr></tbody></table><p>You can define and use your own Secret type by assigning a non-empty string as the
<code>type</code> value for a Secret object (an empty string is treated as an <code>Opaque</code> type).</p><p>Kubernetes doesn't impose any constraints on the type name. However, if you
are using one of the built-in types, you must meet all the requirements defined
for that type.</p><p>If you are defining a type of secret that's for public use, follow the convention
and structure the secret type to have your domain name before the name, separated
by a <code>/</code>. For example: <code>cloud-hosting.example.net/cloud-api-credentials</code>.</p><h3 id=opaque-secrets>Opaque secrets</h3><p><code>Opaque</code> is the default Secret type if omitted from a Secret configuration file.
When you create a Secret using <code>kubectl</code>, you will use the <code>generic</code>
subcommand to indicate an <code>Opaque</code> Secret type. For example, the following
command creates an empty Secret of type <code>Opaque</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic empty-secret
</span></span><span style=display:flex><span>kubectl get secret empty-secret
</span></span></code></pre></div><p>The output looks like:</p><pre tabindex=0><code>NAME           TYPE     DATA   AGE
empty-secret   Opaque   0      2m6s
</code></pre><p>The <code>DATA</code> column shows the number of data items stored in the Secret.
In this case, <code>0</code> means you have created an empty Secret.</p><h3 id=service-account-token-secrets>Service account token Secrets</h3><p>A <code>kubernetes.io/service-account-token</code> type of Secret is used to store a
token credential that identifies a
<a class=glossary-tooltip title='Provides an identity for processes that run in a Pod.' data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/configure-service-account/ target=_blank aria-label='service account'>service account</a>.</p><p>Since 1.22, this type of Secret is no longer used to mount credentials into Pods,
and obtaining tokens via the <a href=/docs/reference/kubernetes-api/authentication-resources/token-request-v1/>TokenRequest</a>
API is recommended instead of using service account token Secret objects.
Tokens obtained from the <code>TokenRequest</code> API are more secure than ones stored in Secret objects,
because they have a bounded lifetime and are not readable by other API clients.
You can use the <a href=/docs/reference/generated/kubectl/kubectl-commands#-em-token-em-><code>kubectl create token</code></a>
command to obtain a token from the <code>TokenRequest</code> API.</p><p>You should only create a service account token Secret object
if you can't use the <code>TokenRequest</code> API to obtain a token,
and the security exposure of persisting a non-expiring token credential
in a readable API object is acceptable to you.</p><p>When using this Secret type, you need to ensure that the
<code>kubernetes.io/service-account.name</code> annotation is set to an existing
service account name. If you are creating both the ServiceAccount and
the Secret objects, you should create the ServiceAccount object first.</p><p>After the Secret is created, a Kubernetes <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>
fills in some other fields such as the <code>kubernetes.io/service-account.uid</code> annotation, and the
<code>token</code> key in the <code>data</code> field, which is populated with an authentication token.</p><p>The following example configuration declares a service account token Secret:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-sa-sample<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/service-account.name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;sa-name&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/service-account-token<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># You can include additional key value pairs as you do with Opaque Secrets</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>extra</span>:<span style=color:#bbb> </span>YmFyCg==<span style=color:#bbb>
</span></span></span></code></pre></div><p>After creating the Secret, wait for Kubernetes to populate the <code>token</code> key in the <code>data</code> field.</p><p>See the <a href=/docs/tasks/configure-pod-container/configure-service-account/>ServiceAccount</a>
documentation for more information on how service accounts work.
You can also check the <code>automountServiceAccountToken</code> field and the
<code>serviceAccountName</code> field of the
<a href=/docs/reference/generated/kubernetes-api/v1.25/#pod-v1-core><code>Pod</code></a>
for information on referencing service account credentials from within Pods.</p><h3 id=docker-config-secrets>Docker config Secrets</h3><p>You can use one of the following <code>type</code> values to create a Secret to
store the credentials for accessing a container image registry:</p><ul><li><code>kubernetes.io/dockercfg</code></li><li><code>kubernetes.io/dockerconfigjson</code></li></ul><p>The <code>kubernetes.io/dockercfg</code> type is reserved to store a serialized
<code>~/.dockercfg</code> which is the legacy format for configuring Docker command line.
When using this Secret type, you have to ensure the Secret <code>data</code> field
contains a <code>.dockercfg</code> key whose value is content of a <code>~/.dockercfg</code> file
encoded in the base64 format.</p><p>The <code>kubernetes.io/dockerconfigjson</code> type is designed for storing a serialized
JSON that follows the same format rules as the <code>~/.docker/config.json</code> file
which is a new format for <code>~/.dockercfg</code>.
When using this Secret type, the <code>data</code> field of the Secret object must
contain a <code>.dockerconfigjson</code> key, in which the content for the
<code>~/.docker/config.json</code> file is provided as a base64 encoded string.</p><p>Below is an example for a <code>kubernetes.io/dockercfg</code> type of Secret:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-dockercfg<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/dockercfg<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>.dockercfg</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span><span style=color:#b44>&#34;&lt;base64 encoded ~/.dockercfg file&gt;&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you do not want to perform the base64 encoding, you can choose to use the
<code>stringData</code> field instead.</div><p>When you create these types of Secrets using a manifest, the API
server checks whether the expected key exists in the <code>data</code> field, and
it verifies if the value provided can be parsed as a valid JSON. The API
server doesn't validate if the JSON actually is a Docker config file.</p><p>When you do not have a Docker config file, or you want to use <code>kubectl</code>
to create a Secret for accessing a container registry, you can do:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret docker-registry secret-tiger-docker <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --docker-email<span style=color:#666>=</span>tiger@acme.example <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --docker-username<span style=color:#666>=</span>tiger <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --docker-password<span style=color:#666>=</span>pass1234 <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --docker-server<span style=color:#666>=</span>my-registry.example:5000
</span></span></code></pre></div><p>That command creates a Secret of type <code>kubernetes.io/dockerconfigjson</code>.
If you dump the <code>.data.dockerconfigjson</code> field from that new Secret and then
decode it from base64:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get secret secret-tiger-docker -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.data.*}&#39;</span> | base64 -d
</span></span></code></pre></div><p>then the output is equivalent to this JSON document (which is also a valid
Docker configuration file):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;auths&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;my-registry.example:5000&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;username&#34;</span>: <span style=color:#b44>&#34;tiger&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;password&#34;</span>: <span style=color:#b44>&#34;pass1234&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;email&#34;</span>: <span style=color:#b44>&#34;tiger@acme.example&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;auth&#34;</span>: <span style=color:#b44>&#34;dGlnZXI6cGFzczEyMzQ=&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>auth</code> value there is base64 encoded; it is obscured but not secret.
Anyone who can read that Secret can learn the registry access bearer token.</div><h3 id=basic-authentication-secret>Basic authentication Secret</h3><p>The <code>kubernetes.io/basic-auth</code> type is provided for storing credentials needed
for basic authentication. When using this Secret type, the <code>data</code> field of the
Secret must contain one of the following two keys:</p><ul><li><code>username</code>: the user name for authentication</li><li><code>password</code>: the password or token for authentication</li></ul><p>Both values for the above two keys are base64 encoded strings. You can, of
course, provide the clear text content using the <code>stringData</code> for Secret
creation.</p><p>The following manifest is an example of a basic authentication Secret:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-basic-auth<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/basic-auth<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>stringData</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>username</span>:<span style=color:#bbb> </span>admin     <span style=color:#bbb> </span><span style=color:#080;font-style:italic># required field for kubernetes.io/basic-auth</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>password</span>:<span style=color:#bbb> </span>t0p-Secret<span style=color:#bbb> </span><span style=color:#080;font-style:italic># required field for kubernetes.io/basic-auth</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The basic authentication Secret type is provided only for convenience.
You can create an <code>Opaque</code> type for credentials used for basic authentication.
However, using the defined and public Secret type (<code>kubernetes.io/basic-auth</code>) helps other
people to understand the purpose of your Secret, and sets a convention for what key names
to expect.
The Kubernetes API verifies that the required keys are set for a Secret of this type.</p><h3 id=ssh-authentication-secrets>SSH authentication secrets</h3><p>The builtin type <code>kubernetes.io/ssh-auth</code> is provided for storing data used in
SSH authentication. When using this Secret type, you will have to specify a
<code>ssh-privatekey</code> key-value pair in the <code>data</code> (or <code>stringData</code>) field
as the SSH credential to use.</p><p>The following manifest is an example of a Secret used for SSH public/private
key authentication:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-ssh-auth<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/ssh-auth<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># the data is abbreviated in this example</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ssh-privatekey</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>     </span><span style=color:#bbb>     </span>MIIEpQIBAAKCAQEAulqb/Y ...<span style=color:#bbb>
</span></span></span></code></pre></div><p>The SSH authentication Secret type is provided only for user's convenience.
You could instead create an <code>Opaque</code> type Secret for credentials used for SSH authentication.
However, using the defined and public Secret type (<code>kubernetes.io/ssh-auth</code>) helps other
people to understand the purpose of your Secret, and sets a convention for what key names
to expect.
and the API server does verify if the required keys are provided in a Secret configuration.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> SSH private keys do not establish trusted communication between an SSH client and
host server on their own. A secondary means of establishing trust is needed to
mitigate "man in the middle" attacks, such as a <code>known_hosts</code> file added to a ConfigMap.</div><h3 id=tls-secrets>TLS secrets</h3><p>Kubernetes provides a builtin Secret type <code>kubernetes.io/tls</code> for storing
a certificate and its associated key that are typically used for TLS.</p><p>One common use for TLS secrets is to configure encryption in transit for
an <a href=/docs/concepts/services-networking/ingress/>Ingress</a>, but you can also use it
with other resources or directly in your workload.
When using this type of Secret, the <code>tls.key</code> and the <code>tls.crt</code> key must be provided
in the <code>data</code> (or <code>stringData</code>) field of the Secret configuration, although the API
server doesn't actually validate the values for each key.</p><p>The following YAML contains an example config for a TLS Secret:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>secret-tls<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>kubernetes.io/tls<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># the data is abbreviated in this example</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tls.crt</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span>MIIC2DCCAcCgAwIBAgIBATANBgkqh ...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tls.key</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    </span><span style=color:#bbb>    </span>MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...<span style=color:#bbb>
</span></span></span></code></pre></div><p>The TLS Secret type is provided for user's convenience. You can create an <code>Opaque</code>
for credentials used for TLS server and/or client. However, using the builtin Secret
type helps ensure the consistency of Secret format in your project; the API server
does verify if the required keys are provided in a Secret configuration.</p><p>When creating a TLS Secret using <code>kubectl</code>, you can use the <code>tls</code> subcommand
as shown in the following example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret tls my-tls-secret <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --cert<span style=color:#666>=</span>path/to/cert/file <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>  --key<span style=color:#666>=</span>path/to/key/file
</span></span></code></pre></div><p>The public/private key pair must exist before hand. The public key certificate
for <code>--cert</code> must be DER format as per
<a href=https://datatracker.ietf.org/doc/html/rfc7468#section-5.1>Section 5.1 of RFC 7468</a>,
and must match the given private key for <code>--key</code> (PKCS #8 in DER format;
<a href=https://datatracker.ietf.org/doc/html/rfc7468#section-11>Section 11 of RFC 7468</a>).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>A kubernetes.io/tls Secret stores the Base64-encoded DER data for keys and
certificates. If you're familiar with PEM format for private keys and for certificates,
the base64 data are the same as that format except that you omit
the initial and the last lines that are used in PEM.</p><p>For example, for a certificate, you do <strong>not</strong> include <code>--------BEGIN CERTIFICATE-----</code>
and <code>-------END CERTIFICATE----</code>.</p></div><h3 id=bootstrap-token-secrets>Bootstrap token Secrets</h3><p>A bootstrap token Secret can be created by explicitly specifying the Secret
<code>type</code> to <code>bootstrap.kubernetes.io/token</code>. This type of Secret is designed for
tokens used during the node bootstrap process. It stores tokens used to sign
well-known ConfigMaps.</p><p>A bootstrap token Secret is usually created in the <code>kube-system</code> namespace and
named in the form <code>bootstrap-token-&lt;token-id></code> where <code>&lt;token-id></code> is a 6 character
string of the token ID.</p><p>As a Kubernetes manifest, a bootstrap token Secret might look like the
following:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>bootstrap-token-5emitj<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>bootstrap.kubernetes.io/token<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>auth-extra-groups</span>:<span style=color:#bbb> </span>c3lzdGVtOmJvb3RzdHJhcHBlcnM6a3ViZWFkbTpkZWZhdWx0LW5vZGUtdG9rZW4=<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>expiration</span>:<span style=color:#bbb> </span>MjAyMC0wOS0xM1QwNDozOToxMFo=<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>token-id</span>:<span style=color:#bbb> </span>NWVtaXRq<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>token-secret</span>:<span style=color:#bbb> </span>a3E0Z2lodnN6emduMXAwcg==<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>usage-bootstrap-authentication</span>:<span style=color:#bbb> </span>dHJ1ZQ==<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>usage-bootstrap-signing</span>:<span style=color:#bbb> </span>dHJ1ZQ==<span style=color:#bbb>
</span></span></span></code></pre></div><p>A bootstrap type Secret has the following keys specified under <code>data</code>:</p><ul><li><code>token-id</code>: A random 6 character string as the token identifier. Required.</li><li><code>token-secret</code>: A random 16 character string as the actual token secret. Required.</li><li><code>description</code>: A human-readable string that describes what the token is
used for. Optional.</li><li><code>expiration</code>: An absolute UTC time using <a href=https://datatracker.ietf.org/doc/html/rfc3339>RFC3339</a> specifying when the token
should be expired. Optional.</li><li><code>usage-bootstrap-&lt;usage></code>: A boolean flag indicating additional usage for
the bootstrap token.</li><li><code>auth-extra-groups</code>: A comma-separated list of group names that will be
authenticated as in addition to the <code>system:bootstrappers</code> group.</li></ul><p>The above YAML may look confusing because the values are all in base64 encoded
strings. In fact, you can create an identical Secret using the following YAML:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Note how the Secret is named</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>bootstrap-token-5emitj<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># A bootstrap token Secret usually resides in the kube-system namespace</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>bootstrap.kubernetes.io/token<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>stringData</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>auth-extra-groups</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;system:bootstrappers:kubeadm:default-node-token&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>expiration</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2020-09-13T04:39:10Z&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># This token ID is used in the name</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>token-id</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5emitj&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>token-secret</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kq4gihvszzgn1p0r&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># This token can be used for authentication</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>usage-bootstrap-authentication</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># and it can be used for signing</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>usage-bootstrap-signing</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;true&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=secret-immutable>Immutable Secrets</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>Kubernetes lets you mark specific Secrets (and ConfigMaps) as <em>immutable</em>.
Preventing changes to the data of an existing Secret has the following benefits:</p><ul><li>protects you from accidental (or unwanted) updates that could cause applications outages</li><li>(for clusters that extensively use Secrets - at least tens of thousands of unique Secret
to Pod mounts), switching to immutable Secrets improves the performance of your cluster
by significantly reducing load on kube-apiserver. The kubelet does not need to maintain
a [watch] on any Secrets that are marked as immutable.</li></ul><h3 id=secret-immutable-create>Marking a Secret as immutable</h3><p>You can create an immutable Secret by setting the <code>immutable</code> field to <code>true</code>. For example,</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Secret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>immutable</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>You can also update any existing mutable Secret to make it immutable.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Once a Secret or ConfigMap is marked as immutable, it is <em>not</em> possible to revert this change
nor to mutate the contents of the <code>data</code> field. You can only delete and recreate the Secret.
Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate
these pods.</div><h2 id=information-security-for-secrets>Information security for Secrets</h2><p>Although ConfigMap and Secret work similarly, Kubernetes applies some additional
protection for Secret objects.</p><p>Secrets often hold values that span a spectrum of importance, many of which can
cause escalations within Kubernetes (e.g. service account tokens) and to
external systems. Even if an individual app can reason about the power of the
Secrets it expects to interact with, other apps within the same namespace can
render those assumptions invalid.</p><p>A Secret is only sent to a node if a Pod on that node requires it.
For mounting secrets into Pods, the kubelet stores a copy of the data into a <code>tmpfs</code>
so that the confidential data is not written to durable storage.
Once the Pod that depends on the Secret is deleted, the kubelet deletes its local copy
of the confidential data from the Secret.</p><p>There may be several containers in a Pod. By default, containers you define
only have access to the default ServiceAccount and its related Secret.
You must explicitly define environment variables or map a volume into a
container in order to provide access to any other Secret.</p><p>There may be Secrets for several Pods on the same node. However, only the
Secrets that a Pod requests are potentially visible within its containers.
Therefore, one Pod does not have access to the Secrets of another Pod.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> Any containers that run with <code>privileged: true</code> on a node can access all
Secrets used on that node.</div><h2 id=what-s-next>What's next</h2><ul><li>For guidelines to manage and improve the security of your Secrets, refer to
<a href=/docs/concepts/security/secrets-good-practices>Good practices for Kubernetes Secrets</a>.</li><li>Learn how to <a href=/docs/tasks/configmap-secret/managing-secret-using-kubectl/>manage Secrets using <code>kubectl</code></a></li><li>Learn how to <a href=/docs/tasks/configmap-secret/managing-secret-using-config-file/>manage Secrets using config file</a></li><li>Learn how to <a href=/docs/tasks/configmap-secret/managing-secret-using-kustomize/>manage Secrets using kustomize</a></li><li>Read the <a href=/docs/reference/kubernetes-api/config-and-storage-resources/secret-v1/>API reference</a> for <code>Secret</code></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-436057b96151ecb8a4a9a9f456b5d0fc>8.4 - Resource Management for Pods and Containers</h1><p>When you specify a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, you can optionally specify how
much of each resource a <a class=glossary-tooltip title='A lightweight and portable executable image that contains software and all of its dependencies.' data-toggle=tooltip data-placement=top href=/docs/concepts/containers/ target=_blank aria-label=container>container</a> needs.
The most common resources to specify are CPU and memory (RAM); there are others.</p><p>When you specify the resource <em>request</em> for containers in a Pod, the
<a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a> uses this
information to decide which node to place the Pod on. When you specify a resource <em>limit</em>
for a container, the kubelet enforces those limits so that the running container is not
allowed to use more of that resource than the limit you set. The kubelet also reserves
at least the <em>request</em> amount of that system resource specifically for that container
to use.</p><h2 id=requests-and-limits>Requests and limits</h2><p>If the node where a Pod is running has enough of a resource available, it's possible (and
allowed) for a container to use more resource than its <code>request</code> for that resource specifies.
However, a container is not allowed to use more than its resource <code>limit</code>.</p><p>For example, if you set a <code>memory</code> request of 256 MiB for a container, and that container is in
a Pod scheduled to a Node with 8GiB of memory and no other Pods, then the container can try to use
more RAM.</p><p>If you set a <code>memory</code> limit of 4GiB for that container, the kubelet (and
<a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>) enforce the limit.
The runtime prevents the container from using more than the configured resource limit. For example:
when a process in the container tries to consume more than the allowed amount of memory,
the system kernel terminates the process that attempted the allocation, with an out of memory
(OOM) error.</p><p>Limits can be implemented either reactively (the system intervenes once it sees a violation)
or by enforcement (the system prevents the container from ever exceeding the limit). Different
runtimes can have different ways to implement the same restrictions.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you specify a limit for a resource, but do not specify any request, and no admission-time
mechanism has applied a default request for that resource, then Kubernetes copies the limit
you specified and uses it as the requested value for the resource.</div><h2 id=resource-types>Resource types</h2><p><em>CPU</em> and <em>memory</em> are each a <em>resource type</em>. A resource type has a base unit.
CPU represents compute processing and is specified in units of <a href=#meaning-of-cpu>Kubernetes CPUs</a>.
Memory is specified in units of bytes.
For Linux workloads, you can specify <em>huge page</em> resources.
Huge pages are a Linux-specific feature where the node kernel allocates blocks of memory
that are much larger than the default page size.</p><p>For example, on a system where the default page size is 4KiB, you could specify a limit,
<code>hugepages-2Mi: 80Mi</code>. If the container tries allocating over 40 2MiB huge pages (a
total of 80 MiB), that allocation fails.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You cannot overcommit <code>hugepages-*</code> resources.
This is different from the <code>memory</code> and <code>cpu</code> resources.</div><p>CPU and memory are collectively referred to as <em>compute resources</em>, or <em>resources</em>. Compute
resources are measurable quantities that can be requested, allocated, and
consumed. They are distinct from
<a href=/docs/concepts/overview/kubernetes-api/>API resources</a>. API resources, such as Pods and
<a href=/docs/concepts/services-networking/service/>Services</a> are objects that can be read and modified
through the Kubernetes API server.</p><h2 id=resource-requests-and-limits-of-pod-and-container>Resource requests and limits of Pod and container</h2><p>For each container, you can specify resource limits and requests,
including the following:</p><ul><li><code>spec.containers[].resources.limits.cpu</code></li><li><code>spec.containers[].resources.limits.memory</code></li><li><code>spec.containers[].resources.limits.hugepages-&lt;size></code></li><li><code>spec.containers[].resources.requests.cpu</code></li><li><code>spec.containers[].resources.requests.memory</code></li><li><code>spec.containers[].resources.requests.hugepages-&lt;size></code></li></ul><p>Although you can only specify requests and limits for individual containers,
it is also useful to think about the overall resource requests and limits for
a Pod.
For a particular resource, a <em>Pod resource request/limit</em> is the sum of the
resource requests/limits of that type for each container in the Pod.</p><h2 id=resource-units-in-kubernetes>Resource units in Kubernetes</h2><h3 id=meaning-of-cpu>CPU resource units</h3><p>Limits and requests for CPU resources are measured in <em>cpu</em> units.
In Kubernetes, 1 CPU unit is equivalent to <strong>1 physical CPU core</strong>,
or <strong>1 virtual core</strong>, depending on whether the node is a physical host
or a virtual machine running inside a physical machine.</p><p>Fractional requests are allowed. When you define a container with
<code>spec.containers[].resources.requests.cpu</code> set to <code>0.5</code>, you are requesting half
as much CPU time compared to if you asked for <code>1.0</code> CPU.
For CPU resource units, the <a href=/docs/reference/kubernetes-api/common-definitions/quantity/>quantity</a> expression <code>0.1</code> is equivalent to the
expression <code>100m</code>, which can be read as "one hundred millicpu". Some people say
"one hundred millicores", and this is understood to mean the same thing.</p><p>CPU resource is always specified as an absolute amount of resource, never as a relative amount. For example,
<code>500m</code> CPU represents the roughly same amount of computing power whether that container
runs on a single-core, dual-core, or 48-core machine.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes doesn't allow you to specify CPU resources with a precision finer than
<code>1m</code>. Because of this, it's useful to specify CPU units less than <code>1.0</code> or <code>1000m</code> using
the milliCPU form; for example, <code>5m</code> rather than <code>0.005</code>.</div><h3 id=meaning-of-memory>Memory resource units</h3><p>Limits and requests for <code>memory</code> are measured in bytes. You can express memory as
a plain integer or as a fixed-point number using one of these
<a href=/docs/reference/kubernetes-api/common-definitions/quantity/>quantity</a> suffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following represent roughly the same value:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>128974848, 129e6, 129M,  128974848000m, 123Mi
</span></span></code></pre></div><p>Pay attention to the case of the suffixes. If you request <code>400m</code> of memory, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (<code>400Mi</code>)
or 400 megabytes (<code>400M</code>).</p><h2 id=example-1>Container resources example</h2><p>The following Pod has two containers. Both containers are defined with a request for
0.25 CPU
and 64MiB (2<sup>26</sup> bytes) of memory. Each container has a limit of 0.5
CPU and 128MiB of memory. You can say the Pod has a request of 0.5 CPU and 128
MiB of memory, and a limit of 1 CPU and 256MiB of memory.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/app:v4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;64Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;128Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>log-aggregator<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/log-aggregator:v6<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;64Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;128Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=how-pods-with-resource-requests-are-scheduled>How Pods with resource requests are scheduled</h2><p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum capacity for each of the resource types: the
amount of CPU and memory it can provide for Pods. The scheduler ensures that,
for each resource type, the sum of the resource requests of the scheduled
containers is less than the capacity of the node.
Note that although actual memory
or CPU resource usage on nodes is very low, the scheduler still refuses to place
a Pod on a node if the capacity check fails. This protects against a resource
shortage on a node when resource usage later increases, for example, during a
daily peak in request rate.</p><h2 id=how-pods-with-resource-limits-are-run>How Kubernetes applies resource requests and limits</h2><p>When the kubelet starts a container as part of a Pod, the kubelet passes that container's
requests and limits for memory and CPU to the container runtime.</p><p>On Linux, the container runtime typically configures
kernel <a class=glossary-tooltip title='A group of Linux processes with optional resource isolation, accounting and limits.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label=cgroups>cgroups</a> that apply and enforce the
limits you defined.</p><ul><li>The CPU limit defines a hard ceiling on how much CPU time that the container can use.
During each scheduling interval (time slice), the Linux kernel checks to see if this
limit is exceeded; if so, the kernel waits before allowing that cgroup to resume execution.</li><li>The CPU request typically defines a weighting. If several different containers (cgroups)
want to run on a contended system, workloads with larger CPU requests are allocated more
CPU time than workloads with small requests.</li><li>The memory request is mainly used during (Kubernetes) Pod scheduling. On a node that uses
cgroups v2, the container runtime might use the memory request as a hint to set
<code>memory.min</code> and <code>memory.low</code>.</li><li>The memory limit defines a memory limit for that cgroup. If the container tries to
allocate more memory than this limit, the Linux kernel out-of-memory subsystem activates
and, typically, intervenes by stopping one of the processes in the container that tried
to allocate memory. If that process is the container's PID 1, and the container is marked
as restartable, Kubernetes restarts the container.</li><li>The memory limit for the Pod or container can also apply to pages in memory backed
volumes, such as an <code>emptyDir</code>. The kubelet tracks <code>tmpfs</code> emptyDir volumes as container
memory use, rather than as local ephemeral storage.</li></ul><p>If a container exceeds its memory request and the node that it runs on becomes short of
memory overall, it is likely that the Pod the container belongs to will be
<a class=glossary-tooltip title='Process of terminating one or more Pods on Nodes' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/ target=_blank aria-label=evicted>evicted</a>.</p><p>A container might or might not be allowed to exceed its CPU limit for extended periods of time.
However, container runtimes don't terminate Pods or containers for excessive CPU usage.</p><p>To determine whether a container cannot be scheduled or is being killed due to resource limits,
see the <a href=#troubleshooting>Troubleshooting</a> section.</p><h3 id=monitoring-compute-memory-resource-usage>Monitoring compute & memory resource usage</h3><p>The kubelet reports the resource usage of a Pod as part of the Pod
<a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/#object-spec-and-status><code>status</code></a>.</p><p>If optional <a href=/docs/tasks/debug/debug-cluster/resource-usage-monitoring/>tools for monitoring</a>
are available in your cluster, then Pod resource usage can be retrieved either
from the <a href=/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/#metrics-api>Metrics API</a>
directly or from your monitoring tools.</p><h2 id=local-ephemeral-storage>Local ephemeral storage</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>Nodes have local ephemeral storage, backed by
locally-attached writeable devices or, sometimes, by RAM.
"Ephemeral" means that there is no long-term guarantee about durability.</p><p>Pods use ephemeral local storage for scratch space, caching, and for logs.
The kubelet can provide scratch space to Pods using local ephemeral storage to
mount <a href=/docs/concepts/storage/volumes/#emptydir><code>emptyDir</code></a>
<a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volumes>volumes</a> into containers.</p><p>The kubelet also uses this kind of storage to hold
<a href=/docs/concepts/cluster-administration/logging/#logging-at-the-node-level>node-level container logs</a>,
container images, and the writable layers of running containers.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> If a node fails, the data in its ephemeral storage can be lost.
Your applications cannot expect any performance SLAs (disk IOPS for example)
from local ephemeral storage.</div><p>As a beta feature, Kubernetes lets you track, reserve and limit the amount
of ephemeral local storage a Pod can consume.</p><h3 id=configurations-for-local-ephemeral-storage>Configurations for local ephemeral storage</h3><p>Kubernetes supports two ways to configure local ephemeral storage on a node:<ul class="nav nav-tabs" id=local-storage-configurations role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#local-storage-configurations-0 role=tab aria-controls=local-storage-configurations-0 aria-selected=true>Single filesystem</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#local-storage-configurations-1 role=tab aria-controls=local-storage-configurations-1>Two filesystems</a></li></ul><div class=tab-content id=local-storage-configurations><div id=local-storage-configurations-0 class="tab-pane show active" role=tabpanel aria-labelledby=local-storage-configurations-0><p><p>In this configuration, you place all different kinds of ephemeral local data
(<code>emptyDir</code> volumes, writeable layers, container images, logs) into one filesystem.
The most effective way to configure the kubelet means dedicating this filesystem
to Kubernetes (kubelet) data.</p><p>The kubelet also writes
<a href=/docs/concepts/cluster-administration/logging/#logging-at-the-node-level>node-level container logs</a>
and treats these similarly to ephemeral local storage.</p><p>The kubelet writes logs to files inside its configured log directory (<code>/var/log</code>
by default); and has a base directory for other locally stored data
(<code>/var/lib/kubelet</code> by default).</p><p>Typically, both <code>/var/lib/kubelet</code> and <code>/var/log</code> are on the system root filesystem,
and the kubelet is designed with that layout in mind.</p><p>Your node can have as many other filesystems, not used for Kubernetes,
as you like.</p></div><div id=local-storage-configurations-1 class=tab-pane role=tabpanel aria-labelledby=local-storage-configurations-1><p><p>You have a filesystem on the node that you're using for ephemeral data that
comes from running Pods: logs, and <code>emptyDir</code> volumes. You can use this filesystem
for other data (for example: system logs not related to Kubernetes); it can even
be the root filesystem.</p><p>The kubelet also writes
<a href=/docs/concepts/cluster-administration/logging/#logging-at-the-node-level>node-level container logs</a>
into the first filesystem, and treats these similarly to ephemeral local storage.</p><p>You also use a separate filesystem, backed by a different logical storage device.
In this configuration, the directory where you tell the kubelet to place
container image layers and writeable layers is on this second filesystem.</p><p>The first filesystem does not hold any image layers or writeable layers.</p><p>Your node can have as many other filesystems, not used for Kubernetes,
as you like.</p></div></div></p><p>The kubelet can measure how much local storage it is using. It does this provided
that you have set up the node using one of the supported configurations for local
ephemeral storage.</p><p>If you have a different configuration, then the kubelet does not apply resource
limits for ephemeral local storage.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The kubelet tracks <code>tmpfs</code> emptyDir volumes as container memory use, rather
than as local ephemeral storage.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The kubelet will only track the root filesystem for ephemeral storage. OS layouts that mount a separate disk to <code>/var/lib/kubelet</code> or <code>/var/lib/containers</code> will not report ephemeral storage correctly.</div><h3 id=setting-requests-and-limits-for-local-ephemeral-storage>Setting requests and limits for local ephemeral storage</h3><p>You can specify <code>ephemeral-storage</code> for managing local ephemeral storage. Each
container of a Pod can specify either or both of the following:</p><ul><li><code>spec.containers[].resources.limits.ephemeral-storage</code></li><li><code>spec.containers[].resources.requests.ephemeral-storage</code></li></ul><p>Limits and requests for <code>ephemeral-storage</code> are measured in byte quantities.
You can express storage as a plain integer or as a fixed-point number using one of these suffixes:
E, P, T, G, M, k. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi,
Mi, Ki. For example, the following quantities all represent roughly the same value:</p><ul><li><code>128974848</code></li><li><code>129e6</code></li><li><code>129M</code></li><li><code>123Mi</code></li></ul><p>Pay attention to the case of the suffixes. If you request <code>400m</code> of ephemeral-storage, this is a request
for 0.4 bytes. Someone who types that probably meant to ask for 400 mebibytes (<code>400Mi</code>)
or 400 megabytes (<code>400M</code>).</p><p>In the following example, the Pod has two containers. Each container has a request of
2GiB of local ephemeral storage. Each container has a limit of 4GiB of local ephemeral
storage. Therefore, the Pod has a request of 4GiB of local ephemeral storage, and
a limit of 8GiB of local ephemeral storage. 500Mi of that limit could be
consumed by the <code>emptyDir</code> volume.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/app:v4<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ephemeral<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/tmp&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>log-aggregator<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>images.my-company.example/log-aggregator:v6<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ephemeral-storage</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;4Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ephemeral<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;/tmp&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>ephemeral<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>sizeLimit</span>:<span style=color:#bbb> </span>500Mi<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=how-pods-with-ephemeral-storage-requests-are-scheduled>How Pods with ephemeral-storage requests are scheduled</h3><p>When you create a Pod, the Kubernetes scheduler selects a node for the Pod to
run on. Each node has a maximum amount of local ephemeral storage it can provide for Pods.
For more information, see
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>Node Allocatable</a>.</p><p>The scheduler ensures that the sum of the resource requests of the scheduled containers is less than the capacity of the node.</p><h3 id=resource-emphemeralstorage-consumption>Ephemeral storage consumption management</h3><p>If the kubelet is managing local ephemeral storage as a resource, then the
kubelet measures storage use in:</p><ul><li><code>emptyDir</code> volumes, except <em>tmpfs</em> <code>emptyDir</code> volumes</li><li>directories holding node-level logs</li><li>writeable container layers</li></ul><p>If a Pod is using more ephemeral storage than you allow it to, the kubelet
sets an eviction signal that triggers Pod eviction.</p><p>For container-level isolation, if a container's writable layer and log
usage exceeds its storage limit, the kubelet marks the Pod for eviction.</p><p>For pod-level isolation the kubelet works out an overall Pod storage limit by
summing the limits for the containers in that Pod. In this case, if the sum of
the local ephemeral storage usage from all containers and also the Pod's <code>emptyDir</code>
volumes exceeds the overall Pod storage limit, then the kubelet also marks the Pod
for eviction.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>If the kubelet is not measuring local ephemeral storage, then a Pod
that exceeds its local storage limit will not be evicted for breaching
local storage resource limits.</p><p>However, if the filesystem space for writeable container layers, node-level logs,
or <code>emptyDir</code> volumes falls low, the node
<a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=taints>taints</a> itself as short on local storage
and this taint triggers eviction for any Pods that don't specifically tolerate the taint.</p><p>See the supported <a href=#configurations-for-local-ephemeral-storage>configurations</a>
for ephemeral local storage.</p></div><p>The kubelet supports different ways to measure Pod storage use:</p><ul class="nav nav-tabs" id=resource-emphemeralstorage-measurement role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#resource-emphemeralstorage-measurement-0 role=tab aria-controls=resource-emphemeralstorage-measurement-0 aria-selected=true>Periodic scanning</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#resource-emphemeralstorage-measurement-1 role=tab aria-controls=resource-emphemeralstorage-measurement-1>Filesystem project quota</a></li></ul><div class=tab-content id=resource-emphemeralstorage-measurement><div id=resource-emphemeralstorage-measurement-0 class="tab-pane show active" role=tabpanel aria-labelledby=resource-emphemeralstorage-measurement-0><p><p>The kubelet performs regular, scheduled checks that scan each
<code>emptyDir</code> volume, container log directory, and writeable container layer.</p><p>The scan measures how much space is used.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>In this mode, the kubelet does not track open file descriptors
for deleted files.</p><p>If you (or a container) create a file inside an <code>emptyDir</code> volume,
something then opens that file, and you delete the file while it is
still open, then the inode for the deleted file stays until you close
that file but the kubelet does not categorize the space as in use.</p></div></div><div id=resource-emphemeralstorage-measurement-1 class=tab-pane role=tabpanel aria-labelledby=resource-emphemeralstorage-measurement-1><p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.15 [alpha]</code></div><p>Project quotas are an operating-system level feature for managing
storage use on filesystems. With Kubernetes, you can enable project
quotas for monitoring storage use. Make sure that the filesystem
backing the <code>emptyDir</code> volumes, on the node, provides project quota support.
For example, XFS and ext4fs offer project quotas.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Project quotas let you monitor storage use; they do not enforce limits.</div><p>Kubernetes uses project IDs starting from <code>1048576</code>. The IDs in use are
registered in <code>/etc/projects</code> and <code>/etc/projid</code>. If project IDs in
this range are used for other purposes on the system, those project
IDs must be registered in <code>/etc/projects</code> and <code>/etc/projid</code> so that
Kubernetes does not use them.</p><p>Quotas are faster and more accurate than directory scanning. When a
directory is assigned to a project, all files created under a
directory are created in that project, and the kernel merely has to
keep track of how many blocks are in use by files in that project.
If a file is created and deleted, but has an open file descriptor,
it continues to consume space. Quota tracking records that space accurately
whereas directory scans overlook the storage used by deleted files.</p><p>If you want to use project quotas, you should:</p><ul><li><p>Enable the <code>LocalStorageCapacityIsolationFSQuotaMonitoring=true</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
using the <code>featureGates</code> field in the
<a href=/docs/reference/config-api/kubelet-config.v1beta1/>kubelet configuration</a>
or the <code>--feature-gates</code> command line flag.</p></li><li><p>Ensure that the root filesystem (or optional runtime filesystem)
has project quotas enabled. All XFS filesystems support project quotas.
For ext4 filesystems, you need to enable the project quota tracking feature
while the filesystem is not mounted.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># For ext4, with /dev/block-device not mounted</span>
</span></span><span style=display:flex><span>sudo tune2fs -O project -Q prjquota /dev/block-device
</span></span></code></pre></div></li><li><p>Ensure that the root filesystem (or optional runtime filesystem) is
mounted with project quotas enabled. For both XFS and ext4fs, the
mount option is named <code>prjquota</code>.</p></li></ul></div></div><h2 id=extended-resources>Extended resources</h2><p>Extended resources are fully-qualified resource names outside the
<code>kubernetes.io</code> domain. They allow cluster operators to advertise and users to
consume the non-Kubernetes-built-in resources.</p><p>There are two steps required to use Extended Resources. First, the cluster
operator must advertise an Extended Resource. Second, users must request the
Extended Resource in Pods.</p><h3 id=managing-extended-resources>Managing extended resources</h3><h4 id=node-level-extended-resources>Node-level extended resources</h4><p>Node-level extended resources are tied to nodes.</p><h5 id=device-plugin-managed-resources>Device plugin managed resources</h5><p>See <a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>Device
Plugin</a>
for how to advertise device plugin managed resources on each node.</p><h5 id=other-resources>Other resources</h5><p>To advertise a new node-level extended resource, the cluster operator can
submit a <code>PATCH</code> HTTP request to the API server to specify the available
quantity in the <code>status.capacity</code> for a node in the cluster. After this
operation, the node's <code>status.capacity</code> will include a new resource. The
<code>status.allocatable</code> field is updated automatically with the new resource
asynchronously by the kubelet.</p><p>Because the scheduler uses the node's <code>status.allocatable</code> value when
evaluating Pod fitness, the scheduler only takes account of the new value after
that asynchronous update. There may be a short delay between patching the
node capacity with a new resource and the time when the first Pod that requests
the resource can be scheduled on that node.</p><p><strong>Example:</strong></p><p>Here is an example showing how to use <code>curl</code> to form an HTTP request that
advertises five "example.com/foo" resources on node <code>k8s-node-1</code> whose master
is <code>k8s-master</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>curl --header <span style=color:#b44>&#34;Content-Type: application/json-patch+json&#34;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--request PATCH <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--data <span style=color:#b44>&#39;[{&#34;op&#34;: &#34;add&#34;, &#34;path&#34;: &#34;/status/capacity/example.com~1foo&#34;, &#34;value&#34;: &#34;5&#34;}]&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>http://k8s-master:8080/api/v1/nodes/k8s-node-1/status
</span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In the preceding request, <code>~1</code> is the encoding for the character <code>/</code>
in the patch path. The operation path value in JSON-Patch is interpreted as a
JSON-Pointer. For more details, see
<a href=https://tools.ietf.org/html/rfc6901#section-3>IETF RFC 6901, section 3</a>.</div><h4 id=cluster-level-extended-resources>Cluster-level extended resources</h4><p>Cluster-level extended resources are not tied to nodes. They are usually managed
by scheduler extenders, which handle the resource consumption and resource quota.</p><p>You can specify the extended resources that are handled by scheduler extenders
in <a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>scheduler configuration</a></p><p><strong>Example:</strong></p><p>The following configuration for a scheduler policy indicates that the
cluster-level extended resource "example.com/foo" is handled by the scheduler
extender.</p><ul><li>The scheduler sends a Pod to the scheduler extender only if the Pod requests
"example.com/foo".</li><li>The <code>ignoredByScheduler</code> field specifies that the scheduler does not check
the "example.com/foo" resource in its <code>PodFitsResources</code> predicate.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Policy&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;extenders&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;urlPrefix&#34;</span>:<span style=color:#b44>&#34;&lt;extender-endpoint&gt;&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;bindVerb&#34;</span>: <span style=color:#b44>&#34;bind&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;managedResources&#34;</span>: [
</span></span><span style=display:flex><span>        {
</span></span><span style=display:flex><span>          <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;example.com/foo&#34;</span>,
</span></span><span style=display:flex><span>          <span style=color:green;font-weight:700>&#34;ignoredByScheduler&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>      ]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=consuming-extended-resources>Consuming extended resources</h3><p>Users can consume extended resources in Pod specs like CPU and memory.
The scheduler takes care of the resource accounting so that no more than the
available amount is simultaneously allocated to Pods.</p><p>The API server restricts quantities of extended resources to whole numbers.
Examples of <em>valid</em> quantities are <code>3</code>, <code>3000m</code> and <code>3Ki</code>. Examples of
<em>invalid</em> quantities are <code>0.5</code> and <code>1500m</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Extended resources replace Opaque Integer Resources.
Users can use any domain name prefix other than <code>kubernetes.io</code> which is reserved.</div><p>To consume an extended resource in a Pod, include the resource name as a key
in the <code>spec.containers[].resources.limits</code> map in the container spec.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Extended resources cannot be overcommitted, so request and limit
must be equal if both are present in a container spec.</div><p>A Pod is scheduled only if all of the resource requests are satisfied, including
CPU, memory and any extended resources. The Pod remains in the <code>PENDING</code> state
as long as the resource request cannot be satisfied.</p><p><strong>Example:</strong></p><p>The Pod below requests 2 CPUs and 1 "example.com/foo" (an extended resource).</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-container<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>myimage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/foo</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>example.com/foo</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=pid-limiting>PID limiting</h2><p>Process ID (PID) limits allow for the configuration of a kubelet
to limit the number of PIDs that a given Pod can consume. See
<a href=/docs/concepts/policy/pid-limiting/>PID Limiting</a> for information.</p><h2 id=troubleshooting>Troubleshooting</h2><h3 id=my-pods-are-pending-with-event-message-failedscheduling>My Pods are pending with event message <code>FailedScheduling</code></h3><p>If the scheduler cannot find any node where a Pod can fit, the Pod remains
unscheduled until a place can be found. An
<a href=/docs/reference/kubernetes-api/cluster-resources/event-v1/>Event</a> is produced
each time the scheduler fails to find a place for the Pod. You can use <code>kubectl</code>
to view the events for a Pod; for example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pod frontend | grep -A <span style=color:#666>9999999999</span> Events
</span></span></code></pre></div><pre tabindex=0><code>Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  23s   default-scheduler  0/42 nodes available: insufficient cpu
</code></pre><p>In the preceding example, the Pod named "frontend" fails to be scheduled due to
insufficient CPU resource on any node. Similar error messages can also suggest
failure due to insufficient memory (PodExceedsFreeMemory). In general, if a Pod
is pending with a message of this type, there are several things to try:</p><ul><li>Add more nodes to the cluster.</li><li>Terminate unneeded Pods to make room for pending Pods.</li><li>Check that the Pod is not larger than all the nodes. For example, if all the
nodes have a capacity of <code>cpu: 1</code>, then a Pod with a request of <code>cpu: 1.1</code> will
never be scheduled.</li><li>Check for node taints. If most of your nodes are tainted, and the new Pod does
not tolerate that taint, the scheduler only considers placements onto the
remaining nodes that don't have that taint.</li></ul><p>You can check node capacities and amounts allocated with the
<code>kubectl describe nodes</code> command. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe nodes e2e-test-node-pool-4lw4
</span></span></code></pre></div><pre tabindex=0><code>Name:            e2e-test-node-pool-4lw4
[ ... lines removed for clarity ...]
Capacity:
 cpu:                               2
 memory:                            7679792Ki
 pods:                              110
Allocatable:
 cpu:                               1800m
 memory:                            7474992Ki
 pods:                              110
[ ... lines removed for clarity ...]
Non-terminated Pods:        (5 in total)
  Namespace    Name                                  CPU Requests  CPU Limits  Memory Requests  Memory Limits
  ---------    ----                                  ------------  ----------  ---------------  -------------
  kube-system  fluentd-gcp-v1.38-28bv1               100m (5%)     0 (0%)      200Mi (2%)       200Mi (2%)
  kube-system  kube-dns-3297075139-61lj3             260m (13%)    0 (0%)      100Mi (1%)       170Mi (2%)
  kube-system  kube-proxy-e2e-test-...               100m (5%)     0 (0%)      0 (0%)           0 (0%)
  kube-system  monitoring-influxdb-grafana-v4-z1m12  200m (10%)    200m (10%)  600Mi (8%)       600Mi (8%)
  kube-system  node-problem-detector-v0.1-fj7m3      20m (1%)      200m (10%)  20Mi (0%)        100Mi (1%)
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  CPU Requests    CPU Limits    Memory Requests    Memory Limits
  ------------    ----------    ---------------    -------------
  680m (34%)      400m (20%)    920Mi (11%)        1070Mi (13%)
</code></pre><p>In the preceding output, you can see that if a Pod requests more than 1.120 CPUs
or more than 6.23Gi of memory, that Pod will not fit on the node.</p><p>By looking at the “Pods” section, you can see which Pods are taking up space on
the node.</p><p>The amount of resources available to Pods is less than the node capacity because
system daemons use a portion of the available resources. Within the Kubernetes API,
each Node has a <code>.status.allocatable</code> field
(see <a href=/docs/reference/kubernetes-api/cluster-resources/node-v1/#NodeStatus>NodeStatus</a>
for details).</p><p>The <code>.status.allocatable</code> field describes the amount of resources that are available
to Pods on that node (for example: 15 virtual CPUs and 7538 MiB of memory).
For more information on node allocatable resources in Kubernetes, see
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/>Reserve Compute Resources for System Daemons</a>.</p><p>You can configure <a href=/docs/concepts/policy/resource-quotas/>resource quotas</a>
to limit the total amount of resources that a namespace can consume.
Kubernetes enforces quotas for objects in particular namespace when there is a
ResourceQuota in that namespace.
For example, if you assign specific namespaces to different teams, you
can add ResourceQuotas into those namespaces. Setting resource quotas helps to
prevent one team from using so much of any resource that this over-use affects other teams.</p><p>You should also consider what access you grant to that namespace:
<strong>full</strong> write access to a namespace allows someone with that access to remove any
resource, including a configured ResourceQuota.</p><h3 id=my-container-is-terminated>My container is terminated</h3><p>Your container might get terminated because it is resource-starved. To check
whether a container is being killed because it is hitting a resource limit, call
<code>kubectl describe pod</code> on the Pod of interest:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe pod simmemleak-hra99
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>Name:                           simmemleak-hra99
Namespace:                      default
Image(s):                       saadali/simmemleak
Node:                           kubernetes-node-tf0f/10.240.216.66
Labels:                         name=simmemleak
Status:                         Running
Reason:
Message:
IP:                             10.244.2.75
Containers:
  simmemleak:
    Image:  saadali/simmemleak:latest
    Limits:
      cpu:          100m
      memory:       50Mi
    State:          Running
      Started:      Tue, 07 Jul 2019 12:54:41 -0700
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Fri, 07 Jul 2019 12:54:30 -0700
      Finished:     Fri, 07 Jul 2019 12:54:33 -0700
    Ready:          False
    Restart Count:  5
Conditions:
  Type      Status
  Ready     False
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  42s   default-scheduler  Successfully assigned simmemleak-hra99 to kubernetes-node-tf0f
  Normal  Pulled     41s   kubelet            Container image &#34;saadali/simmemleak:latest&#34; already present on machine
  Normal  Created    41s   kubelet            Created container simmemleak
  Normal  Started    40s   kubelet            Started container simmemleak
  Normal  Killing    32s   kubelet            Killing container with id ead3fb35-5cf5-44ed-9ae1-488115be66c6: Need to kill Pod
</code></pre><p>In the preceding example, the <code>Restart Count: 5</code> indicates that the <code>simmemleak</code>
container in the Pod was terminated and restarted five times (so far).
The <code>OOMKilled</code> reason shows that the container tried to use more memory than its limit.</p><p>Your next step might be to check the application code for a memory leak. If you
find that the application is behaving how you expect, consider setting a higher
memory limit (and possibly request) for that container.</p><h2 id=what-s-next>What's next</h2><ul><li>Get hands-on experience <a href=/docs/tasks/configure-pod-container/assign-memory-resource/>assigning Memory resources to containers and Pods</a>.</li><li>Get hands-on experience <a href=/docs/tasks/configure-pod-container/assign-cpu-resource/>assigning CPU resources to containers and Pods</a>.</li><li>Read how the API reference defines a <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container>container</a>
and its <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources>resource requirements</a></li><li>Read about <a href=https://xfs.org/index.php/XFS_FAQ#Q:_Quota:_Do_quotas_work_on_XFS.3F>project quotas</a> in XFS</li><li>Read more about the <a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler configuration reference (v1beta3)</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ab6d20f33ad930a67ee7ef57bff6c75e>8.5 - Organizing Cluster Access Using kubeconfig Files</h1><p>Use kubeconfig files to organize information about clusters, users, namespaces, and
authentication mechanisms. The <code>kubectl</code> command-line tool uses kubeconfig files to
find the information it needs to choose a cluster and communicate with the API server
of a cluster.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A file that is used to configure access to clusters is called
a <em>kubeconfig file</em>. This is a generic way of referring to configuration files.
It does not mean that there is a file named <code>kubeconfig</code>.</div><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> Only use kubeconfig files from trusted sources. Using a specially-crafted kubeconfig file could result in malicious code execution or file exposure.
If you must use an untrusted kubeconfig file, inspect it carefully first, much as you would a shell script.</div><p>By default, <code>kubectl</code> looks for a file named <code>config</code> in the <code>$HOME/.kube</code> directory.
You can specify other kubeconfig files by setting the <code>KUBECONFIG</code> environment
variable or by setting the
<a href=/docs/reference/generated/kubectl/kubectl/><code>--kubeconfig</code></a> flag.</p><p>For step-by-step instructions on creating and specifying kubeconfig files, see
<a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters>Configure Access to Multiple Clusters</a>.</p><h2 id=supporting-multiple-clusters-users-and-authentication-mechanisms>Supporting multiple clusters, users, and authentication mechanisms</h2><p>Suppose you have several clusters, and your users and components authenticate
in a variety of ways. For example:</p><ul><li>A running kubelet might authenticate using certificates.</li><li>A user might authenticate using tokens.</li><li>Administrators might have sets of certificates that they provide to individual users.</li></ul><p>With kubeconfig files, you can organize your clusters, users, and namespaces.
You can also define contexts to quickly and easily switch between
clusters and namespaces.</p><h2 id=context>Context</h2><p>A <em>context</em> element in a kubeconfig file is used to group access parameters
under a convenient name. Each context has three parameters: cluster, namespace, and user.
By default, the <code>kubectl</code> command-line tool uses parameters from
the <em>current context</em> to communicate with the cluster.</p><p>To choose the current context:</p><pre tabindex=0><code>kubectl config use-context
</code></pre><h2 id=the-kubeconfig-environment-variable>The KUBECONFIG environment variable</h2><p>The <code>KUBECONFIG</code> environment variable holds a list of kubeconfig files.
For Linux and Mac, the list is colon-delimited. For Windows, the list
is semicolon-delimited. The <code>KUBECONFIG</code> environment variable is not
required. If the <code>KUBECONFIG</code> environment variable doesn't exist,
<code>kubectl</code> uses the default kubeconfig file, <code>$HOME/.kube/config</code>.</p><p>If the <code>KUBECONFIG</code> environment variable does exist, <code>kubectl</code> uses
an effective configuration that is the result of merging the files
listed in the <code>KUBECONFIG</code> environment variable.</p><h2 id=merging-kubeconfig-files>Merging kubeconfig files</h2><p>To see your configuration, enter this command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl config view
</span></span></code></pre></div><p>As described previously, the output might be from a single kubeconfig file,
or it might be the result of merging several kubeconfig files.</p><p>Here are the rules that <code>kubectl</code> uses when it merges kubeconfig files:</p><ol><li><p>If the <code>--kubeconfig</code> flag is set, use only the specified file. Do not merge.
Only one instance of this flag is allowed.</p><p>Otherwise, if the <code>KUBECONFIG</code> environment variable is set, use it as a
list of files that should be merged.
Merge the files listed in the <code>KUBECONFIG</code> environment variable
according to these rules:</p><ul><li>Ignore empty filenames.</li><li>Produce errors for files with content that cannot be deserialized.</li><li>The first file to set a particular value or map key wins.</li><li>Never change the value or map key.
Example: Preserve the context of the first file to set <code>current-context</code>.
Example: If two files specify a <code>red-user</code>, use only values from the first file's <code>red-user</code>.
Even if the second file has non-conflicting entries under <code>red-user</code>, discard them.</li></ul><p>For an example of setting the <code>KUBECONFIG</code> environment variable, see
<a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable>Setting the KUBECONFIG environment variable</a>.</p><p>Otherwise, use the default kubeconfig file, <code>$HOME/.kube/config</code>, with no merging.</p></li><li><p>Determine the context to use based on the first hit in this chain:</p><ol><li>Use the <code>--context</code> command-line flag if it exists.</li><li>Use the <code>current-context</code> from the merged kubeconfig files.</li></ol><p>An empty context is allowed at this point.</p></li><li><p>Determine the cluster and user. At this point, there might or might not be a context.
Determine the cluster and user based on the first hit in this chain,
which is run twice: once for user and once for cluster:</p><ol><li>Use a command-line flag if it exists: <code>--user</code> or <code>--cluster</code>.</li><li>If the context is non-empty, take the user or cluster from the context.</li></ol><p>The user and cluster can be empty at this point.</p></li><li><p>Determine the actual cluster information to use. At this point, there might or
might not be cluster information.
Build each piece of the cluster information based on this chain; the first hit wins:</p><ol><li>Use command line flags if they exist: <code>--server</code>, <code>--certificate-authority</code>, <code>--insecure-skip-tls-verify</code>.</li><li>If any cluster information attributes exist from the merged kubeconfig files, use them.</li><li>If there is no server location, fail.</li></ol></li><li><p>Determine the actual user information to use. Build user information using the same
rules as cluster information, except allow only one authentication
technique per user:</p><ol><li>Use command line flags if they exist: <code>--client-certificate</code>, <code>--client-key</code>, <code>--username</code>, <code>--password</code>, <code>--token</code>.</li><li>Use the <code>user</code> fields from the merged kubeconfig files.</li><li>If there are two conflicting techniques, fail.</li></ol></li><li><p>For any information still missing, use default values and potentially
prompt for authentication information.</p></li></ol><h2 id=file-references>File references</h2><p>File and path references in a kubeconfig file are relative to the location of the kubeconfig file.
File references on the command line are relative to the current working directory.
In <code>$HOME/.kube/config</code>, relative paths are stored relatively, and absolute paths
are stored absolutely.</p><h2 id=proxy>Proxy</h2><p>You can configure <code>kubectl</code> to use a proxy per cluster using <code>proxy-url</code> in your kubeconfig file, like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>clusters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>cluster</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>proxy-url</span>:<span style=color:#bbb> </span>http://proxy.example.org:3128<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>server</span>:<span style=color:#bbb> </span>https://k8s.example.org/k8s/clusters/c-xxyyzz<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>development<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>users</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>developer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>contexts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>context</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>development<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/tasks/access-application-cluster/configure-access-multiple-clusters/>Configure Access to Multiple Clusters</a></li><li><a href=/docs/reference/generated/kubectl/kubectl-commands#config><code>kubectl config</code></a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0f628478dbd58516389164933f9d7da2>8.6 - Resource Management for Windows nodes</h1><p>This page outlines the differences in how resources are managed between Linux and Windows.</p><p>On Linux nodes, <a class=glossary-tooltip title='A group of Linux processes with optional resource isolation, accounting and limits.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label=cgroups>cgroups</a> are used
as a pod boundary for resource control. Containers are created within that boundary
for network, process and file system isolation. The Linux cgroup APIs can be used to
gather CPU, I/O, and memory use statistics.</p><p>In contrast, Windows uses a <a href=https://docs.microsoft.com/windows/win32/procthread/job-objects><em>job object</em></a> per container with a system namespace filter
to contain all processes in a container and provide logical isolation from the
host.
(Job objects are a Windows process isolation mechanism and are different from
what Kubernetes refers to as a <a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Job>Job</a>).</p><p>There is no way to run a Windows container without the namespace filtering in
place. This means that system privileges cannot be asserted in the context of the
host, and thus privileged containers are not available on Windows.
Containers cannot assume an identity from the host because the Security Account Manager
(SAM) is separate.</p><h2 id=resource-management-memory>Memory management</h2><p>Windows does not have an out-of-memory process killer as Linux does. Windows always
treats all user-mode memory allocations as virtual, and pagefiles are mandatory.</p><p>Windows nodes do not overcommit memory for processes. The
net effect is that Windows won't reach out of memory conditions the same way Linux
does, and processes page to disk instead of being subject to out of memory (OOM)
termination. If memory is over-provisioned and all physical memory is exhausted,
then paging can slow down performance.</p><h2 id=resource-management-cpu>CPU management</h2><p>Windows can limit the amount of CPU time allocated for different processes but cannot
guarantee a minimum amount of CPU time.</p><p>On Windows, the kubelet supports a command-line flag to set the
<a href=https://docs.microsoft.com/windows/win32/procthread/scheduling-priorities>scheduling priority</a> of the
kubelet process: <code>--windows-priorityclass</code>. This flag allows the kubelet process to get
more CPU time slices when compared to other processes running on the Windows host.
More information on the allowable values and their meaning is available at
<a href=https://docs.microsoft.com/en-us/windows/win32/procthread/scheduling-priorities#priority-class>Windows Priority Classes</a>.
To ensure that running Pods do not starve the kubelet of CPU cycles, set this flag to <code>ABOVE_NORMAL_PRIORITY_CLASS</code> or above.</p><h2 id=resource-reservation>Resource reservation</h2><p>To account for memory and CPU used by the operating system, the container runtime, and by
Kubernetes host processes such as the kubelet, you can (and should) reserve
memory and CPU resources with the <code>--kube-reserved</code> and/or <code>--system-reserved</code> kubelet flags.
On Windows these values are only used to calculate the node's
<a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>allocatable</a> resources.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>As you deploy workloads, set resource memory and CPU limits on containers.
This also subtracts from <code>NodeAllocatable</code> and helps the cluster-wide scheduler in determining which pods to place on which nodes.</p><p>Scheduling pods without limits may over-provision the Windows nodes and in extreme
cases can cause the nodes to become unhealthy.</p></div><p>On Windows, a good practice is to reserve at least 2GiB of memory.</p><p>To determine how much CPU to reserve,
identify the maximum pod density for each node and monitor the CPU usage of
the system services running there, then choose a value that meets your workload needs.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-712cb3c03ff14a39e5a83a6d9b71d203>9 - Security</h1><div class=lead>Concepts for keeping your cloud-native workload secure.</div></div><div class=td-content><h1 id=pg-04eeb110d75afc8acb2cf7a3db743985>9.1 - Overview of Cloud Native Security</h1><div class=lead>A model for thinking about Kubernetes security in the context of Cloud Native security.</div><p>This overview defines a model for thinking about Kubernetes security in the context of Cloud Native security.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> This container security model provides suggestions, not proven information security policies.</div><h2 id=the-4c-s-of-cloud-native-security>The 4C's of Cloud Native security</h2><p>You can think about security in layers. The 4C's of Cloud Native security are Cloud,
Clusters, Containers, and Code.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This layered approach augments the <a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defense in depth</a>
computing approach to security, which is widely regarded as a best practice for securing
software systems.</div><figure class=diagram-large><img src=/images/docs/4c.png><figcaption><h4>The 4C's of Cloud Native Security</h4></figcaption></figure><p>Each layer of the Cloud Native security model builds upon the next outermost layer.
The Code layer benefits from strong base (Cloud, Cluster, Container) security layers.
You cannot safeguard against poor security standards in the base layers by addressing
security at the Code level.</p><h2 id=cloud>Cloud</h2><p>In many ways, the Cloud (or co-located servers, or the corporate datacenter) is the
<a href=https://en.wikipedia.org/wiki/Trusted_computing_base>trusted computing base</a>
of a Kubernetes cluster. If the Cloud layer is vulnerable (or
configured in a vulnerable way) then there is no guarantee that the components built
on top of this base are secure. Each cloud provider makes security recommendations
for running workloads securely in their environment.</p><h3 id=cloud-provider-security>Cloud provider security</h3><p>If you are running a Kubernetes cluster on your own hardware or a different cloud provider,
consult your documentation for security best practices.
Here are links to some of the popular cloud providers' security documentation:</p><table><caption style=display:none>Cloud provider security</caption><thead><tr><th>IaaS Provider</th><th>Link</th></tr></thead><tbody><tr><td>Alibaba Cloud</td><td><a href=https://www.alibabacloud.com/trust-center>https://www.alibabacloud.com/trust-center</a></td></tr><tr><td>Amazon Web Services</td><td><a href=https://aws.amazon.com/security>https://aws.amazon.com/security</a></td></tr><tr><td>Google Cloud Platform</td><td><a href=https://cloud.google.com/security>https://cloud.google.com/security</a></td></tr><tr><td>Huawei Cloud</td><td><a href=https://www.huaweicloud.com/securecenter/overallsafety>https://www.huaweicloud.com/securecenter/overallsafety</a></td></tr><tr><td>IBM Cloud</td><td><a href=https://www.ibm.com/cloud/security>https://www.ibm.com/cloud/security</a></td></tr><tr><td>Microsoft Azure</td><td><a href=https://docs.microsoft.com/en-us/azure/security/azure-security>https://docs.microsoft.com/en-us/azure/security/azure-security</a></td></tr><tr><td>Oracle Cloud Infrastructure</td><td><a href=https://www.oracle.com/security>https://www.oracle.com/security</a></td></tr><tr><td>VMware vSphere</td><td><a href=https://www.vmware.com/security/hardening-guides>https://www.vmware.com/security/hardening-guides</a></td></tr></tbody></table><h3 id=infrastructure-security>Infrastructure security</h3><p>Suggestions for securing your infrastructure in a Kubernetes cluster:</p><table><caption style=display:none>Infrastructure security</caption><thead><tr><th>Area of Concern for Kubernetes Infrastructure</th><th>Recommendation</th></tr></thead><tbody><tr><td>Network access to API Server (Control plane)</td><td>All access to the Kubernetes control plane is not allowed publicly on the internet and is controlled by network access control lists restricted to the set of IP addresses needed to administer the cluster.</td></tr><tr><td>Network access to Nodes (nodes)</td><td>Nodes should be configured to <em>only</em> accept connections (via network access control lists) from the control plane on the specified ports, and accept connections for services in Kubernetes of type NodePort and LoadBalancer. If possible, these nodes should not be exposed on the public internet entirely.</td></tr><tr><td>Kubernetes access to Cloud Provider API</td><td>Each cloud provider needs to grant a different set of permissions to the Kubernetes control plane and nodes. It is best to provide the cluster with cloud provider access that follows the <a href=https://en.wikipedia.org/wiki/Principle_of_least_privilege>principle of least privilege</a> for the resources it needs to administer. The <a href=https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#iam-roles>Kops documentation</a> provides information about IAM policies and roles.</td></tr><tr><td>Access to etcd</td><td>Access to etcd (the datastore of Kubernetes) should be limited to the control plane only. Depending on your configuration, you should attempt to use etcd over TLS. More information can be found in the <a href=https://github.com/etcd-io/etcd/tree/master/Documentation>etcd documentation</a>.</td></tr><tr><td>etcd Encryption</td><td>Wherever possible it's a good practice to encrypt all storage at rest, and since etcd holds the state of the entire cluster (including Secrets) its disk should especially be encrypted at rest.</td></tr></tbody></table><h2 id=cluster>Cluster</h2><p>There are two areas of concern for securing Kubernetes:</p><ul><li>Securing the cluster components that are configurable</li><li>Securing the applications which run in the cluster</li></ul><h3 id=cluster-components>Components of the Cluster</h3><p>If you want to protect your cluster from accidental or malicious access and adopt
good information practices, read and follow the advice about
<a href=/docs/tasks/administer-cluster/securing-a-cluster/>securing your cluster</a>.</p><h3 id=cluster-applications>Components in the cluster (your application)</h3><p>Depending on the attack surface of your application, you may want to focus on specific
aspects of security. For example: If you are running a service (Service A) that is critical
in a chain of other resources and a separate workload (Service B) which is
vulnerable to a resource exhaustion attack, then the risk of compromising Service A
is high if you do not limit the resources of Service B. The following table lists
areas of security concerns and recommendations for securing workloads running in Kubernetes:</p><table><thead><tr><th>Area of Concern for Workload Security</th><th>Recommendation</th></tr></thead><tbody><tr><td>RBAC Authorization (Access to the Kubernetes API)</td><td><a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/>https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a></td></tr><tr><td>Authentication</td><td><a href=https://kubernetes.io/docs/concepts/security/controlling-access/>https://kubernetes.io/docs/concepts/security/controlling-access/</a></td></tr><tr><td>Application secrets management (and encrypting them in etcd at rest)</td><td><a href=https://kubernetes.io/docs/concepts/configuration/secret/>https://kubernetes.io/docs/concepts/configuration/secret/</a><br><a href=https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/</a></td></tr><tr><td>Ensuring that pods meet defined Pod Security Standards</td><td><a href=https://kubernetes.io/docs/concepts/security/pod-security-standards/#policy-instantiation>https://kubernetes.io/docs/concepts/security/pod-security-standards/#policy-instantiation</a></td></tr><tr><td>Quality of Service (and Cluster resource management)</td><td><a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/>https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/</a></td></tr><tr><td>Network Policies</td><td><a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>https://kubernetes.io/docs/concepts/services-networking/network-policies/</a></td></tr><tr><td>TLS for Kubernetes Ingress</td><td><a href=https://kubernetes.io/docs/concepts/services-networking/ingress/#tls>https://kubernetes.io/docs/concepts/services-networking/ingress/#tls</a></td></tr></tbody></table><h2 id=container>Container</h2><p>Container security is outside the scope of this guide. Here are general recommendations and
links to explore this topic:</p><table><thead><tr><th>Area of Concern for Containers</th><th>Recommendation</th></tr></thead><tbody><tr><td>Container Vulnerability Scanning and OS Dependency Security</td><td>As part of an image build step, you should scan your containers for known vulnerabilities.</td></tr><tr><td>Image Signing and Enforcement</td><td>Sign container images to maintain a system of trust for the content of your containers.</td></tr><tr><td>Disallow privileged users</td><td>When constructing containers, consult your documentation for how to create users inside of the containers that have the least level of operating system privilege necessary in order to carry out the goal of the container.</td></tr><tr><td>Use container runtime with stronger isolation</td><td>Select <a href=/docs/concepts/containers/runtime-class/>container runtime classes</a> that provide stronger isolation.</td></tr></tbody></table><h2 id=code>Code</h2><p>Application code is one of the primary attack surfaces over which you have the most control.
While securing application code is outside of the Kubernetes security topic, here
are recommendations to protect application code:</p><h3 id=code-security>Code security</h3><table><caption style=display:none>Code security</caption><thead><tr><th>Area of Concern for Code</th><th>Recommendation</th></tr></thead><tbody><tr><td>Access over TLS only</td><td>If your code needs to communicate by TCP, perform a TLS handshake with the client ahead of time. With the exception of a few cases, encrypt everything in transit. Going one step further, it's a good idea to encrypt network traffic between services. This can be done through a process known as mutual TLS authentication or <a href=https://en.wikipedia.org/wiki/Mutual_authentication>mTLS</a> which performs a two sided verification of communication between two certificate holding services.</td></tr><tr><td>Limiting port ranges of communication</td><td>This recommendation may be a bit self-explanatory, but wherever possible you should only expose the ports on your service that are absolutely essential for communication or metric gathering.</td></tr><tr><td>3rd Party Dependency Security</td><td>It is a good practice to regularly scan your application's third party libraries for known security vulnerabilities. Each programming language has a tool for performing this check automatically.</td></tr><tr><td>Static Code Analysis</td><td>Most languages provide a way for a snippet of code to be analyzed for any potentially unsafe coding practices. Whenever possible you should perform checks using automated tooling that can scan codebases for common security errors. Some of the tools can be found at: <a href=https://owasp.org/www-community/Source_Code_Analysis_Tools>https://owasp.org/www-community/Source_Code_Analysis_Tools</a></td></tr><tr><td>Dynamic probing attacks</td><td>There are a few automated tools that you can run against your service to try some of the well known service attacks. These include SQL injection, CSRF, and XSS. One of the most popular dynamic analysis tools is the <a href=https://owasp.org/www-project-zap/>OWASP Zed Attack proxy</a> tool.</td></tr></tbody></table><h2 id=what-s-next>What's next</h2><p>Learn about related Kubernetes security topics:</p><ul><li><a href=/docs/concepts/security/pod-security-standards/>Pod security standards</a></li><li><a href=/docs/concepts/services-networking/network-policies/>Network policies for Pods</a></li><li><a href=/docs/concepts/security/controlling-access>Controlling Access to the Kubernetes API</a></li><li><a href=/docs/tasks/administer-cluster/securing-a-cluster/>Securing your cluster</a></li><li><a href=/docs/tasks/tls/managing-tls-in-a-cluster/>Data encryption in transit</a> for the control plane</li><li><a href=/docs/tasks/administer-cluster/encrypt-data/>Data encryption at rest</a></li><li><a href=/docs/concepts/configuration/secret/>Secrets in Kubernetes</a></li><li><a href=/docs/concepts/containers/runtime-class>Runtime class</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1fb24c1dd155f43849da490a74c4b8c5>9.2 - Pod Security Standards</h1><div class=lead>A detailed look at the different policy levels defined in the Pod Security Standards.</div><p>The Pod Security Standards define three different <em>policies</em> to broadly cover the security
spectrum. These policies are <em>cumulative</em> and range from highly-permissive to highly-restrictive.
This guide outlines the requirements of each policy.</p><table><thead><tr><th>Profile</th><th>Description</th></tr></thead><tbody><tr><td><strong style=white-space:nowrap>Privileged</strong></td><td>Unrestricted policy, providing the widest possible level of permissions. This policy allows for known privilege escalations.</td></tr><tr><td><strong style=white-space:nowrap>Baseline</strong></td><td>Minimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration.</td></tr><tr><td><strong style=white-space:nowrap>Restricted</strong></td><td>Heavily restricted policy, following current Pod hardening best practices.</td></tr></tbody></table><h2 id=profile-details>Profile Details</h2><h3 id=privileged>Privileged</h3><p><strong>The <em>Privileged</em> policy is purposely-open, and entirely unrestricted.</strong> This type of policy is
typically aimed at system- and infrastructure-level workloads managed by privileged, trusted users.</p><p>The Privileged policy is defined by an absence of restrictions. Allow-by-default
mechanisms (such as gatekeeper) may be Privileged by default. In contrast, for a deny-by-default mechanism (such as Pod
Security Policy) the Privileged policy should disable all restrictions.</p><h3 id=baseline>Baseline</h3><p><strong>The <em>Baseline</em> policy is aimed at ease of adoption for common containerized workloads while
preventing known privilege escalations.</strong> This policy is targeted at application operators and
developers of non-critical applications. The following listed controls should be
enforced/disallowed:</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In this table, wildcards (<code>*</code>) indicate all elements in a list. For example,
<code>spec.containers[*].securityContext</code> refers to the Security Context object for <em>all defined
containers</em>. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.</div><table><caption style=display:none>Baseline policy specification</caption><tbody><tr><th>Control</th><th>Policy</th></tr><tr><td style=white-space:nowrap>HostProcess</td><td><p>Windows pods offer the ability to run <a href=/docs/tasks/configure-pod-container/create-hostprocess-pod>HostProcess containers</a> which enables privileged access to the Windows node. Privileged access to the host is disallowed in the baseline policy.<div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.windowsOptions.hostProcess</code></li><li><code>spec.containers[*].securityContext.windowsOptions.hostProcess</code></li><li><code>spec.initContainers[*].securityContext.windowsOptions.hostProcess</code></li><li><code>spec.ephemeralContainers[*].securityContext.windowsOptions.hostProcess</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td style=white-space:nowrap>Host Namespaces</td><td><p>Sharing the host namespaces must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.hostNetwork</code></li><li><code>spec.hostPID</code></li><li><code>spec.hostIPC</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td style=white-space:nowrap>Privileged Containers</td><td><p>Privileged Pods disable most security mechanisms and must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.privileged</code></li><li><code>spec.initContainers[*].securityContext.privileged</code></li><li><code>spec.ephemeralContainers[*].securityContext.privileged</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td style=white-space:nowrap>Capabilities</td><td><p>Adding additional capabilities beyond those listed below must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.add</code></li><li><code>spec.initContainers[*].securityContext.capabilities.add</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>AUDIT_WRITE</code></li><li><code>CHOWN</code></li><li><code>DAC_OVERRIDE</code></li><li><code>FOWNER</code></li><li><code>FSETID</code></li><li><code>KILL</code></li><li><code>MKNOD</code></li><li><code>NET_BIND_SERVICE</code></li><li><code>SETFCAP</code></li><li><code>SETGID</code></li><li><code>SETPCAP</code></li><li><code>SETUID</code></li><li><code>SYS_CHROOT</code></li></ul></td></tr><tr><td style=white-space:nowrap>HostPath Volumes</td><td><p>HostPath volumes must be forbidden.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.volumes[*].hostPath</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li></ul></td></tr><tr><td style=white-space:nowrap>Host Ports</td><td><p>HostPorts should be disallowed, or at minimum restricted to a known list.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].ports[*].hostPort</code></li><li><code>spec.initContainers[*].ports[*].hostPort</code></li><li><code>spec.ephemeralContainers[*].ports[*].hostPort</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li>Known list</li><li><code>0</code></li></ul></td></tr><tr><td style=white-space:nowrap>AppArmor</td><td><p>On supported hosts, the <code>runtime/default</code> AppArmor profile is applied by default. The baseline policy should prevent overriding or disabling the default AppArmor profile, or restrict overrides to an allowed set of profiles.</p><p><strong>Restricted Fields</strong></p><ul><li><code>metadata.annotations["container.apparmor.security.beta.kubernetes.io/*"]</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>runtime/default</code></li><li><code>localhost/*</code></li></ul></td></tr><tr><td style=white-space:nowrap>SELinux</td><td><p>Setting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seLinuxOptions.type</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.type</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/""</li><li><code>container_t</code></li><li><code>container_init_t</code></li><li><code>container_kvm_t</code></li></ul><hr><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seLinuxOptions.user</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.securityContext.seLinuxOptions.role</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.role</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.role</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.role</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/""</li></ul></td></tr><tr><td style=white-space:nowrap><code>/proc</code> Mount Type</td><td><p>The default <code>/proc</code> masks are set up to reduce attack surface, and should be required.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.procMount</code></li><li><code>spec.initContainers[*].securityContext.procMount</code></li><li><code>spec.ephemeralContainers[*].securityContext.procMount</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>Default</code></li></ul></td></tr><tr><td>Seccomp</td><td><p>Seccomp profile must not be explicitly set to <code>Unconfined</code>.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seccompProfile.type</code></li><li><code>spec.containers[*].securityContext.seccompProfile.type</code></li><li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul></td></tr><tr><td style=white-space:nowrap>Sysctls</td><td><p>Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed "safe" subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.sysctls[*].name</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>kernel.shm_rmid_forced</code></li><li><code>net.ipv4.ip_local_port_range</code></li><li><code>net.ipv4.ip_unprivileged_port_start</code></li><li><code>net.ipv4.tcp_syncookies</code></li><li><code>net.ipv4.ping_group_range</code></li></ul></td></tr></tbody></table><h3 id=restricted>Restricted</h3><p><strong>The <em>Restricted</em> policy is aimed at enforcing current Pod hardening best practices, at the
expense of some compatibility.</strong> It is targeted at operators and developers of security-critical
applications, as well as lower-trust users. The following listed controls should be
enforced/disallowed:</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In this table, wildcards (<code>*</code>) indicate all elements in a list. For example,
<code>spec.containers[*].securityContext</code> refers to the Security Context object for <em>all defined
containers</em>. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.</div><table><caption style=display:none>Restricted policy specification</caption><tbody><tr><td><strong>Control</strong></td><td><strong>Policy</strong></td></tr><tr><td colspan=2><em>Everything from the baseline profile.</em></td></tr><tr><td style=white-space:nowrap>Volume Types</td><td><p>The restricted policy only permits the following volume types.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.volumes[*]</code></li></ul><p><strong>Allowed Values</strong></p>Every item in the <code>spec.volumes[*]</code> list must set one of the following fields to a non-null value:<ul><li><code>spec.volumes[*].configMap</code></li><li><code>spec.volumes[*].csi</code></li><li><code>spec.volumes[*].downwardAPI</code></li><li><code>spec.volumes[*].emptyDir</code></li><li><code>spec.volumes[*].ephemeral</code></li><li><code>spec.volumes[*].persistentVolumeClaim</code></li><li><code>spec.volumes[*].projected</code></li><li><code>spec.volumes[*].secret</code></li></ul></td></tr><tr><td style=white-space:nowrap>Privilege Escalation (v1.8+)</td><td><p>Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed. <em><a href=#policies-specific-to-linux>This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.initContainers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.ephemeralContainers[*].securityContext.allowPrivilegeEscalation</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>false</code></li></ul></td></tr><tr><td style=white-space:nowrap>Running as Non-root</td><td><p>Containers must be required to run as non-root users.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.runAsNonRoot</code></li><li><code>spec.containers[*].securityContext.runAsNonRoot</code></li><li><code>spec.initContainers[*].securityContext.runAsNonRoot</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsNonRoot</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>true</code></li></ul><small>The container fields may be undefined/<code>nil</code> if the pod-level
<code>spec.securityContext.runAsNonRoot</code> is set to <code>true</code>.</small></td></tr><tr><td style=white-space:nowrap>Running as Non-root user (v1.23+)</td><td><p>Containers must not set <tt>runAsUser</tt> to 0</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.runAsUser</code></li><li><code>spec.containers[*].securityContext.runAsUser</code></li><li><code>spec.initContainers[*].securityContext.runAsUser</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsUser</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>any non-zero value</li><li><code>undefined/null</code></li></ul></td></tr><tr><td style=white-space:nowrap>Seccomp (v1.19+)</td><td><p>Seccomp profile must be explicitly set to one of the allowed values. Both the <code>Unconfined</code> profile and the <em>absence</em> of a profile are prohibited. <em><a href=#policies-specific-to-linux>This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seccompProfile.type</code></li><li><code>spec.containers[*].securityContext.seccompProfile.type</code></li><li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul><small>The container fields may be undefined/<code>nil</code> if the pod-level
<code>spec.securityContext.seccompProfile.type</code> field is set appropriately.
Conversely, the pod-level field may be undefined/<code>nil</code> if _all_ container-
level fields are set.</small></td></tr><tr><td style=white-space:nowrap>Capabilities (v1.22+)</td><td><p>Containers must drop <code>ALL</code> capabilities, and are only permitted to add back
the <code>NET_BIND_SERVICE</code> capability. <em><a href=#policies-specific-to-linux>This is Linux only policy</a> in v1.25+ <code>(.spec.os.name != "windows")</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.drop</code></li><li><code>spec.initContainers[*].securityContext.capabilities.drop</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.drop</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Any list of capabilities that includes <code>ALL</code></li></ul><hr><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.add</code></li><li><code>spec.initContainers[*].securityContext.capabilities.add</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>NET_BIND_SERVICE</code></li></ul></td></tr></tbody></table><h2 id=policy-instantiation>Policy Instantiation</h2><p>Decoupling policy definition from policy instantiation allows for a common understanding and
consistent language of policies across clusters, independent of the underlying enforcement
mechanism.</p><p>As mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcement
of individual policies are not defined here.</p><p><a href=/docs/concepts/security/pod-security-admission/><strong>Pod Security Admission Controller</strong></a></p><ul><li><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-privileged.yaml download=security/podsecurity-privileged.yaml>Privileged namespace</a></li><li><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-baseline.yaml download=security/podsecurity-baseline.yaml>Baseline namespace</a></li><li><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-restricted.yaml download=security/podsecurity-restricted.yaml>Restricted namespace</a></li></ul><h3 id=alternatives>Alternatives</h3><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>Other alternatives for enforcing policies are being developed in the Kubernetes ecosystem, such as:</p><ul><li><a href=https://github.com/kubewarden>Kubewarden</a></li><li><a href=https://kyverno.io/policies/pod-security/>Kyverno</a></li><li><a href=https://github.com/open-policy-agent/gatekeeper>OPA Gatekeeper</a></li></ul><h2 id=pod-os-field>Pod OS field</h2><p>Kubernetes lets you use nodes that run either Linux or Windows. You can mix both kinds of
node in one cluster.
Windows in Kubernetes has some limitations and differentiators from Linux-based
workloads. Specifically, many of the Pod <code>securityContext</code> fields
<a href=/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext>have no effect on Windows</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubelets prior to v1.24 don't enforce the pod OS field, and if a cluster has nodes on versions earlier than v1.24 the restricted policies should be pinned to a version prior to v1.25.</div><h3 id=restricted-pod-security-standard-changes>Restricted Pod Security Standard changes</h3><p>Another important change, made in Kubernetes v1.25 is that the <em>restricted</em> Pod security
has been updated to use the <code>pod.spec.os.name</code> field. Based on the OS name, certain policies that are specific
to a particular OS can be relaxed for the other OS.</p><h4 id=os-specific-policy-controls>OS-specific policy controls</h4><p>Restrictions on the following controls are only required if <code>.spec.os.name</code> is not <code>windows</code>:</p><ul><li>Privilege Escalation</li><li>Seccomp</li><li>Linux Capabilities</li></ul><h2 id=faq>FAQ</h2><h3 id=why-isn-t-there-a-profile-between-privileged-and-baseline>Why isn't there a profile between privileged and baseline?</h3><p>The three profiles defined here have a clear linear progression from most secure (restricted) to least
secure (privileged), and cover a broad set of workloads. Privileges required above the baseline
policy are typically very application specific, so we do not offer a standard profile in this
niche. This is not to say that the privileged profile should always be used in this case, but that
policies in this space need to be defined on a case-by-case basis.</p><p>SIG Auth may reconsider this position in the future, should a clear need for other profiles arise.</p><h3 id=what-s-the-difference-between-a-security-profile-and-a-security-context>What's the difference between a security profile and a security context?</h3><p><a href=/docs/tasks/configure-pod-container/security-context/>Security Contexts</a> configure Pods and
Containers at runtime. Security contexts are defined as part of the Pod and container specifications
in the Pod manifest, and represent parameters to the container runtime.</p><p>Security profiles are control plane mechanisms to enforce specific settings in the Security Context,
as well as other related parameters outside the Security Context. As of July 2021,
<a href=/docs/concepts/security/pod-security-policy/>Pod Security Policies</a> are deprecated in favor of the
built-in <a href=/docs/concepts/security/pod-security-admission/>Pod Security Admission Controller</a>.</p><h3 id=what-about-sandboxed-pods>What about sandboxed Pods?</h3><p>There is not currently an API standard that controls whether a Pod is considered sandboxed or
not. Sandbox Pods may be identified by the use of a sandboxed runtime (such as gVisor or Kata
Containers), but there is no standard definition of what a sandboxed runtime is.</p><p>The protections necessary for sandboxed workloads can differ from others. For example, the need to
restrict privileged permissions is lessened when the workload is isolated from the underlying
kernel. This allows for workloads requiring heightened permissions to still be isolated.</p><p>Additionally, the protection of sandboxed workloads is highly dependent on the method of
sandboxing. As such, no single recommended profile is recommended for all sandboxed workloads.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bc9934fccfeaf880eec6ea79025c0381>9.3 - Pod Security Admission</h1><div class=lead>An overview of the Pod Security Admission Controller, which can enforce the Pod Security Standards.</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>The Kubernetes <a href=/docs/concepts/security/pod-security-standards/>Pod Security Standards</a> define
different isolation levels for Pods. These standards let you define how you want to restrict the
behavior of pods in a clear, consistent fashion.</p><p>Kubernetes offers a built-in <em>Pod Security</em> <a class=glossary-tooltip title='A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/admission-controllers/ target=_blank aria-label='admission controller'>admission controller</a> to enforce the Pod Security Standards. Pod security restrictions
are applied at the <a class=glossary-tooltip title='An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a> level when pods are
created.</p><h3 id=built-in-pod-security-admission-enforcement>Built-in Pod Security admission enforcement</h3><p>This page is part of the documentation for Kubernetes v1.25.
If you are running a different version of Kubernetes, consult the documentation for that release.</p><h2 id=pod-security-levels>Pod Security levels</h2><p>Pod Security admission places requirements on a Pod's <a href=/docs/tasks/configure-pod-container/security-context/>Security
Context</a> and other related fields according
to the three levels defined by the <a href=/docs/concepts/security/pod-security-standards>Pod Security
Standards</a>: <code>privileged</code>, <code>baseline</code>, and
<code>restricted</code>. Refer to the <a href=/docs/concepts/security/pod-security-standards>Pod Security Standards</a>
page for an in-depth look at those requirements.</p><h2 id=pod-security-admission-labels-for-namespaces>Pod Security Admission labels for namespaces</h2><p>Once the feature is enabled or the webhook is installed, you can configure namespaces to define the admission
control mode you want to use for pod security in each namespace. Kubernetes defines a set of
<a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a> that you can set to define which of the
predefined Pod Security Standard levels you want to use for a namespace. The label you select
defines what action the <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>
takes if a potential violation is detected:</p><table><caption style=display:none>Pod Security Admission modes</caption><thead><tr><th style=text-align:left>Mode</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><strong>enforce</strong></td><td style=text-align:left>Policy violations will cause the pod to be rejected.</td></tr><tr><td style=text-align:left><strong>audit</strong></td><td style=text-align:left>Policy violations will trigger the addition of an audit annotation to the event recorded in the <a href=/docs/tasks/debug/debug-cluster/audit/>audit log</a>, but are otherwise allowed.</td></tr><tr><td style=text-align:left><strong>warn</strong></td><td style=text-align:left>Policy violations will trigger a user-facing warning, but are otherwise allowed.</td></tr></tbody></table><p>A namespace can configure any or all modes, or even set a different level for different modes.</p><p>For each mode, there are two labels that determine the policy used:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#080;font-style:italic># The per-mode level label indicates which policy level to apply for the mode.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic>#</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># MODE must be one of `enforce`, `audit`, or `warn`.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># LEVEL must be one of `privileged`, `baseline`, or `restricted`.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>pod-security.kubernetes.io/&lt;MODE&gt;</span>:<span style=color:#bbb> </span>&lt;LEVEL&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># Optional: per-mode version label that can be used to pin the policy to the</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># version that shipped with a given Kubernetes minor version (for example v1.25).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic>#</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># MODE must be one of `enforce`, `audit`, or `warn`.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># VERSION must be a valid Kubernetes minor version, or `latest`.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>pod-security.kubernetes.io/&lt;MODE&gt;-version</span>:<span style=color:#bbb> </span>&lt;VERSION&gt;<span style=color:#bbb>
</span></span></span></code></pre></div><p>Check out <a href=/docs/tasks/configure-pod-container/enforce-standards-namespace-labels>Enforce Pod Security Standards with Namespace Labels</a> to see example usage.</p><h2 id=workload-resources-and-pod-templates>Workload resources and Pod templates</h2><p>Pods are often created indirectly, by creating a <a href=/docs/concepts/workloads/controllers/>workload
object</a> such as a <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a> or <a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Job>Job</a>. The workload object defines a
<em>Pod template</em> and a <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> for the
workload resource creates Pods based on that template. To help catch violations early, both the
audit and warning modes are applied to the workload resources. However, enforce mode is <strong>not</strong>
applied to workload resources, only to the resulting pod objects.</p><h2 id=exemptions>Exemptions</h2><p>You can define <em>exemptions</em> from pod security enforcement in order to allow the creation of pods that
would have otherwise been prohibited due to the policy associated with a given namespace.
Exemptions can be statically configured in the
<a href=/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller>Admission Controller configuration</a>.</p><p>Exemptions must be explicitly enumerated. Requests meeting exemption criteria are <em>ignored</em> by the
Admission Controller (all <code>enforce</code>, <code>audit</code> and <code>warn</code> behaviors are skipped). Exemption dimensions include:</p><ul><li><strong>Usernames:</strong> requests from users with an exempt authenticated (or impersonated) username are
ignored.</li><li><strong>RuntimeClassNames:</strong> pods and <a href=#workload-resources-and-pod-templates>workload resources</a> specifying an exempt runtime class name are
ignored.</li><li><strong>Namespaces:</strong> pods and <a href=#workload-resources-and-pod-templates>workload resources</a> in an exempt namespace are ignored.</li></ul><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Most pods are created by a controller in response to a <a href=#workload-resources-and-pod-templates>workload
resource</a>, meaning that exempting an end user will only
exempt them from enforcement when creating pods directly, but not when creating a workload resource.
Controller service accounts (such as <code>system:serviceaccount:kube-system:replicaset-controller</code>)
should generally not be exempted, as doing so would implicitly exempt any user that can create the
corresponding workload resource.</div><p>Updates to the following pod fields are exempt from policy checks, meaning that if a pod update
request only changes these fields, it will not be denied even if the pod is in violation of the
current policy level:</p><ul><li>Any metadata updates <strong>except</strong> changes to the seccomp or AppArmor annotations:<ul><li><code>seccomp.security.alpha.kubernetes.io/pod</code> (deprecated)</li><li><code>container.seccomp.security.alpha.kubernetes.io/*</code> (deprecated)</li><li><code>container.apparmor.security.beta.kubernetes.io/*</code></li></ul></li><li>Valid updates to <code>.spec.activeDeadlineSeconds</code></li><li>Valid updates to <code>.spec.tolerations</code></li></ul><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/concepts/security/pod-security-standards>Pod Security Standards</a></li><li><a href=/docs/setup/best-practices/enforcing-pod-security-standards>Enforcing Pod Security Standards</a></li><li><a href=/docs/tasks/configure-pod-container/enforce-standards-admission-controller>Enforce Pod Security Standards by Configuring the Built-in Admission Controller</a></li><li><a href=/docs/tasks/configure-pod-container/enforce-standards-namespace-labels>Enforce Pod Security Standards with Namespace Labels</a></li><li><a href=/docs/tasks/configure-pod-container/migrate-from-psp>Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ac71855bb20cbf21edc666e810f4103a>9.4 - Pod Security Policies</h1><div class="alert alert-warning" role=alert><h4 class=alert-heading>Removed feature</h4>PodSecurityPolicy was <a href=/blog/2021/04/08/kubernetes-1-21-release-announcement/#podsecuritypolicy-deprecation>deprecated</a>
in Kubernetes v1.21, and removed from Kubernetes in v1.25.</div><p>Instead of using PodSecurityPolicy, you can enforce similar restrictions on Pods using
either or both:</p><ul><li><a href=/docs/concepts/security/pod-security-admission/>Pod Security Admission</a></li><li>a 3rd party admission plugin, that you deploy and configure yourself</li></ul><p>For a migration guide, see <a href=/docs/tasks/configure-pod-container/migrate-from-psp/>Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a>.
For more information on the removal of this API,
see <a href=/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/>PodSecurityPolicy Deprecation: Past, Present, and Future</a>.</p><p>If you are not running Kubernetes v1.25, check the documentation for
your version of Kubernetes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9a68f631b6bc38c279bbc9a145e34ef2>9.5 - Security For Windows Nodes</h1><p>This page describes security considerations and best practices specific to the Windows operating system.</p><h2 id=protection-for-secret-data-on-nodes>Protection for Secret data on nodes</h2><p>On Windows, data from Secrets are written out in clear text onto the node's local
storage (as compared to using tmpfs / in-memory filesystems on Linux). As a cluster
operator, you should take both of the following additional measures:</p><ol><li>Use file ACLs to secure the Secrets' file location.</li><li>Apply volume-level encryption using
<a href=https://docs.microsoft.com/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server>BitLocker</a>.</li></ol><h2 id=container-users>Container users</h2><p><a href=/docs/tasks/configure-pod-container/configure-runasusername>RunAsUsername</a>
can be specified for Windows Pods or containers to execute the container
processes as specific user. This is roughly equivalent to
<a href=/docs/concepts/security/pod-security-policy/#users-and-groups>RunAsUser</a>.</p><p>Windows containers offer two default user accounts, ContainerUser and ContainerAdministrator.
The differences between these two user accounts are covered in
<a href=https://docs.microsoft.com/virtualization/windowscontainers/manage-containers/container-security#when-to-use-containeradmin-and-containeruser-user-accounts>When to use ContainerAdmin and ContainerUser user accounts</a>
within Microsoft's <em>Secure Windows containers</em> documentation.</p><p>Local users can be added to container images during the container build process.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><ul><li><a href=https://hub.docker.com/_/microsoft-windows-nanoserver>Nano Server</a> based images run as
<code>ContainerUser</code> by default</li><li><a href=https://hub.docker.com/_/microsoft-windows-servercore>Server Core</a> based images run as
<code>ContainerAdministrator</code> by default</li></ul></div><p>Windows containers can also run as Active Directory identities by utilizing
<a href=/docs/tasks/configure-pod-container/configure-gmsa/>Group Managed Service Accounts</a></p><h2 id=pod-level-security-isolation>Pod-level security isolation</h2><p>Linux-specific pod security context mechanisms (such as SELinux, AppArmor, Seccomp, or custom
POSIX capabilities) are not supported on Windows nodes.</p><p>Privileged containers are <a href=/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext>not supported</a>
on Windows.
Instead <a href=/docs/tasks/configure-pod-container/create-hostprocess-pod>HostProcess containers</a>
can be used on Windows to perform many of the tasks performed by privileged containers on Linux.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4d77d1ae4c06aa14f54b385191627881>9.6 - Controlling Access to the Kubernetes API</h1><p>This page provides an overview of controlling access to the Kubernetes API.</p><p>Users access the <a href=/docs/concepts/overview/kubernetes-api/>Kubernetes API</a> using <code>kubectl</code>,
client libraries, or by making REST requests. Both human users and
<a href=/docs/tasks/configure-pod-container/configure-service-account/>Kubernetes service accounts</a> can be
authorized for API access.
When a request reaches the API, it goes through several stages, illustrated in the
following diagram:</p><p><img src=/images/docs/admin/access-control-overview.svg alt="Diagram of request handling steps for Kubernetes API request"></p><h2 id=transport-security>Transport security</h2><p>By default, the Kubernetes API server listens on port 6443 on the first non-localhost network interface, protected by TLS. In a typical production Kubernetes cluster, the API serves on port 443. The port can be changed with the <code>--secure-port</code>, and the listening IP address with the <code>--bind-address</code> flag.</p><p>The API server presents a certificate. This certificate may be signed using
a private certificate authority (CA), or based on a public key infrastructure linked
to a generally recognized CA. The certificate and corresponding private key can be set by using the <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> flags.</p><p>If your cluster uses a private certificate authority, you need a copy of that CA
certificate configured into your <code>~/.kube/config</code> on the client, so that you can
trust the connection and be confident it was not intercepted.</p><p>Your client can present a TLS client certificate at this stage.</p><h2 id=authentication>Authentication</h2><p>Once TLS is established, the HTTP request moves to the Authentication step.
This is shown as step <strong>1</strong> in the diagram.
The cluster creation script or cluster admin configures the API server to run
one or more Authenticator modules.
Authenticators are described in more detail in
<a href=/docs/reference/access-authn-authz/authentication/>Authentication</a>.</p><p>The input to the authentication step is the entire HTTP request; however, it typically
examines the headers and/or client certificate.</p><p>Authentication modules include client certificates, password, and plain tokens,
bootstrap tokens, and JSON Web Tokens (used for service accounts).</p><p>Multiple authentication modules can be specified, in which case each one is tried in sequence,
until one of them succeeds.</p><p>If the request cannot be authenticated, it is rejected with HTTP status code 401.
Otherwise, the user is authenticated as a specific <code>username</code>, and the user name
is available to subsequent steps to use in their decisions. Some authenticators
also provide the group memberships of the user, while other authenticators
do not.</p><p>While Kubernetes uses usernames for access control decisions and in request logging,
it does not have a <code>User</code> object nor does it store usernames or other information about
users in its API.</p><h2 id=authorization>Authorization</h2><p>After the request is authenticated as coming from a specific user, the request must be authorized. This is shown as step <strong>2</strong> in the diagram.</p><p>A request must include the username of the requester, the requested action, and the object affected by the action. The request is authorized if an existing policy declares that the user has permissions to complete the requested action.</p><p>For example, if Bob has the policy below, then he can read pods only in the namespace <code>projectCaribou</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;abac.authorization.kubernetes.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Policy&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;spec&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;user&#34;</span>: <span style=color:#b44>&#34;bob&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;projectCaribou&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;pods&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;readonly&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If Bob makes the following request, the request is authorized because he is allowed to read objects in the <code>projectCaribou</code> namespace:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;authorization.k8s.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;SubjectAccessReview&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;spec&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;resourceAttributes&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;projectCaribou&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;verb&#34;</span>: <span style=color:#b44>&#34;get&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;group&#34;</span>: <span style=color:#b44>&#34;unicorn.example.org&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;pods&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If Bob makes a request to write (<code>create</code> or <code>update</code>) to the objects in the <code>projectCaribou</code> namespace, his authorization is denied. If Bob makes a request to read (<code>get</code>) objects in a different namespace such as <code>projectFish</code>, then his authorization is denied.</p><p>Kubernetes authorization requires that you use common REST attributes to interact with existing organization-wide or cloud-provider-wide access control systems. It is important to use REST formatting because these control systems might interact with other APIs besides the Kubernetes API.</p><p>Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode. When an administrator creates a cluster, they configure the authorization modules that should be used in the API server. If more than one authorization modules are configured, Kubernetes checks each module, and if any module authorizes the request, then the request can proceed. If all of the modules deny the request, then the request is denied (HTTP status code 403).</p><p>To learn more about Kubernetes authorization, including details about creating policies using the supported authorization modules, see <a href=/docs/reference/access-authn-authz/authorization/>Authorization</a>.</p><h2 id=admission-control>Admission control</h2><p>Admission Control modules are software modules that can modify or reject requests.
In addition to the attributes available to Authorization modules, Admission
Control modules can access the contents of the object that is being created or modified.</p><p>Admission controllers act on requests that create, modify, delete, or connect to (proxy) an object.
Admission controllers do not act on requests that merely read objects.
When multiple admission controllers are configured, they are called in order.</p><p>This is shown as step <strong>3</strong> in the diagram.</p><p>Unlike Authentication and Authorization modules, if any admission controller module
rejects, then the request is immediately rejected.</p><p>In addition to rejecting objects, admission controllers can also set complex defaults for
fields.</p><p>The available Admission Control modules are described in <a href=/docs/reference/access-authn-authz/admission-controllers/>Admission Controllers</a>.</p><p>Once a request passes all admission controllers, it is validated using the validation routines
for the corresponding API object, and then written to the object store (shown as step <strong>4</strong>).</p><h2 id=auditing>Auditing</h2><p>Kubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster.
The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.</p><p>For more information, see <a href=/docs/tasks/debug/debug-cluster/audit/>Auditing</a>.</p><h2 id=what-s-next>What's next</h2><p>Read more documentation on authentication, authorization and API access control:</p><ul><li><a href=/docs/reference/access-authn-authz/authentication/>Authenticating</a><ul><li><a href=/docs/reference/access-authn-authz/bootstrap-tokens/>Authenticating with Bootstrap Tokens</a></li></ul></li><li><a href=/docs/reference/access-authn-authz/admission-controllers/>Admission Controllers</a><ul><li><a href=/docs/reference/access-authn-authz/extensible-admission-controllers/>Dynamic Admission Control</a></li></ul></li><li><a href=/docs/reference/access-authn-authz/authorization/>Authorization</a><ul><li><a href=/docs/reference/access-authn-authz/rbac/>Role Based Access Control</a></li><li><a href=/docs/reference/access-authn-authz/abac/>Attribute Based Access Control</a></li><li><a href=/docs/reference/access-authn-authz/node/>Node Authorization</a></li><li><a href=/docs/reference/access-authn-authz/webhook/>Webhook Authorization</a></li></ul></li><li><a href=/docs/reference/access-authn-authz/certificate-signing-requests/>Certificate Signing Requests</a><ul><li>including <a href=/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection>CSR approval</a>
and <a href=/docs/reference/access-authn-authz/certificate-signing-requests/#signing>certificate signing</a></li></ul></li><li>Service accounts<ul><li><a href=/docs/tasks/configure-pod-container/configure-service-account/>Developer guide</a></li><li><a href=/docs/reference/access-authn-authz/service-accounts-admin/>Administration</a></li></ul></li></ul><p>You can learn about:</p><ul><li>how Pods can use
<a href=/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials>Secrets</a>
to obtain API credentials.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-07f58aa0218d666795499c2e2306ff96>9.7 - Role Based Access Control Good Practices</h1><div class=lead>Principles and practices for good RBAC design for cluster operators.</div><p>Kubernetes <a class=glossary-tooltip title='Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/rbac/ target=_blank aria-label=RBAC>RBAC</a> is a key security control
to ensure that cluster users and workloads have only the access to resources required to
execute their roles. It is important to ensure that, when designing permissions for cluster
users, the cluster administrator understands the areas where privilege escalation could occur,
to reduce the risk of excessive access leading to security incidents.</p><p>The good practices laid out here should be read in conjunction with the general
<a href=/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update>RBAC documentation</a>.</p><h2 id=general-good-practice>General good practice</h2><h3 id=least-privilege>Least privilege</h3><p>Ideally, minimal RBAC rights should be assigned to users and service accounts. Only permissions
explicitly required for their operation should be used. While each cluster will be different,
some general rules that can be applied are :</p><ul><li>Assign permissions at the namespace level where possible. Use RoleBindings as opposed to
ClusterRoleBindings to give users rights only within a specific namespace.</li><li>Avoid providing wildcard permissions when possible, especially to all resources.
As Kubernetes is an extensible system, providing wildcard access gives rights
not just to all object types that currently exist in the cluster, but also to all object types
which are created in the future.</li><li>Administrators should not use <code>cluster-admin</code> accounts except where specifically needed.
Providing a low privileged account with
<a href=/docs/reference/access-authn-authz/authentication/#user-impersonation>impersonation rights</a>
can avoid accidental modification of cluster resources.</li><li>Avoid adding users to the <code>system:masters</code> group. Any user who is a member of this group
bypasses all RBAC rights checks and will always have unrestricted superuser access, which cannot be
revoked by removing RoleBindings or ClusterRoleBindings. As an aside, if a cluster is
using an authorization webhook, membership of this group also bypasses that webhook (requests
from users who are members of that group are never sent to the webhook)</li></ul><h3 id=minimize-distribution-of-privileged-tokens>Minimize distribution of privileged tokens</h3><p>Ideally, pods shouldn't be assigned service accounts that have been granted powerful permissions
(for example, any of the rights listed under <a href=#privilege-escalation-risks>privilege escalation risks</a>).
In cases where a workload requires powerful permissions, consider the following practices:</p><ul><li>Limit the number of nodes running powerful pods. Ensure that any DaemonSets you run
are necessary and are run with least privilege to limit the blast radius of container escapes.</li><li>Avoid running powerful pods alongside untrusted or publicly-exposed ones. Consider using
<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>Taints and Toleration</a>,
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity>NodeAffinity</a>, or
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>PodAntiAffinity</a>
to ensure pods don't run alongside untrusted or less-trusted Pods. Pay especial attention to
situations where less-trustworthy Pods are not meeting the <strong>Restricted</strong> Pod Security Standard.</li></ul><h3 id=hardening>Hardening</h3><p>Kubernetes defaults to providing access which may not be required in every cluster. Reviewing
the RBAC rights provided by default can provide opportunities for security hardening.
In general, changes should not be made to rights provided to <code>system:</code> accounts some options
to harden cluster rights exist:</p><ul><li>Review bindings for the <code>system:unauthenticated</code> group and remove them where possible, as this gives
access to anyone who can contact the API server at a network level.</li><li>Avoid the default auto-mounting of service account tokens by setting
<code>automountServiceAccountToken: false</code>. For more details, see
<a href=/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server>using default service account token</a>.
Setting this value for a Pod will overwrite the service account setting, workloads
which require service account tokens can still mount them.</li></ul><h3 id=periodic-review>Periodic review</h3><p>It is vital to periodically review the Kubernetes RBAC settings for redundant entries and
possible privilege escalations.
If an attacker is able to create a user account with the same name as a deleted user,
they can automatically inherit all the rights of the deleted user, especially the
rights assigned to that user.</p><h2 id=privilege-escalation-risks>Kubernetes RBAC - privilege escalation risks</h2><p>Within Kubernetes RBAC there are a number of privileges which, if granted, can allow a user or a service account
to escalate their privileges in the cluster or affect systems outside the cluster.</p><p>This section is intended to provide visibility of the areas where cluster operators
should take care, to ensure that they do not inadvertently allow for more access to clusters than intended.</p><h3 id=listing-secrets>Listing secrets</h3><p>It is generally clear that allowing <code>get</code> access on Secrets will allow a user to read their contents.
It is also important to note that <code>list</code> and <code>watch</code> access also effectively allow for users to reveal the Secret contents.
For example, when a List response is returned (for example, via <code>kubectl get secrets -A -o yaml</code>), the response
includes the contents of all Secrets.</p><h3 id=workload-creation>Workload creation</h3><p>Permission to create workloads (either Pods, or
<a href=/docs/concepts/workloads/controllers/>workload resources</a> that manage Pods) in a namespace
implicitly grants access to many other resources in that namespace, such as Secrets, ConfigMaps, and
PersistentVolumes that can be mounted in Pods. Additionally, since Pods can run as any
<a href=/docs/reference/access-authn-authz/service-accounts-admin/>ServiceAccount</a>, granting permission
to create workloads also implicitly grants the API access levels of any service account in that
namespace.</p><p>Users who can run privileged Pods can use that access to gain node access and potentially to
further elevate their privileges. Where you do not fully trust a user or other principal
with the ability to create suitably secure and isolated Pods, you should enforce either the
<strong>Baseline</strong> or <strong>Restricted</strong> Pod Security Standard.
You can use <a href=/docs/concepts/security/pod-security-admission/>Pod Security admission</a>
or other (third party) mechanisms to implement that enforcement.</p><p>For these reasons, namespaces should be used to separate resources requiring different levels of
trust or tenancy. It is still considered best practice to follow <a href=#least-privilege>least privilege</a>
principles and assign the minimum set of permissions, but boundaries within a namespace should be
considered weak.</p><h3 id=persistent-volume-creation>Persistent volume creation</h3><p>As noted in the <a href=/docs/concepts/security/pod-security-policy/#volumes-and-file-systems>PodSecurityPolicy</a>
documentation, access to create PersistentVolumes can allow for escalation of access to the underlying host.
Where access to persistent storage is required trusted administrators should create
PersistentVolumes, and constrained users should use PersistentVolumeClaims to access that storage.</p><h3 id=access-to-proxy-subresource-of-nodes>Access to <code>proxy</code> subresource of Nodes</h3><p>Users with access to the proxy sub-resource of node objects have rights to the Kubelet API,
which allows for command execution on every pod on the node(s) to which they have rights.
This access bypasses audit logging and admission control, so care should be taken before
granting rights to this resource.</p><h3 id=escalate-verb>Escalate verb</h3><p>Generally, the RBAC system prevents users from creating clusterroles with more rights than the user possesses.
The exception to this is the <code>escalate</code> verb. As noted in the <a href=/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update>RBAC documentation</a>,
users with this right can effectively escalate their privileges.</p><h3 id=bind-verb>Bind verb</h3><p>Similar to the <code>escalate</code> verb, granting users this right allows for the bypass of Kubernetes
in-built protections against privilege escalation, allowing users to create bindings to
roles with rights they do not already have.</p><h3 id=impersonate-verb>Impersonate verb</h3><p>This verb allows users to impersonate and gain the rights of other users in the cluster.
Care should be taken when granting it, to ensure that excessive permissions cannot be gained
via one of the impersonated accounts.</p><h3 id=csrs-and-certificate-issuing>CSRs and certificate issuing</h3><p>The CSR API allows for users with <code>create</code> rights to CSRs and <code>update</code> rights on <code>certificatesigningrequests/approval</code>
where the signer is <code>kubernetes.io/kube-apiserver-client</code> to create new client certificates
which allow users to authenticate to the cluster. Those client certificates can have arbitrary
names including duplicates of Kubernetes system components. This will effectively allow for privilege escalation.</p><h3 id=token-request>Token request</h3><p>Users with <code>create</code> rights on <code>serviceaccounts/token</code> can create TokenRequests to issue
tokens for existing service accounts.</p><h3 id=control-admission-webhooks>Control admission webhooks</h3><p>Users with control over <code>validatingwebhookconfigurations</code> or <code>mutatingwebhookconfigurations</code>
can control webhooks that can read any object admitted to the cluster, and in the case of
mutating webhooks, also mutate admitted objects.</p><h2 id=denial-of-service-risks>Kubernetes RBAC - denial of service risks</h2><h3 id=object-creation-dos>Object creation denial-of-service</h3><p>Users who have rights to create objects in a cluster may be able to create sufficient large
objects to create a denial of service condition either based on the size or number of objects, as discussed in
<a href=https://github.com/kubernetes/kubernetes/issues/107325>etcd used by Kubernetes is vulnerable to OOM attack</a>. This may be
specifically relevant in multi-tenant clusters if semi-trusted or untrusted users
are allowed limited access to a system.</p><p>One option for mitigation of this issue would be to use
<a href=/docs/concepts/policy/resource-quotas/#object-count-quota>resource quotas</a>
to limit the quantity of objects which can be created.</p><h2 id=what-s-next>What's next</h2><ul><li>To learn more about RBAC, see the <a href=/docs/reference/access-authn-authz/rbac/>RBAC documentation</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a7863bfad3d69f33f5b318b9028eecb8>9.8 - Good practices for Kubernetes Secrets</h1><div class=lead>Principles and practices for good Secret management for cluster administrators and application developers.</div><p><p>In Kubernetes, a Secret is an object that stores sensitive information, such as passwords, OAuth tokens, and SSH keys.</p></p><p>Secrets give you more control over how sensitive information is used and reduces
the risk of accidental exposure. Secret values are encoded as base64 strings and
are stored unencrypted by default, but can be configured to be
<a href=/docs/tasks/administer-cluster/encrypt-data/#ensure-all-secrets-are-encrypted>encrypted at rest</a>.</p><p>A <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> can reference the Secret in
a variety of ways, such as in a volume mount or as an environment variable.
Secrets are designed for confidential data and
<a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMaps</a> are
designed for non-confidential data.</p><p>The following good practices are intended for both cluster administrators and
application developers. Use these guidelines to improve the security of your
sensitive information in Secret objects, as well as to more effectively manage
your Secrets.</p><h2 id=cluster-administrators>Cluster administrators</h2><p>This section provides good practices that cluster administrators can use to
improve the security of confidential information in the cluster.</p><h3 id=configure-encryption-at-rest>Configure encryption at rest</h3><p>By default, Secret objects are stored unencrypted in <a class=glossary-tooltip title='Consistent and highly-available key value store used as backing store of Kubernetes for all cluster data.' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>. You should configure encryption of your Secret
data in <code>etcd</code>. For instructions, refer to
<a href=/docs/tasks/administer-cluster/encrypt-data/>Encrypt Secret Data at Rest</a>.</p><h3 id=least-privilege-secrets>Configure least-privilege access to Secrets</h3><p>When planning your access control mechanism, such as Kubernetes
<a class=glossary-tooltip title='Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/rbac/ target=_blank aria-label='Role-based Access Control'>Role-based Access Control</a> <a href=/docs/reference/access-authn-authz/rbac/>(RBAC)</a>,
consider the following guidelines for access to <code>Secret</code> objects. You should
also follow the other guidelines in
<a href=/docs/concepts/security/rbac-good-practices>RBAC good practices</a>.</p><ul><li><strong>Components</strong>: Restrict <code>watch</code> or <code>list</code> access to only the most
privileged, system-level components. Only grant <code>get</code> access for Secrets if
the component's normal behavior requires it.</li><li><strong>Humans</strong>: Restrict <code>get</code>, <code>watch</code>, or <code>list</code> access to Secrets. Only allow
cluster administrators to access <code>etcd</code>. This includes read-only access. For
more complex access control, such as restricting access to Secrets with
specific annotations, consider using third-party authorization mechanisms.</li></ul><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Granting <code>list</code> access to Secrets implicitly lets the subject fetch the
contents of the Secrets.</div><p>A user who can create a Pod that uses a Secret can also see the value of that
Secret. Even if cluster policies do not allow a user to read the Secret
directly, the same user could have access to run a Pod that then exposes the
Secret. You can detect or limit the impact caused by Secret data being exposed,
either intentionally or unintentionally, by a user with this access. Some
recommendations include:</p><ul><li>Use short-lived Secrets</li><li>Implement audit rules that alert on specific events, such as concurrent
reading of multiple Secrets by a single user</li></ul><h3 id=improve-etcd-management-policies>Improve etcd management policies</h3><p>Consider wiping or shredding the durable storage used by <code>etcd</code> once it is
no longer in use.</p><p>If there are multiple <code>etcd</code> instances, configure encrypted SSL/TLS
communication between the instances to protect the Secret data in transit.</p><h3 id=configure-access-to-external-secrets>Configure access to external Secrets</h3><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>You can use third-party Secrets store providers to keep your confidential data
outside your cluster and then configure Pods to access that information.
The <a href=https://secrets-store-csi-driver.sigs.k8s.io/>Kubernetes Secrets Store CSI Driver</a>
is a DaemonSet that lets the kubelet retrieve Secrets from external stores, and
mount the Secrets as a volume into specific Pods that you authorize to access
the data.</p><p>For a list of supported providers, refer to
<a href=https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver>Providers for the Secret Store CSI Driver</a>.</p><h2 id=developers>Developers</h2><p>This section provides good practices for developers to use to improve the
security of confidential data when building and deploying Kubernetes resources.</p><h3 id=restrict-secret-access-to-specific-containers>Restrict Secret access to specific containers</h3><p>If you are defining multiple containers in a Pod, and only one of those
containers needs access to a Secret, define the volume mount or environment
variable configuration so that the other containers do not have access to that
Secret.</p><h3 id=protect-secret-data-after-reading>Protect Secret data after reading</h3><p>Applications still need to protect the value of confidential information after
reading it from an environment variable or volume. For example, your
application must avoid logging the secret data in the clear or transmitting it
to an untrusted party.</p><h3 id=avoid-sharing-secret-manifests>Avoid sharing Secret manifests</h3><p>If you configure a Secret through a
<a class=glossary-tooltip title='A serialized specification of one or more Kubernetes API objects.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-manifest' target=_blank aria-label=manifest>manifest</a>, with the secret
data encoded as base64, sharing this file or checking it in to a source
repository means the secret is available to everyone who can read the manifest.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Base64 encoding is <em>not</em> an encryption method, it provides no additional
confidentiality over plain text.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-9dd9b8c71fa39ff803fd15b0e784069d>9.9 - Multi-tenancy</h1><p>This page provides an overview of available configuration options and best practices for cluster
multi-tenancy.</p><p>Sharing clusters saves costs and simplifies administration. However, sharing clusters also
presents challenges such as security, fairness, and managing <em>noisy neighbors</em>.</p><p>Clusters can be shared in many ways. In some cases, different applications may run in the same
cluster. In other cases, multiple instances of the same application may run in the same cluster,
one for each end user. All these types of sharing are frequently described using the umbrella term
<em>multi-tenancy</em>.</p><p>While Kubernetes does not have first-class concepts of end users or tenants, it provides several
features to help manage different tenancy requirements. These are discussed below.</p><h2 id=use-cases>Use cases</h2><p>The first step to determining how to share your cluster is understanding your use case, so you can
evaluate the patterns and tools available. In general, multi-tenancy in Kubernetes clusters falls
into two broad categories, though many variations and hybrids are also possible.</p><h3 id=multiple-teams>Multiple teams</h3><p>A common form of multi-tenancy is to share a cluster between multiple teams within an
organization, each of whom may operate one or more workloads. These workloads frequently need to
communicate with each other, and with other workloads located on the same or different clusters.</p><p>In this scenario, members of the teams often have direct access to Kubernetes resources via tools
such as <code>kubectl</code>, or indirect access through GitOps controllers or other types of release
automation tools. There is often some level of trust between members of different teams, but
Kubernetes policies such as RBAC, quotas, and network policies are essential to safely and fairly
share clusters.</p><h3 id=multiple-customers>Multiple customers</h3><p>The other major form of multi-tenancy frequently involves a Software-as-a-Service (SaaS) vendor
running multiple instances of a workload for customers. This business model is so strongly
associated with this deployment style that many people call it "SaaS tenancy." However, a better
term might be "multi-customer tenancy,” since SaaS vendors may also use other deployment models,
and this deployment model can also be used outside of SaaS.</p><p>In this scenario, the customers do not have access to the cluster; Kubernetes is invisible from
their perspective and is only used by the vendor to manage the workloads. Cost optimization is
frequently a critical concern, and Kubernetes policies are used to ensure that the workloads are
strongly isolated from each other.</p><h2 id=terminology>Terminology</h2><h3 id=tenants>Tenants</h3><p>When discussing multi-tenancy in Kubernetes, there is no single definition for a "tenant".
Rather, the definition of a tenant will vary depending on whether multi-team or multi-customer
tenancy is being discussed.</p><p>In multi-team usage, a tenant is typically a team, where each team typically deploys a small
number of workloads that scales with the complexity of the service. However, the definition of
"team" may itself be fuzzy, as teams may be organized into higher-level divisions or subdivided
into smaller teams.</p><p>By contrast, if each team deploys dedicated workloads for each new client, they are using a
multi-customer model of tenancy. In this case, a "tenant" is simply a group of users who share a
single workload. This may be as large as an entire company, or as small as a single team at that
company.</p><p>In many cases, the same organization may use both definitions of "tenants" in different contexts.
For example, a platform team may offer shared services such as security tools and databases to
multiple internal “customers” and a SaaS vendor may also have multiple teams sharing a development
cluster. Finally, hybrid architectures are also possible, such as a SaaS provider using a
combination of per-customer workloads for sensitive data, combined with multi-tenant shared
services.</p><figure class=diagram-large><img src=/images/docs/multi-tenancy.png><figcaption><h4>A cluster showing coexisting tenancy models</h4></figcaption></figure><h3 id=isolation>Isolation</h3><p>There are several ways to design and build multi-tenant solutions with Kubernetes. Each of these
methods comes with its own set of tradeoffs that impact the isolation level, implementation
effort, operational complexity, and cost of service.</p><p>A Kubernetes cluster consists of a control plane which runs Kubernetes software, and a data plane
consisting of worker nodes where tenant workloads are executed as pods. Tenant isolation can be
applied in both the control plane and the data plane based on organizational requirements.</p><p>The level of isolation offered is sometimes described using terms like “hard” multi-tenancy, which
implies strong isolation, and “soft” multi-tenancy, which implies weaker isolation. In particular,
"hard" multi-tenancy is often used to describe cases where the tenants do not trust each other,
often from security and resource sharing perspectives (e.g. guarding against attacks such as data
exfiltration or DoS). Since data planes typically have much larger attack surfaces, "hard"
multi-tenancy often requires extra attention to isolating the data-plane, though control plane
isolation also remains critical.</p><p>However, the terms "hard" and "soft" can often be confusing, as there is no single definition that
will apply to all users. Rather, "hardness" or "softness" is better understood as a broad
spectrum, with many different techniques that can be used to maintain different types of isolation
in your clusters, based on your requirements.</p><p>In more extreme cases, it may be easier or necessary to forgo any cluster-level sharing at all and
assign each tenant their dedicated cluster, possibly even running on dedicated hardware if VMs are
not considered an adequate security boundary. This may be easier with managed Kubernetes clusters,
where the overhead of creating and operating clusters is at least somewhat taken on by a cloud
provider. The benefit of stronger tenant isolation must be evaluated against the cost and
complexity of managing multiple clusters. The <a href=https://git.k8s.io/community/sig-multicluster/README.md>Multi-cluster SIG</a>
is responsible for addressing these types of use cases.</p><p>The remainder of this page focuses on isolation techniques used for shared Kubernetes clusters.
However, even if you are considering dedicated clusters, it may be valuable to review these
recommendations, as it will give you the flexibility to shift to shared clusters in the future if
your needs or capabilities change.</p><h2 id=control-plane-isolation>Control plane isolation</h2><p>Control plane isolation ensures that different tenants cannot access or affect each others'
Kubernetes API resources.</p><h3 id=namespaces>Namespaces</h3><p>In Kubernetes, a <a class=glossary-tooltip title='An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=Namespace>Namespace</a> provides a
mechanism for isolating groups of API resources within a single cluster. This isolation has two
key dimensions:</p><ol><li><p>Object names within a namespace can overlap with names in other namespaces, similar to files in
folders. This allows tenants to name their resources without having to consider what other
tenants are doing.</p></li><li><p>Many Kubernetes security policies are scoped to namespaces. For example, RBAC Roles and Network
Policies are namespace-scoped resources. Using RBAC, Users and Service Accounts can be
restricted to a namespace.</p></li></ol><p>In a multi-tenant environment, a Namespace helps segment a tenant's workload into a logical and
distinct management unit. In fact, a common practice is to isolate every workload in its own
namespace, even if multiple workloads are operated by the same tenant. This ensures that each
workload has its own identity and can be configured with an appropriate security policy.</p><p>The namespace isolation model requires configuration of several other Kubernetes resources,
networking plugins, and adherence to security best practices to properly isolate tenant workloads.
These considerations are discussed below.</p><h3 id=access-controls>Access controls</h3><p>The most important type of isolation for the control plane is authorization. If teams or their
workloads can access or modify each others' API resources, they can change or disable all other
types of policies thereby negating any protection those policies may offer. As a result, it is
critical to ensure that each tenant has the appropriate access to only the namespaces they need,
and no more. This is known as the "Principle of Least Privilege."</p><p>Role-based access control (RBAC) is commonly used to enforce authorization in the Kubernetes
control plane, for both users and workloads (service accounts).
<a href=/docs/reference/access-authn-authz/rbac/#role-and-clusterrole>Roles</a> and
<a href=/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding>RoleBindings</a> are
Kubernetes objects that are used at a namespace level to enforce access control in your
application; similar objects exist for authorizing access to cluster-level objects, though these
are less useful for multi-tenant clusters.</p><p>In a multi-team environment, RBAC must be used to restrict tenants' access to the appropriate
namespaces, and ensure that cluster-wide resources can only be accessed or modified by privileged
users such as cluster administrators.</p><p>If a policy ends up granting a user more permissions than they need, this is likely a signal that
the namespace containing the affected resources should be refactored into finer-grained
namespaces. Namespace management tools may simplify the management of these finer-grained
namespaces by applying common RBAC policies to different namespaces, while still allowing
fine-grained policies where necessary.</p><h3 id=quotas>Quotas</h3><p>Kubernetes workloads consume node resources, like CPU and memory. In a multi-tenant environment,
you can use <a href=/docs/concepts/policy/resource-quotas/>Resource Quotas</a> to manage resource usage of
tenant workloads. For the multiple teams use case, where tenants have access to the Kubernetes
API, you can use resource quotas to limit the number of API resources (for example: the number of
Pods, or the number of ConfigMaps) that a tenant can create. Limits on object count ensure
fairness and aim to avoid <em>noisy neighbor</em> issues from affecting other tenants that share a
control plane.</p><p>Resource quotas are namespaced objects. By mapping tenants to namespaces, cluster admins can use
quotas to ensure that a tenant cannot monopolize a cluster's resources or overwhelm its control
plane. Namespace management tools simplify the administration of quotas. In addition, while
Kubernetes quotas only apply within a single namespace, some namespace management tools allow
groups of namespaces to share quotas, giving administrators far more flexibility with less effort
than built-in quotas.</p><p>Quotas prevent a single tenant from consuming greater than their allocated share of resources
hence minimizing the “noisy neighbor” issue, where one tenant negatively impacts the performance
of other tenants' workloads.</p><p>When you apply a quota to namespace, Kubernetes requires you to also specify resource requests and
limits for each container. Limits are the upper bound for the amount of resources that a container
can consume. Containers that attempt to consume resources that exceed the configured limits will
either be throttled or killed, based on the resource type. When resource requests are set lower
than limits, each container is guaranteed the requested amount but there may still be some
potential for impact across workloads.</p><p>Quotas cannot protect against all kinds of resource sharing, such as network traffic.
Node isolation (described below) may be a better solution for this problem.</p><h2 id=data-plane-isolation>Data Plane Isolation</h2><p>Data plane isolation ensures that pods and workloads for different tenants are sufficiently
isolated.</p><h3 id=network-isolation>Network isolation</h3><p>By default, all pods in a Kubernetes cluster are allowed to communicate with each other, and all
network traffic is unencrypted. This can lead to security vulnerabilities where traffic is
accidentally or maliciously sent to an unintended destination, or is intercepted by a workload on
a compromised node.</p><p>Pod-to-pod communication can be controlled using <a href=/docs/concepts/services-networking/network-policies/>Network Policies</a>,
which restrict communication between pods using namespace labels or IP address ranges.
In a multi-tenant environment where strict network isolation between tenants is required, starting
with a default policy that denies communication between pods is recommended with another rule that
allows all pods to query the DNS server for name resolution. With such a default policy in place,
you can begin adding more permissive rules that allow for communication within a namespace.
It is also recommended not to use empty label selector '{}' for namespaceSelector field in network policy definition,
in case traffic need to be allowed between namespaces.
This scheme can be further refined as required. Note that this only applies to pods within a single
control plane; pods that belong to different virtual control planes cannot talk to each other via
Kubernetes networking.</p><p>Namespace management tools may simplify the creation of default or common network policies.
In addition, some of these tools allow you to enforce a consistent set of namespace labels across
your cluster, ensuring that they are a trusted basis for your policies.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> Network policies require a <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni>CNI plugin</a>
that supports the implementation of network policies. Otherwise, NetworkPolicy resources will be ignored.</div><p>More advanced network isolation may be provided by service meshes, which provide OSI Layer 7
policies based on workload identity, in addition to namespaces. These higher-level policies can
make it easier to manage namespace-based multi-tenancy, especially when multiple namespaces are
dedicated to a single tenant. They frequently also offer encryption using mutual TLS, protecting
your data even in the presence of a compromised node, and work across dedicated or virtual clusters.
However, they can be significantly more complex to manage and may not be appropriate for all users.</p><h3 id=storage-isolation>Storage isolation</h3><p>Kubernetes offers several types of volumes that can be used as persistent storage for workloads.
For security and data-isolation, <a href=/docs/concepts/storage/dynamic-provisioning/>dynamic volume provisioning</a>
is recommended and volume types that use node resources should be avoided.</p><p><a href=/docs/concepts/storage/storage-classes/>StorageClasses</a> allow you to describe custom "classes"
of storage offered by your cluster, based on quality-of-service levels, backup policies, or custom
policies determined by the cluster administrators.</p><p>Pods can request storage using a <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumeClaim</a>.
A PersistentVolumeClaim is a namespaced resource, which enables isolating portions of the storage
system and dedicating it to tenants within the shared Kubernetes cluster.
However, it is important to note that a PersistentVolume is a cluster-wide resource and has a
lifecycle independent of workloads and namespaces.</p><p>For example, you can configure a separate StorageClass for each tenant and use this to strengthen isolation.
If a StorageClass is shared, you should set a <a href=/docs/concepts/storage/storage-classes/#reclaim-policy>reclaim policy of <code>Delete</code></a>
to ensure that a PersistentVolume cannot be reused across different namespaces.</p><h3 id=sandboxing-containers>Sandboxing containers</h3><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>Kubernetes pods are composed of one or more containers that execute on worker nodes.
Containers utilize OS-level virtualization and hence offer a weaker isolation boundary than
virtual machines that utilize hardware-based virtualization.</p><p>In a shared environment, unpatched vulnerabilities in the application and system layers can be
exploited by attackers for container breakouts and remote code execution that allow access to host
resources. In some applications, like a Content Management System (CMS), customers may be allowed
the ability to upload and execute untrusted scripts or code. In either case, mechanisms to further
isolate and protect workloads using strong isolation are desirable.</p><p>Sandboxing provides a way to isolate workloads running in a shared cluster. It typically involves
running each pod in a separate execution environment such as a virtual machine or a userspace
kernel. Sandboxing is often recommended when you are running untrusted code, where workloads are
assumed to be malicious. Part of the reason this type of isolation is necessary is because
containers are processes running on a shared kernel; they mount file systems like <code>/sys</code> and <code>/proc</code>
from the underlying host, making them less secure than an application that runs on a virtual
machine which has its own kernel. While controls such as seccomp, AppArmor, and SELinux can be
used to strengthen the security of containers, it is hard to apply a universal set of rules to all
workloads running in a shared cluster. Running workloads in a sandbox environment helps to
insulate the host from container escapes, where an attacker exploits a vulnerability to gain
access to the host system and all the processes/files running on that host.</p><p>Virtual machines and userspace kernels are 2 popular approaches to sandboxing. The following
sandboxing implementations are available:</p><ul><li><a href=https://gvisor.dev/>gVisor</a> intercepts syscalls from containers and runs them through a
userspace kernel, written in Go, with limited access to the underlying host.</li><li><a href=https://katacontainers.io/>Kata Containers</a> is an OCI compliant runtime that allows you to run
containers in a VM. The hardware virtualization available in Kata offers an added layer of
security for containers running untrusted code.</li></ul><h3 id=node-isolation>Node Isolation</h3><p>Node isolation is another technique that you can use to isolate tenant workloads from each other.
With node isolation, a set of nodes is dedicated to running pods from a particular tenant and
co-mingling of tenant pods is prohibited. This configuration reduces the noisy tenant issue, as
all pods running on a node will belong to a single tenant. The risk of information disclosure is
slightly lower with node isolation because an attacker that manages to escape from a container
will only have access to the containers and volumes mounted to that node.</p><p>Although workloads from different tenants are running on different nodes, it is important to be
aware that the kubelet and (unless using virtual control planes) the API service are still shared
services. A skilled attacker could use the permissions assigned to the kubelet or other pods
running on the node to move laterally within the cluster and gain access to tenant workloads
running on other nodes. If this is a major concern, consider implementing compensating controls
such as seccomp, AppArmor or SELinux or explore using sandboxed containers or creating separate
clusters for each tenant.</p><p>Node isolation is a little easier to reason about from a billing standpoint than sandboxing
containers since you can charge back per node rather than per pod. It also has fewer compatibility
and performance issues and may be easier to implement than sandboxing containers.
For example, nodes for each tenant can be configured with taints so that only pods with the
corresponding toleration can run on them. A mutating webhook could then be used to automatically
add tolerations and node affinities to pods deployed into tenant namespaces so that they run on a
specific set of nodes designated for that tenant.</p><p>Node isolation can be implemented using an <a href=/docs/concepts/scheduling-eviction/assign-pod-node/>pod node selectors</a>
or a <a href=https://github.com/virtual-kubelet>Virtual Kubelet</a>.</p><h2 id=additional-considerations>Additional Considerations</h2><p>This section discusses other Kubernetes constructs and patterns that are relevant for multi-tenancy.</p><h3 id=api-priority-and-fairness>API Priority and Fairness</h3><p><a href=/docs/concepts/cluster-administration/flow-control/>API priority and fairness</a> is a Kubernetes
feature that allows you to assign a priority to certain pods running within the cluster.
When an application calls the Kubernetes API, the API server evaluates the priority assigned to pod.
Calls from pods with higher priority are fulfilled before those with a lower priority.
When contention is high, lower priority calls can be queued until the server is less busy or you
can reject the requests.</p><p>Using API priority and fairness will not be very common in SaaS environments unless you are
allowing customers to run applications that interface with the Kubernetes API, for example,
a controller.</p><h3 id=qos>Quality-of-Service (QoS)</h3><p>When you’re running a SaaS application, you may want the ability to offer different
Quality-of-Service (QoS) tiers of service to different tenants. For example, you may have freemium
service that comes with fewer performance guarantees and features and a for-fee service tier with
specific performance guarantees. Fortunately, there are several Kubernetes constructs that can
help you accomplish this within a shared cluster, including network QoS, storage classes, and pod
priority and preemption. The idea with each of these is to provide tenants with the quality of
service that they paid for. Let’s start by looking at networking QoS.</p><p>Typically, all pods on a node share a network interface. Without network QoS, some pods may
consume an unfair share of the available bandwidth at the expense of other pods.
The Kubernetes <a href=https://www.cni.dev/plugins/current/meta/bandwidth/>bandwidth plugin</a> creates an
<a href=/docs/concepts/configuration/manage-resources-containers/#extended-resources>extended resource</a>
for networking that allows you to use Kubernetes resources constructs, i.e. requests/limits, to
apply rate limits to pods by using Linux tc queues.
Be aware that the plugin is considered experimental as per the
<a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping>Network Plugins</a>
documentation and should be thoroughly tested before use in production environments.</p><p>For storage QoS, you will likely want to create different storage classes or profiles with
different performance characteristics. Each storage profile can be associated with a different
tier of service that is optimized for different workloads such IO, redundancy, or throughput.
Additional logic might be necessary to allow the tenant to associate the appropriate storage
profile with their workload.</p><p>Finally, there’s <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>pod priority and preemption</a>
where you can assign priority values to pods. When scheduling pods, the scheduler will try
evicting pods with lower priority when there are insufficient resources to schedule pods that are
assigned a higher priority. If you have a use case where tenants have different service tiers in a
shared cluster e.g. free and paid, you may want to give higher priority to certain tiers using
this feature.</p><h3 id=dns>DNS</h3><p>Kubernetes clusters include a Domain Name System (DNS) service to provide translations from names
to IP addresses, for all Services and Pods. By default, the Kubernetes DNS service allows lookups
across all namespaces in the cluster.</p><p>In multi-tenant environments where tenants can access pods and other Kubernetes resources, or where
stronger isolation is required, it may be necessary to prevent pods from looking up services in other
Namespaces.
You can restrict cross-namespace DNS lookups by configuring security rules for the DNS service.
For example, CoreDNS (the default DNS service for Kubernetes) can leverage Kubernetes metadata
to restrict queries to Pods and Services within a namespace. For more information, read an
<a href=https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy>example</a> of
configuring this within the CoreDNS documentation.</p><p>When a <a href=#virtual-control-plane-per-tenant>Virtual Control Plane per tenant</a> model is used, a DNS
service must be configured per tenant or a multi-tenant DNS service must be used.
Here is an example of a <a href=https://github.com/kubernetes-sigs/cluster-api-provider-nested/blob/main/virtualcluster/doc/tenant-dns.md>customized version of CoreDNS</a>
that supports multiple tenants.</p><h3 id=operators>Operators</h3><p><a href=/docs/concepts/extend-kubernetes/operator/>Operators</a> are Kubernetes controllers that manage
applications. Operators can simplify the management of multiple instances of an application, like
a database service, which makes them a common building block in the multi-consumer (SaaS)
multi-tenancy use case.</p><p>Operators used in a multi-tenant environment should follow a stricter set of guidelines.
Specifically, the Operator should:</p><ul><li>Support creating resources within different tenant namespaces, rather than just in the namespace
in which the Operator is deployed.</li><li>Ensure that the Pods are configured with resource requests and limits, to ensure scheduling and fairness.</li><li>Support configuration of Pods for data-plane isolation techniques such as node isolation and
sandboxed containers.</li></ul><h2 id=implementations>Implementations</h2><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>There are two primary ways to share a Kubernetes cluster for multi-tenancy: using Namespaces
(that is, a Namespace per tenant) or by virtualizing the control plane (that is, virtual control
plane per tenant).</p><p>In both cases, data plane isolation, and management of additional considerations such as API
Priority and Fairness, is also recommended.</p><p>Namespace isolation is well-supported by Kubernetes, has a negligible resource cost, and provides
mechanisms to allow tenants to interact appropriately, such as by allowing service-to-service
communication. However, it can be difficult to configure, and doesn't apply to Kubernetes
resources that can't be namespaced, such as Custom Resource Definitions, Storage Classes, and Webhooks.</p><p>Control plane virtualization allows for isolation of non-namespaced resources at the cost of
somewhat higher resource usage and more difficult cross-tenant sharing. It is a good option when
namespace isolation is insufficient but dedicated clusters are undesirable, due to the high cost
of maintaining them (especially on-prem) or due to their higher overhead and lack of resource
sharing. However, even within a virtualized control plane, you will likely see benefits by using
namespaces as well.</p><p>The two options are discussed in more detail in the following sections.</p><h3 id=namespace-per-tenant>Namespace per tenant</h3><p>As previously mentioned, you should consider isolating each workload in its own namespace, even if
you are using dedicated clusters or virtualized control planes. This ensures that each workload
only has access to its own resources, such as Config Maps and Secrets, and allows you to tailor
dedicated security policies for each workload. In addition, it is a best practice to give each
namespace names that are unique across your entire fleet (that is, even if they are in separate
clusters), as this gives you the flexibility to switch between dedicated and shared clusters in
the future, or to use multi-cluster tooling such as service meshes.</p><p>Conversely, there are also advantages to assigning namespaces at the tenant level, not just the
workload level, since there are often policies that apply to all workloads owned by a single
tenant. However, this raises its own problems. Firstly, this makes it difficult or impossible to
customize policies to individual workloads, and secondly, it may be challenging to come up with a
single level of "tenancy" that should be given a namespace. For example, an organization may have
divisions, teams, and subteams - which should be assigned a namespace?</p><p>To solve this, Kubernetes provides the <a href=https://github.com/kubernetes-sigs/hierarchical-namespaces>Hierarchical Namespace Controller (HNC)</a>,
which allows you to organize your namespaces into hierarchies, and share certain policies and
resources between them. It also helps you manage namespace labels, namespace lifecycles, and
delegated management, and share resource quotas across related namespaces. These capabilities can
be useful in both multi-team and multi-customer scenarios.</p><p>Other projects that provide similar capabilities and aid in managing namespaced resources are
listed below.</p><h4 id=multi-team-tenancy>Multi-team tenancy</h4><ul><li><a href=https://github.com/clastix/capsule>Capsule</a></li><li><a href=https://github.com/loft-sh/kiosk>Kiosk</a></li></ul><h4 id=multi-customer-tenancy>Multi-customer tenancy</h4><ul><li><a href=https://github.com/cloud-ark/kubeplus>Kubeplus</a></li></ul><h4 id=policy-engines>Policy engines</h4><p>Policy engines provide features to validate and generate tenant configurations:</p><ul><li><a href=https://kyverno.io/>Kyverno</a></li><li><a href=https://github.com/open-policy-agent/gatekeeper>OPA/Gatekeeper</a></li></ul><h3 id=virtual-control-plane-per-tenant>Virtual control plane per tenant</h3><p>Another form of control-plane isolation is to use Kubernetes extensions to provide each tenant a
virtual control-plane that enables segmentation of cluster-wide API resources.
<a href=#data-plane-isolation>Data plane isolation</a> techniques can be used with this model to securely
manage worker nodes across tenants.</p><p>The virtual control plane based multi-tenancy model extends namespace-based multi-tenancy by
providing each tenant with dedicated control plane components, and hence complete control over
cluster-wide resources and add-on services. Worker nodes are shared across all tenants, and are
managed by a Kubernetes cluster that is normally inaccessible to tenants.
This cluster is often referred to as a <em>super-cluster</em> (or sometimes as a <em>host-cluster</em>).
Since a tenant’s control-plane is not directly associated with underlying compute resources it is
referred to as a <em>virtual control plane</em>.</p><p>A virtual control plane typically consists of the Kubernetes API server, the controller manager,
and the etcd data store. It interacts with the super cluster via a metadata synchronization
controller which coordinates changes across tenant control planes and the control plane of the
super-cluster.</p><p>By using per-tenant dedicated control planes, most of the isolation problems due to sharing one
API server among all tenants are solved. Examples include noisy neighbors in the control plane,
uncontrollable blast radius of policy misconfigurations, and conflicts between cluster scope
objects such as webhooks and CRDs. Hence, the virtual control plane model is particularly
suitable for cases where each tenant requires access to a Kubernetes API server and expects the
full cluster manageability.</p><p>The improved isolation comes at the cost of running and maintaining an individual virtual control
plane per tenant. In addition, per-tenant control planes do not solve isolation problems in the
data plane, such as node-level noisy neighbors or security threats. These must still be addressed
separately.</p><p>The Kubernetes <a href=https://github.com/kubernetes-sigs/cluster-api-provider-nested/tree/main/virtualcluster>Cluster API - Nested (CAPN)</a>
project provides an implementation of virtual control planes.</p><h4 id=other-implementations>Other implementations</h4><ul><li><a href=https://github.com/clastix/kamaji>Kamaji</a></li><li><a href=https://github.com/loft-sh/vcluster>vcluster</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-265c06c3d1349382453ced9f2a7ecfde>9.10 - Kubernetes API Server Bypass Risks</h1><div class=lead>Security architecture information relating to the API server and other components</div><p>The Kubernetes API server is the main point of entry to a cluster for external parties
(users and services) interacting with it.</p><p>As part of this role, the API server has several key built-in security controls, such as
audit logging and <a class=glossary-tooltip title='A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/admission-controllers/ target=_blank aria-label='admission controllers'>admission controllers</a>. However, there are ways to modify the configuration
or content of the cluster that bypass these controls.</p><p>This page describes the ways in which the security controls built into the
Kubernetes API server can be bypassed, so that cluster operators
and security architects can ensure that these bypasses are appropriately restricted.</p><h2 id=static-pods>Static Pods</h2><p>The <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> on each node loads and
directly manages any manifests that are stored in a named directory or fetched from
a specific URL as <a href=/docs/tasks/configure-pod-container/static-pod><em>static Pods</em></a> in
your cluster. The API server doesn't manage these static Pods. An attacker with write
access to this location could modify the configuration of static pods loaded from that
source, or could introduce new static Pods.</p><p>Static Pods are restricted from accessing other objects in the Kubernetes API. For example,
you can't configure a static Pod to mount a Secret from the cluster. However, these Pods can
take other security sensitive actions, such as using <code>hostPath</code> mounts from the underlying
node.</p><p>By default, the kubelet creates a <a class=glossary-tooltip title='An object in the API server that tracks a static pod on a kubelet.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-mirror-pod' target=_blank aria-label='mirror pod'>mirror pod</a>
so that the static Pods are visible in the Kubernetes API. However, if the attacker uses an invalid
namespace name when creating the Pod, it will not be visible in the Kubernetes API and can only
be discovered by tooling that has access to the affected host(s).</p><p>If a static Pod fails admission control, the kubelet won't register the Pod with the
API server. However, the Pod still runs on the node. For more information, refer to
<a href=https://github.com/kubernetes/kubeadm/issues/1541#issuecomment-487331701>kubeadm issue #1541</a>.</p><h3 id=static-pods-mitigations>Mitigations</h3><ul><li>Only <a href=/docs/tasks/configure-pod-container/static-pod/#static-pod-creation>enable the kubelet static Pod manifest functionality</a>
if required by the node.</li><li>If a node uses the static Pod functionality, restrict filesystem access to the static Pod manifest directory
or URL to users who need the access.</li><li>Restrict access to kubelet configuration parameters and files to prevent an attacker setting
a static Pod path or URL.</li><li>Regularly audit and centrally report all access to directories or web storage locations that host
static Pod manifests and kubelet configuration files.</li></ul><h2 id=kubelet-api>The kubelet API</h2><p>The kubelet provides an HTTP API that is typically exposed on TCP port 10250 on cluster
worker nodes. The API might also be exposed on control plane nodes depending on the Kubernetes
distribution in use. Direct access to the API allows for disclosure of information about
the pods running on a node, the logs from those pods, and execution of commands in
every container running on the node.</p><p>When Kubernetes cluster users have RBAC access to <code>Node</code> object sub-resources, that access
serves as authorization to interact with the kubelet API. The exact access depends on
which sub-resource access has been granted, as detailed in <a href=https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authorization>kubelet authorization</a>.</p><p>Direct access to the kubelet API is not subject to admission control and is not logged
by Kubernetes audit logging. An attacker with direct access to this API may be able to
bypass controls that detect or prevent certain actions.</p><p>The kubelet API can be configured to authenticate requests in a number of ways.
By default, the kubelet configuration allows anonymous access. Most Kubernetes providers
change the default to use webhook and certificate authentication. This lets the control plane
ensure that the caller is authorized to access the <code>nodes</code> API resource or sub-resources.
The default anonymous access doesn't make this assertion with the control plane.</p><h3 id=mitigations>Mitigations</h3><ul><li>Restrict access to sub-resources of the <code>nodes</code> API object using mechanisms such as
<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>. Only grant this access when required,
such as by monitoring services.</li><li>Restrict access to the kubelet port. Only allow specified and trusted IP address
ranges to access the port.</li><li><a href=/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication>Ensure that kubelet authentication is set to webhook or certificate mode</a>.</li><li>Ensure that the unauthenticated "read-only" Kubelet port is not enabled on the cluster.</li></ul><h2 id=the-etcd-api>The etcd API</h2><p>Kubernetes clusters use etcd as a datastore. The <code>etcd</code> service listens on TCP port 2379.
The only clients that need access are the Kubernetes API server and any backup tooling
that you use. Direct access to this API allows for disclosure or modification of any
data held in the cluster.</p><p>Access to the etcd API is typically managed by client certificate authentication.
Any certificate issued by a certificate authority that etcd trusts allows full access
to the data stored inside etcd.</p><p>Direct access to etcd is not subject to Kubernetes admission control and is not logged
by Kubernetes audit logging. An attacker who has read access to the API server's
etcd client certificate private key (or can create a new trusted client certificate) can gain
cluster admin rights by accessing cluster secrets or modifying access rules. Even without
elevating their Kubernetes RBAC privileges, an attacker who can modify etcd can retrieve any API object
or create new workloads inside the cluster.</p><p>Many Kubernetes providers configure
etcd to use mutual TLS (both client and server verify each other's certificate for authentication).
There is no widely accepted implementation of authorization for the etcd API, although
the feature exists. Since there is no authorization model, any certificate
with client access to etcd can be used to gain full access to etcd. Typically, etcd client certificates
that are only used for health checking can also grant full read and write access.</p><h3 id=etcd-api-mitigations>Mitigations</h3><ul><li>Ensure that the certificate authority trusted by etcd is used only for the purposes of
authentication to that service.</li><li>Control access to the private key for the etcd server certificate, and to the API server's
client certificate and key.</li><li>Consider restricting access to the etcd port at a network level, to only allow access
from specified and trusted IP address ranges.</li></ul><h2 id=runtime-socket>Container runtime socket</h2><p>On each node in a Kubernetes cluster, access to interact with containers is controlled
by the container runtime (or runtimes, if you have configured more than one). Typically,
the container runtime exposes a Unix socket that the kubelet can access. An attacker with
access to this socket can launch new containers or interact with running containers.</p><p>At the cluster level, the impact of this access depends on whether the containers that
run on the compromised node have access to Secrets or other confidential
data that an attacker could use to escalate privileges to other worker nodes or to
control plane components.</p><h3 id=runtime-socket-mitigations>Mitigations</h3><ul><li>Ensure that you tightly control filesystem access to container runtime sockets.
When possible, restrict this access to the <code>root</code> user.</li><li>Isolate the kubelet from other components running on the node, using
mechanisms such as Linux kernel namespaces.</li><li>Ensure that you restrict or forbid the use of <a href=/docs/concepts/storage/volumes/#hostpath><code>hostPath</code> mounts</a>
that include the container runtime socket, either directly or by mounting a parent
directory. Also <code>hostPath</code> mounts must be set as read-only to mitigate risks
of attackers bypassing directory restrictions.</li><li>Restrict user access to nodes, and especially restrict superuser access to nodes.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6f8354561fd5286f997909e14b13110c>9.11 - Security Checklist</h1><div class=lead>Baseline checklist for ensuring security in Kubernetes clusters.</div><p>This checklist aims at providing a basic list of guidance with links to more
comprehensive documentation on each topic. It does not claim to be exhaustive
and is meant to evolve.</p><p>On how to read and use this document:</p><ul><li>The order of topics does not reflect an order of priority.</li><li>Some checklist items are detailed in the paragraph below the list of each section.</li></ul><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Checklists are <strong>not</strong> sufficient for attaining a good security posture on their
own. A good security posture requires constant attention and improvement, but a
checklist can be the first step on the never-ending journey towards security
preparedness. Some of the recommendations in this checklist may be too
restrictive or too lax for your specific security needs. Since Kubernetes
security is not "one size fits all", each category of checklist items should be
evaluated on its merits.</div><h2 id=authentication-authorization>Authentication & Authorization</h2><ul><li><input disabled type=checkbox> <code>system:masters</code> group is not used for user or component authentication after bootstrapping.</li><li><input disabled type=checkbox> The kube-controller-manager is running with <code>--use-service-account-credentials</code>
enabled.</li><li><input disabled type=checkbox> The root certificate is protected (either an offline CA, or a managed
online CA with effective access controls).</li><li><input disabled type=checkbox> Intermediate and leaf certificates have an expiry date no more than 3
years in the future.</li><li><input disabled type=checkbox> A process exists for periodic access review, and reviews occur no more
than 24 months apart.</li><li><input disabled type=checkbox> The <a href=/docs/concepts/security/rbac-good-practices/>Role Based Access Control Good Practices</a>
is followed for guidance related to authentication and authorization.</li></ul><p>After bootstrapping, neither users nor components should authenticate to the
Kubernetes API as <code>system:masters</code>. Similarly, running all of
kube-controller-manager as <code>system:masters</code> should be avoided. In fact,
<code>system:masters</code> should only be used as a break-glass mechanism, as opposed to
an admin user.</p><h2 id=network-security>Network security</h2><ul><li><input disabled type=checkbox> CNI plugins in-use supports network policies.</li><li><input disabled type=checkbox> Ingress and egress network policies are applied to all workloads in the
cluster.</li><li><input disabled type=checkbox> Default network policies within each namespace, selecting all pods, denying
everything, are in place.</li><li><input disabled type=checkbox> If appropriate, a service mesh is used to encrypt all communications inside of the cluster.</li><li><input disabled type=checkbox> The Kubernetes API, kubelet API and etcd are not exposed publicly on Internet.</li><li><input disabled type=checkbox> Access from the workloads to the cloud metadata API is filtered.</li><li><input disabled type=checkbox> Use of LoadBalancer and ExternalIPs is restricted.</li></ul><p>A number of <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>Container Network Interface (CNI) plugins</a>
plugins provide the functionality to
restrict network resources that pods may communicate with. This is most commonly done
through <a href=/docs/concepts/services-networking/network-policies/>Network Policies</a>
which provide a namespaced resource to define rules. Default network policies
blocking everything egress and ingress, in each namespace, selecting all the
pods, can be useful to adopt an allow list approach, ensuring that no workloads
is missed.</p><p>Not all CNI plugins provide encryption in transit. If the chosen plugin lacks this
feature, an alternative solution could be to use a service mesh to provide that
functionality.</p><p>The etcd datastore of the control plane should have controls to limit access and
not be publicly exposed on the Internet. Furthermore, mutual TLS (mTLS) should
be used to communicate securely with it. The certificate authority for this
should be unique to etcd.</p><p>External Internet access to the Kubernetes API server should be restricted to
not expose the API publicly. Be careful as many managed Kubernetes distribution
are publicly exposing the API server by default. You can then use a bastion host
to access the server.</p><p>The <a href=/docs/reference/command-line-tools-reference/kubelet/>kubelet</a> API access
should be restricted and not publicly exposed, the defaults authentication and
authorization settings, when no configuration file specified with the <code>--config</code>
flag, are overly permissive.</p><p>If a cloud provider is used for hosting Kubernetes, the access from pods to the cloud
metadata API <code>169.254.169.254</code> should also be restricted or blocked if not needed
because it may leak information.</p><p>For restricted LoadBalancer and ExternalIPs use, see
<a href=https://github.com/kubernetes/kubernetes/issues/97076>CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs</a>
and the <a href=/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips>DenyServiceExternalIPs admission controller</a>
for further information.</p><h2 id=pod-security>Pod security</h2><ul><li><input disabled type=checkbox> RBAC rights to <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code> workloads is only granted if necessary.</li><li><input disabled type=checkbox> Appropriate Pod Security Standards policy is applied for all namespaces and enforced.</li><li><input disabled type=checkbox> Memory limit is set for the workloads with a limit equal or inferior to the request.</li><li><input disabled type=checkbox> CPU limit might be set on sensitive workloads.</li><li><input disabled type=checkbox> For nodes that support it, Seccomp is enabled with appropriate syscalls
profile for programs.</li><li><input disabled type=checkbox> For nodes that support it, AppArmor or SELinux is enabled with appropriate
profile for programs.</li></ul><p>RBAC authorization is crucial but
<a href=/docs/concepts/security/rbac-good-practices/#workload-creation>cannot be granular enough to have authorization on the Pods' resources</a>
(or on any resource that manages Pods). The only granularity is the API verbs
on the resource itself, for example, <code>create</code> on Pods. Without
additional admission, the authorization to create these resources allows direct
unrestricted access to the schedulable nodes of a cluster.</p><p>The <a href=/docs/concepts/security/pod-security-standards/>Pod Security Standards</a>
define three different policies, privileged, baseline and restricted that limit
how fields can be set in the <code>PodSpec</code> regarding security.
These standards can be enforced at the namespace level with the new
<a href=/docs/concepts/security/pod-security-admission/>Pod Security</a> admission,
enabled by default, or by third-party admission webhook. Please note that,
contrary to the removed PodSecurityPolicy admission it replaces,
<a href=/docs/concepts/security/pod-security-admission/>Pod Security</a>
admission can be easily combined with admission webhooks and external services.</p><p>Pod Security admission <code>restricted</code> policy, the most restrictive policy of the
<a href=/docs/concepts/security/pod-security-standards/>Pod Security Standards</a> set,
<a href=/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces>can operate in several modes</a>,
<code>warn</code>, <code>audit</code> or <code>enforce</code> to gradually apply the most appropriate
<a href=/docs/tasks/configure-pod-container/security-context/>security context</a>
according to security best practices. Nevertheless, pods'
<a href=/docs/tasks/configure-pod-container/security-context/>security context</a>
should be separately investigated to limit the privileges and access pods may
have on top of the predefined security standards, for specific use cases.</p><p>For a hands-on tutorial on <a href=/docs/concepts/security/pod-security-admission/>Pod Security</a>,
see the blog post
<a href=/blog/2021/12/09/pod-security-admission-beta/>Kubernetes 1.23: Pod Security Graduates to Beta</a>.</p><p><a href=/docs/concepts/configuration/manage-resources-containers/>Memory and CPU limits</a>
should be set in order to restrict the memory and CPU resources a pod can
consume on a node, and therefore prevent potential DoS attacks from malicious or
breached workloads. Such policy can be enforced by an admission controller.
Please note that CPU limits will throttle usage and thus can have unintended
effects on auto-scaling features or efficiency i.e. running the process in best
effort with the CPU resource available.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Memory limit superior to request can expose the whole node to OOM issues.</div><h3 id=enabling-seccomp>Enabling Seccomp</h3><p>Seccomp can improve the security of your workloads by reducing the Linux kernel
syscall attack surface available inside containers. The seccomp filter mode
leverages BPF to create an allow or deny list of specific syscalls, named
profiles. Those seccomp profiles can be enabled on individual workloads,
<a href=/docs/tutorials/security/seccomp/>a security tutorial is available</a>. In
addition, the <a href=https://github.com/kubernetes-sigs/security-profiles-operator>Kubernetes Security Profiles Operator</a>
is a project to facilitate the management and use of seccomp in clusters.</p><p>For historical context, please note that Docker has been using
<a href=https://docs.docker.com/engine/security/seccomp/>a default seccomp profile</a>
to only allow a restricted set of syscalls since 2016 from
<a href=https://www.docker.com/blog/docker-engine-1-10-security/>Docker Engine 1.10</a>,
but Kubernetes is still not confining workloads by default. The default seccomp
profile can be found <a href=https://github.com/containerd/containerd/blob/main/contrib/seccomp/seccomp_default.go>in containerd</a>
as well. Fortunately, <a href=/blog/2021/08/25/seccomp-default/>Seccomp Default</a>, a
new alpha feature to use a default seccomp profile for all workloads can now be
enabled and tested.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Seccomp is only available on Linux nodes.</div><h3 id=enabling-apparmor-or-selinux>Enabling AppArmor or SELinux</h3><h4 id=apparmor>AppArmor</h4><p><a href=https://apparmor.net/>AppArmor</a> is a Linux kernel security module that can
provide an easy way to implement Mandatory Access Control (MAC) and better
auditing through system logs. To <a href=/docs/tutorials/security/apparmor/>enable AppArmor in Kubernetes</a>,
at least version 1.4 is required. Like seccomp, AppArmor is also configured
through profiles, where each profile is either running in enforcing mode, which
blocks access to disallowed resources or complain mode, which only reports
violations. AppArmor profiles are enforced on a per-container basis, with an
annotation, allowing for processes to gain just the right privileges.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> AppArmor is only available on Linux nodes, and enabled in
<a href=https://gitlab.com/apparmor/apparmor/-/wikis/home#distributions-and-ports>some Linux distributions</a>.</div><h4 id=selinux>SELinux</h4><p><a href=https://github.com/SELinuxProject/selinux-notebook/blob/main/src/selinux_overview.md>SELinux</a> is also a
Linux kernel security module that can provide a mechanism for supporting access
control security policies, including Mandatory Access Controls (MAC). SELinux
labels can be assigned to containers or pods
<a href=/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container>via their <code>securityContext</code> section</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> SELinux is only available on Linux nodes, and enabled in
<a href=https://en.wikipedia.org/wiki/Security-Enhanced_Linux#Implementations>some Linux distributions</a>.</div><h2 id=pod-placement>Pod placement</h2><ul><li><input disabled type=checkbox> Pod placement is done in accordance with the tiers of sensitivity of the
application.</li><li><input disabled type=checkbox> Sensitive applications are running isolated on nodes or with specific
sandboxed runtimes.</li></ul><p>Pods that are on different tiers of sensitivity, for example, an application pod
and the Kubernetes API server, should be deployed onto separate nodes. The
purpose of node isolation is to prevent an application container breakout to
directly providing access to applications with higher level of sensitivity to easily
pivot within the cluster. This separation should be enforced to prevent pods
accidentally being deployed onto the same node. This could be enforced with the
following features:</p><dl><dt><a href=/docs/concepts/scheduling-eviction/assign-pod-node/>Node Selectors</a></dt><dd>Key-value pairs, as part of the pod specification, that specify which nodes to
deploy onto. These can be enforced at the namespace and cluster level with the
<a href=/docs/reference/access-authn-authz/admission-controllers/#podnodeselector>PodNodeSelector</a>
admission controller.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#podtolerationrestriction>PodTolerationRestriction</a></dt><dd>An admission controller that allows administrators to restrict permitted
<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>tolerations</a> within a
namespace. Pods within a namespace may only utilize the tolerations specified on
the namespace object annotation keys that provide a set of default and allowed
tolerations.</dd><dt><a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a></dt><dd>RuntimeClass is a feature for selecting the container runtime configuration.
The container runtime configuration is used to run a Pod's containers and can
provide more or less isolation from the host at the cost of performance
overhead.</dd></dl><h2 id=secrets>Secrets</h2><ul><li><input disabled type=checkbox> ConfigMaps are not used to hold confidential data.</li><li><input disabled type=checkbox> Encryption at rest is configured for the Secret API.</li><li><input disabled type=checkbox> If appropriate, a mechanism to inject secrets stored in third-party storage
is deployed and available.</li><li><input disabled type=checkbox> Service account tokens are not mounted in pods that don't require them.</li><li><input disabled type=checkbox> <a href=/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume>Bound service account token volume</a>
is in-use instead of non-expiring tokens.</li></ul><p>Secrets required for pods should be stored within Kubernetes Secrets as opposed
to alternatives such as ConfigMap. Secret resources stored within etcd should
be <a href=/docs/tasks/administer-cluster/encrypt-data/>encrypted at rest</a>.</p><p>Pods needing secrets should have these automatically mounted through volumes,
preferably stored in memory like with the <a href=/docs/concepts/storage/volumes/#emptydir><code>emptyDir.medium</code> option</a>.
Mechanism can be used to also inject secrets from third-party storages as
volume, like the <a href=https://secrets-store-csi-driver.sigs.k8s.io/>Secrets Store CSI Driver</a>.
This should be done preferentially as compared to providing the pods service
account RBAC access to secrets. This would allow adding secrets into the pod as
environment variables or files. Please note that the environment variable method
might be more prone to leakage due to crash dumps in logs and the
non-confidential nature of environment variable in Linux, as opposed to the
permission mechanism on files.</p><p>Service account tokens should not be mounted into pods that do not require them. This can be configured by setting
<a href=/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server><code>automountServiceAccountToken</code></a>
to <code>false</code> either within the service account to apply throughout the namespace
or specifically for a pod. For Kubernetes v1.22 and above, use
<a href=/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume>Bound Service Accounts</a>
for time-bound service account credentials.</p><h2 id=images>Images</h2><ul><li><input disabled type=checkbox> Minimize unnecessary content in container images.</li><li><input disabled type=checkbox> Container images are configured to be run as unprivileged user.</li><li><input disabled type=checkbox> References to container images are made by sha256 digests (rather than
tags) or the provenance of the image is validated by verifying the image's
digital signature at deploy time <a href=/docs/tasks/administer-cluster/verify-signed-images/#verifying-image-signatures-with-admission-controller>via admission control</a>.</li><li><input disabled type=checkbox> Container images are regularly scanned during creation and in deployment, and
known vulnerable software is patched.</li></ul><p>Container image should contain the bare minimum to run the program they
package. Preferably, only the program and its dependencies, building the image
from the minimal possible base. In particular, image used in production should not
contain shells or debugging utilities, as an
<a href=/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container>ephemeral debug container</a>
can be used for troubleshooting.</p><p>Build images to directly start with an unprivileged user by using the
<a href=https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user><code>USER</code> instruction in Dockerfile</a>.
The <a href=/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod>Security Context</a>
allows a container image to be started with a specific user and group with
<code>runAsUser</code> and <code>runAsGroup</code>, even if not specified in the image manifest.
However, the file permissions in the image layers might make it impossible to just
start the process with a new unprivileged user without image modification.</p><p>Avoid using image tags to reference an image, especially the <code>latest</code> tag, the
image behind a tag can be easily modified in a registry. Prefer using the
complete <code>sha256</code> digest which is unique to the image manifest. This policy can be
enforced via an <a href=/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook>ImagePolicyWebhook</a>.
Image signatures can also be automatically <a href=/docs/tasks/administer-cluster/verify-signed-images/#verifying-image-signatures-with-admission-controller>verified with an admission controller</a>
at deploy time to validate their authenticity and integrity.</p><p>Scanning a container image can prevent critical vulnerabilities from being
deployed to the cluster alongside the container image. Image scanning should be
completed before deploying a container image to a cluster and is usually done
as part of the deployment process in a CI/CD pipeline. The purpose of an image
scan is to obtain information about possible vulnerabilities and their
prevention in the container image, such as a
<a href=https://www.first.org/cvss/>Common Vulnerability Scoring System (CVSS)</a>
score. If the result of the image scans is combined with the pipeline
compliance rules, only properly patched container images will end up in
Production.</p><h2 id=admission-controllers>Admission controllers</h2><ul><li><input disabled type=checkbox> An appropriate selection of admission controllers is enabled.</li><li><input disabled type=checkbox> A pod security policy is enforced by the Pod Security Admission or/and a
webhook admission controller.</li><li><input disabled type=checkbox> The admission chain plugins and webhooks are securely configured.</li></ul><p>Admission controllers can help to improve the security of the cluster. However,
they can present risks themselves as they extend the API server and
<a href=/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/>should be properly secured</a>.</p><p>The following lists present a number of admission controllers that could be
considered to enhance the security posture of your cluster and application. It
includes controllers that may be referenced in other parts of this document.</p><p>This first group of admission controllers includes plugins
<a href=/docs/reference/access-authn-authz/admission-controllers/#which-plugins-are-enabled-by-default>enabled by default</a>,
consider to leave them enabled unless you know what you are doing:</p><dl><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#certificateapproval><code>CertificateApproval</code></a></dt><dd>Performs additional authorization checks to ensure the approving user has
permission to approve certificate request.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#certificatesigning><code>CertificateSigning</code></a></dt><dd>Performs additional authorization checks to ensure the signing user has
permission to sign certificate requests.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#certificatesubjectrestriction><code>CertificateSubjectRestriction</code></a></dt><dd>Rejects any certificate request that specifies a 'group' (or 'organization
attribute') of <code>system:masters</code>.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#limitranger><code>LimitRanger</code></a></dt><dd>Enforce the LimitRange API constraints.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook><code>MutatingAdmissionWebhook</code></a></dt><dd>Allows the use of custom controllers through webhooks, these controllers may
mutate requests that it reviews.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#podsecurity><code>PodSecurity</code></a></dt><dd>Replacement for Pod Security Policy, restricts security contexts of deployed
Pods.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#resourcequota><code>ResourceQuota</code></a></dt><dd>Enforces resource quotas to prevent over-usage of resources.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook><code>ValidatingAdmissionWebhook</code></a></dt><dd>Allows the use of custom controllers through webhooks, these controllers do
not mutate requests that it reviews.</dd></dl><p>The second group includes plugin that are not enabled by default but in general
availability state and recommended to improve your security posture:</p><dl><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips><code>DenyServiceExternalIPs</code></a></dt><dd>Rejects all net-new usage of the <code>Service.spec.externalIPs</code> field. This is a mitigation for
<a href=https://github.com/kubernetes/kubernetes/issues/97076>CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs</a>.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction><code>NodeRestriction</code></a></dt><dd>Restricts kubelet's permissions to only modify the pods API resources they own
or the node API ressource that represent themselves. It also prevents kubelet
from using the <code>node-restriction.kubernetes.io/</code> annotation, which can be used
by an attacker with access to the kubelet's credentials to influence pod
placement to the controlled node.</dd></dl><p>The third group includes plugins that are not enabled by default but could be
considered for certain use cases:</p><dl><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages><code>AlwaysPullImages</code></a></dt><dd>Enforces the usage of the latest version of a tagged image and ensures that the deployer
has permissions to use the image.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook><code>ImagePolicyWebhook</code></a></dt><dd>Allows enforcing additional controls for images through webhooks.</dd></dl><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/concepts/security/rbac-good-practices/>RBAC Good Practices</a> for
further information on authorization.</li><li><a href=/docs/concepts/security/multi-tenancy/>Cluster Multi-tenancy guide</a> for
configuration options recommendations and best practices on multi-tenancy.</li><li><a href=/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/#building-secure-container-images>Blog post "A Closer Look at NSA/CISA Kubernetes Hardening Guidance"</a>
for complementary resource on hardening Kubernetes clusters.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ac9161c6d952925b083ad9602b4e8e7f>10 - Policies</h1><div class=lead>Policies you can configure that apply to groups of resources.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> See <a href=/docs/concepts/services-networking/network-policies/>Network Policies</a>
for documentation about NetworkPolicy in Kubernetes.</div></div><div class=td-content><h1 id=pg-a935ff8c59eb116b43494255cc67f69a>10.1 - Limit Ranges</h1><p>By default, containers run with unbounded <a href=/docs/concepts/configuration/manage-resources-containers/>compute resources</a> on a Kubernetes cluster.
Using Kubernetes <a href=/docs/concepts/policy/resource-quotas/>resource quotas</a>,
administrators (also termed <em>cluster operators</em>) can restrict consumption and creation
of cluster resources (such as CPU time, memory, and persistent storage) within a specified
<a class=glossary-tooltip title='An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a>.
Within a namespace, a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> can consume as much CPU and memory as is allowed by the ResourceQuotas that apply to that namespace. As a cluster operator, or as a namespace-level administrator, you might also be concerned about making sure that a single object cannot monopolize all available resources within a namespace.</p><p>A LimitRange is a policy to constrain the resource allocations (limits and requests) that you can specify for each applicable object kind (such as Pod or <a class=glossary-tooltip title='Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PersistentVolumeClaim>PersistentVolumeClaim</a>) in a namespace.</p><p>A <em>LimitRange</em> provides constraints that can:</p><ul><li>Enforce minimum and maximum compute resources usage per Pod or Container in a namespace.</li><li>Enforce minimum and maximum storage request per <a class=glossary-tooltip title='Claims storage resources defined in a PersistentVolume so that it can be mounted as a volume in a container.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims target=_blank aria-label=PersistentVolumeClaim>PersistentVolumeClaim</a> in a namespace.</li><li>Enforce a ratio between request and limit for a resource in a namespace.</li><li>Set default request/limit for compute resources in a namespace and automatically inject them to Containers at runtime.</li></ul><p>A LimitRange is enforced in a particular namespace when there is a
LimitRange object in that namespace.</p><p>The name of a LimitRange object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><h2 id=constraints-on-resource-limits-and-requests>Constraints on resource limits and requests</h2><ul><li>The administrator creates a LimitRange in a namespace.</li><li>Users create (or try to create) objects in that namespace, such as Pods or PersistentVolumeClaims.</li><li>First, the <code>LimitRange</code> admission controller applies default request and limit values for all Pods (and their containers) that do not set compute resource requirements.</li><li>Second, the <code>LimitRange</code> tracks usage to ensure it does not exceed resource minimum, maximum and ratio defined in any <code>LimitRange</code> present in the namespace.</li><li>If you attempt to create or update an object (Pod or PersistentVolumeClaim) that violates a <code>LimitRange</code> constraint, your request to the API server will fail with an HTTP status code <code>403 Forbidden</code> and a message explaining the constraint that has been violated.</li><li>If you add a <code>LimitRange</code> in a namespace that applies to compute-related resources such as
<code>cpu</code> and <code>memory</code>, you must specify
requests or limits for those values. Otherwise, the system may reject Pod creation.</li><li><code>LimitRange</code> validations occur only at Pod admission stage, not on running Pods.
If you add or modify a LimitRange, the Pods that already exist in that namespace
continue unchanged.</li><li>If two or more <code>LimitRange</code> objects exist in the namespace, it is not deterministic which default value will be applied.</li></ul><h2 id=limitrange-and-admission-checks-for-pods>LimitRange and admission checks for Pods</h2><p>A <code>LimitRange</code> does <strong>not</strong> check the consistency of the default values it applies. This means that a default value for the <em>limit</em> that is set by <code>LimitRange</code> may be less than the <em>request</em> value specified for the container in the spec that a client submits to the API server. If that happens, the final Pod will not be scheduleable.</p><p>For example, you define a <code>LimitRange</code> with this manifest:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/concepts/policy/limit-range/problematic-limit-range.yaml download=concepts/policy/limit-range/problematic-limit-range.yaml><code>concepts/policy/limit-range/problematic-limit-range.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("concepts-policy-limit-range-problematic-limit-range-yaml")' title="Copy concepts/policy/limit-range/problematic-limit-range.yaml to clipboard"></img></div><div class=includecode id=concepts-policy-limit-range-problematic-limit-range-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>LimitRange<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu-resource-constraint<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>default</span>:<span style=color:#bbb> </span><span style=color:#080;font-style:italic># this section defines default limits</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>defaultRequest</span>:<span style=color:#bbb> </span><span style=color:#080;font-style:italic># this section defines default requests</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>max</span>:<span style=color:#bbb> </span><span style=color:#080;font-style:italic># max and min define the limit range</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>min</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>along with a Pod that declares a CPU resource request of <code>700m</code>, but not a limit:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml download=concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml><code>concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("concepts-policy-limit-range-example-conflict-with-limitrange-cpu-yaml")' title="Copy concepts/policy/limit-range/example-conflict-with-limitrange-cpu.yaml to clipboard"></img></div><div class=includecode id=concepts-policy-limit-range-example-conflict-with-limitrange-cpu-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-conflict-with-limitrange-cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>700m<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>then that Pod will not be scheduled, failing with an error similar to:</p><pre tabindex=0><code>Pod &#34;example-conflict-with-limitrange-cpu&#34; is invalid: spec.containers[0].resources.requests: Invalid value: &#34;700m&#34;: must be less than or equal to cpu limit
</code></pre><p>If you set both <code>request</code> and <code>limit</code>, then that new Pod will be scheduled successfully even with the same <code>LimitRange</code> in place:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml download=concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml><code>concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("concepts-policy-limit-range-example-no-conflict-with-limitrange-cpu-yaml")' title="Copy concepts/policy/limit-range/example-no-conflict-with-limitrange-cpu.yaml to clipboard"></img></div><div class=includecode id=concepts-policy-limit-range-example-no-conflict-with-limitrange-cpu-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-no-conflict-with-limitrange-cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>demo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>700m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>700m<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h2 id=example-resource-constraints>Example resource constraints</h2><p>Examples of policies that could be created using <code>LimitRange</code> are:</p><ul><li>In a 2 node cluster with a capacity of 8 GiB RAM and 16 cores, constrain Pods in a namespace to request 100m of CPU with a max limit of 500m for CPU and request 200Mi for Memory with a max limit of 600Mi for Memory.</li><li>Define default CPU limit and request to 150m and memory default request to 300Mi for Containers started with no cpu and memory requests in their specs.</li></ul><p>In the case where the total limits of the namespace is less than the sum of the limits of the Pods/Containers,
there may be contention for resources. In this case, the Containers or Pods will not be created.</p><p>Neither contention nor changes to a LimitRange will affect already created resources.</p><h2 id=what-s-next>What's next</h2><p>For examples on using limits, see:</p><ul><li><a href=/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/>how to configure minimum and maximum CPU constraints per namespace</a>.</li><li><a href=/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/>how to configure minimum and maximum Memory constraints per namespace</a>.</li><li><a href=/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/>how to configure default CPU Requests and Limits per namespace</a>.</li><li><a href=/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/>how to configure default Memory Requests and Limits per namespace</a>.</li><li><a href=/docs/tasks/administer-cluster/limit-storage-consumption/#limitrange-to-limit-requests-for-storage>how to configure minimum and maximum Storage consumption per namespace</a>.</li><li>a <a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>detailed example on configuring quota per namespace</a>.</li></ul><p>Refer to the <a href=https://git.k8s.io/design-proposals-archive/resource-management/admission_control_limit_range.md>LimitRanger design document</a> for context and historical information.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-94ddc6e901c30f256138db11d09f05a3>10.2 - Resource Quotas</h1><p>When several users or teams share a cluster with a fixed number of nodes,
there is a concern that one team could use more than its fair share of resources.</p><p>Resource quotas are a tool for administrators to address this concern.</p><p>A resource quota, defined by a <code>ResourceQuota</code> object, provides constraints that limit
aggregate resource consumption per namespace. It can limit the quantity of objects that can
be created in a namespace by type, as well as the total amount of compute resources that may
be consumed by resources in that namespace.</p><p>Resource quotas work like this:</p><ul><li><p>Different teams work in different namespaces. This can be enforced with <a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>.</p></li><li><p>The administrator creates one ResourceQuota for each namespace.</p></li><li><p>Users create resources (pods, services, etc.) in the namespace, and the quota system
tracks usage to ensure it does not exceed hard resource limits defined in a ResourceQuota.</p></li><li><p>If creating or updating a resource violates a quota constraint, the request will fail with HTTP
status code <code>403 FORBIDDEN</code> with a message explaining the constraint that would have been violated.</p></li><li><p>If quota is enabled in a namespace for compute resources like <code>cpu</code> and <code>memory</code>, users must specify
requests or limits for those values; otherwise, the quota system may reject pod creation. Hint: Use
the <code>LimitRanger</code> admission controller to force defaults for pods that make no compute resource requirements.</p><p>See the <a href=/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/>walkthrough</a>
for an example of how to avoid this problem.</p></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><ul><li>For <code>cpu</code> and <code>memory</code> resources, ResourceQuotas enforce that <strong>every</strong>
(new) pod in that namespace sets a limit for that resource.
If you enforce a resource quota in a namespace for either <code>cpu</code> or <code>memory</code>,
you, and other clients, <strong>must</strong> specify either <code>requests</code> or <code>limits</code> for that resource,
for every new Pod you submit. If you don't, the control plane may reject admission
for that Pod.</li><li>For other resources: ResourceQuota works and will ignore pods in the namespace without setting a limit or request for that resource. It means that you can create a new pod without limit/request ephemeral storage if the resource quota limits the ephemeral storage of this namespace.
You can use a <a href=/docs/concepts/policy/limit-range/>LimitRange</a> to automatically set
a default request for these resources.</li></ul></div><p>The name of a ResourceQuota object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>Examples of policies that could be created using namespaces and quotas are:</p><ul><li>In a cluster with a capacity of 32 GiB RAM, and 16 cores, let team A use 20 GiB and 10 cores,
let B use 10GiB and 4 cores, and hold 2GiB and 2 cores in reserve for future allocation.</li><li>Limit the "testing" namespace to using 1 core and 1GiB RAM. Let the "production" namespace
use any amount.</li></ul><p>In the case where the total capacity of the cluster is less than the sum of the quotas of the namespaces,
there may be contention for resources. This is handled on a first-come-first-served basis.</p><p>Neither contention nor changes to quota will affect already created resources.</p><h2 id=enabling-resource-quota>Enabling Resource Quota</h2><p>Resource Quota support is enabled by default for many Kubernetes distributions. It is
enabled when the <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a>
<code>--enable-admission-plugins=</code> flag has <code>ResourceQuota</code> as
one of its arguments.</p><p>A resource quota is enforced in a particular namespace when there is a
ResourceQuota in that namespace.</p><h2 id=compute-resource-quota>Compute Resource Quota</h2><p>You can limit the total sum of
<a href=/docs/concepts/configuration/manage-resources-containers/>compute resources</a>
that can be requested in a given namespace.</p><p>The following resource types are supported:</p><table><thead><tr><th>Resource Name</th><th>Description</th></tr></thead><tbody><tr><td><code>limits.cpu</code></td><td>Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value.</td></tr><tr><td><code>limits.memory</code></td><td>Across all pods in a non-terminal state, the sum of memory limits cannot exceed this value.</td></tr><tr><td><code>requests.cpu</code></td><td>Across all pods in a non-terminal state, the sum of CPU requests cannot exceed this value.</td></tr><tr><td><code>requests.memory</code></td><td>Across all pods in a non-terminal state, the sum of memory requests cannot exceed this value.</td></tr><tr><td><code>hugepages-&lt;size></code></td><td>Across all pods in a non-terminal state, the number of huge page requests of the specified size cannot exceed this value.</td></tr><tr><td><code>cpu</code></td><td>Same as <code>requests.cpu</code></td></tr><tr><td><code>memory</code></td><td>Same as <code>requests.memory</code></td></tr></tbody></table><h3 id=resource-quota-for-extended-resources>Resource Quota For Extended Resources</h3><p>In addition to the resources mentioned above, in release 1.10, quota support for
<a href=/docs/concepts/configuration/manage-resources-containers/#extended-resources>extended resources</a> is added.</p><p>As overcommit is not allowed for extended resources, it makes no sense to specify both <code>requests</code>
and <code>limits</code> for the same extended resource in a quota. So for extended resources, only quota items
with prefix <code>requests.</code> is allowed for now.</p><p>Take the GPU resource as an example, if the resource name is <code>nvidia.com/gpu</code>, and you want to
limit the total number of GPUs requested in a namespace to 4, you can define a quota as follows:</p><ul><li><code>requests.nvidia.com/gpu: 4</code></li></ul><p>See <a href=#viewing-and-setting-quotas>Viewing and Setting Quotas</a> for more detail information.</p><h2 id=storage-resource-quota>Storage Resource Quota</h2><p>You can limit the total sum of <a href=/docs/concepts/storage/persistent-volumes/>storage resources</a> that can be requested in a given namespace.</p><p>In addition, you can limit consumption of storage resources based on associated storage-class.</p><table><thead><tr><th>Resource Name</th><th>Description</th></tr></thead><tbody><tr><td><code>requests.storage</code></td><td>Across all persistent volume claims, the sum of storage requests cannot exceed this value.</td></tr><tr><td><code>persistentvolumeclaims</code></td><td>The total number of <a href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>PersistentVolumeClaims</a> that can exist in the namespace.</td></tr><tr><td><code>&lt;storage-class-name>.storageclass.storage.k8s.io/requests.storage</code></td><td>Across all persistent volume claims associated with the <code>&lt;storage-class-name></code>, the sum of storage requests cannot exceed this value.</td></tr><tr><td><code>&lt;storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims</code></td><td>Across all persistent volume claims associated with the storage-class-name, the total number of <a href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>persistent volume claims</a> that can exist in the namespace.</td></tr></tbody></table><p>For example, if an operator wants to quota storage with <code>gold</code> storage class separate from <code>bronze</code> storage class, the operator can
define a quota as follows:</p><ul><li><code>gold.storageclass.storage.k8s.io/requests.storage: 500Gi</code></li><li><code>bronze.storageclass.storage.k8s.io/requests.storage: 100Gi</code></li></ul><p>In release 1.8, quota support for local ephemeral storage is added as an alpha feature:</p><table><thead><tr><th>Resource Name</th><th>Description</th></tr></thead><tbody><tr><td><code>requests.ephemeral-storage</code></td><td>Across all pods in the namespace, the sum of local ephemeral storage requests cannot exceed this value.</td></tr><tr><td><code>limits.ephemeral-storage</code></td><td>Across all pods in the namespace, the sum of local ephemeral storage limits cannot exceed this value.</td></tr><tr><td><code>ephemeral-storage</code></td><td>Same as <code>requests.ephemeral-storage</code>.</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> When using a CRI container runtime, container logs will count against the ephemeral storage quota.
This can result in the unexpected eviction of pods that have exhausted their storage quotas.
Refer to <a href=/docs/concepts/cluster-administration/logging/>Logging Architecture</a> for details.</div><h2 id=object-count-quota>Object Count Quota</h2><p>You can set quota for the total number of certain resources of all standard,
namespaced resource types using the following syntax:</p><ul><li><code>count/&lt;resource>.&lt;group></code> for resources from non-core groups</li><li><code>count/&lt;resource></code> for resources from the core group</li></ul><p>Here is an example set of resources users may want to put under object count quota:</p><ul><li><code>count/persistentvolumeclaims</code></li><li><code>count/services</code></li><li><code>count/secrets</code></li><li><code>count/configmaps</code></li><li><code>count/replicationcontrollers</code></li><li><code>count/deployments.apps</code></li><li><code>count/replicasets.apps</code></li><li><code>count/statefulsets.apps</code></li><li><code>count/jobs.batch</code></li><li><code>count/cronjobs.batch</code></li></ul><p>The same syntax can be used for custom resources.
For example, to create a quota on a <code>widgets</code> custom resource in the <code>example.com</code> API group, use <code>count/widgets.example.com</code>.</p><p>When using <code>count/*</code> resource quota, an object is charged against the quota if it exists in server storage.
These types of quotas are useful to protect against exhaustion of storage resources. For example, you may
want to limit the number of Secrets in a server given their large size. Too many Secrets in a cluster can
actually prevent servers and controllers from starting. You can set a quota for Jobs to protect against
a poorly configured CronJob. CronJobs that create too many Jobs in a namespace can lead to a denial of service.</p><p>It is also possible to do generic object count quota on a limited set of resources.
The following types are supported:</p><table><thead><tr><th>Resource Name</th><th>Description</th></tr></thead><tbody><tr><td><code>configmaps</code></td><td>The total number of ConfigMaps that can exist in the namespace.</td></tr><tr><td><code>persistentvolumeclaims</code></td><td>The total number of <a href=/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims>PersistentVolumeClaims</a> that can exist in the namespace.</td></tr><tr><td><code>pods</code></td><td>The total number of Pods in a non-terminal state that can exist in the namespace. A pod is in a terminal state if <code>.status.phase in (Failed, Succeeded)</code> is true.</td></tr><tr><td><code>replicationcontrollers</code></td><td>The total number of ReplicationControllers that can exist in the namespace.</td></tr><tr><td><code>resourcequotas</code></td><td>The total number of ResourceQuotas that can exist in the namespace.</td></tr><tr><td><code>services</code></td><td>The total number of Services that can exist in the namespace.</td></tr><tr><td><code>services.loadbalancers</code></td><td>The total number of Services of type <code>LoadBalancer</code> that can exist in the namespace.</td></tr><tr><td><code>services.nodeports</code></td><td>The total number of Services of type <code>NodePort</code> that can exist in the namespace.</td></tr><tr><td><code>secrets</code></td><td>The total number of Secrets that can exist in the namespace.</td></tr></tbody></table><p>For example, <code>pods</code> quota counts and enforces a maximum on the number of <code>pods</code>
created in a single namespace that are not terminal. You might want to set a <code>pods</code>
quota on a namespace to avoid the case where a user creates many small pods and
exhausts the cluster's supply of Pod IPs.</p><h2 id=quota-scopes>Quota Scopes</h2><p>Each quota can have an associated set of <code>scopes</code>. A quota will only measure usage for a resource if it matches
the intersection of enumerated scopes.</p><p>When a scope is added to the quota, it limits the number of resources it supports to those that pertain to the scope.
Resources specified on the quota outside of the allowed set results in a validation error.</p><table><thead><tr><th>Scope</th><th>Description</th></tr></thead><tbody><tr><td><code>Terminating</code></td><td>Match pods where <code>.spec.activeDeadlineSeconds >= 0</code></td></tr><tr><td><code>NotTerminating</code></td><td>Match pods where <code>.spec.activeDeadlineSeconds is nil</code></td></tr><tr><td><code>BestEffort</code></td><td>Match pods that have best effort quality of service.</td></tr><tr><td><code>NotBestEffort</code></td><td>Match pods that do not have best effort quality of service.</td></tr><tr><td><code>PriorityClass</code></td><td>Match pods that references the specified <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption>priority class</a>.</td></tr><tr><td><code>CrossNamespacePodAffinity</code></td><td>Match pods that have cross-namespace pod <a href=/docs/concepts/scheduling-eviction/assign-pod-node>(anti)affinity terms</a>.</td></tr></tbody></table><p>The <code>BestEffort</code> scope restricts a quota to tracking the following resource:</p><ul><li><code>pods</code></li></ul><p>The <code>Terminating</code>, <code>NotTerminating</code>, <code>NotBestEffort</code> and <code>PriorityClass</code>
scopes restrict a quota to tracking the following resources:</p><ul><li><code>pods</code></li><li><code>cpu</code></li><li><code>memory</code></li><li><code>requests.cpu</code></li><li><code>requests.memory</code></li><li><code>limits.cpu</code></li><li><code>limits.memory</code></li></ul><p>Note that you cannot specify both the <code>Terminating</code> and the <code>NotTerminating</code>
scopes in the same quota, and you cannot specify both the <code>BestEffort</code> and
<code>NotBestEffort</code> scopes in the same quota either.</p><p>The <code>scopeSelector</code> supports the following values in the <code>operator</code> field:</p><ul><li><code>In</code></li><li><code>NotIn</code></li><li><code>Exists</code></li><li><code>DoesNotExist</code></li></ul><p>When using one of the following values as the <code>scopeName</code> when defining the
<code>scopeSelector</code>, the <code>operator</code> must be <code>Exists</code>.</p><ul><li><code>Terminating</code></li><li><code>NotTerminating</code></li><li><code>BestEffort</code></li><li><code>NotBestEffort</code></li></ul><p>If the <code>operator</code> is <code>In</code> or <code>NotIn</code>, the <code>values</code> field must have at least
one value. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- middle<span style=color:#bbb>
</span></span></span></code></pre></div><p>If the <code>operator</code> is <code>Exists</code> or <code>DoesNotExist</code>, the <code>values</code> field must <em>NOT</em> be
specified.</p><h3 id=resource-quota-per-priorityclass>Resource Quota Per PriorityClass</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.17 [stable]</code></div><p>Pods can be created at a specific <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority>priority</a>.
You can control a pod's consumption of system resources based on a pod's priority, by using the <code>scopeSelector</code>
field in the quota spec.</p><p>A quota is matched and consumed only if <code>scopeSelector</code> in the quota spec selects the pod.</p><p>When quota is scoped for priority class using <code>scopeSelector</code> field, quota object
is restricted to track only following resources:</p><ul><li><code>pods</code></li><li><code>cpu</code></li><li><code>memory</code></li><li><code>ephemeral-storage</code></li><li><code>limits.cpu</code></li><li><code>limits.memory</code></li><li><code>limits.ephemeral-storage</code></li><li><code>requests.cpu</code></li><li><code>requests.memory</code></li><li><code>requests.ephemeral-storage</code></li></ul><p>This example creates a quota object and matches it with pods at specific priorities. The example
works as follows:</p><ul><li>Pods in the cluster have one of the three priority classes, "low", "medium", "high".</li><li>One quota object is created for each priority.</li></ul><p>Save the following YAML to a file <code>quota.yml</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>items</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-high<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1000&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>operator </span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;high&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-medium<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>20Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>operator </span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;medium&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-low<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;5&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>10Gi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>operator </span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;low&#34;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div><p>Apply the YAML using <code>kubectl create</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f ./quota.yml
</span></span></code></pre></div><pre tabindex=0><code>resourcequota/pods-high created
resourcequota/pods-medium created
resourcequota/pods-low created
</code></pre><p>Verify that <code>Used</code> quota is <code>0</code> using <code>kubectl describe quota</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe quota
</span></span></code></pre></div><pre tabindex=0><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     1k
memory      0     200Gi
pods        0     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><p>Create a pod with priority "high". Save the following YAML to a
file <code>high-priority-pod.yml</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>ubuntu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;/bin/sh&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;-c&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;while true; do echo hello; sleep 10;done&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;10Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500m&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityClassName</span>:<span style=color:#bbb> </span>high<span style=color:#bbb>
</span></span></span></code></pre></div><p>Apply it with <code>kubectl create</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f ./high-priority-pod.yml
</span></span></code></pre></div><p>Verify that "Used" stats for "high" priority quota, <code>pods-high</code>, has changed and that
the other two quotas are unchanged.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe quota
</span></span></code></pre></div><pre tabindex=0><code>Name:       pods-high
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         500m  1k
memory      10Gi  200Gi
pods        1     10


Name:       pods-low
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     5
memory      0     10Gi
pods        0     10


Name:       pods-medium
Namespace:  default
Resource    Used  Hard
--------    ----  ----
cpu         0     10
memory      0     20Gi
pods        0     10
</code></pre><h3 id=cross-namespace-pod-affinity-quota>Cross-namespace Pod Affinity Quota</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>Operators can use <code>CrossNamespacePodAffinity</code> quota scope to limit which namespaces are allowed to
have pods with affinity terms that cross namespaces. Specifically, it controls which pods are allowed
to set <code>namespaces</code> or <code>namespaceSelector</code> fields in pod affinity terms.</p><p>Preventing users from using cross-namespace affinity terms might be desired since a pod
with anti-affinity constraints can block pods from all other namespaces
from getting scheduled in a failure domain.</p><p>Using this scope operators can prevent certain namespaces (<code>foo-ns</code> in the example below)
from having pods that use cross-namespace pod affinity by creating a resource quota object in
that namespace with <code>CrossNamespaceAffinity</code> scope and hard limit of 0:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>disable-cross-namespace-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>foo-ns<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>hard</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pods</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>CrossNamespaceAffinity<span style=color:#bbb>
</span></span></span></code></pre></div><p>If operators want to disallow using <code>namespaces</code> and <code>namespaceSelector</code> by default, and
only allow it for specific namespaces, they could configure <code>CrossNamespaceAffinity</code>
as a limited resource by setting the kube-apiserver flag --admission-control-config-file
to the path of the following configuration file:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>AdmissionConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>plugins</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;ResourceQuota&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>configuration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuotaConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>limitedResources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>resource</span>:<span style=color:#bbb> </span>pods<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchScopes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>CrossNamespaceAffinity<span style=color:#bbb>
</span></span></span></code></pre></div><p>With the above configuration, pods can use <code>namespaces</code> and <code>namespaceSelector</code> in pod affinity only
if the namespace where they are created have a resource quota object with
<code>CrossNamespaceAffinity</code> scope and a hard limit greater than or equal to the number of pods using those fields.</p><h2 id=requests-vs-limits>Requests compared to Limits</h2><p>When allocating compute resources, each container may specify a request and a limit value for either CPU or memory.
The quota can be configured to quota either value.</p><p>If the quota has a value specified for <code>requests.cpu</code> or <code>requests.memory</code>, then it requires that every incoming
container makes an explicit request for those resources. If the quota has a value specified for <code>limits.cpu</code> or <code>limits.memory</code>,
then it requires that every incoming container specifies an explicit limit for those resources.</p><h2 id=viewing-and-setting-quotas>Viewing and Setting Quotas</h2><p>Kubectl supports creating, updating, and viewing quotas:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create namespace myspace
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; compute-resources.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: ResourceQuota
</span></span></span><span style=display:flex><span><span style=color:#b44>metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>  name: compute-resources
</span></span></span><span style=display:flex><span><span style=color:#b44>spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>  hard:
</span></span></span><span style=display:flex><span><span style=color:#b44>    requests.cpu: &#34;1&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    requests.memory: 1Gi
</span></span></span><span style=display:flex><span><span style=color:#b44>    limits.cpu: &#34;2&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    limits.memory: 2Gi
</span></span></span><span style=display:flex><span><span style=color:#b44>    requests.nvidia.com/gpu: 4
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f ./compute-resources.yaml --namespace<span style=color:#666>=</span>myspace
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; object-counts.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: ResourceQuota
</span></span></span><span style=display:flex><span><span style=color:#b44>metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>  name: object-counts
</span></span></span><span style=display:flex><span><span style=color:#b44>spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>  hard:
</span></span></span><span style=display:flex><span><span style=color:#b44>    configmaps: &#34;10&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    persistentvolumeclaims: &#34;4&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    pods: &#34;4&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    replicationcontrollers: &#34;20&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    secrets: &#34;10&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    services: &#34;10&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>    services.loadbalancers: &#34;2&#34;
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create -f ./object-counts.yaml --namespace<span style=color:#666>=</span>myspace
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get quota --namespace<span style=color:#666>=</span>myspace
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>NAME                    AGE
compute-resources       30s
object-counts           32s
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe quota compute-resources --namespace<span style=color:#666>=</span>myspace
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>Name:                    compute-resources
Namespace:               myspace
Resource                 Used  Hard
--------                 ----  ----
limits.cpu               0     2
limits.memory            0     2Gi
requests.cpu             0     1
requests.memory          0     1Gi
requests.nvidia.com/gpu  0     4
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe quota object-counts --namespace<span style=color:#666>=</span>myspace
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>Name:                   object-counts
Namespace:              myspace
Resource                Used    Hard
--------                ----    ----
configmaps              0       10
persistentvolumeclaims  0       4
pods                    0       4
replicationcontrollers  0       20
secrets                 1       10
services                0       10
services.loadbalancers  0       2
</code></pre><p>Kubectl also supports object count quota for all standard namespaced resources
using the syntax <code>count/&lt;resource>.&lt;group></code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create namespace myspace
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create quota <span style=color:#a2f>test</span> --hard<span style=color:#666>=</span>count/deployments.apps<span style=color:#666>=</span>2,count/replicasets.apps<span style=color:#666>=</span>4,count/pods<span style=color:#666>=</span>3,count/secrets<span style=color:#666>=</span><span style=color:#666>4</span> --namespace<span style=color:#666>=</span>myspace
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create deployment nginx --image<span style=color:#666>=</span>nginx --namespace<span style=color:#666>=</span>myspace --replicas<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe quota --namespace<span style=color:#666>=</span>myspace
</span></span></code></pre></div><pre tabindex=0><code>Name:                         test
Namespace:                    myspace
Resource                      Used  Hard
--------                      ----  ----
count/deployments.apps        1     2
count/pods                    2     3
count/replicasets.apps        1     4
count/secrets                 1     4
</code></pre><h2 id=quota-and-cluster-capacity>Quota and Cluster Capacity</h2><p>ResourceQuotas are independent of the cluster capacity. They are
expressed in absolute units. So, if you add nodes to your cluster, this does <em>not</em>
automatically give each namespace the ability to consume more resources.</p><p>Sometimes more complex policies may be desired, such as:</p><ul><li>Proportionally divide total cluster resources among several teams.</li><li>Allow each tenant to grow resource usage as needed, but have a generous
limit to prevent accidental resource exhaustion.</li><li>Detect demand from one namespace, add nodes, and increase quota.</li></ul><p>Such policies could be implemented using <code>ResourceQuotas</code> as building blocks, by
writing a "controller" that watches the quota usage and adjusts the quota
hard limits of each namespace according to other signals.</p><p>Note that resource quota divides up aggregate cluster resources, but it creates no
restrictions around nodes: pods from several namespaces may run on the same node.</p><h2 id=limit-priority-class-consumption-by-default>Limit Priority Class consumption by default</h2><p>It may be desired that pods at a particular priority, eg. "cluster-services",
should be allowed in a namespace, if and only if, a matching quota object exists.</p><p>With this mechanism, operators are able to restrict usage of certain high
priority classes to a limited number of namespaces and not every namespace
will be able to consume these priority classes by default.</p><p>To enforce this, <code>kube-apiserver</code> flag <code>--admission-control-config-file</code> should be
used to pass path to the following configuration file:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>AdmissionConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>plugins</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;ResourceQuota&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>configuration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuotaConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>limitedResources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>resource</span>:<span style=color:#bbb> </span>pods<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchScopes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;cluster-services&#34;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div><p>Then, create a resource quota object in the <code>kube-system</code> namespace:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/policy/priority-class-resourcequota.yaml download=policy/priority-class-resourcequota.yaml><code>policy/priority-class-resourcequota.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("policy-priority-class-resourcequota-yaml")' title="Copy policy/priority-class-resourcequota.yaml to clipboard"></img></div><div class=includecode id=policy-priority-class-resourcequota-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ResourceQuota<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pods-cluster-services<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scopeSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>operator </span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>scopeName</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;cluster-services&#34;</span>]</span></span></code></pre></div></div></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/policy/priority-class-resourcequota.yaml -n kube-system
</span></span></code></pre></div><pre tabindex=0><code class=language-none data-lang=none>resourcequota/pods-cluster-services created
</code></pre><p>In this case, a pod creation will be allowed if:</p><ol><li>the Pod's <code>priorityClassName</code> is not specified.</li><li>the Pod's <code>priorityClassName</code> is specified to a value other than <code>cluster-services</code>.</li><li>the Pod's <code>priorityClassName</code> is set to <code>cluster-services</code>, it is to be created
in the <code>kube-system</code> namespace, and it has passed the resource quota check.</li></ol><p>A Pod creation request is rejected if its <code>priorityClassName</code> is set to <code>cluster-services</code>
and it is to be created in a namespace other than <code>kube-system</code>.</p><h2 id=what-s-next>What's next</h2><ul><li>See <a href=https://git.k8s.io/design-proposals-archive/resource-management/admission_control_resource_quota.md>ResourceQuota design doc</a> for more information.</li><li>See a <a href=/docs/tasks/administer-cluster/quota-api-object/>detailed example for how to use resource quota</a>.</li><li>Read <a href=https://git.k8s.io/design-proposals-archive/scheduling/pod-priority-resourcequota.md>Quota support for priority class design doc</a>.</li><li>See <a href=https://github.com/kubernetes/kubernetes/pull/36765>LimitedResources</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-7352434db5f5954d2f7656b46fe5a324>10.3 - Process ID Limits And Reservations</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>Kubernetes allow you to limit the number of process IDs (PIDs) that a
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> can use.
You can also reserve a number of allocatable PIDs for each <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a>
for use by the operating system and daemons (rather than by Pods).</p><p>Process IDs (PIDs) are a fundamental resource on nodes. It is trivial to hit the
task limit without hitting any other resource limits, which can then cause
instability to a host machine.</p><p>Cluster administrators require mechanisms to ensure that Pods running in the
cluster cannot induce PID exhaustion that prevents host daemons (such as the
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> or
<a class=glossary-tooltip title='kube-proxy is a network proxy that runs on each node in the cluster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a>,
and potentially also the container runtime) from running.
In addition, it is important to ensure that PIDs are limited among Pods in order
to ensure they have limited impact on other workloads on the same node.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> On certain Linux installations, the operating system sets the PIDs limit to a low default,
such as <code>32768</code>. Consider raising the value of <code>/proc/sys/kernel/pid_max</code>.</div><p>You can configure a kubelet to limit the number of PIDs a given Pod can consume.
For example, if your node's host OS is set to use a maximum of <code>262144</code> PIDs and
expect to host less than <code>250</code> Pods, one can give each Pod a budget of <code>1000</code>
PIDs to prevent using up that node's overall number of available PIDs. If the
admin wants to overcommit PIDs similar to CPU or memory, they may do so as well
with some additional risks. Either way, a single Pod will not be able to bring
the whole machine down. This kind of resource limiting helps to prevent simple
fork bombs from affecting operation of an entire cluster.</p><p>Per-Pod PID limiting allows administrators to protect one Pod from another, but
does not ensure that all Pods scheduled onto that host are unable to impact the node overall.
Per-Pod limiting also does not protect the node agents themselves from PID exhaustion.</p><p>You can also reserve an amount of PIDs for node overhead, separate from the
allocation to Pods. This is similar to how you can reserve CPU, memory, or other
resources for use by the operating system and other facilities outside of Pods
and their containers.</p><p>PID limiting is a an important sibling to <a href=/docs/concepts/configuration/manage-resources-containers/>compute
resource</a> requests
and limits. However, you specify it in a different way: rather than defining a
Pod's resource limit in the <code>.spec</code> for a Pod, you configure the limit as a
setting on the kubelet. Pod-defined PID limits are not currently supported.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> This means that the limit that applies to a Pod may be different depending on
where the Pod is scheduled. To make things simple, it's easiest if all Nodes use
the same PID resource limits and reservations.</div><h2 id=node-pid-limits>Node PID limits</h2><p>Kubernetes allows you to reserve a number of process IDs for the system use. To
configure the reservation, use the parameter <code>pid=&lt;number></code> in the
<code>--system-reserved</code> and <code>--kube-reserved</code> command line options to the kubelet.
The value you specified declares that the specified number of process IDs will
be reserved for the system as a whole and for Kubernetes system daemons
respectively.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Before Kubernetes version 1.20, PID resource limiting with Node-level
reservations required enabling the <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature
gate</a>
<code>SupportNodePidsLimit</code> to work.</div><h2 id=pod-pid-limits>Pod PID limits</h2><p>Kubernetes allows you to limit the number of processes running in a Pod. You
specify this limit at the node level, rather than configuring it as a resource
limit for a particular Pod. Each Node can have a different PID limit.<br>To configure the limit, you can specify the command line parameter <code>--pod-max-pids</code>
to the kubelet, or set <code>PodPidsLimit</code> in the kubelet
<a href=/docs/tasks/administer-cluster/kubelet-config-file/>configuration file</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Before Kubernetes version 1.20, PID resource limiting for Pods required enabling
the <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
<code>SupportPodPidsLimit</code> to work.</div><h2 id=pid-based-eviction>PID based eviction</h2><p>You can configure kubelet to start terminating a Pod when it is misbehaving and consuming abnormal amount of resources.
This feature is called eviction. You can
<a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Configure Out of Resource Handling</a>
for various eviction signals.
Use <code>pid.available</code> eviction signal to configure the threshold for number of PIDs used by Pod.
You can set soft and hard eviction policies.
However, even with the hard eviction policy, if the number of PIDs growing very fast,
node can still get into unstable state by hitting the node PIDs limit.
Eviction signal value is calculated periodically and does NOT enforce the limit.</p><p>PID limiting - per Pod and per Node sets the hard limit.
Once the limit is hit, workload will start experiencing failures when trying to get a new PID.
It may or may not lead to rescheduling of a Pod,
depending on how workload reacts on these failures and how liveleness and readiness
probes are configured for the Pod. However, if limits were set correctly,
you can guarantee that other Pods workload and system processes will not run out of PIDs
when one Pod is misbehaving.</p><h2 id=what-s-next>What's next</h2><ul><li>Refer to the <a href=https://github.com/kubernetes/enhancements/blob/097b4d8276bc9564e56adf72505d43ce9bc5e9e8/keps/sig-node/20190129-pid-limiting.md>PID Limiting enhancement document</a> for more information.</li><li>For historical context, read
<a href=/blog/2019/04/15/process-id-limiting-for-stability-improvements-in-kubernetes-1.14/>Process ID Limiting for Stability Improvements in Kubernetes 1.14</a>.</li><li>Read <a href=/docs/concepts/configuration/manage-resources-containers/>Managing Resources for Containers</a>.</li><li>Learn how to <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Configure Out of Resource Handling</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b528c4464c030f3f044124b38d778f04>10.4 - Node Resource Managers</h1><p>In order to support latency-critical and high-throughput workloads, Kubernetes offers a suite of Resource Managers. The managers aim to co-ordinate and optimise node's resources alignment for pods configured with a specific requirement for CPUs, devices, and memory (hugepages) resources.</p><p>The main manager, the Topology Manager, is a Kubelet component that co-ordinates the overall resource management process through its <a href=/docs/tasks/administer-cluster/topology-manager/>policy</a>.</p><p>The configuration of individual managers is elaborated in dedicated documents:</p><ul><li><a href=/docs/tasks/administer-cluster/cpu-management-policies/>CPU Manager Policies</a></li><li><a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager>Device Manager</a></li><li><a href=/docs/tasks/administer-cluster/memory-manager/>Memory Manager Policies</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c21d05f31057c5bcd2ebdd01f4e62a0e>11 - Scheduling, Preemption and Eviction</h1><div class=lead>In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating Pods with lower Priority so that Pods with higher Priority can schedule on Nodes. Eviction is the process of proactively terminating one or more Pods on resource-starved Nodes.</div><p>In Kubernetes, scheduling refers to making sure that <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>
are matched to <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a> so that the
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> can run them. Preemption
is the process of terminating Pods with lower <a class=glossary-tooltip title='Pod Priority indicates the importance of a Pod relative to other Pods.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority target=_blank aria-label=Priority>Priority</a>
so that Pods with higher Priority can schedule on Nodes. Eviction is the process
of terminating one or more Pods on Nodes.</p><h2 id=scheduling>Scheduling</h2><ul><li><a href=/docs/concepts/scheduling-eviction/kube-scheduler/>Kubernetes Scheduler</a></li><li><a href=/docs/concepts/scheduling-eviction/assign-pod-node/>Assigning Pods to Nodes</a></li><li><a href=/docs/concepts/scheduling-eviction/pod-overhead/>Pod Overhead</a></li><li><a href=/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod Topology Spread Constraints</a></li><li><a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>Taints and Tolerations</a></li><li><a href=/docs/concepts/scheduling-eviction/scheduling-framework>Scheduling Framework</a></li><li><a href=/docs/concepts/scheduling-eviction/scheduler-perf-tuning/>Scheduler Performance Tuning</a></li><li><a href=/docs/concepts/scheduling-eviction/resource-bin-packing/>Resource Bin Packing for Extended Resources</a></li></ul><h2 id=pod-disruption>Pod Disruption</h2><p><a href=/docs/concepts/workloads/pods/disruptions/>Pod disruption</a> is the process by which
Pods on Nodes are terminated either voluntarily or involuntarily.</p><p>Voluntary disruptions are started intentionally by application owners or cluster
administrators. Involuntary disruptions are unintentional and can be triggered by
unavoidable issues like Nodes running out of resources, or by accidental deletions.</p><ul><li><a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority and Preemption</a></li><li><a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a></li><li><a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated Eviction</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-598f36d691ab197f9d995784574b0a12>11.1 - Kubernetes Scheduler</h1><p>In Kubernetes, <em>scheduling</em> refers to making sure that <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>
are matched to <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a> so that
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a> can run them.</p><h2 id=scheduling>Scheduling overview</h2><p>A scheduler watches for newly created Pods that have no Node assigned. For
every Pod that the scheduler discovers, the scheduler becomes responsible
for finding the best Node for that Pod to run on. The scheduler reaches
this placement decision taking into account the scheduling principles
described below.</p><p>If you want to understand why Pods are placed onto a particular Node,
or if you're planning to implement a custom scheduler yourself, this
page will help you learn about scheduling.</p><h2 id=kube-scheduler>kube-scheduler</h2><p><a href=/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a>
is the default scheduler for Kubernetes and runs as part of the
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>.
kube-scheduler is designed so that, if you want and need to, you can
write your own scheduling component and use that instead.</p><p>For every newly created pod or other unscheduled pods, kube-scheduler
selects an optimal node for them to run on. However, every container in
pods has different requirements for resources and every pod also has
different requirements. Therefore, existing nodes need to be filtered
according to the specific scheduling requirements.</p><p>In a cluster, Nodes that meet the scheduling requirements for a Pod
are called <em>feasible</em> nodes. If none of the nodes are suitable, the pod
remains unscheduled until the scheduler is able to place it.</p><p>The scheduler finds feasible Nodes for a Pod and then runs a set of
functions to score the feasible Nodes and picks a Node with the highest
score among the feasible ones to run the Pod. The scheduler then notifies
the API server about this decision in a process called <em>binding</em>.</p><p>Factors that need to be taken into account for scheduling decisions include
individual and collective resource requirements, hardware / software /
policy constraints, affinity and anti-affinity specifications, data
locality, inter-workload interference, and so on.</p><h3 id=kube-scheduler-implementation>Node selection in kube-scheduler</h3><p>kube-scheduler selects a node for the pod in a 2-step operation:</p><ol><li>Filtering</li><li>Scoring</li></ol><p>The <em>filtering</em> step finds the set of Nodes where it's feasible to
schedule the Pod. For example, the PodFitsResources filter checks whether a
candidate Node has enough available resource to meet a Pod's specific
resource requests. After this step, the node list contains any suitable
Nodes; often, there will be more than one. If the list is empty, that
Pod isn't (yet) schedulable.</p><p>In the <em>scoring</em> step, the scheduler ranks the remaining nodes to choose
the most suitable Pod placement. The scheduler assigns a score to each Node
that survived filtering, basing this score on the active scoring rules.</p><p>Finally, kube-scheduler assigns the Pod to the Node with the highest ranking.
If there is more than one node with equal scores, kube-scheduler selects
one of these at random.</p><p>There are two supported ways to configure the filtering and scoring behavior
of the scheduler:</p><ol><li><a href=/docs/reference/scheduling/policies>Scheduling Policies</a> allow you to configure <em>Predicates</em> for filtering and <em>Priorities</em> for scoring.</li><li><a href=/docs/reference/scheduling/config/#profiles>Scheduling Profiles</a> allow you to configure Plugins that implement different scheduling stages, including: <code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code>, and others. You can also configure the kube-scheduler to run different profiles.</li></ol><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/scheduling-eviction/scheduler-perf-tuning/>scheduler performance tuning</a></li><li>Read about <a href=/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod topology spread constraints</a></li><li>Read the <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>reference documentation</a> for kube-scheduler</li><li>Read the <a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler config (v1beta3)</a> reference</li><li>Learn about <a href=/docs/tasks/extend-kubernetes/configure-multiple-schedulers/>configuring multiple schedulers</a></li><li>Learn about <a href=/docs/tasks/administer-cluster/topology-manager/>topology management policies</a></li><li>Learn about <a href=/docs/concepts/scheduling-eviction/pod-overhead/>Pod Overhead</a></li><li>Learn about scheduling of Pods that use volumes in:<ul><li><a href=/docs/concepts/storage/storage-classes/#volume-binding-mode>Volume Topology Support</a></li><li><a href=/docs/concepts/storage/storage-capacity/>Storage Capacity Tracking</a></li><li><a href=/docs/concepts/storage/storage-limits/>Node-specific Volume Limits</a></li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-21169f516071aea5d16734a4c27789a5>11.2 - Assigning Pods to Nodes</h1><p>You can constrain a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> so that it is
<em>restricted</em> to run on particular <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node(s)>node(s)</a>,
or to <em>prefer</em> to run on particular nodes.
There are several ways to do this and the recommended approaches all use
<a href=/docs/concepts/overview/working-with-objects/labels/>label selectors</a> to facilitate the selection.
Often, you do not need to set any such constraints; the
<a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a> will automatically do a reasonable placement
(for example, spreading your Pods across nodes so as not place Pods on a node with insufficient free resources).
However, there are some circumstances where you may want to control which node
the Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it,
or to co-locate Pods from two different services that communicate a lot into the same availability zone.</p><p>You can use any of the following methods to choose where Kubernetes schedules
specific Pods:</p><ul><li><a href=#nodeselector>nodeSelector</a> field matching against <a href=#built-in-node-labels>node labels</a></li><li><a href=#affinity-and-anti-affinity>Affinity and anti-affinity</a></li><li><a href=#nodename>nodeName</a> field</li><li><a href=#pod-topology-spread-constraints>Pod topology spread constraints</a></li></ul><h2 id=built-in-node-labels>Node labels</h2><p>Like many other Kubernetes objects, nodes have
<a href=/docs/concepts/overview/working-with-objects/labels/>labels</a>. You can <a href=/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node>attach labels manually</a>.
Kubernetes also populates a standard set of labels on all nodes in a cluster. See <a href=/docs/reference/labels-annotations-taints/>Well-Known Labels, Annotations and Taints</a>
for a list of common node labels.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The value of these labels is cloud provider specific and is not guaranteed to be reliable.
For example, the value of <code>kubernetes.io/hostname</code> may be the same as the node name in some environments
and a different value in other environments.</div><h3 id=node-isolation-restriction>Node isolation/restriction</h3><p>Adding labels to nodes allows you to target Pods for scheduling on specific
nodes or groups of nodes. You can use this functionality to ensure that specific
Pods only run on nodes with certain isolation, security, or regulatory
properties.</p><p>If you use labels for node isolation, choose label keys that the <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
cannot modify. This prevents a compromised node from setting those labels on
itself so that the scheduler schedules workloads onto the compromised node.</p><p>The <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction><code>NodeRestriction</code> admission plugin</a>
prevents the kubelet from setting or modifying labels with a
<code>node-restriction.kubernetes.io/</code> prefix.</p><p>To make use of that label prefix for node isolation:</p><ol><li>Ensure you are using the <a href=/docs/reference/access-authn-authz/node/>Node authorizer</a> and have <em>enabled</em> the <code>NodeRestriction</code> admission plugin.</li><li>Add labels with the <code>node-restriction.kubernetes.io/</code> prefix to your nodes, and use those labels in your <a href=#nodeselector>node selectors</a>.
For example, <code>example.com.node-restriction.kubernetes.io/fips=true</code> or <code>example.com.node-restriction.kubernetes.io/pci-dss=true</code>.</li></ol><h2 id=nodeselector>nodeSelector</h2><p><code>nodeSelector</code> is the simplest recommended form of node selection constraint.
You can add the <code>nodeSelector</code> field to your Pod specification and specify the
<a href=#built-in-node-labels>node labels</a> you want the target node to have.
Kubernetes only schedules the Pod onto nodes that have each of the labels you
specify.</p><p>See <a href=/docs/tasks/configure-pod-container/assign-pods-nodes>Assign Pods to Nodes</a> for more
information.</p><h2 id=affinity-and-anti-affinity>Affinity and anti-affinity</h2><p><code>nodeSelector</code> is the simplest way to constrain Pods to nodes with specific
labels. Affinity and anti-affinity expands the types of constraints you can
define. Some of the benefits of affinity and anti-affinity include:</p><ul><li>The affinity/anti-affinity language is more expressive. <code>nodeSelector</code> only
selects nodes with all the specified labels. Affinity/anti-affinity gives you
more control over the selection logic.</li><li>You can indicate that a rule is <em>soft</em> or <em>preferred</em>, so that the scheduler
still schedules the Pod even if it can't find a matching node.</li><li>You can constrain a Pod using labels on other Pods running on the node (or other topological domain),
instead of just node labels, which allows you to define rules for which Pods
can be co-located on a node.</li></ul><p>The affinity feature consists of two types of affinity:</p><ul><li><em>Node affinity</em> functions like the <code>nodeSelector</code> field but is more expressive and
allows you to specify soft rules.</li><li><em>Inter-pod affinity/anti-affinity</em> allows you to constrain Pods against labels
on other Pods.</li></ul><h3 id=node-affinity>Node affinity</h3><p>Node affinity is conceptually similar to <code>nodeSelector</code>, allowing you to constrain which nodes your
Pod can be scheduled on based on node labels. There are two types of node
affinity:</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code>: The scheduler can't
schedule the Pod unless the rule is met. This functions like <code>nodeSelector</code>,
but with a more expressive syntax.</li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code>: The scheduler tries to
find a node that meets the rule. If a matching node is not available, the
scheduler still schedules the Pod.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In the preceding types, <code>IgnoredDuringExecution</code> means that if the node labels
change after Kubernetes schedules the Pod, the Pod continues to run.</div><p>You can specify node affinities using the <code>.spec.affinity.nodeAffinity</code> field in
your Pod spec.</p><p>For example, consider the following Pod spec:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-node-affinity.yaml download=pods/pod-with-node-affinity.yaml><code>pods/pod-with-node-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-node-affinity-yaml")' title="Copy pods/pod-with-node-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-node-affinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- antarctica-east1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- antarctica-west1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>another-node-label-key<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- another-node-label-value<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0</span></span></code></pre></div></div></div><p>In this example, the following rules apply:</p><ul><li>The node <em>must</em> have a label with the key <code>topology.kubernetes.io/zone</code> and
the value of that label <em>must</em> be either <code>antarctica-east1</code> or <code>antarctica-west1</code>.</li><li>The node <em>preferably</em> has a label with the key <code>another-node-label-key</code> and
the value <code>another-node-label-value</code>.</li></ul><p>You can use the <code>operator</code> field to specify a logical operator for Kubernetes to use when
interpreting the rules. You can use <code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>,
<code>Gt</code> and <code>Lt</code>.</p><p><code>NotIn</code> and <code>DoesNotExist</code> allow you to define node anti-affinity behavior.
Alternatively, you can use <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>node taints</a>
to repel Pods from specific nodes.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>If you specify both <code>nodeSelector</code> and <code>nodeAffinity</code>, <em>both</em> must be satisfied
for the Pod to be scheduled onto a node.</p><p>If you specify multiple terms in <code>nodeSelectorTerms</code> associated with <code>nodeAffinity</code>
types, then the Pod can be scheduled onto a node if one of the specified terms
can be satisfied (terms are ORed).</p><p>If you specify multiple expressions in a single <code>matchExpressions</code> field associated with a
term in <code>nodeSelectorTerms</code>, then the Pod can be scheduled onto a node only
if all the expressions are satisfied (expressions are ANDed).</p></div><p>See <a href=/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/>Assign Pods to Nodes using Node Affinity</a>
for more information.</p><h4 id=node-affinity-weight>Node affinity weight</h4><p>You can specify a <code>weight</code> between 1 and 100 for each instance of the
<code>preferredDuringSchedulingIgnoredDuringExecution</code> affinity type. When the
scheduler finds nodes that meet all the other scheduling requirements of the Pod, the
scheduler iterates through every preferred rule that the node satisfies and adds the
value of the <code>weight</code> for that expression to a sum.</p><p>The final sum is added to the score of other priority functions for the node.
Nodes with the highest total score are prioritized when the scheduler makes a
scheduling decision for the Pod.</p><p>For example, consider the following Pod spec:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-affinity-anti-affinity.yaml download=pods/pod-with-affinity-anti-affinity.yaml><code>pods/pod-with-affinity-anti-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-affinity-anti-affinity-yaml")' title="Copy pods/pod-with-affinity-anti-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-affinity-anti-affinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-affinity-anti-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>kubernetes.io/os<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- linux<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>label-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- key-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>label-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- key-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>If there are two possible nodes that match the
<code>preferredDuringSchedulingIgnoredDuringExecution</code> rule, one with the
<code>label-1:key-1</code> label and another with the <code>label-2:key-2</code> label, the scheduler
considers the <code>weight</code> of each node and adds the weight to the other scores for
that node, and schedules the Pod onto the node with the highest final score.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you want Kubernetes to successfully schedule the Pods in this example, you
must have existing nodes with the <code>kubernetes.io/os=linux</code> label.</div><h4 id=node-affinity-per-scheduling-profile>Node affinity per scheduling profile</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code></div><p>When configuring multiple <a href=/docs/reference/scheduling/config/#multiple-profiles>scheduling profiles</a>, you can associate
a profile with a node affinity, which is useful if a profile only applies to a specific set of nodes.
To do so, add an <code>addedAffinity</code> to the <code>args</code> field of the <a href=/docs/reference/scheduling/config/#scheduling-plugins><code>NodeAffinity</code> plugin</a>
in the <a href=/docs/reference/scheduling/config/>scheduler configuration</a>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>foo-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeAffinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>addedAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>scheduler-profile<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span>- foo<span style=color:#bbb>
</span></span></span></code></pre></div><p>The <code>addedAffinity</code> is applied to all Pods that set <code>.spec.schedulerName</code> to <code>foo-scheduler</code>, in addition to the
NodeAffinity specified in the PodSpec.
That is, in order to match the Pod, nodes need to satisfy <code>addedAffinity</code> and
the Pod's <code>.spec.NodeAffinity</code>.</p><p>Since the <code>addedAffinity</code> is not visible to end users, its behavior might be
unexpected to them. Use node labels that have a clear correlation to the
scheduler profile name.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The DaemonSet controller, which <a href=/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler>creates Pods for DaemonSets</a>,
does not support scheduling profiles. When the DaemonSet controller creates
Pods, the default Kubernetes scheduler places those Pods and honors any
<code>nodeAffinity</code> rules in the DaemonSet controller.</div><h3 id=inter-pod-affinity-and-anti-affinity>Inter-pod affinity and anti-affinity</h3><p>Inter-pod affinity and anti-affinity allow you to constrain which nodes your
Pods can be scheduled on based on the labels of <strong>Pods</strong> already running on that
node, instead of the node labels.</p><p>Inter-pod affinity and anti-affinity rules take the form "this
Pod should (or, in the case of anti-affinity, should not) run in an X if that X
is already running one or more Pods that meet rule Y", where X is a topology
domain like node, rack, cloud provider zone or region, or similar and Y is the
rule Kubernetes tries to satisfy.</p><p>You express these rules (Y) as <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>label selectors</a>
with an optional associated list of namespaces. Pods are namespaced objects in
Kubernetes, so Pod labels also implicitly have namespaces. Any label selectors
for Pod labels should specify the namespaces in which Kubernetes should look for those
labels.</p><p>You express the topology domain (X) using a <code>topologyKey</code>, which is the key for
the node label that the system uses to denote the domain. For examples, see
<a href=/docs/reference/labels-annotations-taints/>Well-Known Labels, Annotations and Taints</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Inter-pod affinity and anti-affinity require substantial amount of
processing which can slow down scheduling in large clusters significantly. We do
not recommend using them in clusters larger than several hundred nodes.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Pod anti-affinity requires nodes to be consistently labelled, in other words,
every node in the cluster must have an appropriate label matching <code>topologyKey</code>.
If some or all nodes are missing the specified <code>topologyKey</code> label, it can lead
to unintended behavior.</div><h4 id=types-of-inter-pod-affinity-and-anti-affinity>Types of inter-pod affinity and anti-affinity</h4><p>Similar to <a href=#node-affinity>node affinity</a> are two types of Pod affinity and
anti-affinity as follows:</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code></li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code></li></ul><p>For example, you could use
<code>requiredDuringSchedulingIgnoredDuringExecution</code> affinity to tell the scheduler to
co-locate Pods of two services in the same cloud provider zone because they
communicate with each other a lot. Similarly, you could use
<code>preferredDuringSchedulingIgnoredDuringExecution</code> anti-affinity to spread Pods
from a service across multiple cloud provider zones.</p><p>To use inter-pod affinity, use the <code>affinity.podAffinity</code> field in the Pod spec.
For inter-pod anti-affinity, use the <code>affinity.podAntiAffinity</code> field in the Pod
spec.</p><h4 id=an-example-of-a-pod-that-uses-pod-affinity>Pod affinity example</h4><p>Consider the following Pod spec:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-pod-affinity.yaml download=pods/pod-with-pod-affinity.yaml><code>pods/pod-with-pod-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-pod-affinity-yaml")' title="Copy pods/pod-with-pod-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-pod-affinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- S1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinityTerm</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- S2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>This example defines one Pod affinity rule and one Pod anti-affinity rule. The
Pod affinity rule uses the "hard"
<code>requiredDuringSchedulingIgnoredDuringExecution</code>, while the anti-affinity rule
uses the "soft" <code>preferredDuringSchedulingIgnoredDuringExecution</code>.</p><p>The affinity rule says that the scheduler can only schedule a Pod onto a node if
the node is in the same zone as one or more existing Pods with the label
<code>security=S1</code>. More precisely, the scheduler must place the Pod on a node that has the
<code>topology.kubernetes.io/zone=V</code> label, as long as there is at least one node in
that zone that currently has one or more Pods with the Pod label <code>security=S1</code>.</p><p>The anti-affinity rule says that the scheduler should try to avoid scheduling
the Pod onto a node that is in the same zone as one or more Pods with the label
<code>security=S2</code>. More precisely, the scheduler should try to avoid placing the Pod on a node that has the
<code>topology.kubernetes.io/zone=R</code> label if there are other nodes in the
same zone currently running Pods with the <code>Security=S2</code> Pod label.</p><p>To get yourself more familiar with the examples of Pod affinity and anti-affinity,
refer to the <a href=https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md>design proposal</a>.</p><p>You can use the <code>In</code>, <code>NotIn</code>, <code>Exists</code> and <code>DoesNotExist</code> values in the
<code>operator</code> field for Pod affinity and anti-affinity.</p><p>In principle, the <code>topologyKey</code> can be any allowed label key with the following
exceptions for performance and security reasons:</p><ul><li>For Pod affinity and anti-affinity, an empty <code>topologyKey</code> field is not allowed in both <code>requiredDuringSchedulingIgnoredDuringExecution</code>
and <code>preferredDuringSchedulingIgnoredDuringExecution</code>.</li><li>For <code>requiredDuringSchedulingIgnoredDuringExecution</code> Pod anti-affinity rules,
the admission controller <code>LimitPodHardAntiAffinityTopology</code> limits
<code>topologyKey</code> to <code>kubernetes.io/hostname</code>. You can modify or disable the
admission controller if you want to allow custom topologies.</li></ul><p>In addition to <code>labelSelector</code> and <code>topologyKey</code>, you can optionally specify a list
of namespaces which the <code>labelSelector</code> should match against using the
<code>namespaces</code> field at the same level as <code>labelSelector</code> and <code>topologyKey</code>.
If omitted or empty, <code>namespaces</code> defaults to the namespace of the Pod where the
affinity/anti-affinity definition appears.</p><h4 id=namespace-selector>Namespace selector</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>You can also select matching namespaces using <code>namespaceSelector</code>, which is a label query over the set of namespaces.
The affinity term is applied to namespaces selected by both <code>namespaceSelector</code> and the <code>namespaces</code> field.
Note that an empty <code>namespaceSelector</code> ({}) matches all namespaces, while a null or empty <code>namespaces</code> list and
null <code>namespaceSelector</code> matches the namespace of the Pod where the rule is defined.</p><h4 id=more-practical-use-cases>More practical use-cases</h4><p>Inter-pod affinity and anti-affinity can be even more useful when they are used with higher
level collections such as ReplicaSets, StatefulSets, Deployments, etc. These
rules allow you to configure that a set of workloads should
be co-located in the same defined topology; for example, preferring to place two related
Pods onto the same node.</p><p>For example: imagine a three-node cluster. You use the cluster to run a web application
and also an in-memory cache (such as Redis). For this example, also assume that latency between
the web application and the memory cache should be as low as is practical. You could use inter-pod
affinity and anti-affinity to co-locate the web servers with the cache as much as possible.</p><p>In the following example Deployment for the Redis cache, the replicas get the label <code>app=store</code>. The
<code>podAntiAffinity</code> rule tells the scheduler to avoid placing multiple replicas
with the <code>app=store</code> label on a single node. This creates each cache in a
separate node.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-cache<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis:3.2-alpine<span style=color:#bbb>
</span></span></span></code></pre></div><p>The following example Deployment for the web servers creates replicas with the label <code>app=web-store</code>.
The Pod affinity rule tells the scheduler to place each replica on a node that has a Pod
with the label <code>app=store</code>. The Pod anti-affinity rule tells the scheduler never to place
multiple <code>app=web-store</code> servers on a single node.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- web-store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.16-alpine<span style=color:#bbb>
</span></span></span></code></pre></div><p>Creating the two preceding Deployments results in the following cluster layout,
where each web server is co-located with a cache, on three separate nodes.</p><table><thead><tr><th style=text-align:center>node-1</th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th></tr></thead><tbody><tr><td style=text-align:center><em>webserver-1</em></td><td style=text-align:center><em>webserver-2</em></td><td style=text-align:center><em>webserver-3</em></td></tr><tr><td style=text-align:center><em>cache-1</em></td><td style=text-align:center><em>cache-2</em></td><td style=text-align:center><em>cache-3</em></td></tr></tbody></table><p>The overall effect is that each cache instance is likely to be accessed by a single client, that
is running on the same node. This approach aims to minimize both skew (imbalanced load) and latency.</p><p>You might have other reasons to use Pod anti-affinity.
See the <a href=/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure>ZooKeeper tutorial</a>
for an example of a StatefulSet configured with anti-affinity for high
availability, using the same technique as this example.</p><h2 id=nodename>nodeName</h2><p><code>nodeName</code> is a more direct form of node selection than affinity or
<code>nodeSelector</code>. <code>nodeName</code> is a field in the Pod spec. If the <code>nodeName</code> field
is not empty, the scheduler ignores the Pod and the kubelet on the named node
tries to place the Pod on that node. Using <code>nodeName</code> overrules using
<code>nodeSelector</code> or affinity and anti-affinity rules.</p><p>Some of the limitations of using <code>nodeName</code> to select nodes are:</p><ul><li>If the named node does not exist, the Pod will not run, and in
some cases may be automatically deleted.</li><li>If the named node does not have the resources to accommodate the
Pod, the Pod will fail and its reason will indicate why,
for example OutOfmemory or OutOfcpu.</li><li>Node names in cloud environments are not always predictable or
stable.</li></ul><p>Here is an example of a Pod spec using the <code>nodeName</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeName</span>:<span style=color:#bbb> </span>kube-01<span style=color:#bbb>
</span></span></span></code></pre></div><p>The above Pod will only run on the node <code>kube-01</code>.</p><h2 id=pod-topology-spread-constraints>Pod topology spread constraints</h2><p>You can use <em>topology spread constraints</em> to control how <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>
are spread across your cluster among failure-domains such as regions, zones, nodes, or among any other
topology domains that you define. You might do this to improve performance, expected availability, or
overall utilization.</p><p>Read <a href=/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod topology spread constraints</a>
to learn more about how these work.</p><h2 id=what-s-next>What's next</h2><ul><li>Read more about <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>taints and tolerations</a> .</li><li>Read the design docs for <a href=https://git.k8s.io/design-proposals-archive/scheduling/nodeaffinity.md>node affinity</a>
and for <a href=https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md>inter-pod affinity/anti-affinity</a>.</li><li>Learn about how the <a href=/docs/tasks/administer-cluster/topology-manager/>topology manager</a> takes part in node-level
resource allocation decisions.</li><li>Learn how to use <a href=/docs/tasks/configure-pod-container/assign-pods-nodes/>nodeSelector</a>.</li><li>Learn how to use <a href=/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/>affinity and anti-affinity</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-da22fe2278df236f71efbe672f392677>11.3 - Pod Overhead</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>When you run a Pod on a Node, the Pod itself takes an amount of system resources. These
resources are additional to the resources needed to run the container(s) inside the Pod.
In Kubernetes, <em>Pod Overhead</em> is a way to account for the resources consumed by the Pod
infrastructure on top of the container requests & limits.</p><p>In Kubernetes, the Pod's overhead is set at
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks>admission</a>
time according to the overhead associated with the Pod's
<a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a>.</p><p>A pod's overhead is considered in addition to the sum of container resource requests when
scheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod cgroup,
and when carrying out Pod eviction ranking.</p><h2 id=set-up>Configuring Pod overhead</h2><p>You need to make sure a <code>RuntimeClass</code> is utilized which defines the <code>overhead</code> field.</p><h2 id=usage-example>Usage example</h2><p>To work with Pod overhead, you need a RuntimeClass that defines the <code>overhead</code> field. As
an example, you could use the following RuntimeClass definition with a virtualization container
runtime that uses around 120MiB per Pod for the virtual machine and the guest OS:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>overhead</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podFixed</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;120Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Workloads which are created which specify the <code>kata-fc</code> RuntimeClass handler will take the memory and
cpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing.</p><p>Consider running the given example workload, test-pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>stdin</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tty</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>1500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></span></span></code></pre></div><p>At admission time the RuntimeClass <a href=/docs/reference/access-authn-authz/admission-controllers/>admission controller</a>
updates the workload's PodSpec to include the <code>overhead</code> as described in the RuntimeClass. If the PodSpec already has this field defined,
the Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Pod
to include an <code>overhead</code>.</p><p>After the RuntimeClass admission controller has made modifications, you can check the updated
Pod overhead value:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.overhead}&#39;</span>
</span></span></code></pre></div><p>The output is:</p><pre tabindex=0><code>map[cpu:250m memory:120Mi]
</code></pre><p>If a <a href=/docs/concepts/policy/resource-quotas/>ResourceQuota</a> is defined, the sum of container requests as well as the
<code>overhead</code> field are counted.</p><p>When the kube-scheduler is deciding which node should run a new Pod, the scheduler considers that Pod's
<code>overhead</code> as well as the sum of container requests for that Pod. For this example, the scheduler adds the
requests and the overhead, then looks for a node that has 2.25 CPU and 320 MiB of memory available.</p><p>Once a Pod is scheduled to a node, the kubelet on that node creates a new <a class=glossary-tooltip title='A group of Linux processes with optional resource isolation, accounting and limits.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label=cgroup>cgroup</a> for the Pod. It is within this pod that the underlying
container runtime will create containers.</p><p>If the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with limits defined),
the kubelet will set an upper limit for the pod cgroup associated with that resource (cpu.cfs_quota_us for CPU
and memory.limit_in_bytes memory). This upper limit is based on the sum of the container limits plus the <code>overhead</code>
defined in the PodSpec.</p><p>For CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set <code>cpu.shares</code> based on the
sum of container requests plus the <code>overhead</code> defined in the PodSpec.</p><p>Looking at our example, verify the container requests for the workload:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.containers[*].resources.limits}&#39;</span>
</span></span></code></pre></div><p>The total container requests are 2000m CPU and 200MiB of memory:</p><pre tabindex=0><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre><p>Check this against what is observed by the node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe node | grep test-pod -B2
</span></span></code></pre></div><p>The output shows requests for 2250m CPU, and for 320MiB of memory. The requests include Pod overhead:</p><pre tabindex=0><code>  Namespace    Name       CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------    ----       ------------  ----------   ---------------  -------------  ---
  default      test-pod   2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre><h2 id=verify-pod-cgroup-limits>Verify Pod cgroup limits</h2><p>Check the Pod's memory cgroups on the node where the workload is running. In the following example,
<a href=https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md><code>crictl</code></a>
is used on the node, which provides a CLI for CRI-compatible container runtimes. This is an
advanced example to show Pod overhead behavior, and it is not expected that users should need to check
cgroups directly on the node.</p><p>First, on the particular node, determine the Pod identifier:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># Run this on the node where the Pod is scheduled</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>POD_ID</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>sudo crictl pods --name test-pod -q<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div><p>From this, you can determine the cgroup path for the Pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># Run this on the node where the Pod is scheduled</span>
</span></span><span style=display:flex><span>sudo crictl inspectp -o<span style=color:#666>=</span>json <span style=color:#b8860b>$POD_ID</span> | grep cgroupsPath
</span></span></code></pre></div><p>The resulting cgroup path includes the Pod's <code>pause</code> container. The Pod level cgroup is one directory above.</p><pre tabindex=0><code>  &#34;cgroupsPath&#34;: &#34;/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a&#34;
</code></pre><p>In this specific case, the pod cgroup path is <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>.
Verify the Pod level cgroup setting for memory:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># Run this on the node where the Pod is scheduled.</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Also, change the name of the cgroup to match the cgroup allocated for your pod.</span>
</span></span><span style=display:flex><span> cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</span></span></code></pre></div><p>This is 320 MiB, as expected:</p><pre tabindex=0><code>335544320
</code></pre><h3 id=observability>Observability</h3><p>Some <code>kube_pod_overhead_*</code> metrics are available in <a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a>
to help identify when Pod overhead is being utilized and to help observe stability of workloads
running with a defined overhead.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a></li><li>Read the <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>PodOverhead Design</a>
enhancement proposal for extra context</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6b8c85a6a88f4a81e6b79e197c293c31>11.4 - Pod Topology Spread Constraints</h1><p>You can use <em>topology spread constraints</em> to control how
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> are spread across your cluster
among failure-domains such as regions, zones, nodes, and other user-defined topology
domains. This can help to achieve high availability as well as efficient resource
utilization.</p><p>You can set <a href=#cluster-level-default-constraints>cluster-level constraints</a> as a default,
or configure topology spread constraints for individual workloads.</p><h2 id=motivation>Motivation</h2><p>Imagine that you have a cluster of up to twenty nodes, and you want to run a
<a class=glossary-tooltip title='A workload is an application running on Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/ target=_blank aria-label=workload>workload</a>
that automatically scales how many replicas it uses. There could be as few as
two Pods or as many as fifteen.
When there are only two Pods, you'd prefer not to have both of those Pods run on the
same node: you would run the risk that a single node failure takes your workload
offline.</p><p>In addition to this basic usage, there are some advanced usage examples that
enable your workloads to benefit on high availability and cluster utilization.</p><p>As you scale up and run more Pods, a different concern becomes important. Imagine
that you have three nodes running five Pods each. The nodes have enough capacity
to run that many replicas; however, the clients that interact with this workload
are split across three different datacenters (or infrastructure zones). Now you
have less concern about a single node failure, but you notice that latency is
higher than you'd like, and you are paying for network costs associated with
sending network traffic between the different zones.</p><p>You decide that under normal operation you'd prefer to have a similar number of replicas
<a href=/docs/concepts/scheduling-eviction/>scheduled</a> into each infrastructure zone,
and you'd like the cluster to self-heal in the case that there is a problem.</p><p>Pod topology spread constraints offer you a declarative way to configure that.</p><h2 id=topologyspreadconstraints-field><code>topologySpreadConstraints</code> field</h2><p>The Pod API includes a field, <code>spec.topologySpreadConstraints</code>. The usage of this field looks like
the following:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Configure a topology spread constraint</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span>&lt;integer&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>minDomains</span>:<span style=color:#bbb> </span>&lt;integer&gt;<span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional; beta since v1.25</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>&lt;string&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>&lt;string&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb> </span>&lt;object&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabelKeys</span>:<span style=color:#bbb> </span>&lt;list&gt;<span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional; alpha since v1.25</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeAffinityPolicy</span>:<span style=color:#bbb> </span>[Honor|Ignore]<span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional; alpha since v1.25</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeTaintsPolicy</span>:<span style=color:#bbb> </span>[Honor|Ignore]<span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional; alpha since v1.25</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic>### other Pod fields go here</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>You can read more about this field by running <code>kubectl explain Pod.spec.topologySpreadConstraints</code> or
refer to <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling>scheduling</a> section of the API reference for Pod.</p><h3 id=spread-constraint-definition>Spread constraint definition</h3><p>You can define one or multiple <code>topologySpreadConstraints</code> entries to instruct the
kube-scheduler how to place each incoming Pod in relation to the existing Pods across
your cluster. Those fields are:</p><ul><li><p><strong>maxSkew</strong> describes the degree to which Pods may be unevenly distributed. You must
specify this field and the number must be greater than zero. Its semantics differ
according to the value of <code>whenUnsatisfiable</code>:</p><ul><li>if you select <code>whenUnsatisfiable: DoNotSchedule</code>, then <code>maxSkew</code> defines the
maximum permitted difference between the number of matching pods in the target
topology and the <em>global minimum</em>
(the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains).
For example, if you have 3 zones with 2, 2 and 1 matching pods respectively,
<code>MaxSkew</code> is set to 1 then the global minimum is 1.</li><li>if you select <code>whenUnsatisfiable: ScheduleAnyway</code>, the scheduler gives higher
precedence to topologies that would help reduce the skew.</li></ul></li><li><p><strong>minDomains</strong> indicates a minimum number of eligible domains. This field is optional.
A domain is a particular instance of a topology. An eligible domain is a domain whose
nodes match the node selector.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>minDomains</code> field is a beta field and enabled by default in 1.25. You can disable it by disabling the
<code>MinDomainsInPodTopologySpread</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>.</div><ul><li>The value of <code>minDomains</code> must be greater than 0, when specified.
You can only specify <code>minDomains</code> in conjunction with <code>whenUnsatisfiable: DoNotSchedule</code>.</li><li>When the number of eligible domains with match topology keys is less than <code>minDomains</code>,
Pod topology spread treats global minimum as 0, and then the calculation of <code>skew</code> is performed.
The global minimum is the minimum number of matching Pods in an eligible domain,
or zero if the number of eligible domains is less than <code>minDomains</code>.</li><li>When the number of eligible domains with matching topology keys equals or is greater than
<code>minDomains</code>, this value has no effect on scheduling.</li><li>If you do not specify <code>minDomains</code>, the constraint behaves as if <code>minDomains</code> is 1.</li></ul></li><li><p><strong>topologyKey</strong> is the key of <a href=#node-labels>node labels</a>. Nodes that have a label with this key
and identical values are considered to be in the same topology.
We call each instance of a topology (in other words, a &lt;key, value> pair) a domain. The scheduler
will try to put a balanced number of pods into each domain.
Also, we define an eligible domain as a domain whose nodes meet the requirements of
nodeAffinityPolicy and nodeTaintsPolicy.</p></li><li><p><strong>whenUnsatisfiable</strong> indicates how to deal with a Pod if it doesn't satisfy the spread constraint:</p><ul><li><code>DoNotSchedule</code> (default) tells the scheduler not to schedule it.</li><li><code>ScheduleAnyway</code> tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.</li></ul></li><li><p><strong>labelSelector</strong> is used to find matching Pods. Pods
that match this label selector are counted to determine the
number of Pods in their corresponding topology domain.
See <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>Label Selectors</a>
for more details.</p></li><li><p><strong>matchLabelKeys</strong> is a list of pod label keys to select the pods over which
spreading will be calculated. The keys are used to lookup values from the pod labels, those key-value labels are ANDed with <code>labelSelector</code> to select the group of existing pods over which spreading will be calculated for the incoming pod. Keys that don't exist in the pod labels will be ignored. A null or empty list means only match against the <code>labelSelector</code>.</p><p>With <code>matchLabelKeys</code>, users don't need to update the <code>pod.spec</code> between different revisions. The controller/operator just needs to set different values to the same <code>label</code> key for different revisions. The scheduler will assume the values automatically based on <code>matchLabelKeys</code>. For example, if users use Deployment, they can use the label keyed with <code>pod-template-hash</code>, which is added automatically by the Deployment controller, to distinguish between different revisions in a single Deployment.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>kubernetes.io/hostname<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchLabelKeys</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- pod-template-hash<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>matchLabelKeys</code> field is an alpha field added in 1.25. You have to enable the
<code>MatchLabelKeysInPodTopologySpread</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
in order to use it.</div></li><li><p><strong>nodeAffinityPolicy</strong> indicates how we will treat Pod's nodeAffinity/nodeSelector
when calculating pod topology spread skew. Options are:</p><ul><li>Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations.</li><li>Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.</li></ul><p>If this value is null, the behavior is equivalent to the Honor policy.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>nodeAffinityPolicy</code> is an alpha-level field added in 1.25. You have to enable the
<code>NodeInclusionPolicyInPodTopologySpread</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
in order to use it.</div></li><li><p><strong>nodeTaintsPolicy</strong> indicates how we will treat node taints when calculating
pod topology spread skew. Options are:</p><ul><li>Honor: nodes without taints, along with tainted nodes for which the incoming pod
has a toleration, are included.</li><li>Ignore: node taints are ignored. All nodes are included.</li></ul><p>If this value is null, the behavior is equivalent to the Ignore policy.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>nodeTaintsPolicy</code> is an alpha-level field added in 1.25. You have to enable the
<code>NodeInclusionPolicyInPodTopologySpread</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
in order to use it.</div></li></ul><p>When a Pod defines more than one <code>topologySpreadConstraint</code>, those constraints are
combined using a logical AND operation: the kube-scheduler looks for a node for the incoming Pod
that satisfies all the configured constraints.</p><h3 id=node-labels>Node labels</h3><p>Topology spread constraints rely on node labels to identify the topology
domain(s) that each <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> is in.
For example, a node might have labels:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>region</span>:<span style=color:#bbb> </span>us-east-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>zone</span>:<span style=color:#bbb> </span>us-east-1a<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>For brevity, this example doesn't use the
<a href=/docs/reference/labels-annotations-taints/>well-known</a> label keys
<code>topology.kubernetes.io/zone</code> and <code>topology.kubernetes.io/region</code>. However,
those registered label keys are nonetheless recommended rather than the private
(unqualified) label keys <code>region</code> and <code>zone</code> that are used here.</p><p>You can't make a reliable assumption about the meaning of a private label key
between different contexts.</p></div><p>Suppose you have a 4-node cluster with the following labels:</p><pre tabindex=0><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre><p>Then the cluster is logically viewed as below:</p><figure><div class=mermaid>graph TB
subgraph "zoneB"
n3(Node3)
n4(Node4)
end
subgraph "zoneA"
n1(Node1)
n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><h2 id=consistency>Consistency</h2><p>You should set the same Pod topology spread constraints on all pods in a group.</p><p>Usually, if you are using a workload controller such as a Deployment, the pod template
takes care of this for you. If you mix different spread constraints then Kubernetes
follows the API definition of the field; however, the behavior is more likely to become
confusing and troubleshooting is less straightforward.</p><p>You need a mechanism to ensure that all the nodes in a topology domain (such as a
cloud provider region) are labelled consistently.
To avoid you needing to manually label nodes, most clusters automatically
populate well-known labels such as <code>topology.kubernetes.io/hostname</code>. Check whether
your cluster supports this.</p><h2 id=topology-spread-constraint-examples>Topology spread constraint examples</h2><h3 id=example-one-topologyspreadconstraint>Example: one topology spread constraint</h3><p>Suppose you have a 4-node cluster where 3 Pods labelled <code>foo: bar</code> are located in
node1, node2 and node3 respectively:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>If you want an incoming Pod to be evenly spread with existing Pods across zones, you
can use a manifest similar to:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint.yaml download=pods/topology-spread-constraints/one-constraint.yaml><code>pods/topology-spread-constraints/one-constraint.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-one-constraint-yaml")' title="Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-one-constraint-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>From that manifest, <code>topologyKey: zone</code> implies the even distribution will only be applied
to nodes that are labelled <code>zone: &lt;any value></code> (nodes that don't have a <code>zone</code> label
are skipped). The field <code>whenUnsatisfiable: DoNotSchedule</code> tells the scheduler to let the
incoming Pod stay pending if the scheduler can't find a way to satisfy the constraint.</p><p>If the scheduler placed this incoming Pod into zone <code>A</code>, the distribution of Pods would
become <code>[3, 1]</code>. That means the actual skew is then 2 (calculated as <code>3 - 1</code>), which
violates <code>maxSkew: 1</code>. To satisfy the constraints and context for this example, the
incoming Pod can only be placed onto a node in zone <code>B</code>:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
p4(mypod) --> n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>OR</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
p4(mypod) --> n3
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>You can tweak the Pod spec to meet various kinds of requirements:</p><ul><li>Change <code>maxSkew</code> to a bigger value - such as <code>2</code> - so that the incoming Pod can
be placed into zone <code>A</code> as well.</li><li>Change <code>topologyKey</code> to <code>node</code> so as to distribute the Pods evenly across nodes
instead of zones. In the above example, if <code>maxSkew</code> remains <code>1</code>, the incoming
Pod can only be placed onto the node <code>node4</code>.</li><li>Change <code>whenUnsatisfiable: DoNotSchedule</code> to <code>whenUnsatisfiable: ScheduleAnyway</code>
to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs
are satisfied). However, it's preferred to be placed into the topology domain which
has fewer matching Pods. (Be aware that this preference is jointly normalized
with other internal scheduling priorities such as resource usage ratio).</li></ul><h3 id=example-multiple-topologyspreadconstraints>Example: multiple topology spread constraints</h3><p>This builds upon the previous example. Suppose you have a 4-node cluster where 3
existing Pods labeled <code>foo: bar</code> are located on node1, node2 and node3 respectively:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>You can combine two topology spread constraints to control the spread of Pods both
by node and by zone:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml download=pods/topology-spread-constraints/two-constraints.yaml><code>pods/topology-spread-constraints/two-constraints.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-two-constraints-yaml")' title="Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-two-constraints-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>node<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>In this case, to match the first constraint, the incoming Pod can only be placed onto
nodes in zone <code>B</code>; while in terms of the second constraint, the incoming Pod can only be
scheduled to the node <code>node4</code>. The scheduler only considers options that satisfy all
defined constraints, so the only valid placement is onto node <code>node4</code>.</p><h3 id=example-conflicting-topologyspreadconstraints>Example: conflicting topology spread constraints</h3><p>Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p4(Pod) --> n3(Node3)
p5(Pod) --> n3
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n1
p3(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>If you were to apply
<a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml><code>two-constraints.yaml</code></a>
(the manifest from the previous example)
to <strong>this</strong> cluster, you would see that the Pod <code>mypod</code> stays in the <code>Pending</code> state.
This happens because: to satisfy the first constraint, the Pod <code>mypod</code> can only
be placed into zone <code>B</code>; while in terms of the second constraint, the Pod <code>mypod</code>
can only schedule to node <code>node2</code>. The intersection of the two constraints returns
an empty set, and the scheduler cannot place the Pod.</p><p>To overcome this situation, you can either increase the value of <code>maxSkew</code> or modify
one of the constraints to use <code>whenUnsatisfiable: ScheduleAnyway</code>. Depending on
circumstances, you might also decide to delete an existing Pod manually - for example,
if you are troubleshooting why a bug-fix rollout is not making progress.</p><h4 id=interaction-with-node-affinity-and-node-selectors>Interaction with node affinity and node selectors</h4><p>The scheduler will skip the non-matching nodes from the skew calculations if the
incoming Pod has <code>spec.nodeSelector</code> or <code>spec.affinity.nodeAffinity</code> defined.</p><h3 id=example-topologyspreadconstraints-with-nodeaffinity>Example: topology spread constraints with node affinity</h3><p>Suppose you have a 5-node cluster ranging across zones A to C:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><figure><div class=mermaid>graph BT
subgraph "zoneC"
n5(Node5)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n5 k8s;
class zoneC cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>and you know that zone <code>C</code> must be excluded. In this case, you can compose a manifest
as below, so that Pod <code>mypod</code> will be placed into zone <code>B</code> instead of zone <code>C</code>.
Similarly, Kubernetes also respects <code>spec.nodeSelector</code>.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml download=pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml")' title="Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>NotIn<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- zoneC<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><h2 id=implicit-conventions>Implicit conventions</h2><p>There are some implicit conventions worth noting here:</p><ul><li><p>Only the Pods holding the same namespace as the incoming Pod can be matching candidates.</p></li><li><p>The scheduler bypasses any nodes that don't have any <code>topologySpreadConstraints[*].topologyKey</code>
present. This implies that:</p><ol><li>any Pods located on those bypassed nodes do not impact <code>maxSkew</code> calculation - in the
above example, suppose the node <code>node1</code> does not have a label "zone", then the 2 Pods will
be disregarded, hence the incoming Pod will be scheduled into zone <code>A</code>.</li><li>the incoming Pod has no chances to be scheduled onto this kind of nodes -
in the above example, suppose a node <code>node5</code> has the <strong>mistyped</strong> label <code>zone-typo: zoneC</code>
(and no <code>zone</code> label set). After node <code>node5</code> joins the cluster, it will be bypassed and
Pods for this workload aren't scheduled there.</li></ol></li><li><p>Be aware of what will happen if the incoming Pod's
<code>topologySpreadConstraints[*].labelSelector</code> doesn't match its own labels. In the
above example, if you remove the incoming Pod's labels, it can still be placed onto
nodes in zone <code>B</code>, since the constraints are still satisfied. However, after that
placement, the degree of imbalance of the cluster remains unchanged - it's still zone <code>A</code>
having 2 Pods labelled as <code>foo: bar</code>, and zone <code>B</code> having 1 Pod labelled as
<code>foo: bar</code>. If this is not what you expect, update the workload's
<code>topologySpreadConstraints[*].labelSelector</code> to match the labels in the pod template.</p></li></ul><h2 id=cluster-level-default-constraints>Cluster-level default constraints</h2><p>It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:</p><ul><li>It doesn't define any constraints in its <code>.spec.topologySpreadConstraints</code>.</li><li>It belongs to a Service, ReplicaSet, StatefulSet or ReplicationController.</li></ul><p>Default constraints can be set as part of the <code>PodTopologySpread</code> plugin
arguments in a <a href=/docs/reference/scheduling/config/#profiles>scheduling profile</a>.
The constraints are specified with the same <a href=#topologyspreadconstraints-field>API above</a>, except that
<code>labelSelector</code> must be empty. The selectors are calculated from the Services,
ReplicaSets, StatefulSets or ReplicationControllers that the Pod belongs to.</p><p>An example configuration might look like follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PodTopologySpread<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultingType</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <a href=/docs/reference/scheduling/config/#scheduling-plugins><code>SelectorSpread</code> plugin</a>
is disabled by default. The Kubernetes project recommends using <code>PodTopologySpread</code>
to achieve similar behavior.</div><h3 id=internal-default-constraints>Built-in default constraints</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>If you don't configure any cluster-level default constraints for pod topology spreading,
then kube-scheduler acts as if you specified the following default topology constraints:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;topology.kubernetes.io/zone&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span></code></pre></div><p>Also, the legacy <code>SelectorSpread</code> plugin, which provides an equivalent behavior,
is disabled by default.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>The <code>PodTopologySpread</code> plugin does not score the nodes that don't have
the topology keys specified in the spreading constraints. This might result
in a different default behavior compared to the legacy <code>SelectorSpread</code> plugin when
using the default topology constraints.</p><p>If your nodes are not expected to have <strong>both</strong> <code>kubernetes.io/hostname</code> and
<code>topology.kubernetes.io/zone</code> labels set, define your own constraints
instead of using the Kubernetes defaults.</p></div><p>If you don't want to use the default Pod spreading constraints for your cluster,
you can disable those defaults by setting <code>defaultingType</code> to <code>List</code> and leaving
empty <code>defaultConstraints</code> in the <code>PodTopologySpread</code> plugin configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PodTopologySpread<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb> </span>[]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultingType</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=comparison-with-podaffinity-podantiaffinity>Comparison with podAffinity and podAntiAffinity</h2><p>In Kubernetes, <a href=/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>inter-Pod affinity and anti-affinity</a>
control how Pods are scheduled in relation to one another - either more packed
or more scattered.</p><dl><dt><code>podAffinity</code></dt><dd>attracts Pods; you can try to pack any number of Pods into qualifying
topology domain(s).</dd><dt><code>podAntiAffinity</code></dt><dd>repels Pods. If you set this to <code>requiredDuringSchedulingIgnoredDuringExecution</code> mode then
only a single Pod can be scheduled into a single topology domain; if you choose
<code>preferredDuringSchedulingIgnoredDuringExecution</code> then you lose the ability to enforce the
constraint.</dd></dl><p>For finer control, you can specify topology spread constraints to distribute
Pods across different topology domains - to achieve either high availability or
cost-saving. This can also help on rolling update workloads and scaling out
replicas smoothly.</p><p>For more context, see the
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation>Motivation</a>
section of the enhancement proposal about Pod topology spread constraints.</p><h2 id=known-limitations>Known limitations</h2><ul><li><p>There's no guarantee that the constraints remain satisfied when Pods are removed. For
example, scaling down a Deployment may result in imbalanced Pods distribution.</p><p>You can use a tool such as the <a href=https://github.com/kubernetes-sigs/descheduler>Descheduler</a>
to rebalance the Pods distribution.</p></li><li><p>Pods matched on tainted nodes are respected.
See <a href=https://github.com/kubernetes/kubernetes/issues/80921>Issue 80921</a>.</p></li><li><p>The scheduler doesn't have prior knowledge of all the zones or other topology
domains that a cluster has. They are determined from the existing nodes in the
cluster. This could lead to a problem in autoscaled clusters, when a node pool (or
node group) is scaled to zero nodes, and you're expecting the cluster to scale up,
because, in this case, those topology domains won't be considered until there is
at least one node in them.</p><p>You can work around this by using an cluster autoscaling tool that is aware of
Pod topology spread constraints and is also aware of the overall set of topology
domains.</p></li></ul><h2 id=what-s-next>What's next</h2><ul><li>The blog article <a href=/blog/2020/05/introducing-podtopologyspread/>Introducing PodTopologySpread</a>
explains <code>maxSkew</code> in some detail, as well as covering some advanced usage examples.</li><li>Read the <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling>scheduling</a> section of
the API reference for Pod.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ede4960b56a3529ee0bfe7c8fe2d09a5>11.5 - Taints and Tolerations</h1><p><a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity><em>Node affinity</em></a>
is a property of <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> that <em>attracts</em> them to
a set of <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodes>nodes</a> (either as a preference or a
hard requirement). <em>Taints</em> are the opposite -- they allow a node to repel a set of pods.</p><p><em>Tolerations</em> are applied to pods. Tolerations allow the scheduler to schedule pods with matching
taints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also
<a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>evaluates other parameters</a>
as part of its function.</p><p>Taints and tolerations work together to ensure that pods are not scheduled
onto inappropriate nodes. One or more taints are applied to a node; this
marks that the node should not accept any pods that do not tolerate the taints.</p><h2 id=concepts>Concepts</h2><p>You add a taint to a node using <a href=/docs/reference/generated/kubectl/kubectl-commands#taint>kubectl taint</a>.
For example,</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</span></span></code></pre></div><p>places a taint on node <code>node1</code>. The taint has key <code>key1</code>, value <code>value1</code>, and taint effect <code>NoSchedule</code>.
This means that no pod will be able to schedule onto <code>node1</code> unless it has a matching toleration.</p><p>To remove the taint added by the command above, you can run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule-
</span></span></code></pre></div><p>You specify a toleration for a pod in the PodSpec. Both of the following tolerations "match" the
taint created by the <code>kubectl taint</code> line above, and thus a pod with either toleration would be able
to schedule onto <code>node1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Here's an example of a pod that uses tolerations:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-toleration.yaml download=pods/pod-with-toleration.yaml><code>pods/pod-with-toleration.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-toleration-yaml")' title="Copy pods/pod-with-toleration.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-toleration-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example-key&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>The default value for <code>operator</code> is <code>Equal</code>.</p><p>A toleration "matches" a taint if the keys are the same and the effects are the same, and:</p><ul><li>the <code>operator</code> is <code>Exists</code> (in which case no <code>value</code> should be specified), or</li><li>the <code>operator</code> is <code>Equal</code> and the <code>value</code>s are equal.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>There are two special cases:</p><p>An empty <code>key</code> with operator <code>Exists</code> matches all keys, values and effects which means this
will tolerate everything.</p><p>An empty <code>effect</code> matches all effects with key <code>key1</code>.</p></div><p>The above example used <code>effect</code> of <code>NoSchedule</code>. Alternatively, you can use <code>effect</code> of <code>PreferNoSchedule</code>.
This is a "preference" or "soft" version of <code>NoSchedule</code> -- the system will <em>try</em> to avoid placing a
pod that does not tolerate the taint on the node, but it is not required. The third kind of <code>effect</code> is
<code>NoExecute</code>, described later.</p><p>You can put multiple taints on the same node and multiple tolerations on the same pod.
The way Kubernetes processes multiple taints and tolerations is like a filter: start
with all of a node's taints, then ignore the ones for which the pod has a matching toleration; the
remaining un-ignored taints have the indicated effects on the pod. In particular,</p><ul><li>if there is at least one un-ignored taint with effect <code>NoSchedule</code> then Kubernetes will not schedule
the pod onto that node</li><li>if there is no un-ignored taint with effect <code>NoSchedule</code> but there is at least one un-ignored taint with
effect <code>PreferNoSchedule</code> then Kubernetes will <em>try</em> to not schedule the pod onto the node</li><li>if there is at least one un-ignored taint with effect <code>NoExecute</code> then the pod will be evicted from
the node (if it is already running on the node), and will not be
scheduled onto the node (if it is not yet running on the node).</li></ul><p>For example, imagine you taint a node like this</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</span></span><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoExecute
</span></span><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key2</span><span style=color:#666>=</span>value2:NoSchedule
</span></span></code></pre></div><p>And a pod has two tolerations:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>In this case, the pod will not be able to schedule onto the node, because there is no
toleration matching the third taint. But it will be able to continue running if it is
already running on the node when the taint is added, because the third taint is the only
one of the three that is not tolerated by the pod.</p><p>Normally, if a taint with effect <code>NoExecute</code> is added to a node, then any pods that do
not tolerate the taint will be evicted immediately, and pods that do tolerate the
taint will never be evicted. However, a toleration with <code>NoExecute</code> effect can specify
an optional <code>tolerationSeconds</code> field that dictates how long the pod will stay bound
to the node after the taint is added. For example,</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>3600</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>means that if this pod is running and a matching taint is added to the node, then
the pod will stay bound to the node for 3600 seconds, and then be evicted. If the
taint is removed before that time, the pod will not be evicted.</p><h2 id=example-use-cases>Example Use Cases</h2><p>Taints and tolerations are a flexible way to steer pods <em>away</em> from nodes or evict
pods that shouldn't be running. A few of the use cases are</p><ul><li><p><strong>Dedicated Nodes</strong>: If you want to dedicate a set of nodes for exclusive use by
a particular set of users, you can add a taint to those nodes (say,
<code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>) and then add a corresponding
toleration to their pods (this would be done most easily by writing a custom
<a href=/docs/reference/access-authn-authz/admission-controllers/>admission controller</a>).
The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as
well as any other nodes in the cluster. If you want to dedicate the nodes to them <em>and</em>
ensure they <em>only</em> use the dedicated nodes, then you should additionally add a label similar
to the taint to the same set of nodes (e.g. <code>dedicated=groupName</code>), and the admission
controller should additionally add a node affinity to require that the pods can only schedule
onto nodes labeled with <code>dedicated=groupName</code>.</p></li><li><p><strong>Nodes with Special Hardware</strong>: In a cluster where a small subset of nodes have specialized
hardware (for example GPUs), it is desirable to keep pods that don't need the specialized
hardware off of those nodes, thus leaving room for later-arriving pods that do need the
specialized hardware. This can be done by tainting the nodes that have the specialized
hardware (e.g. <code>kubectl taint nodes nodename special=true:NoSchedule</code> or
<code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>) and adding a corresponding
toleration to pods that use the special hardware. As in the dedicated nodes use case,
it is probably easiest to apply the tolerations using a custom
<a href=/docs/reference/access-authn-authz/admission-controllers/>admission controller</a>.
For example, it is recommended to use <a href=/docs/concepts/configuration/manage-resources-containers/#extended-resources>Extended
Resources</a>
to represent the special hardware, taint your special hardware nodes with the
extended resource name and run the
<a href=/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration>ExtendedResourceToleration</a>
admission controller. Now, because the nodes are tainted, no pods without the
toleration will schedule on them. But when you submit a pod that requests the
extended resource, the <code>ExtendedResourceToleration</code> admission controller will
automatically add the correct toleration to the pod and that pod will schedule
on the special hardware nodes. This will make sure that these special hardware
nodes are dedicated for pods requesting such hardware and you don't have to
manually add tolerations to your pods.</p></li><li><p><strong>Taint based Evictions</strong>: A per-pod-configurable eviction behavior
when there are node problems, which is described in the next section.</p></li></ul><h2 id=taint-based-evictions>Taint based Evictions</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code></div><p>The <code>NoExecute</code> taint effect, mentioned above, affects pods that are already
running on the node as follows</p><ul><li>pods that do not tolerate the taint are evicted immediately</li><li>pods that tolerate the taint without specifying <code>tolerationSeconds</code> in
their toleration specification remain bound forever</li><li>pods that tolerate the taint with a specified <code>tolerationSeconds</code> remain
bound for the specified amount of time</li></ul><p>The node controller automatically taints a Node when certain conditions
are true. The following taints are built in:</p><ul><li><code>node.kubernetes.io/not-ready</code>: Node is not ready. This corresponds to
the NodeCondition <code>Ready</code> being "<code>False</code>".</li><li><code>node.kubernetes.io/unreachable</code>: Node is unreachable from the node
controller. This corresponds to the NodeCondition <code>Ready</code> being "<code>Unknown</code>".</li><li><code>node.kubernetes.io/memory-pressure</code>: Node has memory pressure.</li><li><code>node.kubernetes.io/disk-pressure</code>: Node has disk pressure.</li><li><code>node.kubernetes.io/pid-pressure</code>: Node has PID pressure.</li><li><code>node.kubernetes.io/network-unavailable</code>: Node's network is unavailable.</li><li><code>node.kubernetes.io/unschedulable</code>: Node is unschedulable.</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>: When the kubelet is started
with "external" cloud provider, this taint is set on a node to mark it
as unusable. After a controller from the cloud-controller-manager initializes
this node, the kubelet removes this taint.</li></ul><p>In case a node is to be evicted, the node controller or the kubelet adds relevant taints
with <code>NoExecute</code> effect. If the fault condition returns to normal the kubelet or node
controller can remove the relevant taint(s).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The control plane limits the rate of adding node new taints to nodes. This rate limiting
manages the number of evictions that are triggered when many nodes become unreachable at
once (for example: if there is a network disruption).</div><p>You can specify <code>tolerationSeconds</code> for a Pod to define how long that Pod stays bound
to a failing or unresponsive Node.</p><p>For example, you might want to keep an application with a lot of local state
bound to node for a long time in the event of network partition, hoping
that the partition will recover and thus the pod eviction can be avoided.
The toleration you set for that Pod might look like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;node.kubernetes.io/unreachable&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>6000</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Kubernetes automatically adds a toleration for
<code>node.kubernetes.io/not-ready</code> and <code>node.kubernetes.io/unreachable</code>
with <code>tolerationSeconds=300</code>,
unless you, or a controller, set those tolerations explicitly.</p><p>These automatically-added tolerations mean that Pods remain bound to
Nodes for 5 minutes after one of these problems is detected.</p></div><p><a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> pods are created with
<code>NoExecute</code> tolerations for the following taints with no <code>tolerationSeconds</code>:</p><ul><li><code>node.kubernetes.io/unreachable</code></li><li><code>node.kubernetes.io/not-ready</code></li></ul><p>This ensures that DaemonSet pods are never evicted due to these problems.</p><h2 id=taint-nodes-by-condition>Taint Nodes by Condition</h2><p>The control plane, using the node <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>,
automatically creates taints with a <code>NoSchedule</code> effect for
<a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions>node conditions</a>.</p><p>The scheduler checks taints, not node conditions, when it makes scheduling
decisions. This ensures that node conditions don't directly affect scheduling.
For example, if the <code>DiskPressure</code> node condition is active, the control plane
adds the <code>node.kubernetes.io/disk-pressure</code> taint and does not schedule new pods
onto the affected node. If the <code>MemoryPressure</code> node condition is active, the
control plane adds the <code>node.kubernetes.io/memory-pressure</code> taint.</p><p>You can ignore node conditions for newly created pods by adding the corresponding
Pod tolerations. The control plane also adds the <code>node.kubernetes.io/memory-pressure</code>
toleration on pods that have a <a class=glossary-tooltip title='QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-qos-class' target=_blank aria-label='QoS class'>QoS class</a>
other than <code>BestEffort</code>. This is because Kubernetes treats pods in the <code>Guaranteed</code>
or <code>Burstable</code> QoS classes (even pods with no memory request set) as if they are
able to cope with memory pressure, while new <code>BestEffort</code> pods are not scheduled
onto the affected node.</p><p>The DaemonSet controller automatically adds the following <code>NoSchedule</code>
tolerations to all daemons, to prevent DaemonSets from breaking.</p><ul><li><code>node.kubernetes.io/memory-pressure</code></li><li><code>node.kubernetes.io/disk-pressure</code></li><li><code>node.kubernetes.io/pid-pressure</code> (1.14 or later)</li><li><code>node.kubernetes.io/unschedulable</code> (1.10 or later)</li><li><code>node.kubernetes.io/network-unavailable</code> (<em>host network only</em>)</li></ul><p>Adding these tolerations ensures backward compatibility. You can also add
arbitrary tolerations to DaemonSets.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a>
and how you can configure it</li><li>Read about <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-602208c95fe7b1f1170310ce993f5814>11.6 - Scheduling Framework</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code></div><p>The scheduling framework is a pluggable architecture for the Kubernetes scheduler.
It adds a new set of "plugin" APIs to the existing scheduler. Plugins are compiled into the scheduler. The APIs allow most scheduling features to be implemented as plugins, while keeping the
scheduling "core" lightweight and maintainable. Refer to the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md>design proposal of the
scheduling framework</a> for more technical information on the design of the
framework.</p><h1 id=framework-workflow>Framework workflow</h1><p>The Scheduling Framework defines a few extension points. Scheduler plugins
register to be invoked at one or more extension points. Some of these plugins
can change the scheduling decisions and some are informational only.</p><p>Each attempt to schedule one Pod is split into two phases, the <strong>scheduling
cycle</strong> and the <strong>binding cycle</strong>.</p><h2 id=scheduling-cycle-binding-cycle>Scheduling Cycle & Binding Cycle</h2><p>The scheduling cycle selects a node for the Pod, and the binding cycle applies
that decision to the cluster. Together, a scheduling cycle and binding cycle are
referred to as a "scheduling context".</p><p>Scheduling cycles are run serially, while binding cycles may run concurrently.</p><p>A scheduling or binding cycle can be aborted if the Pod is determined to
be unschedulable or if there is an internal error. The Pod will be returned to
the queue and retried.</p><h2 id=extension-points>Extension points</h2><p>The following picture shows the scheduling context of a Pod and the extension
points that the scheduling framework exposes. In this picture "Filter" is
equivalent to "Predicate" and "Scoring" is equivalent to "Priority function".</p><p>One plugin may register at multiple extension points to perform more complex or
stateful tasks.</p><figure class=diagram-large><img src=/images/docs/scheduling-framework-extensions.png><figcaption><h4>scheduling framework extension points</h4></figcaption></figure><h3 id=queue-sort>QueueSort</h3><p>These plugins are used to sort Pods in the scheduling queue. A queue sort plugin
essentially provides a <code>Less(Pod1, Pod2)</code> function. Only one queue sort
plugin may be enabled at a time.</p><h3 id=pre-filter>PreFilter</h3><p>These plugins are used to pre-process info about the Pod, or to check certain
conditions that the cluster or the Pod must meet. If a PreFilter plugin returns
an error, the scheduling cycle is aborted.</p><h3 id=filter>Filter</h3><p>These plugins are used to filter out nodes that cannot run the Pod. For each
node, the scheduler will call filter plugins in their configured order. If any
filter plugin marks the node as infeasible, the remaining plugins will not be
called for that node. Nodes may be evaluated concurrently.</p><h3 id=post-filter>PostFilter</h3><p>These plugins are called after Filter phase, but only when no feasible nodes
were found for the pod. Plugins are called in their configured order. If
any postFilter plugin marks the node as <code>Schedulable</code>, the remaining plugins
will not be called. A typical PostFilter implementation is preemption, which
tries to make the pod schedulable by preempting other Pods.</p><h3 id=pre-score>PreScore</h3><p>These plugins are used to perform "pre-scoring" work, which generates a sharable
state for Score plugins to use. If a PreScore plugin returns an error, the
scheduling cycle is aborted.</p><h3 id=scoring>Score</h3><p>These plugins are used to rank nodes that have passed the filtering phase. The
scheduler will call each scoring plugin for each node. There will be a well
defined range of integers representing the minimum and maximum scores. After the
<a href=#normalize-scoring>NormalizeScore</a> phase, the scheduler will combine node
scores from all plugins according to the configured plugin weights.</p><h3 id=normalize-scoring>NormalizeScore</h3><p>These plugins are used to modify scores before the scheduler computes a final
ranking of Nodes. A plugin that registers for this extension point will be
called with the <a href=#scoring>Score</a> results from the same plugin. This is called
once per plugin per scheduling cycle.</p><p>For example, suppose a plugin <code>BlinkingLightScorer</code> ranks Nodes based on how
many blinking lights they have.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>ScoreNode</span>(_ <span style=color:#666>*</span>v1.pod, n <span style=color:#666>*</span>v1.Node) (<span style=color:#0b0;font-weight:700>int</span>, <span style=color:#0b0;font-weight:700>error</span>) {
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> <span style=color:#00a000>getBlinkingLightCount</span>(n)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>However, the maximum count of blinking lights may be small compared to
<code>NodeScoreMax</code>. To fix this, <code>BlinkingLightScorer</code> should also register for this
extension point.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>NormalizeScores</span>(scores <span style=color:#a2f;font-weight:700>map</span>[<span style=color:#0b0;font-weight:700>string</span>]<span style=color:#0b0;font-weight:700>int</span>) {
</span></span><span style=display:flex><span>    highest <span style=color:#666>:=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> _, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
</span></span><span style=display:flex><span>        highest = <span style=color:#00a000>max</span>(highest, score)
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> node, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
</span></span><span style=display:flex><span>        scores[node] = score<span style=color:#666>*</span>NodeScoreMax<span style=color:#666>/</span>highest
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If any NormalizeScore plugin returns an error, the scheduling cycle is
aborted.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Plugins wishing to perform "pre-reserve" work should use the
NormalizeScore extension point.</div><h3 id=reserve>Reserve</h3><p>A plugin that implements the Reserve extension has two methods, namely <code>Reserve</code>
and <code>Unreserve</code>, that back two informational scheduling phases called Reserve
and Unreserve, respectively. Plugins which maintain runtime state (aka "stateful
plugins") should use these phases to be notified by the scheduler when resources
on a node are being reserved and unreserved for a given Pod.</p><p>The Reserve phase happens before the scheduler actually binds a Pod to its
designated node. It exists to prevent race conditions while the scheduler waits
for the bind to succeed. The <code>Reserve</code> method of each Reserve plugin may succeed
or fail; if one <code>Reserve</code> method call fails, subsequent plugins are not executed
and the Reserve phase is considered to have failed. If the <code>Reserve</code> method of
all plugins succeed, the Reserve phase is considered to be successful and the
rest of the scheduling cycle and the binding cycle are executed.</p><p>The Unreserve phase is triggered if the Reserve phase or a later phase fails.
When this happens, the <code>Unreserve</code> method of <strong>all</strong> Reserve plugins will be
executed in the reverse order of <code>Reserve</code> method calls. This phase exists to
clean up the state associated with the reserved Pod.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> The implementation of the <code>Unreserve</code> method in Reserve plugins must be
idempotent and may not fail.</div><h3 id=permit>Permit</h3><p><em>Permit</em> plugins are invoked at the end of the scheduling cycle for each Pod, to
prevent or delay the binding to the candidate node. A permit plugin can do one of
the three things:</p><ol><li><p><strong>approve</strong><br>Once all Permit plugins approve a Pod, it is sent for binding.</p></li><li><p><strong>deny</strong><br>If any Permit plugin denies a Pod, it is returned to the scheduling queue.
This will trigger the Unreserve phase in <a href=#reserve>Reserve plugins</a>.</p></li><li><p><strong>wait</strong> (with a timeout)<br>If a Permit plugin returns "wait", then the Pod is kept in an internal "waiting"
Pods list, and the binding cycle of this Pod starts but directly blocks until it
gets approved. If a timeout occurs, <strong>wait</strong> becomes <strong>deny</strong>
and the Pod is returned to the scheduling queue, triggering the
Unreserve phase in <a href=#reserve>Reserve plugins</a>.</p></li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong> While any plugin can access the list of "waiting" Pods and approve them
(see <a href=https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle><code>FrameworkHandle</code></a>), we expect only the permit
plugins to approve binding of reserved Pods that are in "waiting" state. Once a Pod
is approved, it is sent to the <a href=#pre-bind>PreBind</a> phase.</div><h3 id=pre-bind>PreBind</h3><p>These plugins are used to perform any work required before a Pod is bound. For
example, a pre-bind plugin may provision a network volume and mount it on the
target node before allowing the Pod to run there.</p><p>If any PreBind plugin returns an error, the Pod is <a href=#reserve>rejected</a> and
returned to the scheduling queue.</p><h3 id=bind>Bind</h3><p>These plugins are used to bind a Pod to a Node. Bind plugins will not be called
until all PreBind plugins have completed. Each bind plugin is called in the
configured order. A bind plugin may choose whether or not to handle the given
Pod. If a bind plugin chooses to handle a Pod, <strong>the remaining bind plugins are
skipped</strong>.</p><h3 id=post-bind>PostBind</h3><p>This is an informational extension point. Post-bind plugins are called after a
Pod is successfully bound. This is the end of a binding cycle, and can be used
to clean up associated resources.</p><h2 id=plugin-api>Plugin API</h2><p>There are two steps to the plugin API. First, plugins must register and get
configured, then they use the extension point interfaces. Extension point
interfaces have the following form.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>type</span> Plugin <span style=color:#a2f;font-weight:700>interface</span> {
</span></span><span style=display:flex><span>    <span style=color:#00a000>Name</span>() <span style=color:#0b0;font-weight:700>string</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>type</span> QueueSortPlugin <span style=color:#a2f;font-weight:700>interface</span> {
</span></span><span style=display:flex><span>    Plugin
</span></span><span style=display:flex><span>    <span style=color:#00a000>Less</span>(<span style=color:#666>*</span>v1.pod, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>bool</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>type</span> PreFilterPlugin <span style=color:#a2f;font-weight:700>interface</span> {
</span></span><span style=display:flex><span>    Plugin
</span></span><span style=display:flex><span>    <span style=color:#00a000>PreFilter</span>(context.Context, <span style=color:#666>*</span>framework.CycleState, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>error</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>// ...
</span></span></span></code></pre></div><h2 id=plugin-configuration>Plugin configuration</h2><p>You can enable or disable plugins in the scheduler configuration. If you are using
Kubernetes v1.18 or later, most scheduling
<a href=/docs/reference/scheduling/config/#scheduling-plugins>plugins</a> are in use and
enabled by default.</p><p>In addition to default plugins, you can also implement your own scheduling
plugins and get them configured along with default plugins. You can visit
<a href=https://github.com/kubernetes-sigs/scheduler-plugins>scheduler-plugins</a> for more details.</p><p>If you are using Kubernetes v1.18 or later, you can configure a set of plugins as
a scheduler profile and then define multiple profiles to fit various kinds of workload.
Learn more at <a href=/docs/reference/scheduling/config/#multiple-profiles>multiple profiles</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d9574a30fcbc631b0d2a57850e161e89>11.7 - Scheduler Performance Tuning</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [beta]</code></div><p><a href=/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler>kube-scheduler</a>
is the Kubernetes default scheduler. It is responsible for placement of Pods
on Nodes in a cluster.</p><p>Nodes in a cluster that meet the scheduling requirements of a Pod are
called <em>feasible</em> Nodes for the Pod. The scheduler finds feasible Nodes
for a Pod and then runs a set of functions to score the feasible Nodes,
picking a Node with the highest score among the feasible ones to run
the Pod. The scheduler then notifies the API server about this decision
in a process called <em>Binding</em>.</p><p>This page explains performance tuning optimizations that are relevant for
large Kubernetes clusters.</p><p>In large clusters, you can tune the scheduler's behaviour balancing
scheduling outcomes between latency (new Pods are placed quickly) and
accuracy (the scheduler rarely makes poor placement decisions).</p><p>You configure this tuning setting via kube-scheduler setting
<code>percentageOfNodesToScore</code>. This KubeSchedulerConfiguration setting determines
a threshold for scheduling nodes in your cluster.</p><h3 id=setting-the-threshold>Setting the threshold</h3><p>The <code>percentageOfNodesToScore</code> option accepts whole numeric values between 0
and 100. The value 0 is a special number which indicates that the kube-scheduler
should use its compiled-in default.
If you set <code>percentageOfNodesToScore</code> above 100, kube-scheduler acts as if you
had set a value of 100.</p><p>To change the value, edit the
<a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler configuration file</a>
and then restart the scheduler.
In many cases, the configuration file can be found at <code>/etc/kubernetes/config/kube-scheduler.yaml</code>.</p><p>After you have made this change, you can run</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -n kube-system | grep kube-scheduler
</span></span></code></pre></div><p>to verify that the kube-scheduler component is healthy.</p><h2 id=percentage-of-nodes-to-score>Node scoring threshold</h2><p>To improve scheduling performance, the kube-scheduler can stop looking for
feasible nodes once it has found enough of them. In large clusters, this saves
time compared to a naive approach that would consider every node.</p><p>You specify a threshold for how many nodes are enough, as a whole number percentage
of all the nodes in your cluster. The kube-scheduler converts this into an
integer number of nodes. During scheduling, if the kube-scheduler has identified
enough feasible nodes to exceed the configured percentage, the kube-scheduler
stops searching for more feasible nodes and moves on to the
<a href=/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation>scoring phase</a>.</p><p><a href=#how-the-scheduler-iterates-over-nodes>How the scheduler iterates over Nodes</a>
describes the process in detail.</p><h3 id=default-threshold>Default threshold</h3><p>If you don't specify a threshold, Kubernetes calculates a figure using a
linear formula that yields 50% for a 100-node cluster and yields 10%
for a 5000-node cluster. The lower bound for the automatic value is 5%.</p><p>This means that, the kube-scheduler always scores at least 5% of your cluster no
matter how large the cluster is, unless you have explicitly set
<code>percentageOfNodesToScore</code> to be smaller than 5.</p><p>If you want the scheduler to score all nodes in your cluster, set
<code>percentageOfNodesToScore</code> to 100.</p><h2 id=example>Example</h2><p>Below is an example configuration that sets <code>percentageOfNodesToScore</code> to 50%.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1alpha1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>algorithmSource</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>provider</span>:<span style=color:#bbb> </span>DefaultProvider<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>percentageOfNodesToScore</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=tuning-percentageofnodestoscore>Tuning percentageOfNodesToScore</h2><p><code>percentageOfNodesToScore</code> must be a value between 1 and 100 with the default
value being calculated based on the cluster size. There is also a hardcoded
minimum value of 50 nodes.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>In clusters with less than 50 feasible nodes, the scheduler still
checks all the nodes because there are not enough feasible nodes to stop
the scheduler's search early.</p><p>In a small cluster, if you set a low value for <code>percentageOfNodesToScore</code>, your
change will have no or little effect, for a similar reason.</p><p>If your cluster has several hundred Nodes or fewer, leave this configuration option
at its default value. Making changes is unlikely to improve the
scheduler's performance significantly.</p></div><p>An important detail to consider when setting this value is that when a smaller
number of nodes in a cluster are checked for feasibility, some nodes are not
sent to be scored for a given Pod. As a result, a Node which could possibly
score a higher value for running the given Pod might not even be passed to the
scoring phase. This would result in a less than ideal placement of the Pod.</p><p>You should avoid setting <code>percentageOfNodesToScore</code> very low so that kube-scheduler
does not make frequent, poor Pod placement decisions. Avoid setting the
percentage to anything below 10%, unless the scheduler's throughput is critical
for your application and the score of nodes is not important. In other words, you
prefer to run the Pod on any Node as long as it is feasible.</p><h2 id=how-the-scheduler-iterates-over-nodes>How the scheduler iterates over Nodes</h2><p>This section is intended for those who want to understand the internal details
of this feature.</p><p>In order to give all the Nodes in a cluster a fair chance of being considered
for running Pods, the scheduler iterates over the nodes in a round robin
fashion. You can imagine that Nodes are in an array. The scheduler starts from
the start of the array and checks feasibility of the nodes until it finds enough
Nodes as specified by <code>percentageOfNodesToScore</code>. For the next Pod, the
scheduler continues from the point in the Node array that it stopped at when
checking feasibility of Nodes for the previous Pod.</p><p>If Nodes are in multiple zones, the scheduler iterates over Nodes in various
zones to ensure that Nodes from different zones are considered in the
feasibility checks. As an example, consider six nodes in two zones:</p><pre tabindex=0><code>Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
</code></pre><p>The Scheduler evaluates feasibility of the nodes in this order:</p><pre tabindex=0><code>Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
</code></pre><p>After going over all the Nodes, it goes back to Node 1.</p><h2 id=what-s-next>What's next</h2><ul><li>Check the <a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler configuration reference (v1beta3)</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-961126cd43559012893979e568396a49>11.8 - Resource Bin Packing</h1><p>In the <a href=/docs/reference/scheduling/config/#scheduling-plugins>scheduling-plugin</a> <code>NodeResourcesFit</code> of kube-scheduler, there are two
scoring strategies that support the bin packing of resources: <code>MostAllocated</code> and <code>RequestedToCapacityRatio</code>.</p><h2 id=enabling-bin-packing-using-mostallocated-strategy>Enabling bin packing using MostAllocated strategy</h2><p>The <code>MostAllocated</code> strategy scores the nodes based on the utilization of resources, favoring the ones with higher allocation.
For each resource type, you can set a weight to modify its influence in the node score.</p><p>To set the <code>MostAllocated</code> strategy for the <code>NodeResourcesFit</code> plugin, use a
<a href=/docs/reference/scheduling/config>scheduler configuration</a> similar to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>scoringStrategy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>MostAllocated<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeResourcesFit<span style=color:#bbb>
</span></span></span></code></pre></div><p>To learn more about other parameters and their default configuration, see the API documentation for
<a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-NodeResourcesFitArgs><code>NodeResourcesFitArgs</code></a>.</p><h2 id=enabling-bin-packing-using-requestedtocapacityratio>Enabling bin packing using RequestedToCapacityRatio</h2><p>The <code>RequestedToCapacityRatio</code> strategy allows the users to specify the resources along with weights for
each resource to score nodes based on the request to capacity ratio. This
allows users to bin pack extended resources by using appropriate parameters
to improve the utilization of scarce resources in large clusters. It favors nodes according to a
configured function of the allocated resources. The behavior of the <code>RequestedToCapacityRatio</code> in
the <code>NodeResourcesFit</code> score function can be controlled by the
<a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-ScoringStrategy>scoringStrategy</a> field.
Within the <code>scoringStrategy</code> field, you can configure two parameters: <code>requestedToCapacityRatio</code> and
<code>resources</code>. The <code>shape</code> in the <code>requestedToCapacityRatio</code>
parameter allows the user to tune the function as least requested or most
requested based on <code>utilization</code> and <code>score</code> values. The <code>resources</code> parameter
consists of <code>name</code> of the resource to be considered during scoring and <code>weight</code>
specify the weight of each resource.</p><p>Below is an example configuration that sets
the bin packing behavior for extended resources <code>intel.com/foo</code> and <code>intel.com/bar</code>
using the <code>requestedToCapacityRatio</code> field.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>scoringStrategy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>requestedToCapacityRatio</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>RequestedToCapacityRatio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeResourcesFit<span style=color:#bbb>
</span></span></span></code></pre></div><p>Referencing the <code>KubeSchedulerConfiguration</code> file with the kube-scheduler
flag <code>--config=/path/to/config/file</code> will pass the configuration to the
scheduler.</p><p>To learn more about other parameters and their default configuration, see the API documentation for
<a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-NodeResourcesFitArgs><code>NodeResourcesFitArgs</code></a>.</p><h3 id=tuning-the-score-function>Tuning the score function</h3><p><code>shape</code> is used to specify the behavior of the <code>RequestedToCapacityRatio</code> function.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The above arguments give the node a <code>score</code> of 0 if <code>utilization</code> is 0% and 10 for
<code>utilization</code> 100%, thus enabling bin packing behavior. To enable least
requested the score value must be reversed as follows.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span></code></pre></div><p><code>resources</code> is an optional parameter which defaults to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>It can be used to add extended resources as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The <code>weight</code> parameter is optional and is set to 1 if not specified. Also, the
<code>weight</code> cannot be set to a negative value.</p><h3 id=node-scoring-for-capacity-allocation>Node scoring for capacity allocation</h3><p>This section is intended for those who want to understand the internal details
of this feature.
Below is an example of how the node score is calculated for a given set of values.</p><p>Requested resources:</p><pre tabindex=0><code>intel.com/foo : 2
memory: 256MB
cpu: 2
</code></pre><p>Resource weights:</p><pre tabindex=0><code>intel.com/foo : 5
memory: 1
cpu: 3
</code></pre><p>FunctionShapePoint {{0, 0}, {100, 10}}</p><p>Node 1 spec:</p><pre tabindex=0><code>Available:
  intel.com/foo: 4
  memory: 1 GB
  cpu: 8

Used:
  intel.com/foo: 1
  memory: 256MB
  cpu: 1
</code></pre><p>Node score:</p><pre tabindex=0><code>intel.com/foo  = resourceScoringFunction((2+1),4)
               = (100 - ((4-3)*100/4)
               = (100 - 25)
               = 75                       # requested + used = 75% * available
               = rawScoringFunction(75) 
               = 7                        # floor(75/10) 

memory         = resourceScoringFunction((256+256),1024)
               = (100 -((1024-512)*100/1024))
               = 50                       # requested + used = 50% * available
               = rawScoringFunction(50)
               = 5                        # floor(50/10)

cpu            = resourceScoringFunction((2+1),8)
               = (100 -((8-3)*100/8))
               = 37.5                     # requested + used = 37.5% * available
               = rawScoringFunction(37.5)
               = 3                        # floor(37.5/10)

NodeScore   =  (7 * 5) + (5 * 1) + (3 * 3) / (5 + 1 + 3)
            =  5
</code></pre><p>Node 2 spec:</p><pre tabindex=0><code>Available:
  intel.com/foo: 8
  memory: 1GB
  cpu: 8
Used:
  intel.com/foo: 2
  memory: 512MB
  cpu: 6
</code></pre><p>Node score:</p><pre tabindex=0><code>intel.com/foo  = resourceScoringFunction((2+2),8)
               =  (100 - ((8-4)*100/8)
               =  (100 - 50)
               =  50
               =  rawScoringFunction(50)
               = 5

memory         = resourceScoringFunction((256+512),1024)
               = (100 -((1024-768)*100/1024))
               = 75
               = rawScoringFunction(75)
               = 7

cpu            = resourceScoringFunction((2+6),8)
               = (100 -((8-8)*100/8))
               = 100
               = rawScoringFunction(100)
               = 10

NodeScore   =  (5 * 5) + (7 * 1) + (10 * 3) / (5 + 1 + 3)
            =  7
</code></pre><h2 id=what-s-next>What's next</h2><ul><li>Read more about the <a href=/docs/concepts/scheduling-eviction/scheduling-framework/>scheduling framework</a></li><li>Read more about <a href=/docs/reference/scheduling/config/>scheduler configuration</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-60e5a2861609e0848d58ce8bf99c4a31>11.9 - Pod Priority and Preemption</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><p><a href=/docs/concepts/workloads/pods/>Pods</a> can have <em>priority</em>. Priority indicates the
importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the
scheduler tries to preempt (evict) lower priority Pods to make scheduling of the
pending Pod possible.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>In a cluster where not all users are trusted, a malicious user could create Pods
at the highest possible priorities, causing other Pods to be evicted/not get
scheduled.
An administrator can use ResourceQuota to prevent users from creating pods at
high priorities.</p><p>See <a href=/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default>limit Priority Class consumption by default</a>
for details.</p></div><h2 id=how-to-use-priority-and-preemption>How to use priority and preemption</h2><p>To use priority and preemption:</p><ol><li><p>Add one or more <a href=#priorityclass>PriorityClasses</a>.</p></li><li><p>Create Pods with<a href=#pod-priority><code>priorityClassName</code></a> set to one of the added
PriorityClasses. Of course you do not need to create the Pods directly;
normally you would add <code>priorityClassName</code> to the Pod template of a
collection object like a Deployment.</p></li></ol><p>Keep reading for more information about these steps.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes already ships with two PriorityClasses:
<code>system-cluster-critical</code> and <code>system-node-critical</code>.
These are common classes and are used to <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/>ensure that critical components are always scheduled first</a>.</div><h2 id=priorityclass>PriorityClass</h2><p>A PriorityClass is a non-namespaced object that defines a mapping from a
priority class name to the integer value of the priority. The name is specified
in the <code>name</code> field of the PriorityClass object's metadata. The value is
specified in the required <code>value</code> field. The higher the value, the higher the
priority.
The name of a PriorityClass object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>,
and it cannot be prefixed with <code>system-</code>.</p><p>A PriorityClass object can have any 32-bit integer value smaller than or equal
to 1 billion. Larger numbers are reserved for critical system Pods that should
not normally be preempted or evicted. A cluster admin should create one
PriorityClass object for each such mapping that they want.</p><p>PriorityClass also has two optional fields: <code>globalDefault</code> and <code>description</code>.
The <code>globalDefault</code> field indicates that the value of this PriorityClass should
be used for Pods without a <code>priorityClassName</code>. Only one PriorityClass with
<code>globalDefault</code> set to true can exist in the system. If there is no
PriorityClass with <code>globalDefault</code> set, the priority of Pods with no
<code>priorityClassName</code> is zero.</p><p>The <code>description</code> field is an arbitrary string. It is meant to tell users of the
cluster when they should use this PriorityClass.</p><h3 id=notes-about-podpriority-and-existing-clusters>Notes about PodPriority and existing clusters</h3><ul><li><p>If you upgrade an existing cluster without this feature, the priority
of your existing Pods is effectively zero.</p></li><li><p>Addition of a PriorityClass with <code>globalDefault</code> set to <code>true</code> does not
change the priorities of existing Pods. The value of such a PriorityClass is
used only for Pods created after the PriorityClass is added.</p></li><li><p>If you delete a PriorityClass, existing Pods that use the name of the
deleted PriorityClass remain unchanged, but you cannot create more Pods that
use the name of the deleted PriorityClass.</p></li></ul><h3 id=example-priorityclass>Example PriorityClass</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>scheduling.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#666>1000000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>globalDefault</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>description</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;This priority class should be used for XYZ service pods only.&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=non-preempting-priority-class>Non-preempting PriorityClass</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>Pods with <code>preemptionPolicy: Never</code> will be placed in the scheduling queue
ahead of lower-priority pods,
but they cannot preempt other pods.
A non-preempting pod waiting to be scheduled will stay in the scheduling queue,
until sufficient resources are free,
and it can be scheduled.
Non-preempting pods,
like other pods,
are subject to scheduler back-off.
This means that if the scheduler tries these pods and they cannot be scheduled,
they will be retried with lower frequency,
allowing other pods with lower priority to be scheduled before them.</p><p>Non-preempting pods may still be preempted by other,
high-priority pods.</p><p><code>preemptionPolicy</code> defaults to <code>PreemptLowerPriority</code>,
which will allow pods of that PriorityClass to preempt lower-priority pods
(as is existing default behavior).
If <code>preemptionPolicy</code> is set to <code>Never</code>,
pods in that PriorityClass will be non-preempting.</p><p>An example use case is for data science workloads.
A user may submit a job that they want to be prioritized above other workloads,
but do not wish to discard existing work by preempting running pods.
The high priority job with <code>preemptionPolicy: Never</code> will be scheduled
ahead of other queued pods,
as soon as sufficient cluster resources "naturally" become free.</p><h3 id=example-non-preempting-priorityclass>Example Non-preempting PriorityClass</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>scheduling.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority-nonpreempting<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#666>1000000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>preemptionPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>globalDefault</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>description</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;This priority class will not cause other pods to be preempted.&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=pod-priority>Pod priority</h2><p>After you have one or more PriorityClasses, you can create Pods that specify one
of those PriorityClass names in their specifications. The priority admission
controller uses the <code>priorityClassName</code> field and populates the integer value of
the priority. If the priority class is not found, the Pod is rejected.</p><p>The following YAML is an example of a Pod configuration that uses the
PriorityClass created in the preceding example. The priority admission
controller checks the specification and resolves the priority of the Pod to
1000000.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityClassName</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=effect-of-pod-priority-on-scheduling-order>Effect of Pod priority on scheduling order</h3><p>When Pod priority is enabled, the scheduler orders pending Pods by
their priority and a pending Pod is placed ahead of other pending Pods
with lower priority in the scheduling queue. As a result, the higher
priority Pod may be scheduled sooner than Pods with lower priority if
its scheduling requirements are met. If such Pod cannot be scheduled,
scheduler will continue and tries to schedule other lower priority Pods.</p><h2 id=preemption>Preemption</h2><p>When Pods are created, they go to a queue and wait to be scheduled. The
scheduler picks a Pod from the queue and tries to schedule it on a Node. If no
Node is found that satisfies all the specified requirements of the Pod,
preemption logic is triggered for the pending Pod. Let's call the pending Pod P.
Preemption logic tries to find a Node where removal of one or more Pods with
lower priority than P would enable P to be scheduled on that Node. If such a
Node is found, one or more lower priority Pods get evicted from the Node. After
the Pods are gone, P can be scheduled on the Node.</p><h3 id=user-exposed-information>User exposed information</h3><p>When Pod P preempts one or more Pods on Node N, <code>nominatedNodeName</code> field of Pod
P's status is set to the name of Node N. This field helps scheduler track
resources reserved for Pod P and also gives users information about preemptions
in their clusters.</p><p>Please note that Pod P is not necessarily scheduled to the "nominated Node".
The scheduler always tries the "nominated Node" before iterating over any other nodes.
After victim Pods are preempted, they get their graceful termination period. If
another node becomes available while scheduler is waiting for the victim Pods to
terminate, scheduler may use the other node to schedule Pod P. As a result
<code>nominatedNodeName</code> and <code>nodeName</code> of Pod spec are not always the same. Also, if
scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P
arrives, scheduler may give Node N to the new higher priority Pod. In such a
case, scheduler clears <code>nominatedNodeName</code> of Pod P. By doing this, scheduler
makes Pod P eligible to preempt Pods on another Node.</p><h3 id=limitations-of-preemption>Limitations of preemption</h3><h4 id=graceful-termination-of-preemption-victims>Graceful termination of preemption victims</h4><p>When Pods are preempted, the victims get their
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>graceful termination period</a>.
They have that much time to finish their work and exit. If they don't, they are
killed. This graceful termination period creates a time gap between the point
that the scheduler preempts Pods and the time when the pending Pod (P) can be
scheduled on the Node (N). In the meantime, the scheduler keeps scheduling other
pending Pods. As victims exit or get terminated, the scheduler tries to schedule
Pods in the pending queue. Therefore, there is usually a time gap between the
point that scheduler preempts victims and the time that Pod P is scheduled. In
order to minimize this gap, one can set graceful termination period of lower
priority Pods to zero or a small number.</p><h4 id=poddisruptionbudget-is-supported-but-not-guaranteed>PodDisruptionBudget is supported, but not guaranteed</h4><p>A <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> (PDB)
allows application owners to limit the number of Pods of a replicated application
that are down simultaneously from voluntary disruptions. Kubernetes supports
PDB when preempting Pods, but respecting PDB is best effort. The scheduler tries
to find victims whose PDB are not violated by preemption, but if no such victims
are found, preemption will still happen, and lower priority Pods will be removed
despite their PDBs being violated.</p><h4 id=inter-pod-affinity-on-lower-priority-pods>Inter-Pod affinity on lower-priority Pods</h4><p>A Node is considered for preemption only when the answer to this question is
yes: "If all the Pods with lower priority than the pending Pod are removed from
the Node, can the pending Pod be scheduled on the Node?"</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Preemption does not necessarily remove all lower-priority
Pods. If the pending Pod can be scheduled by removing fewer than all
lower-priority Pods, then only a portion of the lower-priority Pods are removed.
Even so, the answer to the preceding question must be yes. If the answer is no,
the Node is not considered for preemption.</div><p>If a pending Pod has inter-pod <a class=glossary-tooltip title='Rules used by the scheduler to determine where to place pods' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity target=_blank aria-label=affinity>affinity</a>
to one or more of the lower-priority Pods on the Node, the inter-Pod affinity
rule cannot be satisfied in the absence of those lower-priority Pods. In this case,
the scheduler does not preempt any Pods on the Node. Instead, it looks for another
Node. The scheduler might find a suitable Node or it might not. There is no
guarantee that the pending Pod can be scheduled.</p><p>Our recommended solution for this problem is to create inter-Pod affinity only
towards equal or higher priority Pods.</p><h4 id=cross-node-preemption>Cross node preemption</h4><p>Suppose a Node N is being considered for preemption so that a pending Pod P can
be scheduled on N. P might become feasible on N only if a Pod on another Node is
preempted. Here's an example:</p><ul><li>Pod P is being considered for Node N.</li><li>Pod Q is running on another Node in the same Zone as Node N.</li><li>Pod P has Zone-wide anti-affinity with Pod Q (<code>topologyKey: topology.kubernetes.io/zone</code>).</li><li>There are no other cases of anti-affinity between Pod P and other Pods in
the Zone.</li><li>In order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler
does not perform cross-node preemption. So, Pod P will be deemed
unschedulable on Node N.</li></ul><p>If Pod Q were removed from its Node, the Pod anti-affinity violation would be
gone, and Pod P could possibly be scheduled on Node N.</p><p>We may consider adding cross Node preemption in future versions if there is
enough demand and if we find an algorithm with reasonable performance.</p><h2 id=troubleshooting>Troubleshooting</h2><p>Pod priority and pre-emption can have unwanted side effects. Here are some
examples of potential problems and ways to deal with them.</p><h3 id=pods-are-preempted-unnecessarily>Pods are preempted unnecessarily</h3><p>Preemption removes existing Pods from a cluster under resource pressure to make
room for higher priority pending Pods. If you give high priorities to
certain Pods by mistake, these unintentionally high priority Pods may cause
preemption in your cluster. Pod priority is specified by setting the
<code>priorityClassName</code> field in the Pod's specification. The integer value for
priority is then resolved and populated to the <code>priority</code> field of <code>podSpec</code>.</p><p>To address the problem, you can change the <code>priorityClassName</code> for those Pods
to use lower priority classes, or leave that field empty. An empty
<code>priorityClassName</code> is resolved to zero by default.</p><p>When a Pod is preempted, there will be events recorded for the preempted Pod.
Preemption should happen only when a cluster does not have enough resources for
a Pod. In such cases, preemption happens only when the priority of the pending
Pod (preemptor) is higher than the victim Pods. Preemption must not happen when
there is no pending Pod, or when the pending Pods have equal or lower priority
than the victims. If preemption happens in such scenarios, please file an issue.</p><h3 id=pods-are-preempted-but-the-preemptor-is-not-scheduled>Pods are preempted, but the preemptor is not scheduled</h3><p>When pods are preempted, they receive their requested graceful termination
period, which is by default 30 seconds. If the victim Pods do not terminate within
this period, they are forcibly terminated. Once all the victims go away, the
preemptor Pod can be scheduled.</p><p>While the preemptor Pod is waiting for the victims to go away, a higher priority
Pod may be created that fits on the same Node. In this case, the scheduler will
schedule the higher priority Pod instead of the preemptor.</p><p>This is expected behavior: the Pod with the higher priority should take the place
of a Pod with a lower priority.</p><h3 id=higher-priority-pods-are-preempted-before-lower-priority-pods>Higher priority Pods are preempted before lower priority pods</h3><p>The scheduler tries to find nodes that can run a pending Pod. If no node is
found, the scheduler tries to remove Pods with lower priority from an arbitrary
node in order to make room for the pending pod.
If a node with low priority Pods is not feasible to run the pending Pod, the scheduler
may choose another node with higher priority Pods (compared to the Pods on the
other node) for preemption. The victims must still have lower priority than the
preemptor Pod.</p><p>When there are multiple nodes available for preemption, the scheduler tries to
choose the node with a set of Pods with lowest priority. However, if such Pods
have PodDisruptionBudget that would be violated if they are preempted then the
scheduler may choose another node with higher priority Pods.</p><p>When multiple nodes exist for preemption and none of the above scenarios apply,
the scheduler chooses a node with the lowest priority.</p><h2 id=interactions-of-pod-priority-and-qos>Interactions between Pod priority and quality of service</h2><p>Pod priority and <a class=glossary-tooltip title='QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-qos-class' target=_blank aria-label='QoS class'>QoS class</a>
are two orthogonal features with few interactions and no default restrictions on
setting the priority of a Pod based on its QoS classes. The scheduler's
preemption logic does not consider QoS when choosing preemption targets.
Preemption considers Pod priority and attempts to choose a set of targets with
the lowest priority. Higher-priority Pods are considered for preemption only if
the removal of the lowest priority Pods is not sufficient to allow the scheduler
to schedule the preemptor Pod, or if the lowest priority Pods are protected by
<code>PodDisruptionBudget</code>.</p><p>The kubelet uses Priority to determine pod order for <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>node-pressure eviction</a>.
You can use the QoS class to estimate the order in which pods are most likely
to get evicted. The kubelet ranks pods for eviction based on the following factors:</p><ol><li>Whether the starved resource usage exceeds requests</li><li>Pod Priority</li><li>Amount of resource usage relative to requests</li></ol><p>See <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction>Pod selection for kubelet eviction</a>
for more details.</p><p>kubelet node-pressure eviction does not evict Pods when their
usage does not exceed their requests. If a Pod with lower priority is not
exceeding its requests, it won't be evicted. Another Pod with higher priority
that exceeds its requests may be evicted.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about using ResourceQuotas in connection with PriorityClasses: <a href=/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default>limit Priority Class consumption by default</a></li><li>Learn about <a href=/docs/concepts/workloads/pods/disruptions/>Pod Disruption</a></li><li>Learn about <a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated Eviction</a></li><li>Learn about <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-78e0431b4b7516092662a7c289cbb304>11.10 - Node-pressure Eviction</h1><p>Node-pressure eviction is the process by which the <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> proactively terminates
pods to reclaim resources on nodes.</br></p><p>The <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> monitors resources
like memory, disk space, and filesystem inodes on your cluster's nodes.
When one or more of these resources reach specific consumption levels, the
kubelet can proactively fail one or more pods on the node to reclaim resources
and prevent starvation.</p><p>During a node-pressure eviction, the kubelet sets the <code>PodPhase</code> for the
selected pods to <code>Failed</code>. This terminates the pods.</p><p>Node-pressure eviction is not the same as
<a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated eviction</a>.</p><p>The kubelet does not respect your configured <code>PodDisruptionBudget</code> or the pod's
<code>terminationGracePeriodSeconds</code>. If you use <a href=#soft-eviction-thresholds>soft eviction thresholds</a>,
the kubelet respects your configured <code>eviction-max-pod-grace-period</code>. If you use
<a href=#hard-eviction-thresholds>hard eviction thresholds</a>, it uses a <code>0s</code> grace period for termination.</p><p>If the pods are managed by a <a class=glossary-tooltip title='A workload is an application running on Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/ target=_blank aria-label=workload>workload</a>
resource (such as <a class=glossary-tooltip title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>
or <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>) that
replaces failed pods, the control plane or <code>kube-controller-manager</code> creates new
pods in place of the evicted pods.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The kubelet attempts to <a href=#reclaim-node-resources>reclaim node-level resources</a>
before it terminates end-user pods. For example, it removes unused container
images when disk resources are starved.</div><p>The kubelet uses various parameters to make eviction decisions, like the following:</p><ul><li>Eviction signals</li><li>Eviction thresholds</li><li>Monitoring intervals</li></ul><h3 id=eviction-signals>Eviction signals</h3><p>Eviction signals are the current state of a particular resource at a specific
point in time. Kubelet uses eviction signals to make eviction decisions by
comparing the signals to eviction thresholds, which are the minimum amount of
the resource that should be available on the node.</p><p>Kubelet uses the following eviction signals:</p><table><thead><tr><th>Eviction Signal</th><th>Description</th></tr></thead><tbody><tr><td><code>memory.available</code></td><td><code>memory.available</code> := <code>node.status.capacity[memory]</code> - <code>node.stats.memory.workingSet</code></td></tr><tr><td><code>nodefs.available</code></td><td><code>nodefs.available</code> := <code>node.stats.fs.available</code></td></tr><tr><td><code>nodefs.inodesFree</code></td><td><code>nodefs.inodesFree</code> := <code>node.stats.fs.inodesFree</code></td></tr><tr><td><code>imagefs.available</code></td><td><code>imagefs.available</code> := <code>node.stats.runtime.imagefs.available</code></td></tr><tr><td><code>imagefs.inodesFree</code></td><td><code>imagefs.inodesFree</code> := <code>node.stats.runtime.imagefs.inodesFree</code></td></tr><tr><td><code>pid.available</code></td><td><code>pid.available</code> := <code>node.stats.rlimit.maxpid</code> - <code>node.stats.rlimit.curproc</code></td></tr></tbody></table><p>In this table, the <code>Description</code> column shows how kubelet gets the value of the
signal. Each signal supports either a percentage or a literal value. Kubelet
calculates the percentage value relative to the total capacity associated with
the signal.</p><p>The value for <code>memory.available</code> is derived from the cgroupfs instead of tools
like <code>free -m</code>. This is important because <code>free -m</code> does not work in a
container, and if users use the <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>node allocatable</a>
feature, out of resource decisions
are made local to the end user Pod part of the cgroup hierarchy as well as the
root node. This <a href=/examples/admin/resource/memory-available.sh>script</a>
reproduces the same set of steps that the kubelet performs to calculate
<code>memory.available</code>. The kubelet excludes inactive_file (i.e. # of bytes of
file-backed memory on inactive LRU list) from its calculation as it assumes that
memory is reclaimable under pressure.</p><p>The kubelet supports the following filesystem partitions:</p><ol><li><code>nodefs</code>: The node's main filesystem, used for local disk volumes, emptyDir,
log storage, and more. For example, <code>nodefs</code> contains <code>/var/lib/kubelet/</code>.</li><li><code>imagefs</code>: An optional filesystem that container runtimes use to store container
images and container writable layers.</li></ol><p>Kubelet auto-discovers these filesystems and ignores other filesystems. Kubelet
does not support other configurations.</p><p>Some kubelet garbage collection features are deprecated in favor of eviction:</p><table><thead><tr><th>Existing Flag</th><th>New Flag</th><th>Rationale</th></tr></thead><tbody><tr><td><code>--image-gc-high-threshold</code></td><td><code>--eviction-hard</code> or <code>--eviction-soft</code></td><td>existing eviction signals can trigger image garbage collection</td></tr><tr><td><code>--image-gc-low-threshold</code></td><td><code>--eviction-minimum-reclaim</code></td><td>eviction reclaims achieve the same behavior</td></tr><tr><td><code>--maximum-dead-containers</code></td><td>-</td><td>deprecated once old logs are stored outside of container's context</td></tr><tr><td><code>--maximum-dead-containers-per-container</code></td><td>-</td><td>deprecated once old logs are stored outside of container's context</td></tr><tr><td><code>--minimum-container-ttl-duration</code></td><td>-</td><td>deprecated once old logs are stored outside of container's context</td></tr></tbody></table><h3 id=eviction-thresholds>Eviction thresholds</h3><p>You can specify custom eviction thresholds for the kubelet to use when it makes
eviction decisions.</p><p>Eviction thresholds have the form <code>[eviction-signal][operator][quantity]</code>, where:</p><ul><li><code>eviction-signal</code> is the <a href=#eviction-signals>eviction signal</a> to use.</li><li><code>operator</code> is the <a href=https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators>relational operator</a>
you want, such as <code>&lt;</code> (less than).</li><li><code>quantity</code> is the eviction threshold amount, such as <code>1Gi</code>. The value of <code>quantity</code>
must match the quantity representation used by Kubernetes. You can use either
literal values or percentages (<code>%</code>).</li></ul><p>For example, if a node has <code>10Gi</code> of total memory and you want trigger eviction if
the available memory falls below <code>1Gi</code>, you can define the eviction threshold as
either <code>memory.available&lt;10%</code> or <code>memory.available&lt;1Gi</code>. You cannot use both.</p><p>You can configure soft and hard eviction thresholds.</p><h4 id=soft-eviction-thresholds>Soft eviction thresholds</h4><p>A soft eviction threshold pairs an eviction threshold with a required
administrator-specified grace period. The kubelet does not evict pods until the
grace period is exceeded. The kubelet returns an error on startup if there is no
specified grace period.</p><p>You can specify both a soft eviction threshold grace period and a maximum
allowed pod termination grace period for kubelet to use during evictions. If you
specify a maximum allowed grace period and the soft eviction threshold is met,
the kubelet uses the lesser of the two grace periods. If you do not specify a
maximum allowed grace period, the kubelet kills evicted pods immediately without
graceful termination.</p><p>You can use the following flags to configure soft eviction thresholds:</p><ul><li><code>eviction-soft</code>: A set of eviction thresholds like <code>memory.available&lt;1.5Gi</code>
that can trigger pod eviction if held over the specified grace period.</li><li><code>eviction-soft-grace-period</code>: A set of eviction grace periods like <code>memory.available=1m30s</code>
that define how long a soft eviction threshold must hold before triggering a Pod eviction.</li><li><code>eviction-max-pod-grace-period</code>: The maximum allowed grace period (in seconds)
to use when terminating pods in response to a soft eviction threshold being met.</li></ul><h4 id=hard-eviction-thresholds>Hard eviction thresholds</h4><p>A hard eviction threshold has no grace period. When a hard eviction threshold is
met, the kubelet kills pods immediately without graceful termination to reclaim
the starved resource.</p><p>You can use the <code>eviction-hard</code> flag to configure a set of hard eviction
thresholds like <code>memory.available&lt;1Gi</code>.</p><p>The kubelet has the following default hard eviction thresholds:</p><ul><li><code>memory.available&lt;100Mi</code></li><li><code>nodefs.available&lt;10%</code></li><li><code>imagefs.available&lt;15%</code></li><li><code>nodefs.inodesFree&lt;5%</code> (Linux nodes)</li></ul><p>These default values of hard eviction thresholds will only be set if none
of the parameters is changed. If you changed the value of any parameter,
then the values of other parameters will not be inherited as the default
values and will be set to zero. In order to provide custom values, you
should provide all the thresholds respectively.</p><h3 id=eviction-monitoring-interval>Eviction monitoring interval</h3><p>The kubelet evaluates eviction thresholds based on its configured <code>housekeeping-interval</code>
which defaults to <code>10s</code>.</p><h3 id=node-conditions>Node conditions</h3><p>The kubelet reports node conditions to reflect that the node is under pressure
because hard or soft eviction threshold is met, independent of configured grace
periods.</p><p>The kubelet maps eviction signals to node conditions as follows:</p><table><thead><tr><th>Node Condition</th><th>Eviction Signal</th><th>Description</th></tr></thead><tbody><tr><td><code>MemoryPressure</code></td><td><code>memory.available</code></td><td>Available memory on the node has satisfied an eviction threshold</td></tr><tr><td><code>DiskPressure</code></td><td><code>nodefs.available</code>, <code>nodefs.inodesFree</code>, <code>imagefs.available</code>, or <code>imagefs.inodesFree</code></td><td>Available disk space and inodes on either the node's root filesystem or image filesystem has satisfied an eviction threshold</td></tr><tr><td><code>PIDPressure</code></td><td><code>pid.available</code></td><td>Available processes identifiers on the (Linux) node has fallen below an eviction threshold</td></tr></tbody></table><p>The kubelet updates the node conditions based on the configured
<code>--node-status-update-frequency</code>, which defaults to <code>10s</code>.</p><h4 id=node-condition-oscillation>Node condition oscillation</h4><p>In some cases, nodes oscillate above and below soft eviction thresholds without
holding for the defined grace periods. This causes the reported node condition
to constantly switch between <code>true</code> and <code>false</code>, leading to bad eviction decisions.</p><p>To protect against oscillation, you can use the <code>eviction-pressure-transition-period</code>
flag, which controls how long the kubelet must wait before transitioning a node
condition to a different state. The transition period has a default value of <code>5m</code>.</p><h3 id=reclaim-node-resources>Reclaiming node level resources</h3><p>The kubelet tries to reclaim node-level resources before it evicts end-user pods.</p><p>When a <code>DiskPressure</code> node condition is reported, the kubelet reclaims node-level
resources based on the filesystems on the node.</p><h4 id=with-imagefs>With <code>imagefs</code></h4><p>If the node has a dedicated <code>imagefs</code> filesystem for container runtimes to use,
the kubelet does the following:</p><ul><li>If the <code>nodefs</code> filesystem meets the eviction thresholds, the kubelet garbage collects
dead pods and containers.</li><li>If the <code>imagefs</code> filesystem meets the eviction thresholds, the kubelet
deletes all unused images.</li></ul><h4 id=without-imagefs>Without <code>imagefs</code></h4><p>If the node only has a <code>nodefs</code> filesystem that meets eviction thresholds,
the kubelet frees up disk space in the following order:</p><ol><li>Garbage collect dead pods and containers</li><li>Delete unused images</li></ol><h3 id=pod-selection-for-kubelet-eviction>Pod selection for kubelet eviction</h3><p>If the kubelet's attempts to reclaim node-level resources don't bring the eviction
signal below the threshold, the kubelet begins to evict end-user pods.</p><p>The kubelet uses the following parameters to determine the pod eviction order:</p><ol><li>Whether the pod's resource usage exceeds requests</li><li><a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority</a></li><li>The pod's resource usage relative to requests</li></ol><p>As a result, kubelet ranks and evicts pods in the following order:</p><ol><li><code>BestEffort</code> or <code>Burstable</code> pods where the usage exceeds requests. These pods
are evicted based on their Priority and then by how much their usage level
exceeds the request.</li><li><code>Guaranteed</code> pods and <code>Burstable</code> pods where the usage is less than requests
are evicted last, based on their Priority.</li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The kubelet does not use the pod's QoS class to determine the eviction order.
You can use the QoS class to estimate the most likely pod eviction order when
reclaiming resources like memory. QoS does not apply to EphemeralStorage requests,
so the above scenario will not apply if the node is, for example, under <code>DiskPressure</code>.</div><p><code>Guaranteed</code> pods are guaranteed only when requests and limits are specified for
all the containers and they are equal. These pods will never be evicted because
of another pod's resource consumption. If a system daemon (such as <code>kubelet</code>
and <code>journald</code>) is consuming more resources than were reserved via
<code>system-reserved</code> or <code>kube-reserved</code> allocations, and the node only has
<code>Guaranteed</code> or <code>Burstable</code> pods using less resources than requests left on it,
then the kubelet must choose to evict one of these pods to preserve node stability
and to limit the impact of resource starvation on other pods. In this case, it
will choose to evict pods of lowest Priority first.</p><p>When the kubelet evicts pods in response to <code>inode</code> or <code>PID</code> starvation, it uses
the Priority to determine the eviction order, because <code>inodes</code> and <code>PIDs</code> have no
requests.</p><p>The kubelet sorts pods differently based on whether the node has a dedicated
<code>imagefs</code> filesystem:</p><h4 id=with-imagefs-1>With <code>imagefs</code></h4><p>If <code>nodefs</code> is triggering evictions, the kubelet sorts pods based on <code>nodefs</code>
usage (<code>local volumes + logs of all containers</code>).</p><p>If <code>imagefs</code> is triggering evictions, the kubelet sorts pods based on the
writable layer usage of all containers.</p><h4 id=without-imagefs-1>Without <code>imagefs</code></h4><p>If <code>nodefs</code> is triggering evictions, the kubelet sorts pods based on their total
disk usage (<code>local volumes + logs & writable layer of all containers</code>)</p><h3 id=minimum-eviction-reclaim>Minimum eviction reclaim</h3><p>In some cases, pod eviction only reclaims a small amount of the starved resource.
This can lead to the kubelet repeatedly hitting the configured eviction thresholds
and triggering multiple evictions.</p><p>You can use the <code>--eviction-minimum-reclaim</code> flag or a <a href=/docs/tasks/administer-cluster/kubelet-config-file/>kubelet config file</a>
to configure a minimum reclaim amount for each resource. When the kubelet notices
that a resource is starved, it continues to reclaim that resource until it
reclaims the quantity you specify.</p><p>For example, the following configuration sets minimum reclaim amounts:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>evictionHard</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>memory.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imagefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>evictionMinimumReclaim</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>memory.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imagefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>In this example, if the <code>nodefs.available</code> signal meets the eviction threshold,
the kubelet reclaims the resource until the signal reaches the threshold of <code>1Gi</code>,
and then continues to reclaim the minimum amount of <code>500Mi</code> it until the signal
reaches <code>1.5Gi</code>.</p><p>Similarly, the kubelet reclaims the <code>imagefs</code> resource until the <code>imagefs.available</code>
signal reaches <code>102Gi</code>.</p><p>The default <code>eviction-minimum-reclaim</code> is <code>0</code> for all resources.</p><h3 id=node-out-of-memory-behavior>Node out of memory behavior</h3><p>If the node experiences an out of memory (OOM) event prior to the kubelet
being able to reclaim memory, the node depends on the <a href=https://lwn.net/Articles/391222/>oom_killer</a>
to respond.</p><p>The kubelet sets an <code>oom_score_adj</code> value for each container based on the QoS for the pod.</p><table><thead><tr><th>Quality of Service</th><th>oom_score_adj</th></tr></thead><tbody><tr><td><code>Guaranteed</code></td><td>-997</td></tr><tr><td><code>BestEffort</code></td><td>1000</td></tr><tr><td><code>Burstable</code></td><td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The kubelet also sets an <code>oom_score_adj</code> value of <code>-997</code> for containers in Pods that have
<code>system-node-critical</code> <a class=glossary-tooltip title='Pod Priority indicates the importance of a Pod relative to other Pods.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority target=_blank aria-label=Priority>Priority</a>.</div><p>If the kubelet can't reclaim memory before a node experiences OOM, the
<code>oom_killer</code> calculates an <code>oom_score</code> based on the percentage of memory it's
using on the node, and then adds the <code>oom_score_adj</code> to get an effective <code>oom_score</code>
for each container. It then kills the container with the highest score.</p><p>This means that containers in low QoS pods that consume a large amount of memory
relative to their scheduling requests are killed first.</p><p>Unlike pod eviction, if a container is OOM killed, the <code>kubelet</code> can restart it
based on its <code>RestartPolicy</code>.</p><h3 id=node-pressure-eviction-good-practices>Best practices</h3><p>The following sections describe best practices for eviction configuration.</p><h4 id=schedulable-resources-and-eviction-policies>Schedulable resources and eviction policies</h4><p>When you configure the kubelet with an eviction policy, you should make sure that
the scheduler will not schedule pods if they will trigger eviction because they
immediately induce memory pressure.</p><p>Consider the following scenario:</p><ul><li>Node memory capacity: <code>10Gi</code></li><li>Operator wants to reserve 10% of memory capacity for system daemons (kernel, <code>kubelet</code>, etc.)</li><li>Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.</li></ul><p>For this to work, the kubelet is launched as follows:</p><pre tabindex=0><code>--eviction-hard=memory.available&lt;500Mi
--system-reserved=memory=1.5Gi
</code></pre><p>In this configuration, the <code>--system-reserved</code> flag reserves <code>1.5Gi</code> of memory
for the system, which is <code>10% of the total memory + the eviction threshold amount</code>.</p><p>The node can reach the eviction threshold if a pod is using more than its request,
or if the system is using more than <code>1Gi</code> of memory, which makes the <code>memory.available</code>
signal fall below <code>500Mi</code> and triggers the threshold.</p><h4 id=daemonset>DaemonSet</h4><p>Pod Priority is a major factor in making eviction decisions. If you do not want
the kubelet to evict pods that belong to a <code>DaemonSet</code>, give those pods a high
enough <code>priorityClass</code> in the pod spec. You can also use a lower <code>priorityClass</code>
or the default to only allow <code>DaemonSet</code> pods to run when there are enough
resources.</p><h3 id=known-issues>Known issues</h3><p>The following sections describe known issues related to out of resource handling.</p><h4 id=kubelet-may-not-observe-memory-pressure-right-away>kubelet may not observe memory pressure right away</h4><p>By default, the kubelet polls <code>cAdvisor</code> to collect memory usage stats at a
regular interval. If memory usage increases within that window rapidly, the
kubelet may not observe <code>MemoryPressure</code> fast enough, and the <code>OOMKiller</code>
will still be invoked.</p><p>You can use the <code>--kernel-memcg-notification</code> flag to enable the <code>memcg</code>
notification API on the kubelet to get notified immediately when a threshold
is crossed.</p><p>If you are not trying to achieve extreme utilization, but a sensible measure of
overcommit, a viable workaround for this issue is to use the <code>--kube-reserved</code>
and <code>--system-reserved</code> flags to allocate memory for the system.</p><h4 id=active-file-memory-is-not-considered-as-available-memory>active_file memory is not considered as available memory</h4><p>On Linux, the kernel tracks the number of bytes of file-backed memory on active
LRU list as the <code>active_file</code> statistic. The kubelet treats <code>active_file</code> memory
areas as not reclaimable. For workloads that make intensive use of block-backed
local storage, including ephemeral local storage, kernel-level caches of file
and block data means that many recently accessed cache pages are likely to be
counted as <code>active_file</code>. If enough of these kernel block buffers are on the
active LRU list, the kubelet is liable to observe this as high resource use and
taint the node as experiencing memory pressure - triggering pod eviction.</p><p>For more details, see <a href=https://github.com/kubernetes/kubernetes/issues/43916>https://github.com/kubernetes/kubernetes/issues/43916</a></p><p>You can work around that behavior by setting the memory limit and memory request
the same for containers likely to perform intensive I/O activity. You will need
to estimate or measure an optimal memory limit value for that container.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated Eviction</a></li><li>Learn about <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority and Preemption</a></li><li>Learn about <a href=/docs/tasks/run-application/configure-pdb/>PodDisruptionBudgets</a></li><li>Learn about <a href=/docs/tasks/configure-pod-container/quality-service-pod/>Quality of Service</a> (QoS)</li><li>Check out the <a href=/docs/reference/generated/kubernetes-api/v1.25/#create-eviction-pod-v1-core>Eviction API</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b87723bf81b079042860f0ebd37b0a64>11.11 - API-initiated Eviction</h1><p>API-initiated eviction is the process by which you use the <a href=/docs/reference/generated/kubernetes-api/v1.25/#create-eviction-pod-v1-core>Eviction API</a>
to create an <code>Eviction</code> object that triggers graceful pod termination.</br></p><p>You can request eviction by calling the Eviction API directly, or programmatically
using a client of the <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a>, like the <code>kubectl drain</code> command. This
creates an <code>Eviction</code> object, which causes the API server to terminate the Pod.</p><p>API-initiated evictions respect your configured <a href=/docs/tasks/run-application/configure-pdb/><code>PodDisruptionBudgets</code></a>
and <a href=/docs/concepts/workloads/pods/pod-lifecycle#pod-termination><code>terminationGracePeriodSeconds</code></a>.</p><p>Using the API to create an Eviction object for a Pod is like performing a
policy-controlled <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod><code>DELETE</code> operation</a>
on the Pod.</p><h2 id=calling-the-eviction-api>Calling the Eviction API</h2><p>You can use a <a href=/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api>Kubernetes language client</a>
to access the Kubernetes API and create an <code>Eviction</code> object. To do this, you
POST the attempted operation, similar to the following example:</p><ul class="nav nav-tabs" id=eviction-example role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#eviction-example-0 role=tab aria-controls=eviction-example-0 aria-selected=true>policy/v1</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#eviction-example-1 role=tab aria-controls=eviction-example-1>policy/v1beta1</a></li></ul><div class=tab-content id=eviction-example><div id=eviction-example-0 class="tab-pane show active" role=tabpanel aria-labelledby=eviction-example-0><p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> <code>policy/v1</code> Eviction is available in v1.22+. Use <code>policy/v1beta1</code> with prior releases.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;policy/v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Eviction&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;quux&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></div><div id=eviction-example-1 class=tab-pane role=tabpanel aria-labelledby=eviction-example-1><p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Deprecated in v1.22 in favor of <code>policy/v1</code></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;policy/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Eviction&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;quux&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></div></div><p>Alternatively, you can attempt an eviction operation by accessing the API using
<code>curl</code> or <code>wget</code>, similar to the following example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -v -H <span style=color:#b44>&#39;Content-type: application/json&#39;</span> https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json
</span></span></code></pre></div><h2 id=how-api-initiated-eviction-works>How API-initiated eviction works</h2><p>When you request an eviction using the API, the API server performs admission
checks and responds in one of the following ways:</p><ul><li><code>200 OK</code>: the eviction is allowed, the <code>Eviction</code> subresource is created, and
the Pod is deleted, similar to sending a <code>DELETE</code> request to the Pod URL.</li><li><code>429 Too Many Requests</code>: the eviction is not currently allowed because of the
configured <a class=glossary-tooltip title='An object that limits the number of  of a replicated application, that are down simultaneously from voluntary disruptions.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-pod-disruption-budget' target=_blank aria-label=PodDisruptionBudget>PodDisruptionBudget</a>.
You may be able to attempt the eviction again later. You might also see this
response because of API rate limiting.</li><li><code>500 Internal Server Error</code>: the eviction is not allowed because there is a
misconfiguration, like if multiple PodDisruptionBudgets reference the same Pod.</li></ul><p>If the Pod you want to evict isn't part of a workload that has a
PodDisruptionBudget, the API server always returns <code>200 OK</code> and allows the
eviction.</p><p>If the API server allows the eviction, the Pod is deleted as follows:</p><ol><li>The <code>Pod</code> resource in the API server is updated with a deletion timestamp,
after which the API server considers the <code>Pod</code> resource to be terminated. The
<code>Pod</code> resource is also marked with the configured grace period.</li><li>The <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> on the node where the local Pod is running notices that the <code>Pod</code>
resource is marked for termination and starts to gracefully shut down the
local Pod.</li><li>While the kubelet is shutting the Pod down, the control plane removes the Pod
from <a class=glossary-tooltip title='Endpoints track the IP addresses of Pods with matching Service selectors.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-endpoint' target=_blank aria-label=Endpoint>Endpoint</a> and
<a class=glossary-tooltip title='A way to group network endpoints together with Kubernetes resources.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/endpoint-slices/ target=_blank aria-label=EndpointSlice>EndpointSlice</a>
objects. As a result, controllers no longer consider the Pod as a valid object.</li><li>After the grace period for the Pod expires, the kubelet forcefully terminates
the local Pod.</li><li>The kubelet tells the API server to remove the <code>Pod</code> resource.</li><li>The API server deletes the <code>Pod</code> resource.</li></ol><h2 id=troubleshooting-stuck-evictions>Troubleshooting stuck evictions</h2><p>In some cases, your applications may enter a broken state, where the Eviction
API will only return <code>429</code> or <code>500</code> responses until you intervene. This can
happen if, for example, a ReplicaSet creates pods for your application but new
pods do not enter a <code>Ready</code> state. You may also notice this behavior in cases
where the last evicted Pod had a long termination grace period.</p><p>If you notice stuck evictions, try one of the following solutions:</p><ul><li>Abort or pause the automated operation causing the issue. Investigate the stuck
application before you restart the operation.</li><li>Wait a while, then directly delete the Pod from your cluster control plane
instead of using the Eviction API.</li></ul><h2 id=what-s-next>What's next</h2><ul><li>Learn how to protect your applications with a <a href=/docs/tasks/run-application/configure-pdb/>Pod Disruption Budget</a>.</li><li>Learn about <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a>.</li><li>Learn about <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority and Preemption</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-285a3785fd3d20f437c28d87ca4dadca>12 - Cluster Administration</h1><div class=lead>Lower-level detail relevant to creating or administering a Kubernetes cluster.</div><p>The cluster administration overview is for anyone creating or administering a Kubernetes cluster.
It assumes some familiarity with core Kubernetes <a href=/docs/concepts/>concepts</a>.</p><h2 id=planning-a-cluster>Planning a cluster</h2><p>See the guides in <a href=/docs/setup/>Setup</a> for examples of how to plan, set up, and configure
Kubernetes clusters. The solutions listed in this article are called <em>distros</em>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Not all distros are actively maintained. Choose distros which have been tested with a recent
version of Kubernetes.</div><p>Before choosing a guide, here are some considerations:</p><ul><li>Do you want to try out Kubernetes on your computer, or do you want to build a high-availability,
multi-node cluster? Choose distros best suited for your needs.</li><li>Will you be using <strong>a hosted Kubernetes cluster</strong>, such as
<a href=https://cloud.google.com/kubernetes-engine/>Google Kubernetes Engine</a>, or <strong>hosting your own cluster</strong>?</li><li>Will your cluster be <strong>on-premises</strong>, or <strong>in the cloud (IaaS)</strong>? Kubernetes does not directly
support hybrid clusters. Instead, you can set up multiple clusters.</li><li><strong>If you are configuring Kubernetes on-premises</strong>, consider which
<a href=/docs/concepts/cluster-administration/networking/>networking model</a> fits best.</li><li>Will you be running Kubernetes on <strong>"bare metal" hardware</strong> or on <strong>virtual machines (VMs)</strong>?</li><li>Do you <strong>want to run a cluster</strong>, or do you expect to do <strong>active development of Kubernetes project code</strong>?
If the latter, choose an actively-developed distro. Some distros only use binary releases, but
offer a greater variety of choices.</li><li>Familiarize yourself with the <a href=/docs/concepts/overview/components/>components</a> needed to run a cluster.</li></ul><h2 id=managing-a-cluster>Managing a cluster</h2><ul><li><p>Learn how to <a href=/docs/concepts/architecture/nodes/>manage nodes</a>.</p></li><li><p>Learn how to set up and manage the <a href=/docs/concepts/policy/resource-quotas/>resource quota</a> for shared clusters.</p></li></ul><h2 id=securing-a-cluster>Securing a cluster</h2><ul><li><p><a href=/docs/tasks/administer-cluster/certificates/>Generate Certificates</a> describes the steps to
generate certificates using different tool chains.</p></li><li><p><a href=/docs/concepts/containers/container-environment/>Kubernetes Container Environment</a> describes
the environment for Kubelet managed containers on a Kubernetes node.</p></li><li><p><a href=/docs/concepts/security/controlling-access>Controlling Access to the Kubernetes API</a> describes
how Kubernetes implements access control for its own API.</p></li><li><p><a href=/docs/reference/access-authn-authz/authentication/>Authenticating</a> explains authentication in
Kubernetes, including the various authentication options.</p></li><li><p><a href=/docs/reference/access-authn-authz/authorization/>Authorization</a> is separate from
authentication, and controls how HTTP calls are handled.</p></li><li><p><a href=/docs/reference/access-authn-authz/admission-controllers/>Using Admission Controllers</a>
explains plug-ins which intercepts requests to the Kubernetes API server after authentication
and authorization.</p></li><li><p><a href=/docs/tasks/administer-cluster/sysctl-cluster/>Using Sysctls in a Kubernetes Cluster</a>
describes to an administrator how to use the <code>sysctl</code> command-line tool to set kernel parameters
.</p></li><li><p><a href=/docs/tasks/debug/debug-cluster/audit/>Auditing</a> describes how to interact with Kubernetes'
audit logs.</p></li></ul><h3 id=securing-the-kubelet>Securing the kubelet</h3><ul><li><a href=/docs/concepts/architecture/control-plane-node-communication/>Control Plane-Node communication</a></li><li><a href=/docs/reference/access-authn-authz/kubelet-tls-bootstrapping/>TLS bootstrapping</a></li><li><a href=/docs/reference/access-authn-authz/kubelet-authn-authz/>Kubelet authentication/authorization</a></li></ul><h2 id=optional-cluster-services>Optional Cluster Services</h2><ul><li><p><a href=/docs/concepts/services-networking/dns-pod-service/>DNS Integration</a> describes how to resolve
a DNS name directly to a Kubernetes service.</p></li><li><p><a href=/docs/concepts/cluster-administration/logging/>Logging and Monitoring Cluster Activity</a>
explains how logging in Kubernetes works and how to implement it.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2bf9a93ab5ba014fb6ff70b22c29d432>12.1 - Certificates</h1><p>To learn how to generate certificates for your cluster, see <a href=/docs/tasks/administer-cluster/certificates/>Certificates</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3aeeecf7cdb2a21eb4b31db7a71c81e2>12.2 - Managing Resources</h1><p>You've deployed your application and exposed it via a service. Now what? Kubernetes provides a number of tools to help you manage your application deployment, including scaling and updating. Among the features that we will discuss in more depth are <a href=/docs/concepts/configuration/overview/>configuration files</a> and <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a>.</p><h2 id=organizing-resource-configurations>Organizing resource configurations</h2><p>Many applications require multiple resources to be created, such as a Deployment and a Service. Management of multiple resources can be simplified by grouping them together in the same file (separated by <code>---</code> in YAML). For example:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/nginx-app.yaml download=application/nginx-app.yaml><code>application/nginx-app.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("application-nginx-app-yaml")' title="Copy application/nginx-app.yaml to clipboard"></img></div><div class=includecode id=application-nginx-app-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx-svc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>LoadBalancer<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>my-nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Multiple resources can be created the same way as a single resource:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>service/my-nginx-svc created
</span></span><span style=display:flex><span>deployment.apps/my-nginx created
</span></span></code></pre></div><p>The resources will be created in the order they appear in the file. Therefore, it's best to specify the service first, since that will ensure the scheduler can spread the pods associated with the service as they are created by the controller(s), such as Deployment.</p><p><code>kubectl apply</code> also accepts multiple <code>-f</code> arguments:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-svc.yaml -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><p>And a directory can be specified rather than or in addition to individual files:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx/
</span></span></code></pre></div><p><code>kubectl</code> will read any files with suffixes <code>.yaml</code>, <code>.yml</code>, or <code>.json</code>.</p><p>It is a recommended practice to put resources related to the same microservice or application tier into the same file, and to group all of the files associated with your application in the same directory. If the tiers of your application bind to each other using DNS, you can deploy all of the components of your stack together.</p><p>A URL can also be specified as a configuration source, which is handy for deploying directly from configuration files checked into GitHub:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/nginx/nginx-deployment.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx created
</span></span></code></pre></div><h2 id=bulk-operations-in-kubectl>Bulk operations in kubectl</h2><p>Resource creation isn't the only operation that <code>kubectl</code> can perform in bulk. It can also extract resource names from configuration files in order to perform other operations, in particular to delete the same resources you created:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete -f https://k8s.io/examples/application/nginx-app.yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
</span></span><span style=display:flex><span>service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</span></span></code></pre></div><p>In the case of two resources, you can specify both resources on the command line using the resource/name syntax:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete deployments/my-nginx services/my-nginx-svc
</span></span></code></pre></div><p>For larger numbers of resources, you'll find it easier to specify the selector (label query) specified using <code>-l</code> or <code>--selector</code>, to filter resources by their labels:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl delete deployment,services -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps <span style=color:#b44>&#34;my-nginx&#34;</span> deleted
</span></span><span style=display:flex><span>service <span style=color:#b44>&#34;my-nginx-svc&#34;</span> deleted
</span></span></code></pre></div><p>Because <code>kubectl</code> outputs resource names in the same syntax it accepts, you can chain operations using <code>$()</code> or <code>xargs</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get <span style=color:#a2f;font-weight:700>$(</span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service<span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span>kubectl create -f docs/concepts/cluster-administration/nginx/ -o name | grep service | xargs -i kubectl get <span style=color:#666>{}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME           TYPE           CLUSTER-IP   EXTERNAL-IP   PORT<span style=color:#666>(</span>S<span style=color:#666>)</span>      AGE
</span></span><span style=display:flex><span>my-nginx-svc   LoadBalancer   10.0.0.208   &lt;pending&gt;     80/TCP       0s
</span></span></code></pre></div><p>With the above commands, we first create resources under <code>examples/application/nginx/</code> and print the resources created with <code>-o name</code> output format
(print each resource as resource/name). Then we <code>grep</code> only the "service", and then print it with <code>kubectl get</code>.</p><p>If you happen to organize your resources across several subdirectories within a particular directory, you can recursively perform the operations on the subdirectories also, by specifying <code>--recursive</code> or <code>-R</code> alongside the <code>--filename,-f</code> flag.</p><p>For instance, assume there is a directory <code>project/k8s/development</code> that holds all of the <a class=glossary-tooltip title='A serialized specification of one or more Kubernetes API objects.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-manifest' target=_blank aria-label=manifests>manifests</a> needed for the development environment, organized by resource type:</p><pre tabindex=0><code>project/k8s/development
├── configmap
│   └── my-configmap.yaml
├── deployment
│   └── my-deployment.yaml
└── pvc
    └── my-pvc.yaml
</code></pre><p>By default, performing a bulk operation on <code>project/k8s/development</code> will stop at the first level of the directory, not processing any subdirectories. If we had tried to create the resources in this directory using the following command, we would have encountered an error:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f project/k8s/development
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>error: you must provide one or more resources by argument or filename <span style=color:#666>(</span>.json|.yaml|.yml|stdin<span style=color:#666>)</span>
</span></span></code></pre></div><p>Instead, specify the <code>--recursive</code> or <code>-R</code> flag with the <code>--filename,-f</code> flag as such:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f project/k8s/development --recursive
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>configmap/my-config created
</span></span><span style=display:flex><span>deployment.apps/my-deployment created
</span></span><span style=display:flex><span>persistentvolumeclaim/my-pvc created
</span></span></code></pre></div><p>The <code>--recursive</code> flag works with any operation that accepts the <code>--filename,-f</code> flag such as: <code>kubectl {create,get,delete,describe,rollout}</code> etc.</p><p>The <code>--recursive</code> flag also works when multiple <code>-f</code> arguments are provided:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f project/k8s/namespaces -f project/k8s/development --recursive
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>namespace/development created
</span></span><span style=display:flex><span>namespace/staging created
</span></span><span style=display:flex><span>configmap/my-config created
</span></span><span style=display:flex><span>deployment.apps/my-deployment created
</span></span><span style=display:flex><span>persistentvolumeclaim/my-pvc created
</span></span></code></pre></div><p>If you're interested in learning more about <code>kubectl</code>, go ahead and read <a href=/docs/reference/kubectl/>Command line tool (kubectl)</a>.</p><h2 id=using-labels-effectively>Using labels effectively</h2><p>The examples we've used so far apply at most a single label to any resource. There are many scenarios where multiple labels should be used to distinguish sets from one another.</p><p>For instance, different applications would use different values for the <code>app</code> label, but a multi-tier application, such as the <a href=https://github.com/kubernetes/examples/tree/master/guestbook/>guestbook example</a>, would additionally need to distinguish each tier. The frontend could carry the following labels:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span></code></pre></div><p>while the Redis master and slave would have different <code>tier</code> labels, and perhaps even an additional <code>role</code> label:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>master<span style=color:#bbb>
</span></span></span></code></pre></div><p>and</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>backend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>role</span>:<span style=color:#bbb> </span>slave<span style=color:#bbb>
</span></span></span></code></pre></div><p>The labels allow us to slice and dice our resources along any dimension specified by a label:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f examples/guestbook/all-in-one/guestbook-all-in-one.yaml
</span></span><span style=display:flex><span>kubectl get pods -Lapp -Ltier -Lrole
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                           READY     STATUS    RESTARTS   AGE       APP         TIER       ROLE
</span></span><span style=display:flex><span>guestbook-fe-4nlpb             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
</span></span><span style=display:flex><span>guestbook-fe-ght6d             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
</span></span><span style=display:flex><span>guestbook-fe-jpy62             1/1       Running   <span style=color:#666>0</span>          1m        guestbook   frontend   &lt;none&gt;
</span></span><span style=display:flex><span>guestbook-redis-master-5pg3b   1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    master
</span></span><span style=display:flex><span>guestbook-redis-slave-2q2yf    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
</span></span><span style=display:flex><span>guestbook-redis-slave-qgazl    1/1       Running   <span style=color:#666>0</span>          1m        guestbook   backend    slave
</span></span><span style=display:flex><span>my-nginx-divi2                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
</span></span><span style=display:flex><span>my-nginx-o0ef1                 1/1       Running   <span style=color:#666>0</span>          29m       nginx       &lt;none&gt;     &lt;none&gt;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -lapp<span style=color:#666>=</span>guestbook,role<span style=color:#666>=</span>slave
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                          READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>guestbook-redis-slave-2q2yf   1/1       Running   <span style=color:#666>0</span>          3m
</span></span><span style=display:flex><span>guestbook-redis-slave-qgazl   1/1       Running   <span style=color:#666>0</span>          3m
</span></span></code></pre></div><h2 id=canary-deployments>Canary deployments</h2><p>Another scenario where multiple labels are needed is to distinguish deployments of different releases or configurations of the same component. It is common practice to deploy a <em>canary</em> of a new application release (specified via image tag in the pod template) side by side with the previous release so that the new release can receive live production traffic before fully rolling it out.</p><p>For instance, you can use a <code>track</code> label to differentiate different releases.</p><p>The primary, stable release would have a <code>track</code> label with value as <code>stable</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>stable<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v3<span style=color:#bbb>
</span></span></span></code></pre></div><p>and then you can create a new release of the guestbook frontend that carries the <code>track</code> label with different value (i.e. <code>canary</code>), so that two sets of pods would not overlap:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-canary<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>track</span>:<span style=color:#bbb> </span>canary<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gb-frontend:v4<span style=color:#bbb>
</span></span></span></code></pre></div><p>The frontend service would span both sets of replicas by selecting the common subset of their labels (i.e. omitting the <code>track</code> label), so that the traffic will be redirected to both applications:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>     </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span></code></pre></div><p>You can tweak the number of replicas of the stable and canary releases to determine the ratio of each release that will receive live production traffic (in this case, 3:1).
Once you're confident, you can update the stable track to the new application release and remove the canary one.</p><p>For a more concrete example, check the <a href=https://github.com/kelseyhightower/talks/tree/master/kubecon-eu-2016/demo#deploy-a-canary>tutorial of deploying Ghost</a>.</p><h2 id=updating-labels>Updating labels</h2><p>Sometimes existing pods and other resources need to be relabeled before creating new resources. This can be done with <code>kubectl label</code>.
For example, if you want to label all your nginx pods as frontend tier, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl label pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx <span style=color:#b8860b>tier</span><span style=color:#666>=</span>fe
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>pod/my-nginx-2035384211-j5fhi labeled
</span></span><span style=display:flex><span>pod/my-nginx-2035384211-u2c7e labeled
</span></span><span style=display:flex><span>pod/my-nginx-2035384211-u3t6x labeled
</span></span></code></pre></div><p>This first filters all pods with the label "app=nginx", and then labels them with the "tier=fe".
To see the pods you labeled, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx -L tier
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                        READY     STATUS    RESTARTS   AGE       TIER
</span></span><span style=display:flex><span>my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          23m       fe
</span></span><span style=display:flex><span>my-nginx-2035384211-u2c7e   1/1       Running   <span style=color:#666>0</span>          23m       fe
</span></span><span style=display:flex><span>my-nginx-2035384211-u3t6x   1/1       Running   <span style=color:#666>0</span>          23m       fe
</span></span></code></pre></div><p>This outputs all "app=nginx" pods, with an additional label column of pods' tier (specified with <code>-L</code> or <code>--label-columns</code>).</p><p>For more information, please see <a href=/docs/concepts/overview/working-with-objects/labels/>labels</a> and <a href=/docs/reference/generated/kubectl/kubectl-commands/#label>kubectl label</a>.</p><h2 id=updating-annotations>Updating annotations</h2><p>Sometimes you would want to attach annotations to resources. Annotations are arbitrary non-identifying metadata for retrieval by API clients such as tools, libraries, etc. This can be done with <code>kubectl annotate</code>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl annotate pods my-nginx-v4-9gw19 <span style=color:#b8860b>description</span><span style=color:#666>=</span><span style=color:#b44>&#39;my frontend running nginx&#39;</span>
</span></span><span style=display:flex><span>kubectl get pods my-nginx-v4-9gw19 -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    description: my frontend running nginx
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>For more information, please see <a href=/docs/concepts/overview/working-with-objects/annotations/>annotations</a> and <a href=/docs/reference/generated/kubectl/kubectl-commands/#annotate>kubectl annotate</a> document.</p><h2 id=scaling-your-application>Scaling your application</h2><p>When load on your application grows or shrinks, use <code>kubectl</code> to scale your application. For instance, to decrease the number of nginx replicas from 3 to 1, do:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment/my-nginx --replicas<span style=color:#666>=</span><span style=color:#666>1</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx scaled
</span></span></code></pre></div><p>Now you only have one pod managed by the deployment.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods -l <span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME                        READY     STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>my-nginx-2035384211-j5fhi   1/1       Running   <span style=color:#666>0</span>          30m
</span></span></code></pre></div><p>To have the system automatically choose the number of nginx replicas as needed, ranging from 1 to 3, do:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale deployment/my-nginx --min<span style=color:#666>=</span><span style=color:#666>1</span> --max<span style=color:#666>=</span><span style=color:#666>3</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>horizontalpodautoscaler.autoscaling/my-nginx autoscaled
</span></span></code></pre></div><p>Now your nginx replicas will be scaled up and down as needed, automatically.</p><p>For more information, please see <a href=/docs/reference/generated/kubectl/kubectl-commands/#scale>kubectl scale</a>, <a href=/docs/reference/generated/kubectl/kubectl-commands/#autoscale>kubectl autoscale</a> and <a href=/docs/tasks/run-application/horizontal-pod-autoscale/>horizontal pod autoscaler</a> document.</p><h2 id=in-place-updates-of-resources>In-place updates of resources</h2><p>Sometimes it's necessary to make narrow, non-disruptive updates to resources you've created.</p><h3 id=kubectl-apply>kubectl apply</h3><p>It is suggested to maintain a set of configuration files in source control
(see <a href=https://martinfowler.com/bliki/InfrastructureAsCode.html>configuration as code</a>),
so that they can be maintained and versioned along with the code for the resources they configure.
Then, you can use <a href=/docs/reference/generated/kubectl/kubectl-commands/#apply><code>kubectl apply</code></a> to push your configuration changes to the cluster.</p><p>This command will compare the version of the configuration that you're pushing with the previous version and apply the changes you've made, without overwriting any automated changes to properties you haven't specified.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml
</span></span><span style=display:flex><span>deployment.apps/my-nginx configured
</span></span></code></pre></div><p>Note that <code>kubectl apply</code> attaches an annotation to the resource in order to determine the changes to the configuration since the previous invocation. When it's invoked, <code>kubectl apply</code> does a three-way diff between the previous configuration, the provided input and the current configuration of the resource, in order to determine how to modify the resource.</p><p>Currently, resources are created without this annotation, so the first invocation of <code>kubectl apply</code> will fall back to a two-way diff between the provided input and the current configuration of the resource. During this first invocation, it cannot detect the deletion of properties set when the resource was created. For this reason, it will not remove them.</p><p>All subsequent calls to <code>kubectl apply</code>, and other commands that modify the configuration, such as <code>kubectl replace</code> and <code>kubectl edit</code>, will update the annotation, allowing subsequent calls to <code>kubectl apply</code> to detect and perform deletions using a three-way diff.</p><h3 id=kubectl-edit>kubectl edit</h3><p>Alternatively, you may also update resources with <code>kubectl edit</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment/my-nginx
</span></span></code></pre></div><p>This is equivalent to first <code>get</code> the resource, edit it in text editor, and then <code>apply</code> the resource with the updated version:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment my-nginx -o yaml &gt; /tmp/nginx.yaml
</span></span><span style=display:flex><span>vi /tmp/nginx.yaml
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># do some edit, and then save the file</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl apply -f /tmp/nginx.yaml
</span></span><span style=display:flex><span>deployment.apps/my-nginx configured
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rm /tmp/nginx.yaml
</span></span></code></pre></div><p>This allows you to do more significant changes more easily. Note that you can specify the editor with your <code>EDITOR</code> or <code>KUBE_EDITOR</code> environment variables.</p><p>For more information, please see <a href=/docs/reference/generated/kubectl/kubectl-commands/#edit>kubectl edit</a> document.</p><h3 id=kubectl-patch>kubectl patch</h3><p>You can use <code>kubectl patch</code> to update API objects in place. This command supports JSON patch,
JSON merge patch, and strategic merge patch. See
<a href=/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/>Update API Objects in Place Using kubectl patch</a>
and
<a href=/docs/reference/generated/kubectl/kubectl-commands/#patch>kubectl patch</a>.</p><h2 id=disruptive-updates>Disruptive updates</h2><p>In some cases, you may need to update resource fields that cannot be updated once initialized, or you may want to make a recursive change immediately, such as to fix broken pods created by a Deployment. To change such fields, use <code>replace --force</code>, which deletes and re-creates the resource. In this case, you can modify your original configuration file:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl replace -f https://k8s.io/examples/application/nginx/nginx-deployment.yaml --force
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx deleted
</span></span><span style=display:flex><span>deployment.apps/my-nginx replaced
</span></span></code></pre></div><h2 id=updating-your-application-without-a-service-outage>Updating your application without a service outage</h2><p>At some point, you'll eventually need to update your deployed application, typically by specifying a new image or image tag, as in the canary deployment scenario above. <code>kubectl</code> supports several update operations, each of which is applicable to different scenarios.</p><p>We'll guide you through how to create and update applications with Deployments.</p><p>Let's say you were running version 1.14.2 of nginx:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create deployment my-nginx --image<span style=color:#666>=</span>nginx:1.14.2
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>deployment.apps/my-nginx created
</span></span></code></pre></div><p>with 3 replicas (so the old and new revisions can coexist):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment my-nginx --current-replicas<span style=color:#666>=</span><span style=color:#666>1</span> --replicas<span style=color:#666>=</span><span style=color:#666>3</span>
</span></span></code></pre></div><pre tabindex=0><code>deployment.apps/my-nginx scaled
</code></pre><p>To update to version 1.16.1, change <code>.spec.template.spec.containers[0].image</code> from <code>nginx:1.14.2</code> to <code>nginx:1.16.1</code> using the previous kubectl commands.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment/my-nginx
</span></span></code></pre></div><p>That's it! The Deployment will declaratively update the deployed nginx application progressively behind the scene. It ensures that only a certain number of old replicas may be down while they are being updated, and only a certain number of new replicas may be created above the desired number of pods. To learn more details about it, visit <a href=/docs/concepts/workloads/controllers/deployment/>Deployment page</a>.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/tasks/debug/debug-application/debug-running-pod/>how to use <code>kubectl</code> for application introspection and debugging</a>.</li><li>See <a href=/docs/concepts/configuration/overview/>Configuration Best Practices and Tips</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d649067a69d8d5c7e71564b42b96909e>12.3 - Cluster Networking</h1><p>Networking is a central part of Kubernetes, but it can be challenging to
understand exactly how it is expected to work. There are 4 distinct networking
problems to address:</p><ol><li>Highly-coupled container-to-container communications: this is solved by
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> and <code>localhost</code> communications.</li><li>Pod-to-Pod communications: this is the primary focus of this document.</li><li>Pod-to-Service communications: this is covered by <a href=/docs/concepts/services-networking/service/>Services</a>.</li><li>External-to-Service communications: this is also covered by Services.</li></ol><p>Kubernetes is all about sharing machines between applications. Typically,
sharing machines requires ensuring that two applications do not try to use the
same ports. Coordinating ports across multiple developers is very difficult to
do at scale and exposes users to cluster-level issues outside of their control.</p><p>Dynamic port allocation brings a lot of complications to the system - every
application has to take ports as flags, the API servers have to know how to
insert dynamic port numbers into configuration blocks, services have to know
how to find each other, etc. Rather than deal with this, Kubernetes takes a
different approach.</p><p>To learn about the Kubernetes networking model, see <a href=/docs/concepts/services-networking/>here</a>.</p><h2 id=how-to-implement-the-kubernetes-network-model>How to implement the Kubernetes network model</h2><p>The network model is implemented by the container runtime on each node. The most common container runtimes use <a href=https://github.com/containernetworking/cni>Container Network Interface</a> (CNI) plugins to manage their network and security capabilities. Many different CNI plugins exist from many different vendors. Some of these provide only basic features of adding and removing network interfaces, while others provide more sophisticated solutions, such as integration with other container orchestration systems, running multiple CNI plugins, advanced IPAM features etc.</p><p>See <a href=/docs/concepts/cluster-administration/addons/#networking-and-network-policy>this page</a> for a non-exhaustive list of networking addons supported by Kubernetes.</p><h2 id=what-s-next>What's next</h2><p>The early design of the networking model and its rationale, and some future
plans are described in more detail in the
<a href=https://git.k8s.io/design-proposals-archive/network/networking.md>networking design document</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-c4b1e87a84441f8a90699a345ce48d68>12.4 - Logging Architecture</h1><p>Application logs can help you understand what is happening inside your application. The
logs are particularly useful for debugging problems and monitoring cluster activity. Most
modern applications have some kind of logging mechanism. Likewise, container engines
are designed to support logging. The easiest and most adopted logging method for
containerized applications is writing to standard output and standard error streams.</p><p>However, the native functionality provided by a container engine or runtime is usually
not enough for a complete logging solution.</p><p>For example, you may want to access your application's logs if a container crashes,
a pod gets evicted, or a node dies.</p><p>In a cluster, logs should have a separate storage and lifecycle independent of nodes,
pods, or containers. This concept is called
<a href=#cluster-level-logging-architectures>cluster-level logging</a>.</p><p>Cluster-level logging architectures require a separate backend to store, analyze, and
query logs. Kubernetes does not provide a native storage solution for log data. Instead,
there are many logging solutions that integrate with Kubernetes. The following sections
describe how to handle and store logs on nodes.</p><h2 id=basic-logging-in-kubernetes>Pod and container logs</h2><p>Kubernetes captures logs from each container in a running Pod.</p><p>This example uses a manifest for a <code>Pod</code> with a container
that writes text to the standard output stream, once per second.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/debug/counter-pod.yaml download=debug/counter-pod.yaml><code>debug/counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("debug-counter-pod-yaml")' title="Copy debug/counter-pod.yaml to clipboard"></img></div><div class=includecode id=debug-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c,<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:#b44>&#39;i=0; while true; do echo &#34;$i: $(date)&#34;; i=$((i+1)); sleep 1; done&#39;</span>]<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>To run this pod, use the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/debug/counter-pod.yaml
</span></span></code></pre></div><p>The output is:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>pod/counter created
</span></span></span></code></pre></div><p>To fetch the logs, use the <code>kubectl logs</code> command, as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter
</span></span></code></pre></div><p>The output is similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>0: Fri Apr  1 11:42:23 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>1: Fri Apr  1 11:42:24 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>2: Fri Apr  1 11:42:25 UTC 2022
</span></span></span></code></pre></div><p>You can use <code>kubectl logs --previous</code> to retrieve logs from a previous instantiation of a container.
If your pod has multiple containers, specify which container's logs you want to access by
appending a container name to the command, with a <code>-c</code> flag, like so:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>kubectl logs counter -c count
</span></span></span></code></pre></div><p>See the <a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code> documentation</a> for more details.</p><h3 id=how-nodes-handle-container-logs>How nodes handle container logs</h3><p><img src=/images/docs/user-guide/logging/logging-node-level.png alt="Node level logging"></p><p>A container runtime handles and redirects any output generated to a containerized application's <code>stdout</code> and <code>stderr</code> streams.
Different container runtimes implement this in different ways; however, the integration with the kubelet is standardized
as the <em>CRI logging format</em>.</p><p>By default, if a container restarts, the kubelet keeps one terminated container with its logs. If a pod is evicted from the node,
all corresponding containers are also evicted, along with their logs.</p><p>The kubelet makes logs available to clients via a special feature of the Kubernetes API. The usual way to access this is
by running <code>kubectl logs</code>.</p><h3 id=log-rotation>Log rotation</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>You can configure the kubelet to rotate logs automatically.</p><p>If you configure rotation, the kubelet is responsible for rotating container logs and managing the logging directory structure.
The kubelet sends this information to the container runtime (using CRI),
and the runtime writes the container logs to the given location.</p><p>You can configure two kubelet <a href=/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration>configuration settings</a>,
<code>containerLogMaxSize</code> and <code>containerLogMaxFiles</code>,
using the <a href=/docs/tasks/administer-cluster/kubelet-config-file/>kubelet configuration file</a>.
These settings let you configure the maximum size for each log file and the maximum number of files allowed for each container respectively.</p><p>When you run <a href=/docs/reference/generated/kubectl/kubectl-commands#logs><code>kubectl logs</code></a> as in
the basic logging example, the kubelet on the node handles the request and
reads directly from the log file. The kubelet returns the content of the log file.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Only the contents of the latest log file are available through
<code>kubectl logs</code>.</p><p>For example, if a Pod writes 40 MiB of logs and the kubelet rotates logs
after 10 MiB, running <code>kubectl logs</code> returns at most 10MiB of data.</p></div><h2 id=system-component-logs>System component logs</h2><p>There are two types of system components: those that typically run in a container,
and those components directly involved in running containers. For example:</p><ul><li>The kubelet and container runtime do not run in containers. The kubelet runs
your containers (grouped together in <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=pods>pods</a>)</li><li>The Kubernetes scheduler, controller manager, and API server run within pods
(usually <a class=glossary-tooltip title='A pod managed directly by the kubelet daemon on a specific node.' data-toggle=tooltip data-placement=top href=/docs/tasks/configure-pod-container/static-pod/ target=_blank aria-label='static Pods'>static Pods</a>).
The etcd component runs in the control plane, and most commonly also as a static pod.
If your cluster uses kube-proxy, you typically run this as a <code>DaemonSet</code>.</li></ul><h3 id=log-location-node>Log locations</h3><p>The way that the kubelet and container runtime write logs depends on the operating
system that the node uses:</p><ul class="nav nav-tabs" id=log-location-node-tabs role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#log-location-node-tabs-0 role=tab aria-controls=log-location-node-tabs-0 aria-selected=true>Linux</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#log-location-node-tabs-1 role=tab aria-controls=log-location-node-tabs-1>Windows</a></li></ul><div class=tab-content id=log-location-node-tabs><div id=log-location-node-tabs-0 class="tab-pane show active" role=tabpanel aria-labelledby=log-location-node-tabs-0><p><p>On Linux nodes that use systemd, the kubelet and container runtime write to journald
by default. You use <code>journalctl</code> to read the systemd journal; for example:
<code>journalctl -u kubelet</code>.</p><p>If systemd is not present, the kubelet and container runtime write to <code>.log</code> files in the
<code>/var/log</code> directory. If you want to have logs written elsewhere, you can indirectly
run the kubelet via a helper tool, <code>kube-log-runner</code>, and use that tool to redirect
kubelet logs to a directory that you choose.</p><p>You can also set a logging directory using the deprecated kubelet command line
argument <code>--log-dir</code>. However, the kubelet always directs your container runtime to
write logs into directories within <code>/var/log/pods</code>.</p><p>For more information on <code>kube-log-runner</code>, read <a href=/docs/concepts/cluster-administration/system-logs/#klog>System Logs</a>.</p></div><div id=log-location-node-tabs-1 class=tab-pane role=tabpanel aria-labelledby=log-location-node-tabs-1><p><p>By default, the kubelet writes logs to files within the directory <code>C:\var\logs</code>
(notice that this is not <code>C:\var\log</code>).</p><p>Although <code>C:\var\log</code> is the Kubernetes default location for these logs, several
cluster deployment tools set up Windows nodes to log to <code>C:\var\log\kubelet</code> instead.</p><p>If you want to have logs written elsewhere, you can indirectly
run the kubelet via a helper tool, <code>kube-log-runner</code>, and use that tool to redirect
kubelet logs to a directory that you choose.</p><p>However, the kubelet always directs your container runtime to write logs within the
directory <code>C:\var\log\pods</code>.</p><p>For more information on <code>kube-log-runner</code>, read <a href=/docs/concepts/cluster-administration/system-logs/#klog>System Logs</a>.</p></div></div><p><br></p><p>For Kubernetes cluster components that run in pods, these write to files inside
the <code>/var/log</code> directory, bypassing the default logging mechanism (the components
do not write to the systemd journal). You can use Kubernetes' storage mechanisms
to map persistent storage into the container that runs the component.</p><p>For details about etcd and its logs, view the <a href=https://etcd.io/docs/>etcd documentation</a>.
Again, you can use Kubernetes' storage mechanisms to map persistent storage into
the container that runs the component.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>If you deploy Kubernetes cluster components (such as the scheduler) to log to
a volume shared from the parent node, you need to consider and ensure that those
logs are rotated. <strong>Kubernetes does not manage that log rotation</strong>.</p><p>Your operating system may automatically implement some log rotation - for example,
if you share the directory <code>/var/log</code> into a static Pod for a component, node-level
log rotation treats a file in that directory the same as a file written by any component
outside Kubernetes.</p><p>Some deploy tools account for that log rotation and automate it; others leave this
as your responsibility.</p></div><h2 id=cluster-level-logging-architectures>Cluster-level logging architectures</h2><p>While Kubernetes does not provide a native solution for cluster-level logging, there are several common approaches you can consider. Here are some options:</p><ul><li>Use a node-level logging agent that runs on every node.</li><li>Include a dedicated sidecar container for logging in an application pod.</li><li>Push logs directly to a backend from within an application.</li></ul><h3 id=using-a-node-logging-agent>Using a node logging agent</h3><p><img src=/images/docs/user-guide/logging/logging-with-node-agent.png alt="Using a node level logging agent"></p><p>You can implement cluster-level logging by including a <em>node-level logging agent</em> on each node. The logging agent is a dedicated tool that exposes logs or pushes logs to a backend. Commonly, the logging agent is a container that has access to a directory with log files from all of the application containers on that node.</p><p>Because the logging agent must run on every node, it is recommended to run the agent
as a <code>DaemonSet</code>.</p><p>Node-level logging creates only one agent per node and doesn't require any changes to the applications running on the node.</p><p>Containers write to stdout and stderr, but with no agreed format. A node-level agent collects these logs and forwards them for aggregation.</p><h3 id=sidecar-container-with-logging-agent>Using a sidecar container with the logging agent</h3><p>You can use a sidecar container in one of the following ways:</p><ul><li>The sidecar container streams application logs to its own <code>stdout</code>.</li><li>The sidecar container runs a logging agent, which is configured to pick up logs from an application container.</li></ul><h4 id=streaming-sidecar-container>Streaming sidecar container</h4><p><img src=/images/docs/user-guide/logging/logging-with-streaming-sidecar.png alt="Sidecar container with a streaming container"></p><p>By having your sidecar containers write to their own <code>stdout</code> and <code>stderr</code>
streams, you can take advantage of the kubelet and the logging agent that
already run on each node. The sidecar containers read logs from a file, a socket,
or journald. Each sidecar container prints a log to its own <code>stdout</code> or <code>stderr</code> stream.</p><p>This approach allows you to separate several log streams from different
parts of your application, some of which can lack support
for writing to <code>stdout</code> or <code>stderr</code>. The logic behind redirecting logs
is minimal, so it's not a significant overhead. Additionally, because
<code>stdout</code> and <code>stderr</code> are handled by the kubelet, you can use built-in tools
like <code>kubectl logs</code>.</p><p>For example, a pod runs a single container, and the container
writes to two different log files using two different formats. Here's a
manifest for the Pod:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod.yaml download=admin/logging/two-files-counter-pod.yaml><code>admin/logging/two-files-counter-pod.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-yaml")' title="Copy admin/logging/two-files-counter-pod.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>It is not recommended to write log entries with different formats to the same log
stream, even if you managed to redirect both components to the <code>stdout</code> stream of
the container. Instead, you can create two sidecar containers. Each sidecar
container could tail a particular log file from a shared volume and then redirect
the logs to its own <code>stdout</code> stream.</p><p>Here's a manifest for a pod that has two sidecar containers:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod-streaming-sidecar.yaml download=admin/logging/two-files-counter-pod-streaming-sidecar.yaml><code>admin/logging/two-files-counter-pod-streaming-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-streaming-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-streaming-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-streaming-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/1.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-log-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb> </span>[/bin/sh, -c, &#39;tail -n+1 -F /var/log/2.log&#39;]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Now when you run this pod, you can access each log stream separately by
running the following commands:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-1
</span></span></code></pre></div><p>The output is similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>0: Fri Apr  1 11:42:26 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>1: Fri Apr  1 11:42:27 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>2: Fri Apr  1 11:42:28 UTC 2022
</span></span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs counter count-log-2
</span></span></code></pre></div><p>The output is similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:#888>Fri Apr  1 11:42:29 UTC 2022 INFO 0
</span></span></span><span style=display:flex><span><span style=color:#888>Fri Apr  1 11:42:30 UTC 2022 INFO 0
</span></span></span><span style=display:flex><span><span style=color:#888>Fri Apr  1 11:42:31 UTC 2022 INFO 0
</span></span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span></code></pre></div><p>If you installed a node-level agent in your cluster, that agent picks up those log
streams automatically without any further configuration. If you like, you can configure
the agent to parse log lines depending on the source container.</p><p>Even for Pods that only have low CPU and memory usage (order of a couple of millicores
for cpu and order of several megabytes for memory), writing logs to a file and
then streaming them to <code>stdout</code> can double how much storage you need on the node.
If you have an application that writes to a single file, it's recommended to set
<code>/dev/stdout</code> as the destination rather than implement the streaming sidecar
container approach.</p><p>Sidecar containers can also be used to rotate log files that cannot be rotated by
the application itself. An example of this approach is a small container running
<code>logrotate</code> periodically.
However, it's more straightforward to use <code>stdout</code> and <code>stderr</code> directly, and
leave rotation and retention policies to the kubelet.</p><h4 id=sidecar-container-with-a-logging-agent>Sidecar container with a logging agent</h4><p><img src=/images/docs/user-guide/logging/logging-with-sidecar-agent.png alt="Sidecar container with a logging agent"></p><p>If the node-level logging agent is not flexible enough for your situation, you
can create a sidecar container with a separate logging agent that you have
configured specifically to run with your application.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Using a logging agent in a sidecar container can lead
to significant resource consumption. Moreover, you won't be able to access
those logs using <code>kubectl logs</code> because they are not controlled
by the kubelet.</div><p>Here are two example manifests that you can use to implement a sidecar container with a logging agent.
The first manifest contains a <a href=/docs/tasks/configure-pod-container/configure-pod-configmap/><code>ConfigMap</code></a>
to configure fluentd.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/fluentd-sidecar-config.yaml download=admin/logging/fluentd-sidecar-config.yaml><code>admin/logging/fluentd-sidecar-config.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-fluentd-sidecar-config-yaml")' title="Copy admin/logging/fluentd-sidecar-config.yaml to clipboard"></img></div><div class=includecode id=admin-logging-fluentd-sidecar-config-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ConfigMap<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>data</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>fluentd.conf</span>:<span style=color:#bbb> </span>|<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/1.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/1.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format1
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type tail
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      format none
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      path /var/log/2.log
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      pos_file /var/log/2.log.pos
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      tag count.format2
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/source&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;match **&gt;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      type google_cloud
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>    &lt;/match&gt;</span><span style=color:#bbb>    
</span></span></span></code></pre></div></div></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In the sample configurations, you can replace fluentd with any logging agent, reading
from any source inside an application container.</div><p>The second manifest describes a pod that has a sidecar container running fluentd.
The pod mounts a volume where fluentd can pick up its configuration data.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/logging/two-files-counter-pod-agent-sidecar.yaml download=admin/logging/two-files-counter-pod-agent-sidecar.yaml><code>admin/logging/two-files-counter-pod-agent-sidecar.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("admin-logging-two-files-counter-pod-agent-sidecar-yaml")' title="Copy admin/logging/two-files-counter-pod-agent-sidecar.yaml to clipboard"></img></div><div class=includecode id=admin-logging-two-files-counter-pod-agent-sidecar-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>counter<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- &gt;<span style=color:#b44;font-style:italic>
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      i=0;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      while true;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      do
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$i: $(date)&#34; &gt;&gt; /var/log/1.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        echo &#34;$(date) INFO $i&#34; &gt;&gt; /var/log/2.log;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        i=$((i+1));
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>        sleep 1;
</span></span></span><span style=display:flex><span><span style=color:#b44;font-style:italic>      done</span><span style=color:#bbb>      
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>count-agent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/fluentd-gcp:1.30<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>FLUENTD_ARGS<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span>-c /etc/fluentd-config/fluentd.conf<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/etc/fluentd-config<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>emptyDir</span>:<span style=color:#bbb> </span>{}<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>config-volume<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>configMap</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-config<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h3 id=exposing-logs-directly-from-the-application>Exposing logs directly from the application</h3><p><img src=/images/docs/user-guide/logging/logging-from-application.png alt="Exposing logs directly from the application"></p><p>Cluster-logging that exposes or pushes logs directly from every application is outside the scope of Kubernetes.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/cluster-administration/system-logs/>Kubernetes system logs</a></li><li>Learn about <a href=/docs/concepts/cluster-administration/system-traces/>Traces For Kubernetes System Components</a></li><li>Learn how to <a href=/docs/tasks/debug/debug-application/determine-reason-pod-failure/#customizing-the-termination-message>customise the termination message</a> that Kubernetes records when a Pod fails</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cbfd3654996eae9fcdef009f70fa83f0>12.5 - Metrics For Kubernetes System Components</h1><p>System component metrics can give a better look into what is happening inside them. Metrics are
particularly useful for building dashboards and alerts.</p><p>Kubernetes components emit metrics in <a href=https://prometheus.io/docs/instrumenting/exposition_formats/>Prometheus format</a>.
This format is structured plain text, designed so that people and machines can both read it.</p><h2 id=metrics-in-kubernetes>Metrics in Kubernetes</h2><p>In most cases metrics are available on <code>/metrics</code> endpoint of the HTTP server. For components that
doesn't expose endpoint by default it can be enabled using <code>--bind-address</code> flag.</p><p>Examples of those components:</p><ul><li><a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li><li><a class=glossary-tooltip title='kube-proxy is a network proxy that runs on each node in the cluster.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-proxy/ target=_blank aria-label=kube-proxy>kube-proxy</a></li><li><a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li><li><a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li><li><a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a></li></ul><p>In a production environment you may want to configure <a href=https://prometheus.io/>Prometheus Server</a>
or some other metrics scraper to periodically gather these metrics and make them available in some
kind of time series database.</p><p>Note that <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> also exposes metrics in
<code>/metrics/cadvisor</code>, <code>/metrics/resource</code> and <code>/metrics/probes</code> endpoints. Those metrics do not
have same lifecycle.</p><p>If your cluster uses <a class=glossary-tooltip title='Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/rbac/ target=_blank aria-label=RBAC>RBAC</a>, reading metrics requires
authorization via a user, group or ServiceAccount with a ClusterRole that allows accessing
<code>/metrics</code>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>rbac.authorization.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ClusterRole<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>prometheus<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>nonResourceURLs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/metrics&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- get<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=metric-lifecycle>Metric lifecycle</h2><p>Alpha metric → Stable metric → Deprecated metric → Hidden metric → Deleted metric</p><p>Alpha metrics have no stability guarantees. These metrics can be modified or deleted at any time.</p><p>Stable metrics are guaranteed to not change. This means:</p><ul><li>A stable metric without a deprecated signature will not be deleted or renamed</li><li>A stable metric's type will not be modified</li></ul><p>Deprecated metrics are slated for deletion, but are still available for use.
These metrics include an annotation about the version in which they became deprecated.</p><p>For example:</p><ul><li><p>Before deprecation</p><pre tabindex=0><code># HELP some_counter this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li><li><p>After deprecation</p><pre tabindex=0><code># HELP some_counter (Deprecated since 1.15.0) this counts things
# TYPE some_counter counter
some_counter 0
</code></pre></li></ul><p>Hidden metrics are no longer published for scraping, but are still available for use. To use a
hidden metric, please refer to the <a href=#show-hidden-metrics>Show hidden metrics</a> section.</p><p>Deleted metrics are no longer published and cannot be used.</p><h2 id=show-hidden-metrics>Show hidden metrics</h2><p>As described above, admins can enable hidden metrics through a command-line flag on a specific
binary. This intends to be used as an escape hatch for admins if they missed the migration of the
metrics deprecated in the last release.</p><p>The flag <code>show-hidden-metrics-for-version</code> takes a version for which you want to show metrics
deprecated in that release. The version is expressed as x.y, where x is the major version, y is
the minor version. The patch version is not needed even though a metrics can be deprecated in a
patch release, the reason for that is the metrics deprecation policy runs against the minor release.</p><p>The flag can only take the previous minor version as it's value. All metrics hidden in previous
will be emitted if admins set the previous version to <code>show-hidden-metrics-for-version</code>. The too
old version is not allowed because this violates the metrics deprecated policy.</p><p>Take metric <code>A</code> as an example, here assumed that <code>A</code> is deprecated in 1.n. According to metrics
deprecated policy, we can reach the following conclusion:</p><ul><li>In release <code>1.n</code>, the metric is deprecated, and it can be emitted by default.</li><li>In release <code>1.n+1</code>, the metric is hidden by default and it can be emitted by command line
<code>show-hidden-metrics-for-version=1.n</code>.</li><li>In release <code>1.n+2</code>, the metric should be removed from the codebase. No escape hatch anymore.</li></ul><p>If you're upgrading from release <code>1.12</code> to <code>1.13</code>, but still depend on a metric <code>A</code> deprecated in
<code>1.12</code>, you should set hidden metrics via command line: <code>--show-hidden-metrics=1.12</code> and remember
to remove this metric dependency before upgrading to <code>1.14</code></p><h2 id=disable-accelerator-metrics>Disable accelerator metrics</h2><p>The kubelet collects accelerator metrics through cAdvisor. To collect these metrics, for
accelerators like NVIDIA GPUs, kubelet held an open handle on the driver. This meant that in order
to perform infrastructure changes (for example, updating the driver), a cluster administrator
needed to stop the kubelet agent.</p><p>The responsibility for collecting accelerator metrics now belongs to the vendor rather than the
kubelet. Vendors must provide a container that collects metrics and exposes them to the metrics
service (for example, Prometheus).</p><p>The <a href=/docs/reference/command-line-tools-reference/feature-gates/><code>DisableAcceleratorUsageMetrics</code> feature gate</a>
disables metrics collected by the kubelet, with a
<a href=https://github.com/kubernetes/enhancements/tree/411e51027db842355bd489691af897afc1a41a5e/keps/sig-node/1867-disable-accelerator-usage-metrics#graduation-criteria>timeline for enabling this feature by default</a>.</p><h2 id=component-metrics>Component metrics</h2><h3 id=kube-controller-manager-metrics>kube-controller-manager metrics</h3><p>Controller manager metrics provide important insight into the performance and health of the
controller manager. These metrics include common Go language runtime metrics such as go_routine
count and controller specific metrics such as etcd request latencies or Cloudprovider (AWS, GCE,
OpenStack) API latencies that can be used to gauge the health of a cluster.</p><p>Starting from Kubernetes 1.7, detailed Cloudprovider metrics are available for storage operations
for GCE, AWS, Vsphere and OpenStack.
These metrics can be used to monitor health of persistent volume operations.</p><p>For example, for GCE these metrics are called:</p><pre tabindex=0><code>cloudprovider_gce_api_request_duration_seconds { request = &#34;instance_list&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_insert&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;disk_delete&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;attach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;detach_disk&#34;}
cloudprovider_gce_api_request_duration_seconds { request = &#34;list_disk&#34;}
</code></pre><h3 id=kube-scheduler-metrics>kube-scheduler metrics</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [beta]</code></div><p>The scheduler exposes optional metrics that reports the requested resources and the desired limits
of all running pods. These metrics can be used to build capacity planning dashboards, assess
current or historical scheduling limits, quickly identify workloads that cannot schedule due to
lack of resources, and compare actual usage to the pod's request.</p><p>The kube-scheduler identifies the resource <a href=/docs/concepts/configuration/manage-resources-containers/>requests and limits</a>
configured for each Pod; when either a request or limit is non-zero, the kube-scheduler reports a
metrics timeseries. The time series is labelled by:</p><ul><li>namespace</li><li>pod name</li><li>the node where the pod is scheduled or an empty string if not yet scheduled</li><li>priority</li><li>the assigned scheduler for that pod</li><li>the name of the resource (for example, <code>cpu</code>)</li><li>the unit of the resource if known (for example, <code>cores</code>)</li></ul><p>Once a pod reaches completion (has a <code>restartPolicy</code> of <code>Never</code> or <code>OnFailure</code> and is in the
<code>Succeeded</code> or <code>Failed</code> pod phase, or has been deleted and all containers have a terminated state)
the series is no longer reported since the scheduler is now free to schedule other pods to run.
The two metrics are called <code>kube_pod_resource_request</code> and <code>kube_pod_resource_limit</code>.</p><p>The metrics are exposed at the HTTP endpoint <code>/metrics/resources</code> and require the same
authorization as the <code>/metrics</code> endpoint on the scheduler. You must use the
<code>--show-hidden-metrics-for-version=1.20</code> flag to expose these alpha stability metrics.</p><h2 id=disabling-metrics>Disabling metrics</h2><p>You can explicitly turn off metrics via command line flag <code>--disabled-metrics</code>. This may be
desired if, for example, a metric is causing a performance problem. The input is a list of
disabled metrics (i.e. <code>--disabled-metrics=metric1,metric2</code>).</p><h2 id=metric-cardinality-enforcement>Metric cardinality enforcement</h2><p>Metrics with unbounded dimensions could cause memory issues in the components they instrument. To
limit resource use, you can use the <code>--allow-label-value</code> command line option to dynamically
configure an allow-list of label values for a metric.</p><p>In alpha stage, the flag can only take in a series of mappings as metric label allow-list.
Each mapping is of the format <code>&lt;metric_name>,&lt;label_name>=&lt;allowed_labels></code> where
<code>&lt;allowed_labels></code> is a comma-separated list of acceptable label names.</p><p>The overall format looks like:</p><pre tabindex=0><code>--allow-label-value &lt;metric_name&gt;,&lt;label_name&gt;=&#39;&lt;allow_value1&gt;, &lt;allow_value2&gt;...&#39;, &lt;metric_name2&gt;,&lt;label_name&gt;=&#39;&lt;allow_value1&gt;, &lt;allow_value2&gt;...&#39;, ...
</code></pre><p>Here is an example:</p><pre tabindex=0><code class=language-none data-lang=none>--allow-label-value number_count_metric,odd_number=&#39;1,3,5&#39;, number_count_metric,even_number=&#39;2,4,6&#39;, date_gauge_metric,weekend=&#39;Saturday,Sunday&#39;
</code></pre><h2 id=what-s-next>What's next</h2><ul><li>Read about the <a href=https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md#text-based-format>Prometheus text format</a>
for metrics</li><li>See the list of <a href=https://github.com/kubernetes/kubernetes/blob/master/test/instrumentation/testdata/stable-metrics-list.yaml>stable Kubernetes metrics</a></li><li>Read about the <a href=/docs/reference/using-api/deprecation-policy/#deprecating-a-feature-or-behavior>Kubernetes deprecation policy</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-5cc31ecfba86467f8884856412cfb6b2>12.6 - System Logs</h1><p>System component logs record events happening in cluster, which can be very useful for debugging.
You can configure log verbosity to see more or less detail.
Logs can be as coarse-grained as showing errors within a component, or as fine-grained as showing
step-by-step traces of events (like HTTP access logs, pod state changes, controller actions, or
scheduler decisions).</p><h2 id=klog>Klog</h2><p>klog is the Kubernetes logging library. <a href=https://github.com/kubernetes/klog>klog</a>
generates log messages for the Kubernetes system components.</p><p>For more information about klog configuration, see the <a href=/docs/reference/command-line-tools-reference/>Command line tool reference</a>.</p><p>Kubernetes is in the process of simplifying logging in its components.
The following klog command line flags
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components>are deprecated</a>
starting with Kubernetes 1.23 and will be removed in a future release:</p><ul><li><code>--add-dir-header</code></li><li><code>--alsologtostderr</code></li><li><code>--log-backtrace-at</code></li><li><code>--log-dir</code></li><li><code>--log-file</code></li><li><code>--log-file-max-size</code></li><li><code>--logtostderr</code></li><li><code>--one-output</code></li><li><code>--skip-headers</code></li><li><code>--skip-log-headers</code></li><li><code>--stderrthreshold</code></li></ul><p>Output will always be written to stderr, regardless of the output format. Output redirection is
expected to be handled by the component which invokes a Kubernetes component. This can be a POSIX
shell or a tool like systemd.</p><p>In some cases, for example a distroless container or a Windows system service, those options are
not available. Then the
<a href=https://github.com/kubernetes/kubernetes/blob/d2a8a81639fcff8d1221b900f66d28361a170654/staging/src/k8s.io/component-base/logs/kube-log-runner/README.md><code>kube-log-runner</code></a>
binary can be used as wrapper around a Kubernetes component to redirect
output. A prebuilt binary is included in several Kubernetes base images under
its traditional name as <code>/go-runner</code> and as <code>kube-log-runner</code> in server and
node release archives.</p><p>This table shows how <code>kube-log-runner</code> invocations correspond to shell redirection:</p><table><thead><tr><th>Usage</th><th>POSIX shell (such as bash)</th><th><code>kube-log-runner &lt;options> &lt;cmd></code></th></tr></thead><tbody><tr><td>Merge stderr and stdout, write to stdout</td><td><code>2>&1</code></td><td><code>kube-log-runner</code> (default behavior)</td></tr><tr><td>Redirect both into log file</td><td><code>1>>/tmp/log 2>&1</code></td><td><code>kube-log-runner -log-file=/tmp/log</code></td></tr><tr><td>Copy into log file and to stdout</td><td><code>2>&1 | tee -a /tmp/log</code></td><td><code>kube-log-runner -log-file=/tmp/log -also-stdout</code></td></tr><tr><td>Redirect only stdout into log file</td><td><code>>/tmp/log</code></td><td><code>kube-log-runner -log-file=/tmp/log -redirect-stderr=false</code></td></tr></tbody></table><h3 id=klog-output>Klog output</h3><p>An example of the traditional klog native format:</p><pre tabindex=0><code>I1025 00:15:15.525108       1 httplog.go:79] GET /api/v1/namespaces/kube-system/pods/metrics-server-v0.3.1-57c75779f-9p8wg: (1.512ms) 200 [pod_nanny/v0.0.0 (linux/amd64) kubernetes/$Format 10.56.1.19:51756]
</code></pre><p>The message string may contain line breaks:</p><pre tabindex=0><code>I1025 00:15:15.525108       1 example.go:79] This is a message
which has a line break.
</code></pre><h3 id=structured-logging>Structured Logging</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>Migration to structured log messages is an ongoing process. Not all log messages are structured in
this version. When parsing log files, you must also handle unstructured log messages.</p><p>Log formatting and value serialization are subject to change.</p></div><p>Structured logging introduces a uniform structure in log messages allowing for programmatic
extraction of information. You can store and process structured logs with less effort and cost.
The code which generates a log message determines whether it uses the traditional unstructured
klog output or structured logging.</p><p>The default formatting of structured log messages is as text, with a format that is backward
compatible with traditional klog:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#b44>&lt;klog header&gt; &#34;&lt;message&gt;&#34; &lt;key1&gt;</span><span style=color:#666>=</span><span style=color:#b44>&#34;&lt;value1&gt;&#34; &lt;key2&gt;=&#34;&lt;value2&gt;&#34; ...</span>
</span></span></code></pre></div><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#b44>I1025 00:15:15.525108       1 controller_utils.go:116] &#34;Pod status updated&#34; pod</span><span style=color:#666>=</span><span style=color:#b44>&#34;kube-system/kubedns&#34; status=&#34;ready&#34;</span>
</span></span></code></pre></div><p>Strings are quoted. Other values are formatted with
<a href=https://pkg.go.dev/fmt#hdr-Printing><code>%+v</code></a>, which may cause log messages to
continue on the next line <a href=https://github.com/kubernetes/kubernetes/issues/106428>depending on the data</a>.</p><pre tabindex=0><code>I1025 00:15:15.525108       1 example.go:116] &#34;Example&#34; data=&#34;This is text with a line break\nand \&#34;quotation marks\&#34;.&#34; someInt=1 someFloat=0.1 someStruct={StringField: First line,
second line.}
</code></pre><h3 id=contextual-logging>Contextual Logging</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [alpha]</code></div><p>Contextual logging builds on top of structured logging. It is primarily about
how developers use logging calls: code based on that concept is more flexible
and supports additional use cases as described in the <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging>Contextual Logging
KEP</a>.</p><p>If developers use additional functions like <code>WithValues</code> or <code>WithName</code> in
their components, then log entries contain additional information that gets
passed into functions by their caller.</p><p>Currently this is gated behind the <code>StructuredLogging</code> feature gate and
disabled by default. The infrastructure for this was added in 1.24 without
modifying components. The
<a href=https://github.com/kubernetes/kubernetes/blob/v1.24.0-beta.0/staging/src/k8s.io/component-base/logs/example/cmd/logger.go><code>component-base/logs/example</code></a>
command demonstrates how to use the new logging calls and how a component
behaves that supports contextual logging.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:navy;font-weight:700>$</span> <span style=color:#a2f>cd</span> <span style=color:#b8860b>$GOPATH</span>/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/logs/example/cmd/
</span></span><span style=display:flex><span><span style=color:navy;font-weight:700>$</span> go run . --help
</span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span><span style=display:flex><span><span style=color:#888>      --feature-gates mapStringBool  A set of key=value pairs that describe feature gates for alpha/experimental features. Options are:
</span></span></span><span style=display:flex><span><span style=color:#888>                                     AllAlpha=true|false (ALPHA - default=false)
</span></span></span><span style=display:flex><span><span style=color:#888>                                     AllBeta=true|false (BETA - default=false)
</span></span></span><span style=display:flex><span><span style=color:#888>                                     ContextualLogging=true|false (ALPHA - default=false)
</span></span></span><span style=display:flex><span><span style=color:#888></span><span style=color:navy;font-weight:700>$</span> go run . --feature-gates <span style=color:#b8860b>ContextualLogging</span><span style=color:#666>=</span><span style=color:#a2f>true</span>
</span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span><span style=display:flex><span><span style=color:#888>I0404 18:00:02.916429  451895 logger.go:94] &#34;example/myname: runtime&#34; foo=&#34;bar&#34; duration=&#34;1m0s&#34;
</span></span></span><span style=display:flex><span><span style=color:#888>I0404 18:00:02.916447  451895 logger.go:95] &#34;example: another runtime&#34; foo=&#34;bar&#34; duration=&#34;1m0s&#34;
</span></span></span></code></pre></div><p>The <code>example</code> prefix and <code>foo="bar"</code> were added by the caller of the function
which logs the <code>runtime</code> message and <code>duration="1m0s"</code> value, without having to
modify that function.</p><p>With contextual logging disable, <code>WithValues</code> and <code>WithName</code> do nothing and log
calls go through the global klog logger. Therefore this additional information
is not in the log output anymore:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-console data-lang=console><span style=display:flex><span><span style=color:navy;font-weight:700>$</span> go run . --feature-gates <span style=color:#b8860b>ContextualLogging</span><span style=color:#666>=</span><span style=color:#a2f>false</span>
</span></span><span style=display:flex><span><span style=color:#888>...
</span></span></span><span style=display:flex><span><span style=color:#888>I0404 18:03:31.171945  452150 logger.go:94] &#34;runtime&#34; duration=&#34;1m0s&#34;
</span></span></span><span style=display:flex><span><span style=color:#888>I0404 18:03:31.171962  452150 logger.go:95] &#34;another runtime&#34; duration=&#34;1m0s&#34;
</span></span></span></code></pre></div><h3 id=json-log-format>JSON log format</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [alpha]</code></div><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>JSON output does not support many standard klog flags. For list of unsupported klog flags, see the
<a href=/docs/reference/command-line-tools-reference/>Command line tool reference</a>.</p><p>Not all logs are guaranteed to be written in JSON format (for example, during process start).
If you intend to parse logs, make sure you can handle log lines that are not JSON as well.</p><p>Field names and JSON serialization are subject to change.</p></div><p>The <code>--logging-format=json</code> flag changes the format of logs from klog native format to JSON format.
Example of JSON log format (pretty printed):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;ts&#34;</span>: <span style=color:#666>1580306777.04728</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;v&#34;</span>: <span style=color:#666>4</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;msg&#34;</span>: <span style=color:#b44>&#34;Pod status updated&#34;</span>,
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;pod&#34;</span>:{
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;nginx-1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
</span></span><span style=display:flex><span>   },
</span></span><span style=display:flex><span>   <span style=color:green;font-weight:700>&#34;status&#34;</span>: <span style=color:#b44>&#34;ready&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Keys with special meaning:</p><ul><li><code>ts</code> - timestamp as Unix time (required, float)</li><li><code>v</code> - verbosity (only for info and not for error messages, int)</li><li><code>err</code> - error string (optional, string)</li><li><code>msg</code> - message (required, string)</li></ul><p>List of components currently supporting JSON format:</p><ul><li><a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a></li><li><a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a></li><li><a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=kube-scheduler>kube-scheduler</a></li><li><a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a></li></ul><h3 id=log-verbosity-level>Log verbosity level</h3><p>The <code>-v</code> flag controls log verbosity. Increasing the value increases the number of logged events.
Decreasing the value decreases the number of logged events. Increasing verbosity settings logs
increasingly less severe events. A verbosity setting of 0 logs only critical events.</p><h3 id=log-location>Log location</h3><p>There are two types of system components: those that run in a container and those
that do not run in a container. For example:</p><ul><li>The Kubernetes scheduler and kube-proxy run in a container.</li><li>The kubelet and <a class=glossary-tooltip title='The container runtime is the software that is responsible for running containers.' data-toggle=tooltip data-placement=top href=/docs/setup/production-environment/container-runtimes target=_blank aria-label='container runtime'>container runtime</a>
do not run in containers.</li></ul><p>On machines with systemd, the kubelet and container runtime write to journald.
Otherwise, they write to <code>.log</code> files in the <code>/var/log</code> directory.
System components inside containers always write to <code>.log</code> files in the <code>/var/log</code> directory,
bypassing the default logging mechanism.
Similar to the container logs, you should rotate system component logs in the <code>/var/log</code> directory.
In Kubernetes clusters created by the <code>kube-up.sh</code> script, log rotation is configured by the <code>logrotate</code> tool.
The <code>logrotate</code> tool rotates logs daily, or once the log size is greater than 100MB.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about the <a href=/docs/concepts/cluster-administration/logging/>Kubernetes Logging Architecture</a></li><li>Read about <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/1602-structured-logging>Structured Logging</a></li><li>Read about <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/3077-contextual-logging>Contextual Logging</a></li><li>Read about <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components>deprecation of klog flags</a></li><li>Read about the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/logging.md>Conventions for logging severity</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-3da54ad355f6fe6574d67bd9a9a42bcb>12.7 - Traces For Kubernetes System Components</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [alpha]</code></div><p>System component traces record the latency of and relationships between operations in the cluster.</p><p>Kubernetes components emit traces using the
<a href=https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md#opentelemetry-protocol-specification>OpenTelemetry Protocol</a>
with the gRPC exporter and can be collected and routed to tracing backends using an
<a href=https://github.com/open-telemetry/opentelemetry-collector#-opentelemetry-collector>OpenTelemetry Collector</a>.</p><h2 id=trace-collection>Trace Collection</h2><p>For a complete guide to collecting traces and using the collector, see
<a href=https://opentelemetry.io/docs/collector/getting-started/>Getting Started with the OpenTelemetry Collector</a>.
However, there are a few things to note that are specific to Kubernetes components.</p><p>By default, Kubernetes components export traces using the grpc exporter for OTLP on the
<a href="https://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=opentelemetry">IANA OpenTelemetry port</a>, 4317.
As an example, if the collector is running as a sidecar to a Kubernetes component,
the following receiver configuration will collect spans and log them to standard output:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>receivers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>otlp</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>protocols</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>grpc</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>exporters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Replace this exporter with the exporter for your backend</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>logging</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>logLevel</span>:<span style=color:#bbb> </span>debug<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>service</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>pipelines</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>traces</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>receivers</span>:<span style=color:#bbb> </span>[otlp]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>exporters</span>:<span style=color:#bbb> </span>[logging]<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=component-traces>Component traces</h2><h3 id=kube-apiserver-traces>kube-apiserver traces</h3><p>The kube-apiserver generates spans for incoming HTTP requests, and for outgoing requests
to webhooks, etcd, and re-entrant requests. It propagates the
<a href=https://www.w3.org/TR/trace-context/>W3C Trace Context</a> with outgoing requests
but does not make use of the trace context attached to incoming requests,
as the kube-apiserver is often a public endpoint.</p><h4 id=enabling-tracing-in-the-kube-apiserver>Enabling tracing in the kube-apiserver</h4><p>To enable tracing, enable the <code>APIServerTracing</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the kube-apiserver. Also, provide the kube-apiserver with a tracing configuration file
with <code>--tracing-config-file=&lt;path-to-config></code>. This is an example config that records
spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apiserver.config.k8s.io/v1alpha1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>TracingConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># default value</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic>#endpoint: localhost:4317</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>samplingRatePerMillion</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>For more information about the <code>TracingConfiguration</code> struct, see
<a href=/docs/reference/config-api/apiserver-config.v1alpha1/#apiserver-k8s-io-v1alpha1-TracingConfiguration>API server config API (v1alpha1)</a>.</p><h3 id=kubelet-traces>kubelet traces</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><p>The kubelet CRI interface and authenticated http servers are instrumented to generate
trace spans. As with the apiserver, the endpoint and sampling rate are configurable.
Trace context propagation is also configured. A parent span's sampling decision is always respected.
A provided tracing configuration sampling rate will apply to spans without a parent.
Enabled without a configured endpoint, the default OpenTelemetry Collector reciever address of "localhost:4317" is set.</p><h4 id=enabling-tracing-in-the-kubelet>Enabling tracing in the kubelet</h4><p>To enable tracing, enable the <code>KubeletTracing</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the kubelet. Also, provide the kubelet with a
<a href=https://github.com/kubernetes/component-base/blob/release-1.25/tracing/api/v1/types.go>tracing configuration</a>.
This is an example snippet of a kubelet config that records spans for 1 in 10000 requests, and uses the default OpenTelemetry endpoint:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>featureGates</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>KubeletTracing</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>tracing</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># default value</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic>#endpoint: localhost:4317</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>samplingRatePerMillion</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=stability>Stability</h2><p>Tracing instrumentation is still under active development, and may change
in a variety of ways. This includes span names, attached attributes,
instrumented endpoints, etc. Until this feature graduates to stable,
there are no guarantees of backwards compatibility for tracing instrumentation.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=https://opentelemetry.io/docs/collector/getting-started/>Getting Started with the OpenTelemetry Collector</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-08e94e6a480e0d6b2de72d84a1b97617>12.8 - Proxies in Kubernetes</h1><p>This page explains proxies used with Kubernetes.</p><h2 id=proxies>Proxies</h2><p>There are several different proxies you may encounter when using Kubernetes:</p><ol><li><p>The <a href=/docs/tasks/access-application-cluster/access-cluster/#directly-accessing-the-rest-api>kubectl proxy</a>:</p><ul><li>runs on a user's desktop or in a pod</li><li>proxies from a localhost address to the Kubernetes apiserver</li><li>client to proxy uses HTTP</li><li>proxy to apiserver uses HTTPS</li><li>locates apiserver</li><li>adds authentication headers</li></ul></li><li><p>The <a href=/docs/tasks/access-application-cluster/access-cluster-services/#discovering-builtin-services>apiserver proxy</a>:</p><ul><li>is a bastion built into the apiserver</li><li>connects a user outside of the cluster to cluster IPs which otherwise might not be reachable</li><li>runs in the apiserver processes</li><li>client to proxy uses HTTPS (or http if apiserver so configured)</li><li>proxy to target may use HTTP or HTTPS as chosen by proxy using available information</li><li>can be used to reach a Node, Pod, or Service</li><li>does load balancing when used to reach a Service</li></ul></li><li><p>The <a href=/docs/concepts/services-networking/service/#ips-and-vips>kube proxy</a>:</p><ul><li>runs on each node</li><li>proxies UDP, TCP and SCTP</li><li>does not understand HTTP</li><li>provides load balancing</li><li>is only used to reach services</li></ul></li><li><p>A Proxy/Load-balancer in front of apiserver(s):</p><ul><li>existence and implementation varies from cluster to cluster (e.g. nginx)</li><li>sits between all clients and one or more apiservers</li><li>acts as load balancer if there are several apiservers.</li></ul></li><li><p>Cloud Load Balancers on external services:</p><ul><li>are provided by some cloud providers (e.g. AWS ELB, Google Cloud Load Balancer)</li><li>are created automatically when the Kubernetes service has type <code>LoadBalancer</code></li><li>usually supports UDP/TCP only</li><li>SCTP support is up to the load balancer implementation of the cloud provider</li><li>implementation varies by cloud provider.</li></ul></li></ol><p>Kubernetes users will typically not need to worry about anything other than the first two types. The cluster admin
will typically ensure that the latter types are set up correctly.</p><h2 id=requesting-redirects>Requesting redirects</h2><p>Proxies have replaced redirect capabilities. Redirects have been deprecated.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-31c9327d2332c585341b64ddafa19cdd>12.9 - API Priority and Fairness</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code></div><p>Controlling the behavior of the Kubernetes API server in an overload situation
is a key task for cluster administrators. The <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a> has some controls available
(i.e. the <code>--max-requests-inflight</code> and <code>--max-mutating-requests-inflight</code>
command-line flags) to limit the amount of outstanding work that will be
accepted, preventing a flood of inbound requests from overloading and
potentially crashing the API server, but these flags are not enough to ensure
that the most important requests get through in a period of high traffic.</p><p>The API Priority and Fairness feature (APF) is an alternative that improves upon
aforementioned max-inflight limitations. APF classifies
and isolates requests in a more fine-grained way. It also introduces
a limited amount of queuing, so that no requests are rejected in cases
of very brief bursts. Requests are dispatched from queues using a
fair queuing technique so that, for example, a poorly-behaved
<a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> need not
starve others (even at the same priority level).</p><p>This feature is designed to work well with standard controllers, which
use informers and react to failures of API requests with exponential
back-off, and other clients that also work this way.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Some requests classified as "long-running"—such as remote
command execution or log tailing—are not subject to the API
Priority and Fairness filter. This is also true for the
<code>--max-requests-inflight</code> flag without the API Priority and Fairness
feature enabled. API Priority and Fairness <em>does</em> apply to <strong>watch</strong>
requests. When API Priority and Fairness is disabled, <strong>watch</strong> requests
are not subject to the <code>--max-requests-inflight</code> limit.</div><h2 id=enabling-disabling-api-priority-and-fairness>Enabling/Disabling API Priority and Fairness</h2><p>The API Priority and Fairness feature is controlled by a feature gate
and is enabled by default. See <a href=/docs/reference/command-line-tools-reference/feature-gates/>Feature
Gates</a>
for a general explanation of feature gates and how to enable and
disable them. The name of the feature gate for APF is
"APIPriorityAndFairness". This feature also involves an <a class=glossary-tooltip title='A set of related paths in the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning target=_blank aria-label='API Group'>API Group</a> with: (a) a
<code>v1alpha1</code> version and a <code>v1beta1</code> version, disabled by default, and
(b) <code>v1beta2</code> and <code>v1beta3</code> versions, enabled by default. You can
disable the feature gate and API group beta versions by adding the
following command-line flags to your <code>kube-apiserver</code> invocation:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kube-apiserver <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--feature-gates<span style=color:#666>=</span><span style=color:#b8860b>APIPriorityAndFairness</span><span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>--runtime-config<span style=color:#666>=</span>flowcontrol.apiserver.k8s.io/v1beta2<span style=color:#666>=</span>false,flowcontrol.apiserver.k8s.io/v1beta3<span style=color:#666>=</span><span style=color:#a2f>false</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span> <span style=color:#080;font-style:italic># …and other flags as usual</span>
</span></span></code></pre></div><p>Alternatively, you can enable the v1alpha1 and v1beta1 versions of the API group
with <code>--runtime-config=flowcontrol.apiserver.k8s.io/v1alpha1=true,flowcontrol.apiserver.k8s.io/v1beta1=true</code>.</p><p>The command-line flag <code>--enable-priority-and-fairness=false</code> will disable the
API Priority and Fairness feature, even if other flags have enabled it.</p><h2 id=concepts>Concepts</h2><p>There are several distinct features involved in the API Priority and Fairness
feature. Incoming requests are classified by attributes of the request using
<em>FlowSchemas</em>, and assigned to priority levels. Priority levels add a degree of
isolation by maintaining separate concurrency limits, so that requests assigned
to different priority levels cannot starve each other. Within a priority level,
a fair-queuing algorithm prevents requests from different <em>flows</em> from starving
each other, and allows for requests to be queued to prevent bursty traffic from
causing failed requests when the average load is acceptably low.</p><h3 id=priority-levels>Priority Levels</h3><p>Without APF enabled, overall concurrency in the API server is limited by the
<code>kube-apiserver</code> flags <code>--max-requests-inflight</code> and
<code>--max-mutating-requests-inflight</code>. With APF enabled, the concurrency limits
defined by these flags are summed and then the sum is divided up among a
configurable set of <em>priority levels</em>. Each incoming request is assigned to a
single priority level, and each priority level will only dispatch as many
concurrent requests as its particular limit allows.</p><p>The default configuration, for example, includes separate priority levels for
leader-election requests, requests from built-in controllers, and requests from
Pods. This means that an ill-behaved Pod that floods the API server with
requests cannot prevent leader election or actions by the built-in controllers
from succeeding.</p><p>The concurrency limits of the priority levels are periodically
adjusted, allowing under-utilized priority levels to temporarily lend
concurrency to heavily-utilized levels. These limits are based on
nominal limits and bounds on how much concurrency a priority level may
lend and how much it may borrow, all derived from the configuration
objects mentioned below.</p><h3 id=seats-occupied-by-a-request>Seats Occupied by a Request</h3><p>The above description of concurrency management is the baseline story.
In it, requests have different durations but are counted equally at
any given moment when comparing against a priority level's concurrency
limit. In the baseline story, each request occupies one unit of
concurrency. The word "seat" is used to mean one unit of concurrency,
inspired by the way each passenger on a train or aircraft takes up one
of the fixed supply of seats.</p><p>But some requests take up more than one seat. Some of these are <strong>list</strong>
requests that the server estimates will return a large number of
objects. These have been found to put an exceptionally heavy burden
on the server, among requests that take a similar amount of time to
run. For this reason, the server estimates the number of objects that
will be returned and considers the request to take a number of seats
that is proportional to that estimated number.</p><h3 id=execution-time-tweaks-for-watch-requests>Execution time tweaks for watch requests</h3><p>API Priority and Fairness manages <strong>watch</strong> requests, but this involves a
couple more excursions from the baseline behavior. The first concerns
how long a <strong>watch</strong> request is considered to occupy its seat. Depending
on request parameters, the response to a <strong>watch</strong> request may or may not
begin with <strong>create</strong> notifications for all the relevant pre-existing
objects. API Priority and Fairness considers a <strong>watch</strong> request to be
done with its seat once that initial burst of notifications, if any,
is over.</p><p>The normal notifications are sent in a concurrent burst to all
relevant <strong>watch</strong> response streams whenever the server is notified of an
object create/update/delete. To account for this work, API Priority
and Fairness considers every write request to spend some additional
time occupying seats after the actual writing is done. The server
estimates the number of notifications to be sent and adjusts the write
request's number of seats and seat occupancy time to include this
extra work.</p><h3 id=queuing>Queuing</h3><p>Even within a priority level there may be a large number of distinct sources of
traffic. In an overload situation, it is valuable to prevent one stream of
requests from starving others (in particular, in the relatively common case of a
single buggy client flooding the kube-apiserver with requests, that buggy client
would ideally not have much measurable impact on other clients at all). This is
handled by use of a fair-queuing algorithm to process requests that are assigned
the same priority level. Each request is assigned to a <em>flow</em>, identified by the
name of the matching FlowSchema plus a <em>flow distinguisher</em> — which
is either the requesting user, the target resource's namespace, or nothing — and the
system attempts to give approximately equal weight to requests in different
flows of the same priority level.
To enable distinct handling of distinct instances, controllers that have
many instances should authenticate with distinct usernames</p><p>After classifying a request into a flow, the API Priority and Fairness
feature then may assign the request to a queue. This assignment uses
a technique known as <a class=glossary-tooltip title='A technique for assigning requests to queues that provides better isolation than hashing modulo the number of queues.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-shuffle-sharding' target=_blank aria-label='shuffle sharding'>shuffle sharding</a>, which makes relatively efficient use of
queues to insulate low-intensity flows from high-intensity flows.</p><p>The details of the queuing algorithm are tunable for each priority level, and
allow administrators to trade off memory use, fairness (the property that
independent flows will all make progress when total traffic exceeds capacity),
tolerance for bursty traffic, and the added latency induced by queuing.</p><h3 id=exempt-requests>Exempt requests</h3><p>Some requests are considered sufficiently important that they are not subject to
any of the limitations imposed by this feature. These exemptions prevent an
improperly-configured flow control configuration from totally disabling an API
server.</p><h2 id=resources>Resources</h2><p>The flow control API involves two kinds of resources.
<a href=/docs/reference/generated/kubernetes-api/v1.25/#prioritylevelconfiguration-v1beta2-flowcontrol-apiserver-k8s-io>PriorityLevelConfigurations</a>
define the available priority levels, the share of the available concurrency
budget that each can handle, and allow for fine-tuning queuing behavior.
<a href=/docs/reference/generated/kubernetes-api/v1.25/#flowschema-v1beta2-flowcontrol-apiserver-k8s-io>FlowSchemas</a>
are used to classify individual inbound requests, matching each to a
single PriorityLevelConfiguration. There is also a <code>v1alpha1</code> version
of the same API group, and it has the same Kinds with the same syntax and
semantics.</p><h3 id=prioritylevelconfiguration>PriorityLevelConfiguration</h3><p>A PriorityLevelConfiguration represents a single priority level. Each
PriorityLevelConfiguration has an independent limit on the number of outstanding
requests, and limitations on the number of queued requests.</p><p>The nominal oncurrency limit for a PriorityLevelConfiguration is not
specified in an absolute number of seats, but rather in "nominal
concurrency shares." The total concurrency limit for the API Server is
distributed among the existing PriorityLevelConfigurations in
proportion to these shares, to give each level its nominal limit in
terms of seats. This allows a cluster administrator to scale up or
down the total amount of traffic to a server by restarting
<code>kube-apiserver</code> with a different value for <code>--max-requests-inflight</code>
(or <code>--max-mutating-requests-inflight</code>), and all
PriorityLevelConfigurations will see their maximum allowed concurrency
go up (or down) by the same fraction.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> In the versions before <code>v1beta3</code> the relevant
PriorityLevelConfiguration field is named "assured concurrency shares"
rather than "nominal concurrency shares". Also, in Kubernetes release
1.25 and earlier there were no periodic adjustments: the
nominal/assured limits were always applied without adjustment.</div><p>The bounds on how much concurrency a priority level may lend and how
much it may borrow are expressed in the PriorityLevelConfiguration as
percentages of the level's nominal limit. These are resolved to
absolute numbers of seats by multiplying with the nominal limit /
100.0 and rounding. The dynamically adjusted concurrency limit of a
priority level is constrained to lie between (a) a lower bound of its
nominal limit minus its lendable seats and (b) an upper bound of its
nominal limit plus the seats it may borrow. At each adjustment the
dynamic limits are derived by each priority level reclaiming any lent
seats for which demand recently appeared and then jointly fairly
responding to the recent seat demand on the priority levels, within
the bounds just described.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> With the Priority and Fairness feature enabled, the total concurrency limit for
the server is set to the sum of <code>--max-requests-inflight</code> and
<code>--max-mutating-requests-inflight</code>. There is no longer any distinction made
between mutating and non-mutating requests; if you want to treat them
separately for a given resource, make separate FlowSchemas that match the
mutating and non-mutating verbs respectively.</div><p>When the volume of inbound requests assigned to a single
PriorityLevelConfiguration is more than its permitted concurrency level, the
<code>type</code> field of its specification determines what will happen to extra requests.
A type of <code>Reject</code> means that excess traffic will immediately be rejected with
an HTTP 429 (Too Many Requests) error. A type of <code>Queue</code> means that requests
above the threshold will be queued, with the shuffle sharding and fair queuing techniques used
to balance progress between request flows.</p><p>The queuing configuration allows tuning the fair queuing algorithm for a
priority level. Details of the algorithm can be read in the
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness>enhancement proposal</a>, but in short:</p><ul><li><p>Increasing <code>queues</code> reduces the rate of collisions between different flows, at
the cost of increased memory usage. A value of 1 here effectively disables the
fair-queuing logic, but still allows requests to be queued.</p></li><li><p>Increasing <code>queueLengthLimit</code> allows larger bursts of traffic to be
sustained without dropping any requests, at the cost of increased
latency and memory usage.</p></li><li><p>Changing <code>handSize</code> allows you to adjust the probability of collisions between
different flows and the overall concurrency available to a single flow in an
overload situation.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A larger <code>handSize</code> makes it less likely for two individual flows to collide
(and therefore for one to be able to starve the other), but more likely that
a small number of flows can dominate the apiserver. A larger <code>handSize</code> also
potentially increases the amount of latency that a single high-traffic flow
can cause. The maximum number of queued requests possible from a
single flow is <code>handSize * queueLengthLimit</code>.</div></li></ul><p>Following is a table showing an interesting collection of shuffle
sharding configurations, showing for each the probability that a
given mouse (low-intensity flow) is squished by the elephants (high-intensity flows) for
an illustrative collection of numbers of elephants. See
<a href=https://play.golang.org/p/Gi0PLgVHiUg>https://play.golang.org/p/Gi0PLgVHiUg</a> , which computes this table.</p><table><caption style=display:none>Example Shuffle Sharding Configurations</caption><thead><tr><th>HandSize</th><th>Queues</th><th>1 elephant</th><th>4 elephants</th><th>16 elephants</th></tr></thead><tbody><tr><td>12</td><td>32</td><td>4.428838398950118e-09</td><td>0.11431348830099144</td><td>0.9935089607656024</td></tr><tr><td>10</td><td>32</td><td>1.550093439632541e-08</td><td>0.0626479840223545</td><td>0.9753101519027554</td></tr><tr><td>10</td><td>64</td><td>6.601827268370426e-12</td><td>0.00045571320990370776</td><td>0.49999929150089345</td></tr><tr><td>9</td><td>64</td><td>3.6310049976037345e-11</td><td>0.00045501212304112273</td><td>0.4282314876454858</td></tr><tr><td>8</td><td>64</td><td>2.25929199850899e-10</td><td>0.0004886697053040446</td><td>0.35935114681123076</td></tr><tr><td>8</td><td>128</td><td>6.994461389026097e-13</td><td>3.4055790161620863e-06</td><td>0.02746173137155063</td></tr><tr><td>7</td><td>128</td><td>1.0579122850901972e-11</td><td>6.960839379258192e-06</td><td>0.02406157386340147</td></tr><tr><td>7</td><td>256</td><td>7.597695465552631e-14</td><td>6.728547142019406e-08</td><td>0.0006709661542533682</td></tr><tr><td>6</td><td>256</td><td>2.7134626662687968e-12</td><td>2.9516464018476436e-07</td><td>0.0008895654642000348</td></tr><tr><td>6</td><td>512</td><td>4.116062922897309e-14</td><td>4.982983350480894e-09</td><td>2.26025764343413e-05</td></tr><tr><td>6</td><td>1024</td><td>6.337324016514285e-16</td><td>8.09060164312957e-11</td><td>4.517408062903668e-07</td></tr></tbody></table><h3 id=flowschema>FlowSchema</h3><p>A FlowSchema matches some inbound requests and assigns them to a
priority level. Every inbound request is tested against every
FlowSchema in turn, starting with those with numerically lowest ---
which we take to be the logically highest --- <code>matchingPrecedence</code> and
working onward. The first match wins.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Only the first matching FlowSchema for a given request matters. If multiple
FlowSchemas match a single inbound request, it will be assigned based on the one
with the highest <code>matchingPrecedence</code>. If multiple FlowSchemas with equal
<code>matchingPrecedence</code> match the same request, the one with lexicographically
smaller <code>name</code> will win, but it's better not to rely on this, and instead to
ensure that no two FlowSchemas have the same <code>matchingPrecedence</code>.</div><p>A FlowSchema matches a given request if at least one of its <code>rules</code>
matches. A rule matches if at least one of its <code>subjects</code> <em>and</em> at least
one of its <code>resourceRules</code> or <code>nonResourceRules</code> (depending on whether the
incoming request is for a resource or non-resource URL) matches the request.</p><p>For the <code>name</code> field in subjects, and the <code>verbs</code>, <code>apiGroups</code>, <code>resources</code>,
<code>namespaces</code>, and <code>nonResourceURLs</code> fields of resource and non-resource rules,
the wildcard <code>*</code> may be specified to match all values for the given field,
effectively removing it from consideration.</p><p>A FlowSchema's <code>distinguisherMethod.type</code> determines how requests matching that
schema will be separated into flows. It may be
either <code>ByUser</code>, in which case one requesting user will not be able to starve
other users of capacity, or <code>ByNamespace</code>, in which case requests for resources
in one namespace will not be able to starve requests for resources in other
namespaces of capacity, or it may be blank (or <code>distinguisherMethod</code> may be
omitted entirely), in which case all requests matched by this FlowSchema will be
considered part of a single flow. The correct choice for a given FlowSchema
depends on the resource and your particular environment.</p><h2 id=defaults>Defaults</h2><p>Each kube-apiserver maintains two sorts of APF configuration objects:
mandatory and suggested.</p><h3 id=mandatory-configuration-objects>Mandatory Configuration Objects</h3><p>The four mandatory configuration objects reflect fixed built-in
guardrail behavior. This is behavior that the servers have before
those objects exist, and when those objects exist their specs reflect
this behavior. The four mandatory objects are as follows.</p><ul><li><p>The mandatory <code>exempt</code> priority level is used for requests that are
not subject to flow control at all: they will always be dispatched
immediately. The mandatory <code>exempt</code> FlowSchema classifies all
requests from the <code>system:masters</code> group into this priority
level. You may define other FlowSchemas that direct other requests
to this priority level, if appropriate.</p></li><li><p>The mandatory <code>catch-all</code> priority level is used in combination with
the mandatory <code>catch-all</code> FlowSchema to make sure that every request
gets some kind of classification. Typically you should not rely on
this catch-all configuration, and should create your own catch-all
FlowSchema and PriorityLevelConfiguration (or use the suggested
<code>global-default</code> priority level that is installed by default) as
appropriate. Because it is not expected to be used normally, the
mandatory <code>catch-all</code> priority level has a very small concurrency
share and does not queue requests.</p></li></ul><h3 id=suggested-configuration-objects>Suggested Configuration Objects</h3><p>The suggested FlowSchemas and PriorityLevelConfigurations constitute a
reasonable default configuration. You can modify these and/or create
additional configuration objects if you want. If your cluster is
likely to experience heavy load then you should consider what
configuration will work best.</p><p>The suggested configuration groups requests into six priority levels:</p><ul><li><p>The <code>node-high</code> priority level is for health updates from nodes.</p></li><li><p>The <code>system</code> priority level is for non-health requests from the
<code>system:nodes</code> group, i.e. Kubelets, which must be able to contact
the API server in order for workloads to be able to schedule on
them.</p></li><li><p>The <code>leader-election</code> priority level is for leader election requests from
built-in controllers (in particular, requests for <code>endpoints</code>, <code>configmaps</code>,
or <code>leases</code> coming from the <code>system:kube-controller-manager</code> or
<code>system:kube-scheduler</code> users and service accounts in the <code>kube-system</code>
namespace). These are important to isolate from other traffic because failures
in leader election cause their controllers to fail and restart, which in turn
causes more expensive traffic as the new controllers sync their informers.</p></li><li><p>The <code>workload-high</code> priority level is for other requests from built-in
controllers.</p></li><li><p>The <code>workload-low</code> priority level is for requests from any other service
account, which will typically include all requests from controllers running in
Pods.</p></li><li><p>The <code>global-default</code> priority level handles all other traffic, e.g.
interactive <code>kubectl</code> commands run by nonprivileged users.</p></li></ul><p>The suggested FlowSchemas serve to steer requests into the above
priority levels, and are not enumerated here.</p><h3 id=maintenance-of-the-mandatory-and-suggested-configuration-objects>Maintenance of the Mandatory and Suggested Configuration Objects</h3><p>Each <code>kube-apiserver</code> independently maintains the mandatory and
suggested configuration objects, using initial and periodic behavior.
Thus, in a situation with a mixture of servers of different versions
there may be thrashing as long as different servers have different
opinions of the proper content of these objects.</p><p>Each <code>kube-apiserver</code> makes an initial maintenance pass over the
mandatory and suggested configuration objects, and after that does
periodic maintenance (once per minute) of those objects.</p><p>For the mandatory configuration objects, maintenance consists of
ensuring that the object exists and, if it does, has the proper spec.
The server refuses to allow a creation or update with a spec that is
inconsistent with the server's guardrail behavior.</p><p>Maintenance of suggested configuration objects is designed to allow
their specs to be overridden. Deletion, on the other hand, is not
respected: maintenance will restore the object. If you do not want a
suggested configuration object then you need to keep it around but set
its spec to have minimal consequences. Maintenance of suggested
objects is also designed to support automatic migration when a new
version of the <code>kube-apiserver</code> is rolled out, albeit potentially with
thrashing while there is a mixed population of servers.</p><p>Maintenance of a suggested configuration object consists of creating
it --- with the server's suggested spec --- if the object does not
exist. OTOH, if the object already exists, maintenance behavior
depends on whether the <code>kube-apiservers</code> or the users control the
object. In the former case, the server ensures that the object's spec
is what the server suggests; in the latter case, the spec is left
alone.</p><p>The question of who controls the object is answered by first looking
for an annotation with key <code>apf.kubernetes.io/autoupdate-spec</code>. If
there is such an annotation and its value is <code>true</code> then the
kube-apiservers control the object. If there is such an annotation
and its value is <code>false</code> then the users control the object. If
neither of those condtions holds then the <code>metadata.generation</code> of the
object is consulted. If that is 1 then the kube-apiservers control
the object. Otherwise the users control the object. These rules were
introduced in release 1.22 and their consideration of
<code>metadata.generation</code> is for the sake of migration from the simpler
earlier behavior. Users who wish to control a suggested configuration
object should set its <code>apf.kubernetes.io/autoupdate-spec</code> annotation
to <code>false</code>.</p><p>Maintenance of a mandatory or suggested configuration object also
includes ensuring that it has an <code>apf.kubernetes.io/autoupdate-spec</code>
annotation that accurately reflects whether the kube-apiservers
control the object.</p><p>Maintenance also includes deleting objects that are neither mandatory
nor suggested but are annotated
<code>apf.kubernetes.io/autoupdate-spec=true</code>.</p><h2 id=health-check-concurrency-exemption>Health check concurrency exemption</h2><p>The suggested configuration gives no special treatment to the health
check requests on kube-apiservers from their local kubelets --- which
tend to use the secured port but supply no credentials. With the
suggested config, these requests get assigned to the <code>global-default</code>
FlowSchema and the corresponding <code>global-default</code> priority level,
where other traffic can crowd them out.</p><p>If you add the following additional FlowSchema, this exempts those
requests from rate limiting.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Making this change also allows any hostile party to then send
health-check requests that match this FlowSchema, at any volume they
like. If you have a web traffic filter or similar external security
mechanism to protect your cluster's API server from general internet
traffic, you can configure rules to block any health check requests
that originate from outside your cluster.</div><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/priority-and-fairness/health-for-strangers.yaml download=priority-and-fairness/health-for-strangers.yaml><code>priority-and-fairness/health-for-strangers.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("priority-and-fairness-health-for-strangers-yaml")' title="Copy priority-and-fairness/health-for-strangers.yaml to clipboard"></img></div><div class=includecode id=priority-and-fairness-health-for-strangers-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>flowcontrol.apiserver.k8s.io/v1beta2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>FlowSchema<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>health-for-strangers<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>matchingPrecedence</span>:<span style=color:#bbb> </span><span style=color:#666>1000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityLevelConfiguration</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>exempt<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>nonResourceRules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>nonResourceURLs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/healthz&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/livez&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;/readyz&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>verbs</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:#b44>&#34;*&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>subjects</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Group<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>group</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>system:unauthenticated<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><h2 id=diagnostics>Diagnostics</h2><p>Every HTTP response from an API server with the priority and fairness feature
enabled has two extra headers: <code>X-Kubernetes-PF-FlowSchema-UID</code> and
<code>X-Kubernetes-PF-PriorityLevel-UID</code>, noting the flow schema that matched the request
and the priority level to which it was assigned, respectively. The API objects'
names are not included in these headers in case the requesting user does not
have permission to view them, so when debugging you can use a command like</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get flowschemas -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span><span style=display:flex><span>kubectl get prioritylevelconfigurations -o custom-columns<span style=color:#666>=</span><span style=color:#b44>&#34;uid:{metadata.uid},name:{metadata.name}&#34;</span>
</span></span></code></pre></div><p>to get a mapping of UIDs to names for both FlowSchemas and
PriorityLevelConfigurations.</p><h2 id=observability>Observability</h2><h3 id=metrics>Metrics</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In versions of Kubernetes before v1.20, the labels <code>flow_schema</code> and
<code>priority_level</code> were inconsistently named <code>flowSchema</code> and <code>priorityLevel</code>,
respectively. If you're running Kubernetes versions v1.19 and earlier, you
should refer to the documentation for your version.</div><p>When you enable the API Priority and Fairness feature, the kube-apiserver
exports additional metrics. Monitoring these can help you determine whether your
configuration is inappropriately throttling important traffic, or find
poorly-behaved workloads that may be harming system health.</p><ul><li><p><code>apiserver_flowcontrol_rejected_requests_total</code> is a counter vector
(cumulative since server start) of requests that were rejected,
broken down by the labels <code>flow_schema</code> (indicating the one that
matched the request), <code>priority_level</code> (indicating the one to which
the request was assigned), and <code>reason</code>. The <code>reason</code> label will be
have one of the following values:</p><ul><li><code>queue-full</code>, indicating that too many requests were already
queued,</li><li><code>concurrency-limit</code>, indicating that the
PriorityLevelConfiguration is configured to reject rather than
queue excess requests, or</li><li><code>time-out</code>, indicating that the request was still in the queue
when its queuing time limit expired.</li></ul></li><li><p><code>apiserver_flowcontrol_dispatched_requests_total</code> is a counter
vector (cumulative since server start) of requests that began
executing, broken down by the labels <code>flow_schema</code> (indicating the
one that matched the request) and <code>priority_level</code> (indicating the
one to which the request was assigned).</p></li><li><p><code>apiserver_current_inqueue_requests</code> is a gauge vector of recent
high water marks of the number of queued requests, grouped by a
label named <code>request_kind</code> whose value is <code>mutating</code> or <code>readOnly</code>.
These high water marks describe the largest number seen in the one
second window most recently completed. These complement the older
<code>apiserver_current_inflight_requests</code> gauge vector that holds the
last window's high water mark of number of requests actively being
served.</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_request_count_samples</code> is a
histogram vector of observations of the then-current number of
requests, broken down by the labels <code>phase</code> (which takes on the
values <code>waiting</code> and <code>executing</code>) and <code>request_kind</code> (which takes on
the values <code>mutating</code> and <code>readOnly</code>). The observations are made
periodically at a high rate. Each observed value is a ratio,
between 0 and 1, of a number of requests divided by the
corresponding limit on the number of requests (queue length limit
for waiting and concurrency limit for executing).</p></li><li><p><code>apiserver_flowcontrol_read_vs_write_request_count_watermarks</code> is a
histogram vector of high or low water marks of the number of
requests (divided by the corresponding limit to get a ratio in the
range 0 to 1) broken down by the labels <code>phase</code> (which takes on the
values <code>waiting</code> and <code>executing</code>) and <code>request_kind</code> (which takes on
the values <code>mutating</code> and <code>readOnly</code>); the label <code>mark</code> takes on
values <code>high</code> and <code>low</code>. The water marks are accumulated over
windows bounded by the times when an observation was added to
<code>apiserver_flowcontrol_read_vs_write_request_count_samples</code>. These
water marks show the range of values that occurred between samples.</p></li><li><p><code>apiserver_flowcontrol_current_inqueue_requests</code> is a gauge vector
holding the instantaneous number of queued (not executing) requests,
broken down by the labels <code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_current_executing_requests</code> is a gauge vector
holding the instantaneous number of executing (not waiting in a
queue) requests, broken down by the labels <code>priority_level</code> and
<code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_request_concurrency_in_use</code> is a gauge vector
holding the instantaneous number of occupied seats, broken down by
the labels <code>priority_level</code> and <code>flow_schema</code>.</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_count_samples</code> is a
histogram vector of observations of the then-current number of
requests broken down by the labels <code>phase</code> (which takes on the
values <code>waiting</code> and <code>executing</code>) and <code>priority_level</code>. Each
histogram gets observations taken periodically, up through the last
activity of the relevant sort. The observations are made at a high
rate. Each observed value is a ratio, between 0 and 1, of a number
of requests divided by the corresponding limit on the number of
requests (queue length limit for waiting and concurrency limit for
executing).</p></li><li><p><code>apiserver_flowcontrol_priority_level_request_count_watermarks</code> is a
histogram vector of high or low water marks of the number of
requests (divided by the corresponding limit to get a ratio in the
range 0 to 1) broken down by the labels <code>phase</code> (which takes on the
values <code>waiting</code> and <code>executing</code>) and <code>priority_level</code>; the label
<code>mark</code> takes on values <code>high</code> and <code>low</code>. The water marks are
accumulated over windows bounded by the times when an observation
was added to
<code>apiserver_flowcontrol_priority_level_request_count_samples</code>. These
water marks show the range of values that occurred between samples.</p></li><li><p><code>apiserver_flowcontrol_priority_level_seat_count_samples</code> is a
histogram vector of observations of the utilization of a priority
level's concurrency limit, broken down by <code>priority_level</code>. This
utilization is the fraction (number of seats occupied) /
(concurrency limit). This metric considers all stages of execution
(both normal and the extra delay at the end of a write to cover for
the corresponding notification work) of all requests except WATCHes;
for those it considers only the initial stage that delivers
notifications of pre-existing objects. Each histogram in the vector
is also labeled with <code>phase: executing</code> (there is no seat limit for
the waiting phase). Each histogram gets observations taken
periodically, up through the last activity of the relevant sort.
The observations
are made at a high rate.</p></li><li><p><code>apiserver_flowcontrol_priority_level_seat_count_watermarks</code> is a
histogram vector of high or low water marks of the utilization of a
priority level's concurrency limit, broken down by <code>priority_level</code>
and <code>mark</code> (which takes on values <code>high</code> and <code>low</code>). Each histogram
in the vector is also labeled with <code>phase: executing</code> (there is no
seat limit for the waiting phase). The water marks are accumulated
over windows bounded by the times when an observation was added to
<code>apiserver_flowcontrol_priority_level_seat_count_samples</code>. These
water marks show the range of values that occurred between samples.</p></li><li><p><code>apiserver_flowcontrol_request_queue_length_after_enqueue</code> is a
histogram vector of queue lengths for the queues, broken down by
the labels <code>priority_level</code> and <code>flow_schema</code>, as sampled by the
enqueued requests. Each request that gets queued contributes one
sample to its histogram, reporting the length of the queue immediately
after the request was added. Note that this produces different
statistics than an unbiased survey would.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> An outlier value in a histogram here means it is likely that a single flow
(i.e., requests by one user or for one namespace, depending on
configuration) is flooding the API server, and being throttled. By contrast,
if one priority level's histogram shows that all queues for that priority
level are longer than those for other priority levels, it may be appropriate
to increase that PriorityLevelConfiguration's concurrency shares.</div></li><li><p><code>apiserver_flowcontrol_request_concurrency_limit</code> is the same as
<code>apiserver_flowcontrol_nominal_limit_seats</code>. Before the
introduction of concurrency borrowing between priority levels, this
was always equal to <code>apiserver_flowcontrol_current_limit_seats</code>
(which did not exist as a distinct metric).</p></li><li><p><code>apiserver_flowcontrol_nominal_limit_seats</code> is a gauge vector
holding each priority level's nominal concurrency limit, computed
from the API server's total concurrency limit and the priority
level's configured nominal concurrency shares.</p></li><li><p><code>apiserver_flowcontrol_lower_limit_seats</code> is a gauge vector holding
the lower bound on each priority level's dynamic concurrency limit.</p></li><li><p><code>apiserver_flowcontrol_upper_limit_seats</code> is a gauge vector holding
the upper bound on each priority level's dynamic concurrency limit.</p></li><li><p><code>apiserver_flowcontrol_demand_seats</code> is a histogram vector counting
observations, at the end of every nanosecond, of each priority
level's ratio of (seat demand) / (nominal concurrency limit). A
priority level's seat demand is the sum, over both queued requests
and those in the initial phase of execution, of the maximum of the
number of seats occupied in the request's initial and final
execution phases.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_high_watermark</code> is a gauge vector
holding, for each priority level, the maximum seat demand seen
during the last concurrency borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_average</code> is a gauge vector
holding, for each priority level, the time-weighted average seat
demand seen during the last concurrency borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_demand_seats_stdev</code> is a gauge vector
holding, for each priority level, the time-weighted population
standard deviation of seat demand seen during the last concurrency
borrowing adjustment period.</p></li><li><p><code>apiserver_flowcontrol_target_seats</code> is a gauge vector holding, for
each priority level, the concurrency target going into the borrowing
allocation problem.</p></li><li><p><code>apiserver_flowcontrol_seat_fair_frac</code> is a gauge holding the fair
allocation fraction determined in the last borrowing adjustment.</p></li><li><p><code>apiserver_flowcontrol_current_limit_seats</code> is a gauge vector
holding, for each priority level, the dynamic concurrency limit
derived in the last adjustment.</p></li><li><p><code>apiserver_flowcontrol_request_wait_duration_seconds</code> is a histogram
vector of how long requests spent queued, broken down by the labels
<code>flow_schema</code> (indicating which one matched the request),
<code>priority_level</code> (indicating the one to which the request was
assigned), and <code>execute</code> (indicating whether the request started
executing).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Since each FlowSchema always assigns requests to a single
PriorityLevelConfiguration, you can add the histograms for all the
FlowSchemas for one priority level to get the effective histogram for
requests assigned to that priority level.</div></li><li><p><code>apiserver_flowcontrol_request_execution_seconds</code> is a histogram
vector of how long requests took to actually execute, broken down by
the labels <code>flow_schema</code> (indicating which one matched the request)
and <code>priority_level</code> (indicating the one to which the request was
assigned).</p></li><li><p><code>apiserver_flowcontrol_watch_count_samples</code> is a histogram vector of
the number of active WATCH requests relevant to a given write,
broken down by <code>flow_schema</code> and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_work_estimated_seats</code> is a histogram vector
of the number of estimated seats (maximum of initial and final stage
of execution) associated with requests, broken down by <code>flow_schema</code>
and <code>priority_level</code>.</p></li><li><p><code>apiserver_flowcontrol_request_dispatch_no_accommodation_total</code> is a
counter vec of the number of events that in principle could have led
to a request being dispatched but did not, due to lack of available
concurrency, broken down by <code>flow_schema</code> and <code>priority_level</code>. The
relevant sorts of events are arrival of a request and completion of
a request.</p></li></ul><h3 id=debug-endpoints>Debug endpoints</h3><p>When you enable the API Priority and Fairness feature, the <code>kube-apiserver</code>
serves the following additional paths at its HTTP[S] ports.</p><ul><li><p><code>/debug/api_priority_and_fairness/dump_priority_levels</code> - a listing of
all the priority levels and the current state of each. You can fetch like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_priority_levels
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, ActiveQueues, IsIdle, IsQuiescing, WaitingRequests, ExecutingRequests,
workload-low,      0,            true,   false,       0,               0,
global-default,    0,            true,   false,       0,               0,
exempt,            &lt;none&gt;,       &lt;none&gt;, &lt;none&gt;,      &lt;none&gt;,          &lt;none&gt;,
catch-all,         0,            true,   false,       0,               0,
system,            0,            true,   false,       0,               0,
leader-election,   0,            true,   false,       0,               0,
workload-high,     0,            true,   false,       0,               0,
</code></pre></li><li><p><code>/debug/api_priority_and_fairness/dump_queues</code> - a listing of all the
queues and their current state. You can fetch like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_queues
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, Index,  PendingRequests, ExecutingRequests, VirtualStart,
workload-high,     0,      0,               0,                 0.0000,
workload-high,     1,      0,               0,                 0.0000,
workload-high,     2,      0,               0,                 0.0000,
...
leader-election,   14,     0,               0,                 0.0000,
leader-election,   15,     0,               0,                 0.0000,
</code></pre></li><li><p><code>/debug/api_priority_and_fairness/dump_requests</code> - a listing of all the requests
that are currently waiting in a queue. You can fetch like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw /debug/api_priority_and_fairness/dump_requests
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,
exempt,            &lt;none&gt;,         &lt;none&gt;,     &lt;none&gt;,              &lt;none&gt;,                &lt;none&gt;,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:26:57.179170694Z,
</code></pre><p>In addition to the queued requests, the output includes one phantom line
for each priority level that is exempt from limitation.</p><p>You can get a more detailed listing with a command like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get --raw <span style=color:#b44>&#39;/debug/api_priority_and_fairness/dump_requests?includeRequestDetails=1&#39;</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code class=language-none data-lang=none>PriorityLevelName, FlowSchemaName, QueueIndex, RequestIndexInQueue, FlowDistingsher,       ArriveTime,                     UserName,              Verb,   APIPath,                                                     Namespace, Name,   APIVersion, Resource, SubResource,
system,            system-nodes,   12,         0,                   system:node:127.0.0.1, 2020-07-23T15:31:03.583823404Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
system,            system-nodes,   12,         1,                   system:node:127.0.0.1, 2020-07-23T15:31:03.594555947Z, system:node:127.0.0.1, create, /api/v1/namespaces/scaletest/configmaps,
</code></pre></li></ul><h2 id=what-s-next>What's next</h2><p>For background information on design details for API priority and fairness, see
the <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness>enhancement proposal</a>.
You can make suggestions and feature requests via <a href=https://github.com/kubernetes/community/tree/master/sig-api-machinery>SIG API Machinery</a>
or the feature's <a href=https://kubernetes.slack.com/messages/api-priority-and-fairness>slack channel</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-85d633ae590aa20ec024f1b7af1d74fc>12.10 - Installing Addons</h1><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>Add-ons extend the functionality of Kubernetes.</p><p>This page lists some of the available add-ons and links to their respective installation instructions. The list does not try to be exhaustive.</p><h2 id=networking-and-network-policy>Networking and Network Policy</h2><ul><li><a href=https://www.github.com/noironetworks/aci-containers>ACI</a> provides integrated container networking and network security with Cisco ACI.</li><li><a href=https://antrea.io/>Antrea</a> operates at Layer 3/4 to provide networking and security services for Kubernetes, leveraging Open vSwitch as the networking data plane.</li><li><a href=https://docs.projectcalico.org/latest/introduction/>Calico</a> is a networking and network policy provider. Calico supports a flexible set of networking options so you can choose the most efficient option for your situation, including non-overlay and overlay networks, with or without BGP. Calico uses the same engine to enforce network policy for hosts, pods, and (if using Istio & Envoy) applications at the service mesh layer.</li><li><a href=https://projectcalico.docs.tigera.io/getting-started/kubernetes/flannel/flannel>Canal</a> unites Flannel and Calico, providing networking and network policy.</li><li><a href=https://github.com/cilium/cilium>Cilium</a> is a networking, observability, and security solution with an eBPF-based data plane. Cilium provides a simple flat Layer 3 network with the ability to span multiple clusters in either a native routing or overlay/encapsulation mode, and can enforce network policies on L3-L7 using an identity-based security model that is decoupled from network addressing. Cilium can act as a replacement for kube-proxy; it also offers additional, opt-in observability and security features.</li><li><a href=https://github.com/cni-genie/CNI-Genie>CNI-Genie</a> enables Kubernetes to seamlessly connect to a choice of CNI plugins, such as Calico, Canal, Flannel, or Weave.</li><li><a href=https://contivpp.io/>Contiv</a> provides configurable networking (native L3 using BGP, overlay using vxlan, classic L2, and Cisco-SDN/ACI) for various use cases and a rich policy framework. Contiv project is fully <a href=https://github.com/contiv>open sourced</a>. The <a href=https://github.com/contiv/install>installer</a> provides both kubeadm and non-kubeadm based installation options.</li><li><a href=https://www.juniper.net/us/en/products-services/sdn/contrail/contrail-networking/>Contrail</a>, based on <a href=https://tungsten.io>Tungsten Fabric</a>, is an open source, multi-cloud network virtualization and policy management platform. Contrail and Tungsten Fabric are integrated with orchestration systems such as Kubernetes, OpenShift, OpenStack and Mesos, and provide isolation modes for virtual machines, containers/pods and bare metal workloads.</li><li><a href=https://github.com/flannel-io/flannel#deploying-flannel-manually>Flannel</a> is an overlay network provider that can be used with Kubernetes.</li><li><a href=https://github.com/ZTE/Knitter/>Knitter</a> is a plugin to support multiple network interfaces in a Kubernetes pod.</li><li><a href=https://github.com/k8snetworkplumbingwg/multus-cni>Multus</a> is a Multi plugin for multiple network support in Kubernetes to support all CNI plugins (e.g. Calico, Cilium, Contiv, Flannel), in addition to SRIOV, DPDK, OVS-DPDK and VPP based workloads in Kubernetes.</li><li><a href=https://github.com/ovn-org/ovn-kubernetes/>OVN-Kubernetes</a> is a networking provider for Kubernetes based on <a href=https://github.com/ovn-org/ovn/>OVN (Open Virtual Network)</a>, a virtual networking implementation that came out of the Open vSwitch (OVS) project. OVN-Kubernetes provides an overlay based networking implementation for Kubernetes, including an OVS based implementation of load balancing and network policy.</li><li><a href=https://github.com/akraino-edge-stack/icn-nodus>Nodus</a> is an OVN based CNI controller plugin to provide cloud native based Service function chaining(SFC).</li><li><a href=https://docs.vmware.com/en/VMware-NSX-T-Data-Center/index.html>NSX-T</a> Container Plug-in (NCP) provides integration between VMware NSX-T and container orchestrators such as Kubernetes, as well as integration between NSX-T and container-based CaaS/PaaS platforms such as Pivotal Container Service (PKS) and OpenShift.</li><li><a href=https://github.com/nuagenetworks/nuage-kubernetes/blob/v5.1.1-1/docs/kubernetes-1-installation.rst>Nuage</a> is an SDN platform that provides policy-based networking between Kubernetes Pods and non-Kubernetes environments with visibility and security monitoring.</li><li><a href=https://github.com/romana>Romana</a> is a Layer 3 networking solution for pod networks that also supports the <a href=/docs/concepts/services-networking/network-policies/>NetworkPolicy</a> API.</li><li><a href=https://www.weave.works/docs/net/latest/kubernetes/kube-addon/>Weave Net</a> provides networking and network policy, will carry on working on both sides of a network partition, and does not require an external database.</li></ul><h2 id=service-discovery>Service Discovery</h2><ul><li><a href=https://coredns.io>CoreDNS</a> is a flexible, extensible DNS server which can be <a href=https://github.com/coredns/deployment/tree/master/kubernetes>installed</a> as the in-cluster DNS for pods.</li></ul><h2 id=visualization-amp-control>Visualization & Control</h2><ul><li><a href=https://github.com/kubernetes/dashboard#kubernetes-dashboard>Dashboard</a> is a dashboard web interface for Kubernetes.</li><li><a href=https://www.weave.works/documentation/scope-latest-installing/#k8s>Weave Scope</a> is a tool for graphically visualizing your containers, pods, services etc. Use it in conjunction with a <a href=https://cloud.weave.works/>Weave Cloud account</a> or host the UI yourself.</li></ul><h2 id=infrastructure>Infrastructure</h2><ul><li><a href=https://kubevirt.io/user-guide/#/installation/installation>KubeVirt</a> is an add-on to run virtual machines on Kubernetes. Usually run on bare-metal clusters.</li><li>The
<a href=https://github.com/kubernetes/node-problem-detector>node problem detector</a>
runs on Linux nodes and reports system issues as either
<a href=/docs/reference/kubernetes-api/cluster-resources/event-v1/>Events</a> or
<a href=/docs/concepts/architecture/nodes/#condition>Node conditions</a>.</li></ul><h2 id=legacy-add-ons>Legacy Add-ons</h2><p>There are several other add-ons documented in the deprecated <a href=https://git.k8s.io/kubernetes/cluster/addons>cluster/addons</a> directory.</p><p>Well-maintained ones should be linked to here. PRs welcome!</p></div><div class=td-content style=page-break-before:always><h1 id=pg-7e0d97616b15e2c383c6a0a96ec442cb>13 - Extending Kubernetes</h1><div class=lead>Different ways to change the behavior of your Kubernetes cluster.</div><p>Kubernetes is highly configurable and extensible. As a result, there is rarely a need to fork or
submit patches to the Kubernetes project code.</p><p>This guide describes the options for customizing a Kubernetes cluster. It is aimed at
<a class=glossary-tooltip title='A person who configures, controls, and monitors clusters.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cluster-operator' target=_blank aria-label='cluster operators'>cluster operators</a> who want to understand
how to adapt their Kubernetes cluster to the needs of their work environment. Developers who are
prospective <a class=glossary-tooltip title='A person who customizes the Kubernetes platform to fit the needs of their project.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-platform-developer' target=_blank aria-label='Platform Developers'>Platform Developers</a> or
Kubernetes Project <a class=glossary-tooltip title='Someone who donates code, documentation, or their time to help the Kubernetes project or community.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-contributor' target=_blank aria-label=Contributors>Contributors</a> will also
find it useful as an introduction to what extension points and patterns exist, and their
trade-offs and limitations.</p><p>Customization approaches can be broadly divided into <a href=#configuration>configuration</a>, which only
involves changing command line arguments, local configuration files, or API resources; and <a href=#extensions>extensions</a>,
which involve running additional programs, additional network services, or both.
This document is primarily about <em>extensions</em>.</p><h2 id=configuration>Configuration</h2><p><em>Configuration files</em> and <em>command arguments</em> are documented in the <a href=/docs/reference/>Reference</a> section of the online
documentation, with a page for each binary:</p><ul><li><a href=/docs/reference/command-line-tools-reference/kube-apiserver/><code>kube-apiserver</code></a></li><li><a href=/docs/reference/command-line-tools-reference/kube-controller-manager/><code>kube-controller-manager</code></a></li><li><a href=/docs/reference/command-line-tools-reference/kube-scheduler/><code>kube-scheduler</code></a></li><li><a href=/docs/reference/command-line-tools-reference/kubelet/><code>kubelet</code></a></li><li><a href=/docs/reference/command-line-tools-reference/kube-proxy/><code>kube-proxy</code></a></li></ul><p>Command arguments and configuration files may not always be changeable in a hosted Kubernetes service or a
distribution with managed installation. When they are changeable, they are usually only changeable
by the cluster operator. Also, they are subject to change in future Kubernetes versions, and
setting them may require restarting processes. For those reasons, they should be used only when
there are no other options.</p><p>Built-in <em>policy APIs</em>, such as <a href=/docs/concepts/policy/resource-quotas/>ResourceQuota</a>,
<a href=/docs/concepts/services-networking/network-policies/>NetworkPolicy</a> and Role-based Access Control
(<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>), are built-in Kubernetes APIs that provide declaratively configured policy settings.
APIs are typically usable even with hosted Kubernetes services and with managed Kubernetes installations.
The built-in policy APIs follow the same conventions as other Kubernetes resources such as Pods.
When you use a policy APIs that is <a href=/docs/reference/using-api/#api-versioning>stable</a>, you benefit from a
<a href=/docs/reference/using-api/deprecation-policy/>defined support policy</a> like other Kubernetes APIs.
For these reasons, policy APIs are recommended over <em>configuration files</em> and <em>command arguments</em> where suitable.</p><h2 id=extensions>Extensions</h2><p>Extensions are software components that extend and deeply integrate with Kubernetes.
They adapt it to support new types and new kinds of hardware.</p><p>Many cluster administrators use a hosted or distribution instance of Kubernetes.
These clusters come with extensions pre-installed. As a result, most Kubernetes
users will not need to install extensions and even fewer users will need to author new ones.</p><h3 id=extension-patterns>Extension patterns</h3><p>Kubernetes is designed to be automated by writing client programs. Any
program that reads and/or writes to the Kubernetes API can provide useful
automation. <em>Automation</em> can run on the cluster or off it. By following
the guidance in this doc you can write highly available and robust automation.
Automation generally works with any Kubernetes cluster, including hosted
clusters and managed installations.</p><p>There is a specific pattern for writing client programs that work well with
Kubernetes called the <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>
pattern. Controllers typically read an object's <code>.spec</code>, possibly do things, and then
update the object's <code>.status</code>.</p><p>A controller is a client of the Kubernetes API. When Kubernetes is the client and calls
out to a remote service, Kubernetes calls this a <em>webhook</em>. The remote service is called
a <em>webhook backend</em>. As with custom controllers, webhooks do add a point of failure.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Outside of Kubernetes, the term “webhook” typically refers to a mechanism for asynchronous
notifications, where the webhook call serves as a one-way notification to another system or
component. In the Kubernetes ecosystem, even synchronous HTTP callouts are often
described as “webhooks”.</div><p>In the webhook model, Kubernetes makes a network request to a remote service.
With the alternative <em>binary Plugin</em> model, Kubernetes executes a binary (program).
Binary plugins are used by the kubelet (for example, <a href=https://kubernetes-csi.github.io/docs/>CSI storage plugins</a>
and <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>CNI network plugins</a>),
and by kubectl (see <a href=/docs/tasks/extend-kubectl/kubectl-plugins/>Extend kubectl with plugins</a>).</p><h3 id=extension-points>Extension points</h3><p>This diagram shows the extension points in a Kubernetes cluster and the
clients that access it.</p><figure class=diagram-large><img src=/docs/concepts/extend-kubernetes/extension-points.png alt="Symbolic representation of seven numbered extension points for Kubernetes"><figcaption><p>Kubernetes extension points</p></figcaption></figure><h4 id=key-to-the-figure>Key to the figure</h4><ol><li><p>Users often interact with the Kubernetes API using <code>kubectl</code>. <a href=#client-extensions>Plugins</a>
customise the behaviour of clients. There are generic extensions that can apply to different clients,
as well as specific ways to extend <code>kubectl</code>.</p></li><li><p>The API server handles all requests. Several types of extension points in the API server allow
authenticating requests, or blocking them based on their content, editing content, and handling
deletion. These are described in the <a href=#api-access-extensions>API Access Extensions</a> section.</p></li><li><p>The API server serves various kinds of <em>resources</em>. <em>Built-in resource kinds</em>, such as
<code>pods</code>, are defined by the Kubernetes project and can't be changed.
Read <a href=#api-extensions>API extensions</a> to learn about extending the Kubernetes API.</p></li><li><p>The Kubernetes scheduler <a href=/docs/concepts/scheduling-eviction/assign-pod-node/>decides</a>
which nodes to place pods on. There are several ways to extend scheduling, which are
described in the <a href=#scheduling-extensions>Scheduling extensions</a> section.</p></li><li><p>Much of the behavior of Kubernetes is implemented by programs called
<a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controllers>controllers</a>, that are
clients of the API server. Controllers are often used in conjunction with custom resources.
Read <a href=#combining-new-apis-with-automation>combining new APIs with automation</a> and
<a href=#changing-built-in-resources>Changing built-in resources</a> to learn more.</p></li><li><p>The kubelet runs on servers (nodes), and helps pods appear like virtual servers with their own IPs on
the cluster network. <a href=#network-plugins>Network Plugins</a> allow for different implementations of
pod networking.</p></li><li><p>You can use <a href=#device-plugins>Device Plugins</a> to integrate custom hardware or other special
node-local facilities, and make these available to Pods running in your cluster. The kubelet
includes support for working with device plugins.</p><p>The kubelet also mounts and unmounts
<a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=volume>volume</a> for pods and their containers.
You can use <a href=#storage-plugins>Storage Plugins</a> to add support for new kinds
of storage and other volume types.</p></li></ol><h4 id=extension-flowchart>Extension point choice flowchart</h4><p>If you are unsure where to start, this flowchart can help. Note that some solutions may involve
several types of extensions.</p><figure class=diagram-large><img src=/docs/concepts/extend-kubernetes/flowchart.png alt="Flowchart with questions about use cases and guidance for implementers. Green circles indicate yes; red circles indicate no."><figcaption><p>Flowchart guide to select an extension approach</p></figcaption></figure><hr><h2 id=client-extensions>Client extensions</h2><p>Plugins for kubectl are separate binaries that add or replace the behavior of specific subcommands.
The <code>kubectl</code> tool can also integrate with <a href=/docs/reference/access-authn-authz/authentication/#client-go-credential-plugins>credential plugins</a>
These extensions only affect a individual user's local environment, and so cannot enforce site-wide policies.</p><p>If you want to extend the <code>kubectl</code> tool, read <a href=/docs/tasks/extend-kubectl/kubectl-plugins/>Extend kubectl with plugins</a>.</p><h2 id=api-extensions>API extensions</h2><h3 id=custom-resource-definitions>Custom resource definitions</h3><p>Consider adding a <em>Custom Resource</em> to Kubernetes if you want to define new controllers, application
configuration objects or other declarative APIs, and to manage them using Kubernetes tools, such
as <code>kubectl</code>.</p><p>For more about Custom Resources, see the
<a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>Custom Resources</a> concept guide.</p><h3 id=api-aggregation-layer>API aggregation layer</h3><p>You can use Kubernetes' <a href=/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>API Aggregation Layer</a>
to integrate the Kubernetes API with additional services such as for <a href=/docs/tasks/debug/debug-cluster/resource-metrics-pipeline/>metrics</a>.</p><h3 id=combining-new-apis-with-automation>Combining new APIs with automation</h3><p>A combination of a custom resource API and a control loop is called the
<a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controllers>controllers</a> pattern. If your controller takes
the place of a human operator deploying infrastructure based on a desired state, then the controller
may also be following the <a class=glossary-tooltip title='A specialized controller used to manage a custom resource' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/operator/ target=_blank aria-label='operator pattern'>operator pattern</a>.
The Operator pattern is used to manage specific applications; usually, these are applications that
maintain state and require care in how they are managed.</p><p>You can also make your own custom APIs and control loops that manage other resources, such as storage,
or to define policies (such as an access control restriction).</p><h3 id=changing-built-in-resources>Changing built-in resources</h3><p>When you extend the Kubernetes API by adding custom resources, the added resources always fall
into a new API Groups. You cannot replace or change existing API groups.
Adding an API does not directly let you affect the behavior of existing APIs (such as Pods), whereas
<em>API Access Extensions</em> do.</p><h2 id=api-access-extensions>API access extensions</h2><p>When a request reaches the Kubernetes API Server, it is first <em>authenticated</em>, then <em>authorized</em>,
and is then subject to various types of <em>admission control</em> (some requests are in fact not
authenticated, and get special treatment). See
<a href=/docs/concepts/security/controlling-access/>Controlling Access to the Kubernetes API</a>
for more on this flow.</p><p>Each of the steps in the Kubernetes authentication / authorization flow offers extension points.</p><h3 id=authentication>Authentication</h3><p><a href=/docs/reference/access-authn-authz/authentication/>Authentication</a> maps headers or certificates
in all requests to a username for the client making the request.</p><p>Kubernetes has several built-in authentication methods that it supports. It can also sit behind an
authenticating proxy, and it can send a token from an <code>Authorization:</code> header to a remote service for
verification (an <a href=/docs/reference/access-authn-authz/authentication/#webhook-token-authentication>authentication webhook</a>)
if those don't meet your needs.</p><h3 id=authorization>Authorization</h3><p><a href=/docs/reference/access-authn-authz/authorization/>Authorization</a> determines whether specific
users can read, write, and do other operations on API resources. It works at the level of whole
resources -- it doesn't discriminate based on arbitrary object fields.</p><p>If the built-in authorization options don't meet your needs, an
<a href=/docs/reference/access-authn-authz/webhook/>authorization webhook</a>
allows calling out to custom code that makes an authorization decision.</p><h3 id=dynamic-admission-control>Dynamic admission control</h3><p>After a request is authorized, if it is a write operation, it also goes through
<a href=/docs/reference/access-authn-authz/admission-controllers/>Admission Control</a> steps.
In addition to the built-in steps, there are several extensions:</p><ul><li>The <a href=/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook>Image Policy webhook</a>
restricts what images can be run in containers.</li><li>To make arbitrary admission control decisions, a general
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>Admission webhook</a>
can be used. Admission webhooks can reject creations or updates.
Some admission webhooks modify the incoming request data before it is handled further by Kubernetes.</li></ul><h2 id=infrastructure-extensions>Infrastructure extensions</h2><h3 id=device-plugins>Device plugins</h3><p><em>Device plugins</em> allow a node to discover new Node resources (in addition to the
builtin ones like cpu and memory) via a
<a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>Device Plugin</a>.</p><h3 id=storage-plugins>Storage plugins</h3><p><a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label='Container Storage Interface'>Container Storage Interface</a> (CSI) plugins provide
a way to extend Kubernetes with supports for new kinds of volumes. The volumes can be backed by
durable external storage, or provide ephemeral storage, or they might offer a read-only interface
to information using a filesystem paradigm.</p><p>Kubernetes also includes support for <a href=/docs/concepts/storage/volumes/#flexvolume-deprecated>FlexVolume</a> plugins,
which are deprecated since Kubernetes v1.23 (in favour of CSI).</p><p>FlexVolume plugins allow users to mount volume types that aren't natively supported by Kubernetes. When
you run a Pod that relies on FlexVolume storage, the kubelet calls a binary plugin to mount the volume.
The archived <a href=https://git.k8s.io/design-proposals-archive/storage/flexvolume-deployment.md>FlexVolume</a>
design proposal has more detail on this approach.</p><p>The <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors>Kubernetes Volume Plugin FAQ for Storage Vendors</a>
includes general information on storage plugins.</p><h3 id=network-plugins>Network plugins</h3><p>Your Kubernetes cluster needs a <em>network plugin</em> in order to have a working Pod network
and to support other aspects of the Kubernetes network model.</p><p><a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>Network Plugins</a>
allow Kubernetes to work with different networking topologies and technologies.</p><h2 id=scheduling-extensions>Scheduling extensions</h2><p>The scheduler is a special type of controller that watches pods, and assigns
pods to nodes. The default scheduler can be replaced entirely, while
continuing to use other Kubernetes components, or
<a href=/docs/tasks/extend-kubernetes/configure-multiple-schedulers/>multiple schedulers</a>
can run at the same time.</p><p>This is a significant undertaking, and almost all Kubernetes users find they
do not need to modify the scheduler.</p><p>You can control which <a href=/docs/reference/scheduling/config/#scheduling-plugins>scheduling plugins</a>
are active, or associate sets of plugins with different named <a href=/docs/reference/scheduling/config/#multiple-profiles>scheduler profiles</a>.
You can also write your own plugin that integrates with one or more of the kube-scheduler's
<a href=/docs/concepts/scheduling-eviction/scheduling-framework/#extension-points>extension points</a>.</p><p>Finally, the built in <code>kube-scheduler</code> component supports a
<a href=https://git.k8s.io/design-proposals-archive/scheduling/scheduler_extender.md>webhook</a>
that permits a remote HTTP backend (scheduler extension) to filter and / or prioritize
the nodes that the kube-scheduler chooses for a pod.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You can only affect node filtering
and node prioritization with a scheduler extender webhook; other extension points are
not available through the webhook integration.</div><h2 id=what-s-next>What's next</h2><ul><li>Learn more about infrastructure extensions<ul><li><a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>Device Plugins</a></li><li><a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>Network Plugins</a></li><li>CSI <a href=https://kubernetes-csi.github.io/docs/>storage plugins</a></li></ul></li><li>Learn about <a href=/docs/tasks/extend-kubectl/kubectl-plugins/>kubectl plugins</a></li><li>Learn more about <a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>Custom Resources</a></li><li>Learn more about <a href=/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>Extension API Servers</a></li><li>Learn about <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/>Dynamic admission control</a></li><li>Learn about the <a href=/docs/concepts/extend-kubernetes/operator/>Operator pattern</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-c8937cdc9df96f3328becf04f8211292>13.1 - Compute, Storage, and Networking Extensions</h1><p>This section covers extensions to your cluster that do not come as part as Kubernetes itself.
You can use these extensions to enhance the nodes in your cluster, or to provide the network
fabric that links Pods together.</p><ul><li><p><a href=/docs/concepts/storage/volumes/#csi>CSI</a> and <a href=/docs/concepts/storage/volumes/#flexvolume>FlexVolume</a> storage plugins</p><p><a class=glossary-tooltip title='The Container Storage Interface (CSI) defines a standard interface to expose storage systems to containers.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/#csi target=_blank aria-label='Container Storage Interface'>Container Storage Interface</a> (CSI) plugins
provide a way to extend Kubernetes with supports for new kinds of volumes. The volumes can
be backed by durable external storage, or provide ephemeral storage, or they might offer a
read-only interface to information using a filesystem paradigm.</p><p>Kubernetes also includes support for <a href=/docs/concepts/storage/volumes/#flexvolume>FlexVolume</a>
plugins, which are deprecated since Kubernetes v1.23 (in favour of CSI).</p><p>FlexVolume plugins allow users to mount volume types that aren't natively
supported by Kubernetes. When you run a Pod that relies on FlexVolume
storage, the kubelet calls a binary plugin to mount the volume. The archived
<a href=https://git.k8s.io/design-proposals-archive/storage/flexvolume-deployment.md>FlexVolume</a>
design proposal has more detail on this approach.</p><p>The <a href=https://github.com/kubernetes/community/blob/master/sig-storage/volume-plugin-faq.md#kubernetes-volume-plugin-faq-for-storage-vendors>Kubernetes Volume Plugin FAQ for Storage Vendors</a>
includes general information on storage plugins.</p></li><li><p><a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>Device plugins</a></p><p>Device plugins allow a node to discover new Node facilities (in addition to the
built-in node resources such as <code>cpu</code> and <code>memory</code>), and provide these custom node-local
facilities to Pods that request them.</p></li><li><p><a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>Network plugins</a></p><p>A network plugin allow Kubernetes to work with different networking topologies and technologies.
Your Kubernetes cluster needs a <em>network plugin</em> in order to have a working Pod network
and to support other aspects of the Kubernetes network model.</p><p>Kubernetes 1.25 is compatible with <a class=glossary-tooltip title='Container network interface (CNI) plugins are a type of Network plugin that adheres to the appc/CNI specification.' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ target=_blank aria-label=CNI>CNI</a>
network plugins.</p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1ac2260db9ecccbf0303a899bc27ce6d>13.1.1 - Network Plugins</h1><p>Kubernetes 1.25 supports <a href=https://github.com/containernetworking/cni>Container Network Interface</a>
(CNI) plugins for cluster networking. You must use a CNI plugin that is compatible with your
cluster and that suits your needs. Different plugins are available (both open- and closed- source)
in the wider Kubernetes ecosystem.</p><p>A CNI plugin is required to implement the
<a href=/docs/concepts/services-networking/#the-kubernetes-network-model>Kubernetes network model</a>.</p><p>You must use a CNI plugin that is compatible with the
<a href=https://github.com/containernetworking/cni/blob/spec-v0.4.0/SPEC.md>v0.4.0</a> or later
releases of the CNI specification. The Kubernetes project recommends using a plugin that is
compatible with the <a href=https://github.com/containernetworking/cni/blob/spec-v1.0.0/SPEC.md>v1.0.0</a>
CNI specification (plugins can be compatible with multiple spec versions).</p><h2 id=installation>Installation</h2><p>A Container Runtime, in the networking context, is a daemon on a node configured to provide CRI
Services for kubelet. In particular, the Container Runtime must be configured to load the CNI
plugins required to implement the Kubernetes network model.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Prior to Kubernetes 1.24, the CNI plugins could also be managed by the kubelet using the
<code>cni-bin-dir</code> and <code>network-plugin</code> command-line parameters.
These command-line parameters were removed in Kubernetes 1.24, with management of the CNI no
longer in scope for kubelet.</p><p>See <a href=/docs/tasks/administer-cluster/migrating-from-dockershim/troubleshooting-cni-plugin-related-errors/>Troubleshooting CNI plugin-related errors</a>
if you are facing issues following the removal of dockershim.</p></div><p>For specific information about how a Container Runtime manages the CNI plugins, see the
documentation for that Container Runtime, for example:</p><ul><li><a href=https://github.com/containerd/containerd/blob/main/script/setup/install-cni>containerd</a></li><li><a href=https://github.com/cri-o/cri-o/blob/main/contrib/cni/README.md>CRI-O</a></li></ul><p>For specific information about how to install and manage a CNI plugin, see the documentation for
that plugin or <a href=/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model>networking provider</a>.</p><h2 id=network-plugin-requirements>Network Plugin Requirements</h2><p>For plugin developers and users who regularly build or deploy Kubernetes, the plugin may also need
specific configuration to support kube-proxy. The iptables proxy depends on iptables, and the
plugin may need to ensure that container traffic is made available to iptables. For example, if
the plugin connects containers to a Linux bridge, the plugin must set the
<code>net/bridge/bridge-nf-call-iptables</code> sysctl to <code>1</code> to ensure that the iptables proxy functions
correctly. If the plugin does not use a Linux bridge, but uses something like Open vSwitch or
some other mechanism instead, it should ensure container traffic is appropriately routed for the
proxy.</p><p>By default, if no kubelet network plugin is specified, the <code>noop</code> plugin is used, which sets
<code>net/bridge/bridge-nf-call-iptables=1</code> to ensure simple configurations (like Docker with a bridge)
work correctly with the iptables proxy.</p><h3 id=loopback-cni>Loopback CNI</h3><p>In addition to the CNI plugin installed on the nodes for implementing the Kubernetes network
model, Kubernetes also requires the container runtimes to provide a loopback interface <code>lo</code>, which
is used for each sandbox (pod sandboxes, vm sandboxes, ...).
Implementing the loopback interface can be accomplished by re-using the
<a href=https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback/loopback.go>CNI loopback plugin.</a>
or by developing your own code to achieve this (see
<a href=https://github.com/cri-o/ocicni/blob/release-1.24/pkg/ocicni/util_linux.go#L91>this example from CRI-O</a>).</p><h3 id=support-hostport>Support hostPort</h3><p>The CNI networking plugin supports <code>hostPort</code>. You can use the official
<a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/portmap>portmap</a>
plugin offered by the CNI plugin team or use your own plugin with portMapping functionality.</p><p>If you want to enable <code>hostPort</code> support, you must specify <code>portMappings capability</code> in your
<code>cni-conf-dir</code>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;k8s-pod-network&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;cniVersion&#34;</span>: <span style=color:#b44>&#34;0.4.0&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;plugins&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;calico&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;log_level&#34;</span>: <span style=color:#b44>&#34;info&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;datastore_type&#34;</span>: <span style=color:#b44>&#34;kubernetes&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;nodename&#34;</span>: <span style=color:#b44>&#34;127.0.0.1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;ipam&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;host-local&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;subnet&#34;</span>: <span style=color:#b44>&#34;usePodCidr&#34;</span>
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;policy&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;k8s&#34;</span>
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;kubernetes&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;kubeconfig&#34;</span>: <span style=color:#b44>&#34;/etc/cni/net.d/calico-kubeconfig&#34;</span>
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;portmap&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;capabilities&#34;</span>: {<span style=color:green;font-weight:700>&#34;portMappings&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>}
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=support-traffic-shaping>Support traffic shaping</h3><p><strong>Experimental Feature</strong></p><p>The CNI networking plugin also supports pod ingress and egress traffic shaping. You can use the
official <a href=https://github.com/containernetworking/plugins/tree/master/plugins/meta/bandwidth>bandwidth</a>
plugin offered by the CNI plugin team or use your own plugin with bandwidth control functionality.</p><p>If you want to enable traffic shaping support, you must add the <code>bandwidth</code> plugin to your CNI
configuration file (default <code>/etc/cni/net.d</code>) and ensure that the binary is included in your CNI
bin dir (default <code>/opt/cni/bin</code>).</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;k8s-pod-network&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;cniVersion&#34;</span>: <span style=color:#b44>&#34;0.4.0&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;plugins&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;calico&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;log_level&#34;</span>: <span style=color:#b44>&#34;info&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;datastore_type&#34;</span>: <span style=color:#b44>&#34;kubernetes&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;nodename&#34;</span>: <span style=color:#b44>&#34;127.0.0.1&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;ipam&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;host-local&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;subnet&#34;</span>: <span style=color:#b44>&#34;usePodCidr&#34;</span>
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;policy&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;k8s&#34;</span>
</span></span><span style=display:flex><span>      },
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;kubernetes&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;kubeconfig&#34;</span>: <span style=color:#b44>&#34;/etc/cni/net.d/calico-kubeconfig&#34;</span>
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    },
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;type&#34;</span>: <span style=color:#b44>&#34;bandwidth&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;capabilities&#34;</span>: {<span style=color:green;font-weight:700>&#34;bandwidth&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>}
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Now you can add the <code>kubernetes.io/ingress-bandwidth</code> and <code>kubernetes.io/egress-bandwidth</code>
annotations to your Pod. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/ingress-bandwidth</span>:<span style=color:#bbb> </span>1M<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kubernetes.io/egress-bandwidth</span>:<span style=color:#bbb> </span>1M<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=what-s-next>What's next</h2></div><div class=td-content style=page-break-before:always><h1 id=pg-53e1ea8892ceca307ba19e8d6a7b8d32>13.1.2 - Device Plugins</h1><div class=lead>Device plugins let you configure your cluster with support for devices or resources that require vendor-specific setup, such as GPUs, NICs, FPGAs, or non-volatile main memory.</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.10 [beta]</code></div><p>Kubernetes provides a <a href=https://git.k8s.io/design-proposals-archive/resource-management/device-plugin.md>device plugin framework</a>
that you can use to advertise system hardware resources to the
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a>.</p><p>Instead of customizing the code for Kubernetes itself, vendors can implement a
device plugin that you deploy either manually or as a <a class=glossary-tooltip title='Ensures a copy of a Pod is running across a set of nodes in a cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/daemonset target=_blank aria-label=DaemonSet>DaemonSet</a>.
The targeted devices include GPUs, high-performance NICs, FPGAs, InfiniBand adapters,
and other similar computing resources that may require vendor specific initialization
and setup.</p><h2 id=device-plugin-registration>Device plugin registration</h2><p>The kubelet exports a <code>Registration</code> gRPC service:</p><pre tabindex=0><code class=language-gRPC data-lang=gRPC>service Registration {
	rpc Register(RegisterRequest) returns (Empty) {}
}
</code></pre><p>A device plugin can register itself with the kubelet through this gRPC service.
During the registration, the device plugin needs to send:</p><ul><li>The name of its Unix socket.</li><li>The Device Plugin API version against which it was built.</li><li>The <code>ResourceName</code> it wants to advertise. Here <code>ResourceName</code> needs to follow the
<a href=/docs/concepts/configuration/manage-resources-containers/#extended-resources>extended resource naming scheme</a>
as <code>vendor-domain/resourcetype</code>.
(For example, an NVIDIA GPU is advertised as <code>nvidia.com/gpu</code>.)</li></ul><p>Following a successful registration, the device plugin sends the kubelet the
list of devices it manages, and the kubelet is then in charge of advertising those
resources to the API server as part of the kubelet node status update.
For example, after a device plugin registers <code>hardware-vendor.example/foo</code> with the kubelet
and reports two healthy devices on a node, the node status is updated
to advertise that the node has 2 "Foo" devices installed and available.</p><p>Then, users can request devices as part of a Pod specification
(see <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#Container><code>container</code></a>).
Requesting extended resources is similar to how you manage requests and limits for
other resources, with the following differences:</p><ul><li>Extended resources are only supported as integer resources and cannot be overcommitted.</li><li>Devices cannot be shared between containers.</li></ul><h3 id=example-pod>Example</h3><p>Suppose a Kubernetes cluster is running a device plugin that advertises resource <code>hardware-vendor.example/foo</code>
on certain nodes. Here is an example of a pod requesting this resource to run a demo workload:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>demo-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>demo-container-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>hardware-vendor.example/foo</span>:<span style=color:#bbb> </span><span style=color:#666>2</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic>#</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># This Pod needs 2 of the hardware-vendor.example/foo devices</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># and can only schedule onto a Node that&#39;s able to satisfy</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># that need.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic>#</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># If the Node has more than 2 of those devices available, the</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># remainder would be available for other Pods to use.</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=device-plugin-implementation>Device plugin implementation</h2><p>The general workflow of a device plugin includes the following steps:</p><ul><li><p>Initialization. During this phase, the device plugin performs vendor specific
initialization and setup to make sure the devices are in a ready state.</p></li><li><p>The plugin starts a gRPC service, with a Unix socket under host path
<code>/var/lib/kubelet/device-plugins/</code>, that implements the following interfaces:</p><pre tabindex=0><code class=language-gRPC data-lang=gRPC>service DevicePlugin {
      // GetDevicePluginOptions returns options to be communicated with Device Manager.
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch returns a stream of List of Devices
      // Whenever a Device state change or a Device disappears, ListAndWatch
      // returns the new list
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate is called during container creation so that the Device
      // Plugin can run device specific operations and instruct Kubelet
      // of the steps to make the Device available in the container
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // GetPreferredAllocation returns a preferred set of devices to allocate
      // from a list of available ones. The resulting preferred allocation is not
      // guaranteed to be the allocation ultimately performed by the
      // devicemanager. It is only designed to help the devicemanager make a more
      // informed allocation decision when possible.
      rpc GetPreferredAllocation(PreferredAllocationRequest) returns (PreferredAllocationResponse) {}

      // PreStartContainer is called, if indicated by Device Plugin during registeration phase,
      // before each container start. Device plugin can run device specific operations
      // such as resetting the device before making devices available to the container.
      rpc PreStartContainer(PreStartContainerRequest) returns (PreStartContainerResponse) {}
}
</code></pre><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Plugins are not required to provide useful implementations for
<code>GetPreferredAllocation()</code> or <code>PreStartContainer()</code>. Flags indicating which
(if any) of these calls are available should be set in the <code>DevicePluginOptions</code>
message sent back by a call to <code>GetDevicePluginOptions()</code>. The <code>kubelet</code> will
always call <code>GetDevicePluginOptions()</code> to see which optional functions are
available, before calling any of them directly.</div></li><li><p>The plugin registers itself with the kubelet through the Unix socket at host
path <code>/var/lib/kubelet/device-plugins/kubelet.sock</code>.</p></li><li><p>After successfully registering itself, the device plugin runs in serving mode, during which it keeps
monitoring device health and reports back to the kubelet upon any device state changes.
It is also responsible for serving <code>Allocate</code> gRPC requests. During <code>Allocate</code>, the device plugin may
do device-specific preparation; for example, GPU cleanup or QRNG initialization.
If the operations succeed, the device plugin returns an <code>AllocateResponse</code> that contains container
runtime configurations for accessing the allocated devices. The kubelet passes this information
to the container runtime.</p></li></ul><h3 id=handling-kubelet-restarts>Handling kubelet restarts</h3><p>A device plugin is expected to detect kubelet restarts and re-register itself with the new
kubelet instance. In the current implementation, a new kubelet instance deletes all the existing Unix sockets
under <code>/var/lib/kubelet/device-plugins</code> when it starts. A device plugin can monitor the deletion
of its Unix socket and re-register itself upon such an event.</p><h2 id=device-plugin-deployment>Device plugin deployment</h2><p>You can deploy a device plugin as a DaemonSet, as a package for your node's operating system,
or manually.</p><p>The canonical directory <code>/var/lib/kubelet/device-plugins</code> requires privileged access,
so a device plugin must run in a privileged security context.
If you're deploying a device plugin as a DaemonSet, <code>/var/lib/kubelet/device-plugins</code>
must be mounted as a <a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volume>Volume</a>
in the plugin's <a href=/docs/reference/generated/kubernetes-api/v1.25/#podspec-v1-core>PodSpec</a>.</p><p>If you choose the DaemonSet approach you can rely on Kubernetes to: place the device plugin's
Pod onto Nodes, to restart the daemon Pod after failure, and to help automate upgrades.</p><h2 id=api-compatibility>API compatibility</h2><p>Kubernetes device plugin support is in beta. The API may change before stabilization,
in incompatible ways. As a project, Kubernetes recommends that device plugin developers:</p><ul><li>Watch for changes in future releases.</li><li>Support multiple versions of the device plugin API for backward/forward compatibility.</li></ul><p>If you enable the DevicePlugins feature and run device plugins on nodes that need to be upgraded to
a Kubernetes release with a newer device plugin API version, upgrade your device plugins
to support both versions before upgrading these nodes. Taking that approach will
ensure the continuous functioning of the device allocations during the upgrade.</p><h2 id=monitoring-device-plugin-resources>Monitoring device plugin resources</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.15 [beta]</code></div><p>In order to monitor resources provided by device plugins, monitoring agents need to be able to
discover the set of devices that are in-use on the node and obtain metadata to describe which
container the metric should be associated with. <a href=https://prometheus.io/>Prometheus</a> metrics
exposed by device monitoring agents should follow the
<a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/instrumentation.md>Kubernetes Instrumentation Guidelines</a>,
identifying containers using <code>pod</code>, <code>namespace</code>, and <code>container</code> prometheus labels.</p><p>The kubelet provides a gRPC service to enable discovery of in-use devices, and to provide metadata
for these devices:</p><pre tabindex=0><code class=language-gRPC data-lang=gRPC>// PodResourcesLister is a service provided by the kubelet that provides information about the
// node resources consumed by pods and containers on the node
service PodResourcesLister {
    rpc List(ListPodResourcesRequest) returns (ListPodResourcesResponse) {}
    rpc GetAllocatableResources(AllocatableResourcesRequest) returns (AllocatableResourcesResponse) {}
}
</code></pre><h3 id=grpc-endpoint-list><code>List</code> gRPC endpoint</h3><p>The <code>List</code> endpoint provides information on resources of running pods, with details such as the
id of exclusively allocated CPUs, device id as it was reported by device plugins and id of
the NUMA node where these devices are allocated. Also, for NUMA-based machines, it contains the
information about memory and hugepages reserved for a container.</p><pre tabindex=0><code class=language-gRPC data-lang=gRPC>// ListPodResourcesResponse is the response returned by List function
message ListPodResourcesResponse {
    repeated PodResources pod_resources = 1;
}

// PodResources contains information about the node resources assigned to a pod
message PodResources {
    string name = 1;
    string namespace = 2;
    repeated ContainerResources containers = 3;
}

// ContainerResources contains information about the resources assigned to a container
message ContainerResources {
    string name = 1;
    repeated ContainerDevices devices = 2;
    repeated int64 cpu_ids = 3;
    repeated ContainerMemory memory = 4;
}

// ContainerMemory contains information about memory and hugepages assigned to a container
message ContainerMemory {
    string memory_type = 1;
    uint64 size = 2;
    TopologyInfo topology = 3;
}

// Topology describes hardware topology of the resource
message TopologyInfo {
        repeated NUMANode nodes = 1;
}

// NUMA representation of NUMA node
message NUMANode {
        int64 ID = 1;
}

// ContainerDevices contains information about the devices assigned to a container
message ContainerDevices {
    string resource_name = 1;
    repeated string device_ids = 2;
    TopologyInfo topology = 3;
}
</code></pre><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>cpu_ids in the <code>ContainerResources</code> in the <code>List</code> endpoint correspond to exclusive CPUs allocated
to a partilar container. If the goal is to evaluate CPUs that belong to the shared pool, the <code>List</code>
endpoint needs to be used in conjunction with the <code>GetAllocatableResources</code> endpoint as explained
below:</p><ol><li>Call <code>GetAllocatableResources</code> to get a list of all the allocatable CPUs</li><li>Call <code>GetCpuIds</code> on all <code>ContainerResources</code> in the system</li><li>Subtract out all of the CPUs from the <code>GetCpuIds</code> calls from the <code>GetAllocatableResources</code> call</li></ol></div><h3 id=grpc-endpoint-getallocatableresources><code>GetAllocatableResources</code> gRPC endpoint</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div><p>GetAllocatableResources provides information on resources initially available on the worker node.
It provides more information than kubelet exports to APIServer.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p><code>GetAllocatableResources</code> should only be used to evaluate <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>allocatable</a>
resources on a node. If the goal is to evaluate free/unallocated resources it should be used in
conjunction with the List() endpoint. The result obtained by <code>GetAllocatableResources</code> would remain
the same unless the underlying resources exposed to kubelet change. This happens rarely but when
it does (for example: hotplug/hotunplug, device health changes), client is expected to call
<code>GetAlloctableResources</code> endpoint.</p><p>However, calling <code>GetAllocatableResources</code> endpoint is not sufficient in case of cpu and/or memory
update and Kubelet needs to be restarted to reflect the correct resource capacity and allocatable.</p></div><pre tabindex=0><code class=language-gRPC data-lang=gRPC>// AllocatableResourcesResponses contains informations about all the devices known by the kubelet
message AllocatableResourcesResponse {
    repeated ContainerDevices devices = 1;
    repeated int64 cpu_ids = 2;
    repeated ContainerMemory memory = 3;
}
</code></pre><p>Starting from Kubernetes v1.23, the <code>GetAllocatableResources</code> is enabled by default.
You can disable it by turning off the <code>KubeletPodResourcesGetAllocatable</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>.</p><p>Preceding Kubernetes v1.23, to enable this feature <code>kubelet</code> must be started with the following flag:</p><pre tabindex=0><code>--feature-gates=KubeletPodResourcesGetAllocatable=true
</code></pre><p><code>ContainerDevices</code> do expose the topology information declaring to which NUMA cells the device is
affine. The NUMA cells are identified using a opaque integer ID, which value is consistent to
what device plugins report
<a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager>when they register themselves to the kubelet</a>.</p><p>The gRPC service is served over a unix socket at <code>/var/lib/kubelet/pod-resources/kubelet.sock</code>.
Monitoring agents for device plugin resources can be deployed as a daemon, or as a DaemonSet.
The canonical directory <code>/var/lib/kubelet/pod-resources</code> requires privileged access, so monitoring
agents must run in a privileged security context. If a device monitoring agent is running as a
DaemonSet, <code>/var/lib/kubelet/pod-resources</code> must be mounted as a
<a class=glossary-tooltip title='A directory containing data, accessible to the containers in a pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/storage/volumes/ target=_blank aria-label=Volume>Volume</a> in the device monitoring agent's
<a href=/docs/reference/generated/kubernetes-api/v1.25/#podspec-v1-core>PodSpec</a>.</p><p>Support for the <code>PodResourcesLister service</code> requires <code>KubeletPodResources</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> to be enabled.
It is enabled by default starting with Kubernetes 1.15 and is v1 since Kubernetes 1.20.</p><h2 id=device-plugin-integration-with-the-topology-manager>Device plugin integration with the Topology Manager</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [beta]</code></div><p>The Topology Manager is a Kubelet component that allows resources to be co-ordinated in a Topology
aligned manner. In order to do this, the Device Plugin API was extended to include a
<code>TopologyInfo</code> struct.</p><pre tabindex=0><code class=language-gRPC data-lang=gRPC>message TopologyInfo {
    repeated NUMANode nodes = 1;
}

message NUMANode {
    int64 ID = 1;
}
</code></pre><p>Device Plugins that wish to leverage the Topology Manager can send back a populated TopologyInfo
struct as part of the device registration, along with the device IDs and the health of the device.
The device manager will then use this information to consult with the Topology Manager and make
resource assignment decisions.</p><p><code>TopologyInfo</code> supports setting a <code>nodes</code> field to either <code>nil</code> or a list of NUMA nodes. This
allows the Device Plugin to advertise a device that spans multiple NUMA nodes.</p><p>Setting <code>TopologyInfo</code> to <code>nil</code> or providing an empty list of NUMA nodes for a given device
indicates that the Device Plugin does not have a NUMA affinity preference for that device.</p><p>An example <code>TopologyInfo</code> struct populated for a device by a Device Plugin:</p><pre tabindex=0><code>pluginapi.Device{ID: &#34;25102017&#34;, Health: pluginapi.Healthy, Topology:&amp;pluginapi.TopologyInfo{Nodes: []*pluginapi.NUMANode{&amp;pluginapi.NUMANode{ID: 0,},}}}
</code></pre><h2 id=examples>Device plugin examples</h2><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>Here are some examples of device plugin implementations:</p><ul><li>The <a href=https://github.com/RadeonOpenCompute/k8s-device-plugin>AMD GPU device plugin</a></li><li>The <a href=https://github.com/intel/intel-device-plugins-for-kubernetes>Intel device plugins</a> for
Intel GPU, FPGA, QAT, VPU, SGX, DSA, DLB and IAA devices</li><li>The <a href=https://github.com/kubevirt/kubernetes-device-plugins>KubeVirt device plugins</a> for
hardware-assisted virtualization</li><li>The <a href=https://github.com/GoogleCloudPlatform/container-engine-accelerators/tree/master/cmd/nvidia_gpu>NVIDIA GPU device plugin for Container-Optimized OS</a></li><li>The <a href=https://github.com/hustcat/k8s-rdma-device-plugin>RDMA device plugin</a></li><li>The <a href=https://github.com/collabora/k8s-socketcan>SocketCAN device plugin</a></li><li>The <a href=https://github.com/vikaschoudhary16/sfc-device-plugin>Solarflare device plugin</a></li><li>The <a href=https://github.com/intel/sriov-network-device-plugin>SR-IOV Network device plugin</a></li><li>The <a href=https://github.com/Xilinx/FPGA_as_a_Service/tree/master/k8s-device-plugin>Xilinx FPGA device plugins</a> for Xilinx FPGA devices</li></ul><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/tasks/manage-gpus/scheduling-gpus/>scheduling GPU resources</a> using device
plugins</li><li>Learn about <a href=/docs/tasks/administer-cluster/extended-resource-node/>advertising extended resources</a>
on a node</li><li>Learn about the <a href=/docs/tasks/administer-cluster/topology-manager/>Topology Manager</a></li><li>Read about using <a href=/blog/2019/04/24/hardware-accelerated-ssl/tls-termination-in-ingress-controllers-using-kubernetes-device-plugins-and-runtimeclass/>hardware acceleration for TLS ingress</a>
with Kubernetes</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-0af41d3bd7c785621b58b7564793396a>13.2 - Extending the Kubernetes API</h1></div><div class=td-content><h1 id=pg-342388440304e19ce30c0f8ada1c77ce>13.2.1 - Custom Resources</h1><p><em>Custom resources</em> are extensions of the Kubernetes API. This page discusses when to add a custom
resource to your Kubernetes cluster and when to use a standalone service. It describes the two
methods for adding custom resources and how to choose between them.</p><h2 id=custom-resources>Custom resources</h2><p>A <em>resource</em> is an endpoint in the <a href=/docs/concepts/overview/kubernetes-api/>Kubernetes API</a> that
stores a collection of <a href=/docs/concepts/overview/working-with-objects/kubernetes-objects/>API objects</a>
of a certain kind; for example, the built-in <em>pods</em> resource contains a collection of Pod objects.</p><p>A <em>custom resource</em> is an extension of the Kubernetes API that is not necessarily available in a default
Kubernetes installation. It represents a customization of a particular Kubernetes installation. However,
many core Kubernetes functions are now built using custom resources, making Kubernetes more modular.</p><p>Custom resources can appear and disappear in a running cluster through dynamic registration,
and cluster admins can update custom resources independently of the cluster itself.
Once a custom resource is installed, users can create and access its objects using
<a class=glossary-tooltip title='A command line tool for communicating with a Kubernetes cluster.' data-toggle=tooltip data-placement=top href=/docs/user-guide/kubectl-overview/ target=_blank aria-label=kubectl>kubectl</a>, just as they do for built-in resources
like <em>Pods</em>.</p><h2 id=custom-controllers>Custom controllers</h2><p>On their own, custom resources let you store and retrieve structured data.
When you combine a custom resource with a <em>custom controller</em>, custom resources
provide a true <em>declarative API</em>.</p><p>The Kubernetes <a href=/docs/concepts/overview/kubernetes-api/>declarative API</a>
enforces a separation of responsibilities. You declare the desired state of
your resource. The Kubernetes controller keeps the current state of Kubernetes
objects in sync with your declared desired state. This is in contrast to an
imperative API, where you <em>instruct</em> a server what to do.</p><p>You can deploy and update a custom controller on a running cluster, independently
of the cluster's lifecycle. Custom controllers can work with any kind of resource,
but they are especially effective when combined with custom resources. The
<a href=/docs/concepts/extend-kubernetes/operator/>Operator pattern</a> combines custom
resources and custom controllers. You can use custom controllers to encode domain knowledge
for specific applications into an extension of the Kubernetes API.</p><h2 id=should-i-add-a-custom-resource-to-my-kubernetes-cluster>Should I add a custom resource to my Kubernetes cluster?</h2><p>When creating a new API, consider whether to
<a href=/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>aggregate your API with the Kubernetes cluster APIs</a>
or let your API stand alone.</p><table><thead><tr><th>Consider API aggregation if:</th><th>Prefer a stand-alone API if:</th></tr></thead><tbody><tr><td>Your API is <a href=#declarative-apis>Declarative</a>.</td><td>Your API does not fit the <a href=#declarative-apis>Declarative</a> model.</td></tr><tr><td>You want your new types to be readable and writable using <code>kubectl</code>.</td><td><code>kubectl</code> support is not required</td></tr><tr><td>You want to view your new types in a Kubernetes UI, such as dashboard, alongside built-in types.</td><td>Kubernetes UI support is not required.</td></tr><tr><td>You are developing a new API.</td><td>You already have a program that serves your API and works well.</td></tr><tr><td>You are willing to accept the format restriction that Kubernetes puts on REST resource paths, such as API Groups and Namespaces. (See the <a href=/docs/concepts/overview/kubernetes-api/>API Overview</a>.)</td><td>You need to have specific REST paths to be compatible with an already defined REST API.</td></tr><tr><td>Your resources are naturally scoped to a cluster or namespaces of a cluster.</td><td>Cluster or namespace scoped resources are a poor fit; you need control over the specifics of resource paths.</td></tr><tr><td>You want to reuse <a href=#common-features>Kubernetes API support features</a>.</td><td>You don't need those features.</td></tr></tbody></table><h3 id=declarative-apis>Declarative APIs</h3><p>In a Declarative API, typically:</p><ul><li>Your API consists of a relatively small number of relatively small objects (resources).</li><li>The objects define configuration of applications or infrastructure.</li><li>The objects are updated relatively infrequently.</li><li>Humans often need to read and write the objects.</li><li>The main operations on the objects are CRUD-y (creating, reading, updating and deleting).</li><li>Transactions across objects are not required: the API represents a desired state, not an exact state.</li></ul><p>Imperative APIs are not declarative.
Signs that your API might not be declarative include:</p><ul><li>The client says "do this", and then gets a synchronous response back when it is done.</li><li>The client says "do this", and then gets an operation ID back, and has to check a separate
Operation object to determine completion of the request.</li><li>You talk about Remote Procedure Calls (RPCs).</li><li>Directly storing large amounts of data; for example, > a few kB per object, or > 1000s of objects.</li><li>High bandwidth access (10s of requests per second sustained) needed.</li><li>Store end-user data (such as images, PII, etc.) or other large-scale data processed by applications.</li><li>The natural operations on the objects are not CRUD-y.</li><li>The API is not easily modeled as objects.</li><li>You chose to represent pending operations with an operation ID or an operation object.</li></ul><h2 id=should-i-use-a-configmap-or-a-custom-resource>Should I use a ConfigMap or a custom resource?</h2><p>Use a ConfigMap if any of the following apply:</p><ul><li>There is an existing, well-documented configuration file format, such as a <code>mysql.cnf</code> or
<code>pom.xml</code>.</li><li>You want to put the entire configuration into one key of a ConfigMap.</li><li>The main use of the configuration file is for a program running in a Pod on your cluster to
consume the file to configure itself.</li><li>Consumers of the file prefer to consume via file in a Pod or environment variable in a pod,
rather than the Kubernetes API.</li><li>You want to perform rolling updates via Deployment, etc., when the file is updated.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Use a <a class=glossary-tooltip title='Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secret>Secret</a> for sensitive data, which is similar
to a ConfigMap but more secure.</div><p>Use a custom resource (CRD or Aggregated API) if most of the following apply:</p><ul><li>You want to use Kubernetes client libraries and CLIs to create and update the new resource.</li><li>You want top-level support from <code>kubectl</code>; for example, <code>kubectl get my-object object-name</code>.</li><li>You want to build new automation that watches for updates on the new object, and then CRUD other
objects, or vice versa.</li><li>You want to write automation that handles updates to the object.</li><li>You want to use Kubernetes API conventions like <code>.spec</code>, <code>.status</code>, and <code>.metadata</code>.</li><li>You want the object to be an abstraction over a collection of controlled resources, or a
summarization of other resources.</li></ul><h2 id=adding-custom-resources>Adding custom resources</h2><p>Kubernetes provides two ways to add custom resources to your cluster:</p><ul><li>CRDs are simple and can be created without any programming.</li><li><a href=/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>API Aggregation</a>
requires programming, but allows more control over API behaviors like how data is stored and
conversion between API versions.</li></ul><p>Kubernetes provides these two options to meet the needs of different users, so that neither ease
of use nor flexibility is compromised.</p><p>Aggregated APIs are subordinate API servers that sit behind the primary API server, which acts as
a proxy. This arrangement is called <a href=/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>API Aggregation</a>(AA).
To users, the Kubernetes API appears extended.</p><p>CRDs allow users to create new types of resources without adding another API server. You do not
need to understand API Aggregation to use CRDs.</p><p>Regardless of how they are installed, the new resources are referred to as Custom Resources to
distinguish them from built-in Kubernetes resources (like pods).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Avoid using a Custom Resource as data storage for application, end user, or monitoring data:
architecture designs that store application data within the Kubernetes API typically represent
a design that is too closely coupled.</p><p>Architecturally, <a href=https://www.cncf.io/about/faq/#what-is-cloud-native>cloud native</a> application architectures
favor loose coupling between components. If part of your workload requires a backing service for
its routine operation, run that backing service as a component or consume it as an external service.
This way, your workload does not rely on the Kubernetes API for its normal operation.</p></div><h2 id=customresourcedefinitions>CustomResourceDefinitions</h2><p>The <a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/>CustomResourceDefinition</a>
API resource allows you to define custom resources.
Defining a CRD object creates a new custom resource with a name and schema that you specify.
The Kubernetes API serves and handles the storage of your custom resource.
The name of a CRD object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>This frees you from writing your own API server to handle the custom resource,
but the generic nature of the implementation means you have less flexibility than with
<a href=#api-server-aggregation>API server aggregation</a>.</p><p>Refer to the <a href=https://github.com/kubernetes/sample-controller>custom controller example</a>
for an example of how to register a new custom resource, work with instances of your new resource type,
and use a controller to handle events.</p><h2 id=api-server-aggregation>API server aggregation</h2><p>Usually, each resource in the Kubernetes API requires code that handles REST requests and manages
persistent storage of objects. The main Kubernetes API server handles built-in resources like
<em>pods</em> and <em>services</em>, and can also generically handle custom resources through
<a href=#customresourcedefinitions>CRDs</a>.</p><p>The <a href=/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>aggregation layer</a>
allows you to provide specialized implementations for your custom resources by writing and
deploying your own API server.
The main API server delegates requests to your API server for the custom resources that you handle,
making them available to all of its clients.</p><h2 id=choosing-a-method-for-adding-custom-resources>Choosing a method for adding custom resources</h2><p>CRDs are easier to use. Aggregated APIs are more flexible. Choose the method that best meets your needs.</p><p>Typically, CRDs are a good fit if:</p><ul><li>You have a handful of fields</li><li>You are using the resource within your company, or as part of a small open-source project (as
opposed to a commercial product)</li></ul><h3 id=comparing-ease-of-use>Comparing ease of use</h3><p>CRDs are easier to create than Aggregated APIs.</p><table><thead><tr><th>CRDs</th><th>Aggregated API</th></tr></thead><tbody><tr><td>Do not require programming. Users can choose any language for a CRD controller.</td><td>Requires programming and building binary and image.</td></tr><tr><td>No additional service to run; CRDs are handled by API server.</td><td>An additional service to create and that could fail.</td></tr><tr><td>No ongoing support once the CRD is created. Any bug fixes are picked up as part of normal Kubernetes Master upgrades.</td><td>May need to periodically pickup bug fixes from upstream and rebuild and update the Aggregated API server.</td></tr><tr><td>No need to handle multiple versions of your API; for example, when you control the client for this resource, you can upgrade it in sync with the API.</td><td>You need to handle multiple versions of your API; for example, when developing an extension to share with the world.</td></tr></tbody></table><h3 id=advanced-features-and-flexibility>Advanced features and flexibility</h3><p>Aggregated APIs offer more advanced API features and customization of other features; for example, the storage layer.</p><table><thead><tr><th>Feature</th><th>Description</th><th>CRDs</th><th>Aggregated API</th></tr></thead><tbody><tr><td>Validation</td><td>Help users prevent errors and allow you to evolve your API independently of your clients. These features are most useful when there are many clients who can't all update at the same time.</td><td>Yes. Most validation can be specified in the CRD using <a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation>OpenAPI v3.0 validation</a>. Any other validations supported by addition of a <a href=/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook-alpha-in-1-8-beta-in-1-9>Validating Webhook</a>.</td><td>Yes, arbitrary validation checks</td></tr><tr><td>Defaulting</td><td>See above</td><td>Yes, either via <a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting>OpenAPI v3.0 validation</a> <code>default</code> keyword (GA in 1.17), or via a <a href=/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook>Mutating Webhook</a> (though this will not be run when reading from etcd for old objects).</td><td>Yes</td></tr><tr><td>Multi-versioning</td><td>Allows serving the same object through two API versions. Can help ease API changes like renaming fields. Less important if you control your client versions.</td><td><a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning>Yes</a></td><td>Yes</td></tr><tr><td>Custom Storage</td><td>If you need storage with a different performance mode (for example, a time-series database instead of key-value store) or isolation for security (for example, encryption of sensitive information, etc.)</td><td>No</td><td>Yes</td></tr><tr><td>Custom Business Logic</td><td>Perform arbitrary checks or actions when creating, reading, updating or deleting an object</td><td>Yes, using <a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>Webhooks</a>.</td><td>Yes</td></tr><tr><td>Scale Subresource</td><td>Allows systems like HorizontalPodAutoscaler and PodDisruptionBudget interact with your new resource</td><td><a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#scale-subresource>Yes</a></td><td>Yes</td></tr><tr><td>Status Subresource</td><td>Allows fine-grained access control where user writes the spec section and the controller writes the status section. Allows incrementing object Generation on custom resource data mutation (requires separate spec and status sections in the resource)</td><td><a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource>Yes</a></td><td>Yes</td></tr><tr><td>Other Subresources</td><td>Add operations other than CRUD, such as "logs" or "exec".</td><td>No</td><td>Yes</td></tr><tr><td>strategic-merge-patch</td><td>The new endpoints support PATCH with <code>Content-Type: application/strategic-merge-patch+json</code>. Useful for updating objects that may be modified both locally, and by the server. For more information, see <a href=/docs/tasks/manage-kubernetes-objects/update-api-object-kubectl-patch/>"Update API Objects in Place Using kubectl patch"</a></td><td>No</td><td>Yes</td></tr><tr><td>Protocol Buffers</td><td>The new resource supports clients that want to use Protocol Buffers</td><td>No</td><td>Yes</td></tr><tr><td>OpenAPI Schema</td><td>Is there an OpenAPI (swagger) schema for the types that can be dynamically fetched from the server? Is the user protected from misspelling field names by ensuring only allowed fields are set? Are types enforced (in other words, don't put an <code>int</code> in a <code>string</code> field?)</td><td>Yes, based on the <a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation>OpenAPI v3.0 validation</a> schema (GA in 1.16).</td><td>Yes</td></tr></tbody></table><h3 id=common-features>Common Features</h3><p>When you create a custom resource, either via a CRD or an AA, you get many features for your API,
compared to implementing it outside the Kubernetes platform:</p><table><thead><tr><th>Feature</th><th>What it does</th></tr></thead><tbody><tr><td>CRUD</td><td>The new endpoints support CRUD basic operations via HTTP and <code>kubectl</code></td></tr><tr><td>Watch</td><td>The new endpoints support Kubernetes Watch operations via HTTP</td></tr><tr><td>Discovery</td><td>Clients like <code>kubectl</code> and dashboard automatically offer list, display, and field edit operations on your resources</td></tr><tr><td>json-patch</td><td>The new endpoints support PATCH with <code>Content-Type: application/json-patch+json</code></td></tr><tr><td>merge-patch</td><td>The new endpoints support PATCH with <code>Content-Type: application/merge-patch+json</code></td></tr><tr><td>HTTPS</td><td>The new endpoints uses HTTPS</td></tr><tr><td>Built-in Authentication</td><td>Access to the extension uses the core API server (aggregation layer) for authentication</td></tr><tr><td>Built-in Authorization</td><td>Access to the extension can reuse the authorization used by the core API server; for example, RBAC.</td></tr><tr><td>Finalizers</td><td>Block deletion of extension resources until external cleanup happens.</td></tr><tr><td>Admission Webhooks</td><td>Set default values and validate extension resources during any create/update/delete operation.</td></tr><tr><td>UI/CLI Display</td><td>Kubectl, dashboard can display extension resources.</td></tr><tr><td>Unset versus Empty</td><td>Clients can distinguish unset fields from zero-valued fields.</td></tr><tr><td>Client Libraries Generation</td><td>Kubernetes provides generic client libraries, as well as tools to generate type-specific client libraries.</td></tr><tr><td>Labels and annotations</td><td>Common metadata across objects that tools know how to edit for core and custom resources.</td></tr></tbody></table><h2 id=preparing-to-install-a-custom-resource>Preparing to install a custom resource</h2><p>There are several points to be aware of before adding a custom resource to your cluster.</p><h3 id=third-party-code-and-new-points-of-failure>Third party code and new points of failure</h3><p>While creating a CRD does not automatically add any new points of failure (for example, by causing
third party code to run on your API server), packages (for example, Charts) or other installation
bundles often include CRDs as well as a Deployment of third-party code that implements the
business logic for a new custom resource.</p><p>Installing an Aggregated API server always involves running a new Deployment.</p><h3 id=storage>Storage</h3><p>Custom resources consume storage space in the same way that ConfigMaps do. Creating too many
custom resources may overload your API server's storage space.</p><p>Aggregated API servers may use the same storage as the main API server, in which case the same
warning applies.</p><h3 id=authentication-authorization-and-auditing>Authentication, authorization, and auditing</h3><p>CRDs always use the same authentication, authorization, and audit logging as the built-in
resources of your API server.</p><p>If you use RBAC for authorization, most RBAC roles will not grant access to the new resources
(except the cluster-admin role or any role created with wildcard rules). You'll need to explicitly
grant access to the new resources. CRDs and Aggregated APIs often come bundled with new role
definitions for the types they add.</p><p>Aggregated API servers may or may not use the same authentication, authorization, and auditing as
the primary API server.</p><h2 id=accessing-a-custom-resource>Accessing a custom resource</h2><p>Kubernetes <a href=/docs/reference/using-api/client-libraries/>client libraries</a> can be used to access
custom resources. Not all client libraries support custom resources. The <em>Go</em> and <em>Python</em> client
libraries do.</p><p>When you add a custom resource, you can access it using:</p><ul><li><code>kubectl</code></li><li>The Kubernetes dynamic client.</li><li>A REST client that you write.</li><li>A client generated using <a href=https://github.com/kubernetes/code-generator>Kubernetes client generation tools</a>
(generating one is an advanced undertaking, but some projects may provide a client along with
the CRD or AA).</li></ul><h2 id=what-s-next>What's next</h2><ul><li>Learn how to <a href=/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/>Extend the Kubernetes API with the aggregation layer</a>.</li><li>Learn how to <a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/>Extend the Kubernetes API with CustomResourceDefinition</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1ea4977c0ebf97569bf54a477faa7fa5>13.2.2 - Kubernetes API Aggregation Layer</h1><p>The aggregation layer allows Kubernetes to be extended with additional APIs, beyond what is
offered by the core Kubernetes APIs.
The additional APIs can either be ready-made solutions such as a
<a href=https://github.com/kubernetes-sigs/metrics-server>metrics server</a>, or APIs that you develop yourself.</p><p>The aggregation layer is different from
<a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>Custom Resources</a>,
which are a way to make the <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label=kube-apiserver>kube-apiserver</a>
recognise new kinds of object.</p><h2 id=aggregation-layer>Aggregation layer</h2><p>The aggregation layer runs in-process with the kube-apiserver. Until an extension resource is
registered, the aggregation layer will do nothing. To register an API, you add an <em>APIService</em>
object, which "claims" the URL path in the Kubernetes API. At that point, the aggregation layer
will proxy anything sent to that API path (e.g. <code>/apis/myextension.mycompany.io/v1/…</code>) to the
registered APIService.</p><p>The most common way to implement the APIService is to run an <em>extension API server</em> in Pod(s) that
run in your cluster. If you're using the extension API server to manage resources in your cluster,
the extension API server (also written as "extension-apiserver") is typically paired with one or
more <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controllers>controllers</a>. The apiserver-builder
library provides a skeleton for both extension API servers and the associated controller(s).</p><h3 id=response-latency>Response latency</h3><p>Extension API servers should have low latency networking to and from the kube-apiserver.
Discovery requests are required to round-trip from the kube-apiserver in five seconds or less.</p><p>If your extension API server cannot achieve that latency requirement, consider making changes that
let you meet it.</p><h2 id=what-s-next>What's next</h2><ul><li>To get the aggregator working in your environment, <a href=/docs/tasks/extend-kubernetes/configure-aggregation-layer/>configure the aggregation layer</a>.</li><li>Then, <a href=/docs/tasks/extend-kubernetes/setup-extension-api-server/>setup an extension api-server</a> to work with the aggregation layer.</li><li>Read about <a href=/docs/reference/kubernetes-api/cluster-resources/api-service-v1/>APIService</a> in the API reference</li></ul><p>Alternatively: learn how to
<a href=/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/>extend the Kubernetes API using Custom Resource Definitions</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-3131452556176159fb269593c1a52012>13.3 - Operator pattern</h1><p>Operators are software extensions to Kubernetes that make use of
<a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>custom resources</a>
to manage applications and their components. Operators follow
Kubernetes principles, notably the <a href=/docs/concepts/architecture/controller>control loop</a>.</p><h2 id=motivation>Motivation</h2><p>The <em>operator pattern</em> aims to capture the key aim of a human operator who
is managing a service or set of services. Human operators who look after
specific applications and services have deep knowledge of how the system
ought to behave, how to deploy it, and how to react if there are problems.</p><p>People who run workloads on Kubernetes often like to use automation to take
care of repeatable tasks. The operator pattern captures how you can write
code to automate a task beyond what Kubernetes itself provides.</p><h2 id=operators-in-kubernetes>Operators in Kubernetes</h2><p>Kubernetes is designed for automation. Out of the box, you get lots of
built-in automation from the core of Kubernetes. You can use Kubernetes
to automate deploying and running workloads, <em>and</em> you can automate how
Kubernetes does that.</p><p>Kubernetes' <a class=glossary-tooltip title='A specialized controller used to manage a custom resource' data-toggle=tooltip data-placement=top href=/docs/concepts/extend-kubernetes/operator/ target=_blank aria-label='operator pattern'>operator pattern</a>
concept lets you extend the cluster's behaviour without modifying the code of Kubernetes
itself by linking <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controllers>controllers</a> to
one or more custom resources. Operators are clients of the Kubernetes API that act as
controllers for a <a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>Custom Resource</a>.</p><h2 id=example>An example operator</h2><p>Some of the things that you can use an operator to automate include:</p><ul><li>deploying an application on demand</li><li>taking and restoring backups of that application's state</li><li>handling upgrades of the application code alongside related changes such
as database schemas or extra configuration settings</li><li>publishing a Service to applications that don't support Kubernetes APIs to
discover them</li><li>simulating failure in all or part of your cluster to test its resilience</li><li>choosing a leader for a distributed application without an internal
member election process</li></ul><p>What might an operator look like in more detail? Here's an example:</p><ol><li>A custom resource named SampleDB, that you can configure into the cluster.</li><li>A Deployment that makes sure a Pod is running that contains the
controller part of the operator.</li><li>A container image of the operator code.</li><li>Controller code that queries the control plane to find out what SampleDB
resources are configured.</li><li>The core of the operator is code to tell the API server how to make
reality match the configured resources.<ul><li>If you add a new SampleDB, the operator sets up PersistentVolumeClaims
to provide durable database storage, a StatefulSet to run SampleDB and
a Job to handle initial configuration.</li><li>If you delete it, the operator takes a snapshot, then makes sure that
the StatefulSet and Volumes are also removed.</li></ul></li><li>The operator also manages regular database backups. For each SampleDB
resource, the operator determines when to create a Pod that can connect
to the database and take backups. These Pods would rely on a ConfigMap
and / or a Secret that has database connection details and credentials.</li><li>Because the operator aims to provide robust automation for the resource
it manages, there would be additional supporting code. For this example,
code checks to see if the database is running an old version and, if so,
creates Job objects that upgrade it for you.</li></ol><h2 id=deploying-operators>Deploying operators</h2><p>The most common way to deploy an operator is to add the
Custom Resource Definition and its associated Controller to your cluster.
The Controller will normally run outside of the
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>,
much as you would run any containerized application.
For example, you can run the controller in your cluster as a Deployment.</p><h2 id=using-operators>Using an operator</h2><p>Once you have an operator deployed, you'd use it by adding, modifying or
deleting the kind of resource that the operator uses. Following the above
example, you would set up a Deployment for the operator itself, and then:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get SampleDB                   <span style=color:#080;font-style:italic># find configured databases</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl edit SampleDB/example-database <span style=color:#080;font-style:italic># manually change some settings</span>
</span></span></code></pre></div><p>…and that's it! The operator will take care of applying the changes
as well as keeping the existing service in good shape.</p><h2 id=writing-operator>Writing your own operator</h2><p>If there isn't an operator in the ecosystem that implements the behavior you
want, you can code your own.</p><p>You also implement an operator (that is, a Controller) using any language / runtime
that can act as a <a href=/docs/reference/using-api/client-libraries/>client for the Kubernetes API</a>.</p><p>Following are a few libraries and tools you can use to write your own cloud native
operator.</p><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><ul><li><a href=https://juju.is/>Charmed Operator Framework</a></li><li><a href=https://github.com/java-operator-sdk/java-operator-sdk>Java Operator SDK</a></li><li><a href=https://github.com/nolar/kopf>Kopf</a> (Kubernetes Operator Pythonic Framework)</li><li><a href=https://kube.rs/>kube-rs</a> (Rust)</li><li><a href=https://book.kubebuilder.io/>kubebuilder</a></li><li><a href=https://buehler.github.io/dotnet-operator-sdk/>KubeOps</a> (.NET operator SDK)</li><li><a href=https://kudo.dev/>KUDO</a> (Kubernetes Universal Declarative Operator)</li><li><a href=https://metacontroller.github.io/metacontroller/intro.html>Metacontroller</a> along with WebHooks that
you implement yourself</li><li><a href=https://operatorframework.io>Operator Framework</a></li><li><a href=https://github.com/flant/shell-operator>shell-operator</a></li></ul><h2 id=what-s-next>What's next</h2><ul><li>Read the <a class=glossary-tooltip title='Cloud Native Computing Foundation' data-toggle=tooltip data-placement=top href=https://cncf.io/ target=_blank aria-label=CNCF>CNCF</a>
<a href=https://github.com/cncf/tag-app-delivery/blob/eece8f7307f2970f46f100f51932db106db46968/operator-wg/whitepaper/Operator-WhitePaper_v1-0.md>Operator White Paper</a>.</li><li>Learn more about <a href=/docs/concepts/extend-kubernetes/api-extension/custom-resources/>Custom Resources</a></li><li>Find ready-made operators on <a href=https://operatorhub.io/>OperatorHub.io</a> to suit your use case</li><li><a href=https://operatorhub.io/>Publish</a> your operator for other people to use</li><li>Read <a href=https://web.archive.org/web/20170129131616/https://coreos.com/blog/introducing-operators.html>CoreOS' original article</a>
that introduced the operator pattern (this is an archived version of the original article).</li><li>Read an <a href=https://cloud.google.com/blog/products/containers-kubernetes/best-practices-for-building-kubernetes-operators-and-stateful-apps>article</a>
from Google Cloud about best practices for building operators</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>