<!doctype html><html lang=en class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/containers/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/containers/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/containers/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/containers/><link rel=alternate hreflang=it href=https://kubernetes.io/it/docs/concepts/containers/><link rel=alternate hreflang=de href=https://kubernetes.io/de/docs/concepts/containers/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/containers/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/containers/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/containers/><link rel=alternate hreflang=vi href=https://kubernetes.io/vi/docs/concepts/containers/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/concepts/containers/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Containers | Kubernetes</title><meta property="og:title" content="Containers"><meta property="og:description" content="Technology for packaging an application along with its runtime dependencies."><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/concepts/containers/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Containers"><meta itemprop=description content="Technology for packaging an application along with its runtime dependencies."><meta name=twitter:card content="summary"><meta name=twitter:title content="Containers"><meta name=twitter:description content="Technology for packaging an application along with its runtime dependencies."><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Technology for packaging an application along with its runtime dependencies."><meta property="og:description" content="Technology for packaging an application along with its runtime dependencies."><meta name=twitter:description content="Technology for packaging an application along with its runtime dependencies."><meta property="og:url" content="https://kubernetes.io/docs/concepts/containers/"><meta property="og:title" content="Containers"><meta name=twitter:title content="Containers"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/concepts/containers/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/concepts/containers/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/concepts/containers/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/concepts/containers/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/concepts/containers/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/docs/concepts/containers/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/containers/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/containers/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/containers/>Français (French)</a>
<a class=dropdown-item href=/it/docs/concepts/containers/>Italiano (Italian)</a>
<a class=dropdown-item href=/de/docs/concepts/containers/>Deutsch (German)</a>
<a class=dropdown-item href=/es/docs/concepts/containers/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/containers/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/containers/>Bahasa Indonesia</a>
<a class=dropdown-item href=/vi/docs/concepts/containers/>Tiếng Việt (Vietnamese)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/concepts/containers/>Return to the regular view of this page</a>.</p></div><h1 class=title>Containers</h1><div class=lead>Technology for packaging an application along with its runtime dependencies.</div><ul><li>1: <a href=#pg-16042b4652ad19e565c7263824029a43>Images</a></li><li>2: <a href=#pg-643212488f778acf04bebed65ba34441>Container Environment</a></li><li>3: <a href=#pg-a858027489648786a3b16264e451272b>Runtime Class</a></li><li>4: <a href=#pg-e6941d969d81540208a3e78bc56f43bc>Container Lifecycle Hooks</a></li></ul><div class=content><p>Each container that you run is repeatable; the standardization from having
dependencies included means that you get the same behavior wherever you
run it.</p><p>Containers decouple applications from underlying host infrastructure.
This makes deployment easier in different cloud or OS environments.</p><p>Each <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> in a Kubernetes
cluster runs the containers that form the
<a href=/docs/concepts/workloads/pods/>Pods</a> assigned to that node.
Containers in a Pod are co-located and co-scheduled to run on the same node.</p><h2 id=container-images>Container images</h2><p>A <a href=/docs/concepts/containers/images/>container image</a> is a ready-to-run
software package, containing everything needed to run an application:
the code and any runtime it requires, application and system libraries,
and default values for any essential settings.</p><p>Containers are intended to be stateless and
<a href=https://glossary.cncf.io/immutable-infrastructure/>immutable</a>:
you should not change
the code of a container that is already running. If you have a containerized
application and want to make changes, the correct process is to build a new
image that includes the change, then recreate the container to start from the
updated image.</p><h2 id=container-runtimes>Container runtimes</h2><p>The container runtime is the software that is responsible for running containers.</p><p>Kubernetes supports container runtimes such as
<a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a>, <a class=glossary-tooltip title='A lightweight container runtime specifically for Kubernetes' data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a>,
and any other implementation of the <a href=https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md>Kubernetes CRI (Container Runtime
Interface)</a>.</p><p>Usually, you can allow your cluster to pick the default container runtime
for a Pod. If you need to use more than one container runtime in your cluster,
you can specify the <a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a>
for a Pod to make sure that Kubernetes runs those containers using a
particular container runtime.</p><p>You can also use RuntimeClass to run different Pods with the same container
runtime but with different settings.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-16042b4652ad19e565c7263824029a43>1 - Images</h1><p>A container image represents binary data that encapsulates an application and all its
software dependencies. Container images are executable software bundles that can run
standalone and that make very well defined assumptions about their runtime environment.</p><p>You typically create a container image of your application and push it to a registry
before referring to it in a
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a></p><p>This page provides an outline of the container image concept.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you are looking for the container images for a Kubernetes
release (such as v1.25, the latest minor release),
visit <a href=https://kubernetes.io/releases/download/>Download Kubernetes</a>.</div><h2 id=image-names>Image names</h2><p>Container images are usually given a name such as <code>pause</code>, <code>example/mycontainer</code>, or <code>kube-apiserver</code>.
Images can also include a registry hostname; for example: <code>fictional.registry.example/imagename</code>,
and possibly a port number as well; for example: <code>fictional.registry.example:10443/imagename</code>.</p><p>If you don't specify a registry hostname, Kubernetes assumes that you mean the Docker public registry.</p><p>After the image name part you can add a <em>tag</em> (in the same way you would when using with commands like <code>docker</code> or <code>podman</code>).
Tags let you identify different versions of the same series of images.</p><p>Image tags consist of lowercase and uppercase letters, digits, underscores (<code>_</code>),
periods (<code>.</code>), and dashes (<code>-</code>).<br>There are additional rules about where you can place the separator
characters (<code>_</code>, <code>-</code>, and <code>.</code>) inside an image tag.<br>If you don't specify a tag, Kubernetes assumes you mean the tag <code>latest</code>.</p><h2 id=updating-images>Updating images</h2><p>When you first create a <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>,
<a class=glossary-tooltip title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>, Pod, or other
object that includes a Pod template, then by default the pull policy of all
containers in that pod will be set to <code>IfNotPresent</code> if it is not explicitly
specified. This policy causes the
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> to skip pulling an
image if it already exists.</p><h3 id=image-pull-policy>Image pull policy</h3><p>The <code>imagePullPolicy</code> for a container and the tag of the image affect when the
<a href=/docs/reference/command-line-tools-reference/kubelet/>kubelet</a> attempts to pull (download) the specified image.</p><p>Here's a list of the values you can set for <code>imagePullPolicy</code> and the effects
these values have:</p><dl><dt><code>IfNotPresent</code></dt><dd>the image is pulled only if it is not already present locally.</dd><dt><code>Always</code></dt><dd>every time the kubelet launches a container, the kubelet queries the container
image registry to resolve the name to an image
<a href=https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier>digest</a>. If the kubelet has a
container image with that exact digest cached locally, the kubelet uses its cached
image; otherwise, the kubelet pulls the image with the resolved digest,
and uses that image to launch the container.</dd><dt><code>Never</code></dt><dd>the kubelet does not try fetching the image. If the image is somehow already present
locally, the kubelet attempts to start the container; otherwise, startup fails.
See <a href=#pre-pulled-images>pre-pulled images</a> for more details.</dd></dl><p>The caching semantics of the underlying image provider make even
<code>imagePullPolicy: Always</code> efficient, as long as the registry is reliably accessible.
Your container runtime can notice that the image layers already exist on the node
so that they don't need to be downloaded again.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>You should avoid using the <code>:latest</code> tag when deploying containers in production as
it is harder to track which version of the image is running and more difficult to
roll back properly.</p><p>Instead, specify a meaningful tag such as <code>v1.42.0</code>.</p></div><p>To make sure the Pod always uses the same version of a container image, you can specify
the image's digest;
replace <code>&lt;image-name>:&lt;tag></code> with <code>&lt;image-name>@&lt;digest></code>
(for example, <code>image@sha256:45b23dee08af5e43a7fea6c4cf9c25ccf269ee113168c19722f87876677c5cb2</code>).</p><p>When using image tags, if the image registry were to change the code that the tag on that image represents, you might end up with a mix of Pods running the old and new code. An image digest uniquely identifies a specific version of the image, so Kubernetes runs the same code every time it starts a container with that image name and digest specified. Specifying an image by digest fixes the code that you run so that a change at the registry cannot lead to that mix of versions.</p><p>There are third-party <a href=/docs/reference/access-authn-authz/admission-controllers/>admission controllers</a>
that mutate Pods (and pod templates) when they are created, so that the
running workload is defined based on an image digest rather than a tag.
That might be useful if you want to make sure that all your workload is
running the same code no matter what tag changes happen at the registry.</p><h4 id=imagepullpolicy-defaulting>Default image pull policy</h4><p>When you (or a controller) submit a new Pod to the API server, your cluster sets the
<code>imagePullPolicy</code> field when specific conditions are met:</p><ul><li>if you omit the <code>imagePullPolicy</code> field, and the tag for the container image is
<code>:latest</code>, <code>imagePullPolicy</code> is automatically set to <code>Always</code>;</li><li>if you omit the <code>imagePullPolicy</code> field, and you don't specify the tag for the
container image, <code>imagePullPolicy</code> is automatically set to <code>Always</code>;</li><li>if you omit the <code>imagePullPolicy</code> field, and you specify the tag for the
container image that isn't <code>:latest</code>, the <code>imagePullPolicy</code> is automatically set to
<code>IfNotPresent</code>.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>The value of <code>imagePullPolicy</code> of the container is always set when the object is
first <em>created</em>, and is not updated if the image's tag later changes.</p><p>For example, if you create a Deployment with an image whose tag is <em>not</em>
<code>:latest</code>, and later update that Deployment's image to a <code>:latest</code> tag, the
<code>imagePullPolicy</code> field will <em>not</em> change to <code>Always</code>. You must manually change
the pull policy of any object after its initial creation.</p></div><h4 id=required-image-pull>Required image pull</h4><p>If you would like to always force a pull, you can do one of the following:</p><ul><li>Set the <code>imagePullPolicy</code> of the container to <code>Always</code>.</li><li>Omit the <code>imagePullPolicy</code> and use <code>:latest</code> as the tag for the image to use;
Kubernetes will set the policy to <code>Always</code> when you submit the Pod.</li><li>Omit the <code>imagePullPolicy</code> and the tag for the image to use;
Kubernetes will set the policy to <code>Always</code> when you submit the Pod.</li><li>Enable the <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>AlwaysPullImages</a> admission controller.</li></ul><h3 id=imagepullbackoff>ImagePullBackOff</h3><p>When a kubelet starts creating containers for a Pod using a container runtime,
it might be possible the container is in <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-state-waiting>Waiting</a>
state because of <code>ImagePullBackOff</code>.</p><p>The status <code>ImagePullBackOff</code> means that a container could not start because Kubernetes
could not pull a container image (for reasons such as invalid image name, or pulling
from a private registry without <code>imagePullSecret</code>). The <code>BackOff</code> part indicates
that Kubernetes will keep trying to pull the image, with an increasing back-off delay.</p><p>Kubernetes raises the delay between each attempt until it reaches a compiled-in limit,
which is 300 seconds (5 minutes).</p><h2 id=multi-architecture-images-with-image-indexes>Multi-architecture images with image indexes</h2><p>As well as providing binary images, a container registry can also serve a <a href=https://github.com/opencontainers/image-spec/blob/master/image-index.md>container image index</a>. An image index can point to multiple <a href=https://github.com/opencontainers/image-spec/blob/master/manifest.md>image manifests</a> for architecture-specific versions of a container. The idea is that you can have a name for an image (for example: <code>pause</code>, <code>example/mycontainer</code>, <code>kube-apiserver</code>) and allow different systems to fetch the right binary image for the machine architecture they are using.</p><p>Kubernetes itself typically names container images with a suffix <code>-$(ARCH)</code>. For backward compatibility, please generate the older images with suffixes. The idea is to generate say <code>pause</code> image which has the manifest for all the arch(es) and say <code>pause-amd64</code> which is backwards compatible for older configurations or YAML files which may have hard coded the images with suffixes.</p><h2 id=using-a-private-registry>Using a private registry</h2><p>Private registries may require keys to read images from them.<br>Credentials can be provided in several ways:</p><ul><li>Configuring Nodes to Authenticate to a Private Registry<ul><li>all pods can read any configured private registries</li><li>requires node configuration by cluster administrator</li></ul></li><li>Pre-pulled Images<ul><li>all pods can use any images cached on a node</li><li>requires root access to all nodes to set up</li></ul></li><li>Specifying ImagePullSecrets on a Pod<ul><li>only pods which provide own keys can access the private registry</li></ul></li><li>Vendor-specific or local extensions<ul><li>if you're using a custom node configuration, you (or your cloud
provider) can implement your mechanism for authenticating the node
to the container registry.</li></ul></li></ul><p>These options are explained in more detail below.</p><h3 id=configuring-nodes-to-authenticate-to-a-private-registry>Configuring nodes to authenticate to a private registry</h3><p>Specific instructions for setting credentials depends on the container runtime and registry you chose to use. You should refer to your solution's documentation for the most accurate information.</p><p>For an example of configuring a private container image registry, see the
<a href=/docs/tasks/configure-pod-container/pull-image-private-registry>Pull an Image from a Private Registry</a>
task. That example uses a private registry in Docker Hub.</p><h3 id=config-json>Interpretation of config.json</h3><p>The interpretation of <code>config.json</code> varies between the original Docker
implementation and the Kubernetes interpretation. In Docker, the <code>auths</code> keys
can only specify root URLs, whereas Kubernetes allows glob URLs as well as
prefix-matched paths. This means that a <code>config.json</code> like this is valid:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;auths&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;*my-registry.io/images&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:green;font-weight:700>&#34;auth&#34;</span>: <span style=color:#b44>&#34;…&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The root URL (<code>*my-registry.io</code>) is matched by using the following syntax:</p><pre tabindex=0><code>pattern:
    { term }

term:
    &#39;*&#39;         matches any sequence of non-Separator characters
    &#39;?&#39;         matches any single non-Separator character
    &#39;[&#39; [ &#39;^&#39; ] { character-range } &#39;]&#39;
                character class (must be non-empty)
    c           matches character c (c != &#39;*&#39;, &#39;?&#39;, &#39;\\&#39;, &#39;[&#39;)
    &#39;\\&#39; c      matches character c

character-range:
    c           matches character c (c != &#39;\\&#39;, &#39;-&#39;, &#39;]&#39;)
    &#39;\\&#39; c      matches character c
    lo &#39;-&#39; hi   matches character c for lo &lt;= c &lt;= hi
</code></pre><p>Image pull operations would now pass the credentials to the CRI container
runtime for every valid pattern. For example the following container image names
would match successfully:</p><ul><li><code>my-registry.io/images</code></li><li><code>my-registry.io/images/my-image</code></li><li><code>my-registry.io/images/another-image</code></li><li><code>sub.my-registry.io/images/my-image</code></li><li><code>a.sub.my-registry.io/images/my-image</code></li></ul><p>The kubelet performs image pulls sequentially for every found credential. This
means, that multiple entries in <code>config.json</code> are possible, too:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;auths&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;my-registry.io/images&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:green;font-weight:700>&#34;auth&#34;</span>: <span style=color:#b44>&#34;…&#34;</span>
</span></span><span style=display:flex><span>        },
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;my-registry.io/images/subpath&#34;</span>: {
</span></span><span style=display:flex><span>            <span style=color:green;font-weight:700>&#34;auth&#34;</span>: <span style=color:#b44>&#34;…&#34;</span>
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If now a container specifies an image <code>my-registry.io/images/subpath/my-image</code>
to be pulled, then the kubelet will try to download them from both
authentication sources if one of them fails.</p><h3 id=pre-pulled-images>Pre-pulled images</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This approach is suitable if you can control node configuration. It
will not work reliably if your cloud provider manages nodes and replaces
them automatically.</div><p>By default, the kubelet tries to pull each image from the specified registry.
However, if the <code>imagePullPolicy</code> property of the container is set to <code>IfNotPresent</code> or <code>Never</code>,
then a local image is used (preferentially or exclusively, respectively).</p><p>If you want to rely on pre-pulled images as a substitute for registry authentication,
you must ensure all nodes in the cluster have the same pre-pulled images.</p><p>This can be used to preload certain images for speed or as an alternative to authenticating to a private registry.</p><p>All pods will have read access to any pre-pulled images.</p><h3 id=specifying-imagepullsecrets-on-a-pod>Specifying imagePullSecrets on a Pod</h3><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This is the recommended approach to run containers based on images
in private registries.</div><p>Kubernetes supports specifying container image registry keys on a Pod.
<code>imagePullSecrets</code> must all be in the same namespace as the Pod. The referenced
Secrets must be of type <code>kubernetes.io/dockercfg</code> or <code>kubernetes.io/dockerconfigjson</code>.</p><h4 id=creating-a-secret-with-a-docker-config>Creating a Secret with a Docker config</h4><p>You need to know the username, registry password and client email address for authenticating
to the registry, as well as its hostname.
Run the following command, substituting the appropriate uppercase values:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret docker-registry &lt;name&gt; --docker-server<span style=color:#666>=</span>DOCKER_REGISTRY_SERVER --docker-username<span style=color:#666>=</span>DOCKER_USER --docker-password<span style=color:#666>=</span>DOCKER_PASSWORD --docker-email<span style=color:#666>=</span>DOCKER_EMAIL
</span></span></code></pre></div><p>If you already have a Docker credentials file then, rather than using the above
command, you can import the credentials file as a Kubernetes
<a class=glossary-tooltip title='Stores sensitive information, such as passwords, OAuth tokens, and ssh keys.' data-toggle=tooltip data-placement=top href=/docs/concepts/configuration/secret/ target=_blank aria-label=Secrets>Secrets</a>.<br><a href=/docs/tasks/configure-pod-container/pull-image-private-registry/#registry-secret-existing-credentials>Create a Secret based on existing Docker credentials</a> explains how to set this up.</p><p>This is particularly useful if you are using multiple private container
registries, as <code>kubectl create secret docker-registry</code> creates a Secret that
only works with a single private registry.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Pods can only reference image pull secrets in their own namespace,
so this process needs to be done one time per namespace.</div><h4 id=referring-to-an-imagepullsecrets-on-a-pod>Referring to an imagePullSecrets on a Pod</h4><p>Now, you can create pods which reference that secret by adding an <code>imagePullSecrets</code>
section to a Pod definition. Each item in the <code>imagePullSecrets</code> array can only
reference a Secret in the same namespace.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt; pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>apiVersion: v1
</span></span></span><span style=display:flex><span><span style=color:#b44>kind: Pod
</span></span></span><span style=display:flex><span><span style=color:#b44>metadata:
</span></span></span><span style=display:flex><span><span style=color:#b44>  name: foo
</span></span></span><span style=display:flex><span><span style=color:#b44>  namespace: awesomeapps
</span></span></span><span style=display:flex><span><span style=color:#b44>spec:
</span></span></span><span style=display:flex><span><span style=color:#b44>  containers:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: foo
</span></span></span><span style=display:flex><span><span style=color:#b44>      image: janedoe/awesomeapp:v1
</span></span></span><span style=display:flex><span><span style=color:#b44>  imagePullSecrets:
</span></span></span><span style=display:flex><span><span style=color:#b44>    - name: myregistrykey
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cat <span style=color:#b44>&lt;&lt;EOF &gt;&gt; ./kustomization.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>resources:
</span></span></span><span style=display:flex><span><span style=color:#b44>- pod.yaml
</span></span></span><span style=display:flex><span><span style=color:#b44>EOF</span>
</span></span></code></pre></div><p>This needs to be done for each pod that is using a private registry.</p><p>However, setting of this field can be automated by setting the imagePullSecrets
in a <a href=/docs/tasks/configure-pod-container/configure-service-account/>ServiceAccount</a> resource.</p><p>Check <a href=/docs/tasks/configure-pod-container/configure-service-account/#add-imagepullsecrets-to-a-service-account>Add ImagePullSecrets to a Service Account</a> for detailed instructions.</p><p>You can use this in conjunction with a per-node <code>.docker/config.json</code>. The credentials
will be merged.</p><h2 id=use-cases>Use cases</h2><p>There are a number of solutions for configuring private registries. Here are some
common use cases and suggested solutions.</p><ol><li>Cluster running only non-proprietary (e.g. open-source) images. No need to hide images.<ul><li>Use public images from a public registry<ul><li>No configuration required.</li><li>Some cloud providers automatically cache or mirror public images, which improves availability and reduces the time to pull images.</li></ul></li></ul></li><li>Cluster running some proprietary images which should be hidden to those outside the company, but
visible to all cluster users.<ul><li>Use a hosted private registry<ul><li>Manual configuration may be required on the nodes that need to access to private registry</li></ul></li><li>Or, run an internal private registry behind your firewall with open read access.<ul><li>No Kubernetes configuration is required.</li></ul></li><li>Use a hosted container image registry service that controls image access<ul><li>It will work better with cluster autoscaling than manual node configuration.</li></ul></li><li>Or, on a cluster where changing the node configuration is inconvenient, use <code>imagePullSecrets</code>.</li></ul></li><li>Cluster with proprietary images, a few of which require stricter access control.<ul><li>Ensure <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>AlwaysPullImages admission controller</a> is active. Otherwise, all Pods potentially have access to all images.</li><li>Move sensitive data into a "Secret" resource, instead of packaging it in an image.</li></ul></li><li>A multi-tenant cluster where each tenant needs own private registry.<ul><li>Ensure <a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages>AlwaysPullImages admission controller</a> is active. Otherwise, all Pods of all tenants potentially have access to all images.</li><li>Run a private registry with authorization required.</li><li>Generate registry credential for each tenant, put into secret, and populate secret to each tenant namespace.</li><li>The tenant adds that secret to imagePullSecrets of each namespace.</li></ul></li></ol><p>If you need access to multiple registries, you can create one secret for each registry.</p><h2 id=what-s-next>What's next</h2><ul><li>Read the <a href=https://github.com/opencontainers/image-spec/blob/master/manifest.md>OCI Image Manifest Specification</a>.</li><li>Learn about <a href=/docs/concepts/architecture/garbage-collection/#container-image-garbage-collection>container image garbage collection</a>.</li><li>Learn more about <a href=/docs/tasks/configure-pod-container/pull-image-private-registry>pulling an Image from a Private Registry</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-643212488f778acf04bebed65ba34441>2 - Container Environment</h1><p>This page describes the resources available to Containers in the Container environment.</p><h2 id=container-environment>Container environment</h2><p>The Kubernetes Container environment provides several important resources to Containers:</p><ul><li>A filesystem, which is a combination of an <a href=/docs/concepts/containers/images/>image</a> and one or more <a href=/docs/concepts/storage/volumes/>volumes</a>.</li><li>Information about the Container itself.</li><li>Information about other objects in the cluster.</li></ul><h3 id=container-information>Container information</h3><p>The <em>hostname</em> of a Container is the name of the Pod in which the Container is running.
It is available through the <code>hostname</code> command or the
<a href=https://man7.org/linux/man-pages/man2/gethostname.2.html><code>gethostname</code></a>
function call in libc.</p><p>The Pod name and namespace are available as environment variables through the
<a href=/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/>downward API</a>.</p><p>User defined environment variables from the Pod definition are also available to the Container,
as are any environment variables specified statically in the container image.</p><h3 id=cluster-information>Cluster information</h3><p>A list of all services that were running when a Container was created is available to that Container as environment variables.
This list is limited to services within the same namespace as the new Container's Pod and Kubernetes control plane services.</p><p>For a service named <em>foo</em> that maps to a Container named <em>bar</em>,
the following variables are defined:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_HOST</span><span style=color:#666>=</span>&lt;the host the service is running on&gt;
</span></span><span style=display:flex><span><span style=color:#b8860b>FOO_SERVICE_PORT</span><span style=color:#666>=</span>&lt;the port the service is running on&gt;
</span></span></code></pre></div><p>Services have dedicated IP addresses and are available to the Container via DNS,
if <a href=https://releases.k8s.io/v1.25.0/cluster/addons/dns/>DNS addon</a> is enabled. </p><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=/docs/concepts/containers/container-lifecycle-hooks/>Container lifecycle hooks</a>.</li><li>Get hands-on experience
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>attaching handlers to Container lifecycle events</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a858027489648786a3b16264e451272b>3 - Runtime Class</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [stable]</code></div><p>This page describes the RuntimeClass resource and runtime selection mechanism.</p><p>RuntimeClass is a feature for selecting the container runtime configuration. The container runtime
configuration is used to run a Pod's containers.</p><h2 id=motivation>Motivation</h2><p>You can set a different RuntimeClass between different Pods to provide a balance of
performance versus security. For example, if part of your workload deserves a high
level of information security assurance, you might choose to schedule those Pods so
that they run in a container runtime that uses hardware virtualization. You'd then
benefit from the extra isolation of the alternative runtime, at the expense of some
additional overhead.</p><p>You can also use RuntimeClass to run different Pods with the same container runtime
but with different settings.</p><h2 id=setup>Setup</h2><ol><li>Configure the CRI implementation on nodes (runtime dependent)</li><li>Create the corresponding RuntimeClass resources</li></ol><h3 id=1-configure-the-cri-implementation-on-nodes>1. Configure the CRI implementation on nodes</h3><p>The configurations available through RuntimeClass are Container Runtime Interface (CRI)
implementation dependent. See the corresponding documentation (<a href=#cri-configuration>below</a>) for your
CRI implementation for how to configure.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> RuntimeClass assumes a homogeneous node configuration across the cluster by default (which means
that all nodes are configured the same way with respect to container runtimes). To support
heterogeneous node configurations, see <a href=#scheduling>Scheduling</a> below.</div><p>The configurations have a corresponding <code>handler</code> name, referenced by the RuntimeClass. The
handler must be a valid <a href=/docs/concepts/overview/working-with-objects/names/#dns-label-names>DNS label name</a>.</p><h3 id=2-create-the-corresponding-runtimeclass-resources>2. Create the corresponding RuntimeClass resources</h3><p>The configurations setup in step 1 should each have an associated <code>handler</code> name, which identifies
the configuration. For each handler, create a corresponding RuntimeClass object.</p><p>The RuntimeClass resource currently only has 2 significant fields: the RuntimeClass name
(<code>metadata.name</code>) and the handler (<code>handler</code>). The object definition looks like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#080;font-style:italic># RuntimeClass is defined in the node.k8s.io API group</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># The name the RuntimeClass will be referenced by.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># RuntimeClass is a non-namespaced resource.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myclass <span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># The name of the corresponding CRI configuration</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>myconfiguration <span style=color:#bbb>
</span></span></span></code></pre></div><p>The name of a RuntimeClass object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> It is recommended that RuntimeClass write operations (create/update/patch/delete) be
restricted to the cluster administrator. This is typically the default. See
<a href=/docs/reference/access-authn-authz/authorization/>Authorization Overview</a> for more details.</div><h2 id=usage>Usage</h2><p>Once RuntimeClasses are configured for the cluster, you can specify a
<code>runtimeClassName</code> in the Pod spec to use it. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>myclass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># ...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>This will instruct the kubelet to use the named RuntimeClass to run this pod. If the named
RuntimeClass does not exist, or the CRI cannot run the corresponding handler, the pod will enter the
<code>Failed</code> terminal <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase>phase</a>. Look for a
corresponding <a href=/docs/tasks/debug/debug-application/debug-running-pod/>event</a> for an
error message.</p><p>If no <code>runtimeClassName</code> is specified, the default RuntimeHandler will be used, which is equivalent
to the behavior when the RuntimeClass feature is disabled.</p><h3 id=cri-configuration>CRI Configuration</h3><p>For more details on setting up CRI runtimes, see <a href=/docs/setup/production-environment/container-runtimes/>CRI installation</a>.</p><h4 id=hahahugoshortcode-s3-hbhb><a class=glossary-tooltip title='A container runtime with an emphasis on simplicity, robustness and portability' data-toggle=tooltip data-placement=top href=https://containerd.io/docs/ target=_blank aria-label=containerd>containerd</a></h4><p>Runtime handlers are configured through containerd's configuration at
<code>/etc/containerd/config.toml</code>. Valid handlers are configured under the runtimes section:</p><pre tabindex=0><code>[plugins.&#34;io.containerd.grpc.v1.cri&#34;.containerd.runtimes.${HANDLER_NAME}]
</code></pre><p>See containerd's <a href=https://github.com/containerd/containerd/blob/main/docs/cri/config.md>config documentation</a>
for more details:</p><h4 id=hahahugoshortcode-s4-hbhb><a class=glossary-tooltip title='A lightweight container runtime specifically for Kubernetes' data-toggle=tooltip data-placement=top href=https://cri-o.io/#what-is-cri-o target=_blank aria-label=CRI-O>CRI-O</a></h4><p>Runtime handlers are configured through CRI-O's configuration at <code>/etc/crio/crio.conf</code>. Valid
handlers are configured under the
<a href=https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md#crioruntime-table>crio.runtime table</a>:</p><pre tabindex=0><code>[crio.runtime.runtimes.${HANDLER_NAME}]
  runtime_path = &#34;${PATH_TO_BINARY}&#34;
</code></pre><p>See CRI-O's <a href=https://github.com/cri-o/cri-o/blob/master/docs/crio.conf.5.md>config documentation</a> for more details.</p><h2 id=scheduling>Scheduling</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.16 [beta]</code></div><p>By specifying the <code>scheduling</code> field for a RuntimeClass, you can set constraints to
ensure that Pods running with this RuntimeClass are scheduled to nodes that support it.
If <code>scheduling</code> is not set, this RuntimeClass is assumed to be supported by all nodes.</p><p>To ensure pods land on nodes supporting a specific RuntimeClass, that set of nodes should have a
common label which is then selected by the <code>runtimeclass.scheduling.nodeSelector</code> field. The
RuntimeClass's nodeSelector is merged with the pod's nodeSelector in admission, effectively taking
the intersection of the set of nodes selected by each. If there is a conflict, the pod will be
rejected.</p><p>If the supported nodes are tainted to prevent other RuntimeClass pods from running on the node, you
can add <code>tolerations</code> to the RuntimeClass. As with the <code>nodeSelector</code>, the tolerations are merged
with the pod's tolerations in admission, effectively taking the union of the set of nodes tolerated
by each.</p><p>To learn more about configuring the node selector and tolerations, see
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/>Assigning Pods to Nodes</a>.</p><h3 id=pod-overhead>Pod Overhead</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>You can specify <em>overhead</em> resources that are associated with running a Pod. Declaring overhead allows
the cluster (including the scheduler) to account for it when making decisions about Pods and resources.</p><p>Pod overhead is defined in RuntimeClass through the <code>overhead</code> field. Through the use of this field,
you can specify the overhead of running pods utilizing this RuntimeClass and ensure these overheads
are accounted for in Kubernetes.</p><h2 id=what-s-next>What's next</h2><ul><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md>RuntimeClass Design</a></li><li><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/585-runtime-class/README.md#runtimeclass-scheduling>RuntimeClass Scheduling Design</a></li><li>Read about the <a href=/docs/concepts/scheduling-eviction/pod-overhead/>Pod Overhead</a> concept</li><li><a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>PodOverhead Feature Design</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e6941d969d81540208a3e78bc56f43bc>4 - Container Lifecycle Hooks</h1><p>This page describes how kubelet managed Containers can use the Container lifecycle hook framework
to run code triggered by events during their management lifecycle.</p><h2 id=overview>Overview</h2><p>Analogous to many programming language frameworks that have component lifecycle hooks, such as Angular,
Kubernetes provides Containers with lifecycle hooks.
The hooks enable Containers to be aware of events in their management lifecycle
and run code implemented in a handler when the corresponding lifecycle hook is executed.</p><h2 id=container-hooks>Container hooks</h2><p>There are two hooks that are exposed to Containers:</p><p><code>PostStart</code></p><p>This hook is executed immediately after a container is created.
However, there is no guarantee that the hook will execute before the container ENTRYPOINT.
No parameters are passed to the handler.</p><p><code>PreStop</code></p><p>This hook is called immediately before a container is terminated due to an API request or management
event such as a liveness/startup probe failure, preemption, resource contention and others. A call
to the <code>PreStop</code> hook fails if the container is already in a terminated or completed state and the
hook must complete before the TERM signal to stop the container can be sent. The Pod's termination
grace period countdown begins before the <code>PreStop</code> hook is executed, so regardless of the outcome of
the handler, the container will eventually terminate within the Pod's termination grace period. No
parameters are passed to the handler.</p><p>A more detailed description of the termination behavior can be found in
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>Termination of Pods</a>.</p><h3 id=hook-handler-implementations>Hook handler implementations</h3><p>Containers can access a hook by implementing and registering a handler for that hook.
There are two types of hook handlers that can be implemented for Containers:</p><ul><li>Exec - Executes a specific command, such as <code>pre-stop.sh</code>, inside the cgroups and namespaces of the Container.
Resources consumed by the command are counted against the Container.</li><li>HTTP - Executes an HTTP request against a specific endpoint on the Container.</li></ul><h3 id=hook-handler-execution>Hook handler execution</h3><p>When a Container lifecycle management hook is called,
the Kubernetes management system executes the handler according to the hook action,
<code>httpGet</code> and <code>tcpSocket</code> are executed by the kubelet process, and <code>exec</code> is executed in the container.</p><p>Hook handler calls are synchronous within the context of the Pod containing the Container.
This means that for a <code>PostStart</code> hook,
the Container ENTRYPOINT and hook fire asynchronously.
However, if the hook takes too long to run or hangs,
the Container cannot reach a <code>running</code> state.</p><p><code>PreStop</code> hooks are not executed asynchronously from the signal to stop the Container; the hook must
complete its execution before the TERM signal can be sent. If a <code>PreStop</code> hook hangs during
execution, the Pod's phase will be <code>Terminating</code> and remain there until the Pod is killed after its
<code>terminationGracePeriodSeconds</code> expires. This grace period applies to the total time it takes for
both the <code>PreStop</code> hook to execute and for the Container to stop normally. If, for example,
<code>terminationGracePeriodSeconds</code> is 60, and the hook takes 55 seconds to complete, and the Container
takes 10 seconds to stop normally after receiving the signal, then the Container will be killed
before it can stop normally, since <code>terminationGracePeriodSeconds</code> is less than the total time
(55+10) it takes for these two things to happen.</p><p>If either a <code>PostStart</code> or <code>PreStop</code> hook fails,
it kills the Container.</p><p>Users should make their hook handlers as lightweight as possible.
There are cases, however, when long running commands make sense,
such as when saving state prior to stopping a Container.</p><h3 id=hook-delivery-guarantees>Hook delivery guarantees</h3><p>Hook delivery is intended to be <em>at least once</em>,
which means that a hook may be called multiple times for any given event,
such as for <code>PostStart</code> or <code>PreStop</code>.
It is up to the hook implementation to handle this correctly.</p><p>Generally, only single deliveries are made.
If, for example, an HTTP hook receiver is down and is unable to take traffic,
there is no attempt to resend.
In some rare cases, however, double delivery may occur.
For instance, if a kubelet restarts in the middle of sending a hook,
the hook might be resent after the kubelet comes back up.</p><h3 id=debugging-hook-handlers>Debugging Hook handlers</h3><p>The logs for a Hook handler are not exposed in Pod events.
If a handler fails for some reason, it broadcasts an event.
For <code>PostStart</code>, this is the <code>FailedPostStartHook</code> event,
and for <code>PreStop</code>, this is the <code>FailedPreStopHook</code> event.
To generate a failed <code>FailedPostStartHook</code> event yourself, modify the <a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/lifecycle-events.yaml>lifecycle-events.yaml</a> file to change the postStart command to "badcommand" and apply it.
Here is some example output of the resulting events you see from running <code>kubectl describe pod lifecycle-demo</code>:</p><pre tabindex=0><code>Events:
  Type     Reason               Age              From               Message
  ----     ------               ----             ----               -------
  Normal   Scheduled            7s               default-scheduler  Successfully assigned default/lifecycle-demo to ip-XXX-XXX-XX-XX.us-east-2...
  Normal   Pulled               6s               kubelet            Successfully pulled image &#34;nginx&#34; in 229.604315ms
  Normal   Pulling              4s (x2 over 6s)  kubelet            Pulling image &#34;nginx&#34;
  Normal   Created              4s (x2 over 5s)  kubelet            Created container lifecycle-demo-container
  Normal   Started              4s (x2 over 5s)  kubelet            Started container lifecycle-demo-container
  Warning  FailedPostStartHook  4s (x2 over 5s)  kubelet            Exec lifecycle hook ([badcommand]) for Container &#34;lifecycle-demo-container&#34; in Pod &#34;lifecycle-demo_default(30229739-9651-4e5a-9a32-a8f1688862db)&#34; failed - error: command &#39;badcommand&#39; exited with 126: , message: &#34;OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \&#34;badcommand\&#34;: executable file not found in $PATH: unknown\r\n&#34;
  Normal   Killing              4s (x2 over 5s)  kubelet            FailedPostStartHook
  Normal   Pulled               4s               kubelet            Successfully pulled image &#34;nginx&#34; in 215.66395ms
  Warning  BackOff              2s (x2 over 3s)  kubelet            Back-off restarting failed container
</code></pre><h2 id=what-s-next>What's next</h2><ul><li>Learn more about the <a href=/docs/concepts/containers/container-environment/>Container environment</a>.</li><li>Get hands-on experience
<a href=/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/>attaching handlers to Container lifecycle events</a>.</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>