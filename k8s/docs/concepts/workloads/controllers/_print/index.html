<!doctype html><html lang=en class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/workloads/controllers/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/workloads/controllers/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/workloads/controllers/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/workloads/controllers/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/workloads/controllers/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/workloads/controllers/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/workloads/controllers/><link rel=alternate hreflang=uk href=https://kubernetes.io/uk/docs/concepts/workloads/controllers/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/concepts/workloads/controllers/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Workload Resources | Kubernetes</title><meta property="og:title" content="Workload Resources"><meta property="og:description" content="Production-Grade Container Orchestration"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/concepts/workloads/controllers/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Workload Resources"><meta itemprop=description content="Production-Grade Container Orchestration"><meta name=twitter:card content="summary"><meta name=twitter:title content="Workload Resources"><meta name=twitter:description content="Production-Grade Container Orchestration"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content><meta property="og:description" content><meta name=twitter:description content><meta property="og:url" content="https://kubernetes.io/docs/concepts/workloads/controllers/"><meta property="og:title" content="Workload Resources"><meta name=twitter:title content="Workload Resources"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/concepts/workloads/controllers/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/concepts/workloads/controllers/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/concepts/workloads/controllers/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/concepts/workloads/controllers/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/concepts/workloads/controllers/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/docs/concepts/workloads/controllers/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/workloads/controllers/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/workloads/controllers/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/workloads/controllers/>Français (French)</a>
<a class=dropdown-item href=/es/docs/concepts/workloads/controllers/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/workloads/controllers/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/workloads/controllers/>Bahasa Indonesia</a>
<a class=dropdown-item href=/uk/docs/concepts/workloads/controllers/>Українська (Ukrainian)</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/concepts/workloads/controllers/>Return to the regular view of this page</a>.</p></div><h1 class=title>Workload Resources</h1><ul><li>1: <a href=#pg-a2dc0393e0c4079e1c504b6429844e86>Deployments</a></li><li>2: <a href=#pg-d459b930218774655fa7fd1620625539>ReplicaSet</a></li><li>3: <a href=#pg-6d72299952c37ca8cc61b416e5bdbcd4>StatefulSets</a></li><li>4: <a href=#pg-41600eb8b6631c88848156f381e9d588>DaemonSet</a></li><li>5: <a href=#pg-cc7cc3c4907039d9f863162e20bfbbef>Jobs</a></li><li>6: <a href=#pg-4de50a37ebb6f2340484192126cb7a04>Automatic Clean-up for Finished Jobs</a></li><li>7: <a href=#pg-2e4cec01c525b45eccd6010e21cc76d9>CronJob</a></li><li>8: <a href=#pg-27f1331d515d95f76aa1156088b4ad91>ReplicationController</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-a2dc0393e0c4079e1c504b6429844e86>1 - Deployments</h1><p>A <em>Deployment</em> provides declarative updates for <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> and
<a class=glossary-tooltip title='ReplicaSet ensures that a specified number of Pod replicas are running at one time' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/replicaset/ target=_blank aria-label=ReplicaSets>ReplicaSets</a>.</p><p>You describe a <em>desired state</em> in a Deployment, and the Deployment <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a> changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Do not manage ReplicaSets owned by a Deployment. Consider opening an issue in the main Kubernetes repository if your use case is not covered below.</div><h2 id=use-case>Use Case</h2><p>The following are typical use cases for Deployments:</p><ul><li><a href=#creating-a-deployment>Create a Deployment to rollout a ReplicaSet</a>. The ReplicaSet creates Pods in the background. Check the status of the rollout to see if it succeeds or not.</li><li><a href=#updating-a-deployment>Declare the new state of the Pods</a> by updating the PodTemplateSpec of the Deployment. A new ReplicaSet is created and the Deployment manages moving the Pods from the old ReplicaSet to the new one at a controlled rate. Each new ReplicaSet updates the revision of the Deployment.</li><li><a href=#rolling-back-a-deployment>Rollback to an earlier Deployment revision</a> if the current state of the Deployment is not stable. Each rollback updates the revision of the Deployment.</li><li><a href=#scaling-a-deployment>Scale up the Deployment to facilitate more load</a>.</li><li><a href=#pausing-and-resuming-a-deployment>Pause the rollout of a Deployment</a> to apply multiple fixes to its PodTemplateSpec and then resume it to start a new rollout.</li><li><a href=#deployment-status>Use the status of the Deployment</a> as an indicator that a rollout has stuck.</li><li><a href=#clean-up-policy>Clean up older ReplicaSets</a> that you don't need anymore.</li></ul><h2 id=creating-a-deployment>Creating a Deployment</h2><p>The following is an example of a Deployment. It creates a ReplicaSet to bring up three <code>nginx</code> Pods:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/nginx-deployment.yaml download=controllers/nginx-deployment.yaml><code>controllers/nginx-deployment.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-nginx-deployment-yaml")' title="Copy controllers/nginx-deployment.yaml to clipboard"></img></div><div class=includecode id=controllers-nginx-deployment-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.14.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>In this example:</p><ul><li><p>A Deployment named <code>nginx-deployment</code> is created, indicated by the <code>.metadata.name</code> field.</p></li><li><p>The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the <code>.spec.replicas</code> field.</p></li><li><p>The <code>.spec.selector</code> field defines how the created ReplicaSet finds which Pods to manage.
In this case, you select a label that is defined in the Pod template (<code>app: nginx</code>).
However, more sophisticated selection rules are possible,
as long as the Pod template itself satisfies the rule.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>.spec.selector.matchLabels</code> field is a map of {key,value} pairs.
A single {key,value} in the <code>matchLabels</code> map is equivalent to an element of <code>matchExpressions</code>,
whose <code>key</code> field is "key", the <code>operator</code> is "In", and the <code>values</code> array contains only "value".
All of the requirements, from both <code>matchLabels</code> and <code>matchExpressions</code>, must be satisfied in order to match.</div></li><li><p>The <code>template</code> field contains the following sub-fields:</p><ul><li>The Pods are labeled <code>app: nginx</code>using the <code>.metadata.labels</code> field.</li><li>The Pod template's specification, or <code>.template.spec</code> field, indicates that
the Pods run one container, <code>nginx</code>, which runs the <code>nginx</code>
<a href=https://hub.docker.com/>Docker Hub</a> image at version 1.14.2.</li><li>Create one container and name it <code>nginx</code> using the <code>.spec.template.spec.containers[0].name</code> field.</li></ul></li></ul><p>Before you begin, make sure your Kubernetes cluster is up and running.
Follow the steps given below to create the above Deployment:</p><ol><li><p>Create the Deployment by running the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml
</span></span></code></pre></div></li><li><p>Run <code>kubectl get deployments</code> to check if the Deployment was created.</p><p>If the Deployment is still being created, the output is similar to the following:</p><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   0/3     0            0           1s
</code></pre><p>When you inspect the Deployments in your cluster, the following fields are displayed:</p><ul><li><code>NAME</code> lists the names of the Deployments in the namespace.</li><li><code>READY</code> displays how many replicas of the application are available to your users. It follows the pattern ready/desired.</li><li><code>UP-TO-DATE</code> displays the number of replicas that have been updated to achieve the desired state.</li><li><code>AVAILABLE</code> displays how many replicas of the application are available to your users.</li><li><code>AGE</code> displays the amount of time that the application has been running.</li></ul><p>Notice how the number of desired replicas is 3 according to <code>.spec.replicas</code> field.</p></li><li><p>To see the Deployment rollout status, run <code>kubectl rollout status deployment/nginx-deployment</code>.</p><p>The output is similar to:</p><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
deployment &#34;nginx-deployment&#34; successfully rolled out
</code></pre></li><li><p>Run the <code>kubectl get deployments</code> again a few seconds later.
The output is similar to this:</p><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           18s
</code></pre><p>Notice that the Deployment has created all three replicas, and all replicas are up-to-date (they contain the latest Pod template) and available.</p></li><li><p>To see the ReplicaSet (<code>rs</code>) created by the Deployment, run <code>kubectl get rs</code>. The output is similar to this:</p><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75675f5897   3         3         3       18s
</code></pre><p>ReplicaSet output shows the following fields:</p><ul><li><code>NAME</code> lists the names of the ReplicaSets in the namespace.</li><li><code>DESIRED</code> displays the desired number of <em>replicas</em> of the application, which you define when you create the Deployment. This is the <em>desired state</em>.</li><li><code>CURRENT</code> displays how many replicas are currently running.</li><li><code>READY</code> displays how many replicas of the application are available to your users.</li><li><code>AGE</code> displays the amount of time that the application has been running.</li></ul><p>Notice that the name of the ReplicaSet is always formatted as <code>[DEPLOYMENT-NAME]-[HASH]</code>.
The <code>HASH</code> string is the same as the <code>pod-template-hash</code> label on the ReplicaSet.</p></li><li><p>To see the labels automatically generated for each Pod, run <code>kubectl get pods --show-labels</code>.
The output is similar to:</p><pre tabindex=0><code>NAME                                READY     STATUS    RESTARTS   AGE       LABELS
nginx-deployment-75675f5897-7ci7o   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-kzszj   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
nginx-deployment-75675f5897-qqcnn   1/1       Running   0          18s       app=nginx,pod-template-hash=3123191453
</code></pre><p>The created ReplicaSet ensures that there are three <code>nginx</code> Pods.</p></li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>You must specify an appropriate selector and Pod template labels in a Deployment
(in this case, <code>app: nginx</code>).</p><p>Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.</p></div><h3 id=pod-template-hash-label>Pod-template-hash label</h3><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Do not change this label.</div><p>The <code>pod-template-hash</code> label is added by the Deployment controller to every ReplicaSet that a Deployment creates or adopts.</p><p>This label ensures that child ReplicaSets of a Deployment do not overlap. It is generated by hashing the <code>PodTemplate</code> of the ReplicaSet and using the resulting hash as the label value that is added to the ReplicaSet selector, Pod template labels,
and in any existing Pods that the ReplicaSet might have.</p><h2 id=updating-a-deployment>Updating a Deployment</h2><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A Deployment's rollout is triggered if and only if the Deployment's Pod template (that is, <code>.spec.template</code>)
is changed, for example if the labels or container images of the template are updated. Other updates, such as scaling the Deployment, do not trigger a rollout.</div><p>Follow the steps given below to update your Deployment:</p><ol><li><p>Let's update the nginx Pods to use the <code>nginx:1.16.1</code> image instead of the <code>nginx:1.14.2</code> image.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment.v1.apps/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.16.1
</span></span></code></pre></div><p>or use the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.16.1
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre><p>Alternatively, you can <code>edit</code> the Deployment and change <code>.spec.template.spec.containers[0].image</code> from <code>nginx:1.14.2</code> to <code>nginx:1.16.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl edit deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to:</p><pre tabindex=0><code>deployment.apps/nginx-deployment edited
</code></pre></li><li><p>To see the rollout status, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
</code></pre><p>or</p><pre tabindex=0><code>deployment &#34;nginx-deployment&#34; successfully rolled out
</code></pre></li></ol><p>Get more details on your updated Deployment:</p><ul><li><p>After the rollout succeeds, you can view the Deployment by running <code>kubectl get deployments</code>.
The output is similar to this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-ini data-lang=ini><span style=display:flex><span><span style=color:#b44>NAME               READY   UP-TO-DATE   AVAILABLE   AGE</span>
</span></span><span style=display:flex><span><span style=color:#b44>nginx-deployment   3/3     3            3           36s</span>
</span></span></code></pre></div></li><li><p>Run <code>kubectl get rs</code> to see that the Deployment updated the Pods by creating a new ReplicaSet and scaling it
up to 3 replicas, as well as scaling down the old ReplicaSet to 0 replicas.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       6s
nginx-deployment-2035384211   0         0         0       36s
</code></pre></li><li><p>Running <code>get pods</code> should now show only the new Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                                READY     STATUS    RESTARTS   AGE
nginx-deployment-1564180365-khku8   1/1       Running   0          14s
nginx-deployment-1564180365-nacti   1/1       Running   0          14s
nginx-deployment-1564180365-z9gth   1/1       Running   0          14s
</code></pre><p>Next time you want to update these Pods, you only need to update the Deployment's Pod template again.</p><p>Deployment ensures that only a certain number of Pods are down while they are being updated. By default,
it ensures that at least 75% of the desired number of Pods are up (25% max unavailable).</p><p>Deployment also ensures that only a certain number of Pods are created above the desired number of Pods.
By default, it ensures that at most 125% of the desired number of Pods are up (25% max surge).</p><p>For example, if you look at the above Deployment closely, you will see that it first creates a new Pod,
then deletes an old Pod, and creates another new one. It does not kill old Pods until a sufficient number of
new Pods have come up, and does not create new Pods until a sufficient number of old Pods have been killed.
It makes sure that at least 3 Pods are available and that at max 4 Pods in total are available. In case of
a Deployment with 4 replicas, the number of Pods would be between 3 and 5.</p></li><li><p>Get details of your Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployments
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Thu, 30 Nov 2017 10:56:25 +0000
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=2
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
   Containers:
    nginx:
      Image:        nginx:1.16.1
      Port:         80/TCP
      Environment:  &lt;none&gt;
      Mounts:       &lt;none&gt;
    Volumes:        &lt;none&gt;
  Conditions:
    Type           Status  Reason
    ----           ------  ------
    Available      True    MinimumReplicasAvailable
    Progressing    True    NewReplicaSetAvailable
  OldReplicaSets:  &lt;none&gt;
  NewReplicaSet:   nginx-deployment-1564180365 (3/3 replicas created)
  Events:
    Type    Reason             Age   From                   Message
    ----    ------             ----  ----                   -------
    Normal  ScalingReplicaSet  2m    deployment-controller  Scaled up replica set nginx-deployment-2035384211 to 3
    Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 1
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 2
    Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 2
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 1
    Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1564180365 to 3
    Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-2035384211 to 0
</code></pre><p>Here you see that when you first created the Deployment, it created a ReplicaSet (nginx-deployment-2035384211)
and scaled it up to 3 replicas directly. When you updated the Deployment, it created a new ReplicaSet
(nginx-deployment-1564180365) and scaled it up to 1 and waited for it to come up. Then it scaled down the old ReplicaSet
to 2 and scaled up the new ReplicaSet to 2 so that at least 3 Pods were available and at most 4 Pods were created at all times.
It then continued scaling up and down the new and the old ReplicaSet, with the same rolling update strategy.
Finally, you'll have 3 available replicas in the new ReplicaSet, and the old ReplicaSet is scaled down to 0.</p></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes doesn't count terminating Pods when calculating the number of <code>availableReplicas</code>, which must be between
<code>replicas - maxUnavailable</code> and <code>replicas + maxSurge</code>. As a result, you might notice that there are more Pods than
expected during a rollout, and that the total resources consumed by the Deployment is more than <code>replicas + maxSurge</code>
until the <code>terminationGracePeriodSeconds</code> of the terminating Pods expires.</div><h3 id=rollover-aka-multiple-updates-in-flight>Rollover (aka multiple updates in-flight)</h3><p>Each time a new Deployment is observed by the Deployment controller, a ReplicaSet is created to bring up
the desired Pods. If the Deployment is updated, the existing ReplicaSet that controls Pods whose labels
match <code>.spec.selector</code> but whose template does not match <code>.spec.template</code> are scaled down. Eventually, the new
ReplicaSet is scaled to <code>.spec.replicas</code> and all old ReplicaSets is scaled to 0.</p><p>If you update a Deployment while an existing rollout is in progress, the Deployment creates a new ReplicaSet
as per the update and start scaling that up, and rolls over the ReplicaSet that it was scaling up previously
-- it will add it to its list of old ReplicaSets and start scaling it down.</p><p>For example, suppose you create a Deployment to create 5 replicas of <code>nginx:1.14.2</code>,
but then update the Deployment to create 5 replicas of <code>nginx:1.16.1</code>, when only 3
replicas of <code>nginx:1.14.2</code> had been created. In that case, the Deployment immediately starts
killing the 3 <code>nginx:1.14.2</code> Pods that it had created, and starts creating
<code>nginx:1.16.1</code> Pods. It does not wait for the 5 replicas of <code>nginx:1.14.2</code> to be created
before changing course.</p><h3 id=label-selector-updates>Label selector updates</h3><p>It is generally discouraged to make label selector updates and it is suggested to plan your selectors up front.
In any case, if you need to perform a label selector update, exercise great caution and make sure you have grasped
all of the implications.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In API version <code>apps/v1</code>, a Deployment's label selector is immutable after it gets created.</div><ul><li>Selector additions require the Pod template labels in the Deployment spec to be updated with the new label too,
otherwise a validation error is returned. This change is a non-overlapping one, meaning that the new selector does
not select ReplicaSets and Pods created with the old selector, resulting in orphaning all old ReplicaSets and
creating a new ReplicaSet.</li><li>Selector updates changes the existing value in a selector key -- result in the same behavior as additions.</li><li>Selector removals removes an existing key from the Deployment selector -- do not require any changes in the
Pod template labels. Existing ReplicaSets are not orphaned, and a new ReplicaSet is not created, but note that the
removed label still exists in any existing Pods and ReplicaSets.</li></ul><h2 id=rolling-back-a-deployment>Rolling Back a Deployment</h2><p>Sometimes, you may want to rollback a Deployment; for example, when the Deployment is not stable, such as crash looping.
By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want
(you can change that by modifying revision history limit).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A Deployment's revision is created when a Deployment's rollout is triggered. This means that the
new revision is created if and only if the Deployment's Pod template (<code>.spec.template</code>) is changed,
for example if you update the labels or container images of the template. Other updates, such as scaling the Deployment,
do not create a Deployment revision, so that you can facilitate simultaneous manual- or auto-scaling.
This means that when you roll back to an earlier revision, only the Deployment's Pod template part is
rolled back.</div><ul><li><p>Suppose that you made a typo while updating the Deployment, by putting the image name as <code>nginx:1.161</code> instead of <code>nginx:1.16.1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.161 
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>The rollout gets stuck. You can verify it by checking the rollout status:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Waiting for rollout to finish: 1 out of 3 new replicas have been updated...
</code></pre></li><li><p>Press Ctrl-C to stop the above rollout status watch. For more information on stuck rollouts,
<a href=#deployment-status>read more here</a>.</p></li><li><p>You see that the number of old replicas (<code>nginx-deployment-1564180365</code> and <code>nginx-deployment-2035384211</code>) is 2, and new replicas (nginx-deployment-3066724191) is 1.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-1564180365   3         3         3       25s
nginx-deployment-2035384211   0         0         0       36s
nginx-deployment-3066724191   1         1         0       6s
</code></pre></li><li><p>Looking at the Pods created, you see that 1 Pod created by new ReplicaSet is stuck in an image pull loop.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                                READY     STATUS             RESTARTS   AGE
nginx-deployment-1564180365-70iae   1/1       Running            0          25s
nginx-deployment-1564180365-jbqqo   1/1       Running            0          25s
nginx-deployment-1564180365-hysrc   1/1       Running            0          25s
nginx-deployment-3066724191-08mng   0/1       ImagePullBackOff   0          6s
</code></pre><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The Deployment controller stops the bad rollout automatically, and stops scaling up the new ReplicaSet. This depends on the rollingUpdate parameters (<code>maxUnavailable</code> specifically) that you have specified. Kubernetes by default sets the value to 25%.</div></li><li><p>Get the description of the Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Name:           nginx-deployment
Namespace:      default
CreationTimestamp:  Tue, 15 Mar 2016 14:48:04 -0700
Labels:         app=nginx
Selector:       app=nginx
Replicas:       3 desired | 1 updated | 4 total | 3 available | 1 unavailable
StrategyType:       RollingUpdate
MinReadySeconds:    0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.161
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    ReplicaSetUpdated
OldReplicaSets:     nginx-deployment-1564180365 (3/3 replicas created)
NewReplicaSet:      nginx-deployment-3066724191 (1/1 replicas created)
Events:
  FirstSeen LastSeen    Count   From                    SubObjectPath   Type        Reason              Message
  --------- --------    -----   ----                    -------------   --------    ------              -------
  1m        1m          1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-2035384211 to 3
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 1
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 2
  22s       22s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 2
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 1
  21s       21s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-1564180365 to 3
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled down replica set nginx-deployment-2035384211 to 0
  13s       13s         1       {deployment-controller }                Normal      ScalingReplicaSet   Scaled up replica set nginx-deployment-3066724191 to 1
</code></pre><p>To fix this, you need to rollback to a previous revision of Deployment that is stable.</p></li></ul><h3 id=checking-rollout-history-of-a-deployment>Checking Rollout History of a Deployment</h3><p>Follow the steps given below to check the rollout history:</p><ol><li><p>First, check the revisions of this Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployments &#34;nginx-deployment&#34;
REVISION    CHANGE-CAUSE
1           kubectl apply --filename=https://k8s.io/examples/controllers/nginx-deployment.yaml
2           kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
3           kubectl set image deployment/nginx-deployment nginx=nginx:1.161
</code></pre><p><code>CHANGE-CAUSE</code> is copied from the Deployment annotation <code>kubernetes.io/change-cause</code> to its revisions upon creation. You can specify the<code>CHANGE-CAUSE</code> message by:</p><ul><li>Annotating the Deployment with <code>kubectl annotate deployment/nginx-deployment kubernetes.io/change-cause="image updated to 1.16.1"</code></li><li>Manually editing the manifest of the resource.</li></ul></li><li><p>To see the details of each revision, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment/nginx-deployment --revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployments &#34;nginx-deployment&#34; revision 2
  Labels:       app=nginx
          pod-template-hash=1159050644
  Annotations:  kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
  Containers:
   nginx:
    Image:      nginx:1.16.1
    Port:       80/TCP
     QoS Tier:
        cpu:      BestEffort
        memory:   BestEffort
    Environment Variables:      &lt;none&gt;
  No volumes.
</code></pre></li></ol><h3 id=rolling-back-to-a-previous-revision>Rolling Back to a Previous Revision</h3><p>Follow the steps given below to rollback the Deployment from the current version to the previous version, which is version 2.</p><ol><li><p>Now you've decided to undo the current rollout and rollback to the previous revision:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment rolled back
</code></pre><p>Alternatively, you can rollback to a specific revision by specifying it with <code>--to-revision</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout undo deployment/nginx-deployment --to-revision<span style=color:#666>=</span><span style=color:#666>2</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment rolled back
</code></pre><p>For more details about rollout related commands, read <a href=/docs/reference/generated/kubectl/kubectl-commands#rollout><code>kubectl rollout</code></a>.</p><p>The Deployment is now rolled back to a previous stable revision. As you can see, a <code>DeploymentRollback</code> event
for rolling back to revision 2 is generated from Deployment controller.</p></li><li><p>Check if the rollback was successful and the Deployment is running as expected, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment   3/3     3            3           30m
</code></pre></li><li><p>Get the description of the Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Sun, 02 Sep 2018 18:17:55 -0500
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision=4
                        kubernetes.io/change-cause=kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:        nginx:1.16.1
    Port:         80/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   nginx-deployment-c4747d96c (3/3 replicas created)
Events:
  Type    Reason              Age   From                   Message
  ----    ------              ----  ----                   -------
  Normal  ScalingReplicaSet   12m   deployment-controller  Scaled up replica set nginx-deployment-75675f5897 to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 2
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 1
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-c4747d96c to 3
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled down replica set nginx-deployment-75675f5897 to 0
  Normal  ScalingReplicaSet   11m   deployment-controller  Scaled up replica set nginx-deployment-595696685f to 1
  Normal  DeploymentRollback  15s   deployment-controller  Rolled back deployment &#34;nginx-deployment&#34; to revision 2
  Normal  ScalingReplicaSet   15s   deployment-controller  Scaled down replica set nginx-deployment-595696685f to 0
</code></pre></li></ol><h2 id=scaling-a-deployment>Scaling a Deployment</h2><p>You can scale a Deployment by using the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl scale deployment/nginx-deployment --replicas<span style=color:#666>=</span><span style=color:#666>10</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment scaled
</code></pre><p>Assuming <a href=/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/>horizontal Pod autoscaling</a> is enabled
in your cluster, you can set up an autoscaler for your Deployment and choose the minimum and maximum number of
Pods you want to run based on the CPU utilization of your existing Pods.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale deployment/nginx-deployment --min<span style=color:#666>=</span><span style=color:#666>10</span> --max<span style=color:#666>=</span><span style=color:#666>15</span> --cpu-percent<span style=color:#666>=</span><span style=color:#666>80</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment scaled
</code></pre><h3 id=proportional-scaling>Proportional scaling</h3><p>RollingUpdate Deployments support running multiple versions of an application at the same time. When you
or an autoscaler scales a RollingUpdate Deployment that is in the middle of a rollout (either in progress
or paused), the Deployment controller balances the additional replicas in the existing active
ReplicaSets (ReplicaSets with Pods) in order to mitigate risk. This is called <em>proportional scaling</em>.</p><p>For example, you are running a Deployment with 10 replicas, <a href=#max-surge>maxSurge</a>=3, and <a href=#max-unavailable>maxUnavailable</a>=2.</p><ul><li><p>Ensure that the 10 replicas in your Deployment are running.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     10        10        10           10          50s
</code></pre></li><li><p>You update to a new image which happens to be unresolvable from inside the cluster.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:sometag
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>The image update starts a new rollout with ReplicaSet nginx-deployment-1989198191, but it's blocked due to the
<code>maxUnavailable</code> requirement that you mentioned above. Check out the rollout status:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><pre><code>The output is similar to this:
</code></pre><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   5         5         0         9s
nginx-deployment-618515232    8         8         8         1m
</code></pre></li><li><p>Then a new scaling request for the Deployment comes along. The autoscaler increments the Deployment replicas
to 15. The Deployment controller needs to decide where to add these new 5 replicas. If you weren't using
proportional scaling, all 5 of them would be added in the new ReplicaSet. With proportional scaling, you
spread the additional replicas across all ReplicaSets. Bigger proportions go to the ReplicaSets with the
most replicas and lower proportions go to ReplicaSets with less replicas. Any leftovers are added to the
ReplicaSet with the most replicas. ReplicaSets with zero replicas are not scaled up.</p></li></ul><p>In our example above, 3 replicas are added to the old ReplicaSet and 2 replicas are added to the
new ReplicaSet. The rollout process should eventually move all replicas to the new ReplicaSet, assuming
the new replicas become healthy. To confirm this, run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                 DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment     15        18        7            8           7m
</code></pre><p>The rollout status confirms how the replicas were added to each ReplicaSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME                          DESIRED   CURRENT   READY     AGE
nginx-deployment-1989198191   7         7         0         7m
nginx-deployment-618515232    11        11        11        7m
</code></pre><h2 id=pausing-and-resuming-a-deployment>Pausing and Resuming a rollout of a Deployment</h2><p>When you update a Deployment, or plan to, you can pause rollouts
for that Deployment before you trigger one or more updates. When
you're ready to apply those changes, you resume rollouts for the
Deployment. This approach allows you to
apply multiple fixes in between pausing and resuming without triggering unnecessary rollouts.</p><ul><li><p>For example, with a Deployment that was created:</p><p>Get the Deployment details:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get deploy
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx     3         3         3            3           1m
</code></pre><p>Get the rollout status:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         1m
</code></pre></li><li><p>Pause by running the following command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout pause deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment paused
</code></pre></li><li><p>Then update the image of the Deployment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> image deployment/nginx-deployment <span style=color:#b8860b>nginx</span><span style=color:#666>=</span>nginx:1.16.1
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment image updated
</code></pre></li><li><p>Notice that no new rollout started:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout <span style=color:#a2f>history</span> deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployments &#34;nginx&#34;
REVISION  CHANGE-CAUSE
1   &lt;none&gt;
</code></pre></li><li><p>Get the rollout status to verify that the existing ReplicaSet has not changed:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   3         3         3         2m
</code></pre></li><li><p>You can make as many updates as you wish, for example, update the resources that will be used:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl <span style=color:#a2f>set</span> resources deployment/nginx-deployment -c<span style=color:#666>=</span>nginx --limits<span style=color:#666>=</span><span style=color:#b8860b>cpu</span><span style=color:#666>=</span>200m,memory<span style=color:#666>=</span>512Mi
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment resource requirements updated
</code></pre><p>The initial state of the Deployment prior to pausing its rollout will continue its function, but new updates to
the Deployment will not have any effect as long as the Deployment rollout is paused.</p></li><li><p>Eventually, resume the Deployment rollout and observe a new ReplicaSet coming up with all the new updates:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout resume deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment resumed
</code></pre></li><li><p>Watch the status of the rollout until it's done.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs -w
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   2         2         2         2m
nginx-3926361531   2         2         0         6s
nginx-3926361531   2         2         1         18s
nginx-2142116321   1         2         2         2m
nginx-2142116321   1         2         2         2m
nginx-3926361531   3         2         1         18s
nginx-3926361531   3         2         1         18s
nginx-2142116321   1         1         1         2m
nginx-3926361531   3         3         1         18s
nginx-3926361531   3         3         2         19s
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         1         1         2m
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         20s
</code></pre></li><li><p>Get the status of the latest rollout:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>NAME               DESIRED   CURRENT   READY     AGE
nginx-2142116321   0         0         0         2m
nginx-3926361531   3         3         3         28s
</code></pre></li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You cannot rollback a paused Deployment until you resume it.</div><h2 id=deployment-status>Deployment status</h2><p>A Deployment enters various states during its lifecycle. It can be <a href=#progressing-deployment>progressing</a> while
rolling out a new ReplicaSet, it can be <a href=#complete-deployment>complete</a>, or it can <a href=#failed-deployment>fail to progress</a>.</p><h3 id=progressing-deployment>Progressing Deployment</h3><p>Kubernetes marks a Deployment as <em>progressing</em> when one of the following tasks is performed:</p><ul><li>The Deployment creates a new ReplicaSet.</li><li>The Deployment is scaling up its newest ReplicaSet.</li><li>The Deployment is scaling down its older ReplicaSet(s).</li><li>New Pods become ready or available (ready for at least <a href=#min-ready-seconds>MinReadySeconds</a>).</li></ul><p>When the rollout becomes “progressing”, the Deployment controller adds a condition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "True"</code></li><li><code>reason: NewReplicaSetCreated</code> | <code>reason: FoundNewReplicaSet</code> | <code>reason: ReplicaSetUpdated</code></li></ul><p>You can monitor the progress for a Deployment by using <code>kubectl rollout status</code>.</p><h3 id=complete-deployment>Complete Deployment</h3><p>Kubernetes marks a Deployment as <em>complete</em> when it has the following characteristics:</p><ul><li>All of the replicas associated with the Deployment have been updated to the latest version you've specified, meaning any
updates you've requested have been completed.</li><li>All of the replicas associated with the Deployment are available.</li><li>No old replicas for the Deployment are running.</li></ul><p>When the rollout becomes “complete”, the Deployment controller sets a condition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "True"</code></li><li><code>reason: NewReplicaSetAvailable</code></li></ul><p>This <code>Progressing</code> condition will retain a status value of <code>"True"</code> until a new rollout
is initiated. The condition holds even when availability of replicas changes (which
does instead affect the <code>Available</code> condition).</p><p>You can check if a Deployment has completed by using <code>kubectl rollout status</code>. If the rollout completed
successfully, <code>kubectl rollout status</code> returns a zero exit code.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Waiting for rollout to finish: 2 of 3 updated replicas are available...
deployment &#34;nginx-deployment&#34; successfully rolled out
</code></pre><p>and the exit status from <code>kubectl rollout</code> is 0 (success):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$?</span>
</span></span></code></pre></div><pre tabindex=0><code>0
</code></pre><h3 id=failed-deployment>Failed Deployment</h3><p>Your Deployment may get stuck trying to deploy its newest ReplicaSet without ever completing. This can occur
due to some of the following factors:</p><ul><li>Insufficient quota</li><li>Readiness probe failures</li><li>Image pull errors</li><li>Insufficient permissions</li><li>Limit ranges</li><li>Application runtime misconfiguration</li></ul><p>One way you can detect this condition is to specify a deadline parameter in your Deployment spec:
(<a href=#progress-deadline-seconds><code>.spec.progressDeadlineSeconds</code></a>). <code>.spec.progressDeadlineSeconds</code> denotes the
number of seconds the Deployment controller waits before indicating (in the Deployment status) that the
Deployment progress has stalled.</p><p>The following <code>kubectl</code> command sets the spec with <code>progressDeadlineSeconds</code> to make the controller report
lack of progress of a rollout for a Deployment after 10 minutes:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl patch deployment/nginx-deployment -p <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;progressDeadlineSeconds&#34;:600}}&#39;</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>deployment.apps/nginx-deployment patched
</code></pre><p>Once the deadline has been exceeded, the Deployment controller adds a DeploymentCondition with the following
attributes to the Deployment's <code>.status.conditions</code>:</p><ul><li><code>type: Progressing</code></li><li><code>status: "False"</code></li><li><code>reason: ProgressDeadlineExceeded</code></li></ul><p>This condition can also fail early and is then set to status value of <code>"False"</code> due to reasons as <code>ReplicaSetCreateError</code>.
Also, the deadline is not taken into account anymore once the Deployment rollout completes.</p><p>See the <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#typical-status-properties>Kubernetes API conventions</a> for more information on status conditions.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes takes no action on a stalled Deployment other than to report a status condition with
<code>reason: ProgressDeadlineExceeded</code>. Higher level orchestrators can take advantage of it and act accordingly, for
example, rollback the Deployment to its previous version.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you pause a Deployment rollout, Kubernetes does not check progress against your specified deadline.
You can safely pause a Deployment rollout in the middle of a rollout and resume without triggering
the condition for exceeding the deadline.</div><p>You may experience transient errors with your Deployments, either due to a low timeout that you have set or
due to any other kind of error that can be treated as transient. For example, let's suppose you have
insufficient quota. If you describe the Deployment you will notice the following section:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe deployment nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>&lt;...&gt;
Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     True    ReplicaSetUpdated
  ReplicaFailure  True    FailedCreate
&lt;...&gt;
</code></pre><p>If you run <code>kubectl get deployment nginx-deployment -o yaml</code>, the Deployment status is similar to this:</p><pre tabindex=0><code>status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: Replica set &#34;nginx-deployment-4262182780&#34; is progressing.
    reason: ReplicaSetUpdated
    status: &#34;True&#34;
    type: Progressing
  - lastTransitionTime: 2016-10-04T12:25:42Z
    lastUpdateTime: 2016-10-04T12:25:42Z
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: &#34;True&#34;
    type: Available
  - lastTransitionTime: 2016-10-04T12:25:39Z
    lastUpdateTime: 2016-10-04T12:25:39Z
    message: &#39;Error creating: pods &#34;nginx-deployment-4262182780-&#34; is forbidden: exceeded quota:
      object-counts, requested: pods=1, used: pods=3, limited: pods=2&#39;
    reason: FailedCreate
    status: &#34;True&#34;
    type: ReplicaFailure
  observedGeneration: 3
  replicas: 2
  unavailableReplicas: 2
</code></pre><p>Eventually, once the Deployment progress deadline is exceeded, Kubernetes updates the status and the
reason for the Progressing condition:</p><pre tabindex=0><code>Conditions:
  Type            Status  Reason
  ----            ------  ------
  Available       True    MinimumReplicasAvailable
  Progressing     False   ProgressDeadlineExceeded
  ReplicaFailure  True    FailedCreate
</code></pre><p>You can address an issue of insufficient quota by scaling down your Deployment, by scaling down other
controllers you may be running, or by increasing quota in your namespace. If you satisfy the quota
conditions and the Deployment controller then completes the Deployment rollout, you'll see the
Deployment's status update with a successful condition (<code>status: "True"</code> and <code>reason: NewReplicaSetAvailable</code>).</p><pre tabindex=0><code>Conditions:
  Type          Status  Reason
  ----          ------  ------
  Available     True    MinimumReplicasAvailable
  Progressing   True    NewReplicaSetAvailable
</code></pre><p><code>type: Available</code> with <code>status: "True"</code> means that your Deployment has minimum availability. Minimum availability is dictated
by the parameters specified in the deployment strategy. <code>type: Progressing</code> with <code>status: "True"</code> means that your Deployment
is either in the middle of a rollout and it is progressing or that it has successfully completed its progress and the minimum
required new replicas are available (see the Reason of the condition for the particulars - in our case
<code>reason: NewReplicaSetAvailable</code> means that the Deployment is complete).</p><p>You can check if a Deployment has failed to progress by using <code>kubectl rollout status</code>. <code>kubectl rollout status</code>
returns a non-zero exit code if the Deployment has exceeded the progression deadline.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl rollout status deployment/nginx-deployment
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...
error: deployment &#34;nginx&#34; exceeded its progress deadline
</code></pre><p>and the exit status from <code>kubectl rollout</code> is 1 (indicating an error):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$?</span>
</span></span></code></pre></div><pre tabindex=0><code>1
</code></pre><h3 id=operating-on-a-failed-deployment>Operating on a failed deployment</h3><p>All actions that apply to a complete Deployment also apply to a failed Deployment. You can scale it up/down, roll back
to a previous revision, or even pause it if you need to apply multiple tweaks in the Deployment Pod template.</p><h2 id=clean-up-policy>Clean up Policy</h2><p>You can set <code>.spec.revisionHistoryLimit</code> field in a Deployment to specify how many old ReplicaSets for
this Deployment you want to retain. The rest will be garbage-collected in the background. By default,
it is 10.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Explicitly setting this field to 0, will result in cleaning up all the history of your Deployment
thus that Deployment will not be able to roll back.</div><h2 id=canary-deployment>Canary Deployment</h2><p>If you want to roll out releases to a subset of users or servers using the Deployment, you
can create multiple Deployments, one for each release, following the canary pattern described in
<a href=/docs/concepts/cluster-administration/manage-deployment/#canary-deployments>managing resources</a>.</p><h2 id=writing-a-deployment-spec>Writing a Deployment Spec</h2><p>As with all other Kubernetes configs, a Deployment needs <code>.apiVersion</code>, <code>.kind</code>, and <code>.metadata</code> fields.
For general information about working with config files, see
<a href=/docs/tasks/run-application/run-stateless-application-deployment/>deploying applications</a>,
configuring containers, and <a href=/docs/concepts/overview/working-with-objects/object-management/>using kubectl to manage resources</a> documents.
The name of a Deployment object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>A Deployment also needs a <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> and <code>.spec.selector</code> are the only required fields of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>Pod template</a>. It has exactly the same schema as a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href=#selector>selector</a>.</p><p>Only a <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>.spec.template.spec.restartPolicy</code></a> equal to <code>Always</code> is
allowed, which is the default if not specified.</p><h3 id=replicas>Replicas</h3><p><code>.spec.replicas</code> is an optional field that specifies the number of desired Pods. It defaults to 1.</p><p>Should you manually scale a Deployment, example via <code>kubectl scale deployment deployment --replicas=X</code>, and then you update that Deployment based on a manifest
(for example: by running <code>kubectl apply -f deployment.yaml</code>),
then applying that manifest overwrites the manual scaling that you previously did.</p><p>If a <a href=/docs/tasks/run-application/horizontal-pod-autoscale/>HorizontalPodAutoscaler</a> (or any
similar API for horizontal scaling) is managing scaling for a Deployment, don't set <code>.spec.replicas</code>.</p><p>Instead, allow the Kubernetes
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> to manage the
<code>.spec.replicas</code> field automatically.</p><h3 id=selector>Selector</h3><p><code>.spec.selector</code> is a required field that specifies a <a href=/docs/concepts/overview/working-with-objects/labels/>label selector</a>
for the Pods targeted by this Deployment.</p><p><code>.spec.selector</code> must match <code>.spec.template.metadata.labels</code>, or it will be rejected by the API.</p><p>In API version <code>apps/v1</code>, <code>.spec.selector</code> and <code>.metadata.labels</code> do not default to <code>.spec.template.metadata.labels</code> if not set. So they must be set explicitly. Also note that <code>.spec.selector</code> is immutable after creation of the Deployment in <code>apps/v1</code>.</p><p>A Deployment may terminate Pods whose labels match the selector if their template is different
from <code>.spec.template</code> or if the total number of such Pods exceeds <code>.spec.replicas</code>. It brings up new
Pods with <code>.spec.template</code> if the number of Pods is less than the desired number.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You should not create other Pods whose labels match this selector, either directly, by creating
another Deployment, or by creating another controller such as a ReplicaSet or a ReplicationController. If you
do so, the first Deployment thinks that it created these other Pods. Kubernetes does not stop you from doing this.</div><p>If you have multiple controllers that have overlapping selectors, the controllers will fight with each
other and won't behave correctly.</p><h3 id=strategy>Strategy</h3><p><code>.spec.strategy</code> specifies the strategy used to replace old Pods by new ones.
<code>.spec.strategy.type</code> can be "Recreate" or "RollingUpdate". "RollingUpdate" is
the default value.</p><h4 id=recreate-deployment>Recreate Deployment</h4><p>All existing Pods are killed before new ones are created when <code>.spec.strategy.type==Recreate</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This will only guarantee Pod termination previous to creation for upgrades. If you upgrade a Deployment, all Pods
of the old revision will be terminated immediately. Successful removal is awaited before any Pod of the new
revision is created. If you manually delete a Pod, the lifecycle is controlled by the ReplicaSet and the
replacement will be created immediately (even if the old Pod is still in a Terminating state). If you need an
"at most" guarantee for your Pods, you should consider using a
<a href=/docs/concepts/workloads/controllers/statefulset/>StatefulSet</a>.</div><h4 id=rolling-update-deployment>Rolling Update Deployment</h4><p>The Deployment updates Pods in a rolling update
fashion when <code>.spec.strategy.type==RollingUpdate</code>. You can specify <code>maxUnavailable</code> and <code>maxSurge</code> to control
the rolling update process.</p><h5 id=max-unavailable>Max Unavailable</h5><p><code>.spec.strategy.rollingUpdate.maxUnavailable</code> is an optional field that specifies the maximum number
of Pods that can be unavailable during the update process. The value can be an absolute number (for example, 5)
or a percentage of desired Pods (for example, 10%). The absolute number is calculated from percentage by
rounding down. The value cannot be 0 if <code>.spec.strategy.rollingUpdate.maxSurge</code> is 0. The default value is 25%.</p><p>For example, when this value is set to 30%, the old ReplicaSet can be scaled down to 70% of desired
Pods immediately when the rolling update starts. Once new Pods are ready, old ReplicaSet can be scaled
down further, followed by scaling up the new ReplicaSet, ensuring that the total number of Pods available
at all times during the update is at least 70% of the desired Pods.</p><h5 id=max-surge>Max Surge</h5><p><code>.spec.strategy.rollingUpdate.maxSurge</code> is an optional field that specifies the maximum number of Pods
that can be created over the desired number of Pods. The value can be an absolute number (for example, 5) or a
percentage of desired Pods (for example, 10%). The value cannot be 0 if <code>MaxUnavailable</code> is 0. The absolute number
is calculated from the percentage by rounding up. The default value is 25%.</p><p>For example, when this value is set to 30%, the new ReplicaSet can be scaled up immediately when the
rolling update starts, such that the total number of old and new Pods does not exceed 130% of desired
Pods. Once old Pods have been killed, the new ReplicaSet can be scaled up further, ensuring that the
total number of Pods running at any time during the update is at most 130% of desired Pods.</p><h3 id=progress-deadline-seconds>Progress Deadline Seconds</h3><p><code>.spec.progressDeadlineSeconds</code> is an optional field that specifies the number of seconds you want
to wait for your Deployment to progress before the system reports back that the Deployment has
<a href=#failed-deployment>failed progressing</a> - surfaced as a condition with <code>type: Progressing</code>, <code>status: "False"</code>.
and <code>reason: ProgressDeadlineExceeded</code> in the status of the resource. The Deployment controller will keep
retrying the Deployment. This defaults to 600. In the future, once automatic rollback will be implemented, the Deployment
controller will roll back a Deployment as soon as it observes such a condition.</p><p>If specified, this field needs to be greater than <code>.spec.minReadySeconds</code>.</p><h3 id=min-ready-seconds>Min Ready Seconds</h3><p><code>.spec.minReadySeconds</code> is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be ready without any of its containers crashing, for it to be considered available.
This defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>Container Probes</a>.</p><h3 id=revision-history-limit>Revision History Limit</h3><p>A Deployment's revision history is stored in the ReplicaSets it controls.</p><p><code>.spec.revisionHistoryLimit</code> is an optional field that specifies the number of old ReplicaSets to retain
to allow rollback. These old ReplicaSets consume resources in <code>etcd</code> and crowd the output of <code>kubectl get rs</code>. The configuration of each Deployment revision is stored in its ReplicaSets; therefore, once an old ReplicaSet is deleted, you lose the ability to rollback to that revision of Deployment. By default, 10 old ReplicaSets will be kept, however its ideal value depends on the frequency and stability of new Deployments.</p><p>More specifically, setting this field to zero means that all old ReplicaSets with 0 replicas will be cleaned up.
In this case, a new Deployment rollout cannot be undone, since its revision history is cleaned up.</p><h3 id=paused>Paused</h3><p><code>.spec.paused</code> is an optional boolean field for pausing and resuming a Deployment. The only difference between
a paused Deployment and one that is not paused, is that any changes into the PodTemplateSpec of the paused
Deployment will not trigger new rollouts as long as it is paused. A Deployment is not paused by default when
it is created.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li><a href=/docs/tasks/run-application/run-stateless-application-deployment/>Run a Stateless Application Using a Deployment</a>.</li><li><code>Deployment</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/deployment-v1/>Deployment</a>
object definition to understand the API for deployments.</li><li>Read about <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-d459b930218774655fa7fd1620625539>2 - ReplicaSet</h1><p>A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often
used to guarantee the availability of a specified number of identical Pods.</p><h2 id=how-a-replicaset-works>How a ReplicaSet works</h2><p>A ReplicaSet is defined with fields, including a selector that specifies how to identify Pods it can acquire, a number
of replicas indicating how many Pods it should be maintaining, and a pod template specifying the data of new Pods
it should create to meet the number of replicas criteria. A ReplicaSet then fulfills its purpose by creating
and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod
template.</p><p>A ReplicaSet is linked to its Pods via the Pods' <a href=/docs/concepts/architecture/garbage-collection/#owners-and-dependents>metadata.ownerReferences</a>
field, which specifies what resource the current object is owned by. All Pods acquired by a ReplicaSet have their owning
ReplicaSet's identifying information within their ownerReferences field. It's through this link that the ReplicaSet
knows of the state of the Pods it is maintaining and plans accordingly.</p><p>A ReplicaSet identifies new Pods to acquire by using its selector. If there is a Pod that has no
OwnerReference or the OwnerReference is not a <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a> and it
matches a ReplicaSet's selector, it will be immediately acquired by said ReplicaSet.</p><h2 id=when-to-use-a-replicaset>When to use a ReplicaSet</h2><p>A ReplicaSet ensures that a specified number of pod replicas are running at any given
time. However, a Deployment is a higher-level concept that manages ReplicaSets and
provides declarative updates to Pods along with a lot of other useful features.
Therefore, we recommend using Deployments instead of directly using ReplicaSets, unless
you require custom update orchestration or don't require updates at all.</p><p>This actually means that you may never need to manipulate ReplicaSet objects:
use a Deployment instead, and define your application in the spec section.</p><h2 id=example>Example</h2><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/frontend.yaml download=controllers/frontend.yaml><code>controllers/frontend.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-frontend-yaml")' title="Copy controllers/frontend.yaml to clipboard"></img></div><div class=includecode id=controllers-frontend-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>guestbook<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># modify replicas according to your case</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>php-redis<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google_samples/gb-frontend:v3<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Saving this manifest into <code>frontend.yaml</code> and submitting it to a Kubernetes cluster will
create the defined ReplicaSet and the Pods that it manages.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>You can then get the current ReplicaSets deployed:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get rs
</span></span></code></pre></div><p>And see the frontend one you created:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME       DESIRED   CURRENT   READY   AGE
</span></span><span style=display:flex><span>frontend   <span style=color:#666>3</span>         <span style=color:#666>3</span>         <span style=color:#666>3</span>       6s
</span></span></code></pre></div><p>You can also check on the state of the ReplicaSet:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe rs/frontend
</span></span></code></pre></div><p>And you will see output similar to:</p><pre tabindex=0><code>Name:         frontend
Namespace:    default
Selector:     tier=frontend
Labels:       app=guestbook
              tier=frontend
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&#34;apiVersion&#34;:&#34;apps/v1&#34;,&#34;kind&#34;:&#34;ReplicaSet&#34;,&#34;metadata&#34;:{&#34;annotations&#34;:{},&#34;labels&#34;:{&#34;app&#34;:&#34;guestbook&#34;,&#34;tier&#34;:&#34;frontend&#34;},&#34;name&#34;:&#34;frontend&#34;,...
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  tier=frontend
  Containers:
   php-redis:
    Image:        gcr.io/google_samples/gb-frontend:v3
    Port:         &lt;none&gt;
    Host Port:    &lt;none&gt;
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
  Normal  SuccessfulCreate  117s  replicaset-controller  Created pod: frontend-wtsmm
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-b2zdv
  Normal  SuccessfulCreate  116s  replicaset-controller  Created pod: frontend-vcmts
</code></pre><p>And lastly you can check for the Pods brought up:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>You should see Pod information similar to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>NAME             READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>frontend-b2zdv   1/1     Running   <span style=color:#666>0</span>          6m36s
</span></span><span style=display:flex><span>frontend-vcmts   1/1     Running   <span style=color:#666>0</span>          6m36s
</span></span><span style=display:flex><span>frontend-wtsmm   1/1     Running   <span style=color:#666>0</span>          6m36s
</span></span></code></pre></div><p>You can also verify that the owner reference of these pods is set to the frontend ReplicaSet.
To do this, get the yaml of one of the Pods running:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods frontend-b2zdv -o yaml
</span></span></code></pre></div><p>The output will look similar to this, with the frontend ReplicaSet's info set in the metadata's ownerReferences field:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>creationTimestamp</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2020-02-12T07:06:16Z&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>generateName</span>:<span style=color:#bbb> </span>frontend-<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-b2zdv<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>default<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ownerReferences</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>blockOwnerDeletion</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>controller</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>uid</span>:<span style=color:#bbb> </span>f391f6db-bb9b-4c09-ae74-6a1f77f3d5cf<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=non-template-pod-acquisitions>Non-Template Pod acquisitions</h2><p>While you can create bare Pods with no problems, it is strongly recommended to make sure that the bare Pods do not have
labels which match the selector of one of your ReplicaSets. The reason for this is because a ReplicaSet is not limited
to owning Pods specified by its template-- it can acquire other Pods in the manner specified in the previous sections.</p><p>Take the previous frontend ReplicaSet example, and the Pods specified in the following manifest:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-rs.yaml download=pods/pod-rs.yaml><code>pods/pod-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-rs-yaml")' title="Copy pods/pod-rs.yaml to clipboard"></img></div><div class=includecode id=pods-pod-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:2.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pod2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>gcr.io/google-samples/hello-app:1.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>As those Pods do not have a Controller (or any object) as their owner reference and match the selector of the frontend
ReplicaSet, they will immediately be acquired by it.</p><p>Suppose you create the Pods after the frontend ReplicaSet has been deployed and has set up its initial Pod replicas to
fulfill its replica count requirement:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>The new Pods will be acquired by the ReplicaSet, and then immediately terminated as the ReplicaSet would be over
its desired count.</p><p>Fetching the Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>The output shows that the new Pods are either already terminated, or in the process of being terminated:</p><pre tabindex=0><code>NAME             READY   STATUS        RESTARTS   AGE
frontend-b2zdv   1/1     Running       0          10m
frontend-vcmts   1/1     Running       0          10m
frontend-wtsmm   1/1     Running       0          10m
pod1             0/1     Terminating   0          1s
pod2             0/1     Terminating   0          1s
</code></pre><p>If you create the Pods first:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/pods/pod-rs.yaml
</span></span></code></pre></div><p>And then create the ReplicaSet however:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/controllers/frontend.yaml
</span></span></code></pre></div><p>You shall see that the ReplicaSet has acquired the Pods and has only created new ones according to its spec until the
number of its new Pods and the original matches its desired count. As fetching the Pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span></code></pre></div><p>Will reveal in its output:</p><pre tabindex=0><code>NAME             READY   STATUS    RESTARTS   AGE
frontend-hmmj2   1/1     Running   0          9s
pod1             1/1     Running   0          36s
pod2             1/1     Running   0          36s
</code></pre><p>In this manner, a ReplicaSet can own a non-homogenous set of Pods</p><h2 id=writing-a-replicaset-manifest>Writing a ReplicaSet manifest</h2><p>As with all other Kubernetes API objects, a ReplicaSet needs the <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.
For ReplicaSets, the <code>kind</code> is always a ReplicaSet.</p><p>The name of a ReplicaSet object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>A ReplicaSet also needs a <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>pod template</a> which is also
required to have labels in place. In our <code>frontend.yaml</code> example we had one label: <code>tier: frontend</code>.
Be careful not to overlap with the selectors of other controllers, lest they try to adopt this Pod.</p><p>For the template's <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy>restart policy</a> field,
<code>.spec.template.spec.restartPolicy</code>, the only allowed value is <code>Always</code>, which is the default.</p><h3 id=pod-selector>Pod Selector</h3><p>The <code>.spec.selector</code> field is a <a href=/docs/concepts/overview/working-with-objects/labels/>label selector</a>. As discussed
<a href=#how-a-replicaset-works>earlier</a> these are the labels used to identify potential Pods to acquire. In our
<code>frontend.yaml</code> example, the selector was:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tier</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span></code></pre></div><p>In the ReplicaSet, <code>.spec.template.metadata.labels</code> must match <code>spec.selector</code>, or it will
be rejected by the API.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> For 2 ReplicaSets specifying the same <code>.spec.selector</code> but different
<code>.spec.template.metadata.labels</code> and <code>.spec.template.spec</code> fields, each ReplicaSet ignores the
Pods created by the other ReplicaSet.</div><h3 id=replicas>Replicas</h3><p>You can specify how many Pods should run concurrently by setting <code>.spec.replicas</code>. The ReplicaSet will create/delete
its Pods to match this number.</p><p>If you do not specify <code>.spec.replicas</code>, then it defaults to 1.</p><h2 id=working-with-replicasets>Working with ReplicaSets</h2><h3 id=deleting-a-replicaset-and-its-pods>Deleting a ReplicaSet and its Pods</h3><p>To delete a ReplicaSet and all of its Pods, use
<a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>. The
<a href=/docs/concepts/architecture/garbage-collection/>Garbage collector</a> automatically deletes all of
the dependent Pods by default.</p><p>When using the REST API or the <code>client-go</code> library, you must set <code>propagationPolicy</code> to
<code>Background</code> or <code>Foreground</code> in the <code>-d</code> option. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Foreground&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><h3 id=deleting-just-a-replicaset>Deleting just a ReplicaSet</h3><p>You can delete a ReplicaSet without affecting any of its Pods using
<a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>
with the <code>--cascade=orphan</code> option.
When using the REST API or the <code>client-go</code> library, you must set <code>propagationPolicy</code> to <code>Orphan</code>.
For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl proxy --port<span style=color:#666>=</span><span style=color:#666>8080</span>
</span></span><span style=display:flex><span>curl -X DELETE  <span style=color:#b44>&#39;localhost:8080/apis/apps/v1/namespaces/default/replicasets/frontend&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -d <span style=color:#b44>&#39;{&#34;kind&#34;:&#34;DeleteOptions&#34;,&#34;apiVersion&#34;:&#34;v1&#34;,&#34;propagationPolicy&#34;:&#34;Orphan&#34;}&#39;</span> <span style=color:#b62;font-weight:700>\
</span></span></span><span style=display:flex><span><span style=color:#b62;font-weight:700></span>&gt; -H <span style=color:#b44>&#34;Content-Type: application/json&#34;</span>
</span></span></code></pre></div><p>Once the original is deleted, you can create a new ReplicaSet to replace it. As long
as the old and new <code>.spec.selector</code> are the same, then the new one will adopt the old Pods.
However, it will not make any effort to make existing Pods match a new, different pod template.
To update Pods to a new spec in a controlled way, use a
<a href=/docs/concepts/workloads/controllers/deployment/#creating-a-deployment>Deployment</a>, as
ReplicaSets do not support a rolling update directly.</p><h3 id=isolating-pods-from-a-replicaset>Isolating Pods from a ReplicaSet</h3><p>You can remove Pods from a ReplicaSet by changing their labels. This technique may be used to remove Pods
from service for debugging, data recovery, etc. Pods that are removed in this way will be replaced automatically (
assuming that the number of replicas is not also changed).</p><h3 id=scaling-a-replicaset>Scaling a ReplicaSet</h3><p>A ReplicaSet can be easily scaled up or down by simply updating the <code>.spec.replicas</code> field. The ReplicaSet controller
ensures that a desired number of Pods with a matching label selector are available and operational.</p><p>When scaling down, the ReplicaSet controller chooses which pods to delete by sorting the available pods to
prioritize scaling down pods based on the following general algorithm:</p><ol><li>Pending (and unschedulable) pods are scaled down first</li><li>If <code>controller.kubernetes.io/pod-deletion-cost</code> annotation is set, then
the pod with the lower value will come first.</li><li>Pods on nodes with more replicas come before pods on nodes with fewer replicas.</li><li>If the pods' creation times differ, the pod that was created more recently
comes before the older pod (the creation times are bucketed on an integer log scale
when the <code>LogarithmicScaleDown</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a> is enabled)</li></ol><p>If all of the above match, then selection is random.</p><h3 id=pod-deletion-cost>Pod deletion cost</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.22 [beta]</code></div><p>Using the <a href=/docs/reference/labels-annotations-taints/#pod-deletion-cost><code>controller.kubernetes.io/pod-deletion-cost</code></a>
annotation, users can set a preference regarding which pods to remove first when downscaling a ReplicaSet.</p><p>The annotation should be set on the pod, the range is [-2147483647, 2147483647]. It represents the cost of
deleting a pod compared to other pods belonging to the same ReplicaSet. Pods with lower deletion
cost are preferred to be deleted before pods with higher deletion cost.</p><p>The implicit value for this annotation for pods that don't set it is 0; negative values are permitted.
Invalid values will be rejected by the API server.</p><p>This feature is beta and enabled by default. You can disable it using the
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
<code>PodDeletionCost</code> in both kube-apiserver and kube-controller-manager.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><ul><li>This is honored on a best-effort basis, so it does not offer any guarantees on pod deletion order.</li><li>Users should avoid updating the annotation frequently, such as updating it based on a metric value,
because doing so will generate a significant number of pod updates on the apiserver.</li></ul></div><h4 id=example-use-case>Example Use Case</h4><p>The different pods of an application could have different utilization levels. On scale down, the application
may prefer to remove the pods with lower utilization. To avoid frequently updating the pods, the application
should update <code>controller.kubernetes.io/pod-deletion-cost</code> once before issuing a scale down (setting the
annotation to a value proportional to pod utilization level). This works if the application itself controls
the down scaling; for example, the driver pod of a Spark deployment.</p><h3 id=replicaset-as-a-horizontal-pod-autoscaler-target>ReplicaSet as a Horizontal Pod Autoscaler Target</h3><p>A ReplicaSet can also be a target for
<a href=/docs/tasks/run-application/horizontal-pod-autoscale/>Horizontal Pod Autoscalers (HPA)</a>. That is,
a ReplicaSet can be auto-scaled by an HPA. Here is an example HPA targeting
the ReplicaSet we created in the previous example.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/hpa-rs.yaml download=controllers/hpa-rs.yaml><code>controllers/hpa-rs.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-hpa-rs-yaml")' title="Copy controllers/hpa-rs.yaml to clipboard"></img></div><div class=includecode id=controllers-hpa-rs-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>autoscaling/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>HorizontalPodAutoscaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend-scaler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>scaleTargetRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicaSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>frontend<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>minReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>maxReplicas</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>targetCPUUtilizationPercentage</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Saving this manifest into <code>hpa-rs.yaml</code> and submitting it to a Kubernetes cluster should
create the defined HPA that autoscales the target ReplicaSet depending on the CPU usage
of the replicated Pods.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/hpa-rs.yaml
</span></span></code></pre></div><p>Alternatively, you can use the <code>kubectl autoscale</code> command to accomplish the same
(and it's easier!)</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl autoscale rs frontend --max<span style=color:#666>=</span><span style=color:#666>10</span> --min<span style=color:#666>=</span><span style=color:#666>3</span> --cpu-percent<span style=color:#666>=</span><span style=color:#666>50</span>
</span></span></code></pre></div><h2 id=alternatives-to-replicaset>Alternatives to ReplicaSet</h2><h3 id=deployment-recommended>Deployment (recommended)</h3><p><a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> is an object which can own ReplicaSets and update
them and their Pods via declarative, server-side rolling updates.
While ReplicaSets can be used independently, today they're mainly used by Deployments as a mechanism to orchestrate Pod
creation, deletion and updates. When you use Deployments you don't have to worry about managing the ReplicaSets that
they create. Deployments own and manage their ReplicaSets.
As such, it is recommended to use Deployments when you want ReplicaSets.</p><h3 id=bare-pods>Bare Pods</h3><p>Unlike the case where a user directly created Pods, a ReplicaSet replaces Pods that are deleted or
terminated for any reason, such as in the case of node failure or disruptive node maintenance,
such as a kernel upgrade. For this reason, we recommend that you use a ReplicaSet even if your
application requires only a single Pod. Think of it similarly to a process supervisor, only it
supervises multiple Pods across multiple nodes instead of individual processes on a single node. A
ReplicaSet delegates local container restarts to some agent on the node such as Kubelet.</p><h3 id=job>Job</h3><p>Use a <a href=/docs/concepts/workloads/controllers/job/><code>Job</code></a> instead of a ReplicaSet for Pods that are
expected to terminate on their own (that is, batch jobs).</p><h3 id=daemonset>DaemonSet</h3><p>Use a <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> instead of a ReplicaSet for Pods that provide a
machine-level function, such as machine monitoring or machine logging. These Pods have a lifetime that is tied
to a machine lifetime: the Pod needs to be running on the machine before other Pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h3 id=replicationcontroller>ReplicationController</h3><p>ReplicaSets are the successors to <a href=/docs/concepts/workloads/controllers/replicationcontroller/>ReplicationControllers</a>.
The two serve the same purpose, and behave similarly, except that a ReplicationController does not support set-based
selector requirements as described in the <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>labels user guide</a>.
As such, ReplicaSets are preferred over ReplicationControllers</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li>Learn about <a href=/docs/concepts/workloads/controllers/deployment/>Deployments</a>.</li><li><a href=/docs/tasks/run-application/run-stateless-application-deployment/>Run a Stateless Application Using a Deployment</a>,
which relies on ReplicaSets to work.</li><li><code>ReplicaSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/replica-set-v1/>ReplicaSet</a>
object definition to understand the API for replica sets.</li><li>Read about <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6d72299952c37ca8cc61b416e5bdbcd4>3 - StatefulSets</h1><p>StatefulSet is the workload API object used to manage stateful applications.</p><p>Manages the deployment and scaling of a set of <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>, <em>and provides guarantees about the ordering and uniqueness</em> of these Pods.</p><p>Like a <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.</p><p>If you want to use storage volumes to provide persistence for your workload, you can use a StatefulSet as part of the solution. Although individual Pods in a StatefulSet are susceptible to failure, the persistent Pod identifiers make it easier to match existing volumes to the new Pods that replace any that have failed.</p><h2 id=using-statefulsets>Using StatefulSets</h2><p>StatefulSets are valuable for applications that require one or more of the
following.</p><ul><li>Stable, unique network identifiers.</li><li>Stable, persistent storage.</li><li>Ordered, graceful deployment and scaling.</li><li>Ordered, automated rolling updates.</li></ul><p>In the above, stable is synonymous with persistence across Pod (re)scheduling.
If an application doesn't require any stable identifiers or ordered deployment,
deletion, or scaling, you should deploy your application using a workload object
that provides a set of stateless replicas.
<a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a> or
<a href=/docs/concepts/workloads/controllers/replicaset/>ReplicaSet</a> may be better suited to your stateless needs.</p><h2 id=limitations>Limitations</h2><ul><li>The storage for a given Pod must either be provisioned by a
<a href=https://github.com/kubernetes/examples/tree/master/staging/persistent-volume-provisioning/README.md>PersistentVolume Provisioner</a>
based on the requested <code>storage class</code>, or pre-provisioned by an admin.</li><li>Deleting and/or scaling a StatefulSet down will <em>not</em> delete the volumes associated with the
StatefulSet. This is done to ensure data safety, which is generally more valuable than an
automatic purge of all related StatefulSet resources.</li><li>StatefulSets currently require a <a href=/docs/concepts/services-networking/service/#headless-services>Headless Service</a>
to be responsible for the network identity of the Pods. You are responsible for creating this
Service.</li><li>StatefulSets do not provide any guarantees on the termination of pods when a StatefulSet is
deleted. To achieve ordered and graceful termination of the pods in the StatefulSet, it is
possible to scale the StatefulSet down to 0 prior to deletion.</li><li>When using <a href=#rolling-updates>Rolling Updates</a> with the default
<a href=#pod-management-policies>Pod Management Policy</a> (<code>OrderedReady</code>),
it's possible to get into a broken state that requires
<a href=#forced-rollback>manual intervention to repair</a>.</li></ul><h2 id=components>Components</h2><p>The example below demonstrates the components of a StatefulSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Service<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>port</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>clusterIP</span>:<span style=color:#bbb> </span>None<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># has to match .spec.template.metadata.labels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>serviceName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;nginx&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># by default is 1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>minReadySeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb> </span><span style=color:#080;font-style:italic># by default is 0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb> </span><span style=color:#080;font-style:italic># has to match .spec.selector.matchLabels</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/nginx-slim:0.8<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/usr/share/nginx/html<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>volumeClaimTemplates</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>www<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>accessModes</span>:<span style=color:#bbb> </span>[<span style=color:#bbb> </span><span style=color:#b44>&#34;ReadWriteOnce&#34;</span><span style=color:#bbb> </span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>storageClassName</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;my-storage-class&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>storage</span>:<span style=color:#bbb> </span>1Gi<span style=color:#bbb>
</span></span></span></code></pre></div><p>In the above example:</p><ul><li>A Headless Service, named <code>nginx</code>, is used to control the network domain.</li><li>The StatefulSet, named <code>web</code>, has a Spec that indicates that 3 replicas of the nginx container will be launched in unique Pods.</li><li>The <code>volumeClaimTemplates</code> will provide stable storage using
<a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a> provisioned by a
PersistentVolume Provisioner.</li></ul><p>The name of a StatefulSet object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><h3 id=pod-selector>Pod Selector</h3><p>You must set the <code>.spec.selector</code> field of a StatefulSet to match the labels of its
<code>.spec.template.metadata.labels</code>. Failing to specify a matching Pod Selector will result in a
validation error during StatefulSet creation.</p><h3 id=volume-claim-templates>Volume Claim Templates</h3><p>You can set the <code>.spec.volumeClaimTemplates</code> which can provide stable storage using
<a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumes</a> provisioned by a PersistentVolume
Provisioner.</p><h3 id=minimum-ready-seconds>Minimum ready seconds</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p><code>.spec.minReadySeconds</code> is an optional field that specifies the minimum number of seconds for which a newly
created Pod should be running and ready without any of its containers crashing, for it to be considered available.
This is used to check progression of a rollout when using a <a href=#rolling-updates>Rolling Update</a> strategy.
This field defaults to 0 (the Pod will be considered available as soon as it is ready). To learn more about when
a Pod is considered ready, see <a href=/docs/concepts/workloads/pods/pod-lifecycle/#container-probes>Container Probes</a>.</p><h2 id=pod-identity>Pod Identity</h2><p>StatefulSet Pods have a unique identity that consists of an ordinal, a
stable network identity, and stable storage. The identity sticks to the Pod,
regardless of which node it's (re)scheduled on.</p><h3 id=ordinal-index>Ordinal Index</h3><p>For a StatefulSet with N replicas, each Pod in the StatefulSet will be
assigned an integer ordinal, from 0 up through N-1, that is unique over the Set.</p><h3 id=stable-network-id>Stable Network ID</h3><p>Each Pod in a StatefulSet derives its hostname from the name of the StatefulSet
and the ordinal of the Pod. The pattern for the constructed hostname
is <code>$(statefulset name)-$(ordinal)</code>. The example above will create three Pods
named <code>web-0,web-1,web-2</code>.
A StatefulSet can use a <a href=/docs/concepts/services-networking/service/#headless-services>Headless Service</a>
to control the domain of its Pods. The domain managed by this Service takes the form:
<code>$(service name).$(namespace).svc.cluster.local</code>, where "cluster.local" is the
cluster domain.
As each Pod is created, it gets a matching DNS subdomain, taking the form:
<code>$(podname).$(governing service domain)</code>, where the governing service is defined
by the <code>serviceName</code> field on the StatefulSet.</p><p>Depending on how DNS is configured in your cluster, you may not be able to look up the DNS
name for a newly-run Pod immediately. This behavior can occur when other clients in the
cluster have already sent queries for the hostname of the Pod before it was created.
Negative caching (normal in DNS) means that the results of previous failed lookups are
remembered and reused, even after the Pod is running, for at least a few seconds.</p><p>If you need to discover Pods promptly after they are created, you have a few options:</p><ul><li>Query the Kubernetes API directly (for example, using a watch) rather than relying on DNS lookups.</li><li>Decrease the time of caching in your Kubernetes DNS provider (typically this means editing the
config map for CoreDNS, which currently caches for 30 seconds).</li></ul><p>As mentioned in the <a href=#limitations>limitations</a> section, you are responsible for
creating the <a href=/docs/concepts/services-networking/service/#headless-services>Headless Service</a>
responsible for the network identity of the pods.</p><p>Here are some examples of choices for Cluster Domain, Service name,
StatefulSet name, and how that affects the DNS names for the StatefulSet's Pods.</p><table><thead><tr><th>Cluster Domain</th><th>Service (ns/name)</th><th>StatefulSet (ns/name)</th><th>StatefulSet Domain</th><th>Pod DNS</th><th>Pod Hostname</th></tr></thead><tbody><tr><td>cluster.local</td><td>default/nginx</td><td>default/web</td><td>nginx.default.svc.cluster.local</td><td>web-{0..N-1}.nginx.default.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>cluster.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.cluster.local</td><td>web-{0..N-1}.nginx.foo.svc.cluster.local</td><td>web-{0..N-1}</td></tr><tr><td>kube.local</td><td>foo/nginx</td><td>foo/web</td><td>nginx.foo.svc.kube.local</td><td>web-{0..N-1}.nginx.foo.svc.kube.local</td><td>web-{0..N-1}</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Cluster Domain will be set to <code>cluster.local</code> unless
<a href=/docs/concepts/services-networking/dns-pod-service/>otherwise configured</a>.</div><h3 id=stable-storage>Stable Storage</h3><p>For each VolumeClaimTemplate entry defined in a StatefulSet, each Pod receives one
PersistentVolumeClaim. In the nginx example above, each Pod receives a single PersistentVolume
with a StorageClass of <code>my-storage-class</code> and 1 Gib of provisioned storage. If no StorageClass
is specified, then the default StorageClass will be used. When a Pod is (re)scheduled
onto a node, its <code>volumeMounts</code> mount the PersistentVolumes associated with its
PersistentVolume Claims. Note that, the PersistentVolumes associated with the
Pods' PersistentVolume Claims are not deleted when the Pods, or StatefulSet are deleted.
This must be done manually.</p><h3 id=pod-name-label>Pod Name Label</h3><p>When the StatefulSet <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> creates a Pod,
it adds a label, <code>statefulset.kubernetes.io/pod-name</code>, that is set to the name of
the Pod. This label allows you to attach a Service to a specific Pod in
the StatefulSet.</p><h2 id=deployment-and-scaling-guarantees>Deployment and Scaling Guarantees</h2><ul><li>For a StatefulSet with N replicas, when Pods are being deployed, they are created sequentially, in order from {0..N-1}.</li><li>When Pods are being deleted, they are terminated in reverse order, from {N-1..0}.</li><li>Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready.</li><li>Before a Pod is terminated, all of its successors must be completely shutdown.</li></ul><p>The StatefulSet should not specify a <code>pod.Spec.TerminationGracePeriodSeconds</code> of 0. This practice
is unsafe and strongly discouraged. For further explanation, please refer to
<a href=/docs/tasks/run-application/force-delete-stateful-set-pod/>force deleting StatefulSet Pods</a>.</p><p>When the nginx example above is created, three Pods will be deployed in the order
web-0, web-1, web-2. web-1 will not be deployed before web-0 is
<a href=/docs/concepts/workloads/pods/pod-lifecycle/>Running and Ready</a>, and web-2 will not be deployed until
web-1 is Running and Ready. If web-0 should fail, after web-1 is Running and Ready, but before
web-2 is launched, web-2 will not be launched until web-0 is successfully relaunched and
becomes Running and Ready.</p><p>If a user were to scale the deployed example by patching the StatefulSet such that
<code>replicas=1</code>, web-2 would be terminated first. web-1 would not be terminated until web-2
is fully shutdown and deleted. If web-0 were to fail after web-2 has been terminated and
is completely shutdown, but prior to web-1's termination, web-1 would not be terminated
until web-0 is Running and Ready.</p><h3 id=pod-management-policies>Pod Management Policies</h3><p>StatefulSet allows you to relax its ordering guarantees while
preserving its uniqueness and identity guarantees via its <code>.spec.podManagementPolicy</code> field.</p><h4 id=orderedready-pod-management>OrderedReady Pod Management</h4><p><code>OrderedReady</code> pod management is the default for StatefulSets. It implements the behavior
described <a href=#deployment-and-scaling-guarantees>above</a>.</p><h4 id=parallel-pod-management>Parallel Pod Management</h4><p><code>Parallel</code> pod management tells the StatefulSet controller to launch or
terminate all Pods in parallel, and to not wait for Pods to become Running
and Ready or completely terminated prior to launching or terminating another
Pod. This option only affects the behavior for scaling operations. Updates are not
affected.</p><h2 id=update-strategies>Update strategies</h2><p>A StatefulSet's <code>.spec.updateStrategy</code> field allows you to configure
and disable automated rolling updates for containers, labels, resource request/limits, and
annotations for the Pods in a StatefulSet. There are two possible values:</p><dl><dt><code>OnDelete</code></dt><dd>When a StatefulSet's <code>.spec.updateStrategy.type</code> is set to <code>OnDelete</code>,
the StatefulSet controller will not automatically update the Pods in a
StatefulSet. Users must manually delete Pods to cause the controller to
create new Pods that reflect modifications made to a StatefulSet's <code>.spec.template</code>.</dd><dt><code>RollingUpdate</code></dt><dd>The <code>RollingUpdate</code> update strategy implements automated, rolling updates for the Pods in a
StatefulSet. This is the default update strategy.</dd></dl><h2 id=rolling-updates>Rolling Updates</h2><p>When a StatefulSet's <code>.spec.updateStrategy.type</code> is set to <code>RollingUpdate</code>, the
StatefulSet controller will delete and recreate each Pod in the StatefulSet. It will proceed
in the same order as Pod termination (from the largest ordinal to the smallest), updating
each Pod one at a time.</p><p>The Kubernetes control plane waits until an updated Pod is Running and Ready prior
to updating its predecessor. If you have set <code>.spec.minReadySeconds</code> (see
<a href=#minimum-ready-seconds>Minimum Ready Seconds</a>), the control plane additionally waits that
amount of time after the Pod turns ready, before moving on.</p><h3 id=partitions>Partitioned rolling updates</h3><p>The <code>RollingUpdate</code> update strategy can be partitioned, by specifying a
<code>.spec.updateStrategy.rollingUpdate.partition</code>. If a partition is specified, all Pods with an
ordinal that is greater than or equal to the partition will be updated when the StatefulSet's
<code>.spec.template</code> is updated. All Pods with an ordinal that is less than the partition will not
be updated, and, even if they are deleted, they will be recreated at the previous version. If a
StatefulSet's <code>.spec.updateStrategy.rollingUpdate.partition</code> is greater than its <code>.spec.replicas</code>,
updates to its <code>.spec.template</code> will not be propagated to its Pods.
In most cases you will not need to use a partition, but they are useful if you want to stage an
update, roll out a canary, or perform a phased roll out.</p><h3 id=maximum-unavailable-pods>Maximum unavailable Pods</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [alpha]</code></div><p>You can control the maximum number of Pods that can be unavailable during an update
by specifying the <code>.spec.updateStrategy.rollingUpdate.maxUnavailable</code> field.
The value can be an absolute number (for example, <code>5</code>) or a percentage of desired
Pods (for example, <code>10%</code>). Absolute number is calculated from the percentage value
by rounding it up. This field cannot be 0. The default setting is 1.</p><p>This field applies to all Pods in the range <code>0</code> to <code>replicas - 1</code>. If there is any
unavailable Pod in the range <code>0</code> to <code>replicas - 1</code>, it will be counted towards
<code>maxUnavailable</code>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>maxUnavailable</code> field is in Alpha stage and it is honored only by API servers
that are running with the <code>MaxUnavailableStatefulSet</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
enabled.</div><h3 id=forced-rollback>Forced rollback</h3><p>When using <a href=#rolling-updates>Rolling Updates</a> with the default
<a href=#pod-management-policies>Pod Management Policy</a> (<code>OrderedReady</code>),
it's possible to get into a broken state that requires manual intervention to repair.</p><p>If you update the Pod template to a configuration that never becomes Running and
Ready (for example, due to a bad binary or application-level configuration error),
StatefulSet will stop the rollout and wait.</p><p>In this state, it's not enough to revert the Pod template to a good configuration.
Due to a <a href=https://github.com/kubernetes/kubernetes/issues/67250>known issue</a>,
StatefulSet will continue to wait for the broken Pod to become Ready
(which never happens) before it will attempt to revert it back to the working
configuration.</p><p>After reverting the template, you must also delete any Pods that StatefulSet had
already attempted to run with the bad configuration.
StatefulSet will then begin to recreate the Pods using the reverted template.</p><h2 id=persistentvolumeclaim-retention>PersistentVolumeClaim retention</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [alpha]</code></div><p>The optional <code>.spec.persistentVolumeClaimRetentionPolicy</code> field controls if
and how PVCs are deleted during the lifecycle of a StatefulSet. You must enable the
<code>StatefulSetAutoDeletePVC</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the API server and the controller manager to use this field.
Once enabled, there are two policies you can configure for each StatefulSet:</p><dl><dt><code>whenDeleted</code></dt><dd>configures the volume retention behavior that applies when the StatefulSet is deleted</dd><dt><code>whenScaled</code></dt><dd>configures the volume retention behavior that applies when the replica count of
the StatefulSet is reduced; for example, when scaling down the set.</dd></dl><p>For each policy that you can configure, you can set the value to either <code>Delete</code> or <code>Retain</code>.</p><dl><dt><code>Delete</code></dt><dd>The PVCs created from the StatefulSet <code>volumeClaimTemplate</code> are deleted for each Pod
affected by the policy. With the <code>whenDeleted</code> policy all PVCs from the
<code>volumeClaimTemplate</code> are deleted after their Pods have been deleted. With the
<code>whenScaled</code> policy, only PVCs corresponding to Pod replicas being scaled down are
deleted, after their Pods have been deleted.</dd><dt><code>Retain</code> (default)</dt><dd>PVCs from the <code>volumeClaimTemplate</code> are not affected when their Pod is
deleted. This is the behavior before this new feature.</dd></dl><p>Bear in mind that these policies <strong>only</strong> apply when Pods are being removed due to the
StatefulSet being deleted or scaled down. For example, if a Pod associated with a StatefulSet
fails due to node failure, and the control plane creates a replacement Pod, the StatefulSet
retains the existing PVC. The existing volume is unaffected, and the cluster will attach it to
the node where the new Pod is about to launch.</p><p>The default for policies is <code>Retain</code>, matching the StatefulSet behavior before this new feature.</p><p>Here is an example policy.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>StatefulSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>persistentVolumeClaimRetentionPolicy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenDeleted</span>:<span style=color:#bbb> </span>Retain<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenScaled</span>:<span style=color:#bbb> </span>Delete<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The StatefulSet <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> adds
<a href=/docs/concepts/overview/working-with-objects/owners-dependents/#owner-references-in-object-specifications>owner references</a>
to its PVCs, which are then deleted by the <a class=glossary-tooltip title='A collective term for the various mechanisms Kubernetes uses to clean up cluster resources.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/garbage-collection/ target=_blank aria-label='garbage collector'>garbage collector</a> after the Pod is terminated. This enables the Pod to
cleanly unmount all volumes before the PVCs are deleted (and before the backing PV and
volume are deleted, depending on the retain policy). When you set the <code>whenDeleted</code>
policy to <code>Delete</code>, an owner reference to the StatefulSet instance is placed on all PVCs
associated with that StatefulSet.</p><p>The <code>whenScaled</code> policy must delete PVCs only when a Pod is scaled down, and not when a
Pod is deleted for another reason. When reconciling, the StatefulSet controller compares
its desired replica count to the actual Pods present on the cluster. Any StatefulSet Pod
whose id greater than the replica count is condemned and marked for deletion. If the
<code>whenScaled</code> policy is <code>Delete</code>, the condemned Pods are first set as owners to the
associated StatefulSet template PVCs, before the Pod is deleted. This causes the PVCs
to be garbage collected after only the condemned Pods have terminated.</p><p>This means that if the controller crashes and restarts, no Pod will be deleted before its
owner reference has been updated appropriate to the policy. If a condemned Pod is
force-deleted while the controller is down, the owner reference may or may not have been
set up, depending on when the controller crashed. It may take several reconcile loops to
update the owner references, so some condemned Pods may have set up owner references and
others may not. For this reason we recommend waiting for the controller to come back up,
which will verify owner references before terminating Pods. If that is not possible, the
operator should verify the owner references on PVCs to ensure the expected objects are
deleted when Pods are force-deleted.</p><h3 id=replicas>Replicas</h3><p><code>.spec.replicas</code> is an optional field that specifies the number of desired Pods. It defaults to 1.</p><p>Should you manually scale a deployment, example via <code>kubectl scale statefulset statefulset --replicas=X</code>, and then you update that StatefulSet
based on a manifest (for example: by running <code>kubectl apply -f statefulset.yaml</code>), then applying that manifest overwrites the manual scaling
that you previously did.</p><p>If a <a href=/docs/tasks/run-application/horizontal-pod-autoscale/>HorizontalPodAutoscaler</a>
(or any similar API for horizontal scaling) is managing scaling for a
Statefulset, don't set <code>.spec.replicas</code>. Instead, allow the Kubernetes
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> to manage
the <code>.spec.replicas</code> field automatically.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li>Find out how to use StatefulSets<ul><li>Follow an example of <a href=/docs/tutorials/stateful-application/basic-stateful-set/>deploying a stateful application</a>.</li><li>Follow an example of <a href=/docs/tutorials/stateful-application/cassandra/>deploying Cassandra with Stateful Sets</a>.</li><li>Follow an example of <a href=/docs/tasks/run-application/run-replicated-stateful-application/>running a replicated stateful application</a>.</li><li>Learn how to <a href=/docs/tasks/run-application/scale-stateful-set/>scale a StatefulSet</a>.</li><li>Learn what's involved when you <a href=/docs/tasks/run-application/delete-stateful-set/>delete a StatefulSet</a>.</li><li>Learn how to <a href=/docs/tasks/configure-pod-container/configure-volume-storage/>configure a Pod to use a volume for storage</a>.</li><li>Learn how to <a href=/docs/tasks/configure-pod-container/configure-persistent-volume-storage/>configure a Pod to use a PersistentVolume for storage</a>.</li></ul></li><li><code>StatefulSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/stateful-set-v1/>StatefulSet</a>
object definition to understand the API for stateful sets.</li><li>Read about <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> and how
you can use it to manage application availability during disruptions.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-41600eb8b6631c88848156f381e9d588>4 - DaemonSet</h1><p>A <em>DaemonSet</em> ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the
cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage
collected. Deleting a DaemonSet will clean up the Pods it created.</p><p>Some typical uses of a DaemonSet are:</p><ul><li>running a cluster storage daemon on every node</li><li>running a logs collection daemon on every node</li><li>running a node monitoring daemon on every node</li></ul><p>In a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.
A more complex setup might use multiple DaemonSets for a single type of daemon, but with
different flags and/or different memory and cpu requests for different hardware types.</p><h2 id=writing-a-daemonset-spec>Writing a DaemonSet Spec</h2><h3 id=create-a-daemonset>Create a DaemonSet</h3><p>You can describe a DaemonSet in a YAML file. For example, the <code>daemonset.yaml</code> file below
describes a DaemonSet that runs the fluentd-elasticsearch Docker image:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/daemonset.yaml download=controllers/daemonset.yaml><code>controllers/daemonset.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-daemonset-yaml")' title="Copy controllers/daemonset.yaml to clipboard"></img></div><div class=includecode id=controllers-daemonset-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>DaemonSet<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>namespace</span>:<span style=color:#bbb> </span>kube-system<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>k8s-app</span>:<span style=color:#bbb> </span>fluentd-logging<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># these tolerations are to have the daemonset runnable on control plane nodes</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:#080;font-style:italic># remove them if your control plane nodes should not run pods</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node-role.kubernetes.io/control-plane<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Exists<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>node-role.kubernetes.io/master<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>Exists<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span>NoSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>fluentd-elasticsearch<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>quay.io/fluentd_elasticsearch/fluentd:v2.5.2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requests</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>100m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>volumeMounts</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>mountPath</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>terminationGracePeriodSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>30</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>volumes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>varlog<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>hostPath</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>path</span>:<span style=color:#bbb> </span>/var/log<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Create a DaemonSet based on the YAML file:</p><pre tabindex=0><code>kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml
</code></pre><h3 id=required-fields>Required Fields</h3><p>As with all other Kubernetes config, a DaemonSet needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields. For
general information about working with config files, see
<a href=/docs/tasks/run-application/run-stateless-application-deployment/>running stateless applications</a>
and <a href=/docs/concepts/overview/working-with-objects/object-management/>object management using kubectl</a>.</p><p>The name of a DaemonSet object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>A DaemonSet also needs a
<a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code></a>
section.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> is one of the required fields in <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>pod template</a>.
It has exactly the same schema as a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>,
except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a Pod template in a DaemonSet has to specify appropriate
labels (see <a href=#pod-selector>pod selector</a>).</p><p>A Pod Template in a DaemonSet must have a <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>RestartPolicy</code></a>
equal to <code>Always</code>, or be unspecified, which defaults to <code>Always</code>.</p><h3 id=pod-selector>Pod Selector</h3><p>The <code>.spec.selector</code> field is a pod selector. It works the same as the <code>.spec.selector</code> of
a <a href=/docs/concepts/workloads/controllers/job/>Job</a>.</p><p>You must specify a pod selector that matches the labels of the
<code>.spec.template</code>.
Also, once a DaemonSet is created,
its <code>.spec.selector</code> can not be mutated. Mutating the pod selector can lead to the
unintentional orphaning of Pods, and it was found to be confusing to users.</p><p>The <code>.spec.selector</code> is an object consisting of two fields:</p><ul><li><code>matchLabels</code> - works the same as the <code>.spec.selector</code> of a
<a href=/docs/concepts/workloads/controllers/replicationcontroller/>ReplicationController</a>.</li><li><code>matchExpressions</code> - allows to build more sophisticated selectors by specifying key,
list of values and an operator that relates the key and values.</li></ul><p>When the two are specified the result is ANDed.</p><p>The <code>.spec.selector</code> must match the <code>.spec.template.metadata.labels</code>.
Config with these two not matching will be rejected by the API.</p><h3 id=running-pods-on-select-nodes>Running Pods on select Nodes</h3><p>If you specify a <code>.spec.template.spec.nodeSelector</code>, then the DaemonSet controller will
create Pods on nodes which match that <a href=/docs/concepts/scheduling-eviction/assign-pod-node/>node selector</a>.
Likewise if you specify a <code>.spec.template.spec.affinity</code>,
then DaemonSet controller will create Pods on nodes which match that
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/>node affinity</a>.
If you do not specify either, then the DaemonSet controller will create Pods on all nodes.</p><h2 id=how-daemon-pods-are-scheduled>How Daemon Pods are scheduled</h2><h3 id=scheduled-by-default-scheduler>Scheduled by default scheduler</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes 1.17 [stable]</code></div><p>A DaemonSet ensures that all eligible nodes run a copy of a Pod. Normally, the
node that a Pod runs on is selected by the Kubernetes scheduler. However,
DaemonSet pods are created and scheduled by the DaemonSet controller instead.
That introduces the following issues:</p><ul><li>Inconsistent Pod behavior: Normal Pods waiting to be scheduled are created
and in <code>Pending</code> state, but DaemonSet pods are not created in <code>Pending</code>
state. This is confusing to the user.</li><li><a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod preemption</a>
is handled by default scheduler. When preemption is enabled, the DaemonSet controller
will make scheduling decisions without considering pod priority and preemption.</li></ul><p><code>ScheduleDaemonSetPods</code> allows you to schedule DaemonSets using the default
scheduler instead of the DaemonSet controller, by adding the <code>NodeAffinity</code> term
to the DaemonSet pods, instead of the <code>.spec.nodeName</code> term. The default
scheduler is then used to bind the pod to the target host. If node affinity of
the DaemonSet pod already exists, it is replaced (the original node affinity was
taken into account before selecting the target host). The DaemonSet controller only
performs these operations when creating or modifying DaemonSet pods, and no
changes are made to the <code>spec.template</code> of the DaemonSet.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>matchFields</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>metadata.name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- target-host-name<span style=color:#bbb>
</span></span></span></code></pre></div><p>In addition, <code>node.kubernetes.io/unschedulable:NoSchedule</code> toleration is added
automatically to DaemonSet Pods. The default scheduler ignores
<code>unschedulable</code> Nodes when scheduling DaemonSet Pods.</p><h3 id=taints-and-tolerations>Taints and Tolerations</h3><p>Although Daemon Pods respect
<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>taints and tolerations</a>,
the following tolerations are added to DaemonSet Pods automatically according to
the related features.</p><table><thead><tr><th>Toleration Key</th><th>Effect</th><th>Version</th><th>Description</th></tr></thead><tbody><tr><td><code>node.kubernetes.io/not-ready</code></td><td>NoExecute</td><td>1.13+</td><td>DaemonSet pods will not be evicted when there are node problems such as a network partition.</td></tr><tr><td><code>node.kubernetes.io/unreachable</code></td><td>NoExecute</td><td>1.13+</td><td>DaemonSet pods will not be evicted when there are node problems such as a network partition.</td></tr><tr><td><code>node.kubernetes.io/disk-pressure</code></td><td>NoSchedule</td><td>1.8+</td><td>DaemonSet pods tolerate disk-pressure attributes by default scheduler.</td></tr><tr><td><code>node.kubernetes.io/memory-pressure</code></td><td>NoSchedule</td><td>1.8+</td><td>DaemonSet pods tolerate memory-pressure attributes by default scheduler.</td></tr><tr><td><code>node.kubernetes.io/unschedulable</code></td><td>NoSchedule</td><td>1.12+</td><td>DaemonSet pods tolerate unschedulable attributes by default scheduler.</td></tr><tr><td><code>node.kubernetes.io/network-unavailable</code></td><td>NoSchedule</td><td>1.12+</td><td>DaemonSet pods, who uses host network, tolerate network-unavailable attributes by default scheduler.</td></tr></tbody></table><h2 id=communicating-with-daemon-pods>Communicating with Daemon Pods</h2><p>Some possible patterns for communicating with Pods in a DaemonSet are:</p><ul><li><strong>Push</strong>: Pods in the DaemonSet are configured to send updates to another service, such
as a stats database. They do not have clients.</li><li><strong>NodeIP and Known Port</strong>: Pods in the DaemonSet can use a <code>hostPort</code>, so that the pods
are reachable via the node IPs.
Clients know the list of node IPs somehow, and know the port by convention.</li><li><strong>DNS</strong>: Create a <a href=/docs/concepts/services-networking/service/#headless-services>headless service</a>
with the same pod selector, and then discover DaemonSets using the <code>endpoints</code>
resource or retrieve multiple A records from DNS.</li><li><strong>Service</strong>: Create a service with the same Pod selector, and use the service to reach a
daemon on a random node. (No way to reach specific node.)</li></ul><h2 id=updating-a-daemonset>Updating a DaemonSet</h2><p>If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete
Pods from newly not-matching nodes.</p><p>You can modify the Pods that a DaemonSet creates. However, Pods do not allow all
fields to be updated. Also, the DaemonSet controller will use the original template the next
time a node (even with the same name) is created.</p><p>You can delete a DaemonSet. If you specify <code>--cascade=orphan</code> with <code>kubectl</code>, then the Pods
will be left on the nodes. If you subsequently create a new DaemonSet with the same selector,
the new DaemonSet adopts the existing Pods. If any Pods need replacing the DaemonSet replaces
them according to its <code>updateStrategy</code>.</p><p>You can <a href=/docs/tasks/manage-daemon/update-daemon-set/>perform a rolling update</a> on a DaemonSet.</p><h2 id=alternatives-to-daemonset>Alternatives to DaemonSet</h2><h3 id=init-scripts>Init scripts</h3><p>It is certainly possible to run daemon processes by directly starting them on a node (e.g. using
<code>init</code>, <code>upstartd</code>, or <code>systemd</code>). This is perfectly fine. However, there are several advantages to
running such processes via a DaemonSet:</p><ul><li>Ability to monitor and manage logs for daemons in the same way as applications.</li><li>Same config language and tools (e.g. Pod templates, <code>kubectl</code>) for daemons and applications.</li><li>Running daemons in containers with resource limits increases isolation between daemons from app
containers. However, this can also be accomplished by running the daemons in a container but not in a Pod.</li></ul><h3 id=bare-pods>Bare Pods</h3><p>It is possible to create Pods directly which specify a particular node to run on. However,
a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of
node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should
use a DaemonSet rather than creating individual Pods.</p><h3 id=static-pods>Static Pods</h3><p>It is possible to create Pods by writing a file to a certain directory watched by Kubelet. These
are called <a href=/docs/tasks/configure-pod-container/static-pod/>static pods</a>.
Unlike DaemonSet, static Pods cannot be managed with kubectl
or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful
in cluster bootstrapping cases. Also, static Pods may be deprecated in the future.</p><h3 id=deployments>Deployments</h3><p>DaemonSets are similar to <a href=/docs/concepts/workloads/controllers/deployment/>Deployments</a> in that
they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers,
storage servers).</p><p>Use a Deployment for stateless services, like frontends, where scaling up and down the
number of replicas and rolling out updates are more important than controlling exactly which host
the Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on
all or certain hosts, if the DaemonSet provides node-level functionality that allows other Pods to run correctly on that particular node.</p><p>For example, <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>network plugins</a> often include a component that runs as a DaemonSet. The DaemonSet component makes sure that the node where it's running has working cluster networking.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.<ul><li>Learn about <a href=#static-pods>static Pods</a>, which are useful for running Kubernetes
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> components.</li></ul></li><li>Find out how to use DaemonSets<ul><li><a href=/docs/tasks/manage-daemon/update-daemon-set/>Perform a rolling update on a DaemonSet</a></li><li><a href=/docs/tasks/manage-daemon/rollback-daemon-set/>Perform a rollback on a DaemonSet</a>
(for example, if a roll out didn't work how you expected).</li></ul></li><li>Understand <a href=/docs/concepts/scheduling-eviction/assign-pod-node/>how Kubernetes assigns Pods to Nodes</a>.</li><li>Learn about <a href=/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>device plugins</a> and
<a href=/docs/concepts/cluster-administration/addons/>add ons</a>, which often run as DaemonSets.</li><li><code>DaemonSet</code> is a top-level resource in the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/daemon-set-v1/>DaemonSet</a>
object definition to understand the API for daemon sets.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-cc7cc3c4907039d9f863162e20bfbbef>5 - Jobs</h1><p>A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate.
As pods successfully complete, the Job tracks the successful completions. When a specified number
of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up
the Pods it created. Suspending a Job will delete its active Pods until the Job
is resumed again.</p><p>A simple case is to create one Job object in order to reliably run one Pod to completion.
The Job object will start a new Pod if the first Pod fails or is deleted (for example
due to a node hardware failure or a node reboot).</p><p>You can also use a Job to run multiple Pods in parallel.</p><p>If you want to run a Job (either a single task, or several in parallel) on a schedule,
see <a href=/docs/concepts/workloads/controllers/cron-jobs/>CronJob</a>.</p><h2 id=running-an-example-job>Running an example Job</h2><p>Here is an example Job config. It computes π to 2000 places and prints it out.
It takes around 10s to complete.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/job.yaml download=controllers/job.yaml><code>controllers/job.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-job-yaml")' title="Copy controllers/job.yaml to clipboard"></img></div><div class=includecode id=controllers-job-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl:5.34.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>4</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>You can run the example with this command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>job.batch/pi created
</code></pre><p>Check on the status of the Job with <code>kubectl</code>:</p><ul class="nav nav-tabs" id=check-status-of-job role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#check-status-of-job-0 role=tab aria-controls=check-status-of-job-0 aria-selected=true>kubectl describe job pi</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#check-status-of-job-1 role=tab aria-controls=check-status-of-job-1>kubectl get job pi -o yaml</a></li></ul><div class=tab-content id=check-status-of-job><div id=check-status-of-job-0 class="tab-pane show active" role=tabpanel aria-labelledby=check-status-of-job-0><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>Name:           pi
</span></span><span style=display:flex><span>Namespace:      default
</span></span><span style=display:flex><span>Selector:       controller-uid<span style=color:#666>=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span style=display:flex><span>Labels:         controller-uid<span style=color:#666>=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span style=display:flex><span>                job-name<span style=color:#666>=</span>pi
</span></span><span style=display:flex><span>Annotations:    kubectl.kubernetes.io/last-applied-configuration:
</span></span><span style=display:flex><span>                  <span style=color:#666>{</span><span style=color:#b44>&#34;apiVersion&#34;</span>:<span style=color:#b44>&#34;batch/v1&#34;</span>,<span style=color:#b44>&#34;kind&#34;</span>:<span style=color:#b44>&#34;Job&#34;</span>,<span style=color:#b44>&#34;metadata&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;annotations&#34;</span>:<span style=color:#666>{}</span>,<span style=color:#b44>&#34;name&#34;</span>:<span style=color:#b44>&#34;pi&#34;</span>,<span style=color:#b44>&#34;namespace&#34;</span>:<span style=color:#b44>&#34;default&#34;</span><span style=color:#666>}</span>,<span style=color:#b44>&#34;spec&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;backoffLimit&#34;</span>:4,<span style=color:#b44>&#34;template&#34;</span>:...
</span></span><span style=display:flex><span>Parallelism:    <span style=color:#666>1</span>
</span></span><span style=display:flex><span>Completions:    <span style=color:#666>1</span>
</span></span><span style=display:flex><span>Start Time:     Mon, <span style=color:#666>02</span> Dec <span style=color:#666>2019</span> 15:20:11 +0200
</span></span><span style=display:flex><span>Completed At:   Mon, <span style=color:#666>02</span> Dec <span style=color:#666>2019</span> 15:21:16 +0200
</span></span><span style=display:flex><span>Duration:       65s
</span></span><span style=display:flex><span>Pods Statuses:  <span style=color:#666>0</span> Running / <span style=color:#666>1</span> Succeeded / <span style=color:#666>0</span> Failed
</span></span><span style=display:flex><span>Pod Template:
</span></span><span style=display:flex><span>  Labels:  controller-uid<span style=color:#666>=</span>c9948307-e56d-4b5d-8302-ae2d7b7da67c
</span></span><span style=display:flex><span>           job-name<span style=color:#666>=</span>pi
</span></span><span style=display:flex><span>  Containers:
</span></span><span style=display:flex><span>   pi:
</span></span><span style=display:flex><span>    Image:      perl:5.34.0
</span></span><span style=display:flex><span>    Port:       &lt;none&gt;
</span></span><span style=display:flex><span>    Host Port:  &lt;none&gt;
</span></span><span style=display:flex><span>    Command:
</span></span><span style=display:flex><span>      perl
</span></span><span style=display:flex><span>      -Mbignum<span style=color:#666>=</span>bpi
</span></span><span style=display:flex><span>      -wle
</span></span><span style=display:flex><span>      print bpi<span style=color:#666>(</span>2000<span style=color:#666>)</span>
</span></span><span style=display:flex><span>    Environment:  &lt;none&gt;
</span></span><span style=display:flex><span>    Mounts:       &lt;none&gt;
</span></span><span style=display:flex><span>  Volumes:        &lt;none&gt;
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type    Reason            Age   From            Message
</span></span><span style=display:flex><span>  ----    ------            ----  ----            -------
</span></span><span style=display:flex><span>  Normal  SuccessfulCreate  14m   job-controller  Created pod: pi-5rwd7
</span></span></code></pre></div></div><div id=check-status-of-job-1 class=tab-pane role=tabpanel aria-labelledby=check-status-of-job-1><p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>
</span></span><span style=display:flex><span>apiVersion: batch/v1
</span></span><span style=display:flex><span>kind: Job
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  annotations:
</span></span><span style=display:flex><span>    kubectl.kubernetes.io/last-applied-configuration: |
</span></span><span style=display:flex><span>      <span style=color:#666>{</span><span style=color:#b44>&#34;apiVersion&#34;</span>:<span style=color:#b44>&#34;batch/v1&#34;</span>,<span style=color:#b44>&#34;kind&#34;</span>:<span style=color:#b44>&#34;Job&#34;</span>,<span style=color:#b44>&#34;metadata&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;annotations&#34;</span>:<span style=color:#666>{}</span>,<span style=color:#b44>&#34;name&#34;</span>:<span style=color:#b44>&#34;pi&#34;</span>,<span style=color:#b44>&#34;namespace&#34;</span>:<span style=color:#b44>&#34;default&#34;</span><span style=color:#666>}</span>,<span style=color:#b44>&#34;spec&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;backoffLimit&#34;</span>:4,<span style=color:#b44>&#34;template&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;spec&#34;</span>:<span style=color:#666>{</span><span style=color:#b44>&#34;containers&#34;</span>:<span style=color:#666>[{</span><span style=color:#b44>&#34;command&#34;</span>:<span style=color:#666>[</span><span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#b44>&#34;print bpi(2000)&#34;</span><span style=color:#666>]</span>,<span style=color:#b44>&#34;image&#34;</span>:<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#b44>&#34;name&#34;</span>:<span style=color:#b44>&#34;pi&#34;</span><span style=color:#666>}]</span>,<span style=color:#b44>&#34;restartPolicy&#34;</span>:<span style=color:#b44>&#34;Never&#34;</span><span style=color:#666>}}}}</span>
</span></span><span style=display:flex><span>  creationTimestamp: <span style=color:#b44>&#34;2022-06-15T08:40:15Z&#34;</span>
</span></span><span style=display:flex><span>  generation: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style=display:flex><span>    job-name: pi
</span></span><span style=display:flex><span>  name: pi
</span></span><span style=display:flex><span>  namespace: default
</span></span><span style=display:flex><span>  resourceVersion: <span style=color:#b44>&#34;987&#34;</span>
</span></span><span style=display:flex><span>  uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  backoffLimit: <span style=color:#666>4</span>
</span></span><span style=display:flex><span>  completionMode: NonIndexed
</span></span><span style=display:flex><span>  completions: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  parallelism: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style=display:flex><span>  suspend: <span style=color:#a2f>false</span>
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      creationTimestamp: null
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        controller-uid: 863452e6-270d-420e-9b94-53a54146c223
</span></span><span style=display:flex><span>        job-name: pi
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - command:
</span></span><span style=display:flex><span>        - perl
</span></span><span style=display:flex><span>        - -Mbignum<span style=color:#666>=</span>bpi
</span></span><span style=display:flex><span>        - -wle
</span></span><span style=display:flex><span>        - print bpi<span style=color:#666>(</span>2000<span style=color:#666>)</span>
</span></span><span style=display:flex><span>        image: perl:5.34.0
</span></span><span style=display:flex><span>        imagePullPolicy: Always
</span></span><span style=display:flex><span>        name: pi
</span></span><span style=display:flex><span>        resources: <span style=color:#666>{}</span>
</span></span><span style=display:flex><span>        terminationMessagePath: /dev/termination-log
</span></span><span style=display:flex><span>        terminationMessagePolicy: File
</span></span><span style=display:flex><span>      dnsPolicy: ClusterFirst
</span></span><span style=display:flex><span>      restartPolicy: Never
</span></span><span style=display:flex><span>      schedulerName: default-scheduler
</span></span><span style=display:flex><span>      securityContext: <span style=color:#666>{}</span>
</span></span><span style=display:flex><span>      terminationGracePeriodSeconds: <span style=color:#666>30</span>
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  active: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  ready: <span style=color:#666>1</span>
</span></span><span style=display:flex><span>  startTime: <span style=color:#b44>&#34;2022-06-15T08:40:15Z&#34;</span>
</span></span></code></pre></div></div></div><p>To view completed Pods of a Job, use <code>kubectl get pods</code>.</p><p>To list all the Pods that belong to a Job in a machine readable form, you can use a command like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>pods</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl get pods --selector<span style=color:#666>=</span>job-name<span style=color:#666>=</span>pi --output<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.items[*].metadata.name}&#39;</span><span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>pi-5rwd7
</code></pre><p>Here, the selector is the same as the selector for the Job. The <code>--output=jsonpath</code> option specifies an expression
with the name from each Pod in the returned list.</p><p>View the standard output of one of the pods:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl logs <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
</code></pre><h2 id=writing-a-job-spec>Writing a Job spec</h2><p>As with all other Kubernetes config, a Job needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.
Its name must be a valid <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.</p><p>A Job also needs a <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> is the only required field of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>pod template</a>. It has exactly the same schema as a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a pod template in a Job must specify appropriate
labels (see <a href=#pod-selector>pod selector</a>) and an appropriate restart policy.</p><p>Only a <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>RestartPolicy</code></a> equal to <code>Never</code> or <code>OnFailure</code> is allowed.</p><h3 id=pod-selector>Pod selector</h3><p>The <code>.spec.selector</code> field is optional. In almost all cases you should not specify it.
See section <a href=#specifying-your-own-pod-selector>specifying your own pod selector</a>.</p><h3 id=parallel-jobs>Parallel execution for Jobs</h3><p>There are three main types of task suitable to run as a Job:</p><ol><li>Non-parallel Jobs<ul><li>normally, only one Pod is started, unless the Pod fails.</li><li>the Job is complete as soon as its Pod terminates successfully.</li></ul></li><li>Parallel Jobs with a <em>fixed completion count</em>:<ul><li>specify a non-zero positive value for <code>.spec.completions</code>.</li><li>the Job represents the overall task, and is complete when there are <code>.spec.completions</code> successful Pods.</li><li>when using <code>.spec.completionMode="Indexed"</code>, each Pod gets a different index in the range 0 to <code>.spec.completions-1</code>.</li></ul></li><li>Parallel Jobs with a <em>work queue</em>:<ul><li>do not specify <code>.spec.completions</code>, default to <code>.spec.parallelism</code>.</li><li>the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue.</li><li>each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done.</li><li>when <em>any</em> Pod from the Job terminates with success, no new Pods are created.</li><li>once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success.</li><li>once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They should all be in the process of exiting.</li></ul></li></ol><p>For a <em>non-parallel</em> Job, you can leave both <code>.spec.completions</code> and <code>.spec.parallelism</code> unset. When both are
unset, both are defaulted to 1.</p><p>For a <em>fixed completion count</em> Job, you should set <code>.spec.completions</code> to the number of completions needed.
You can set <code>.spec.parallelism</code>, or leave it unset and it will default to 1.</p><p>For a <em>work queue</em> Job, you must leave <code>.spec.completions</code> unset, and set <code>.spec.parallelism</code> to
a non-negative integer.</p><p>For more information about how to make use of the different types of job, see the <a href=#job-patterns>job patterns</a> section.</p><h4 id=controlling-parallelism>Controlling parallelism</h4><p>The requested parallelism (<code>.spec.parallelism</code>) can be set to any non-negative value.
If it is unspecified, it defaults to 1.
If it is specified as 0, then the Job is effectively paused until it is increased.</p><p>Actual parallelism (number of pods running at any instant) may be more or less than requested
parallelism, for a variety of reasons:</p><ul><li>For <em>fixed completion count</em> Jobs, the actual number of pods running in parallel will not exceed the number of
remaining completions. Higher values of <code>.spec.parallelism</code> are effectively ignored.</li><li>For <em>work queue</em> Jobs, no new Pods are started after any Pod has succeeded -- remaining Pods are allowed to complete, however.</li><li>If the Job <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a> has not had time to react.</li><li>If the Job controller failed to create Pods for any reason (lack of <code>ResourceQuota</code>, lack of permission, etc.),
then there may be fewer pods than requested.</li><li>The Job controller may throttle new Pod creation due to excessive previous pod failures in the same Job.</li><li>When a Pod is gracefully shut down, it takes time to stop.</li></ul><h3 id=completion-mode>Completion mode</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>Jobs with <em>fixed completion count</em> - that is, jobs that have non null
<code>.spec.completions</code> - can have a completion mode that is specified in <code>.spec.completionMode</code>:</p><ul><li><p><code>NonIndexed</code> (default): the Job is considered complete when there have been
<code>.spec.completions</code> successfully completed Pods. In other words, each Pod
completion is homologous to each other. Note that Jobs that have null
<code>.spec.completions</code> are implicitly <code>NonIndexed</code>.</p></li><li><p><code>Indexed</code>: the Pods of a Job get an associated completion index from 0 to
<code>.spec.completions-1</code>. The index is available through three mechanisms:</p><ul><li>The Pod annotation <code>batch.kubernetes.io/job-completion-index</code>.</li><li>As part of the Pod hostname, following the pattern <code>$(job-name)-$(index)</code>.
When you use an Indexed Job in combination with a
<a class=glossary-tooltip title='A way to expose an application running on a set of Pods as a network service.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/service/ target=_blank aria-label=Service>Service</a>, Pods within the Job can use
the deterministic hostnames to address each other via DNS. For more information about
how to configure this, see <a href=/docs/tasks/job/job-with-pod-to-pod-communication/>Job with Pod-to-Pod Communication</a>.</li><li>From the containerized task, in the environment variable <code>JOB_COMPLETION_INDEX</code>.</li></ul><p>The Job is considered complete when there is one successfully completed Pod
for each index. For more information about how to use this mode, see
<a href=/docs/tasks/job/indexed-parallel-processing-static/>Indexed Job for Parallel Processing with Static Work Assignment</a>.
Note that, although rare, more than one Pod could be started for the same
index, but only one of them will count towards the completion count.</p></li></ul><h2 id=handling-pod-and-container-failures>Handling Pod and container failures</h2><p>A container in a Pod may fail for a number of reasons, such as because the process in it exited with
a non-zero exit code, or the container was killed for exceeding a memory limit, etc. If this
happens, and the <code>.spec.template.spec.restartPolicy = "OnFailure"</code>, then the Pod stays
on the node, but the container is re-run. Therefore, your program needs to handle the case when it is
restarted locally, or else specify <code>.spec.template.spec.restartPolicy = "Never"</code>.
See <a href=/docs/concepts/workloads/pods/pod-lifecycle/#example-states>pod lifecycle</a> for more information on <code>restartPolicy</code>.</p><p>An entire Pod can also fail, for a number of reasons, such as when the pod is kicked off the node
(node is upgraded, rebooted, deleted, etc.), or if a container of the Pod fails and the
<code>.spec.template.spec.restartPolicy = "Never"</code>. When a Pod fails, then the Job controller
starts a new Pod. This means that your application needs to handle the case when it is restarted in a new
pod. In particular, it needs to handle temporary files, locks, incomplete output and the like
caused by previous runs.</p><p>Note that even if you specify <code>.spec.parallelism = 1</code> and <code>.spec.completions = 1</code> and
<code>.spec.template.spec.restartPolicy = "Never"</code>, the same program may
sometimes be started twice.</p><p>If you do specify <code>.spec.parallelism</code> and <code>.spec.completions</code> both greater than 1, then there may be
multiple pods running at once. Therefore, your pods must also be tolerant of concurrency.</p><h3 id=pod-backoff-failure-policy>Pod backoff failure policy</h3><p>There are situations where you want to fail a Job after some amount of retries
due to a logical error in configuration etc.
To do so, set <code>.spec.backoffLimit</code> to specify the number of retries before
considering a Job as failed. The back-off limit is set by default to 6. Failed
Pods associated with the Job are recreated by the Job controller with an
exponential back-off delay (10s, 20s, 40s ...) capped at six minutes.</p><p>The number of retries is calculated in two ways:</p><ul><li>The number of Pods with <code>.status.phase = "Failed"</code>.</li><li>When using <code>restartPolicy = "OnFailure"</code>, the number of retries in all the
containers of Pods with <code>.status.phase</code> equal to <code>Pending</code> or <code>Running</code>.</li></ul><p>If either of the calculations reaches the <code>.spec.backoffLimit</code>, the Job is
considered failed.</p><p>When the <a href=#job-tracking-with-finalizers><code>JobTrackingWithFinalizers</code></a> feature is
disabled, the number of failed Pods is only based on Pods that are still present
in the API.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If your job has <code>restartPolicy = "OnFailure"</code>, keep in mind that your Pod running the Job
will be terminated once the job backoff limit has been reached. This can make debugging the Job's executable more difficult. We suggest setting
<code>restartPolicy = "Never"</code> when debugging the Job or using a logging system to ensure output
from failed Jobs is not lost inadvertently.</div><h2 id=job-termination-and-cleanup>Job termination and cleanup</h2><p>When a Job completes, no more Pods are created, but the Pods are <a href=#pod-backoff-failure-policy>usually</a> not deleted either.
Keeping them around
allows you to still view the logs of completed pods to check for errors, warnings, or other diagnostic output.
The job object also remains after it is completed so that you can view its status. It is up to the user to delete
old jobs after noting their status. Delete the job with <code>kubectl</code> (e.g. <code>kubectl delete jobs/pi</code> or <code>kubectl delete -f ./job.yaml</code>). When you delete the job using <code>kubectl</code>, all the pods it created are deleted too.</p><p>By default, a Job will run uninterrupted unless a Pod fails (<code>restartPolicy=Never</code>) or a Container exits in error (<code>restartPolicy=OnFailure</code>), at which point the Job defers to the
<code>.spec.backoffLimit</code> described above. Once <code>.spec.backoffLimit</code> has been reached the Job will be marked as failed and any running Pods will be terminated.</p><p>Another way to terminate a Job is by setting an active deadline.
Do this by setting the <code>.spec.activeDeadlineSeconds</code> field of the Job to a number of seconds.
The <code>activeDeadlineSeconds</code> applies to the duration of the job, no matter how many Pods are created.
Once a Job reaches <code>activeDeadlineSeconds</code>, all of its running Pods are terminated and the Job status will become <code>type: Failed</code> with <code>reason: DeadlineExceeded</code>.</p><p>Note that a Job's <code>.spec.activeDeadlineSeconds</code> takes precedence over its <code>.spec.backoffLimit</code>. Therefore, a Job that is retrying one or more failed Pods will not deploy additional Pods once it reaches the time limit specified by <code>activeDeadlineSeconds</code>, even if the <code>backoffLimit</code> is not yet reached.</p><p>Example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi-with-timeout<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>activeDeadlineSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl:5.34.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><p>Note that both the Job spec and the <a href=/docs/concepts/workloads/pods/init-containers/#detailed-behavior>Pod template spec</a> within the Job have an <code>activeDeadlineSeconds</code> field. Ensure that you set this field at the proper level.</p><p>Keep in mind that the <code>restartPolicy</code> applies to the Pod, and not to the Job itself: there is no automatic Job restart once the Job status is <code>type: Failed</code>.
That is, the Job termination mechanisms activated with <code>.spec.activeDeadlineSeconds</code> and <code>.spec.backoffLimit</code> result in a permanent Job failure that requires manual intervention to resolve.</p><h2 id=clean-up-finished-jobs-automatically>Clean up finished jobs automatically</h2><p>Finished Jobs are usually no longer needed in the system. Keeping them around in
the system will put pressure on the API server. If the Jobs are managed directly
by a higher level controller, such as
<a href=/docs/concepts/workloads/controllers/cron-jobs/>CronJobs</a>, the Jobs can be
cleaned up by CronJobs based on the specified capacity-based cleanup policy.</p><h3 id=ttl-mechanism-for-finished-jobs>TTL mechanism for finished Jobs</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code></div><p>Another way to clean up finished Jobs (either <code>Complete</code> or <code>Failed</code>)
automatically is to use a TTL mechanism provided by a
<a href=/docs/concepts/workloads/controllers/ttlafterfinished/>TTL controller</a> for
finished resources, by specifying the <code>.spec.ttlSecondsAfterFinished</code> field of
the Job.</p><p>When the TTL controller cleans up the Job, it will delete the Job cascadingly,
i.e. delete its dependent objects, such as Pods, together with the Job. Note
that when the Job is deleted, its lifecycle guarantees, such as finalizers, will
be honored.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi-with-ttl<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>ttlSecondsAfterFinished</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>perl:5.34.0<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;perl&#34;</span>,<span style=color:#bbb>  </span><span style=color:#b44>&#34;-Mbignum=bpi&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;-wle&#34;</span>,<span style=color:#bbb> </span><span style=color:#b44>&#34;print bpi(2000)&#34;</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span></code></pre></div><p>The Job <code>pi-with-ttl</code> will be eligible to be automatically deleted, <code>100</code>
seconds after it finishes.</p><p>If the field is set to <code>0</code>, the Job will be eligible to be automatically deleted
immediately after it finishes. If the field is unset, this Job won't be cleaned
up by the TTL controller after it finishes.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>It is recommended to set <code>ttlSecondsAfterFinished</code> field because unmanaged jobs
(Jobs that you created directly, and not indirectly through other workload APIs
such as CronJob) have a default deletion
policy of <code>orphanDependents</code> causing Pods created by an unmanaged Job to be left around
after that Job is fully deleted.
Even though the <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a> eventually
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection>garbage collects</a>
the Pods from a deleted Job after they either fail or complete, sometimes those
lingering pods may cause cluster performance degradation or in worst case cause the
cluster to go offline due to this degradation.</p><p>You can use <a href=/docs/concepts/policy/limit-range/>LimitRanges</a> and
<a href=/docs/concepts/policy/resource-quotas/>ResourceQuotas</a> to place a
cap on the amount of resources that a particular namespace can
consume.</p></div><h2 id=job-patterns>Job patterns</h2><p>The Job object can be used to support reliable parallel execution of Pods. The Job object is not
designed to support closely-communicating parallel processes, as commonly found in scientific
computing. It does support parallel processing of a set of independent but related <em>work items</em>.
These might be emails to be sent, frames to be rendered, files to be transcoded, ranges of keys in a
NoSQL database to scan, and so on.</p><p>In a complex system, there may be multiple different sets of work items. Here we are just
considering one set of work items that the user wants to manage together — a <em>batch job</em>.</p><p>There are several different patterns for parallel computation, each with strengths and weaknesses.
The tradeoffs are:</p><ul><li>One Job object for each work item, vs. a single Job object for all work items. The latter is
better for large numbers of work items. The former creates some overhead for the user and for the
system to manage large numbers of Job objects.</li><li>Number of pods created equals number of work items, vs. each Pod can process multiple work items.
The former typically requires less modification to existing code and containers. The latter
is better for large numbers of work items, for similar reasons to the previous bullet.</li><li>Several approaches use a work queue. This requires running a queue service,
and modifications to the existing program or container to make it use the work queue.
Other approaches are easier to adapt to an existing containerised application.</li></ul><p>The tradeoffs are summarized here, with columns 2 to 4 corresponding to the above tradeoffs.
The pattern names are also links to examples and more detailed description.</p><table><thead><tr><th>Pattern</th><th style=text-align:center>Single Job object</th><th style=text-align:center>Fewer pods than work items?</th><th style=text-align:center>Use app unmodified?</th></tr></thead><tbody><tr><td><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Queue with Pod Per Work Item</a></td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>sometimes</td></tr><tr><td><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Queue with Variable Pod Count</a></td><td style=text-align:center>✓</td><td style=text-align:center>✓</td><td style=text-align:center></td></tr><tr><td><a href=/docs/tasks/job/indexed-parallel-processing-static/>Indexed Job with Static Work Assignment</a></td><td style=text-align:center>✓</td><td style=text-align:center></td><td style=text-align:center>✓</td></tr><tr><td><a href=/docs/tasks/job/parallel-processing-expansion/>Job Template Expansion</a></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>✓</td></tr><tr><td><a href=/docs/tasks/job/job-with-pod-to-pod-communication/>Job with Pod-to-Pod Communication</a></td><td style=text-align:center>✓</td><td style=text-align:center>sometimes</td><td style=text-align:center>sometimes</td></tr></tbody></table><p>When you specify completions with <code>.spec.completions</code>, each Pod created by the Job controller
has an identical <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>spec</code></a>. This means that
all pods for a task will have the same command line and the same
image, the same volumes, and (almost) the same environment variables. These patterns
are different ways to arrange for pods to work on different things.</p><p>This table shows the required settings for <code>.spec.parallelism</code> and <code>.spec.completions</code> for each of the patterns.
Here, <code>W</code> is the number of work items.</p><table><thead><tr><th>Pattern</th><th style=text-align:center><code>.spec.completions</code></th><th style=text-align:center><code>.spec.parallelism</code></th></tr></thead><tbody><tr><td><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Queue with Pod Per Work Item</a></td><td style=text-align:center>W</td><td style=text-align:center>any</td></tr><tr><td><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Queue with Variable Pod Count</a></td><td style=text-align:center>null</td><td style=text-align:center>any</td></tr><tr><td><a href=/docs/tasks/job/indexed-parallel-processing-static/>Indexed Job with Static Work Assignment</a></td><td style=text-align:center>W</td><td style=text-align:center>any</td></tr><tr><td><a href=/docs/tasks/job/parallel-processing-expansion/>Job Template Expansion</a></td><td style=text-align:center>1</td><td style=text-align:center>should be 1</td></tr><tr><td><a href=/docs/tasks/job/job-with-pod-to-pod-communication/>Job with Pod-to-Pod Communication</a></td><td style=text-align:center>W</td><td style=text-align:center>W</td></tr></tbody></table><h2 id=advanced-usage>Advanced usage</h2><h3 id=suspending-a-job>Suspending a Job</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>When a Job is created, the Job controller will immediately begin creating Pods
to satisfy the Job's requirements and will continue to do so until the Job is
complete. However, you may want to temporarily suspend a Job's execution and
resume it later, or start Jobs in suspended state and have a custom controller
decide later when to start them.</p><p>To suspend a Job, you can update the <code>.spec.suspend</code> field of
the Job to true; later, when you want to resume it again, update it to false.
Creating a Job with <code>.spec.suspend</code> set to true will create it in the suspended
state.</p><p>When a Job is resumed from suspension, its <code>.status.startTime</code> field will be
reset to the current time. This means that the <code>.spec.activeDeadlineSeconds</code>
timer will be stopped and reset when a Job is suspended and resumed.</p><p>When you suspend a Job, any running Pods that don't have a status of <code>Completed</code> will be <a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>terminated</a>.
with a SIGTERM signal. The Pod's graceful termination period will be honored and
your Pod must handle this signal in this period. This may involve saving
progress for later or undoing changes. Pods terminated this way will not count
towards the Job's <code>completions</code> count.</p><p>An example Job definition in the suspended state can be like so:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get job myjob -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>myjob<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>suspend</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parallelism</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>completions</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>You can also toggle Job suspension by patching the Job using the command line.</p><p>Suspend an active Job:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl patch job/myjob --type<span style=color:#666>=</span>strategic --patch <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;suspend&#34;:true}}&#39;</span>
</span></span></code></pre></div><p>Resume a suspended Job:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl patch job/myjob --type<span style=color:#666>=</span>strategic --patch <span style=color:#b44>&#39;{&#34;spec&#34;:{&#34;suspend&#34;:false}}&#39;</span>
</span></span></code></pre></div><p>The Job's status can be used to determine if a Job is suspended or has been
suspended in the past:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get jobs/myjob -o yaml
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># .metadata and .spec omitted</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>conditions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>lastProbeTime</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2021-02-05T13:14:33Z&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>lastTransitionTime</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2021-02-05T13:14:33Z&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>status</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;True&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>Suspended<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>startTime</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2021-02-05T13:13:48Z&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The Job condition of type "Suspended" with status "True" means the Job is
suspended; the <code>lastTransitionTime</code> field can be used to determine how long the
Job has been suspended for. If the status of that condition is "False", then the
Job was previously suspended and is now running. If such a condition does not
exist in the Job's status, the Job has never been stopped.</p><p>Events are also created when the Job is suspended and resumed:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe jobs/myjob
</span></span></code></pre></div><pre tabindex=0><code>Name:           myjob
...
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
  Normal  Suspended         11m   job-controller  Job suspended
  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
  Normal  Resumed           3s    job-controller  Job resumed
</code></pre><p>The last four events, particularly the "Suspended" and "Resumed" events, are
directly a result of toggling the <code>.spec.suspend</code> field. In the time between
these two events, we see that no Pods were created, but Pod creation restarted
as soon as the Job was resumed.</p><h3 id=mutable-scheduling-directives>Mutable Scheduling Directives</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In order to use this behavior, you must enable the <code>JobMutableNodeSchedulingDirectives</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>API server</a>.
It is enabled by default.</div><p>In most cases a parallel job will want the pods to run with constraints,
like all in the same zone, or all either on GPU model x or y but not a mix of both.</p><p>The <a href=#suspending-a-job>suspend</a> field is the first step towards achieving those semantics. Suspend allows a
custom queue controller to decide when a job should start; However, once a job is unsuspended,
a custom queue controller has no influence on where the pods of a job will actually land.</p><p>This feature allows updating a Job's scheduling directives before it starts, which gives custom queue
controllers the ability to influence pod placement while at the same time offloading actual
pod-to-node assignment to kube-scheduler. This is allowed only for suspended Jobs that have never
been unsuspended before.</p><p>The fields in a Job's pod template that can be updated are node affinity, node selector,
tolerations, labels and annotations.</p><h3 id=specifying-your-own-pod-selector>Specifying your own Pod selector</h3><p>Normally, when you create a Job object, you do not specify <code>.spec.selector</code>.
The system defaulting logic adds this field when the Job is created.
It picks a selector value that will not overlap with any other jobs.</p><p>However, in some cases, you might need to override this automatically set selector.
To do this, you can specify the <code>.spec.selector</code> of the Job.</p><p>Be very careful when doing this. If you specify a label selector which is not
unique to the pods of that Job, and which matches unrelated Pods, then pods of the unrelated
job may be deleted, or this Job may count other Pods as completing it, or one or both
Jobs may refuse to create Pods or run to completion. If a non-unique selector is
chosen, then other controllers (e.g. ReplicationController) and their Pods may behave
in unpredictable ways too. Kubernetes will not stop you from making a mistake when
specifying <code>.spec.selector</code>.</p><p>Here is an example of a case when you might want to use this feature.</p><p>Say Job <code>old</code> is already running. You want existing Pods
to keep running, but you want the rest of the Pods it creates
to use a different pod template and for the Job to have a new name.
You cannot update the Job because these fields are not updatable.
Therefore, you delete Job <code>old</code> but <em>leave its pods
running</em>, using <code>kubectl delete jobs/old --cascade=orphan</code>.
Before deleting it, you make a note of what selector it uses:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get job old -o yaml
</span></span></code></pre></div><p>The output is similar to this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>old<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>controller-uid</span>:<span style=color:#bbb> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>Then you create a new Job with name <code>new</code> and you explicitly specify the same selector.
Since the existing Pods have label <code>controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002</code>,
they are controlled by Job <code>new</code> as well.</p><p>You need to specify <code>manualSelector: true</code> in the new Job since you are not using
the selector that the system normally generates for you automatically.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>new<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>manualSelector</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>controller-uid</span>:<span style=color:#bbb> </span>a8f3d00d-c6d2-11e5-9f87-42010af00002<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>...<span style=color:#bbb>
</span></span></span></code></pre></div><p>The new Job itself will have a different uid from <code>a8f3d00d-c6d2-11e5-9f87-42010af00002</code>. Setting
<code>manualSelector: true</code> tells the system that you know what you are doing and to allow this
mismatch.</p><h3 id=pod-failure-policy>Pod failure policy</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [alpha]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> You can only configure a Pod failure policy for a Job if you have the
<code>JobPodFailurePolicy</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
enabled in your cluster. Additionally, it is recommended
to enable the <code>PodDisruptionConditions</code> feature gate in order to be able to detect and handle
Pod disruption conditions in the Pod failure policy (see also:
<a href=/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions>Pod disruption conditions</a>). Both feature gates are
available in Kubernetes v1.25.</div><p>A Pod failure policy, defined with the <code>.spec.podFailurePolicy</code> field, enables
your cluster to handle Pod failures based on the container exit codes and the
Pod conditions.</p><p>In some situations, you may want to have a better control when handling Pod
failures than the control provided by the <a href=#pod-backoff-failure-policy>Pod backoff failure policy</a>,
which is based on the Job's <code>.spec.backoffLimit</code>. These are some examples of use cases:</p><ul><li>To optimize costs of running workloads by avoiding unnecessary Pod restarts,
you can terminate a Job as soon as one of its Pods fails with an exit code
indicating a software bug.</li><li>To guarantee that your Job finishes even if there are disruptions, you can
ignore Pod failures caused by disruptions (such <a class=glossary-tooltip title='Preemption logic in Kubernetes helps a pending Pod to find a suitable Node by evicting low priority Pods existing on that Node.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#preemption target=_blank aria-label=preemption>preemption</a>,
<a class=glossary-tooltip title='API-initiated eviction is the process by which you use the Eviction API to create an Eviction object that triggers graceful pod termination.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/api-eviction/ target=_blank aria-label='API-initiated eviction'>API-initiated eviction</a>
or <a class=glossary-tooltip title='A core object consisting of three required properties: key, value, and effect. Taints prevent the scheduling of pods on nodes or node groups.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/taint-and-toleration/ target=_blank aria-label=taint>taint</a>-based eviction) so
that they don't count towards the <code>.spec.backoffLimit</code> limit of retries.</li></ul><p>You can configure a Pod failure policy, in the <code>.spec.podFailurePolicy</code> field,
to meet the above use cases. This policy can handle Pod failures based on the
container exit codes and the Pod conditions.</p><p>Here is a manifest for a Job that defines a <code>podFailurePolicy</code>:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples//controllers/job-pod-failure-policy-example.yaml download=/controllers/job-pod-failure-policy-example.yaml><code>/controllers/job-pod-failure-policy-example.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-job-pod-failure-policy-example-yaml")' title="Copy /controllers/job-pod-failure-policy-example.yaml to clipboard"></img></div><div class=includecode id=controllers-job-pod-failure-policy-example-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Job<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>job-pod-failure-policy-example<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>completions</span>:<span style=color:#bbb> </span><span style=color:#666>12</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>parallelism</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>main<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>docker.io/library/bash:5<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb> </span>[<span style=color:#b44>&#34;bash&#34;</span>]<span style=color:#bbb>        </span><span style=color:#080;font-style:italic># example command simulating a bug which triggers the FailJob action</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- echo &#34;Hello world!&#34; &amp;&amp; sleep 5 &amp;&amp; exit 42<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>backoffLimit</span>:<span style=color:#bbb> </span><span style=color:#666>6</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podFailurePolicy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>rules</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>action</span>:<span style=color:#bbb> </span>FailJob<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>onExitCodes</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>containerName</span>:<span style=color:#bbb> </span>main     <span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>operator: In             # one of</span>:<span style=color:#bbb> </span>In, NotIn<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb> </span>[<span style=color:#666>42</span>]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>action: Ignore             # one of</span>:<span style=color:#bbb> </span>Ignore, FailJob, Count<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>onPodConditions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>DisruptionTarget  <span style=color:#bbb> </span><span style=color:#080;font-style:italic># indicates Pod disruption</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>In the example above, the first rule of the Pod failure policy specifies that
the Job should be marked failed if the <code>main</code> container fails with the 42 exit
code. The following are the rules for the <code>main</code> container specifically:</p><ul><li>an exit code of 0 means that the container succeeded</li><li>an exit code of 42 means that the <strong>entire Job</strong> failed</li><li>any other exit code represents that the container failed, and hence the entire
Pod. The Pod will be re-created if the total number of restarts is
below <code>backoffLimit</code>. If the <code>backoffLimit</code> is reached the <strong>entire Job</strong> failed.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Because the Pod template specifies a <code>restartPolicy: Never</code>,
the kubelet does not restart the <code>main</code> container in that particular Pod.</div><p>The second rule of the Pod failure policy, specifying the <code>Ignore</code> action for
failed Pods with condition <code>DisruptionTarget</code> excludes Pod disruptions from
being counted towards the <code>.spec.backoffLimit</code> limit of retries.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If the Job failed, either by the Pod failure policy or Pod backoff
failure policy, and the Job is running multiple Pods, Kubernetes terminates all
the Pods in that Job that are still Pending or Running.</div><p>These are some requirements and semantics of the API:</p><ul><li>if you want to use a <code>.spec.podFailurePolicy</code> field for a Job, you must
also define that Job's pod template with <code>.spec.restartPolicy</code> set to <code>Never</code>.</li><li>the Pod failure policy rules you specify under <code>spec.podFailurePolicy.rules</code>
are evaluated in order. Once a rule matches a Pod failure, the remaining rules
are ignored. When no rule matches the Pod failure, the default
handling applies.</li><li>you may want to restrict a rule to a specific container by specifing its name
in<code>spec.podFailurePolicy.rules[*].containerName</code>. When not specified the rule
applies to all containers. When specified, it should match one the container
or <code>initContainer</code> names in the Pod template.</li><li>you may specify the action taken when a Pod failure policy is matched by
<code>spec.podFailurePolicy.rules[*].action</code>. Possible values are:<ul><li><code>FailJob</code>: use to indicate that the Pod's job should be marked as Failed and
all running Pods should be terminated.</li><li><code>Ignore</code>: use to indicate that the counter towards the <code>.spec.backoffLimit</code>
should not be incremented and a replacement Pod should be created.</li><li><code>Count</code>: use to indicate that the Pod should be handled in the default way.
The counter towards the <code>.spec.backoffLimit</code> should be incremented.</li></ul></li></ul><h3 id=job-tracking-with-finalizers>Job tracking with finalizers</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>In order to use this behavior, you must enable the <code>JobTrackingWithFinalizers</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
on the <a href=/docs/reference/command-line-tools-reference/kube-apiserver/>API server</a>
and the <a href=/docs/reference/command-line-tools-reference/kube-controller-manager/>controller manager</a>.</p><p>When enabled, the control plane tracks new Jobs using the behavior described
below. Jobs created before the feature was enabled are unaffected. As a user,
the only difference you would see is that the control plane tracking of Job
completion is more accurate.</p></div><p>When this feature isn't enabled, the Job <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a>
relies on counting the Pods that exist in the cluster to track the Job status,
that is, to keep the counters for <code>succeeded</code> and <code>failed</code> Pods.
However, Pods can be removed for a number of reasons, including:</p><ul><li>The garbage collector that removes orphan Pods when a Node goes down.</li><li>The garbage collector that removes finished Pods (in <code>Succeeded</code> or <code>Failed</code>
phase) after a threshold.</li><li>Human intervention to delete Pods belonging to a Job.</li><li>An external controller (not provided as part of Kubernetes) that removes or
replaces Pods.</li></ul><p>If you enable the <code>JobTrackingWithFinalizers</code> feature for your cluster, the
control plane keeps track of the Pods that belong to any Job and notices if any
such Pod is removed from the API server. To do that, the Job controller creates Pods with
the finalizer <code>batch.kubernetes.io/job-tracking</code>. The controller removes the
finalizer only after the Pod has been accounted for in the Job status, allowing
the Pod to be removed by other controllers or users.</p><p>The Job controller uses the new algorithm for new Jobs only. Jobs created
before the feature is enabled are unaffected. You can determine if the Job
controller is tracking a Job using Pod finalizers by checking if the Job has the
annotation <code>batch.kubernetes.io/job-tracking</code>. You should <strong>not</strong> manually add
or remove this annotation from Jobs.</p><h2 id=alternatives>Alternatives</h2><h3 id=bare-pods>Bare Pods</h3><p>When the node that a Pod is running on reboots or fails, the pod is terminated
and will not be restarted. However, a Job will create new Pods to replace terminated ones.
For this reason, we recommend that you use a Job rather than a bare Pod, even if your application
requires only a single Pod.</p><h3 id=replication-controller>Replication Controller</h3><p>Jobs are complementary to <a href=/docs/concepts/workloads/controllers/replicationcontroller/>Replication Controllers</a>.
A Replication Controller manages Pods which are not expected to terminate (e.g. web servers), and a Job
manages Pods that are expected to terminate (e.g. batch tasks).</p><p>As discussed in <a href=/docs/concepts/workloads/pods/pod-lifecycle/>Pod Lifecycle</a>, <code>Job</code> is <em>only</em> appropriate
for pods with <code>RestartPolicy</code> equal to <code>OnFailure</code> or <code>Never</code>.
(Note: If <code>RestartPolicy</code> is not set, the default value is <code>Always</code>.)</p><h3 id=single-job-starts-controller-pod>Single Job starts controller Pod</h3><p>Another pattern is for a single Job to create a Pod which then creates other Pods, acting as a sort
of custom controller for those Pods. This allows the most flexibility, but may be somewhat
complicated to get started with and offers less integration with Kubernetes.</p><p>One example of this pattern would be a Job which starts a Pod which runs a script that in turn
starts a Spark master controller (see <a href=https://github.com/kubernetes/examples/tree/master/staging/spark/README.md>spark example</a>), runs a spark
driver, and then cleans up.</p><p>An advantage of this approach is that the overall process gets the completion guarantee of a Job
object, but maintains complete control over what Pods are created and how work is assigned to them.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li>Read about different ways of running Jobs:<ul><li><a href=/docs/tasks/job/coarse-parallel-processing-work-queue/>Coarse Parallel Processing Using a Work Queue</a></li><li><a href=/docs/tasks/job/fine-parallel-processing-work-queue/>Fine Parallel Processing Using a Work Queue</a></li><li>Use an <a href=/docs/tasks/job/indexed-parallel-processing-static/>indexed Job for parallel processing with static work assignment</a></li><li>Create multiple Jobs based on a template: <a href=/docs/tasks/job/parallel-processing-expansion/>Parallel Processing using Expansions</a></li></ul></li><li>Follow the links within <a href=#clean-up-finished-jobs-automatically>Clean up finished jobs automatically</a>
to learn more about how your cluster can clean up completed and / or failed tasks.</li><li><code>Job</code> is part of the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/job-v1/>Job</a>
object definition to understand the API for jobs.</li><li>Read about <a href=/docs/concepts/workloads/controllers/cron-jobs/><code>CronJob</code></a>, which you
can use to define a series of Jobs that will run based on a schedule, similar to
the UNIX tool <code>cron</code>.</li><li>Practice how to configure handling of retriable and non-retriable pod failures
using <code>podFailurePolicy</code>, based on the step-by-step <a href=/docs/tasks/job/pod-failure-policy/>examples</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-4de50a37ebb6f2340484192126cb7a04>6 - Automatic Clean-up for Finished Jobs</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [stable]</code></div><p>TTL-after-finished <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> provides a
TTL (time to live) mechanism to limit the lifetime of resource objects that
have finished execution. TTL controller only handles
<a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Jobs>Jobs</a>.</p><h2 id=ttl-after-finished-controller>TTL-after-finished Controller</h2><p>The TTL-after-finished controller is only supported for Jobs. A cluster operator can use this feature to clean
up finished Jobs (either <code>Complete</code> or <code>Failed</code>) automatically by specifying the
<code>.spec.ttlSecondsAfterFinished</code> field of a Job, as in this
<a href=/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically>example</a>.
The TTL-after-finished controller will assume that a job is eligible to be cleaned up
TTL seconds after the job has finished, in other words, when the TTL has expired. When the
TTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete
its dependent objects together with it. Note that when the job is deleted,
its lifecycle guarantees, such as finalizers, will be honored.</p><p>The TTL seconds can be set at any time. Here are some examples for setting the
<code>.spec.ttlSecondsAfterFinished</code> field of a Job:</p><ul><li>Specify this field in the job manifest, so that a Job can be cleaned up
automatically some time after it finishes.</li><li>Set this field of existing, already finished jobs, to adopt this new
feature.</li><li>Use a
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>mutating admission webhook</a>
to set this field dynamically at job creation time. Cluster administrators can
use this to enforce a TTL policy for finished jobs.</li><li>Use a
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#admission-webhooks>mutating admission webhook</a>
to set this field dynamically after the job has finished, and choose
different TTL values based on job status, labels, etc.</li></ul><h2 id=caveat>Caveat</h2><h3 id=updating-ttl-seconds>Updating TTL Seconds</h3><p>Note that the TTL period, e.g. <code>.spec.ttlSecondsAfterFinished</code> field of Jobs,
can be modified after the job is created or has finished. However, once the
Job becomes eligible to be deleted (when the TTL has expired), the system won't
guarantee that the Jobs will be kept, even if an update to extend the TTL
returns a successful API response.</p><h3 id=time-skew>Time Skew</h3><p>Because TTL-after-finished controller uses timestamps stored in the Kubernetes jobs to
determine whether the TTL has expired or not, this feature is sensitive to time
skew in the cluster, which may cause TTL-after-finish controller to clean up job objects
at the wrong time.</p><p>Clocks aren't always correct, but the difference should be
very small. Please be aware of this risk when setting a non-zero TTL.</p><h2 id=what-s-next>What's next</h2><ul><li><p><a href=/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically>Clean up Jobs automatically</a></p></li><li><p><a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/592-ttl-after-finish/README.md>Design doc</a></p></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-2e4cec01c525b45eccd6010e21cc76d9>7 - CronJob</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.21 [stable]</code></div><p>A <em>CronJob</em> creates <a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Jobs>Jobs</a> on a repeating schedule.</p><p>One CronJob object is like one line of a <em>crontab</em> (cron table) file. It runs a job periodically
on a given schedule, written in <a href=https://en.wikipedia.org/wiki/Cron>Cron</a> format.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>All <strong>CronJob</strong> <code>schedule:</code> times are based on the timezone of the
<a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>.</p><p>If your control plane runs the kube-controller-manager in Pods or bare
containers, the timezone set for the kube-controller-manager container determines the timezone
that the cron job controller uses.</p></div><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong><p>The <a href=/docs/reference/kubernetes-api/workload-resources/cron-job-v1/>v1 CronJob API</a>
does not officially support setting timezone as explained above.</p><p>Setting variables such as <code>CRON_TZ</code> or <code>TZ</code> is not officially supported by the Kubernetes project.
<code>CRON_TZ</code> or <code>TZ</code> is an implementation detail of the internal library being used
for parsing and calculating the next Job creation time. Any usage of it is not
recommended in a production cluster.</p></div><p>When creating the manifest for a CronJob resource, make sure the name you provide
is a valid <a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.
The name must be no longer than 52 characters. This is because the CronJob controller will automatically
append 11 characters to the job name provided and there is a constraint that the
maximum length of a Job name is no more than 63 characters.</p><h2 id=cronjob>CronJob</h2><p>CronJobs are meant for performing regular scheduled actions such as backups,
report generation, and so on. Each of those tasks should be configured to recur
indefinitely (for example: once a day / week / month); you can define the point
in time within that interval when the job should start.</p><h3 id=example>Example</h3><p>This example CronJob manifest prints the current time and a hello message every minute:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/application/job/cronjob.yaml download=application/job/cronjob.yaml><code>application/job/cronjob.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("application-job-cronjob-yaml")' title="Copy application/job/cronjob.yaml to clipboard"></img></div><div class=includecode id=application-job-cronjob-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>batch/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>CronJob<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>schedule</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;* * * * *&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>jobTemplate</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>hello<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- /bin/sh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- -c<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- date; echo Hello from the Kubernetes cluster<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>restartPolicy</span>:<span style=color:#bbb> </span>OnFailure<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>(<a href=/docs/tasks/job/automated-tasks-with-cron-jobs/>Running Automated Tasks with a CronJob</a>
takes you through this example in more detail).</p><h3 id=cron-schedule-syntax>Cron schedule syntax</h3><pre tabindex=0><code># ┌───────────── minute (0 - 59)
# │ ┌───────────── hour (0 - 23)
# │ │ ┌───────────── day of the month (1 - 31)
# │ │ │ ┌───────────── month (1 - 12)
# │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday;
# │ │ │ │ │                                   7 is also Sunday on some systems)
# │ │ │ │ │                                   OR sun, mon, tue, wed, thu, fri, sat
# │ │ │ │ │
# * * * * *
</code></pre><table><thead><tr><th>Entry</th><th>Description</th><th>Equivalent to</th></tr></thead><tbody><tr><td>@yearly (or @annually)</td><td>Run once a year at midnight of 1 January</td><td>0 0 1 1 *</td></tr><tr><td>@monthly</td><td>Run once a month at midnight of the first day of the month</td><td>0 0 1 * *</td></tr><tr><td>@weekly</td><td>Run once a week at midnight on Sunday morning</td><td>0 0 * * 0</td></tr><tr><td>@daily (or @midnight)</td><td>Run once a day at midnight</td><td>0 0 * * *</td></tr><tr><td>@hourly</td><td>Run once an hour at the beginning of the hour</td><td>0 * * * *</td></tr></tbody></table><p>For example, the line below states that the task must be started every Friday at midnight, as well as on the 13th of each month at midnight:</p><p><code>0 0 13 * 5</code></p><p>To generate CronJob schedule expressions, you can also use web tools like <a href=https://crontab.guru/>crontab.guru</a>.</p><h2 id=time-zones>Time zones</h2><p>For CronJobs with no time zone specified, the kube-controller-manager interprets schedules relative to its local time zone.</p><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [beta]</code></div><p>If you enable the <code>CronJobTimeZone</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>,
you can specify a time zone for a CronJob (if you don't enable that feature gate, or if you are using a version of
Kubernetes that does not have experimental time zone support, all CronJobs in your cluster have an unspecified
timezone).</p><p>When you have the feature enabled, you can set <code>spec.timeZone</code> to the name of a valid <a href=https://en.wikipedia.org/wiki/List_of_tz_database_time_zones>time zone</a>. For example, setting
<code>spec.timeZone: "Etc/UTC"</code> instructs Kubernetes to interpret the schedule relative to Coordinated Universal Time.</p><p>A time zone database from the Go standard library is included in the binaries and used as a fallback in case an external database is not available on the system.</p><h2 id=cron-job-limitations>CronJob limitations</h2><p>A cron job creates a job object <em>about</em> once per execution time of its schedule. We say "about" because there
are certain circumstances where two jobs might be created, or no job might be created. We attempt to make these rare,
but do not completely prevent them. Therefore, jobs should be <em>idempotent</em>.</p><p>If <code>startingDeadlineSeconds</code> is set to a large value or left unset (the default)
and if <code>concurrencyPolicy</code> is set to <code>Allow</code>, the jobs will always run
at least once.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> If <code>startingDeadlineSeconds</code> is set to a value less than 10 seconds, the CronJob may not be scheduled. This is because the CronJob controller checks things every 10 seconds.</div><p>For every CronJob, the CronJob <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=Controller>Controller</a> checks how many schedules it missed in the duration from its last scheduled time until now. If there are more than 100 missed schedules, then it does not start the job and logs the error.</p><pre tabindex=0><code>Cannot determine if job needs to be started. Too many missed start time (&gt; 100). Set or decrease .spec.startingDeadlineSeconds or check clock skew.
</code></pre><p>It is important to note that if the <code>startingDeadlineSeconds</code> field is set (not <code>nil</code>), the controller counts how many missed jobs occurred from the value of <code>startingDeadlineSeconds</code> until now rather than from the last scheduled time until now. For example, if <code>startingDeadlineSeconds</code> is <code>200</code>, the controller counts how many missed jobs occurred in the last 200 seconds.</p><p>A CronJob is counted as missed if it has failed to be created at its scheduled time. For example, if <code>concurrencyPolicy</code> is set to <code>Forbid</code> and a CronJob was attempted to be scheduled when there was a previous schedule still running, then it would count as missed.</p><p>For example, suppose a CronJob is set to schedule a new Job every one minute beginning at <code>08:30:00</code>, and its
<code>startingDeadlineSeconds</code> field is not set. If the CronJob controller happens to
be down from <code>08:29:00</code> to <code>10:21:00</code>, the job will not start as the number of missed jobs which missed their schedule is greater than 100.</p><p>To illustrate this concept further, suppose a CronJob is set to schedule a new Job every one minute beginning at <code>08:30:00</code>, and its
<code>startingDeadlineSeconds</code> is set to 200 seconds. If the CronJob controller happens to
be down for the same period as the previous example (<code>08:29:00</code> to <code>10:21:00</code>,) the Job will still start at 10:22:00. This happens as the controller now checks how many missed schedules happened in the last 200 seconds (i.e., 3 missed schedules), rather than from the last scheduled time until now.</p><p>The CronJob is only responsible for creating Jobs that match its schedule, and
the Job in turn is responsible for the management of the Pods it represents.</p><h2 id=new-controller>Controller version</h2><p>Starting with Kubernetes v1.21 the second version of the CronJob controller
is the default implementation. To disable the default CronJob controller
and use the original CronJob controller instead, pass the <code>CronJobControllerV2</code>
<a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
flag to the <a class=glossary-tooltip title='Control Plane component that runs controller processes.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-controller-manager/ target=_blank aria-label=kube-controller-manager>kube-controller-manager</a>,
and set this flag to <code>false</code>. For example:</p><pre tabindex=0><code>--feature-gates=&#34;CronJobControllerV2=false&#34;
</code></pre><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods/>Pods</a> and
<a href=/docs/concepts/workloads/controllers/job/>Jobs</a>, two concepts
that CronJobs rely upon.</li><li>Read about the <a href=https://pkg.go.dev/github.com/robfig/cron/v3#hdr-CRON_Expression_Format>format</a>
of CronJob <code>.spec.schedule</code> fields.</li><li>For instructions on creating and working with CronJobs, and for an example
of a CronJob manifest,
see <a href=/docs/tasks/job/automated-tasks-with-cron-jobs/>Running automated tasks with CronJobs</a>.</li><li>For instructions to clean up failed or completed jobs automatically,
see <a href=/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically>Clean up Jobs automatically</a></li><li><code>CronJob</code> is part of the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/cron-job-v1/>CronJob</a>
object definition to understand the API for Kubernetes cron jobs.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-27f1331d515d95f76aa1156088b4ad91>8 - ReplicationController</h1><div class="alert alert-info note callout" role=alert><strong>Note:</strong> A <a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> that configures a <a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a> is now the recommended way to set up replication.</div><p>A <em>ReplicationController</em> ensures that a specified number of pod replicas are running at any one
time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is
always up and available.</p><h2 id=how-a-replicationcontroller-works>How a ReplicationController Works</h2><p>If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the
ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a
ReplicationController are automatically replaced if they fail, are deleted, or are terminated.
For example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.
For this reason, you should use a ReplicationController even if your application requires
only a single pod. A ReplicationController is similar to a process supervisor,
but instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods
across multiple nodes.</p><p>ReplicationController is often abbreviated to "rc" in discussion, and as a shortcut in
kubectl commands.</p><p>A simple case is to create one ReplicationController object to reliably run one instance of
a Pod indefinitely. A more complex use case is to run several identical replicas of a replicated
service, such as web servers.</p><h2 id=running-an-example-replicationcontroller>Running an example ReplicationController</h2><p>This example ReplicationController config runs three copies of the nginx web server.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/controllers/replication.yaml download=controllers/replication.yaml><code>controllers/replication.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("controllers-replication-yaml")' title="Copy controllers/replication.yaml to clipboard"></img></div><div class=includecode id=controllers-replication-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>ReplicationController<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>ports</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>containerPort</span>:<span style=color:#bbb> </span><span style=color:#666>80</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>Run the example job by downloading the example file and then running this command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f https://k8s.io/examples/controllers/replication.yaml
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>replicationcontroller/nginx created
</code></pre><p>Check on the status of the ReplicationController using this command:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe replicationcontrollers/nginx
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>Name:        nginx
Namespace:   default
Selector:    app=nginx
Labels:      app=nginx
Annotations:    &lt;none&gt;
Replicas:    3 current / 3 desired
Pods Status: 0 Running / 3 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:       app=nginx
  Containers:
   nginx:
    Image:              nginx
    Port:               80/TCP
    Environment:        &lt;none&gt;
    Mounts:             &lt;none&gt;
  Volumes:              &lt;none&gt;
Events:
  FirstSeen       LastSeen     Count    From                        SubobjectPath    Type      Reason              Message
  ---------       --------     -----    ----                        -------------    ----      ------              -------
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-qrm3m
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-3ntk0
  20s             20s          1        {replication-controller }                    Normal    SuccessfulCreate    Created pod: nginx-4ok8v
</code></pre><p>Here, three pods are created, but none is running yet, perhaps because the image is being pulled.
A little later, the same command may show:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>Pods Status:    <span style=color:#666>3</span> Running / <span style=color:#666>0</span> Waiting / <span style=color:#666>0</span> Succeeded / <span style=color:#666>0</span> Failed
</span></span></code></pre></div><p>To list all the pods that belong to the ReplicationController in a machine readable form, you can use a command like this:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:#b8860b>pods</span><span style=color:#666>=</span><span style=color:#a2f;font-weight:700>$(</span>kubectl get pods --selector<span style=color:#666>=</span><span style=color:#b8860b>app</span><span style=color:#666>=</span>nginx --output<span style=color:#666>=</span><span style=color:#b8860b>jsonpath</span><span style=color:#666>={</span>.items..metadata.name<span style=color:#666>}</span><span style=color:#a2f;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#a2f>echo</span> <span style=color:#b8860b>$pods</span>
</span></span></code></pre></div><p>The output is similar to this:</p><pre tabindex=0><code>nginx-3ntk0 nginx-4ok8v nginx-qrm3m
</code></pre><p>Here, the selector is the same as the selector for the ReplicationController (seen in the
<code>kubectl describe</code> output), and in a different form in <code>replication.yaml</code>. The <code>--output=jsonpath</code> option
specifies an expression with the name from each pod in the returned list.</p><h2 id=writing-a-replicationcontroller-spec>Writing a ReplicationController Spec</h2><p>As with all other Kubernetes config, a ReplicationController needs <code>apiVersion</code>, <code>kind</code>, and <code>metadata</code> fields.
The name of a ReplicationController object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>.
For general information about working with configuration files, see <a href=/docs/concepts/overview/working-with-objects/object-management/>object management</a>.</p><p>A ReplicationController also needs a <a href=https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status><code>.spec</code> section</a>.</p><h3 id=pod-template>Pod Template</h3><p>The <code>.spec.template</code> is the only required field of the <code>.spec</code>.</p><p>The <code>.spec.template</code> is a <a href=/docs/concepts/workloads/pods/#pod-templates>pod template</a>. It has exactly the same schema as a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a>, except it is nested and does not have an <code>apiVersion</code> or <code>kind</code>.</p><p>In addition to required fields for a Pod, a pod template in a ReplicationController must specify appropriate
labels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See <a href=#pod-selector>pod selector</a>.</p><p>Only a <a href=/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy><code>.spec.template.spec.restartPolicy</code></a> equal to <code>Always</code> is allowed, which is the default if not specified.</p><p>For local container restarts, ReplicationControllers delegate to an agent on the node,
for example the <a href=/docs/reference/command-line-tools-reference/kubelet/>Kubelet</a>.</p><h3 id=labels-on-the-replicationcontroller>Labels on the ReplicationController</h3><p>The ReplicationController can itself have labels (<code>.metadata.labels</code>). Typically, you
would set these the same as the <code>.spec.template.metadata.labels</code>; if <code>.metadata.labels</code> is not specified
then it defaults to <code>.spec.template.metadata.labels</code>. However, they are allowed to be
different, and the <code>.metadata.labels</code> do not affect the behavior of the ReplicationController.</p><h3 id=pod-selector>Pod Selector</h3><p>The <code>.spec.selector</code> field is a <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>label selector</a>. A ReplicationController
manages all the pods with labels that match the selector. It does not distinguish
between pods that it created or deleted and pods that another person or process created or
deleted. This allows the ReplicationController to be replaced without affecting the running pods.</p><p>If specified, the <code>.spec.template.metadata.labels</code> must be equal to the <code>.spec.selector</code>, or it will
be rejected by the API. If <code>.spec.selector</code> is unspecified, it will be defaulted to
<code>.spec.template.metadata.labels</code>.</p><p>Also you should not normally create any pods whose labels match this selector, either directly, with
another ReplicationController, or with another controller such as Job. If you do so, the
ReplicationController thinks that it created the other pods. Kubernetes does not stop you
from doing this.</p><p>If you do end up with multiple controllers that have overlapping selectors, you
will have to manage the deletion yourself (see <a href=#working-with-replicationcontrollers>below</a>).</p><h3 id=multiple-replicas>Multiple Replicas</h3><p>You can specify how many pods should run concurrently by setting <code>.spec.replicas</code> to the number
of pods you would like to have running concurrently. The number running at any time may be higher
or lower, such as if the replicas were just increased or decreased, or if a pod is gracefully
shutdown, and a replacement starts early.</p><p>If you do not specify <code>.spec.replicas</code>, then it defaults to 1.</p><h2 id=working-with-replicationcontrollers>Working with ReplicationControllers</h2><h3 id=deleting-a-replicationcontroller-and-its-pods>Deleting a ReplicationController and its Pods</h3><p>To delete a ReplicationController and all its pods, use <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>. Kubectl will scale the ReplicationController to zero and wait
for it to delete each pod before deleting the ReplicationController itself. If this kubectl
command is interrupted, it can be restarted.</p><p>When using the REST API or <a href=/docs/reference/using-api/client-libraries>client library</a>, you need to do the steps explicitly (scale replicas to
0, wait for pod deletions, then delete the ReplicationController).</p><h3 id=deleting-only-a-replicationcontroller>Deleting only a ReplicationController</h3><p>You can delete a ReplicationController without affecting any of its pods.</p><p>Using kubectl, specify the <code>--cascade=orphan</code> option to <a href=/docs/reference/generated/kubectl/kubectl-commands#delete><code>kubectl delete</code></a>.</p><p>When using the REST API or <a href=/docs/reference/using-api/client-libraries>client library</a>, you can delete the ReplicationController object.</p><p>Once the original is deleted, you can create a new ReplicationController to replace it. As long
as the old and new <code>.spec.selector</code> are the same, then the new one will adopt the old pods.
However, it will not make any effort to make existing pods match a new, different pod template.
To update pods to a new spec in a controlled way, use a <a href=#rolling-updates>rolling update</a>.</p><h3 id=isolating-pods-from-a-replicationcontroller>Isolating pods from a ReplicationController</h3><p>Pods may be removed from a ReplicationController's target set by changing their labels. This technique may be used to remove pods from service for debugging and data recovery. Pods that are removed in this way will be replaced automatically (assuming that the number of replicas is not also changed).</p><h2 id=common-usage-patterns>Common usage patterns</h2><h3 id=rescheduling>Rescheduling</h3><p>As mentioned above, whether you have 1 pod you want to keep running, or 1000, a ReplicationController will ensure that the specified number of pods exists, even in the event of node failure or pod termination (for example, due to an action by another control agent).</p><h3 id=scaling>Scaling</h3><p>The ReplicationController enables scaling the number of replicas up or down, either manually or by an auto-scaling control agent, by updating the <code>replicas</code> field.</p><h3 id=rolling-updates>Rolling updates</h3><p>The ReplicationController is designed to facilitate rolling updates to a service by replacing pods one-by-one.</p><p>As explained in <a href=https://issue.k8s.io/1353>#1353</a>, the recommended approach is to create a new ReplicationController with 1 replica, scale the new (+1) and old (-1) controllers one by one, and then delete the old controller after it reaches 0 replicas. This predictably updates the set of pods regardless of unexpected failures.</p><p>Ideally, the rolling update controller would take application readiness into account, and would ensure that a sufficient number of pods were productively serving at any given time.</p><p>The two ReplicationControllers would need to create pods with at least one differentiating label, such as the image tag of the primary container of the pod, since it is typically image updates that motivate rolling updates.</p><h3 id=multiple-release-tracks>Multiple release tracks</h3><p>In addition to running multiple releases of an application while a rolling update is in progress, it's common to run multiple releases for an extended period of time, or even continuously, using multiple release tracks. The tracks would be differentiated by labels.</p><p>For instance, a service might target all pods with <code>tier in (frontend), environment in (prod)</code>. Now say you have 10 replicated pods that make up this tier. But you want to be able to 'canary' a new version of this component. You could set up a ReplicationController with <code>replicas</code> set to 9 for the bulk of the replicas, with labels <code>tier=frontend, environment=prod, track=stable</code>, and another ReplicationController with <code>replicas</code> set to 1 for the canary, with labels <code>tier=frontend, environment=prod, track=canary</code>. Now the service is covering both the canary and non-canary pods. But you can mess with the ReplicationControllers separately to test things out, monitor the results, etc.</p><h3 id=using-replicationcontrollers-with-services>Using ReplicationControllers with Services</h3><p>Multiple ReplicationControllers can sit behind a single service, so that, for example, some traffic
goes to the old version, and some goes to the new version.</p><p>A ReplicationController will never terminate on its own, but it isn't expected to be as long-lived as services. Services may be composed of pods controlled by multiple ReplicationControllers, and it is expected that many ReplicationControllers may be created and destroyed over the lifetime of a service (for instance, to perform an update of pods that run the service). Both services themselves and their clients should remain oblivious to the ReplicationControllers that maintain the pods of the services.</p><h2 id=writing-programs-for-replication>Writing programs for Replication</h2><p>Pods created by a ReplicationController are intended to be fungible and semantically identical, though their configurations may become heterogeneous over time. This is an obvious fit for replicated stateless servers, but ReplicationControllers can also be used to maintain availability of master-elected, sharded, and worker-pool applications. Such applications should use dynamic work assignment mechanisms, such as the <a href=https://www.rabbitmq.com/tutorials/tutorial-two-python.html>RabbitMQ work queues</a>, as opposed to static/one-time customization of the configuration of each pod, which is considered an anti-pattern. Any pod customization performed, such as vertical auto-sizing of resources (for example, cpu or memory), should be performed by another online controller process, not unlike the ReplicationController itself.</p><h2 id=responsibilities-of-the-replicationcontroller>Responsibilities of the ReplicationController</h2><p>The ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, <a href=https://issue.k8s.io/620>readiness</a> and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.</p><p>The ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in <a href=https://issue.k8s.io/492>#492</a>), which would change its <code>replicas</code> field. We will not add scheduling policies (for example, <a href=https://issue.k8s.io/367#issuecomment-48428019>spreading</a>) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation (<a href=https://issue.k8s.io/170>#170</a>).</p><p>The ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The "macro" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like <a href=https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1>Asgard</a> managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.</p><h2 id=api-object>API Object</h2><p>Replication controller is a top-level resource in the Kubernetes REST API. More details about the
API object can be found at:
<a href=/docs/reference/generated/kubernetes-api/v1.25/#replicationcontroller-v1-core>ReplicationController API object</a>.</p><h2 id=alternatives-to-replicationcontroller>Alternatives to ReplicationController</h2><h3 id=replicaset>ReplicaSet</h3><p><a href=/docs/concepts/workloads/controllers/replicaset/><code>ReplicaSet</code></a> is the next-generation ReplicationController that supports the new <a href=/docs/concepts/overview/working-with-objects/labels/#set-based-requirement>set-based label selector</a>.
It's mainly used by <a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a> as a mechanism to orchestrate pod creation, deletion and updates.
Note that we recommend using Deployments instead of directly using Replica Sets, unless you require custom update orchestration or don't require updates at all.</p><h3 id=deployment-recommended>Deployment (Recommended)</h3><p><a href=/docs/concepts/workloads/controllers/deployment/><code>Deployment</code></a> is a higher-level API object that updates its underlying Replica Sets and their Pods. Deployments are recommended if you want the rolling update functionality, because they are declarative, server-side, and have additional features.</p><h3 id=bare-pods>Bare Pods</h3><p>Unlike in the case where a user directly created pods, a ReplicationController replaces pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, we recommend that you use a ReplicationController even if your application requires only a single pod. Think of it similarly to a process supervisor, only it supervises multiple pods across multiple nodes instead of individual processes on a single node. A ReplicationController delegates local container restarts to some agent on the node, such as the kubelet.</p><h3 id=job>Job</h3><p>Use a <a href=/docs/concepts/workloads/controllers/job/><code>Job</code></a> instead of a ReplicationController for pods that are expected to terminate on their own
(that is, batch jobs).</p><h3 id=daemonset>DaemonSet</h3><p>Use a <a href=/docs/concepts/workloads/controllers/daemonset/><code>DaemonSet</code></a> instead of a ReplicationController for pods that provide a
machine-level function, such as machine monitoring or machine logging. These pods have a lifetime that is tied
to a machine lifetime: the pod needs to be running on the machine before other pods start, and are
safe to terminate when the machine is otherwise ready to be rebooted/shutdown.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/workloads/pods>Pods</a>.</li><li>Learn about <a href=/docs/concepts/workloads/controllers/deployment/>Deployment</a>, the replacement
for ReplicationController.</li><li><code>ReplicationController</code> is part of the Kubernetes REST API.
Read the
<a href=/docs/reference/kubernetes-api/workload-resources/replication-controller-v1/>ReplicationController</a>
object definition to understand the API for replication controllers.</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>