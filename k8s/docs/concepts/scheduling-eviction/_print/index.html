<!doctype html><html lang=en class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/scheduling-eviction/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/scheduling-eviction/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/concepts/scheduling-eviction/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Scheduling, Preemption and Eviction | Kubernetes</title><meta property="og:title" content="Scheduling, Preemption and Eviction"><meta property="og:description" content="In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating  Pods with lower Priority so that Pods with higher Priority can schedule on  Nodes. Eviction is the process of proactively terminating one or more Pods on resource-starved Nodes.
"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/concepts/scheduling-eviction/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Scheduling, Preemption and Eviction"><meta itemprop=description content="In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating  Pods with lower Priority so that Pods with higher Priority can schedule on  Nodes. Eviction is the process of proactively terminating one or more Pods on resource-starved Nodes.
"><meta name=twitter:card content="summary"><meta name=twitter:title content="Scheduling, Preemption and Eviction"><meta name=twitter:description content="In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating  Pods with lower Priority so that Pods with higher Priority can schedule on  Nodes. Eviction is the process of proactively terminating one or more Pods on resource-starved Nodes.
"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating  Pods with lower Priority so that Pods with higher Priority can schedule on  Nodes. Eviction is the process of proactively terminating one or more Pods on resource-starved Nodes.
"><meta property="og:description" content="In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating  Pods with lower Priority so that Pods with higher Priority can schedule on  Nodes. Eviction is the process of proactively terminating one or more Pods on resource-starved Nodes.
"><meta name=twitter:description content="In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating  Pods with lower Priority so that Pods with higher Priority can schedule on  Nodes. Eviction is the process of proactively terminating one or more Pods on resource-starved Nodes.
"><meta property="og:url" content="https://kubernetes.io/docs/concepts/scheduling-eviction/"><meta property="og:title" content="Scheduling, Preemption and Eviction"><meta name=twitter:title content="Scheduling, Preemption and Eviction"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/concepts/scheduling-eviction/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/concepts/scheduling-eviction/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/concepts/scheduling-eviction/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/concepts/scheduling-eviction/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/concepts/scheduling-eviction/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/docs/concepts/scheduling-eviction/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/scheduling-eviction/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/scheduling-eviction/>日本語 (Japanese)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/scheduling-eviction/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/scheduling-eviction/>Bahasa Indonesia</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/concepts/scheduling-eviction/>Return to the regular view of this page</a>.</p></div><h1 class=title>Scheduling, Preemption and Eviction</h1><div class=lead>In Kubernetes, scheduling refers to making sure that Pods are matched to Nodes so that the kubelet can run them. Preemption is the process of terminating Pods with lower Priority so that Pods with higher Priority can schedule on Nodes. Eviction is the process of proactively terminating one or more Pods on resource-starved Nodes.</div><ul><li>1: <a href=#pg-598f36d691ab197f9d995784574b0a12>Kubernetes Scheduler</a></li><li>2: <a href=#pg-21169f516071aea5d16734a4c27789a5>Assigning Pods to Nodes</a></li><li>3: <a href=#pg-da22fe2278df236f71efbe672f392677>Pod Overhead</a></li><li>4: <a href=#pg-6b8c85a6a88f4a81e6b79e197c293c31>Pod Topology Spread Constraints</a></li><li>5: <a href=#pg-ede4960b56a3529ee0bfe7c8fe2d09a5>Taints and Tolerations</a></li><li>6: <a href=#pg-602208c95fe7b1f1170310ce993f5814>Scheduling Framework</a></li><li>7: <a href=#pg-d9574a30fcbc631b0d2a57850e161e89>Scheduler Performance Tuning</a></li><li>8: <a href=#pg-961126cd43559012893979e568396a49>Resource Bin Packing</a></li><li>9: <a href=#pg-60e5a2861609e0848d58ce8bf99c4a31>Pod Priority and Preemption</a></li><li>10: <a href=#pg-78e0431b4b7516092662a7c289cbb304>Node-pressure Eviction</a></li><li>11: <a href=#pg-b87723bf81b079042860f0ebd37b0a64>API-initiated Eviction</a></li></ul><div class=content><p>In Kubernetes, scheduling refers to making sure that <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>
are matched to <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a> so that the
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> can run them. Preemption
is the process of terminating Pods with lower <a class=glossary-tooltip title='Pod Priority indicates the importance of a Pod relative to other Pods.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority target=_blank aria-label=Priority>Priority</a>
so that Pods with higher Priority can schedule on Nodes. Eviction is the process
of terminating one or more Pods on Nodes.</p><h2 id=scheduling>Scheduling</h2><ul><li><a href=/docs/concepts/scheduling-eviction/kube-scheduler/>Kubernetes Scheduler</a></li><li><a href=/docs/concepts/scheduling-eviction/assign-pod-node/>Assigning Pods to Nodes</a></li><li><a href=/docs/concepts/scheduling-eviction/pod-overhead/>Pod Overhead</a></li><li><a href=/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod Topology Spread Constraints</a></li><li><a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>Taints and Tolerations</a></li><li><a href=/docs/concepts/scheduling-eviction/scheduling-framework>Scheduling Framework</a></li><li><a href=/docs/concepts/scheduling-eviction/scheduler-perf-tuning/>Scheduler Performance Tuning</a></li><li><a href=/docs/concepts/scheduling-eviction/resource-bin-packing/>Resource Bin Packing for Extended Resources</a></li></ul><h2 id=pod-disruption>Pod Disruption</h2><p><a href=/docs/concepts/workloads/pods/disruptions/>Pod disruption</a> is the process by which
Pods on Nodes are terminated either voluntarily or involuntarily.</p><p>Voluntary disruptions are started intentionally by application owners or cluster
administrators. Involuntary disruptions are unintentional and can be triggered by
unavoidable issues like Nodes running out of resources, or by accidental deletions.</p><ul><li><a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority and Preemption</a></li><li><a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a></li><li><a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated Eviction</a></li></ul></div></div><div class=td-content style=page-break-before:always><h1 id=pg-598f36d691ab197f9d995784574b0a12>1 - Kubernetes Scheduler</h1><p>In Kubernetes, <em>scheduling</em> refers to making sure that <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>
are matched to <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=Nodes>Nodes</a> so that
<a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=Kubelet>Kubelet</a> can run them.</p><h2 id=scheduling>Scheduling overview</h2><p>A scheduler watches for newly created Pods that have no Node assigned. For
every Pod that the scheduler discovers, the scheduler becomes responsible
for finding the best Node for that Pod to run on. The scheduler reaches
this placement decision taking into account the scheduling principles
described below.</p><p>If you want to understand why Pods are placed onto a particular Node,
or if you're planning to implement a custom scheduler yourself, this
page will help you learn about scheduling.</p><h2 id=kube-scheduler>kube-scheduler</h2><p><a href=/docs/reference/command-line-tools-reference/kube-scheduler/>kube-scheduler</a>
is the default scheduler for Kubernetes and runs as part of the
<a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>.
kube-scheduler is designed so that, if you want and need to, you can
write your own scheduling component and use that instead.</p><p>For every newly created pod or other unscheduled pods, kube-scheduler
selects an optimal node for them to run on. However, every container in
pods has different requirements for resources and every pod also has
different requirements. Therefore, existing nodes need to be filtered
according to the specific scheduling requirements.</p><p>In a cluster, Nodes that meet the scheduling requirements for a Pod
are called <em>feasible</em> nodes. If none of the nodes are suitable, the pod
remains unscheduled until the scheduler is able to place it.</p><p>The scheduler finds feasible Nodes for a Pod and then runs a set of
functions to score the feasible Nodes and picks a Node with the highest
score among the feasible ones to run the Pod. The scheduler then notifies
the API server about this decision in a process called <em>binding</em>.</p><p>Factors that need to be taken into account for scheduling decisions include
individual and collective resource requirements, hardware / software /
policy constraints, affinity and anti-affinity specifications, data
locality, inter-workload interference, and so on.</p><h3 id=kube-scheduler-implementation>Node selection in kube-scheduler</h3><p>kube-scheduler selects a node for the pod in a 2-step operation:</p><ol><li>Filtering</li><li>Scoring</li></ol><p>The <em>filtering</em> step finds the set of Nodes where it's feasible to
schedule the Pod. For example, the PodFitsResources filter checks whether a
candidate Node has enough available resource to meet a Pod's specific
resource requests. After this step, the node list contains any suitable
Nodes; often, there will be more than one. If the list is empty, that
Pod isn't (yet) schedulable.</p><p>In the <em>scoring</em> step, the scheduler ranks the remaining nodes to choose
the most suitable Pod placement. The scheduler assigns a score to each Node
that survived filtering, basing this score on the active scoring rules.</p><p>Finally, kube-scheduler assigns the Pod to the Node with the highest ranking.
If there is more than one node with equal scores, kube-scheduler selects
one of these at random.</p><p>There are two supported ways to configure the filtering and scoring behavior
of the scheduler:</p><ol><li><a href=/docs/reference/scheduling/policies>Scheduling Policies</a> allow you to configure <em>Predicates</em> for filtering and <em>Priorities</em> for scoring.</li><li><a href=/docs/reference/scheduling/config/#profiles>Scheduling Profiles</a> allow you to configure Plugins that implement different scheduling stages, including: <code>QueueSort</code>, <code>Filter</code>, <code>Score</code>, <code>Bind</code>, <code>Reserve</code>, <code>Permit</code>, and others. You can also configure the kube-scheduler to run different profiles.</li></ol><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/scheduling-eviction/scheduler-perf-tuning/>scheduler performance tuning</a></li><li>Read about <a href=/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod topology spread constraints</a></li><li>Read the <a href=/docs/reference/command-line-tools-reference/kube-scheduler/>reference documentation</a> for kube-scheduler</li><li>Read the <a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler config (v1beta3)</a> reference</li><li>Learn about <a href=/docs/tasks/extend-kubernetes/configure-multiple-schedulers/>configuring multiple schedulers</a></li><li>Learn about <a href=/docs/tasks/administer-cluster/topology-manager/>topology management policies</a></li><li>Learn about <a href=/docs/concepts/scheduling-eviction/pod-overhead/>Pod Overhead</a></li><li>Learn about scheduling of Pods that use volumes in:<ul><li><a href=/docs/concepts/storage/storage-classes/#volume-binding-mode>Volume Topology Support</a></li><li><a href=/docs/concepts/storage/storage-capacity/>Storage Capacity Tracking</a></li><li><a href=/docs/concepts/storage/storage-limits/>Node-specific Volume Limits</a></li></ul></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-21169f516071aea5d16734a4c27789a5>2 - Assigning Pods to Nodes</h1><p>You can constrain a <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> so that it is
<em>restricted</em> to run on particular <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node(s)>node(s)</a>,
or to <em>prefer</em> to run on particular nodes.
There are several ways to do this and the recommended approaches all use
<a href=/docs/concepts/overview/working-with-objects/labels/>label selectors</a> to facilitate the selection.
Often, you do not need to set any such constraints; the
<a class=glossary-tooltip title='Control plane component that watches for newly created pods with no assigned node, and selects a node for them to run on.' data-toggle=tooltip data-placement=top href=/docs/reference/command-line-tools-reference/kube-scheduler/ target=_blank aria-label=scheduler>scheduler</a> will automatically do a reasonable placement
(for example, spreading your Pods across nodes so as not place Pods on a node with insufficient free resources).
However, there are some circumstances where you may want to control which node
the Pod deploys to, for example, to ensure that a Pod ends up on a node with an SSD attached to it,
or to co-locate Pods from two different services that communicate a lot into the same availability zone.</p><p>You can use any of the following methods to choose where Kubernetes schedules
specific Pods:</p><ul><li><a href=#nodeselector>nodeSelector</a> field matching against <a href=#built-in-node-labels>node labels</a></li><li><a href=#affinity-and-anti-affinity>Affinity and anti-affinity</a></li><li><a href=#nodename>nodeName</a> field</li><li><a href=#pod-topology-spread-constraints>Pod topology spread constraints</a></li></ul><h2 id=built-in-node-labels>Node labels</h2><p>Like many other Kubernetes objects, nodes have
<a href=/docs/concepts/overview/working-with-objects/labels/>labels</a>. You can <a href=/docs/tasks/configure-pod-container/assign-pods-nodes/#add-a-label-to-a-node>attach labels manually</a>.
Kubernetes also populates a standard set of labels on all nodes in a cluster. See <a href=/docs/reference/labels-annotations-taints/>Well-Known Labels, Annotations and Taints</a>
for a list of common node labels.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The value of these labels is cloud provider specific and is not guaranteed to be reliable.
For example, the value of <code>kubernetes.io/hostname</code> may be the same as the node name in some environments
and a different value in other environments.</div><h3 id=node-isolation-restriction>Node isolation/restriction</h3><p>Adding labels to nodes allows you to target Pods for scheduling on specific
nodes or groups of nodes. You can use this functionality to ensure that specific
Pods only run on nodes with certain isolation, security, or regulatory
properties.</p><p>If you use labels for node isolation, choose label keys that the <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a>
cannot modify. This prevents a compromised node from setting those labels on
itself so that the scheduler schedules workloads onto the compromised node.</p><p>The <a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction><code>NodeRestriction</code> admission plugin</a>
prevents the kubelet from setting or modifying labels with a
<code>node-restriction.kubernetes.io/</code> prefix.</p><p>To make use of that label prefix for node isolation:</p><ol><li>Ensure you are using the <a href=/docs/reference/access-authn-authz/node/>Node authorizer</a> and have <em>enabled</em> the <code>NodeRestriction</code> admission plugin.</li><li>Add labels with the <code>node-restriction.kubernetes.io/</code> prefix to your nodes, and use those labels in your <a href=#nodeselector>node selectors</a>.
For example, <code>example.com.node-restriction.kubernetes.io/fips=true</code> or <code>example.com.node-restriction.kubernetes.io/pci-dss=true</code>.</li></ol><h2 id=nodeselector>nodeSelector</h2><p><code>nodeSelector</code> is the simplest recommended form of node selection constraint.
You can add the <code>nodeSelector</code> field to your Pod specification and specify the
<a href=#built-in-node-labels>node labels</a> you want the target node to have.
Kubernetes only schedules the Pod onto nodes that have each of the labels you
specify.</p><p>See <a href=/docs/tasks/configure-pod-container/assign-pods-nodes>Assign Pods to Nodes</a> for more
information.</p><h2 id=affinity-and-anti-affinity>Affinity and anti-affinity</h2><p><code>nodeSelector</code> is the simplest way to constrain Pods to nodes with specific
labels. Affinity and anti-affinity expands the types of constraints you can
define. Some of the benefits of affinity and anti-affinity include:</p><ul><li>The affinity/anti-affinity language is more expressive. <code>nodeSelector</code> only
selects nodes with all the specified labels. Affinity/anti-affinity gives you
more control over the selection logic.</li><li>You can indicate that a rule is <em>soft</em> or <em>preferred</em>, so that the scheduler
still schedules the Pod even if it can't find a matching node.</li><li>You can constrain a Pod using labels on other Pods running on the node (or other topological domain),
instead of just node labels, which allows you to define rules for which Pods
can be co-located on a node.</li></ul><p>The affinity feature consists of two types of affinity:</p><ul><li><em>Node affinity</em> functions like the <code>nodeSelector</code> field but is more expressive and
allows you to specify soft rules.</li><li><em>Inter-pod affinity/anti-affinity</em> allows you to constrain Pods against labels
on other Pods.</li></ul><h3 id=node-affinity>Node affinity</h3><p>Node affinity is conceptually similar to <code>nodeSelector</code>, allowing you to constrain which nodes your
Pod can be scheduled on based on node labels. There are two types of node
affinity:</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code>: The scheduler can't
schedule the Pod unless the rule is met. This functions like <code>nodeSelector</code>,
but with a more expressive syntax.</li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code>: The scheduler tries to
find a node that meets the rule. If a matching node is not available, the
scheduler still schedules the Pod.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In the preceding types, <code>IgnoredDuringExecution</code> means that if the node labels
change after Kubernetes schedules the Pod, the Pod continues to run.</div><p>You can specify node affinities using the <code>.spec.affinity.nodeAffinity</code> field in
your Pod spec.</p><p>For example, consider the following Pod spec:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-node-affinity.yaml download=pods/pod-with-node-affinity.yaml><code>pods/pod-with-node-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-node-affinity-yaml")' title="Copy pods/pod-with-node-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-node-affinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- antarctica-east1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- antarctica-west1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>another-node-label-key<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- another-node-label-value<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0</span></span></code></pre></div></div></div><p>In this example, the following rules apply:</p><ul><li>The node <em>must</em> have a label with the key <code>topology.kubernetes.io/zone</code> and
the value of that label <em>must</em> be either <code>antarctica-east1</code> or <code>antarctica-west1</code>.</li><li>The node <em>preferably</em> has a label with the key <code>another-node-label-key</code> and
the value <code>another-node-label-value</code>.</li></ul><p>You can use the <code>operator</code> field to specify a logical operator for Kubernetes to use when
interpreting the rules. You can use <code>In</code>, <code>NotIn</code>, <code>Exists</code>, <code>DoesNotExist</code>,
<code>Gt</code> and <code>Lt</code>.</p><p><code>NotIn</code> and <code>DoesNotExist</code> allow you to define node anti-affinity behavior.
Alternatively, you can use <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>node taints</a>
to repel Pods from specific nodes.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>If you specify both <code>nodeSelector</code> and <code>nodeAffinity</code>, <em>both</em> must be satisfied
for the Pod to be scheduled onto a node.</p><p>If you specify multiple terms in <code>nodeSelectorTerms</code> associated with <code>nodeAffinity</code>
types, then the Pod can be scheduled onto a node if one of the specified terms
can be satisfied (terms are ORed).</p><p>If you specify multiple expressions in a single <code>matchExpressions</code> field associated with a
term in <code>nodeSelectorTerms</code>, then the Pod can be scheduled onto a node only
if all the expressions are satisfied (expressions are ANDed).</p></div><p>See <a href=/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/>Assign Pods to Nodes using Node Affinity</a>
for more information.</p><h4 id=node-affinity-weight>Node affinity weight</h4><p>You can specify a <code>weight</code> between 1 and 100 for each instance of the
<code>preferredDuringSchedulingIgnoredDuringExecution</code> affinity type. When the
scheduler finds nodes that meet all the other scheduling requirements of the Pod, the
scheduler iterates through every preferred rule that the node satisfies and adds the
value of the <code>weight</code> for that expression to a sum.</p><p>The final sum is added to the score of other priority functions for the node.
Nodes with the highest total score are prioritized when the scheduler makes a
scheduling decision for the Pod.</p><p>For example, consider the following Pod spec:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-affinity-anti-affinity.yaml download=pods/pod-with-affinity-anti-affinity.yaml><code>pods/pod-with-affinity-anti-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-affinity-anti-affinity-yaml")' title="Copy pods/pod-with-affinity-anti-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-affinity-anti-affinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-affinity-anti-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>kubernetes.io/os<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- linux<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>label-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- key-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>preference</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>label-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- key-2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-node-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>If there are two possible nodes that match the
<code>preferredDuringSchedulingIgnoredDuringExecution</code> rule, one with the
<code>label-1:key-1</code> label and another with the <code>label-2:key-2</code> label, the scheduler
considers the <code>weight</code> of each node and adds the weight to the other scores for
that node, and schedules the Pod onto the node with the highest final score.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> If you want Kubernetes to successfully schedule the Pods in this example, you
must have existing nodes with the <code>kubernetes.io/os=linux</code> label.</div><h4 id=node-affinity-per-scheduling-profile>Node affinity per scheduling profile</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.20 [beta]</code></div><p>When configuring multiple <a href=/docs/reference/scheduling/config/#multiple-profiles>scheduling profiles</a>, you can associate
a profile with a node affinity, which is useful if a profile only applies to a specific set of nodes.
To do so, add an <code>addedAffinity</code> to the <code>args</code> field of the <a href=/docs/reference/scheduling/config/#scheduling-plugins><code>NodeAffinity</code> plugin</a>
in the <a href=/docs/reference/scheduling/config/>scheduler configuration</a>. For example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>foo-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeAffinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>addedAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>scheduler-profile<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span>- foo<span style=color:#bbb>
</span></span></span></code></pre></div><p>The <code>addedAffinity</code> is applied to all Pods that set <code>.spec.schedulerName</code> to <code>foo-scheduler</code>, in addition to the
NodeAffinity specified in the PodSpec.
That is, in order to match the Pod, nodes need to satisfy <code>addedAffinity</code> and
the Pod's <code>.spec.NodeAffinity</code>.</p><p>Since the <code>addedAffinity</code> is not visible to end users, its behavior might be
unexpected to them. Use node labels that have a clear correlation to the
scheduler profile name.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The DaemonSet controller, which <a href=/docs/concepts/workloads/controllers/daemonset/#scheduled-by-default-scheduler>creates Pods for DaemonSets</a>,
does not support scheduling profiles. When the DaemonSet controller creates
Pods, the default Kubernetes scheduler places those Pods and honors any
<code>nodeAffinity</code> rules in the DaemonSet controller.</div><h3 id=inter-pod-affinity-and-anti-affinity>Inter-pod affinity and anti-affinity</h3><p>Inter-pod affinity and anti-affinity allow you to constrain which nodes your
Pods can be scheduled on based on the labels of <strong>Pods</strong> already running on that
node, instead of the node labels.</p><p>Inter-pod affinity and anti-affinity rules take the form "this
Pod should (or, in the case of anti-affinity, should not) run in an X if that X
is already running one or more Pods that meet rule Y", where X is a topology
domain like node, rack, cloud provider zone or region, or similar and Y is the
rule Kubernetes tries to satisfy.</p><p>You express these rules (Y) as <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>label selectors</a>
with an optional associated list of namespaces. Pods are namespaced objects in
Kubernetes, so Pod labels also implicitly have namespaces. Any label selectors
for Pod labels should specify the namespaces in which Kubernetes should look for those
labels.</p><p>You express the topology domain (X) using a <code>topologyKey</code>, which is the key for
the node label that the system uses to denote the domain. For examples, see
<a href=/docs/reference/labels-annotations-taints/>Well-Known Labels, Annotations and Taints</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Inter-pod affinity and anti-affinity require substantial amount of
processing which can slow down scheduling in large clusters significantly. We do
not recommend using them in clusters larger than several hundred nodes.</div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Pod anti-affinity requires nodes to be consistently labelled, in other words,
every node in the cluster must have an appropriate label matching <code>topologyKey</code>.
If some or all nodes are missing the specified <code>topologyKey</code> label, it can lead
to unintended behavior.</div><h4 id=types-of-inter-pod-affinity-and-anti-affinity>Types of inter-pod affinity and anti-affinity</h4><p>Similar to <a href=#node-affinity>node affinity</a> are two types of Pod affinity and
anti-affinity as follows:</p><ul><li><code>requiredDuringSchedulingIgnoredDuringExecution</code></li><li><code>preferredDuringSchedulingIgnoredDuringExecution</code></li></ul><p>For example, you could use
<code>requiredDuringSchedulingIgnoredDuringExecution</code> affinity to tell the scheduler to
co-locate Pods of two services in the same cloud provider zone because they
communicate with each other a lot. Similarly, you could use
<code>preferredDuringSchedulingIgnoredDuringExecution</code> anti-affinity to spread Pods
from a service across multiple cloud provider zones.</p><p>To use inter-pod affinity, use the <code>affinity.podAffinity</code> field in the Pod spec.
For inter-pod anti-affinity, use the <code>affinity.podAntiAffinity</code> field in the Pod
spec.</p><h4 id=an-example-of-a-pod-that-uses-pod-affinity>Pod affinity example</h4><p>Consider the following Pod spec:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-pod-affinity.yaml download=pods/pod-with-pod-affinity.yaml><code>pods/pod-with-pod-affinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-pod-affinity-yaml")' title="Copy pods/pod-with-pod-affinity.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-pod-affinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- S1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>preferredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinityTerm</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>security<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- S2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>with-pod-affinity<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:2.0<span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>This example defines one Pod affinity rule and one Pod anti-affinity rule. The
Pod affinity rule uses the "hard"
<code>requiredDuringSchedulingIgnoredDuringExecution</code>, while the anti-affinity rule
uses the "soft" <code>preferredDuringSchedulingIgnoredDuringExecution</code>.</p><p>The affinity rule says that the scheduler can only schedule a Pod onto a node if
the node is in the same zone as one or more existing Pods with the label
<code>security=S1</code>. More precisely, the scheduler must place the Pod on a node that has the
<code>topology.kubernetes.io/zone=V</code> label, as long as there is at least one node in
that zone that currently has one or more Pods with the Pod label <code>security=S1</code>.</p><p>The anti-affinity rule says that the scheduler should try to avoid scheduling
the Pod onto a node that is in the same zone as one or more Pods with the label
<code>security=S2</code>. More precisely, the scheduler should try to avoid placing the Pod on a node that has the
<code>topology.kubernetes.io/zone=R</code> label if there are other nodes in the
same zone currently running Pods with the <code>Security=S2</code> Pod label.</p><p>To get yourself more familiar with the examples of Pod affinity and anti-affinity,
refer to the <a href=https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md>design proposal</a>.</p><p>You can use the <code>In</code>, <code>NotIn</code>, <code>Exists</code> and <code>DoesNotExist</code> values in the
<code>operator</code> field for Pod affinity and anti-affinity.</p><p>In principle, the <code>topologyKey</code> can be any allowed label key with the following
exceptions for performance and security reasons:</p><ul><li>For Pod affinity and anti-affinity, an empty <code>topologyKey</code> field is not allowed in both <code>requiredDuringSchedulingIgnoredDuringExecution</code>
and <code>preferredDuringSchedulingIgnoredDuringExecution</code>.</li><li>For <code>requiredDuringSchedulingIgnoredDuringExecution</code> Pod anti-affinity rules,
the admission controller <code>LimitPodHardAntiAffinityTopology</code> limits
<code>topologyKey</code> to <code>kubernetes.io/hostname</code>. You can modify or disable the
admission controller if you want to allow custom topologies.</li></ul><p>In addition to <code>labelSelector</code> and <code>topologyKey</code>, you can optionally specify a list
of namespaces which the <code>labelSelector</code> should match against using the
<code>namespaces</code> field at the same level as <code>labelSelector</code> and <code>topologyKey</code>.
If omitted or empty, <code>namespaces</code> defaults to the namespace of the Pod where the
affinity/anti-affinity definition appears.</p><h4 id=namespace-selector>Namespace selector</h4><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>You can also select matching namespaces using <code>namespaceSelector</code>, which is a label query over the set of namespaces.
The affinity term is applied to namespaces selected by both <code>namespaceSelector</code> and the <code>namespaces</code> field.
Note that an empty <code>namespaceSelector</code> ({}) matches all namespaces, while a null or empty <code>namespaces</code> list and
null <code>namespaceSelector</code> matches the namespace of the Pod where the rule is defined.</p><h4 id=more-practical-use-cases>More practical use-cases</h4><p>Inter-pod affinity and anti-affinity can be even more useful when they are used with higher
level collections such as ReplicaSets, StatefulSets, Deployments, etc. These
rules allow you to configure that a set of workloads should
be co-located in the same defined topology; for example, preferring to place two related
Pods onto the same node.</p><p>For example: imagine a three-node cluster. You use the cluster to run a web application
and also an in-memory cache (such as Redis). For this example, also assume that latency between
the web application and the memory cache should be as low as is practical. You could use inter-pod
affinity and anti-affinity to co-locate the web servers with the cache as much as possible.</p><p>In the following example Deployment for the Redis cache, the replicas get the label <code>app=store</code>. The
<code>podAntiAffinity</code> rule tells the scheduler to avoid placing multiple replicas
with the <code>app=store</code> label on a single node. This creates each cache in a
separate node.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-cache<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>redis-server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>redis:3.2-alpine<span style=color:#bbb>
</span></span></span></code></pre></div><p>The following example Deployment for the web servers creates replicas with the label <code>app=web-store</code>.
The Pod affinity rule tells the scheduler to place each replica on a node that has a Pod
with the label <code>app=store</code>. The Pod anti-affinity rule tells the scheduler never to place
multiple <code>app=web-store</code> servers on a single node.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>apps/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Deployment<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>selector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>replicas</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>template</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>app</span>:<span style=color:#bbb> </span>web-store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAntiAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- web-store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>podAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span>- store<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>web-app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx:1.16-alpine<span style=color:#bbb>
</span></span></span></code></pre></div><p>Creating the two preceding Deployments results in the following cluster layout,
where each web server is co-located with a cache, on three separate nodes.</p><table><thead><tr><th style=text-align:center>node-1</th><th style=text-align:center>node-2</th><th style=text-align:center>node-3</th></tr></thead><tbody><tr><td style=text-align:center><em>webserver-1</em></td><td style=text-align:center><em>webserver-2</em></td><td style=text-align:center><em>webserver-3</em></td></tr><tr><td style=text-align:center><em>cache-1</em></td><td style=text-align:center><em>cache-2</em></td><td style=text-align:center><em>cache-3</em></td></tr></tbody></table><p>The overall effect is that each cache instance is likely to be accessed by a single client, that
is running on the same node. This approach aims to minimize both skew (imbalanced load) and latency.</p><p>You might have other reasons to use Pod anti-affinity.
See the <a href=/docs/tutorials/stateful-application/zookeeper/#tolerating-node-failure>ZooKeeper tutorial</a>
for an example of a StatefulSet configured with anti-affinity for high
availability, using the same technique as this example.</p><h2 id=nodename>nodeName</h2><p><code>nodeName</code> is a more direct form of node selection than affinity or
<code>nodeSelector</code>. <code>nodeName</code> is a field in the Pod spec. If the <code>nodeName</code> field
is not empty, the scheduler ignores the Pod and the kubelet on the named node
tries to place the Pod on that node. Using <code>nodeName</code> overrules using
<code>nodeSelector</code> or affinity and anti-affinity rules.</p><p>Some of the limitations of using <code>nodeName</code> to select nodes are:</p><ul><li>If the named node does not exist, the Pod will not run, and in
some cases may be automatically deleted.</li><li>If the named node does not have the resources to accommodate the
Pod, the Pod will fail and its reason will indicate why,
for example OutOfmemory or OutOfcpu.</li><li>Node names in cloud environments are not always predictable or
stable.</li></ul><p>Here is an example of a Pod spec using the <code>nodeName</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodeName</span>:<span style=color:#bbb> </span>kube-01<span style=color:#bbb>
</span></span></span></code></pre></div><p>The above Pod will only run on the node <code>kube-01</code>.</p><h2 id=pod-topology-spread-constraints>Pod topology spread constraints</h2><p>You can use <em>topology spread constraints</em> to control how <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a>
are spread across your cluster among failure-domains such as regions, zones, nodes, or among any other
topology domains that you define. You might do this to improve performance, expected availability, or
overall utilization.</p><p>Read <a href=/docs/concepts/scheduling-eviction/topology-spread-constraints/>Pod topology spread constraints</a>
to learn more about how these work.</p><h2 id=what-s-next>What's next</h2><ul><li>Read more about <a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>taints and tolerations</a> .</li><li>Read the design docs for <a href=https://git.k8s.io/design-proposals-archive/scheduling/nodeaffinity.md>node affinity</a>
and for <a href=https://git.k8s.io/design-proposals-archive/scheduling/podaffinity.md>inter-pod affinity/anti-affinity</a>.</li><li>Learn about how the <a href=/docs/tasks/administer-cluster/topology-manager/>topology manager</a> takes part in node-level
resource allocation decisions.</li><li>Learn how to use <a href=/docs/tasks/configure-pod-container/assign-pods-nodes/>nodeSelector</a>.</li><li>Learn how to use <a href=/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/>affinity and anti-affinity</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-da22fe2278df236f71efbe672f392677>3 - Pod Overhead</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>When you run a Pod on a Node, the Pod itself takes an amount of system resources. These
resources are additional to the resources needed to run the container(s) inside the Pod.
In Kubernetes, <em>Pod Overhead</em> is a way to account for the resources consumed by the Pod
infrastructure on top of the container requests & limits.</p><p>In Kubernetes, the Pod's overhead is set at
<a href=/docs/reference/access-authn-authz/extensible-admission-controllers/#what-are-admission-webhooks>admission</a>
time according to the overhead associated with the Pod's
<a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a>.</p><p>A pod's overhead is considered in addition to the sum of container resource requests when
scheduling a Pod. Similarly, the kubelet will include the Pod overhead when sizing the Pod cgroup,
and when carrying out Pod eviction ranking.</p><h2 id=set-up>Configuring Pod overhead</h2><p>You need to make sure a <code>RuntimeClass</code> is utilized which defines the <code>overhead</code> field.</p><h2 id=usage-example>Usage example</h2><p>To work with Pod overhead, you need a RuntimeClass that defines the <code>overhead</code> field. As
an example, you could use the following RuntimeClass definition with a virtualization container
runtime that uses around 120MiB per Pod for the virtual machine and the guest OS:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>node.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>RuntimeClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>handler</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>overhead</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>podFixed</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;120Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;250m&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Workloads which are created which specify the <code>kata-fc</code> RuntimeClass handler will take the memory and
cpu overheads into account for resource quota calculations, node scheduling, as well as Pod cgroup sizing.</p><p>Consider running the given example workload, test-pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>test-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>runtimeClassName</span>:<span style=color:#bbb> </span>kata-fc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>busybox-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>busybox:1.28<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>stdin</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>tty</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx-ctr<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>cpu</span>:<span style=color:#bbb> </span>1500m<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>memory</span>:<span style=color:#bbb> </span>100Mi<span style=color:#bbb>
</span></span></span></code></pre></div><p>At admission time the RuntimeClass <a href=/docs/reference/access-authn-authz/admission-controllers/>admission controller</a>
updates the workload's PodSpec to include the <code>overhead</code> as described in the RuntimeClass. If the PodSpec already has this field defined,
the Pod will be rejected. In the given example, since only the RuntimeClass name is specified, the admission controller mutates the Pod
to include an <code>overhead</code>.</p><p>After the RuntimeClass admission controller has made modifications, you can check the updated
Pod overhead value:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.overhead}&#39;</span>
</span></span></code></pre></div><p>The output is:</p><pre tabindex=0><code>map[cpu:250m memory:120Mi]
</code></pre><p>If a <a href=/docs/concepts/policy/resource-quotas/>ResourceQuota</a> is defined, the sum of container requests as well as the
<code>overhead</code> field are counted.</p><p>When the kube-scheduler is deciding which node should run a new Pod, the scheduler considers that Pod's
<code>overhead</code> as well as the sum of container requests for that Pod. For this example, the scheduler adds the
requests and the overhead, then looks for a node that has 2.25 CPU and 320 MiB of memory available.</p><p>Once a Pod is scheduled to a node, the kubelet on that node creates a new <a class=glossary-tooltip title='A group of Linux processes with optional resource isolation, accounting and limits.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-cgroup' target=_blank aria-label=cgroup>cgroup</a> for the Pod. It is within this pod that the underlying
container runtime will create containers.</p><p>If the resource has a limit defined for each container (Guaranteed QoS or Burstable QoS with limits defined),
the kubelet will set an upper limit for the pod cgroup associated with that resource (cpu.cfs_quota_us for CPU
and memory.limit_in_bytes memory). This upper limit is based on the sum of the container limits plus the <code>overhead</code>
defined in the PodSpec.</p><p>For CPU, if the Pod is Guaranteed or Burstable QoS, the kubelet will set <code>cpu.shares</code> based on the
sum of container requests plus the <code>overhead</code> defined in the PodSpec.</p><p>Looking at our example, verify the container requests for the workload:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pod test-pod -o <span style=color:#b8860b>jsonpath</span><span style=color:#666>=</span><span style=color:#b44>&#39;{.spec.containers[*].resources.limits}&#39;</span>
</span></span></code></pre></div><p>The total container requests are 2000m CPU and 200MiB of memory:</p><pre tabindex=0><code>map[cpu: 500m memory:100Mi] map[cpu:1500m memory:100Mi]
</code></pre><p>Check this against what is observed by the node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe node | grep test-pod -B2
</span></span></code></pre></div><p>The output shows requests for 2250m CPU, and for 320MiB of memory. The requests include Pod overhead:</p><pre tabindex=0><code>  Namespace    Name       CPU Requests  CPU Limits   Memory Requests  Memory Limits  AGE
  ---------    ----       ------------  ----------   ---------------  -------------  ---
  default      test-pod   2250m (56%)   2250m (56%)  320Mi (1%)       320Mi (1%)     36m
</code></pre><h2 id=verify-pod-cgroup-limits>Verify Pod cgroup limits</h2><p>Check the Pod's memory cgroups on the node where the workload is running. In the following example,
<a href=https://github.com/kubernetes-sigs/cri-tools/blob/master/docs/crictl.md><code>crictl</code></a>
is used on the node, which provides a CLI for CRI-compatible container runtimes. This is an
advanced example to show Pod overhead behavior, and it is not expected that users should need to check
cgroups directly on the node.</p><p>First, on the particular node, determine the Pod identifier:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># Run this on the node where the Pod is scheduled</span>
</span></span><span style=display:flex><span><span style=color:#b8860b>POD_ID</span><span style=color:#666>=</span><span style=color:#b44>&#34;</span><span style=color:#a2f;font-weight:700>$(</span>sudo crictl pods --name test-pod -q<span style=color:#a2f;font-weight:700>)</span><span style=color:#b44>&#34;</span>
</span></span></code></pre></div><p>From this, you can determine the cgroup path for the Pod:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># Run this on the node where the Pod is scheduled</span>
</span></span><span style=display:flex><span>sudo crictl inspectp -o<span style=color:#666>=</span>json <span style=color:#b8860b>$POD_ID</span> | grep cgroupsPath
</span></span></code></pre></div><p>The resulting cgroup path includes the Pod's <code>pause</code> container. The Pod level cgroup is one directory above.</p><pre tabindex=0><code>  &#34;cgroupsPath&#34;: &#34;/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/7ccf55aee35dd16aca4189c952d83487297f3cd760f1bbf09620e206e7d0c27a&#34;
</code></pre><p>In this specific case, the pod cgroup path is <code>kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2</code>.
Verify the Pod level cgroup setting for memory:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#080;font-style:italic># Run this on the node where the Pod is scheduled.</span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic># Also, change the name of the cgroup to match the cgroup allocated for your pod.</span>
</span></span><span style=display:flex><span> cat /sys/fs/cgroup/memory/kubepods/podd7f4b509-cf94-4951-9417-d1087c92a5b2/memory.limit_in_bytes
</span></span></code></pre></div><p>This is 320 MiB, as expected:</p><pre tabindex=0><code>335544320
</code></pre><h3 id=observability>Observability</h3><p>Some <code>kube_pod_overhead_*</code> metrics are available in <a href=https://github.com/kubernetes/kube-state-metrics>kube-state-metrics</a>
to help identify when Pod overhead is being utilized and to help observe stability of workloads
running with a defined overhead.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn more about <a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a></li><li>Read the <a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/688-pod-overhead>PodOverhead Design</a>
enhancement proposal for extra context</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6b8c85a6a88f4a81e6b79e197c293c31>4 - Pod Topology Spread Constraints</h1><p>You can use <em>topology spread constraints</em> to control how
<a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> are spread across your cluster
among failure-domains such as regions, zones, nodes, and other user-defined topology
domains. This can help to achieve high availability as well as efficient resource
utilization.</p><p>You can set <a href=#cluster-level-default-constraints>cluster-level constraints</a> as a default,
or configure topology spread constraints for individual workloads.</p><h2 id=motivation>Motivation</h2><p>Imagine that you have a cluster of up to twenty nodes, and you want to run a
<a class=glossary-tooltip title='A workload is an application running on Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/ target=_blank aria-label=workload>workload</a>
that automatically scales how many replicas it uses. There could be as few as
two Pods or as many as fifteen.
When there are only two Pods, you'd prefer not to have both of those Pods run on the
same node: you would run the risk that a single node failure takes your workload
offline.</p><p>In addition to this basic usage, there are some advanced usage examples that
enable your workloads to benefit on high availability and cluster utilization.</p><p>As you scale up and run more Pods, a different concern becomes important. Imagine
that you have three nodes running five Pods each. The nodes have enough capacity
to run that many replicas; however, the clients that interact with this workload
are split across three different datacenters (or infrastructure zones). Now you
have less concern about a single node failure, but you notice that latency is
higher than you'd like, and you are paying for network costs associated with
sending network traffic between the different zones.</p><p>You decide that under normal operation you'd prefer to have a similar number of replicas
<a href=/docs/concepts/scheduling-eviction/>scheduled</a> into each infrastructure zone,
and you'd like the cluster to self-heal in the case that there is a problem.</p><p>Pod topology spread constraints offer you a declarative way to configure that.</p><h2 id=topologyspreadconstraints-field><code>topologySpreadConstraints</code> field</h2><p>The Pod API includes a field, <code>spec.topologySpreadConstraints</code>. The usage of this field looks like
the following:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#00f;font-weight:700>---</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>example-pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic># Configure a topology spread constraint</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span>&lt;integer&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>minDomains</span>:<span style=color:#bbb> </span>&lt;integer&gt;<span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional; beta since v1.25</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>&lt;string&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>&lt;string&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb> </span>&lt;object&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabelKeys</span>:<span style=color:#bbb> </span>&lt;list&gt;<span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional; alpha since v1.25</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeAffinityPolicy</span>:<span style=color:#bbb> </span>[Honor|Ignore]<span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional; alpha since v1.25</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>nodeTaintsPolicy</span>:<span style=color:#bbb> </span>[Honor|Ignore]<span style=color:#bbb> </span><span style=color:#080;font-style:italic># optional; alpha since v1.25</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#080;font-style:italic>### other Pod fields go here</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>You can read more about this field by running <code>kubectl explain Pod.spec.topologySpreadConstraints</code> or
refer to <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling>scheduling</a> section of the API reference for Pod.</p><h3 id=spread-constraint-definition>Spread constraint definition</h3><p>You can define one or multiple <code>topologySpreadConstraints</code> entries to instruct the
kube-scheduler how to place each incoming Pod in relation to the existing Pods across
your cluster. Those fields are:</p><ul><li><p><strong>maxSkew</strong> describes the degree to which Pods may be unevenly distributed. You must
specify this field and the number must be greater than zero. Its semantics differ
according to the value of <code>whenUnsatisfiable</code>:</p><ul><li>if you select <code>whenUnsatisfiable: DoNotSchedule</code>, then <code>maxSkew</code> defines the
maximum permitted difference between the number of matching pods in the target
topology and the <em>global minimum</em>
(the minimum number of matching pods in an eligible domain or zero if the number of eligible domains is less than MinDomains).
For example, if you have 3 zones with 2, 2 and 1 matching pods respectively,
<code>MaxSkew</code> is set to 1 then the global minimum is 1.</li><li>if you select <code>whenUnsatisfiable: ScheduleAnyway</code>, the scheduler gives higher
precedence to topologies that would help reduce the skew.</li></ul></li><li><p><strong>minDomains</strong> indicates a minimum number of eligible domains. This field is optional.
A domain is a particular instance of a topology. An eligible domain is a domain whose
nodes match the node selector.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>minDomains</code> field is a beta field and enabled by default in 1.25. You can disable it by disabling the
<code>MinDomainsInPodTopologySpread</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>.</div><ul><li>The value of <code>minDomains</code> must be greater than 0, when specified.
You can only specify <code>minDomains</code> in conjunction with <code>whenUnsatisfiable: DoNotSchedule</code>.</li><li>When the number of eligible domains with match topology keys is less than <code>minDomains</code>,
Pod topology spread treats global minimum as 0, and then the calculation of <code>skew</code> is performed.
The global minimum is the minimum number of matching Pods in an eligible domain,
or zero if the number of eligible domains is less than <code>minDomains</code>.</li><li>When the number of eligible domains with matching topology keys equals or is greater than
<code>minDomains</code>, this value has no effect on scheduling.</li><li>If you do not specify <code>minDomains</code>, the constraint behaves as if <code>minDomains</code> is 1.</li></ul></li><li><p><strong>topologyKey</strong> is the key of <a href=#node-labels>node labels</a>. Nodes that have a label with this key
and identical values are considered to be in the same topology.
We call each instance of a topology (in other words, a &lt;key, value> pair) a domain. The scheduler
will try to put a balanced number of pods into each domain.
Also, we define an eligible domain as a domain whose nodes meet the requirements of
nodeAffinityPolicy and nodeTaintsPolicy.</p></li><li><p><strong>whenUnsatisfiable</strong> indicates how to deal with a Pod if it doesn't satisfy the spread constraint:</p><ul><li><code>DoNotSchedule</code> (default) tells the scheduler not to schedule it.</li><li><code>ScheduleAnyway</code> tells the scheduler to still schedule it while prioritizing nodes that minimize the skew.</li></ul></li><li><p><strong>labelSelector</strong> is used to find matching Pods. Pods
that match this label selector are counted to determine the
number of Pods in their corresponding topology domain.
See <a href=/docs/concepts/overview/working-with-objects/labels/#label-selectors>Label Selectors</a>
for more details.</p></li><li><p><strong>matchLabelKeys</strong> is a list of pod label keys to select the pods over which
spreading will be calculated. The keys are used to lookup values from the pod labels, those key-value labels are ANDed with <code>labelSelector</code> to select the group of existing pods over which spreading will be calculated for the incoming pod. Keys that don't exist in the pod labels will be ignored. A null or empty list means only match against the <code>labelSelector</code>.</p><p>With <code>matchLabelKeys</code>, users don't need to update the <code>pod.spec</code> between different revisions. The controller/operator just needs to set different values to the same <code>label</code> key for different revisions. The scheduler will assume the values automatically based on <code>matchLabelKeys</code>. For example, if users use Deployment, they can use the label keyed with <code>pod-template-hash</code>, which is added automatically by the Deployment controller, to distinguish between different revisions in a single Deployment.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>kubernetes.io/hostname<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>matchLabelKeys</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- app<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- pod-template-hash<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>matchLabelKeys</code> field is an alpha field added in 1.25. You have to enable the
<code>MatchLabelKeysInPodTopologySpread</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
in order to use it.</div></li><li><p><strong>nodeAffinityPolicy</strong> indicates how we will treat Pod's nodeAffinity/nodeSelector
when calculating pod topology spread skew. Options are:</p><ul><li>Honor: only nodes matching nodeAffinity/nodeSelector are included in the calculations.</li><li>Ignore: nodeAffinity/nodeSelector are ignored. All nodes are included in the calculations.</li></ul><p>If this value is null, the behavior is equivalent to the Honor policy.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>nodeAffinityPolicy</code> is an alpha-level field added in 1.25. You have to enable the
<code>NodeInclusionPolicyInPodTopologySpread</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
in order to use it.</div></li><li><p><strong>nodeTaintsPolicy</strong> indicates how we will treat node taints when calculating
pod topology spread skew. Options are:</p><ul><li>Honor: nodes without taints, along with tainted nodes for which the incoming pod
has a toleration, are included.</li><li>Ignore: node taints are ignored. All nodes are included.</li></ul><p>If this value is null, the behavior is equivalent to the Ignore policy.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <code>nodeTaintsPolicy</code> is an alpha-level field added in 1.25. You have to enable the
<code>NodeInclusionPolicyInPodTopologySpread</code> <a href=/docs/reference/command-line-tools-reference/feature-gates/>feature gate</a>
in order to use it.</div></li></ul><p>When a Pod defines more than one <code>topologySpreadConstraint</code>, those constraints are
combined using a logical AND operation: the kube-scheduler looks for a node for the incoming Pod
that satisfies all the configured constraints.</p><h3 id=node-labels>Node labels</h3><p>Topology spread constraints rely on node labels to identify the topology
domain(s) that each <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=node>node</a> is in.
For example, a node might have labels:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>region</span>:<span style=color:#bbb> </span>us-east-1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>zone</span>:<span style=color:#bbb> </span>us-east-1a<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>For brevity, this example doesn't use the
<a href=/docs/reference/labels-annotations-taints/>well-known</a> label keys
<code>topology.kubernetes.io/zone</code> and <code>topology.kubernetes.io/region</code>. However,
those registered label keys are nonetheless recommended rather than the private
(unqualified) label keys <code>region</code> and <code>zone</code> that are used here.</p><p>You can't make a reliable assumption about the meaning of a private label key
between different contexts.</p></div><p>Suppose you have a 4-node cluster with the following labels:</p><pre tabindex=0><code>NAME    STATUS   ROLES    AGE     VERSION   LABELS
node1   Ready    &lt;none&gt;   4m26s   v1.16.0   node=node1,zone=zoneA
node2   Ready    &lt;none&gt;   3m58s   v1.16.0   node=node2,zone=zoneA
node3   Ready    &lt;none&gt;   3m17s   v1.16.0   node=node3,zone=zoneB
node4   Ready    &lt;none&gt;   2m43s   v1.16.0   node=node4,zone=zoneB
</code></pre><p>Then the cluster is logically viewed as below:</p><figure><div class=mermaid>graph TB
subgraph "zoneB"
n3(Node3)
n4(Node4)
end
subgraph "zoneA"
n1(Node1)
n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><h2 id=consistency>Consistency</h2><p>You should set the same Pod topology spread constraints on all pods in a group.</p><p>Usually, if you are using a workload controller such as a Deployment, the pod template
takes care of this for you. If you mix different spread constraints then Kubernetes
follows the API definition of the field; however, the behavior is more likely to become
confusing and troubleshooting is less straightforward.</p><p>You need a mechanism to ensure that all the nodes in a topology domain (such as a
cloud provider region) are labelled consistently.
To avoid you needing to manually label nodes, most clusters automatically
populate well-known labels such as <code>topology.kubernetes.io/hostname</code>. Check whether
your cluster supports this.</p><h2 id=topology-spread-constraint-examples>Topology spread constraint examples</h2><h3 id=example-one-topologyspreadconstraint>Example: one topology spread constraint</h3><p>Suppose you have a 4-node cluster where 3 Pods labelled <code>foo: bar</code> are located in
node1, node2 and node3 respectively:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>If you want an incoming Pod to be evenly spread with existing Pods across zones, you
can use a manifest similar to:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint.yaml download=pods/topology-spread-constraints/one-constraint.yaml><code>pods/topology-spread-constraints/one-constraint.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-one-constraint-yaml")' title="Copy pods/topology-spread-constraints/one-constraint.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-one-constraint-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>From that manifest, <code>topologyKey: zone</code> implies the even distribution will only be applied
to nodes that are labelled <code>zone: &lt;any value></code> (nodes that don't have a <code>zone</code> label
are skipped). The field <code>whenUnsatisfiable: DoNotSchedule</code> tells the scheduler to let the
incoming Pod stay pending if the scheduler can't find a way to satisfy the constraint.</p><p>If the scheduler placed this incoming Pod into zone <code>A</code>, the distribution of Pods would
become <code>[3, 1]</code>. That means the actual skew is then 2 (calculated as <code>3 - 1</code>), which
violates <code>maxSkew: 1</code>. To satisfy the constraints and context for this example, the
incoming Pod can only be placed onto a node in zone <code>B</code>:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
p4(mypod) --> n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>OR</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
p4(mypod) --> n3
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>You can tweak the Pod spec to meet various kinds of requirements:</p><ul><li>Change <code>maxSkew</code> to a bigger value - such as <code>2</code> - so that the incoming Pod can
be placed into zone <code>A</code> as well.</li><li>Change <code>topologyKey</code> to <code>node</code> so as to distribute the Pods evenly across nodes
instead of zones. In the above example, if <code>maxSkew</code> remains <code>1</code>, the incoming
Pod can only be placed onto the node <code>node4</code>.</li><li>Change <code>whenUnsatisfiable: DoNotSchedule</code> to <code>whenUnsatisfiable: ScheduleAnyway</code>
to ensure the incoming Pod to be always schedulable (suppose other scheduling APIs
are satisfied). However, it's preferred to be placed into the topology domain which
has fewer matching Pods. (Be aware that this preference is jointly normalized
with other internal scheduling priorities such as resource usage ratio).</li></ul><h3 id=example-multiple-topologyspreadconstraints>Example: multiple topology spread constraints</h3><p>This builds upon the previous example. Suppose you have a 4-node cluster where 3
existing Pods labeled <code>foo: bar</code> are located on node1, node2 and node3 respectively:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>You can combine two topology spread constraints to control the spread of Pods both
by node and by zone:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml download=pods/topology-spread-constraints/two-constraints.yaml><code>pods/topology-spread-constraints/two-constraints.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-two-constraints-yaml")' title="Copy pods/topology-spread-constraints/two-constraints.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-two-constraints-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>node<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><p>In this case, to match the first constraint, the incoming Pod can only be placed onto
nodes in zone <code>B</code>; while in terms of the second constraint, the incoming Pod can only be
scheduled to the node <code>node4</code>. The scheduler only considers options that satisfy all
defined constraints, so the only valid placement is onto node <code>node4</code>.</p><h3 id=example-conflicting-topologyspreadconstraints>Example: conflicting topology spread constraints</h3><p>Multiple constraints can lead to conflicts. Suppose you have a 3-node cluster across 2 zones:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p4(Pod) --> n3(Node3)
p5(Pod) --> n3
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n1
p3(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3,p4,p5 k8s;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>If you were to apply
<a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/two-constraints.yaml><code>two-constraints.yaml</code></a>
(the manifest from the previous example)
to <strong>this</strong> cluster, you would see that the Pod <code>mypod</code> stays in the <code>Pending</code> state.
This happens because: to satisfy the first constraint, the Pod <code>mypod</code> can only
be placed into zone <code>B</code>; while in terms of the second constraint, the Pod <code>mypod</code>
can only schedule to node <code>node2</code>. The intersection of the two constraints returns
an empty set, and the scheduler cannot place the Pod.</p><p>To overcome this situation, you can either increase the value of <code>maxSkew</code> or modify
one of the constraints to use <code>whenUnsatisfiable: ScheduleAnyway</code>. Depending on
circumstances, you might also decide to delete an existing Pod manually - for example,
if you are troubleshooting why a bug-fix rollout is not making progress.</p><h4 id=interaction-with-node-affinity-and-node-selectors>Interaction with node affinity and node selectors</h4><p>The scheduler will skip the non-matching nodes from the skew calculations if the
incoming Pod has <code>spec.nodeSelector</code> or <code>spec.affinity.nodeAffinity</code> defined.</p><h3 id=example-topologyspreadconstraints-with-nodeaffinity>Example: topology spread constraints with node affinity</h3><p>Suppose you have a 5-node cluster ranging across zones A to C:</p><figure><div class=mermaid>graph BT
subgraph "zoneB"
p3(Pod) --> n3(Node3)
n4(Node4)
end
subgraph "zoneA"
p1(Pod) --> n1(Node1)
p2(Pod) --> n2(Node2)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n1,n2,n3,n4,p1,p2,p3 k8s;
class p4 plain;
class zoneA,zoneB cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><figure><div class=mermaid>graph BT
subgraph "zoneC"
n5(Node5)
end
classDef plain fill:#ddd,stroke:#fff,stroke-width:4px,color:#000;
classDef k8s fill:#326ce5,stroke:#fff,stroke-width:4px,color:#fff;
classDef cluster fill:#fff,stroke:#bbb,stroke-width:2px,color:#326ce5;
class n5 k8s;
class zoneC cluster;</div></figure><noscript><div class="alert alert-secondary callout" role=alert><em class=javascript-required>JavaScript must be <a href=https://www.enable-javascript.com/>enabled</a> to view this content</em></div></noscript><p>and you know that zone <code>C</code> must be excluded. In this case, you can compose a manifest
as below, so that Pod <code>mypod</code> will be placed into zone <code>B</code> instead of zone <code>C</code>.
Similarly, Kubernetes also respects <code>spec.nodeSelector</code>.</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml download=pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml><code>pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml")' title="Copy pods/topology-spread-constraints/one-constraint-with-nodeaffinity.yaml to clipboard"></img></div><div class=includecode id=pods-topology-spread-constraints-one-constraint-with-nodeaffinity-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>mypod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>topologySpreadConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>DoNotSchedule<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>labelSelector</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>matchLabels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>foo</span>:<span style=color:#bbb> </span>bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>affinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span>zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span>NotIn<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- zoneC<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>pause<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>registry.k8s.io/pause:3.1</span></span></code></pre></div></div></div><h2 id=implicit-conventions>Implicit conventions</h2><p>There are some implicit conventions worth noting here:</p><ul><li><p>Only the Pods holding the same namespace as the incoming Pod can be matching candidates.</p></li><li><p>The scheduler bypasses any nodes that don't have any <code>topologySpreadConstraints[*].topologyKey</code>
present. This implies that:</p><ol><li>any Pods located on those bypassed nodes do not impact <code>maxSkew</code> calculation - in the
above example, suppose the node <code>node1</code> does not have a label "zone", then the 2 Pods will
be disregarded, hence the incoming Pod will be scheduled into zone <code>A</code>.</li><li>the incoming Pod has no chances to be scheduled onto this kind of nodes -
in the above example, suppose a node <code>node5</code> has the <strong>mistyped</strong> label <code>zone-typo: zoneC</code>
(and no <code>zone</code> label set). After node <code>node5</code> joins the cluster, it will be bypassed and
Pods for this workload aren't scheduled there.</li></ol></li><li><p>Be aware of what will happen if the incoming Pod's
<code>topologySpreadConstraints[*].labelSelector</code> doesn't match its own labels. In the
above example, if you remove the incoming Pod's labels, it can still be placed onto
nodes in zone <code>B</code>, since the constraints are still satisfied. However, after that
placement, the degree of imbalance of the cluster remains unchanged - it's still zone <code>A</code>
having 2 Pods labelled as <code>foo: bar</code>, and zone <code>B</code> having 1 Pod labelled as
<code>foo: bar</code>. If this is not what you expect, update the workload's
<code>topologySpreadConstraints[*].labelSelector</code> to match the labels in the pod template.</p></li></ul><h2 id=cluster-level-default-constraints>Cluster-level default constraints</h2><p>It is possible to set default topology spread constraints for a cluster. Default
topology spread constraints are applied to a Pod if, and only if:</p><ul><li>It doesn't define any constraints in its <code>.spec.topologySpreadConstraints</code>.</li><li>It belongs to a Service, ReplicaSet, StatefulSet or ReplicationController.</li></ul><p>Default constraints can be set as part of the <code>PodTopologySpread</code> plugin
arguments in a <a href=/docs/reference/scheduling/config/#profiles>scheduling profile</a>.
The constraints are specified with the same <a href=#topologyspreadconstraints-field>API above</a>, except that
<code>labelSelector</code> must be empty. The selectors are calculated from the Services,
ReplicaSets, StatefulSets or ReplicationControllers that the Pod belongs to.</p><p>An example configuration might look like follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PodTopologySpread<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span>topology.kubernetes.io/zone<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultingType</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The <a href=/docs/reference/scheduling/config/#scheduling-plugins><code>SelectorSpread</code> plugin</a>
is disabled by default. The Kubernetes project recommends using <code>PodTopologySpread</code>
to achieve similar behavior.</div><h3 id=internal-default-constraints>Built-in default constraints</h3><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>If you don't configure any cluster-level default constraints for pod topology spreading,
then kube-scheduler acts as if you specified the following default topology constraints:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;kubernetes.io/hostname&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>maxSkew</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>topologyKey</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;topology.kubernetes.io/zone&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>whenUnsatisfiable</span>:<span style=color:#bbb> </span>ScheduleAnyway<span style=color:#bbb>
</span></span></span></code></pre></div><p>Also, the legacy <code>SelectorSpread</code> plugin, which provides an equivalent behavior,
is disabled by default.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>The <code>PodTopologySpread</code> plugin does not score the nodes that don't have
the topology keys specified in the spreading constraints. This might result
in a different default behavior compared to the legacy <code>SelectorSpread</code> plugin when
using the default topology constraints.</p><p>If your nodes are not expected to have <strong>both</strong> <code>kubernetes.io/hostname</code> and
<code>topology.kubernetes.io/zone</code> labels set, define your own constraints
instead of using the Kubernetes defaults.</p></div><p>If you don't want to use the default Pod spreading constraints for your cluster,
you can disable those defaults by setting <code>defaultingType</code> to <code>List</code> and leaving
empty <code>defaultConstraints</code> in the <code>PodTopologySpread</code> plugin configuration:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>schedulerName</span>:<span style=color:#bbb> </span>default-scheduler<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>PodTopologySpread<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultConstraints</span>:<span style=color:#bbb> </span>[]<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>defaultingType</span>:<span style=color:#bbb> </span>List<span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=comparison-with-podaffinity-podantiaffinity>Comparison with podAffinity and podAntiAffinity</h2><p>In Kubernetes, <a href=/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>inter-Pod affinity and anti-affinity</a>
control how Pods are scheduled in relation to one another - either more packed
or more scattered.</p><dl><dt><code>podAffinity</code></dt><dd>attracts Pods; you can try to pack any number of Pods into qualifying
topology domain(s).</dd><dt><code>podAntiAffinity</code></dt><dd>repels Pods. If you set this to <code>requiredDuringSchedulingIgnoredDuringExecution</code> mode then
only a single Pod can be scheduled into a single topology domain; if you choose
<code>preferredDuringSchedulingIgnoredDuringExecution</code> then you lose the ability to enforce the
constraint.</dd></dl><p>For finer control, you can specify topology spread constraints to distribute
Pods across different topology domains - to achieve either high availability or
cost-saving. This can also help on rolling update workloads and scaling out
replicas smoothly.</p><p>For more context, see the
<a href=https://github.com/kubernetes/enhancements/tree/master/keps/sig-scheduling/895-pod-topology-spread#motivation>Motivation</a>
section of the enhancement proposal about Pod topology spread constraints.</p><h2 id=known-limitations>Known limitations</h2><ul><li><p>There's no guarantee that the constraints remain satisfied when Pods are removed. For
example, scaling down a Deployment may result in imbalanced Pods distribution.</p><p>You can use a tool such as the <a href=https://github.com/kubernetes-sigs/descheduler>Descheduler</a>
to rebalance the Pods distribution.</p></li><li><p>Pods matched on tainted nodes are respected.
See <a href=https://github.com/kubernetes/kubernetes/issues/80921>Issue 80921</a>.</p></li><li><p>The scheduler doesn't have prior knowledge of all the zones or other topology
domains that a cluster has. They are determined from the existing nodes in the
cluster. This could lead to a problem in autoscaled clusters, when a node pool (or
node group) is scaled to zero nodes, and you're expecting the cluster to scale up,
because, in this case, those topology domains won't be considered until there is
at least one node in them.</p><p>You can work around this by using an cluster autoscaling tool that is aware of
Pod topology spread constraints and is also aware of the overall set of topology
domains.</p></li></ul><h2 id=what-s-next>What's next</h2><ul><li>The blog article <a href=/blog/2020/05/introducing-podtopologyspread/>Introducing PodTopologySpread</a>
explains <code>maxSkew</code> in some detail, as well as covering some advanced usage examples.</li><li>Read the <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#scheduling>scheduling</a> section of
the API reference for Pod.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ede4960b56a3529ee0bfe7c8fe2d09a5>5 - Taints and Tolerations</h1><p><a href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity><em>Node affinity</em></a>
is a property of <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pods>Pods</a> that <em>attracts</em> them to
a set of <a class=glossary-tooltip title='A node is a worker machine in Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/nodes/ target=_blank aria-label=nodes>nodes</a> (either as a preference or a
hard requirement). <em>Taints</em> are the opposite -- they allow a node to repel a set of pods.</p><p><em>Tolerations</em> are applied to pods. Tolerations allow the scheduler to schedule pods with matching
taints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also
<a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>evaluates other parameters</a>
as part of its function.</p><p>Taints and tolerations work together to ensure that pods are not scheduled
onto inappropriate nodes. One or more taints are applied to a node; this
marks that the node should not accept any pods that do not tolerate the taints.</p><h2 id=concepts>Concepts</h2><p>You add a taint to a node using <a href=/docs/reference/generated/kubectl/kubectl-commands#taint>kubectl taint</a>.
For example,</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</span></span></code></pre></div><p>places a taint on node <code>node1</code>. The taint has key <code>key1</code>, value <code>value1</code>, and taint effect <code>NoSchedule</code>.
This means that no pod will be able to schedule onto <code>node1</code> unless it has a matching toleration.</p><p>To remove the taint added by the command above, you can run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule-
</span></span></code></pre></div><p>You specify a toleration for a pod in the PodSpec. Both of the following tolerations "match" the
taint created by the <code>kubectl taint</code> line above, and thus a pod with either toleration would be able
to schedule onto <code>node1</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>Here's an example of a pod that uses tolerations:</p><div class=highlight><div class=copy-code-icon style=text-align:right><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/pods/pod-with-toleration.yaml download=pods/pod-with-toleration.yaml><code>pods/pod-with-toleration.yaml</code></a>
<img src=/images/copycode.svg style=max-height:24px;cursor:pointer onclick='copyCode("pods-pod-with-toleration-yaml")' title="Copy pods/pod-with-toleration.yaml to clipboard"></img></div><div class=includecode id=pods-pod-with-toleration-yaml><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;example-key&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div></div></div><p>The default value for <code>operator</code> is <code>Equal</code>.</p><p>A toleration "matches" a taint if the keys are the same and the effects are the same, and:</p><ul><li>the <code>operator</code> is <code>Exists</code> (in which case no <code>value</code> should be specified), or</li><li>the <code>operator</code> is <code>Equal</code> and the <code>value</code>s are equal.</li></ul><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>There are two special cases:</p><p>An empty <code>key</code> with operator <code>Exists</code> matches all keys, values and effects which means this
will tolerate everything.</p><p>An empty <code>effect</code> matches all effects with key <code>key1</code>.</p></div><p>The above example used <code>effect</code> of <code>NoSchedule</code>. Alternatively, you can use <code>effect</code> of <code>PreferNoSchedule</code>.
This is a "preference" or "soft" version of <code>NoSchedule</code> -- the system will <em>try</em> to avoid placing a
pod that does not tolerate the taint on the node, but it is not required. The third kind of <code>effect</code> is
<code>NoExecute</code>, described later.</p><p>You can put multiple taints on the same node and multiple tolerations on the same pod.
The way Kubernetes processes multiple taints and tolerations is like a filter: start
with all of a node's taints, then ignore the ones for which the pod has a matching toleration; the
remaining un-ignored taints have the indicated effects on the pod. In particular,</p><ul><li>if there is at least one un-ignored taint with effect <code>NoSchedule</code> then Kubernetes will not schedule
the pod onto that node</li><li>if there is no un-ignored taint with effect <code>NoSchedule</code> but there is at least one un-ignored taint with
effect <code>PreferNoSchedule</code> then Kubernetes will <em>try</em> to not schedule the pod onto the node</li><li>if there is at least one un-ignored taint with effect <code>NoExecute</code> then the pod will be evicted from
the node (if it is already running on the node), and will not be
scheduled onto the node (if it is not yet running on the node).</li></ul><p>For example, imagine you taint a node like this</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoSchedule
</span></span><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key1</span><span style=color:#666>=</span>value1:NoExecute
</span></span><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#b8860b>key2</span><span style=color:#666>=</span>value2:NoSchedule
</span></span></code></pre></div><p>And a pod has two tolerations:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>In this case, the pod will not be able to schedule onto the node, because there is no
toleration matching the third taint. But it will be able to continue running if it is
already running on the node when the taint is added, because the third taint is the only
one of the three that is not tolerated by the pod.</p><p>Normally, if a taint with effect <code>NoExecute</code> is added to a node, then any pods that do
not tolerate the taint will be evicted immediately, and pods that do tolerate the
taint will never be evicted. However, a toleration with <code>NoExecute</code> effect can specify
an optional <code>tolerationSeconds</code> field that dictates how long the pod will stay bound
to the node after the taint is added. For example,</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;key1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Equal&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;value1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>3600</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>means that if this pod is running and a matching taint is added to the node, then
the pod will stay bound to the node for 3600 seconds, and then be evicted. If the
taint is removed before that time, the pod will not be evicted.</p><h2 id=example-use-cases>Example Use Cases</h2><p>Taints and tolerations are a flexible way to steer pods <em>away</em> from nodes or evict
pods that shouldn't be running. A few of the use cases are</p><ul><li><p><strong>Dedicated Nodes</strong>: If you want to dedicate a set of nodes for exclusive use by
a particular set of users, you can add a taint to those nodes (say,
<code>kubectl taint nodes nodename dedicated=groupName:NoSchedule</code>) and then add a corresponding
toleration to their pods (this would be done most easily by writing a custom
<a href=/docs/reference/access-authn-authz/admission-controllers/>admission controller</a>).
The pods with the tolerations will then be allowed to use the tainted (dedicated) nodes as
well as any other nodes in the cluster. If you want to dedicate the nodes to them <em>and</em>
ensure they <em>only</em> use the dedicated nodes, then you should additionally add a label similar
to the taint to the same set of nodes (e.g. <code>dedicated=groupName</code>), and the admission
controller should additionally add a node affinity to require that the pods can only schedule
onto nodes labeled with <code>dedicated=groupName</code>.</p></li><li><p><strong>Nodes with Special Hardware</strong>: In a cluster where a small subset of nodes have specialized
hardware (for example GPUs), it is desirable to keep pods that don't need the specialized
hardware off of those nodes, thus leaving room for later-arriving pods that do need the
specialized hardware. This can be done by tainting the nodes that have the specialized
hardware (e.g. <code>kubectl taint nodes nodename special=true:NoSchedule</code> or
<code>kubectl taint nodes nodename special=true:PreferNoSchedule</code>) and adding a corresponding
toleration to pods that use the special hardware. As in the dedicated nodes use case,
it is probably easiest to apply the tolerations using a custom
<a href=/docs/reference/access-authn-authz/admission-controllers/>admission controller</a>.
For example, it is recommended to use <a href=/docs/concepts/configuration/manage-resources-containers/#extended-resources>Extended
Resources</a>
to represent the special hardware, taint your special hardware nodes with the
extended resource name and run the
<a href=/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration>ExtendedResourceToleration</a>
admission controller. Now, because the nodes are tainted, no pods without the
toleration will schedule on them. But when you submit a pod that requests the
extended resource, the <code>ExtendedResourceToleration</code> admission controller will
automatically add the correct toleration to the pod and that pod will schedule
on the special hardware nodes. This will make sure that these special hardware
nodes are dedicated for pods requesting such hardware and you don't have to
manually add tolerations to your pods.</p></li><li><p><strong>Taint based Evictions</strong>: A per-pod-configurable eviction behavior
when there are node problems, which is described in the next section.</p></li></ul><h2 id=taint-based-evictions>Taint based Evictions</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.18 [stable]</code></div><p>The <code>NoExecute</code> taint effect, mentioned above, affects pods that are already
running on the node as follows</p><ul><li>pods that do not tolerate the taint are evicted immediately</li><li>pods that tolerate the taint without specifying <code>tolerationSeconds</code> in
their toleration specification remain bound forever</li><li>pods that tolerate the taint with a specified <code>tolerationSeconds</code> remain
bound for the specified amount of time</li></ul><p>The node controller automatically taints a Node when certain conditions
are true. The following taints are built in:</p><ul><li><code>node.kubernetes.io/not-ready</code>: Node is not ready. This corresponds to
the NodeCondition <code>Ready</code> being "<code>False</code>".</li><li><code>node.kubernetes.io/unreachable</code>: Node is unreachable from the node
controller. This corresponds to the NodeCondition <code>Ready</code> being "<code>Unknown</code>".</li><li><code>node.kubernetes.io/memory-pressure</code>: Node has memory pressure.</li><li><code>node.kubernetes.io/disk-pressure</code>: Node has disk pressure.</li><li><code>node.kubernetes.io/pid-pressure</code>: Node has PID pressure.</li><li><code>node.kubernetes.io/network-unavailable</code>: Node's network is unavailable.</li><li><code>node.kubernetes.io/unschedulable</code>: Node is unschedulable.</li><li><code>node.cloudprovider.kubernetes.io/uninitialized</code>: When the kubelet is started
with "external" cloud provider, this taint is set on a node to mark it
as unusable. After a controller from the cloud-controller-manager initializes
this node, the kubelet removes this taint.</li></ul><p>In case a node is to be evicted, the node controller or the kubelet adds relevant taints
with <code>NoExecute</code> effect. If the fault condition returns to normal the kubelet or node
controller can remove the relevant taint(s).</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The control plane limits the rate of adding node new taints to nodes. This rate limiting
manages the number of evictions that are triggered when many nodes become unreachable at
once (for example: if there is a network disruption).</div><p>You can specify <code>tolerationSeconds</code> for a Pod to define how long that Pod stays bound
to a failing or unresponsive Node.</p><p>For example, you might want to keep an application with a lot of local state
bound to node for a long time in the event of network partition, hoping
that the partition will recover and thus the pod eviction can be avoided.
The toleration you set for that Pod might look like:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>tolerations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>key</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;node.kubernetes.io/unreachable&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>operator</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>effect</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;NoExecute&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>tolerationSeconds</span>:<span style=color:#bbb> </span><span style=color:#666>6000</span><span style=color:#bbb>
</span></span></span></code></pre></div><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>Kubernetes automatically adds a toleration for
<code>node.kubernetes.io/not-ready</code> and <code>node.kubernetes.io/unreachable</code>
with <code>tolerationSeconds=300</code>,
unless you, or a controller, set those tolerations explicitly.</p><p>These automatically-added tolerations mean that Pods remain bound to
Nodes for 5 minutes after one of these problems is detected.</p></div><p><a href=/docs/concepts/workloads/controllers/daemonset/>DaemonSet</a> pods are created with
<code>NoExecute</code> tolerations for the following taints with no <code>tolerationSeconds</code>:</p><ul><li><code>node.kubernetes.io/unreachable</code></li><li><code>node.kubernetes.io/not-ready</code></li></ul><p>This ensures that DaemonSet pods are never evicted due to these problems.</p><h2 id=taint-nodes-by-condition>Taint Nodes by Condition</h2><p>The control plane, using the node <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a>,
automatically creates taints with a <code>NoSchedule</code> effect for
<a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/#node-conditions>node conditions</a>.</p><p>The scheduler checks taints, not node conditions, when it makes scheduling
decisions. This ensures that node conditions don't directly affect scheduling.
For example, if the <code>DiskPressure</code> node condition is active, the control plane
adds the <code>node.kubernetes.io/disk-pressure</code> taint and does not schedule new pods
onto the affected node. If the <code>MemoryPressure</code> node condition is active, the
control plane adds the <code>node.kubernetes.io/memory-pressure</code> taint.</p><p>You can ignore node conditions for newly created pods by adding the corresponding
Pod tolerations. The control plane also adds the <code>node.kubernetes.io/memory-pressure</code>
toleration on pods that have a <a class=glossary-tooltip title='QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-qos-class' target=_blank aria-label='QoS class'>QoS class</a>
other than <code>BestEffort</code>. This is because Kubernetes treats pods in the <code>Guaranteed</code>
or <code>Burstable</code> QoS classes (even pods with no memory request set) as if they are
able to cope with memory pressure, while new <code>BestEffort</code> pods are not scheduled
onto the affected node.</p><p>The DaemonSet controller automatically adds the following <code>NoSchedule</code>
tolerations to all daemons, to prevent DaemonSets from breaking.</p><ul><li><code>node.kubernetes.io/memory-pressure</code></li><li><code>node.kubernetes.io/disk-pressure</code></li><li><code>node.kubernetes.io/pid-pressure</code> (1.14 or later)</li><li><code>node.kubernetes.io/unschedulable</code> (1.10 or later)</li><li><code>node.kubernetes.io/network-unavailable</code> (<em>host network only</em>)</li></ul><p>Adding these tolerations ensures backward compatibility. You can also add
arbitrary tolerations to DaemonSets.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a>
and how you can configure it</li><li>Read about <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-602208c95fe7b1f1170310ce993f5814>6 - Scheduling Framework</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.19 [stable]</code></div><p>The scheduling framework is a pluggable architecture for the Kubernetes scheduler.
It adds a new set of "plugin" APIs to the existing scheduler. Plugins are compiled into the scheduler. The APIs allow most scheduling features to be implemented as plugins, while keeping the
scheduling "core" lightweight and maintainable. Refer to the <a href=https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/624-scheduling-framework/README.md>design proposal of the
scheduling framework</a> for more technical information on the design of the
framework.</p><h1 id=framework-workflow>Framework workflow</h1><p>The Scheduling Framework defines a few extension points. Scheduler plugins
register to be invoked at one or more extension points. Some of these plugins
can change the scheduling decisions and some are informational only.</p><p>Each attempt to schedule one Pod is split into two phases, the <strong>scheduling
cycle</strong> and the <strong>binding cycle</strong>.</p><h2 id=scheduling-cycle-binding-cycle>Scheduling Cycle & Binding Cycle</h2><p>The scheduling cycle selects a node for the Pod, and the binding cycle applies
that decision to the cluster. Together, a scheduling cycle and binding cycle are
referred to as a "scheduling context".</p><p>Scheduling cycles are run serially, while binding cycles may run concurrently.</p><p>A scheduling or binding cycle can be aborted if the Pod is determined to
be unschedulable or if there is an internal error. The Pod will be returned to
the queue and retried.</p><h2 id=extension-points>Extension points</h2><p>The following picture shows the scheduling context of a Pod and the extension
points that the scheduling framework exposes. In this picture "Filter" is
equivalent to "Predicate" and "Scoring" is equivalent to "Priority function".</p><p>One plugin may register at multiple extension points to perform more complex or
stateful tasks.</p><figure class=diagram-large><img src=/images/docs/scheduling-framework-extensions.png><figcaption><h4>scheduling framework extension points</h4></figcaption></figure><h3 id=queue-sort>QueueSort</h3><p>These plugins are used to sort Pods in the scheduling queue. A queue sort plugin
essentially provides a <code>Less(Pod1, Pod2)</code> function. Only one queue sort
plugin may be enabled at a time.</p><h3 id=pre-filter>PreFilter</h3><p>These plugins are used to pre-process info about the Pod, or to check certain
conditions that the cluster or the Pod must meet. If a PreFilter plugin returns
an error, the scheduling cycle is aborted.</p><h3 id=filter>Filter</h3><p>These plugins are used to filter out nodes that cannot run the Pod. For each
node, the scheduler will call filter plugins in their configured order. If any
filter plugin marks the node as infeasible, the remaining plugins will not be
called for that node. Nodes may be evaluated concurrently.</p><h3 id=post-filter>PostFilter</h3><p>These plugins are called after Filter phase, but only when no feasible nodes
were found for the pod. Plugins are called in their configured order. If
any postFilter plugin marks the node as <code>Schedulable</code>, the remaining plugins
will not be called. A typical PostFilter implementation is preemption, which
tries to make the pod schedulable by preempting other Pods.</p><h3 id=pre-score>PreScore</h3><p>These plugins are used to perform "pre-scoring" work, which generates a sharable
state for Score plugins to use. If a PreScore plugin returns an error, the
scheduling cycle is aborted.</p><h3 id=scoring>Score</h3><p>These plugins are used to rank nodes that have passed the filtering phase. The
scheduler will call each scoring plugin for each node. There will be a well
defined range of integers representing the minimum and maximum scores. After the
<a href=#normalize-scoring>NormalizeScore</a> phase, the scheduler will combine node
scores from all plugins according to the configured plugin weights.</p><h3 id=normalize-scoring>NormalizeScore</h3><p>These plugins are used to modify scores before the scheduler computes a final
ranking of Nodes. A plugin that registers for this extension point will be
called with the <a href=#scoring>Score</a> results from the same plugin. This is called
once per plugin per scheduling cycle.</p><p>For example, suppose a plugin <code>BlinkingLightScorer</code> ranks Nodes based on how
many blinking lights they have.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>ScoreNode</span>(_ <span style=color:#666>*</span>v1.pod, n <span style=color:#666>*</span>v1.Node) (<span style=color:#0b0;font-weight:700>int</span>, <span style=color:#0b0;font-weight:700>error</span>) {
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>return</span> <span style=color:#00a000>getBlinkingLightCount</span>(n)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>However, the maximum count of blinking lights may be small compared to
<code>NodeScoreMax</code>. To fix this, <code>BlinkingLightScorer</code> should also register for this
extension point.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>func</span> <span style=color:#00a000>NormalizeScores</span>(scores <span style=color:#a2f;font-weight:700>map</span>[<span style=color:#0b0;font-weight:700>string</span>]<span style=color:#0b0;font-weight:700>int</span>) {
</span></span><span style=display:flex><span>    highest <span style=color:#666>:=</span> <span style=color:#666>0</span>
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> _, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
</span></span><span style=display:flex><span>        highest = <span style=color:#00a000>max</span>(highest, score)
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    <span style=color:#a2f;font-weight:700>for</span> node, score <span style=color:#666>:=</span> <span style=color:#a2f;font-weight:700>range</span> scores {
</span></span><span style=display:flex><span>        scores[node] = score<span style=color:#666>*</span>NodeScoreMax<span style=color:#666>/</span>highest
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If any NormalizeScore plugin returns an error, the scheduling cycle is
aborted.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Plugins wishing to perform "pre-reserve" work should use the
NormalizeScore extension point.</div><h3 id=reserve>Reserve</h3><p>A plugin that implements the Reserve extension has two methods, namely <code>Reserve</code>
and <code>Unreserve</code>, that back two informational scheduling phases called Reserve
and Unreserve, respectively. Plugins which maintain runtime state (aka "stateful
plugins") should use these phases to be notified by the scheduler when resources
on a node are being reserved and unreserved for a given Pod.</p><p>The Reserve phase happens before the scheduler actually binds a Pod to its
designated node. It exists to prevent race conditions while the scheduler waits
for the bind to succeed. The <code>Reserve</code> method of each Reserve plugin may succeed
or fail; if one <code>Reserve</code> method call fails, subsequent plugins are not executed
and the Reserve phase is considered to have failed. If the <code>Reserve</code> method of
all plugins succeed, the Reserve phase is considered to be successful and the
rest of the scheduling cycle and the binding cycle are executed.</p><p>The Unreserve phase is triggered if the Reserve phase or a later phase fails.
When this happens, the <code>Unreserve</code> method of <strong>all</strong> Reserve plugins will be
executed in the reverse order of <code>Reserve</code> method calls. This phase exists to
clean up the state associated with the reserved Pod.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> The implementation of the <code>Unreserve</code> method in Reserve plugins must be
idempotent and may not fail.</div><h3 id=permit>Permit</h3><p><em>Permit</em> plugins are invoked at the end of the scheduling cycle for each Pod, to
prevent or delay the binding to the candidate node. A permit plugin can do one of
the three things:</p><ol><li><p><strong>approve</strong><br>Once all Permit plugins approve a Pod, it is sent for binding.</p></li><li><p><strong>deny</strong><br>If any Permit plugin denies a Pod, it is returned to the scheduling queue.
This will trigger the Unreserve phase in <a href=#reserve>Reserve plugins</a>.</p></li><li><p><strong>wait</strong> (with a timeout)<br>If a Permit plugin returns "wait", then the Pod is kept in an internal "waiting"
Pods list, and the binding cycle of this Pod starts but directly blocks until it
gets approved. If a timeout occurs, <strong>wait</strong> becomes <strong>deny</strong>
and the Pod is returned to the scheduling queue, triggering the
Unreserve phase in <a href=#reserve>Reserve plugins</a>.</p></li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong> While any plugin can access the list of "waiting" Pods and approve them
(see <a href=https://git.k8s.io/enhancements/keps/sig-scheduling/624-scheduling-framework#frameworkhandle><code>FrameworkHandle</code></a>), we expect only the permit
plugins to approve binding of reserved Pods that are in "waiting" state. Once a Pod
is approved, it is sent to the <a href=#pre-bind>PreBind</a> phase.</div><h3 id=pre-bind>PreBind</h3><p>These plugins are used to perform any work required before a Pod is bound. For
example, a pre-bind plugin may provision a network volume and mount it on the
target node before allowing the Pod to run there.</p><p>If any PreBind plugin returns an error, the Pod is <a href=#reserve>rejected</a> and
returned to the scheduling queue.</p><h3 id=bind>Bind</h3><p>These plugins are used to bind a Pod to a Node. Bind plugins will not be called
until all PreBind plugins have completed. Each bind plugin is called in the
configured order. A bind plugin may choose whether or not to handle the given
Pod. If a bind plugin chooses to handle a Pod, <strong>the remaining bind plugins are
skipped</strong>.</p><h3 id=post-bind>PostBind</h3><p>This is an informational extension point. Post-bind plugins are called after a
Pod is successfully bound. This is the end of a binding cycle, and can be used
to clean up associated resources.</p><h2 id=plugin-api>Plugin API</h2><p>There are two steps to the plugin API. First, plugins must register and get
configured, then they use the extension point interfaces. Extension point
interfaces have the following form.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#a2f;font-weight:700>type</span> Plugin <span style=color:#a2f;font-weight:700>interface</span> {
</span></span><span style=display:flex><span>    <span style=color:#00a000>Name</span>() <span style=color:#0b0;font-weight:700>string</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>type</span> QueueSortPlugin <span style=color:#a2f;font-weight:700>interface</span> {
</span></span><span style=display:flex><span>    Plugin
</span></span><span style=display:flex><span>    <span style=color:#00a000>Less</span>(<span style=color:#666>*</span>v1.pod, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>bool</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a2f;font-weight:700>type</span> PreFilterPlugin <span style=color:#a2f;font-weight:700>interface</span> {
</span></span><span style=display:flex><span>    Plugin
</span></span><span style=display:flex><span>    <span style=color:#00a000>PreFilter</span>(context.Context, <span style=color:#666>*</span>framework.CycleState, <span style=color:#666>*</span>v1.pod) <span style=color:#0b0;font-weight:700>error</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#080;font-style:italic>// ...
</span></span></span></code></pre></div><h2 id=plugin-configuration>Plugin configuration</h2><p>You can enable or disable plugins in the scheduler configuration. If you are using
Kubernetes v1.18 or later, most scheduling
<a href=/docs/reference/scheduling/config/#scheduling-plugins>plugins</a> are in use and
enabled by default.</p><p>In addition to default plugins, you can also implement your own scheduling
plugins and get them configured along with default plugins. You can visit
<a href=https://github.com/kubernetes-sigs/scheduler-plugins>scheduler-plugins</a> for more details.</p><p>If you are using Kubernetes v1.18 or later, you can configure a set of plugins as
a scheduler profile and then define multiple profiles to fit various kinds of workload.
Learn more at <a href=/docs/reference/scheduling/config/#multiple-profiles>multiple profiles</a>.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-d9574a30fcbc631b0d2a57850e161e89>7 - Scheduler Performance Tuning</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [beta]</code></div><p><a href=/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler>kube-scheduler</a>
is the Kubernetes default scheduler. It is responsible for placement of Pods
on Nodes in a cluster.</p><p>Nodes in a cluster that meet the scheduling requirements of a Pod are
called <em>feasible</em> Nodes for the Pod. The scheduler finds feasible Nodes
for a Pod and then runs a set of functions to score the feasible Nodes,
picking a Node with the highest score among the feasible ones to run
the Pod. The scheduler then notifies the API server about this decision
in a process called <em>Binding</em>.</p><p>This page explains performance tuning optimizations that are relevant for
large Kubernetes clusters.</p><p>In large clusters, you can tune the scheduler's behaviour balancing
scheduling outcomes between latency (new Pods are placed quickly) and
accuracy (the scheduler rarely makes poor placement decisions).</p><p>You configure this tuning setting via kube-scheduler setting
<code>percentageOfNodesToScore</code>. This KubeSchedulerConfiguration setting determines
a threshold for scheduling nodes in your cluster.</p><h3 id=setting-the-threshold>Setting the threshold</h3><p>The <code>percentageOfNodesToScore</code> option accepts whole numeric values between 0
and 100. The value 0 is a special number which indicates that the kube-scheduler
should use its compiled-in default.
If you set <code>percentageOfNodesToScore</code> above 100, kube-scheduler acts as if you
had set a value of 100.</p><p>To change the value, edit the
<a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler configuration file</a>
and then restart the scheduler.
In many cases, the configuration file can be found at <code>/etc/kubernetes/config/kube-scheduler.yaml</code>.</p><p>After you have made this change, you can run</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl get pods -n kube-system | grep kube-scheduler
</span></span></code></pre></div><p>to verify that the kube-scheduler component is healthy.</p><h2 id=percentage-of-nodes-to-score>Node scoring threshold</h2><p>To improve scheduling performance, the kube-scheduler can stop looking for
feasible nodes once it has found enough of them. In large clusters, this saves
time compared to a naive approach that would consider every node.</p><p>You specify a threshold for how many nodes are enough, as a whole number percentage
of all the nodes in your cluster. The kube-scheduler converts this into an
integer number of nodes. During scheduling, if the kube-scheduler has identified
enough feasible nodes to exceed the configured percentage, the kube-scheduler
stops searching for more feasible nodes and moves on to the
<a href=/docs/concepts/scheduling-eviction/kube-scheduler/#kube-scheduler-implementation>scoring phase</a>.</p><p><a href=#how-the-scheduler-iterates-over-nodes>How the scheduler iterates over Nodes</a>
describes the process in detail.</p><h3 id=default-threshold>Default threshold</h3><p>If you don't specify a threshold, Kubernetes calculates a figure using a
linear formula that yields 50% for a 100-node cluster and yields 10%
for a 5000-node cluster. The lower bound for the automatic value is 5%.</p><p>This means that, the kube-scheduler always scores at least 5% of your cluster no
matter how large the cluster is, unless you have explicitly set
<code>percentageOfNodesToScore</code> to be smaller than 5.</p><p>If you want the scheduler to score all nodes in your cluster, set
<code>percentageOfNodesToScore</code> to 100.</p><h2 id=example>Example</h2><p>Below is an example configuration that sets <code>percentageOfNodesToScore</code> to 50%.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1alpha1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>algorithmSource</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>provider</span>:<span style=color:#bbb> </span>DefaultProvider<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#00f;font-weight:700>...</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>percentageOfNodesToScore</span>:<span style=color:#bbb> </span><span style=color:#666>50</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=tuning-percentageofnodestoscore>Tuning percentageOfNodesToScore</h2><p><code>percentageOfNodesToScore</code> must be a value between 1 and 100 with the default
value being calculated based on the cluster size. There is also a hardcoded
minimum value of 50 nodes.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><p>In clusters with less than 50 feasible nodes, the scheduler still
checks all the nodes because there are not enough feasible nodes to stop
the scheduler's search early.</p><p>In a small cluster, if you set a low value for <code>percentageOfNodesToScore</code>, your
change will have no or little effect, for a similar reason.</p><p>If your cluster has several hundred Nodes or fewer, leave this configuration option
at its default value. Making changes is unlikely to improve the
scheduler's performance significantly.</p></div><p>An important detail to consider when setting this value is that when a smaller
number of nodes in a cluster are checked for feasibility, some nodes are not
sent to be scored for a given Pod. As a result, a Node which could possibly
score a higher value for running the given Pod might not even be passed to the
scoring phase. This would result in a less than ideal placement of the Pod.</p><p>You should avoid setting <code>percentageOfNodesToScore</code> very low so that kube-scheduler
does not make frequent, poor Pod placement decisions. Avoid setting the
percentage to anything below 10%, unless the scheduler's throughput is critical
for your application and the score of nodes is not important. In other words, you
prefer to run the Pod on any Node as long as it is feasible.</p><h2 id=how-the-scheduler-iterates-over-nodes>How the scheduler iterates over Nodes</h2><p>This section is intended for those who want to understand the internal details
of this feature.</p><p>In order to give all the Nodes in a cluster a fair chance of being considered
for running Pods, the scheduler iterates over the nodes in a round robin
fashion. You can imagine that Nodes are in an array. The scheduler starts from
the start of the array and checks feasibility of the nodes until it finds enough
Nodes as specified by <code>percentageOfNodesToScore</code>. For the next Pod, the
scheduler continues from the point in the Node array that it stopped at when
checking feasibility of Nodes for the previous Pod.</p><p>If Nodes are in multiple zones, the scheduler iterates over Nodes in various
zones to ensure that Nodes from different zones are considered in the
feasibility checks. As an example, consider six nodes in two zones:</p><pre tabindex=0><code>Zone 1: Node 1, Node 2, Node 3, Node 4
Zone 2: Node 5, Node 6
</code></pre><p>The Scheduler evaluates feasibility of the nodes in this order:</p><pre tabindex=0><code>Node 1, Node 5, Node 2, Node 6, Node 3, Node 4
</code></pre><p>After going over all the Nodes, it goes back to Node 1.</p><h2 id=what-s-next>What's next</h2><ul><li>Check the <a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/>kube-scheduler configuration reference (v1beta3)</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-961126cd43559012893979e568396a49>8 - Resource Bin Packing</h1><p>In the <a href=/docs/reference/scheduling/config/#scheduling-plugins>scheduling-plugin</a> <code>NodeResourcesFit</code> of kube-scheduler, there are two
scoring strategies that support the bin packing of resources: <code>MostAllocated</code> and <code>RequestedToCapacityRatio</code>.</p><h2 id=enabling-bin-packing-using-mostallocated-strategy>Enabling bin packing using MostAllocated strategy</h2><p>The <code>MostAllocated</code> strategy scores the nodes based on the utilization of resources, favoring the ones with higher allocation.
For each resource type, you can set a weight to modify its influence in the node score.</p><p>To set the <code>MostAllocated</code> strategy for the <code>NodeResourcesFit</code> plugin, use a
<a href=/docs/reference/scheduling/config>scheduler configuration</a> similar to the following:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>scoringStrategy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>MostAllocated<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeResourcesFit<span style=color:#bbb>
</span></span></span></code></pre></div><p>To learn more about other parameters and their default configuration, see the API documentation for
<a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-NodeResourcesFitArgs><code>NodeResourcesFitArgs</code></a>.</p><h2 id=enabling-bin-packing-using-requestedtocapacityratio>Enabling bin packing using RequestedToCapacityRatio</h2><p>The <code>RequestedToCapacityRatio</code> strategy allows the users to specify the resources along with weights for
each resource to score nodes based on the request to capacity ratio. This
allows users to bin pack extended resources by using appropriate parameters
to improve the utilization of scarce resources in large clusters. It favors nodes according to a
configured function of the allocated resources. The behavior of the <code>RequestedToCapacityRatio</code> in
the <code>NodeResourcesFit</code> score function can be controlled by the
<a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-ScoringStrategy>scoringStrategy</a> field.
Within the <code>scoringStrategy</code> field, you can configure two parameters: <code>requestedToCapacityRatio</code> and
<code>resources</code>. The <code>shape</code> in the <code>requestedToCapacityRatio</code>
parameter allows the user to tune the function as least requested or most
requested based on <code>utilization</code> and <code>score</code> values. The <code>resources</code> parameter
consists of <code>name</code> of the resource to be considered during scoring and <code>weight</code>
specify the weight of each resource.</p><p>Below is an example configuration that sets
the bin packing behavior for extended resources <code>intel.com/foo</code> and <code>intel.com/bar</code>
using the <code>requestedToCapacityRatio</code> field.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubescheduler.config.k8s.io/v1beta3<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeSchedulerConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>profiles</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span>- <span style=color:green;font-weight:700>pluginConfig</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:green;font-weight:700>scoringStrategy</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/bar<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>requestedToCapacityRatio</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:green;font-weight:700>type</span>:<span style=color:#bbb> </span>RequestedToCapacityRatio<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>NodeResourcesFit<span style=color:#bbb>
</span></span></span></code></pre></div><p>Referencing the <code>KubeSchedulerConfiguration</code> file with the kube-scheduler
flag <code>--config=/path/to/config/file</code> will pass the configuration to the
scheduler.</p><p>To learn more about other parameters and their default configuration, see the API documentation for
<a href=/docs/reference/config-api/kube-scheduler-config.v1beta3/#kubescheduler-config-k8s-io-v1beta3-NodeResourcesFitArgs><code>NodeResourcesFitArgs</code></a>.</p><h3 id=tuning-the-score-function>Tuning the score function</h3><p><code>shape</code> is used to specify the behavior of the <code>RequestedToCapacityRatio</code> function.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb> </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>   </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The above arguments give the node a <code>score</code> of 0 if <code>utilization</code> is 0% and 10 for
<code>utilization</code> 100%, thus enabling bin packing behavior. To enable least
requested the score value must be reversed as follows.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>shape</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>10</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>utilization</span>:<span style=color:#bbb> </span><span style=color:#666>100</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>score</span>:<span style=color:#bbb> </span><span style=color:#666>0</span><span style=color:#bbb>
</span></span></span></code></pre></div><p><code>resources</code> is an optional parameter which defaults to:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>It can be used to add extended resources as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>intel.com/foo<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>5</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>cpu<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>3</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>memory<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>weight</span>:<span style=color:#bbb> </span><span style=color:#666>1</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>The <code>weight</code> parameter is optional and is set to 1 if not specified. Also, the
<code>weight</code> cannot be set to a negative value.</p><h3 id=node-scoring-for-capacity-allocation>Node scoring for capacity allocation</h3><p>This section is intended for those who want to understand the internal details
of this feature.
Below is an example of how the node score is calculated for a given set of values.</p><p>Requested resources:</p><pre tabindex=0><code>intel.com/foo : 2
memory: 256MB
cpu: 2
</code></pre><p>Resource weights:</p><pre tabindex=0><code>intel.com/foo : 5
memory: 1
cpu: 3
</code></pre><p>FunctionShapePoint {{0, 0}, {100, 10}}</p><p>Node 1 spec:</p><pre tabindex=0><code>Available:
  intel.com/foo: 4
  memory: 1 GB
  cpu: 8

Used:
  intel.com/foo: 1
  memory: 256MB
  cpu: 1
</code></pre><p>Node score:</p><pre tabindex=0><code>intel.com/foo  = resourceScoringFunction((2+1),4)
               = (100 - ((4-3)*100/4)
               = (100 - 25)
               = 75                       # requested + used = 75% * available
               = rawScoringFunction(75) 
               = 7                        # floor(75/10) 

memory         = resourceScoringFunction((256+256),1024)
               = (100 -((1024-512)*100/1024))
               = 50                       # requested + used = 50% * available
               = rawScoringFunction(50)
               = 5                        # floor(50/10)

cpu            = resourceScoringFunction((2+1),8)
               = (100 -((8-3)*100/8))
               = 37.5                     # requested + used = 37.5% * available
               = rawScoringFunction(37.5)
               = 3                        # floor(37.5/10)

NodeScore   =  (7 * 5) + (5 * 1) + (3 * 3) / (5 + 1 + 3)
            =  5
</code></pre><p>Node 2 spec:</p><pre tabindex=0><code>Available:
  intel.com/foo: 8
  memory: 1GB
  cpu: 8
Used:
  intel.com/foo: 2
  memory: 512MB
  cpu: 6
</code></pre><p>Node score:</p><pre tabindex=0><code>intel.com/foo  = resourceScoringFunction((2+2),8)
               =  (100 - ((8-4)*100/8)
               =  (100 - 50)
               =  50
               =  rawScoringFunction(50)
               = 5

memory         = resourceScoringFunction((256+512),1024)
               = (100 -((1024-768)*100/1024))
               = 75
               = rawScoringFunction(75)
               = 7

cpu            = resourceScoringFunction((2+6),8)
               = (100 -((8-8)*100/8))
               = 100
               = rawScoringFunction(100)
               = 10

NodeScore   =  (5 * 5) + (7 * 1) + (10 * 3) / (5 + 1 + 3)
            =  7
</code></pre><h2 id=what-s-next>What's next</h2><ul><li>Read more about the <a href=/docs/concepts/scheduling-eviction/scheduling-framework/>scheduling framework</a></li><li>Read more about <a href=/docs/reference/scheduling/config/>scheduler configuration</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-60e5a2861609e0848d58ce8bf99c4a31>9 - Pod Priority and Preemption</h1><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.14 [stable]</code></div><p><a href=/docs/concepts/workloads/pods/>Pods</a> can have <em>priority</em>. Priority indicates the
importance of a Pod relative to other Pods. If a Pod cannot be scheduled, the
scheduler tries to preempt (evict) lower priority Pods to make scheduling of the
pending Pod possible.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong><p>In a cluster where not all users are trusted, a malicious user could create Pods
at the highest possible priorities, causing other Pods to be evicted/not get
scheduled.
An administrator can use ResourceQuota to prevent users from creating pods at
high priorities.</p><p>See <a href=/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default>limit Priority Class consumption by default</a>
for details.</p></div><h2 id=how-to-use-priority-and-preemption>How to use priority and preemption</h2><p>To use priority and preemption:</p><ol><li><p>Add one or more <a href=#priorityclass>PriorityClasses</a>.</p></li><li><p>Create Pods with<a href=#pod-priority><code>priorityClassName</code></a> set to one of the added
PriorityClasses. Of course you do not need to create the Pods directly;
normally you would add <code>priorityClassName</code> to the Pod template of a
collection object like a Deployment.</p></li></ol><p>Keep reading for more information about these steps.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubernetes already ships with two PriorityClasses:
<code>system-cluster-critical</code> and <code>system-node-critical</code>.
These are common classes and are used to <a href=/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/>ensure that critical components are always scheduled first</a>.</div><h2 id=priorityclass>PriorityClass</h2><p>A PriorityClass is a non-namespaced object that defines a mapping from a
priority class name to the integer value of the priority. The name is specified
in the <code>name</code> field of the PriorityClass object's metadata. The value is
specified in the required <code>value</code> field. The higher the value, the higher the
priority.
The name of a PriorityClass object must be a valid
<a href=/docs/concepts/overview/working-with-objects/names#dns-subdomain-names>DNS subdomain name</a>,
and it cannot be prefixed with <code>system-</code>.</p><p>A PriorityClass object can have any 32-bit integer value smaller than or equal
to 1 billion. Larger numbers are reserved for critical system Pods that should
not normally be preempted or evicted. A cluster admin should create one
PriorityClass object for each such mapping that they want.</p><p>PriorityClass also has two optional fields: <code>globalDefault</code> and <code>description</code>.
The <code>globalDefault</code> field indicates that the value of this PriorityClass should
be used for Pods without a <code>priorityClassName</code>. Only one PriorityClass with
<code>globalDefault</code> set to true can exist in the system. If there is no
PriorityClass with <code>globalDefault</code> set, the priority of Pods with no
<code>priorityClassName</code> is zero.</p><p>The <code>description</code> field is an arbitrary string. It is meant to tell users of the
cluster when they should use this PriorityClass.</p><h3 id=notes-about-podpriority-and-existing-clusters>Notes about PodPriority and existing clusters</h3><ul><li><p>If you upgrade an existing cluster without this feature, the priority
of your existing Pods is effectively zero.</p></li><li><p>Addition of a PriorityClass with <code>globalDefault</code> set to <code>true</code> does not
change the priorities of existing Pods. The value of such a PriorityClass is
used only for Pods created after the PriorityClass is added.</p></li><li><p>If you delete a PriorityClass, existing Pods that use the name of the
deleted PriorityClass remain unchanged, but you cannot create more Pods that
use the name of the deleted PriorityClass.</p></li></ul><h3 id=example-priorityclass>Example PriorityClass</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>scheduling.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#666>1000000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>globalDefault</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>description</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;This priority class should be used for XYZ service pods only.&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=non-preempting-priority-class>Non-preempting PriorityClass</h2><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.24 [stable]</code></div><p>Pods with <code>preemptionPolicy: Never</code> will be placed in the scheduling queue
ahead of lower-priority pods,
but they cannot preempt other pods.
A non-preempting pod waiting to be scheduled will stay in the scheduling queue,
until sufficient resources are free,
and it can be scheduled.
Non-preempting pods,
like other pods,
are subject to scheduler back-off.
This means that if the scheduler tries these pods and they cannot be scheduled,
they will be retried with lower frequency,
allowing other pods with lower priority to be scheduled before them.</p><p>Non-preempting pods may still be preempted by other,
high-priority pods.</p><p><code>preemptionPolicy</code> defaults to <code>PreemptLowerPriority</code>,
which will allow pods of that PriorityClass to preempt lower-priority pods
(as is existing default behavior).
If <code>preemptionPolicy</code> is set to <code>Never</code>,
pods in that PriorityClass will be non-preempting.</p><p>An example use case is for data science workloads.
A user may submit a job that they want to be prioritized above other workloads,
but do not wish to discard existing work by preempting running pods.
The high priority job with <code>preemptionPolicy: Never</code> will be scheduled
ahead of other queued pods,
as soon as sufficient cluster resources "naturally" become free.</p><h3 id=example-non-preempting-priorityclass>Example Non-preempting PriorityClass</h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>scheduling.k8s.io/v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>PriorityClass<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>high-priority-nonpreempting<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>value</span>:<span style=color:#bbb> </span><span style=color:#666>1000000</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>preemptionPolicy</span>:<span style=color:#bbb> </span>Never<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>globalDefault</span>:<span style=color:#bbb> </span><span style=color:#a2f;font-weight:700>false</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>description</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;This priority class will not cause other pods to be preempted.&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h2 id=pod-priority>Pod priority</h2><p>After you have one or more PriorityClasses, you can create Pods that specify one
of those PriorityClass names in their specifications. The priority admission
controller uses the <code>priorityClassName</code> field and populates the integer value of
the priority. If the priority class is not found, the Pod is rejected.</p><p>The following YAML is an example of a Pod configuration that uses the
PriorityClass created in the preceding example. The priority admission
controller checks the specification and resolves the priority of the Pod to
1000000.</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>v1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>Pod<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>labels</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>env</span>:<span style=color:#bbb> </span>test<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:green;font-weight:700>name</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>image</span>:<span style=color:#bbb> </span>nginx<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:green;font-weight:700>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>priorityClassName</span>:<span style=color:#bbb> </span>high-priority<span style=color:#bbb>
</span></span></span></code></pre></div><h3 id=effect-of-pod-priority-on-scheduling-order>Effect of Pod priority on scheduling order</h3><p>When Pod priority is enabled, the scheduler orders pending Pods by
their priority and a pending Pod is placed ahead of other pending Pods
with lower priority in the scheduling queue. As a result, the higher
priority Pod may be scheduled sooner than Pods with lower priority if
its scheduling requirements are met. If such Pod cannot be scheduled,
scheduler will continue and tries to schedule other lower priority Pods.</p><h2 id=preemption>Preemption</h2><p>When Pods are created, they go to a queue and wait to be scheduled. The
scheduler picks a Pod from the queue and tries to schedule it on a Node. If no
Node is found that satisfies all the specified requirements of the Pod,
preemption logic is triggered for the pending Pod. Let's call the pending Pod P.
Preemption logic tries to find a Node where removal of one or more Pods with
lower priority than P would enable P to be scheduled on that Node. If such a
Node is found, one or more lower priority Pods get evicted from the Node. After
the Pods are gone, P can be scheduled on the Node.</p><h3 id=user-exposed-information>User exposed information</h3><p>When Pod P preempts one or more Pods on Node N, <code>nominatedNodeName</code> field of Pod
P's status is set to the name of Node N. This field helps scheduler track
resources reserved for Pod P and also gives users information about preemptions
in their clusters.</p><p>Please note that Pod P is not necessarily scheduled to the "nominated Node".
The scheduler always tries the "nominated Node" before iterating over any other nodes.
After victim Pods are preempted, they get their graceful termination period. If
another node becomes available while scheduler is waiting for the victim Pods to
terminate, scheduler may use the other node to schedule Pod P. As a result
<code>nominatedNodeName</code> and <code>nodeName</code> of Pod spec are not always the same. Also, if
scheduler preempts Pods on Node N, but then a higher priority Pod than Pod P
arrives, scheduler may give Node N to the new higher priority Pod. In such a
case, scheduler clears <code>nominatedNodeName</code> of Pod P. By doing this, scheduler
makes Pod P eligible to preempt Pods on another Node.</p><h3 id=limitations-of-preemption>Limitations of preemption</h3><h4 id=graceful-termination-of-preemption-victims>Graceful termination of preemption victims</h4><p>When Pods are preempted, the victims get their
<a href=/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination>graceful termination period</a>.
They have that much time to finish their work and exit. If they don't, they are
killed. This graceful termination period creates a time gap between the point
that the scheduler preempts Pods and the time when the pending Pod (P) can be
scheduled on the Node (N). In the meantime, the scheduler keeps scheduling other
pending Pods. As victims exit or get terminated, the scheduler tries to schedule
Pods in the pending queue. Therefore, there is usually a time gap between the
point that scheduler preempts victims and the time that Pod P is scheduled. In
order to minimize this gap, one can set graceful termination period of lower
priority Pods to zero or a small number.</p><h4 id=poddisruptionbudget-is-supported-but-not-guaranteed>PodDisruptionBudget is supported, but not guaranteed</h4><p>A <a href=/docs/concepts/workloads/pods/disruptions/>PodDisruptionBudget</a> (PDB)
allows application owners to limit the number of Pods of a replicated application
that are down simultaneously from voluntary disruptions. Kubernetes supports
PDB when preempting Pods, but respecting PDB is best effort. The scheduler tries
to find victims whose PDB are not violated by preemption, but if no such victims
are found, preemption will still happen, and lower priority Pods will be removed
despite their PDBs being violated.</p><h4 id=inter-pod-affinity-on-lower-priority-pods>Inter-Pod affinity on lower-priority Pods</h4><p>A Node is considered for preemption only when the answer to this question is
yes: "If all the Pods with lower priority than the pending Pod are removed from
the Node, can the pending Pod be scheduled on the Node?"</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Preemption does not necessarily remove all lower-priority
Pods. If the pending Pod can be scheduled by removing fewer than all
lower-priority Pods, then only a portion of the lower-priority Pods are removed.
Even so, the answer to the preceding question must be yes. If the answer is no,
the Node is not considered for preemption.</div><p>If a pending Pod has inter-pod <a class=glossary-tooltip title='Rules used by the scheduler to determine where to place pods' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity target=_blank aria-label=affinity>affinity</a>
to one or more of the lower-priority Pods on the Node, the inter-Pod affinity
rule cannot be satisfied in the absence of those lower-priority Pods. In this case,
the scheduler does not preempt any Pods on the Node. Instead, it looks for another
Node. The scheduler might find a suitable Node or it might not. There is no
guarantee that the pending Pod can be scheduled.</p><p>Our recommended solution for this problem is to create inter-Pod affinity only
towards equal or higher priority Pods.</p><h4 id=cross-node-preemption>Cross node preemption</h4><p>Suppose a Node N is being considered for preemption so that a pending Pod P can
be scheduled on N. P might become feasible on N only if a Pod on another Node is
preempted. Here's an example:</p><ul><li>Pod P is being considered for Node N.</li><li>Pod Q is running on another Node in the same Zone as Node N.</li><li>Pod P has Zone-wide anti-affinity with Pod Q (<code>topologyKey: topology.kubernetes.io/zone</code>).</li><li>There are no other cases of anti-affinity between Pod P and other Pods in
the Zone.</li><li>In order to schedule Pod P on Node N, Pod Q can be preempted, but scheduler
does not perform cross-node preemption. So, Pod P will be deemed
unschedulable on Node N.</li></ul><p>If Pod Q were removed from its Node, the Pod anti-affinity violation would be
gone, and Pod P could possibly be scheduled on Node N.</p><p>We may consider adding cross Node preemption in future versions if there is
enough demand and if we find an algorithm with reasonable performance.</p><h2 id=troubleshooting>Troubleshooting</h2><p>Pod priority and pre-emption can have unwanted side effects. Here are some
examples of potential problems and ways to deal with them.</p><h3 id=pods-are-preempted-unnecessarily>Pods are preempted unnecessarily</h3><p>Preemption removes existing Pods from a cluster under resource pressure to make
room for higher priority pending Pods. If you give high priorities to
certain Pods by mistake, these unintentionally high priority Pods may cause
preemption in your cluster. Pod priority is specified by setting the
<code>priorityClassName</code> field in the Pod's specification. The integer value for
priority is then resolved and populated to the <code>priority</code> field of <code>podSpec</code>.</p><p>To address the problem, you can change the <code>priorityClassName</code> for those Pods
to use lower priority classes, or leave that field empty. An empty
<code>priorityClassName</code> is resolved to zero by default.</p><p>When a Pod is preempted, there will be events recorded for the preempted Pod.
Preemption should happen only when a cluster does not have enough resources for
a Pod. In such cases, preemption happens only when the priority of the pending
Pod (preemptor) is higher than the victim Pods. Preemption must not happen when
there is no pending Pod, or when the pending Pods have equal or lower priority
than the victims. If preemption happens in such scenarios, please file an issue.</p><h3 id=pods-are-preempted-but-the-preemptor-is-not-scheduled>Pods are preempted, but the preemptor is not scheduled</h3><p>When pods are preempted, they receive their requested graceful termination
period, which is by default 30 seconds. If the victim Pods do not terminate within
this period, they are forcibly terminated. Once all the victims go away, the
preemptor Pod can be scheduled.</p><p>While the preemptor Pod is waiting for the victims to go away, a higher priority
Pod may be created that fits on the same Node. In this case, the scheduler will
schedule the higher priority Pod instead of the preemptor.</p><p>This is expected behavior: the Pod with the higher priority should take the place
of a Pod with a lower priority.</p><h3 id=higher-priority-pods-are-preempted-before-lower-priority-pods>Higher priority Pods are preempted before lower priority pods</h3><p>The scheduler tries to find nodes that can run a pending Pod. If no node is
found, the scheduler tries to remove Pods with lower priority from an arbitrary
node in order to make room for the pending pod.
If a node with low priority Pods is not feasible to run the pending Pod, the scheduler
may choose another node with higher priority Pods (compared to the Pods on the
other node) for preemption. The victims must still have lower priority than the
preemptor Pod.</p><p>When there are multiple nodes available for preemption, the scheduler tries to
choose the node with a set of Pods with lowest priority. However, if such Pods
have PodDisruptionBudget that would be violated if they are preempted then the
scheduler may choose another node with higher priority Pods.</p><p>When multiple nodes exist for preemption and none of the above scenarios apply,
the scheduler chooses a node with the lowest priority.</p><h2 id=interactions-of-pod-priority-and-qos>Interactions between Pod priority and quality of service</h2><p>Pod priority and <a class=glossary-tooltip title='QoS Class (Quality of Service Class) provides a way for Kubernetes to classify pods within the cluster into several classes and make decisions about scheduling and eviction.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-qos-class' target=_blank aria-label='QoS class'>QoS class</a>
are two orthogonal features with few interactions and no default restrictions on
setting the priority of a Pod based on its QoS classes. The scheduler's
preemption logic does not consider QoS when choosing preemption targets.
Preemption considers Pod priority and attempts to choose a set of targets with
the lowest priority. Higher-priority Pods are considered for preemption only if
the removal of the lowest priority Pods is not sufficient to allow the scheduler
to schedule the preemptor Pod, or if the lowest priority Pods are protected by
<code>PodDisruptionBudget</code>.</p><p>The kubelet uses Priority to determine pod order for <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>node-pressure eviction</a>.
You can use the QoS class to estimate the order in which pods are most likely
to get evicted. The kubelet ranks pods for eviction based on the following factors:</p><ol><li>Whether the starved resource usage exceeds requests</li><li>Pod Priority</li><li>Amount of resource usage relative to requests</li></ol><p>See <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction>Pod selection for kubelet eviction</a>
for more details.</p><p>kubelet node-pressure eviction does not evict Pods when their
usage does not exceed their requests. If a Pod with lower priority is not
exceeding its requests, it won't be evicted. Another Pod with higher priority
that exceeds its requests may be evicted.</p><h2 id=what-s-next>What's next</h2><ul><li>Read about using ResourceQuotas in connection with PriorityClasses: <a href=/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default>limit Priority Class consumption by default</a></li><li>Learn about <a href=/docs/concepts/workloads/pods/disruptions/>Pod Disruption</a></li><li>Learn about <a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated Eviction</a></li><li>Learn about <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-78e0431b4b7516092662a7c289cbb304>10 - Node-pressure Eviction</h1><p>Node-pressure eviction is the process by which the <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> proactively terminates
pods to reclaim resources on nodes.</br></p><p>The <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> monitors resources
like memory, disk space, and filesystem inodes on your cluster's nodes.
When one or more of these resources reach specific consumption levels, the
kubelet can proactively fail one or more pods on the node to reclaim resources
and prevent starvation.</p><p>During a node-pressure eviction, the kubelet sets the <code>PodPhase</code> for the
selected pods to <code>Failed</code>. This terminates the pods.</p><p>Node-pressure eviction is not the same as
<a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated eviction</a>.</p><p>The kubelet does not respect your configured <code>PodDisruptionBudget</code> or the pod's
<code>terminationGracePeriodSeconds</code>. If you use <a href=#soft-eviction-thresholds>soft eviction thresholds</a>,
the kubelet respects your configured <code>eviction-max-pod-grace-period</code>. If you use
<a href=#hard-eviction-thresholds>hard eviction thresholds</a>, it uses a <code>0s</code> grace period for termination.</p><p>If the pods are managed by a <a class=glossary-tooltip title='A workload is an application running on Kubernetes.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/ target=_blank aria-label=workload>workload</a>
resource (such as <a class=glossary-tooltip title='Manages deployment and scaling of a set of Pods, with durable storage and persistent identifiers for each Pod.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/statefulset/ target=_blank aria-label=StatefulSet>StatefulSet</a>
or <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a>) that
replaces failed pods, the control plane or <code>kube-controller-manager</code> creates new
pods in place of the evicted pods.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The kubelet attempts to <a href=#reclaim-node-resources>reclaim node-level resources</a>
before it terminates end-user pods. For example, it removes unused container
images when disk resources are starved.</div><p>The kubelet uses various parameters to make eviction decisions, like the following:</p><ul><li>Eviction signals</li><li>Eviction thresholds</li><li>Monitoring intervals</li></ul><h3 id=eviction-signals>Eviction signals</h3><p>Eviction signals are the current state of a particular resource at a specific
point in time. Kubelet uses eviction signals to make eviction decisions by
comparing the signals to eviction thresholds, which are the minimum amount of
the resource that should be available on the node.</p><p>Kubelet uses the following eviction signals:</p><table><thead><tr><th>Eviction Signal</th><th>Description</th></tr></thead><tbody><tr><td><code>memory.available</code></td><td><code>memory.available</code> := <code>node.status.capacity[memory]</code> - <code>node.stats.memory.workingSet</code></td></tr><tr><td><code>nodefs.available</code></td><td><code>nodefs.available</code> := <code>node.stats.fs.available</code></td></tr><tr><td><code>nodefs.inodesFree</code></td><td><code>nodefs.inodesFree</code> := <code>node.stats.fs.inodesFree</code></td></tr><tr><td><code>imagefs.available</code></td><td><code>imagefs.available</code> := <code>node.stats.runtime.imagefs.available</code></td></tr><tr><td><code>imagefs.inodesFree</code></td><td><code>imagefs.inodesFree</code> := <code>node.stats.runtime.imagefs.inodesFree</code></td></tr><tr><td><code>pid.available</code></td><td><code>pid.available</code> := <code>node.stats.rlimit.maxpid</code> - <code>node.stats.rlimit.curproc</code></td></tr></tbody></table><p>In this table, the <code>Description</code> column shows how kubelet gets the value of the
signal. Each signal supports either a percentage or a literal value. Kubelet
calculates the percentage value relative to the total capacity associated with
the signal.</p><p>The value for <code>memory.available</code> is derived from the cgroupfs instead of tools
like <code>free -m</code>. This is important because <code>free -m</code> does not work in a
container, and if users use the <a href=/docs/tasks/administer-cluster/reserve-compute-resources/#node-allocatable>node allocatable</a>
feature, out of resource decisions
are made local to the end user Pod part of the cgroup hierarchy as well as the
root node. This <a href=/examples/admin/resource/memory-available.sh>script</a>
reproduces the same set of steps that the kubelet performs to calculate
<code>memory.available</code>. The kubelet excludes inactive_file (i.e. # of bytes of
file-backed memory on inactive LRU list) from its calculation as it assumes that
memory is reclaimable under pressure.</p><p>The kubelet supports the following filesystem partitions:</p><ol><li><code>nodefs</code>: The node's main filesystem, used for local disk volumes, emptyDir,
log storage, and more. For example, <code>nodefs</code> contains <code>/var/lib/kubelet/</code>.</li><li><code>imagefs</code>: An optional filesystem that container runtimes use to store container
images and container writable layers.</li></ol><p>Kubelet auto-discovers these filesystems and ignores other filesystems. Kubelet
does not support other configurations.</p><p>Some kubelet garbage collection features are deprecated in favor of eviction:</p><table><thead><tr><th>Existing Flag</th><th>New Flag</th><th>Rationale</th></tr></thead><tbody><tr><td><code>--image-gc-high-threshold</code></td><td><code>--eviction-hard</code> or <code>--eviction-soft</code></td><td>existing eviction signals can trigger image garbage collection</td></tr><tr><td><code>--image-gc-low-threshold</code></td><td><code>--eviction-minimum-reclaim</code></td><td>eviction reclaims achieve the same behavior</td></tr><tr><td><code>--maximum-dead-containers</code></td><td>-</td><td>deprecated once old logs are stored outside of container's context</td></tr><tr><td><code>--maximum-dead-containers-per-container</code></td><td>-</td><td>deprecated once old logs are stored outside of container's context</td></tr><tr><td><code>--minimum-container-ttl-duration</code></td><td>-</td><td>deprecated once old logs are stored outside of container's context</td></tr></tbody></table><h3 id=eviction-thresholds>Eviction thresholds</h3><p>You can specify custom eviction thresholds for the kubelet to use when it makes
eviction decisions.</p><p>Eviction thresholds have the form <code>[eviction-signal][operator][quantity]</code>, where:</p><ul><li><code>eviction-signal</code> is the <a href=#eviction-signals>eviction signal</a> to use.</li><li><code>operator</code> is the <a href=https://en.wikipedia.org/wiki/Relational_operator#Standard_relational_operators>relational operator</a>
you want, such as <code>&lt;</code> (less than).</li><li><code>quantity</code> is the eviction threshold amount, such as <code>1Gi</code>. The value of <code>quantity</code>
must match the quantity representation used by Kubernetes. You can use either
literal values or percentages (<code>%</code>).</li></ul><p>For example, if a node has <code>10Gi</code> of total memory and you want trigger eviction if
the available memory falls below <code>1Gi</code>, you can define the eviction threshold as
either <code>memory.available&lt;10%</code> or <code>memory.available&lt;1Gi</code>. You cannot use both.</p><p>You can configure soft and hard eviction thresholds.</p><h4 id=soft-eviction-thresholds>Soft eviction thresholds</h4><p>A soft eviction threshold pairs an eviction threshold with a required
administrator-specified grace period. The kubelet does not evict pods until the
grace period is exceeded. The kubelet returns an error on startup if there is no
specified grace period.</p><p>You can specify both a soft eviction threshold grace period and a maximum
allowed pod termination grace period for kubelet to use during evictions. If you
specify a maximum allowed grace period and the soft eviction threshold is met,
the kubelet uses the lesser of the two grace periods. If you do not specify a
maximum allowed grace period, the kubelet kills evicted pods immediately without
graceful termination.</p><p>You can use the following flags to configure soft eviction thresholds:</p><ul><li><code>eviction-soft</code>: A set of eviction thresholds like <code>memory.available&lt;1.5Gi</code>
that can trigger pod eviction if held over the specified grace period.</li><li><code>eviction-soft-grace-period</code>: A set of eviction grace periods like <code>memory.available=1m30s</code>
that define how long a soft eviction threshold must hold before triggering a Pod eviction.</li><li><code>eviction-max-pod-grace-period</code>: The maximum allowed grace period (in seconds)
to use when terminating pods in response to a soft eviction threshold being met.</li></ul><h4 id=hard-eviction-thresholds>Hard eviction thresholds</h4><p>A hard eviction threshold has no grace period. When a hard eviction threshold is
met, the kubelet kills pods immediately without graceful termination to reclaim
the starved resource.</p><p>You can use the <code>eviction-hard</code> flag to configure a set of hard eviction
thresholds like <code>memory.available&lt;1Gi</code>.</p><p>The kubelet has the following default hard eviction thresholds:</p><ul><li><code>memory.available&lt;100Mi</code></li><li><code>nodefs.available&lt;10%</code></li><li><code>imagefs.available&lt;15%</code></li><li><code>nodefs.inodesFree&lt;5%</code> (Linux nodes)</li></ul><p>These default values of hard eviction thresholds will only be set if none
of the parameters is changed. If you changed the value of any parameter,
then the values of other parameters will not be inherited as the default
values and will be set to zero. In order to provide custom values, you
should provide all the thresholds respectively.</p><h3 id=eviction-monitoring-interval>Eviction monitoring interval</h3><p>The kubelet evaluates eviction thresholds based on its configured <code>housekeeping-interval</code>
which defaults to <code>10s</code>.</p><h3 id=node-conditions>Node conditions</h3><p>The kubelet reports node conditions to reflect that the node is under pressure
because hard or soft eviction threshold is met, independent of configured grace
periods.</p><p>The kubelet maps eviction signals to node conditions as follows:</p><table><thead><tr><th>Node Condition</th><th>Eviction Signal</th><th>Description</th></tr></thead><tbody><tr><td><code>MemoryPressure</code></td><td><code>memory.available</code></td><td>Available memory on the node has satisfied an eviction threshold</td></tr><tr><td><code>DiskPressure</code></td><td><code>nodefs.available</code>, <code>nodefs.inodesFree</code>, <code>imagefs.available</code>, or <code>imagefs.inodesFree</code></td><td>Available disk space and inodes on either the node's root filesystem or image filesystem has satisfied an eviction threshold</td></tr><tr><td><code>PIDPressure</code></td><td><code>pid.available</code></td><td>Available processes identifiers on the (Linux) node has fallen below an eviction threshold</td></tr></tbody></table><p>The kubelet updates the node conditions based on the configured
<code>--node-status-update-frequency</code>, which defaults to <code>10s</code>.</p><h4 id=node-condition-oscillation>Node condition oscillation</h4><p>In some cases, nodes oscillate above and below soft eviction thresholds without
holding for the defined grace periods. This causes the reported node condition
to constantly switch between <code>true</code> and <code>false</code>, leading to bad eviction decisions.</p><p>To protect against oscillation, you can use the <code>eviction-pressure-transition-period</code>
flag, which controls how long the kubelet must wait before transitioning a node
condition to a different state. The transition period has a default value of <code>5m</code>.</p><h3 id=reclaim-node-resources>Reclaiming node level resources</h3><p>The kubelet tries to reclaim node-level resources before it evicts end-user pods.</p><p>When a <code>DiskPressure</code> node condition is reported, the kubelet reclaims node-level
resources based on the filesystems on the node.</p><h4 id=with-imagefs>With <code>imagefs</code></h4><p>If the node has a dedicated <code>imagefs</code> filesystem for container runtimes to use,
the kubelet does the following:</p><ul><li>If the <code>nodefs</code> filesystem meets the eviction thresholds, the kubelet garbage collects
dead pods and containers.</li><li>If the <code>imagefs</code> filesystem meets the eviction thresholds, the kubelet
deletes all unused images.</li></ul><h4 id=without-imagefs>Without <code>imagefs</code></h4><p>If the node only has a <code>nodefs</code> filesystem that meets eviction thresholds,
the kubelet frees up disk space in the following order:</p><ol><li>Garbage collect dead pods and containers</li><li>Delete unused images</li></ol><h3 id=pod-selection-for-kubelet-eviction>Pod selection for kubelet eviction</h3><p>If the kubelet's attempts to reclaim node-level resources don't bring the eviction
signal below the threshold, the kubelet begins to evict end-user pods.</p><p>The kubelet uses the following parameters to determine the pod eviction order:</p><ol><li>Whether the pod's resource usage exceeds requests</li><li><a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority</a></li><li>The pod's resource usage relative to requests</li></ol><p>As a result, kubelet ranks and evicts pods in the following order:</p><ol><li><code>BestEffort</code> or <code>Burstable</code> pods where the usage exceeds requests. These pods
are evicted based on their Priority and then by how much their usage level
exceeds the request.</li><li><code>Guaranteed</code> pods and <code>Burstable</code> pods where the usage is less than requests
are evicted last, based on their Priority.</li></ol><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The kubelet does not use the pod's QoS class to determine the eviction order.
You can use the QoS class to estimate the most likely pod eviction order when
reclaiming resources like memory. QoS does not apply to EphemeralStorage requests,
so the above scenario will not apply if the node is, for example, under <code>DiskPressure</code>.</div><p><code>Guaranteed</code> pods are guaranteed only when requests and limits are specified for
all the containers and they are equal. These pods will never be evicted because
of another pod's resource consumption. If a system daemon (such as <code>kubelet</code>
and <code>journald</code>) is consuming more resources than were reserved via
<code>system-reserved</code> or <code>kube-reserved</code> allocations, and the node only has
<code>Guaranteed</code> or <code>Burstable</code> pods using less resources than requests left on it,
then the kubelet must choose to evict one of these pods to preserve node stability
and to limit the impact of resource starvation on other pods. In this case, it
will choose to evict pods of lowest Priority first.</p><p>When the kubelet evicts pods in response to <code>inode</code> or <code>PID</code> starvation, it uses
the Priority to determine the eviction order, because <code>inodes</code> and <code>PIDs</code> have no
requests.</p><p>The kubelet sorts pods differently based on whether the node has a dedicated
<code>imagefs</code> filesystem:</p><h4 id=with-imagefs-1>With <code>imagefs</code></h4><p>If <code>nodefs</code> is triggering evictions, the kubelet sorts pods based on <code>nodefs</code>
usage (<code>local volumes + logs of all containers</code>).</p><p>If <code>imagefs</code> is triggering evictions, the kubelet sorts pods based on the
writable layer usage of all containers.</p><h4 id=without-imagefs-1>Without <code>imagefs</code></h4><p>If <code>nodefs</code> is triggering evictions, the kubelet sorts pods based on their total
disk usage (<code>local volumes + logs & writable layer of all containers</code>)</p><h3 id=minimum-eviction-reclaim>Minimum eviction reclaim</h3><p>In some cases, pod eviction only reclaims a small amount of the starved resource.
This can lead to the kubelet repeatedly hitting the configured eviction thresholds
and triggering multiple evictions.</p><p>You can use the <code>--eviction-minimum-reclaim</code> flag or a <a href=/docs/tasks/administer-cluster/kubelet-config-file/>kubelet config file</a>
to configure a minimum reclaim amount for each resource. When the kubelet notices
that a resource is starved, it continues to reclaim that resource until it
reclaims the quantity you specify.</p><p>For example, the following configuration sets minimum reclaim amounts:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:green;font-weight:700>apiVersion</span>:<span style=color:#bbb> </span>kubelet.config.k8s.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>kind</span>:<span style=color:#bbb> </span>KubeletConfiguration<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>evictionHard</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>memory.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;1Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imagefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;100Gi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>evictionMinimumReclaim</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>memory.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;0Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>nodefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;500Mi&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:green;font-weight:700>imagefs.available</span>:<span style=color:#bbb> </span><span style=color:#b44>&#34;2Gi&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>In this example, if the <code>nodefs.available</code> signal meets the eviction threshold,
the kubelet reclaims the resource until the signal reaches the threshold of <code>1Gi</code>,
and then continues to reclaim the minimum amount of <code>500Mi</code> it until the signal
reaches <code>1.5Gi</code>.</p><p>Similarly, the kubelet reclaims the <code>imagefs</code> resource until the <code>imagefs.available</code>
signal reaches <code>102Gi</code>.</p><p>The default <code>eviction-minimum-reclaim</code> is <code>0</code> for all resources.</p><h3 id=node-out-of-memory-behavior>Node out of memory behavior</h3><p>If the node experiences an out of memory (OOM) event prior to the kubelet
being able to reclaim memory, the node depends on the <a href=https://lwn.net/Articles/391222/>oom_killer</a>
to respond.</p><p>The kubelet sets an <code>oom_score_adj</code> value for each container based on the QoS for the pod.</p><table><thead><tr><th>Quality of Service</th><th>oom_score_adj</th></tr></thead><tbody><tr><td><code>Guaranteed</code></td><td>-997</td></tr><tr><td><code>BestEffort</code></td><td>1000</td></tr><tr><td><code>Burstable</code></td><td>min(max(2, 1000 - (1000 * memoryRequestBytes) / machineMemoryCapacityBytes), 999)</td></tr></tbody></table><div class="alert alert-info note callout" role=alert><strong>Note:</strong> The kubelet also sets an <code>oom_score_adj</code> value of <code>-997</code> for containers in Pods that have
<code>system-node-critical</code> <a class=glossary-tooltip title='Pod Priority indicates the importance of a Pod relative to other Pods.' data-toggle=tooltip data-placement=top href=/docs/concepts/scheduling-eviction/pod-priority-preemption/#pod-priority target=_blank aria-label=Priority>Priority</a>.</div><p>If the kubelet can't reclaim memory before a node experiences OOM, the
<code>oom_killer</code> calculates an <code>oom_score</code> based on the percentage of memory it's
using on the node, and then adds the <code>oom_score_adj</code> to get an effective <code>oom_score</code>
for each container. It then kills the container with the highest score.</p><p>This means that containers in low QoS pods that consume a large amount of memory
relative to their scheduling requests are killed first.</p><p>Unlike pod eviction, if a container is OOM killed, the <code>kubelet</code> can restart it
based on its <code>RestartPolicy</code>.</p><h3 id=node-pressure-eviction-good-practices>Best practices</h3><p>The following sections describe best practices for eviction configuration.</p><h4 id=schedulable-resources-and-eviction-policies>Schedulable resources and eviction policies</h4><p>When you configure the kubelet with an eviction policy, you should make sure that
the scheduler will not schedule pods if they will trigger eviction because they
immediately induce memory pressure.</p><p>Consider the following scenario:</p><ul><li>Node memory capacity: <code>10Gi</code></li><li>Operator wants to reserve 10% of memory capacity for system daemons (kernel, <code>kubelet</code>, etc.)</li><li>Operator wants to evict Pods at 95% memory utilization to reduce incidence of system OOM.</li></ul><p>For this to work, the kubelet is launched as follows:</p><pre tabindex=0><code>--eviction-hard=memory.available&lt;500Mi
--system-reserved=memory=1.5Gi
</code></pre><p>In this configuration, the <code>--system-reserved</code> flag reserves <code>1.5Gi</code> of memory
for the system, which is <code>10% of the total memory + the eviction threshold amount</code>.</p><p>The node can reach the eviction threshold if a pod is using more than its request,
or if the system is using more than <code>1Gi</code> of memory, which makes the <code>memory.available</code>
signal fall below <code>500Mi</code> and triggers the threshold.</p><h4 id=daemonset>DaemonSet</h4><p>Pod Priority is a major factor in making eviction decisions. If you do not want
the kubelet to evict pods that belong to a <code>DaemonSet</code>, give those pods a high
enough <code>priorityClass</code> in the pod spec. You can also use a lower <code>priorityClass</code>
or the default to only allow <code>DaemonSet</code> pods to run when there are enough
resources.</p><h3 id=known-issues>Known issues</h3><p>The following sections describe known issues related to out of resource handling.</p><h4 id=kubelet-may-not-observe-memory-pressure-right-away>kubelet may not observe memory pressure right away</h4><p>By default, the kubelet polls <code>cAdvisor</code> to collect memory usage stats at a
regular interval. If memory usage increases within that window rapidly, the
kubelet may not observe <code>MemoryPressure</code> fast enough, and the <code>OOMKiller</code>
will still be invoked.</p><p>You can use the <code>--kernel-memcg-notification</code> flag to enable the <code>memcg</code>
notification API on the kubelet to get notified immediately when a threshold
is crossed.</p><p>If you are not trying to achieve extreme utilization, but a sensible measure of
overcommit, a viable workaround for this issue is to use the <code>--kube-reserved</code>
and <code>--system-reserved</code> flags to allocate memory for the system.</p><h4 id=active-file-memory-is-not-considered-as-available-memory>active_file memory is not considered as available memory</h4><p>On Linux, the kernel tracks the number of bytes of file-backed memory on active
LRU list as the <code>active_file</code> statistic. The kubelet treats <code>active_file</code> memory
areas as not reclaimable. For workloads that make intensive use of block-backed
local storage, including ephemeral local storage, kernel-level caches of file
and block data means that many recently accessed cache pages are likely to be
counted as <code>active_file</code>. If enough of these kernel block buffers are on the
active LRU list, the kubelet is liable to observe this as high resource use and
taint the node as experiencing memory pressure - triggering pod eviction.</p><p>For more details, see <a href=https://github.com/kubernetes/kubernetes/issues/43916>https://github.com/kubernetes/kubernetes/issues/43916</a></p><p>You can work around that behavior by setting the memory limit and memory request
the same for containers likely to perform intensive I/O activity. You will need
to estimate or measure an optimal memory limit value for that container.</p><h2 id=what-s-next>What's next</h2><ul><li>Learn about <a href=/docs/concepts/scheduling-eviction/api-eviction/>API-initiated Eviction</a></li><li>Learn about <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority and Preemption</a></li><li>Learn about <a href=/docs/tasks/run-application/configure-pdb/>PodDisruptionBudgets</a></li><li>Learn about <a href=/docs/tasks/configure-pod-container/quality-service-pod/>Quality of Service</a> (QoS)</li><li>Check out the <a href=/docs/reference/generated/kubernetes-api/v1.25/#create-eviction-pod-v1-core>Eviction API</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-b87723bf81b079042860f0ebd37b0a64>11 - API-initiated Eviction</h1><p>API-initiated eviction is the process by which you use the <a href=/docs/reference/generated/kubernetes-api/v1.25/#create-eviction-pod-v1-core>Eviction API</a>
to create an <code>Eviction</code> object that triggers graceful pod termination.</br></p><p>You can request eviction by calling the Eviction API directly, or programmatically
using a client of the <a class=glossary-tooltip title='Control plane component that serves the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/components/#kube-apiserver target=_blank aria-label='API server'>API server</a>, like the <code>kubectl drain</code> command. This
creates an <code>Eviction</code> object, which causes the API server to terminate the Pod.</p><p>API-initiated evictions respect your configured <a href=/docs/tasks/run-application/configure-pdb/><code>PodDisruptionBudgets</code></a>
and <a href=/docs/concepts/workloads/pods/pod-lifecycle#pod-termination><code>terminationGracePeriodSeconds</code></a>.</p><p>Using the API to create an Eviction object for a Pod is like performing a
policy-controlled <a href=/docs/reference/kubernetes-api/workload-resources/pod-v1/#delete-delete-a-pod><code>DELETE</code> operation</a>
on the Pod.</p><h2 id=calling-the-eviction-api>Calling the Eviction API</h2><p>You can use a <a href=/docs/tasks/administer-cluster/access-cluster-api/#programmatic-access-to-the-api>Kubernetes language client</a>
to access the Kubernetes API and create an <code>Eviction</code> object. To do this, you
POST the attempted operation, similar to the following example:</p><ul class="nav nav-tabs" id=eviction-example role=tablist><li class=nav-item><a data-toggle=tab class="nav-link active" href=#eviction-example-0 role=tab aria-controls=eviction-example-0 aria-selected=true>policy/v1</a></li><li class=nav-item><a data-toggle=tab class=nav-link href=#eviction-example-1 role=tab aria-controls=eviction-example-1>policy/v1beta1</a></li></ul><div class=tab-content id=eviction-example><div id=eviction-example-0 class="tab-pane show active" role=tabpanel aria-labelledby=eviction-example-0><p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> <code>policy/v1</code> Eviction is available in v1.22+. Use <code>policy/v1beta1</code> with prior releases.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;policy/v1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Eviction&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;quux&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></div><div id=eviction-example-1 class=tab-pane role=tabpanel aria-labelledby=eviction-example-1><p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Deprecated in v1.22 in favor of <code>policy/v1</code></div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;policy/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Eviction&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;metadata&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;name&#34;</span>: <span style=color:#b44>&#34;quux&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;default&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></div></div><p>Alternatively, you can attempt an eviction operation by accessing the API using
<code>curl</code> or <code>wget</code>, similar to the following example:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>curl -v -H <span style=color:#b44>&#39;Content-type: application/json&#39;</span> https://your-cluster-api-endpoint.example/api/v1/namespaces/default/pods/quux/eviction -d @eviction.json
</span></span></code></pre></div><h2 id=how-api-initiated-eviction-works>How API-initiated eviction works</h2><p>When you request an eviction using the API, the API server performs admission
checks and responds in one of the following ways:</p><ul><li><code>200 OK</code>: the eviction is allowed, the <code>Eviction</code> subresource is created, and
the Pod is deleted, similar to sending a <code>DELETE</code> request to the Pod URL.</li><li><code>429 Too Many Requests</code>: the eviction is not currently allowed because of the
configured <a class=glossary-tooltip title='An object that limits the number of  of a replicated application, that are down simultaneously from voluntary disruptions.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-pod-disruption-budget' target=_blank aria-label=PodDisruptionBudget>PodDisruptionBudget</a>.
You may be able to attempt the eviction again later. You might also see this
response because of API rate limiting.</li><li><code>500 Internal Server Error</code>: the eviction is not allowed because there is a
misconfiguration, like if multiple PodDisruptionBudgets reference the same Pod.</li></ul><p>If the Pod you want to evict isn't part of a workload that has a
PodDisruptionBudget, the API server always returns <code>200 OK</code> and allows the
eviction.</p><p>If the API server allows the eviction, the Pod is deleted as follows:</p><ol><li>The <code>Pod</code> resource in the API server is updated with a deletion timestamp,
after which the API server considers the <code>Pod</code> resource to be terminated. The
<code>Pod</code> resource is also marked with the configured grace period.</li><li>The <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> on the node where the local Pod is running notices that the <code>Pod</code>
resource is marked for termination and starts to gracefully shut down the
local Pod.</li><li>While the kubelet is shutting the Pod down, the control plane removes the Pod
from <a class=glossary-tooltip title='Endpoints track the IP addresses of Pods with matching Service selectors.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-endpoint' target=_blank aria-label=Endpoint>Endpoint</a> and
<a class=glossary-tooltip title='A way to group network endpoints together with Kubernetes resources.' data-toggle=tooltip data-placement=top href=/docs/concepts/services-networking/endpoint-slices/ target=_blank aria-label=EndpointSlice>EndpointSlice</a>
objects. As a result, controllers no longer consider the Pod as a valid object.</li><li>After the grace period for the Pod expires, the kubelet forcefully terminates
the local Pod.</li><li>The kubelet tells the API server to remove the <code>Pod</code> resource.</li><li>The API server deletes the <code>Pod</code> resource.</li></ol><h2 id=troubleshooting-stuck-evictions>Troubleshooting stuck evictions</h2><p>In some cases, your applications may enter a broken state, where the Eviction
API will only return <code>429</code> or <code>500</code> responses until you intervene. This can
happen if, for example, a ReplicaSet creates pods for your application but new
pods do not enter a <code>Ready</code> state. You may also notice this behavior in cases
where the last evicted Pod had a long termination grace period.</p><p>If you notice stuck evictions, try one of the following solutions:</p><ul><li>Abort or pause the automated operation causing the issue. Investigate the stuck
application before you restart the operation.</li><li>Wait a while, then directly delete the Pod from your cluster control plane
instead of using the Eviction API.</li></ul><h2 id=what-s-next>What's next</h2><ul><li>Learn how to protect your applications with a <a href=/docs/tasks/run-application/configure-pdb/>Pod Disruption Budget</a>.</li><li>Learn about <a href=/docs/concepts/scheduling-eviction/node-pressure-eviction/>Node-pressure Eviction</a>.</li><li>Learn about <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>Pod Priority and Preemption</a>.</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>