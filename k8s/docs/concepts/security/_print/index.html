<!doctype html><html lang=en class=no-js><head><meta name=robots content="noindex, nofollow"><link rel=alternate hreflang=zh-cn href=https://kubernetes.io/zh-cn/docs/concepts/security/><link rel=alternate hreflang=ko href=https://kubernetes.io/ko/docs/concepts/security/><link rel=alternate hreflang=ja href=https://kubernetes.io/ja/docs/concepts/security/><link rel=alternate hreflang=fr href=https://kubernetes.io/fr/docs/concepts/security/><link rel=alternate hreflang=es href=https://kubernetes.io/es/docs/concepts/security/><link rel=alternate hreflang=pt-br href=https://kubernetes.io/pt-br/docs/concepts/security/><link rel=alternate hreflang=id href=https://kubernetes.io/id/docs/concepts/security/><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><link rel=canonical type=text/html href=https://kubernetes.io/docs/concepts/security/><link rel="shortcut icon" type=image/png href=/images/favicon.png><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=manifest href=/manifest.webmanifest><link rel=apple-touch-icon href=/images/kubernetes-192x192.png><title>Security | Kubernetes</title><meta property="og:title" content="Security"><meta property="og:description" content="Concepts for keeping your cloud-native workload secure.
"><meta property="og:type" content="website"><meta property="og:url" content="https://kubernetes.io/docs/concepts/security/"><meta property="og:site_name" content="Kubernetes"><meta itemprop=name content="Security"><meta itemprop=description content="Concepts for keeping your cloud-native workload secure.
"><meta name=twitter:card content="summary"><meta name=twitter:title content="Security"><meta name=twitter:description content="Concepts for keeping your cloud-native workload secure.
"><link href=/scss/main.css rel=stylesheet><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","url":"https://kubernetes.io","logo":"https://kubernetes.io/images/favicon.png","potentialAction":{"@type":"SearchAction","target":"https://kubernetes.io/search/?q={search_term_string}","query-input":"required name=search_term_string"}}</script><meta name=theme-color content="#326ce5"><link rel=stylesheet href=/css/feature-states.css><meta name=description content="Concepts for keeping your cloud-native workload secure.
"><meta property="og:description" content="Concepts for keeping your cloud-native workload secure.
"><meta name=twitter:description content="Concepts for keeping your cloud-native workload secure.
"><meta property="og:url" content="https://kubernetes.io/docs/concepts/security/"><meta property="og:title" content="Security"><meta name=twitter:title content="Security"><meta name=twitter:image content="https://kubernetes.io/images/favicon.png"><meta name=twitter:image:alt content="Kubernetes"><meta property="og:image" content="/images/kubernetes-horizontal-color.png"><meta property="og:type" content="article"><script src=/js/jquery-3.6.0.min.js intregrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script></head><body class=td-section><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" data-auto-burger=primary><a class=navbar-brand href=/></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-2 mb-lg-0"><a class="nav-link active" href=/docs/>Documentation</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/blog/>Kubernetes Blog</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/training/>Training</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/partners/>Partners</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/community/>Community</a></li><li class="nav-item mr-2 mb-lg-0"><a class=nav-link href=/case-studies/>Case Studies</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>Versions</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/releases>Release Information</a>
<a class=dropdown-item href=https://kubernetes.io/docs/concepts/security/>v1.25</a>
<a class=dropdown-item href=https://v1-24.docs.kubernetes.io/docs/concepts/security/>v1.24</a>
<a class=dropdown-item href=https://v1-23.docs.kubernetes.io/docs/concepts/security/>v1.23</a>
<a class=dropdown-item href=https://v1-22.docs.kubernetes.io/docs/concepts/security/>v1.22</a>
<a class=dropdown-item href=https://v1-21.docs.kubernetes.io/docs/concepts/security/>v1.21</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdownMenuLink role=button data-toggle=dropdown aria-haspopup=true aria-expanded=false>English</a><div class="dropdown-menu dropdown-menu-right" aria-labelledby=navbarDropdownMenuLink><a class=dropdown-item href=/zh-cn/docs/concepts/security/>中文 (Chinese)</a>
<a class=dropdown-item href=/ko/docs/concepts/security/>한국어 (Korean)</a>
<a class=dropdown-item href=/ja/docs/concepts/security/>日本語 (Japanese)</a>
<a class=dropdown-item href=/fr/docs/concepts/security/>Français (French)</a>
<a class=dropdown-item href=/es/docs/concepts/security/>Español (Spanish)</a>
<a class=dropdown-item href=/pt-br/docs/concepts/security/>Português (Portuguese)</a>
<a class=dropdown-item href=/id/docs/concepts/security/>Bahasa Indonesia</a></div></li></ul></div><button id=hamburger onclick=kub.toggleMenu() data-auto-burger-exclude><div></div></button></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 pl-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/docs/concepts/security/>Return to the regular view of this page</a>.</p></div><h1 class=title>Security</h1><div class=lead>Concepts for keeping your cloud-native workload secure.</div><ul><li>1: <a href=#pg-04eeb110d75afc8acb2cf7a3db743985>Overview of Cloud Native Security</a></li><li>2: <a href=#pg-1fb24c1dd155f43849da490a74c4b8c5>Pod Security Standards</a></li><li>3: <a href=#pg-bc9934fccfeaf880eec6ea79025c0381>Pod Security Admission</a></li><li>4: <a href=#pg-ac71855bb20cbf21edc666e810f4103a>Pod Security Policies</a></li><li>5: <a href=#pg-9a68f631b6bc38c279bbc9a145e34ef2>Security For Windows Nodes</a></li><li>6: <a href=#pg-4d77d1ae4c06aa14f54b385191627881>Controlling Access to the Kubernetes API</a></li><li>7: <a href=#pg-07f58aa0218d666795499c2e2306ff96>Role Based Access Control Good Practices</a></li><li>8: <a href=#pg-a7863bfad3d69f33f5b318b9028eecb8>Good practices for Kubernetes Secrets</a></li><li>9: <a href=#pg-9dd9b8c71fa39ff803fd15b0e784069d>Multi-tenancy</a></li><li>10: <a href=#pg-265c06c3d1349382453ced9f2a7ecfde>Kubernetes API Server Bypass Risks</a></li><li>11: <a href=#pg-6f8354561fd5286f997909e14b13110c>Security Checklist</a></li></ul><div class=content></div></div><div class=td-content><h1 id=pg-04eeb110d75afc8acb2cf7a3db743985>1 - Overview of Cloud Native Security</h1><div class=lead>A model for thinking about Kubernetes security in the context of Cloud Native security.</div><p>This overview defines a model for thinking about Kubernetes security in the context of Cloud Native security.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> This container security model provides suggestions, not proven information security policies.</div><h2 id=the-4c-s-of-cloud-native-security>The 4C's of Cloud Native security</h2><p>You can think about security in layers. The 4C's of Cloud Native security are Cloud,
Clusters, Containers, and Code.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> This layered approach augments the <a href=https://en.wikipedia.org/wiki/Defense_in_depth_(computing)>defense in depth</a>
computing approach to security, which is widely regarded as a best practice for securing
software systems.</div><figure class=diagram-large><img src=/images/docs/4c.png><figcaption><h4>The 4C's of Cloud Native Security</h4></figcaption></figure><p>Each layer of the Cloud Native security model builds upon the next outermost layer.
The Code layer benefits from strong base (Cloud, Cluster, Container) security layers.
You cannot safeguard against poor security standards in the base layers by addressing
security at the Code level.</p><h2 id=cloud>Cloud</h2><p>In many ways, the Cloud (or co-located servers, or the corporate datacenter) is the
<a href=https://en.wikipedia.org/wiki/Trusted_computing_base>trusted computing base</a>
of a Kubernetes cluster. If the Cloud layer is vulnerable (or
configured in a vulnerable way) then there is no guarantee that the components built
on top of this base are secure. Each cloud provider makes security recommendations
for running workloads securely in their environment.</p><h3 id=cloud-provider-security>Cloud provider security</h3><p>If you are running a Kubernetes cluster on your own hardware or a different cloud provider,
consult your documentation for security best practices.
Here are links to some of the popular cloud providers' security documentation:</p><table><caption style=display:none>Cloud provider security</caption><thead><tr><th>IaaS Provider</th><th>Link</th></tr></thead><tbody><tr><td>Alibaba Cloud</td><td><a href=https://www.alibabacloud.com/trust-center>https://www.alibabacloud.com/trust-center</a></td></tr><tr><td>Amazon Web Services</td><td><a href=https://aws.amazon.com/security>https://aws.amazon.com/security</a></td></tr><tr><td>Google Cloud Platform</td><td><a href=https://cloud.google.com/security>https://cloud.google.com/security</a></td></tr><tr><td>Huawei Cloud</td><td><a href=https://www.huaweicloud.com/securecenter/overallsafety>https://www.huaweicloud.com/securecenter/overallsafety</a></td></tr><tr><td>IBM Cloud</td><td><a href=https://www.ibm.com/cloud/security>https://www.ibm.com/cloud/security</a></td></tr><tr><td>Microsoft Azure</td><td><a href=https://docs.microsoft.com/en-us/azure/security/azure-security>https://docs.microsoft.com/en-us/azure/security/azure-security</a></td></tr><tr><td>Oracle Cloud Infrastructure</td><td><a href=https://www.oracle.com/security>https://www.oracle.com/security</a></td></tr><tr><td>VMware vSphere</td><td><a href=https://www.vmware.com/security/hardening-guides>https://www.vmware.com/security/hardening-guides</a></td></tr></tbody></table><h3 id=infrastructure-security>Infrastructure security</h3><p>Suggestions for securing your infrastructure in a Kubernetes cluster:</p><table><caption style=display:none>Infrastructure security</caption><thead><tr><th>Area of Concern for Kubernetes Infrastructure</th><th>Recommendation</th></tr></thead><tbody><tr><td>Network access to API Server (Control plane)</td><td>All access to the Kubernetes control plane is not allowed publicly on the internet and is controlled by network access control lists restricted to the set of IP addresses needed to administer the cluster.</td></tr><tr><td>Network access to Nodes (nodes)</td><td>Nodes should be configured to <em>only</em> accept connections (via network access control lists) from the control plane on the specified ports, and accept connections for services in Kubernetes of type NodePort and LoadBalancer. If possible, these nodes should not be exposed on the public internet entirely.</td></tr><tr><td>Kubernetes access to Cloud Provider API</td><td>Each cloud provider needs to grant a different set of permissions to the Kubernetes control plane and nodes. It is best to provide the cluster with cloud provider access that follows the <a href=https://en.wikipedia.org/wiki/Principle_of_least_privilege>principle of least privilege</a> for the resources it needs to administer. The <a href=https://github.com/kubernetes/kops/blob/master/docs/iam_roles.md#iam-roles>Kops documentation</a> provides information about IAM policies and roles.</td></tr><tr><td>Access to etcd</td><td>Access to etcd (the datastore of Kubernetes) should be limited to the control plane only. Depending on your configuration, you should attempt to use etcd over TLS. More information can be found in the <a href=https://github.com/etcd-io/etcd/tree/master/Documentation>etcd documentation</a>.</td></tr><tr><td>etcd Encryption</td><td>Wherever possible it's a good practice to encrypt all storage at rest, and since etcd holds the state of the entire cluster (including Secrets) its disk should especially be encrypted at rest.</td></tr></tbody></table><h2 id=cluster>Cluster</h2><p>There are two areas of concern for securing Kubernetes:</p><ul><li>Securing the cluster components that are configurable</li><li>Securing the applications which run in the cluster</li></ul><h3 id=cluster-components>Components of the Cluster</h3><p>If you want to protect your cluster from accidental or malicious access and adopt
good information practices, read and follow the advice about
<a href=/docs/tasks/administer-cluster/securing-a-cluster/>securing your cluster</a>.</p><h3 id=cluster-applications>Components in the cluster (your application)</h3><p>Depending on the attack surface of your application, you may want to focus on specific
aspects of security. For example: If you are running a service (Service A) that is critical
in a chain of other resources and a separate workload (Service B) which is
vulnerable to a resource exhaustion attack, then the risk of compromising Service A
is high if you do not limit the resources of Service B. The following table lists
areas of security concerns and recommendations for securing workloads running in Kubernetes:</p><table><thead><tr><th>Area of Concern for Workload Security</th><th>Recommendation</th></tr></thead><tbody><tr><td>RBAC Authorization (Access to the Kubernetes API)</td><td><a href=https://kubernetes.io/docs/reference/access-authn-authz/rbac/>https://kubernetes.io/docs/reference/access-authn-authz/rbac/</a></td></tr><tr><td>Authentication</td><td><a href=https://kubernetes.io/docs/concepts/security/controlling-access/>https://kubernetes.io/docs/concepts/security/controlling-access/</a></td></tr><tr><td>Application secrets management (and encrypting them in etcd at rest)</td><td><a href=https://kubernetes.io/docs/concepts/configuration/secret/>https://kubernetes.io/docs/concepts/configuration/secret/</a><br><a href=https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/>https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/</a></td></tr><tr><td>Ensuring that pods meet defined Pod Security Standards</td><td><a href=https://kubernetes.io/docs/concepts/security/pod-security-standards/#policy-instantiation>https://kubernetes.io/docs/concepts/security/pod-security-standards/#policy-instantiation</a></td></tr><tr><td>Quality of Service (and Cluster resource management)</td><td><a href=https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/>https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/</a></td></tr><tr><td>Network Policies</td><td><a href=https://kubernetes.io/docs/concepts/services-networking/network-policies/>https://kubernetes.io/docs/concepts/services-networking/network-policies/</a></td></tr><tr><td>TLS for Kubernetes Ingress</td><td><a href=https://kubernetes.io/docs/concepts/services-networking/ingress/#tls>https://kubernetes.io/docs/concepts/services-networking/ingress/#tls</a></td></tr></tbody></table><h2 id=container>Container</h2><p>Container security is outside the scope of this guide. Here are general recommendations and
links to explore this topic:</p><table><thead><tr><th>Area of Concern for Containers</th><th>Recommendation</th></tr></thead><tbody><tr><td>Container Vulnerability Scanning and OS Dependency Security</td><td>As part of an image build step, you should scan your containers for known vulnerabilities.</td></tr><tr><td>Image Signing and Enforcement</td><td>Sign container images to maintain a system of trust for the content of your containers.</td></tr><tr><td>Disallow privileged users</td><td>When constructing containers, consult your documentation for how to create users inside of the containers that have the least level of operating system privilege necessary in order to carry out the goal of the container.</td></tr><tr><td>Use container runtime with stronger isolation</td><td>Select <a href=/docs/concepts/containers/runtime-class/>container runtime classes</a> that provide stronger isolation.</td></tr></tbody></table><h2 id=code>Code</h2><p>Application code is one of the primary attack surfaces over which you have the most control.
While securing application code is outside of the Kubernetes security topic, here
are recommendations to protect application code:</p><h3 id=code-security>Code security</h3><table><caption style=display:none>Code security</caption><thead><tr><th>Area of Concern for Code</th><th>Recommendation</th></tr></thead><tbody><tr><td>Access over TLS only</td><td>If your code needs to communicate by TCP, perform a TLS handshake with the client ahead of time. With the exception of a few cases, encrypt everything in transit. Going one step further, it's a good idea to encrypt network traffic between services. This can be done through a process known as mutual TLS authentication or <a href=https://en.wikipedia.org/wiki/Mutual_authentication>mTLS</a> which performs a two sided verification of communication between two certificate holding services.</td></tr><tr><td>Limiting port ranges of communication</td><td>This recommendation may be a bit self-explanatory, but wherever possible you should only expose the ports on your service that are absolutely essential for communication or metric gathering.</td></tr><tr><td>3rd Party Dependency Security</td><td>It is a good practice to regularly scan your application's third party libraries for known security vulnerabilities. Each programming language has a tool for performing this check automatically.</td></tr><tr><td>Static Code Analysis</td><td>Most languages provide a way for a snippet of code to be analyzed for any potentially unsafe coding practices. Whenever possible you should perform checks using automated tooling that can scan codebases for common security errors. Some of the tools can be found at: <a href=https://owasp.org/www-community/Source_Code_Analysis_Tools>https://owasp.org/www-community/Source_Code_Analysis_Tools</a></td></tr><tr><td>Dynamic probing attacks</td><td>There are a few automated tools that you can run against your service to try some of the well known service attacks. These include SQL injection, CSRF, and XSS. One of the most popular dynamic analysis tools is the <a href=https://owasp.org/www-project-zap/>OWASP Zed Attack proxy</a> tool.</td></tr></tbody></table><h2 id=what-s-next>What's next</h2><p>Learn about related Kubernetes security topics:</p><ul><li><a href=/docs/concepts/security/pod-security-standards/>Pod security standards</a></li><li><a href=/docs/concepts/services-networking/network-policies/>Network policies for Pods</a></li><li><a href=/docs/concepts/security/controlling-access>Controlling Access to the Kubernetes API</a></li><li><a href=/docs/tasks/administer-cluster/securing-a-cluster/>Securing your cluster</a></li><li><a href=/docs/tasks/tls/managing-tls-in-a-cluster/>Data encryption in transit</a> for the control plane</li><li><a href=/docs/tasks/administer-cluster/encrypt-data/>Data encryption at rest</a></li><li><a href=/docs/concepts/configuration/secret/>Secrets in Kubernetes</a></li><li><a href=/docs/concepts/containers/runtime-class>Runtime class</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1fb24c1dd155f43849da490a74c4b8c5>2 - Pod Security Standards</h1><div class=lead>A detailed look at the different policy levels defined in the Pod Security Standards.</div><p>The Pod Security Standards define three different <em>policies</em> to broadly cover the security
spectrum. These policies are <em>cumulative</em> and range from highly-permissive to highly-restrictive.
This guide outlines the requirements of each policy.</p><table><thead><tr><th>Profile</th><th>Description</th></tr></thead><tbody><tr><td><strong style=white-space:nowrap>Privileged</strong></td><td>Unrestricted policy, providing the widest possible level of permissions. This policy allows for known privilege escalations.</td></tr><tr><td><strong style=white-space:nowrap>Baseline</strong></td><td>Minimally restrictive policy which prevents known privilege escalations. Allows the default (minimally specified) Pod configuration.</td></tr><tr><td><strong style=white-space:nowrap>Restricted</strong></td><td>Heavily restricted policy, following current Pod hardening best practices.</td></tr></tbody></table><h2 id=profile-details>Profile Details</h2><h3 id=privileged>Privileged</h3><p><strong>The <em>Privileged</em> policy is purposely-open, and entirely unrestricted.</strong> This type of policy is
typically aimed at system- and infrastructure-level workloads managed by privileged, trusted users.</p><p>The Privileged policy is defined by an absence of restrictions. Allow-by-default
mechanisms (such as gatekeeper) may be Privileged by default. In contrast, for a deny-by-default mechanism (such as Pod
Security Policy) the Privileged policy should disable all restrictions.</p><h3 id=baseline>Baseline</h3><p><strong>The <em>Baseline</em> policy is aimed at ease of adoption for common containerized workloads while
preventing known privilege escalations.</strong> This policy is targeted at application operators and
developers of non-critical applications. The following listed controls should be
enforced/disallowed:</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In this table, wildcards (<code>*</code>) indicate all elements in a list. For example,
<code>spec.containers[*].securityContext</code> refers to the Security Context object for <em>all defined
containers</em>. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.</div><table><caption style=display:none>Baseline policy specification</caption><tbody><tr><th>Control</th><th>Policy</th></tr><tr><td style=white-space:nowrap>HostProcess</td><td><p>Windows pods offer the ability to run <a href=/docs/tasks/configure-pod-container/create-hostprocess-pod>HostProcess containers</a> which enables privileged access to the Windows node. Privileged access to the host is disallowed in the baseline policy.<div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.23 [beta]</code></div></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.windowsOptions.hostProcess</code></li><li><code>spec.containers[*].securityContext.windowsOptions.hostProcess</code></li><li><code>spec.initContainers[*].securityContext.windowsOptions.hostProcess</code></li><li><code>spec.ephemeralContainers[*].securityContext.windowsOptions.hostProcess</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td style=white-space:nowrap>Host Namespaces</td><td><p>Sharing the host namespaces must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.hostNetwork</code></li><li><code>spec.hostPID</code></li><li><code>spec.hostIPC</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td style=white-space:nowrap>Privileged Containers</td><td><p>Privileged Pods disable most security mechanisms and must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.privileged</code></li><li><code>spec.initContainers[*].securityContext.privileged</code></li><li><code>spec.ephemeralContainers[*].securityContext.privileged</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>false</code></li></ul></td></tr><tr><td style=white-space:nowrap>Capabilities</td><td><p>Adding additional capabilities beyond those listed below must be disallowed.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.add</code></li><li><code>spec.initContainers[*].securityContext.capabilities.add</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>AUDIT_WRITE</code></li><li><code>CHOWN</code></li><li><code>DAC_OVERRIDE</code></li><li><code>FOWNER</code></li><li><code>FSETID</code></li><li><code>KILL</code></li><li><code>MKNOD</code></li><li><code>NET_BIND_SERVICE</code></li><li><code>SETFCAP</code></li><li><code>SETGID</code></li><li><code>SETPCAP</code></li><li><code>SETUID</code></li><li><code>SYS_CHROOT</code></li></ul></td></tr><tr><td style=white-space:nowrap>HostPath Volumes</td><td><p>HostPath volumes must be forbidden.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.volumes[*].hostPath</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li></ul></td></tr><tr><td style=white-space:nowrap>Host Ports</td><td><p>HostPorts should be disallowed, or at minimum restricted to a known list.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].ports[*].hostPort</code></li><li><code>spec.initContainers[*].ports[*].hostPort</code></li><li><code>spec.ephemeralContainers[*].ports[*].hostPort</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li>Known list</li><li><code>0</code></li></ul></td></tr><tr><td style=white-space:nowrap>AppArmor</td><td><p>On supported hosts, the <code>runtime/default</code> AppArmor profile is applied by default. The baseline policy should prevent overriding or disabling the default AppArmor profile, or restrict overrides to an allowed set of profiles.</p><p><strong>Restricted Fields</strong></p><ul><li><code>metadata.annotations["container.apparmor.security.beta.kubernetes.io/*"]</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>runtime/default</code></li><li><code>localhost/*</code></li></ul></td></tr><tr><td style=white-space:nowrap>SELinux</td><td><p>Setting the SELinux type is restricted, and setting a custom SELinux user or role option is forbidden.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seLinuxOptions.type</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.type</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/""</li><li><code>container_t</code></li><li><code>container_init_t</code></li><li><code>container_kvm_t</code></li></ul><hr><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seLinuxOptions.user</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.user</code></li><li><code>spec.securityContext.seLinuxOptions.role</code></li><li><code>spec.containers[*].securityContext.seLinuxOptions.role</code></li><li><code>spec.initContainers[*].securityContext.seLinuxOptions.role</code></li><li><code>spec.ephemeralContainers[*].securityContext.seLinuxOptions.role</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/""</li></ul></td></tr><tr><td style=white-space:nowrap><code>/proc</code> Mount Type</td><td><p>The default <code>/proc</code> masks are set up to reduce attack surface, and should be required.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.procMount</code></li><li><code>spec.initContainers[*].securityContext.procMount</code></li><li><code>spec.ephemeralContainers[*].securityContext.procMount</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>Default</code></li></ul></td></tr><tr><td>Seccomp</td><td><p>Seccomp profile must not be explicitly set to <code>Unconfined</code>.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seccompProfile.type</code></li><li><code>spec.containers[*].securityContext.seccompProfile.type</code></li><li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul></td></tr><tr><td style=white-space:nowrap>Sysctls</td><td><p>Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed "safe" subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.sysctls[*].name</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>kernel.shm_rmid_forced</code></li><li><code>net.ipv4.ip_local_port_range</code></li><li><code>net.ipv4.ip_unprivileged_port_start</code></li><li><code>net.ipv4.tcp_syncookies</code></li><li><code>net.ipv4.ping_group_range</code></li></ul></td></tr></tbody></table><h3 id=restricted>Restricted</h3><p><strong>The <em>Restricted</em> policy is aimed at enforcing current Pod hardening best practices, at the
expense of some compatibility.</strong> It is targeted at operators and developers of security-critical
applications, as well as lower-trust users. The following listed controls should be
enforced/disallowed:</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> In this table, wildcards (<code>*</code>) indicate all elements in a list. For example,
<code>spec.containers[*].securityContext</code> refers to the Security Context object for <em>all defined
containers</em>. If any of the listed containers fails to meet the requirements, the entire pod will
fail validation.</div><table><caption style=display:none>Restricted policy specification</caption><tbody><tr><td><strong>Control</strong></td><td><strong>Policy</strong></td></tr><tr><td colspan=2><em>Everything from the baseline profile.</em></td></tr><tr><td style=white-space:nowrap>Volume Types</td><td><p>The restricted policy only permits the following volume types.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.volumes[*]</code></li></ul><p><strong>Allowed Values</strong></p>Every item in the <code>spec.volumes[*]</code> list must set one of the following fields to a non-null value:<ul><li><code>spec.volumes[*].configMap</code></li><li><code>spec.volumes[*].csi</code></li><li><code>spec.volumes[*].downwardAPI</code></li><li><code>spec.volumes[*].emptyDir</code></li><li><code>spec.volumes[*].ephemeral</code></li><li><code>spec.volumes[*].persistentVolumeClaim</code></li><li><code>spec.volumes[*].projected</code></li><li><code>spec.volumes[*].secret</code></li></ul></td></tr><tr><td style=white-space:nowrap>Privilege Escalation (v1.8+)</td><td><p>Privilege escalation (such as via set-user-ID or set-group-ID file mode) should not be allowed. <em><a href=#policies-specific-to-linux>This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.initContainers[*].securityContext.allowPrivilegeEscalation</code></li><li><code>spec.ephemeralContainers[*].securityContext.allowPrivilegeEscalation</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>false</code></li></ul></td></tr><tr><td style=white-space:nowrap>Running as Non-root</td><td><p>Containers must be required to run as non-root users.</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.runAsNonRoot</code></li><li><code>spec.containers[*].securityContext.runAsNonRoot</code></li><li><code>spec.initContainers[*].securityContext.runAsNonRoot</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsNonRoot</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>true</code></li></ul><small>The container fields may be undefined/<code>nil</code> if the pod-level
<code>spec.securityContext.runAsNonRoot</code> is set to <code>true</code>.</small></td></tr><tr><td style=white-space:nowrap>Running as Non-root user (v1.23+)</td><td><p>Containers must not set <tt>runAsUser</tt> to 0</p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.runAsUser</code></li><li><code>spec.containers[*].securityContext.runAsUser</code></li><li><code>spec.initContainers[*].securityContext.runAsUser</code></li><li><code>spec.ephemeralContainers[*].securityContext.runAsUser</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>any non-zero value</li><li><code>undefined/null</code></li></ul></td></tr><tr><td style=white-space:nowrap>Seccomp (v1.19+)</td><td><p>Seccomp profile must be explicitly set to one of the allowed values. Both the <code>Unconfined</code> profile and the <em>absence</em> of a profile are prohibited. <em><a href=#policies-specific-to-linux>This is Linux only policy</a> in v1.25+ <code>(spec.os.name != windows)</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.securityContext.seccompProfile.type</code></li><li><code>spec.containers[*].securityContext.seccompProfile.type</code></li><li><code>spec.initContainers[*].securityContext.seccompProfile.type</code></li><li><code>spec.ephemeralContainers[*].securityContext.seccompProfile.type</code></li></ul><p><strong>Allowed Values</strong></p><ul><li><code>RuntimeDefault</code></li><li><code>Localhost</code></li></ul><small>The container fields may be undefined/<code>nil</code> if the pod-level
<code>spec.securityContext.seccompProfile.type</code> field is set appropriately.
Conversely, the pod-level field may be undefined/<code>nil</code> if _all_ container-
level fields are set.</small></td></tr><tr><td style=white-space:nowrap>Capabilities (v1.22+)</td><td><p>Containers must drop <code>ALL</code> capabilities, and are only permitted to add back
the <code>NET_BIND_SERVICE</code> capability. <em><a href=#policies-specific-to-linux>This is Linux only policy</a> in v1.25+ <code>(.spec.os.name != "windows")</code></em></p><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.drop</code></li><li><code>spec.initContainers[*].securityContext.capabilities.drop</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.drop</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Any list of capabilities that includes <code>ALL</code></li></ul><hr><p><strong>Restricted Fields</strong></p><ul><li><code>spec.containers[*].securityContext.capabilities.add</code></li><li><code>spec.initContainers[*].securityContext.capabilities.add</code></li><li><code>spec.ephemeralContainers[*].securityContext.capabilities.add</code></li></ul><p><strong>Allowed Values</strong></p><ul><li>Undefined/nil</li><li><code>NET_BIND_SERVICE</code></li></ul></td></tr></tbody></table><h2 id=policy-instantiation>Policy Instantiation</h2><p>Decoupling policy definition from policy instantiation allows for a common understanding and
consistent language of policies across clusters, independent of the underlying enforcement
mechanism.</p><p>As mechanisms mature, they will be defined below on a per-policy basis. The methods of enforcement
of individual policies are not defined here.</p><p><a href=/docs/concepts/security/pod-security-admission/><strong>Pod Security Admission Controller</strong></a></p><ul><li><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-privileged.yaml download=security/podsecurity-privileged.yaml>Privileged namespace</a></li><li><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-baseline.yaml download=security/podsecurity-baseline.yaml>Baseline namespace</a></li><li><a href=https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/security/podsecurity-restricted.yaml download=security/podsecurity-restricted.yaml>Restricted namespace</a></li></ul><h3 id=alternatives>Alternatives</h3><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>Other alternatives for enforcing policies are being developed in the Kubernetes ecosystem, such as:</p><ul><li><a href=https://github.com/kubewarden>Kubewarden</a></li><li><a href=https://kyverno.io/policies/pod-security/>Kyverno</a></li><li><a href=https://github.com/open-policy-agent/gatekeeper>OPA Gatekeeper</a></li></ul><h2 id=pod-os-field>Pod OS field</h2><p>Kubernetes lets you use nodes that run either Linux or Windows. You can mix both kinds of
node in one cluster.
Windows in Kubernetes has some limitations and differentiators from Linux-based
workloads. Specifically, many of the Pod <code>securityContext</code> fields
<a href=/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext>have no effect on Windows</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Kubelets prior to v1.24 don't enforce the pod OS field, and if a cluster has nodes on versions earlier than v1.24 the restricted policies should be pinned to a version prior to v1.25.</div><h3 id=restricted-pod-security-standard-changes>Restricted Pod Security Standard changes</h3><p>Another important change, made in Kubernetes v1.25 is that the <em>restricted</em> Pod security
has been updated to use the <code>pod.spec.os.name</code> field. Based on the OS name, certain policies that are specific
to a particular OS can be relaxed for the other OS.</p><h4 id=os-specific-policy-controls>OS-specific policy controls</h4><p>Restrictions on the following controls are only required if <code>.spec.os.name</code> is not <code>windows</code>:</p><ul><li>Privilege Escalation</li><li>Seccomp</li><li>Linux Capabilities</li></ul><h2 id=faq>FAQ</h2><h3 id=why-isn-t-there-a-profile-between-privileged-and-baseline>Why isn't there a profile between privileged and baseline?</h3><p>The three profiles defined here have a clear linear progression from most secure (restricted) to least
secure (privileged), and cover a broad set of workloads. Privileges required above the baseline
policy are typically very application specific, so we do not offer a standard profile in this
niche. This is not to say that the privileged profile should always be used in this case, but that
policies in this space need to be defined on a case-by-case basis.</p><p>SIG Auth may reconsider this position in the future, should a clear need for other profiles arise.</p><h3 id=what-s-the-difference-between-a-security-profile-and-a-security-context>What's the difference between a security profile and a security context?</h3><p><a href=/docs/tasks/configure-pod-container/security-context/>Security Contexts</a> configure Pods and
Containers at runtime. Security contexts are defined as part of the Pod and container specifications
in the Pod manifest, and represent parameters to the container runtime.</p><p>Security profiles are control plane mechanisms to enforce specific settings in the Security Context,
as well as other related parameters outside the Security Context. As of July 2021,
<a href=/docs/concepts/security/pod-security-policy/>Pod Security Policies</a> are deprecated in favor of the
built-in <a href=/docs/concepts/security/pod-security-admission/>Pod Security Admission Controller</a>.</p><h3 id=what-about-sandboxed-pods>What about sandboxed Pods?</h3><p>There is not currently an API standard that controls whether a Pod is considered sandboxed or
not. Sandbox Pods may be identified by the use of a sandboxed runtime (such as gVisor or Kata
Containers), but there is no standard definition of what a sandboxed runtime is.</p><p>The protections necessary for sandboxed workloads can differ from others. For example, the need to
restrict privileged permissions is lessened when the workload is isolated from the underlying
kernel. This allows for workloads requiring heightened permissions to still be isolated.</p><p>Additionally, the protection of sandboxed workloads is highly dependent on the method of
sandboxing. As such, no single recommended profile is recommended for all sandboxed workloads.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-bc9934fccfeaf880eec6ea79025c0381>3 - Pod Security Admission</h1><div class=lead>An overview of the Pod Security Admission Controller, which can enforce the Pod Security Standards.</div><div style=margin-top:10px;margin-bottom:10px><b>FEATURE STATE:</b> <code>Kubernetes v1.25 [stable]</code></div><p>The Kubernetes <a href=/docs/concepts/security/pod-security-standards/>Pod Security Standards</a> define
different isolation levels for Pods. These standards let you define how you want to restrict the
behavior of pods in a clear, consistent fashion.</p><p>Kubernetes offers a built-in <em>Pod Security</em> <a class=glossary-tooltip title='A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/admission-controllers/ target=_blank aria-label='admission controller'>admission controller</a> to enforce the Pod Security Standards. Pod security restrictions
are applied at the <a class=glossary-tooltip title='An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=namespace>namespace</a> level when pods are
created.</p><h3 id=built-in-pod-security-admission-enforcement>Built-in Pod Security admission enforcement</h3><p>This page is part of the documentation for Kubernetes v1.25.
If you are running a different version of Kubernetes, consult the documentation for that release.</p><h2 id=pod-security-levels>Pod Security levels</h2><p>Pod Security admission places requirements on a Pod's <a href=/docs/tasks/configure-pod-container/security-context/>Security
Context</a> and other related fields according
to the three levels defined by the <a href=/docs/concepts/security/pod-security-standards>Pod Security
Standards</a>: <code>privileged</code>, <code>baseline</code>, and
<code>restricted</code>. Refer to the <a href=/docs/concepts/security/pod-security-standards>Pod Security Standards</a>
page for an in-depth look at those requirements.</p><h2 id=pod-security-admission-labels-for-namespaces>Pod Security Admission labels for namespaces</h2><p>Once the feature is enabled or the webhook is installed, you can configure namespaces to define the admission
control mode you want to use for pod security in each namespace. Kubernetes defines a set of
<a class=glossary-tooltip title='Tags objects with identifying attributes that are meaningful and relevant to users.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/labels target=_blank aria-label=labels>labels</a> that you can set to define which of the
predefined Pod Security Standard levels you want to use for a namespace. The label you select
defines what action the <a class=glossary-tooltip title='The container orchestration layer that exposes the API and interfaces to define, deploy, and manage the lifecycle of containers.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-control-plane' target=_blank aria-label='control plane'>control plane</a>
takes if a potential violation is detected:</p><table><caption style=display:none>Pod Security Admission modes</caption><thead><tr><th style=text-align:left>Mode</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:left><strong>enforce</strong></td><td style=text-align:left>Policy violations will cause the pod to be rejected.</td></tr><tr><td style=text-align:left><strong>audit</strong></td><td style=text-align:left>Policy violations will trigger the addition of an audit annotation to the event recorded in the <a href=/docs/tasks/debug/debug-cluster/audit/>audit log</a>, but are otherwise allowed.</td></tr><tr><td style=text-align:left><strong>warn</strong></td><td style=text-align:left>Policy violations will trigger a user-facing warning, but are otherwise allowed.</td></tr></tbody></table><p>A namespace can configure any or all modes, or even set a different level for different modes.</p><p>For each mode, there are two labels that determine the policy used:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#080;font-style:italic># The per-mode level label indicates which policy level to apply for the mode.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic>#</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># MODE must be one of `enforce`, `audit`, or `warn`.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># LEVEL must be one of `privileged`, `baseline`, or `restricted`.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>pod-security.kubernetes.io/&lt;MODE&gt;</span>:<span style=color:#bbb> </span>&lt;LEVEL&gt;<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># Optional: per-mode version label that can be used to pin the policy to the</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># version that shipped with a given Kubernetes minor version (for example v1.25).</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic>#</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># MODE must be one of `enforce`, `audit`, or `warn`.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:#080;font-style:italic># VERSION must be a valid Kubernetes minor version, or `latest`.</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:green;font-weight:700>pod-security.kubernetes.io/&lt;MODE&gt;-version</span>:<span style=color:#bbb> </span>&lt;VERSION&gt;<span style=color:#bbb>
</span></span></span></code></pre></div><p>Check out <a href=/docs/tasks/configure-pod-container/enforce-standards-namespace-labels>Enforce Pod Security Standards with Namespace Labels</a> to see example usage.</p><h2 id=workload-resources-and-pod-templates>Workload resources and Pod templates</h2><p>Pods are often created indirectly, by creating a <a href=/docs/concepts/workloads/controllers/>workload
object</a> such as a <a class=glossary-tooltip title='Manages a replicated application on your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/deployment/ target=_blank aria-label=Deployment>Deployment</a> or <a class=glossary-tooltip title='A finite or batch task that runs to completion.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/controllers/job/ target=_blank aria-label=Job>Job</a>. The workload object defines a
<em>Pod template</em> and a <a class=glossary-tooltip title='A control loop that watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state.' data-toggle=tooltip data-placement=top href=/docs/concepts/architecture/controller/ target=_blank aria-label=controller>controller</a> for the
workload resource creates Pods based on that template. To help catch violations early, both the
audit and warning modes are applied to the workload resources. However, enforce mode is <strong>not</strong>
applied to workload resources, only to the resulting pod objects.</p><h2 id=exemptions>Exemptions</h2><p>You can define <em>exemptions</em> from pod security enforcement in order to allow the creation of pods that
would have otherwise been prohibited due to the policy associated with a given namespace.
Exemptions can be statically configured in the
<a href=/docs/tasks/configure-pod-container/enforce-standards-admission-controller/#configure-the-admission-controller>Admission Controller configuration</a>.</p><p>Exemptions must be explicitly enumerated. Requests meeting exemption criteria are <em>ignored</em> by the
Admission Controller (all <code>enforce</code>, <code>audit</code> and <code>warn</code> behaviors are skipped). Exemption dimensions include:</p><ul><li><strong>Usernames:</strong> requests from users with an exempt authenticated (or impersonated) username are
ignored.</li><li><strong>RuntimeClassNames:</strong> pods and <a href=#workload-resources-and-pod-templates>workload resources</a> specifying an exempt runtime class name are
ignored.</li><li><strong>Namespaces:</strong> pods and <a href=#workload-resources-and-pod-templates>workload resources</a> in an exempt namespace are ignored.</li></ul><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Most pods are created by a controller in response to a <a href=#workload-resources-and-pod-templates>workload
resource</a>, meaning that exempting an end user will only
exempt them from enforcement when creating pods directly, but not when creating a workload resource.
Controller service accounts (such as <code>system:serviceaccount:kube-system:replicaset-controller</code>)
should generally not be exempted, as doing so would implicitly exempt any user that can create the
corresponding workload resource.</div><p>Updates to the following pod fields are exempt from policy checks, meaning that if a pod update
request only changes these fields, it will not be denied even if the pod is in violation of the
current policy level:</p><ul><li>Any metadata updates <strong>except</strong> changes to the seccomp or AppArmor annotations:<ul><li><code>seccomp.security.alpha.kubernetes.io/pod</code> (deprecated)</li><li><code>container.seccomp.security.alpha.kubernetes.io/*</code> (deprecated)</li><li><code>container.apparmor.security.beta.kubernetes.io/*</code></li></ul></li><li>Valid updates to <code>.spec.activeDeadlineSeconds</code></li><li>Valid updates to <code>.spec.tolerations</code></li></ul><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/concepts/security/pod-security-standards>Pod Security Standards</a></li><li><a href=/docs/setup/best-practices/enforcing-pod-security-standards>Enforcing Pod Security Standards</a></li><li><a href=/docs/tasks/configure-pod-container/enforce-standards-admission-controller>Enforce Pod Security Standards by Configuring the Built-in Admission Controller</a></li><li><a href=/docs/tasks/configure-pod-container/enforce-standards-namespace-labels>Enforce Pod Security Standards with Namespace Labels</a></li><li><a href=/docs/tasks/configure-pod-container/migrate-from-psp>Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ac71855bb20cbf21edc666e810f4103a>4 - Pod Security Policies</h1><div class="alert alert-warning" role=alert><h4 class=alert-heading>Removed feature</h4>PodSecurityPolicy was <a href=/blog/2021/04/08/kubernetes-1-21-release-announcement/#podsecuritypolicy-deprecation>deprecated</a>
in Kubernetes v1.21, and removed from Kubernetes in v1.25.</div><p>Instead of using PodSecurityPolicy, you can enforce similar restrictions on Pods using
either or both:</p><ul><li><a href=/docs/concepts/security/pod-security-admission/>Pod Security Admission</a></li><li>a 3rd party admission plugin, that you deploy and configure yourself</li></ul><p>For a migration guide, see <a href=/docs/tasks/configure-pod-container/migrate-from-psp/>Migrate from PodSecurityPolicy to the Built-In PodSecurity Admission Controller</a>.
For more information on the removal of this API,
see <a href=/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/>PodSecurityPolicy Deprecation: Past, Present, and Future</a>.</p><p>If you are not running Kubernetes v1.25, check the documentation for
your version of Kubernetes.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-9a68f631b6bc38c279bbc9a145e34ef2>5 - Security For Windows Nodes</h1><p>This page describes security considerations and best practices specific to the Windows operating system.</p><h2 id=protection-for-secret-data-on-nodes>Protection for Secret data on nodes</h2><p>On Windows, data from Secrets are written out in clear text onto the node's local
storage (as compared to using tmpfs / in-memory filesystems on Linux). As a cluster
operator, you should take both of the following additional measures:</p><ol><li>Use file ACLs to secure the Secrets' file location.</li><li>Apply volume-level encryption using
<a href=https://docs.microsoft.com/windows/security/information-protection/bitlocker/bitlocker-how-to-deploy-on-windows-server>BitLocker</a>.</li></ol><h2 id=container-users>Container users</h2><p><a href=/docs/tasks/configure-pod-container/configure-runasusername>RunAsUsername</a>
can be specified for Windows Pods or containers to execute the container
processes as specific user. This is roughly equivalent to
<a href=/docs/concepts/security/pod-security-policy/#users-and-groups>RunAsUser</a>.</p><p>Windows containers offer two default user accounts, ContainerUser and ContainerAdministrator.
The differences between these two user accounts are covered in
<a href=https://docs.microsoft.com/virtualization/windowscontainers/manage-containers/container-security#when-to-use-containeradmin-and-containeruser-user-accounts>When to use ContainerAdmin and ContainerUser user accounts</a>
within Microsoft's <em>Secure Windows containers</em> documentation.</p><p>Local users can be added to container images during the container build process.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong><ul><li><a href=https://hub.docker.com/_/microsoft-windows-nanoserver>Nano Server</a> based images run as
<code>ContainerUser</code> by default</li><li><a href=https://hub.docker.com/_/microsoft-windows-servercore>Server Core</a> based images run as
<code>ContainerAdministrator</code> by default</li></ul></div><p>Windows containers can also run as Active Directory identities by utilizing
<a href=/docs/tasks/configure-pod-container/configure-gmsa/>Group Managed Service Accounts</a></p><h2 id=pod-level-security-isolation>Pod-level security isolation</h2><p>Linux-specific pod security context mechanisms (such as SELinux, AppArmor, Seccomp, or custom
POSIX capabilities) are not supported on Windows nodes.</p><p>Privileged containers are <a href=/docs/concepts/windows/intro/#compatibility-v1-pod-spec-containers-securitycontext>not supported</a>
on Windows.
Instead <a href=/docs/tasks/configure-pod-container/create-hostprocess-pod>HostProcess containers</a>
can be used on Windows to perform many of the tasks performed by privileged containers on Linux.</p></div><div class=td-content style=page-break-before:always><h1 id=pg-4d77d1ae4c06aa14f54b385191627881>6 - Controlling Access to the Kubernetes API</h1><p>This page provides an overview of controlling access to the Kubernetes API.</p><p>Users access the <a href=/docs/concepts/overview/kubernetes-api/>Kubernetes API</a> using <code>kubectl</code>,
client libraries, or by making REST requests. Both human users and
<a href=/docs/tasks/configure-pod-container/configure-service-account/>Kubernetes service accounts</a> can be
authorized for API access.
When a request reaches the API, it goes through several stages, illustrated in the
following diagram:</p><p><img src=/images/docs/admin/access-control-overview.svg alt="Diagram of request handling steps for Kubernetes API request"></p><h2 id=transport-security>Transport security</h2><p>By default, the Kubernetes API server listens on port 6443 on the first non-localhost network interface, protected by TLS. In a typical production Kubernetes cluster, the API serves on port 443. The port can be changed with the <code>--secure-port</code>, and the listening IP address with the <code>--bind-address</code> flag.</p><p>The API server presents a certificate. This certificate may be signed using
a private certificate authority (CA), or based on a public key infrastructure linked
to a generally recognized CA. The certificate and corresponding private key can be set by using the <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> flags.</p><p>If your cluster uses a private certificate authority, you need a copy of that CA
certificate configured into your <code>~/.kube/config</code> on the client, so that you can
trust the connection and be confident it was not intercepted.</p><p>Your client can present a TLS client certificate at this stage.</p><h2 id=authentication>Authentication</h2><p>Once TLS is established, the HTTP request moves to the Authentication step.
This is shown as step <strong>1</strong> in the diagram.
The cluster creation script or cluster admin configures the API server to run
one or more Authenticator modules.
Authenticators are described in more detail in
<a href=/docs/reference/access-authn-authz/authentication/>Authentication</a>.</p><p>The input to the authentication step is the entire HTTP request; however, it typically
examines the headers and/or client certificate.</p><p>Authentication modules include client certificates, password, and plain tokens,
bootstrap tokens, and JSON Web Tokens (used for service accounts).</p><p>Multiple authentication modules can be specified, in which case each one is tried in sequence,
until one of them succeeds.</p><p>If the request cannot be authenticated, it is rejected with HTTP status code 401.
Otherwise, the user is authenticated as a specific <code>username</code>, and the user name
is available to subsequent steps to use in their decisions. Some authenticators
also provide the group memberships of the user, while other authenticators
do not.</p><p>While Kubernetes uses usernames for access control decisions and in request logging,
it does not have a <code>User</code> object nor does it store usernames or other information about
users in its API.</p><h2 id=authorization>Authorization</h2><p>After the request is authenticated as coming from a specific user, the request must be authorized. This is shown as step <strong>2</strong> in the diagram.</p><p>A request must include the username of the requester, the requested action, and the object affected by the action. The request is authorized if an existing policy declares that the user has permissions to complete the requested action.</p><p>For example, if Bob has the policy below, then he can read pods only in the namespace <code>projectCaribou</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;abac.authorization.kubernetes.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;Policy&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;spec&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;user&#34;</span>: <span style=color:#b44>&#34;bob&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;projectCaribou&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;pods&#34;</span>,
</span></span><span style=display:flex><span>        <span style=color:green;font-weight:700>&#34;readonly&#34;</span>: <span style=color:#a2f;font-weight:700>true</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If Bob makes the following request, the request is authorized because he is allowed to read objects in the <code>projectCaribou</code> namespace:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;apiVersion&#34;</span>: <span style=color:#b44>&#34;authorization.k8s.io/v1beta1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;kind&#34;</span>: <span style=color:#b44>&#34;SubjectAccessReview&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:green;font-weight:700>&#34;spec&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:green;font-weight:700>&#34;resourceAttributes&#34;</span>: {
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;namespace&#34;</span>: <span style=color:#b44>&#34;projectCaribou&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;verb&#34;</span>: <span style=color:#b44>&#34;get&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;group&#34;</span>: <span style=color:#b44>&#34;unicorn.example.org&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:green;font-weight:700>&#34;resource&#34;</span>: <span style=color:#b44>&#34;pods&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If Bob makes a request to write (<code>create</code> or <code>update</code>) to the objects in the <code>projectCaribou</code> namespace, his authorization is denied. If Bob makes a request to read (<code>get</code>) objects in a different namespace such as <code>projectFish</code>, then his authorization is denied.</p><p>Kubernetes authorization requires that you use common REST attributes to interact with existing organization-wide or cloud-provider-wide access control systems. It is important to use REST formatting because these control systems might interact with other APIs besides the Kubernetes API.</p><p>Kubernetes supports multiple authorization modules, such as ABAC mode, RBAC Mode, and Webhook mode. When an administrator creates a cluster, they configure the authorization modules that should be used in the API server. If more than one authorization modules are configured, Kubernetes checks each module, and if any module authorizes the request, then the request can proceed. If all of the modules deny the request, then the request is denied (HTTP status code 403).</p><p>To learn more about Kubernetes authorization, including details about creating policies using the supported authorization modules, see <a href=/docs/reference/access-authn-authz/authorization/>Authorization</a>.</p><h2 id=admission-control>Admission control</h2><p>Admission Control modules are software modules that can modify or reject requests.
In addition to the attributes available to Authorization modules, Admission
Control modules can access the contents of the object that is being created or modified.</p><p>Admission controllers act on requests that create, modify, delete, or connect to (proxy) an object.
Admission controllers do not act on requests that merely read objects.
When multiple admission controllers are configured, they are called in order.</p><p>This is shown as step <strong>3</strong> in the diagram.</p><p>Unlike Authentication and Authorization modules, if any admission controller module
rejects, then the request is immediately rejected.</p><p>In addition to rejecting objects, admission controllers can also set complex defaults for
fields.</p><p>The available Admission Control modules are described in <a href=/docs/reference/access-authn-authz/admission-controllers/>Admission Controllers</a>.</p><p>Once a request passes all admission controllers, it is validated using the validation routines
for the corresponding API object, and then written to the object store (shown as step <strong>4</strong>).</p><h2 id=auditing>Auditing</h2><p>Kubernetes auditing provides a security-relevant, chronological set of records documenting the sequence of actions in a cluster.
The cluster audits the activities generated by users, by applications that use the Kubernetes API, and by the control plane itself.</p><p>For more information, see <a href=/docs/tasks/debug/debug-cluster/audit/>Auditing</a>.</p><h2 id=what-s-next>What's next</h2><p>Read more documentation on authentication, authorization and API access control:</p><ul><li><a href=/docs/reference/access-authn-authz/authentication/>Authenticating</a><ul><li><a href=/docs/reference/access-authn-authz/bootstrap-tokens/>Authenticating with Bootstrap Tokens</a></li></ul></li><li><a href=/docs/reference/access-authn-authz/admission-controllers/>Admission Controllers</a><ul><li><a href=/docs/reference/access-authn-authz/extensible-admission-controllers/>Dynamic Admission Control</a></li></ul></li><li><a href=/docs/reference/access-authn-authz/authorization/>Authorization</a><ul><li><a href=/docs/reference/access-authn-authz/rbac/>Role Based Access Control</a></li><li><a href=/docs/reference/access-authn-authz/abac/>Attribute Based Access Control</a></li><li><a href=/docs/reference/access-authn-authz/node/>Node Authorization</a></li><li><a href=/docs/reference/access-authn-authz/webhook/>Webhook Authorization</a></li></ul></li><li><a href=/docs/reference/access-authn-authz/certificate-signing-requests/>Certificate Signing Requests</a><ul><li>including <a href=/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection>CSR approval</a>
and <a href=/docs/reference/access-authn-authz/certificate-signing-requests/#signing>certificate signing</a></li></ul></li><li>Service accounts<ul><li><a href=/docs/tasks/configure-pod-container/configure-service-account/>Developer guide</a></li><li><a href=/docs/reference/access-authn-authz/service-accounts-admin/>Administration</a></li></ul></li></ul><p>You can learn about:</p><ul><li>how Pods can use
<a href=/docs/concepts/configuration/secret/#service-accounts-automatically-create-and-attach-secrets-with-api-credentials>Secrets</a>
to obtain API credentials.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-07f58aa0218d666795499c2e2306ff96>7 - Role Based Access Control Good Practices</h1><div class=lead>Principles and practices for good RBAC design for cluster operators.</div><p>Kubernetes <a class=glossary-tooltip title='Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/rbac/ target=_blank aria-label=RBAC>RBAC</a> is a key security control
to ensure that cluster users and workloads have only the access to resources required to
execute their roles. It is important to ensure that, when designing permissions for cluster
users, the cluster administrator understands the areas where privilege escalation could occur,
to reduce the risk of excessive access leading to security incidents.</p><p>The good practices laid out here should be read in conjunction with the general
<a href=/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update>RBAC documentation</a>.</p><h2 id=general-good-practice>General good practice</h2><h3 id=least-privilege>Least privilege</h3><p>Ideally, minimal RBAC rights should be assigned to users and service accounts. Only permissions
explicitly required for their operation should be used. While each cluster will be different,
some general rules that can be applied are :</p><ul><li>Assign permissions at the namespace level where possible. Use RoleBindings as opposed to
ClusterRoleBindings to give users rights only within a specific namespace.</li><li>Avoid providing wildcard permissions when possible, especially to all resources.
As Kubernetes is an extensible system, providing wildcard access gives rights
not just to all object types that currently exist in the cluster, but also to all object types
which are created in the future.</li><li>Administrators should not use <code>cluster-admin</code> accounts except where specifically needed.
Providing a low privileged account with
<a href=/docs/reference/access-authn-authz/authentication/#user-impersonation>impersonation rights</a>
can avoid accidental modification of cluster resources.</li><li>Avoid adding users to the <code>system:masters</code> group. Any user who is a member of this group
bypasses all RBAC rights checks and will always have unrestricted superuser access, which cannot be
revoked by removing RoleBindings or ClusterRoleBindings. As an aside, if a cluster is
using an authorization webhook, membership of this group also bypasses that webhook (requests
from users who are members of that group are never sent to the webhook)</li></ul><h3 id=minimize-distribution-of-privileged-tokens>Minimize distribution of privileged tokens</h3><p>Ideally, pods shouldn't be assigned service accounts that have been granted powerful permissions
(for example, any of the rights listed under <a href=#privilege-escalation-risks>privilege escalation risks</a>).
In cases where a workload requires powerful permissions, consider the following practices:</p><ul><li>Limit the number of nodes running powerful pods. Ensure that any DaemonSets you run
are necessary and are run with least privilege to limit the blast radius of container escapes.</li><li>Avoid running powerful pods alongside untrusted or publicly-exposed ones. Consider using
<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>Taints and Toleration</a>,
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity>NodeAffinity</a>, or
<a href=/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity>PodAntiAffinity</a>
to ensure pods don't run alongside untrusted or less-trusted Pods. Pay especial attention to
situations where less-trustworthy Pods are not meeting the <strong>Restricted</strong> Pod Security Standard.</li></ul><h3 id=hardening>Hardening</h3><p>Kubernetes defaults to providing access which may not be required in every cluster. Reviewing
the RBAC rights provided by default can provide opportunities for security hardening.
In general, changes should not be made to rights provided to <code>system:</code> accounts some options
to harden cluster rights exist:</p><ul><li>Review bindings for the <code>system:unauthenticated</code> group and remove them where possible, as this gives
access to anyone who can contact the API server at a network level.</li><li>Avoid the default auto-mounting of service account tokens by setting
<code>automountServiceAccountToken: false</code>. For more details, see
<a href=/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server>using default service account token</a>.
Setting this value for a Pod will overwrite the service account setting, workloads
which require service account tokens can still mount them.</li></ul><h3 id=periodic-review>Periodic review</h3><p>It is vital to periodically review the Kubernetes RBAC settings for redundant entries and
possible privilege escalations.
If an attacker is able to create a user account with the same name as a deleted user,
they can automatically inherit all the rights of the deleted user, especially the
rights assigned to that user.</p><h2 id=privilege-escalation-risks>Kubernetes RBAC - privilege escalation risks</h2><p>Within Kubernetes RBAC there are a number of privileges which, if granted, can allow a user or a service account
to escalate their privileges in the cluster or affect systems outside the cluster.</p><p>This section is intended to provide visibility of the areas where cluster operators
should take care, to ensure that they do not inadvertently allow for more access to clusters than intended.</p><h3 id=listing-secrets>Listing secrets</h3><p>It is generally clear that allowing <code>get</code> access on Secrets will allow a user to read their contents.
It is also important to note that <code>list</code> and <code>watch</code> access also effectively allow for users to reveal the Secret contents.
For example, when a List response is returned (for example, via <code>kubectl get secrets -A -o yaml</code>), the response
includes the contents of all Secrets.</p><h3 id=workload-creation>Workload creation</h3><p>Permission to create workloads (either Pods, or
<a href=/docs/concepts/workloads/controllers/>workload resources</a> that manage Pods) in a namespace
implicitly grants access to many other resources in that namespace, such as Secrets, ConfigMaps, and
PersistentVolumes that can be mounted in Pods. Additionally, since Pods can run as any
<a href=/docs/reference/access-authn-authz/service-accounts-admin/>ServiceAccount</a>, granting permission
to create workloads also implicitly grants the API access levels of any service account in that
namespace.</p><p>Users who can run privileged Pods can use that access to gain node access and potentially to
further elevate their privileges. Where you do not fully trust a user or other principal
with the ability to create suitably secure and isolated Pods, you should enforce either the
<strong>Baseline</strong> or <strong>Restricted</strong> Pod Security Standard.
You can use <a href=/docs/concepts/security/pod-security-admission/>Pod Security admission</a>
or other (third party) mechanisms to implement that enforcement.</p><p>For these reasons, namespaces should be used to separate resources requiring different levels of
trust or tenancy. It is still considered best practice to follow <a href=#least-privilege>least privilege</a>
principles and assign the minimum set of permissions, but boundaries within a namespace should be
considered weak.</p><h3 id=persistent-volume-creation>Persistent volume creation</h3><p>As noted in the <a href=/docs/concepts/security/pod-security-policy/#volumes-and-file-systems>PodSecurityPolicy</a>
documentation, access to create PersistentVolumes can allow for escalation of access to the underlying host.
Where access to persistent storage is required trusted administrators should create
PersistentVolumes, and constrained users should use PersistentVolumeClaims to access that storage.</p><h3 id=access-to-proxy-subresource-of-nodes>Access to <code>proxy</code> subresource of Nodes</h3><p>Users with access to the proxy sub-resource of node objects have rights to the Kubelet API,
which allows for command execution on every pod on the node(s) to which they have rights.
This access bypasses audit logging and admission control, so care should be taken before
granting rights to this resource.</p><h3 id=escalate-verb>Escalate verb</h3><p>Generally, the RBAC system prevents users from creating clusterroles with more rights than the user possesses.
The exception to this is the <code>escalate</code> verb. As noted in the <a href=/docs/reference/access-authn-authz/rbac/#restrictions-on-role-creation-or-update>RBAC documentation</a>,
users with this right can effectively escalate their privileges.</p><h3 id=bind-verb>Bind verb</h3><p>Similar to the <code>escalate</code> verb, granting users this right allows for the bypass of Kubernetes
in-built protections against privilege escalation, allowing users to create bindings to
roles with rights they do not already have.</p><h3 id=impersonate-verb>Impersonate verb</h3><p>This verb allows users to impersonate and gain the rights of other users in the cluster.
Care should be taken when granting it, to ensure that excessive permissions cannot be gained
via one of the impersonated accounts.</p><h3 id=csrs-and-certificate-issuing>CSRs and certificate issuing</h3><p>The CSR API allows for users with <code>create</code> rights to CSRs and <code>update</code> rights on <code>certificatesigningrequests/approval</code>
where the signer is <code>kubernetes.io/kube-apiserver-client</code> to create new client certificates
which allow users to authenticate to the cluster. Those client certificates can have arbitrary
names including duplicates of Kubernetes system components. This will effectively allow for privilege escalation.</p><h3 id=token-request>Token request</h3><p>Users with <code>create</code> rights on <code>serviceaccounts/token</code> can create TokenRequests to issue
tokens for existing service accounts.</p><h3 id=control-admission-webhooks>Control admission webhooks</h3><p>Users with control over <code>validatingwebhookconfigurations</code> or <code>mutatingwebhookconfigurations</code>
can control webhooks that can read any object admitted to the cluster, and in the case of
mutating webhooks, also mutate admitted objects.</p><h2 id=denial-of-service-risks>Kubernetes RBAC - denial of service risks</h2><h3 id=object-creation-dos>Object creation denial-of-service</h3><p>Users who have rights to create objects in a cluster may be able to create sufficient large
objects to create a denial of service condition either based on the size or number of objects, as discussed in
<a href=https://github.com/kubernetes/kubernetes/issues/107325>etcd used by Kubernetes is vulnerable to OOM attack</a>. This may be
specifically relevant in multi-tenant clusters if semi-trusted or untrusted users
are allowed limited access to a system.</p><p>One option for mitigation of this issue would be to use
<a href=/docs/concepts/policy/resource-quotas/#object-count-quota>resource quotas</a>
to limit the quantity of objects which can be created.</p><h2 id=what-s-next>What's next</h2><ul><li>To learn more about RBAC, see the <a href=/docs/reference/access-authn-authz/rbac/>RBAC documentation</a>.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-a7863bfad3d69f33f5b318b9028eecb8>8 - Good practices for Kubernetes Secrets</h1><div class=lead>Principles and practices for good Secret management for cluster administrators and application developers.</div><p><p>In Kubernetes, a Secret is an object that stores sensitive information, such as passwords, OAuth tokens, and SSH keys.</p></p><p>Secrets give you more control over how sensitive information is used and reduces
the risk of accidental exposure. Secret values are encoded as base64 strings and
are stored unencrypted by default, but can be configured to be
<a href=/docs/tasks/administer-cluster/encrypt-data/#ensure-all-secrets-are-encrypted>encrypted at rest</a>.</p><p>A <a class=glossary-tooltip title='A Pod represents a set of running containers in your cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/workloads/pods/ target=_blank aria-label=Pod>Pod</a> can reference the Secret in
a variety of ways, such as in a volume mount or as an environment variable.
Secrets are designed for confidential data and
<a href=/docs/tasks/configure-pod-container/configure-pod-configmap/>ConfigMaps</a> are
designed for non-confidential data.</p><p>The following good practices are intended for both cluster administrators and
application developers. Use these guidelines to improve the security of your
sensitive information in Secret objects, as well as to more effectively manage
your Secrets.</p><h2 id=cluster-administrators>Cluster administrators</h2><p>This section provides good practices that cluster administrators can use to
improve the security of confidential information in the cluster.</p><h3 id=configure-encryption-at-rest>Configure encryption at rest</h3><p>By default, Secret objects are stored unencrypted in <a class=glossary-tooltip title='Consistent and highly-available key value store used as backing store of Kubernetes for all cluster data.' data-toggle=tooltip data-placement=top href=/docs/tasks/administer-cluster/configure-upgrade-etcd/ target=_blank aria-label=etcd>etcd</a>. You should configure encryption of your Secret
data in <code>etcd</code>. For instructions, refer to
<a href=/docs/tasks/administer-cluster/encrypt-data/>Encrypt Secret Data at Rest</a>.</p><h3 id=least-privilege-secrets>Configure least-privilege access to Secrets</h3><p>When planning your access control mechanism, such as Kubernetes
<a class=glossary-tooltip title='Manages authorization decisions, allowing admins to dynamically configure access policies through the Kubernetes API.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/rbac/ target=_blank aria-label='Role-based Access Control'>Role-based Access Control</a> <a href=/docs/reference/access-authn-authz/rbac/>(RBAC)</a>,
consider the following guidelines for access to <code>Secret</code> objects. You should
also follow the other guidelines in
<a href=/docs/concepts/security/rbac-good-practices>RBAC good practices</a>.</p><ul><li><strong>Components</strong>: Restrict <code>watch</code> or <code>list</code> access to only the most
privileged, system-level components. Only grant <code>get</code> access for Secrets if
the component's normal behavior requires it.</li><li><strong>Humans</strong>: Restrict <code>get</code>, <code>watch</code>, or <code>list</code> access to Secrets. Only allow
cluster administrators to access <code>etcd</code>. This includes read-only access. For
more complex access control, such as restricting access to Secrets with
specific annotations, consider using third-party authorization mechanisms.</li></ul><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Granting <code>list</code> access to Secrets implicitly lets the subject fetch the
contents of the Secrets.</div><p>A user who can create a Pod that uses a Secret can also see the value of that
Secret. Even if cluster policies do not allow a user to read the Secret
directly, the same user could have access to run a Pod that then exposes the
Secret. You can detect or limit the impact caused by Secret data being exposed,
either intentionally or unintentionally, by a user with this access. Some
recommendations include:</p><ul><li>Use short-lived Secrets</li><li>Implement audit rules that alert on specific events, such as concurrent
reading of multiple Secrets by a single user</li></ul><h3 id=improve-etcd-management-policies>Improve etcd management policies</h3><p>Consider wiping or shredding the durable storage used by <code>etcd</code> once it is
no longer in use.</p><p>If there are multiple <code>etcd</code> instances, configure encrypted SSL/TLS
communication between the instances to protect the Secret data in transit.</p><h3 id=configure-access-to-external-secrets>Configure access to external Secrets</h3><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>You can use third-party Secrets store providers to keep your confidential data
outside your cluster and then configure Pods to access that information.
The <a href=https://secrets-store-csi-driver.sigs.k8s.io/>Kubernetes Secrets Store CSI Driver</a>
is a DaemonSet that lets the kubelet retrieve Secrets from external stores, and
mount the Secrets as a volume into specific Pods that you authorize to access
the data.</p><p>For a list of supported providers, refer to
<a href=https://secrets-store-csi-driver.sigs.k8s.io/concepts.html#provider-for-the-secrets-store-csi-driver>Providers for the Secret Store CSI Driver</a>.</p><h2 id=developers>Developers</h2><p>This section provides good practices for developers to use to improve the
security of confidential data when building and deploying Kubernetes resources.</p><h3 id=restrict-secret-access-to-specific-containers>Restrict Secret access to specific containers</h3><p>If you are defining multiple containers in a Pod, and only one of those
containers needs access to a Secret, define the volume mount or environment
variable configuration so that the other containers do not have access to that
Secret.</p><h3 id=protect-secret-data-after-reading>Protect Secret data after reading</h3><p>Applications still need to protect the value of confidential information after
reading it from an environment variable or volume. For example, your
application must avoid logging the secret data in the clear or transmitting it
to an untrusted party.</p><h3 id=avoid-sharing-secret-manifests>Avoid sharing Secret manifests</h3><p>If you configure a Secret through a
<a class=glossary-tooltip title='A serialized specification of one or more Kubernetes API objects.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-manifest' target=_blank aria-label=manifest>manifest</a>, with the secret
data encoded as base64, sharing this file or checking it in to a source
repository means the secret is available to everyone who can read the manifest.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Base64 encoding is <em>not</em> an encryption method, it provides no additional
confidentiality over plain text.</div></div><div class=td-content style=page-break-before:always><h1 id=pg-9dd9b8c71fa39ff803fd15b0e784069d>9 - Multi-tenancy</h1><p>This page provides an overview of available configuration options and best practices for cluster
multi-tenancy.</p><p>Sharing clusters saves costs and simplifies administration. However, sharing clusters also
presents challenges such as security, fairness, and managing <em>noisy neighbors</em>.</p><p>Clusters can be shared in many ways. In some cases, different applications may run in the same
cluster. In other cases, multiple instances of the same application may run in the same cluster,
one for each end user. All these types of sharing are frequently described using the umbrella term
<em>multi-tenancy</em>.</p><p>While Kubernetes does not have first-class concepts of end users or tenants, it provides several
features to help manage different tenancy requirements. These are discussed below.</p><h2 id=use-cases>Use cases</h2><p>The first step to determining how to share your cluster is understanding your use case, so you can
evaluate the patterns and tools available. In general, multi-tenancy in Kubernetes clusters falls
into two broad categories, though many variations and hybrids are also possible.</p><h3 id=multiple-teams>Multiple teams</h3><p>A common form of multi-tenancy is to share a cluster between multiple teams within an
organization, each of whom may operate one or more workloads. These workloads frequently need to
communicate with each other, and with other workloads located on the same or different clusters.</p><p>In this scenario, members of the teams often have direct access to Kubernetes resources via tools
such as <code>kubectl</code>, or indirect access through GitOps controllers or other types of release
automation tools. There is often some level of trust between members of different teams, but
Kubernetes policies such as RBAC, quotas, and network policies are essential to safely and fairly
share clusters.</p><h3 id=multiple-customers>Multiple customers</h3><p>The other major form of multi-tenancy frequently involves a Software-as-a-Service (SaaS) vendor
running multiple instances of a workload for customers. This business model is so strongly
associated with this deployment style that many people call it "SaaS tenancy." However, a better
term might be "multi-customer tenancy,” since SaaS vendors may also use other deployment models,
and this deployment model can also be used outside of SaaS.</p><p>In this scenario, the customers do not have access to the cluster; Kubernetes is invisible from
their perspective and is only used by the vendor to manage the workloads. Cost optimization is
frequently a critical concern, and Kubernetes policies are used to ensure that the workloads are
strongly isolated from each other.</p><h2 id=terminology>Terminology</h2><h3 id=tenants>Tenants</h3><p>When discussing multi-tenancy in Kubernetes, there is no single definition for a "tenant".
Rather, the definition of a tenant will vary depending on whether multi-team or multi-customer
tenancy is being discussed.</p><p>In multi-team usage, a tenant is typically a team, where each team typically deploys a small
number of workloads that scales with the complexity of the service. However, the definition of
"team" may itself be fuzzy, as teams may be organized into higher-level divisions or subdivided
into smaller teams.</p><p>By contrast, if each team deploys dedicated workloads for each new client, they are using a
multi-customer model of tenancy. In this case, a "tenant" is simply a group of users who share a
single workload. This may be as large as an entire company, or as small as a single team at that
company.</p><p>In many cases, the same organization may use both definitions of "tenants" in different contexts.
For example, a platform team may offer shared services such as security tools and databases to
multiple internal “customers” and a SaaS vendor may also have multiple teams sharing a development
cluster. Finally, hybrid architectures are also possible, such as a SaaS provider using a
combination of per-customer workloads for sensitive data, combined with multi-tenant shared
services.</p><figure class=diagram-large><img src=/images/docs/multi-tenancy.png><figcaption><h4>A cluster showing coexisting tenancy models</h4></figcaption></figure><h3 id=isolation>Isolation</h3><p>There are several ways to design and build multi-tenant solutions with Kubernetes. Each of these
methods comes with its own set of tradeoffs that impact the isolation level, implementation
effort, operational complexity, and cost of service.</p><p>A Kubernetes cluster consists of a control plane which runs Kubernetes software, and a data plane
consisting of worker nodes where tenant workloads are executed as pods. Tenant isolation can be
applied in both the control plane and the data plane based on organizational requirements.</p><p>The level of isolation offered is sometimes described using terms like “hard” multi-tenancy, which
implies strong isolation, and “soft” multi-tenancy, which implies weaker isolation. In particular,
"hard" multi-tenancy is often used to describe cases where the tenants do not trust each other,
often from security and resource sharing perspectives (e.g. guarding against attacks such as data
exfiltration or DoS). Since data planes typically have much larger attack surfaces, "hard"
multi-tenancy often requires extra attention to isolating the data-plane, though control plane
isolation also remains critical.</p><p>However, the terms "hard" and "soft" can often be confusing, as there is no single definition that
will apply to all users. Rather, "hardness" or "softness" is better understood as a broad
spectrum, with many different techniques that can be used to maintain different types of isolation
in your clusters, based on your requirements.</p><p>In more extreme cases, it may be easier or necessary to forgo any cluster-level sharing at all and
assign each tenant their dedicated cluster, possibly even running on dedicated hardware if VMs are
not considered an adequate security boundary. This may be easier with managed Kubernetes clusters,
where the overhead of creating and operating clusters is at least somewhat taken on by a cloud
provider. The benefit of stronger tenant isolation must be evaluated against the cost and
complexity of managing multiple clusters. The <a href=https://git.k8s.io/community/sig-multicluster/README.md>Multi-cluster SIG</a>
is responsible for addressing these types of use cases.</p><p>The remainder of this page focuses on isolation techniques used for shared Kubernetes clusters.
However, even if you are considering dedicated clusters, it may be valuable to review these
recommendations, as it will give you the flexibility to shift to shared clusters in the future if
your needs or capabilities change.</p><h2 id=control-plane-isolation>Control plane isolation</h2><p>Control plane isolation ensures that different tenants cannot access or affect each others'
Kubernetes API resources.</p><h3 id=namespaces>Namespaces</h3><p>In Kubernetes, a <a class=glossary-tooltip title='An abstraction used by Kubernetes to support isolation of groups of resources within a single cluster.' data-toggle=tooltip data-placement=top href=/docs/concepts/overview/working-with-objects/namespaces target=_blank aria-label=Namespace>Namespace</a> provides a
mechanism for isolating groups of API resources within a single cluster. This isolation has two
key dimensions:</p><ol><li><p>Object names within a namespace can overlap with names in other namespaces, similar to files in
folders. This allows tenants to name their resources without having to consider what other
tenants are doing.</p></li><li><p>Many Kubernetes security policies are scoped to namespaces. For example, RBAC Roles and Network
Policies are namespace-scoped resources. Using RBAC, Users and Service Accounts can be
restricted to a namespace.</p></li></ol><p>In a multi-tenant environment, a Namespace helps segment a tenant's workload into a logical and
distinct management unit. In fact, a common practice is to isolate every workload in its own
namespace, even if multiple workloads are operated by the same tenant. This ensures that each
workload has its own identity and can be configured with an appropriate security policy.</p><p>The namespace isolation model requires configuration of several other Kubernetes resources,
networking plugins, and adherence to security best practices to properly isolate tenant workloads.
These considerations are discussed below.</p><h3 id=access-controls>Access controls</h3><p>The most important type of isolation for the control plane is authorization. If teams or their
workloads can access or modify each others' API resources, they can change or disable all other
types of policies thereby negating any protection those policies may offer. As a result, it is
critical to ensure that each tenant has the appropriate access to only the namespaces they need,
and no more. This is known as the "Principle of Least Privilege."</p><p>Role-based access control (RBAC) is commonly used to enforce authorization in the Kubernetes
control plane, for both users and workloads (service accounts).
<a href=/docs/reference/access-authn-authz/rbac/#role-and-clusterrole>Roles</a> and
<a href=/docs/reference/access-authn-authz/rbac/#rolebinding-and-clusterrolebinding>RoleBindings</a> are
Kubernetes objects that are used at a namespace level to enforce access control in your
application; similar objects exist for authorizing access to cluster-level objects, though these
are less useful for multi-tenant clusters.</p><p>In a multi-team environment, RBAC must be used to restrict tenants' access to the appropriate
namespaces, and ensure that cluster-wide resources can only be accessed or modified by privileged
users such as cluster administrators.</p><p>If a policy ends up granting a user more permissions than they need, this is likely a signal that
the namespace containing the affected resources should be refactored into finer-grained
namespaces. Namespace management tools may simplify the management of these finer-grained
namespaces by applying common RBAC policies to different namespaces, while still allowing
fine-grained policies where necessary.</p><h3 id=quotas>Quotas</h3><p>Kubernetes workloads consume node resources, like CPU and memory. In a multi-tenant environment,
you can use <a href=/docs/concepts/policy/resource-quotas/>Resource Quotas</a> to manage resource usage of
tenant workloads. For the multiple teams use case, where tenants have access to the Kubernetes
API, you can use resource quotas to limit the number of API resources (for example: the number of
Pods, or the number of ConfigMaps) that a tenant can create. Limits on object count ensure
fairness and aim to avoid <em>noisy neighbor</em> issues from affecting other tenants that share a
control plane.</p><p>Resource quotas are namespaced objects. By mapping tenants to namespaces, cluster admins can use
quotas to ensure that a tenant cannot monopolize a cluster's resources or overwhelm its control
plane. Namespace management tools simplify the administration of quotas. In addition, while
Kubernetes quotas only apply within a single namespace, some namespace management tools allow
groups of namespaces to share quotas, giving administrators far more flexibility with less effort
than built-in quotas.</p><p>Quotas prevent a single tenant from consuming greater than their allocated share of resources
hence minimizing the “noisy neighbor” issue, where one tenant negatively impacts the performance
of other tenants' workloads.</p><p>When you apply a quota to namespace, Kubernetes requires you to also specify resource requests and
limits for each container. Limits are the upper bound for the amount of resources that a container
can consume. Containers that attempt to consume resources that exceed the configured limits will
either be throttled or killed, based on the resource type. When resource requests are set lower
than limits, each container is guaranteed the requested amount but there may still be some
potential for impact across workloads.</p><p>Quotas cannot protect against all kinds of resource sharing, such as network traffic.
Node isolation (described below) may be a better solution for this problem.</p><h2 id=data-plane-isolation>Data Plane Isolation</h2><p>Data plane isolation ensures that pods and workloads for different tenants are sufficiently
isolated.</p><h3 id=network-isolation>Network isolation</h3><p>By default, all pods in a Kubernetes cluster are allowed to communicate with each other, and all
network traffic is unencrypted. This can lead to security vulnerabilities where traffic is
accidentally or maliciously sent to an unintended destination, or is intercepted by a workload on
a compromised node.</p><p>Pod-to-pod communication can be controlled using <a href=/docs/concepts/services-networking/network-policies/>Network Policies</a>,
which restrict communication between pods using namespace labels or IP address ranges.
In a multi-tenant environment where strict network isolation between tenants is required, starting
with a default policy that denies communication between pods is recommended with another rule that
allows all pods to query the DNS server for name resolution. With such a default policy in place,
you can begin adding more permissive rules that allow for communication within a namespace.
It is also recommended not to use empty label selector '{}' for namespaceSelector field in network policy definition,
in case traffic need to be allowed between namespaces.
This scheme can be further refined as required. Note that this only applies to pods within a single
control plane; pods that belong to different virtual control planes cannot talk to each other via
Kubernetes networking.</p><p>Namespace management tools may simplify the creation of default or common network policies.
In addition, some of these tools allow you to enforce a consistent set of namespace labels across
your cluster, ensuring that they are a trusted basis for your policies.</p><div class="alert alert-danger warning callout" role=alert><strong>Warning:</strong> Network policies require a <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni>CNI plugin</a>
that supports the implementation of network policies. Otherwise, NetworkPolicy resources will be ignored.</div><p>More advanced network isolation may be provided by service meshes, which provide OSI Layer 7
policies based on workload identity, in addition to namespaces. These higher-level policies can
make it easier to manage namespace-based multi-tenancy, especially when multiple namespaces are
dedicated to a single tenant. They frequently also offer encryption using mutual TLS, protecting
your data even in the presence of a compromised node, and work across dedicated or virtual clusters.
However, they can be significantly more complex to manage and may not be appropriate for all users.</p><h3 id=storage-isolation>Storage isolation</h3><p>Kubernetes offers several types of volumes that can be used as persistent storage for workloads.
For security and data-isolation, <a href=/docs/concepts/storage/dynamic-provisioning/>dynamic volume provisioning</a>
is recommended and volume types that use node resources should be avoided.</p><p><a href=/docs/concepts/storage/storage-classes/>StorageClasses</a> allow you to describe custom "classes"
of storage offered by your cluster, based on quality-of-service levels, backup policies, or custom
policies determined by the cluster administrators.</p><p>Pods can request storage using a <a href=/docs/concepts/storage/persistent-volumes/>PersistentVolumeClaim</a>.
A PersistentVolumeClaim is a namespaced resource, which enables isolating portions of the storage
system and dedicating it to tenants within the shared Kubernetes cluster.
However, it is important to note that a PersistentVolume is a cluster-wide resource and has a
lifecycle independent of workloads and namespaces.</p><p>For example, you can configure a separate StorageClass for each tenant and use this to strengthen isolation.
If a StorageClass is shared, you should set a <a href=/docs/concepts/storage/storage-classes/#reclaim-policy>reclaim policy of <code>Delete</code></a>
to ensure that a PersistentVolume cannot be reused across different namespaces.</p><h3 id=sandboxing-containers>Sandboxing containers</h3><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>Kubernetes pods are composed of one or more containers that execute on worker nodes.
Containers utilize OS-level virtualization and hence offer a weaker isolation boundary than
virtual machines that utilize hardware-based virtualization.</p><p>In a shared environment, unpatched vulnerabilities in the application and system layers can be
exploited by attackers for container breakouts and remote code execution that allow access to host
resources. In some applications, like a Content Management System (CMS), customers may be allowed
the ability to upload and execute untrusted scripts or code. In either case, mechanisms to further
isolate and protect workloads using strong isolation are desirable.</p><p>Sandboxing provides a way to isolate workloads running in a shared cluster. It typically involves
running each pod in a separate execution environment such as a virtual machine or a userspace
kernel. Sandboxing is often recommended when you are running untrusted code, where workloads are
assumed to be malicious. Part of the reason this type of isolation is necessary is because
containers are processes running on a shared kernel; they mount file systems like <code>/sys</code> and <code>/proc</code>
from the underlying host, making them less secure than an application that runs on a virtual
machine which has its own kernel. While controls such as seccomp, AppArmor, and SELinux can be
used to strengthen the security of containers, it is hard to apply a universal set of rules to all
workloads running in a shared cluster. Running workloads in a sandbox environment helps to
insulate the host from container escapes, where an attacker exploits a vulnerability to gain
access to the host system and all the processes/files running on that host.</p><p>Virtual machines and userspace kernels are 2 popular approaches to sandboxing. The following
sandboxing implementations are available:</p><ul><li><a href=https://gvisor.dev/>gVisor</a> intercepts syscalls from containers and runs them through a
userspace kernel, written in Go, with limited access to the underlying host.</li><li><a href=https://katacontainers.io/>Kata Containers</a> is an OCI compliant runtime that allows you to run
containers in a VM. The hardware virtualization available in Kata offers an added layer of
security for containers running untrusted code.</li></ul><h3 id=node-isolation>Node Isolation</h3><p>Node isolation is another technique that you can use to isolate tenant workloads from each other.
With node isolation, a set of nodes is dedicated to running pods from a particular tenant and
co-mingling of tenant pods is prohibited. This configuration reduces the noisy tenant issue, as
all pods running on a node will belong to a single tenant. The risk of information disclosure is
slightly lower with node isolation because an attacker that manages to escape from a container
will only have access to the containers and volumes mounted to that node.</p><p>Although workloads from different tenants are running on different nodes, it is important to be
aware that the kubelet and (unless using virtual control planes) the API service are still shared
services. A skilled attacker could use the permissions assigned to the kubelet or other pods
running on the node to move laterally within the cluster and gain access to tenant workloads
running on other nodes. If this is a major concern, consider implementing compensating controls
such as seccomp, AppArmor or SELinux or explore using sandboxed containers or creating separate
clusters for each tenant.</p><p>Node isolation is a little easier to reason about from a billing standpoint than sandboxing
containers since you can charge back per node rather than per pod. It also has fewer compatibility
and performance issues and may be easier to implement than sandboxing containers.
For example, nodes for each tenant can be configured with taints so that only pods with the
corresponding toleration can run on them. A mutating webhook could then be used to automatically
add tolerations and node affinities to pods deployed into tenant namespaces so that they run on a
specific set of nodes designated for that tenant.</p><p>Node isolation can be implemented using an <a href=/docs/concepts/scheduling-eviction/assign-pod-node/>pod node selectors</a>
or a <a href=https://github.com/virtual-kubelet>Virtual Kubelet</a>.</p><h2 id=additional-considerations>Additional Considerations</h2><p>This section discusses other Kubernetes constructs and patterns that are relevant for multi-tenancy.</p><h3 id=api-priority-and-fairness>API Priority and Fairness</h3><p><a href=/docs/concepts/cluster-administration/flow-control/>API priority and fairness</a> is a Kubernetes
feature that allows you to assign a priority to certain pods running within the cluster.
When an application calls the Kubernetes API, the API server evaluates the priority assigned to pod.
Calls from pods with higher priority are fulfilled before those with a lower priority.
When contention is high, lower priority calls can be queued until the server is less busy or you
can reject the requests.</p><p>Using API priority and fairness will not be very common in SaaS environments unless you are
allowing customers to run applications that interface with the Kubernetes API, for example,
a controller.</p><h3 id=qos>Quality-of-Service (QoS)</h3><p>When you’re running a SaaS application, you may want the ability to offer different
Quality-of-Service (QoS) tiers of service to different tenants. For example, you may have freemium
service that comes with fewer performance guarantees and features and a for-fee service tier with
specific performance guarantees. Fortunately, there are several Kubernetes constructs that can
help you accomplish this within a shared cluster, including network QoS, storage classes, and pod
priority and preemption. The idea with each of these is to provide tenants with the quality of
service that they paid for. Let’s start by looking at networking QoS.</p><p>Typically, all pods on a node share a network interface. Without network QoS, some pods may
consume an unfair share of the available bandwidth at the expense of other pods.
The Kubernetes <a href=https://www.cni.dev/plugins/current/meta/bandwidth/>bandwidth plugin</a> creates an
<a href=/docs/concepts/configuration/manage-resources-containers/#extended-resources>extended resource</a>
for networking that allows you to use Kubernetes resources constructs, i.e. requests/limits, to
apply rate limits to pods by using Linux tc queues.
Be aware that the plugin is considered experimental as per the
<a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#support-traffic-shaping>Network Plugins</a>
documentation and should be thoroughly tested before use in production environments.</p><p>For storage QoS, you will likely want to create different storage classes or profiles with
different performance characteristics. Each storage profile can be associated with a different
tier of service that is optimized for different workloads such IO, redundancy, or throughput.
Additional logic might be necessary to allow the tenant to associate the appropriate storage
profile with their workload.</p><p>Finally, there’s <a href=/docs/concepts/scheduling-eviction/pod-priority-preemption/>pod priority and preemption</a>
where you can assign priority values to pods. When scheduling pods, the scheduler will try
evicting pods with lower priority when there are insufficient resources to schedule pods that are
assigned a higher priority. If you have a use case where tenants have different service tiers in a
shared cluster e.g. free and paid, you may want to give higher priority to certain tiers using
this feature.</p><h3 id=dns>DNS</h3><p>Kubernetes clusters include a Domain Name System (DNS) service to provide translations from names
to IP addresses, for all Services and Pods. By default, the Kubernetes DNS service allows lookups
across all namespaces in the cluster.</p><p>In multi-tenant environments where tenants can access pods and other Kubernetes resources, or where
stronger isolation is required, it may be necessary to prevent pods from looking up services in other
Namespaces.
You can restrict cross-namespace DNS lookups by configuring security rules for the DNS service.
For example, CoreDNS (the default DNS service for Kubernetes) can leverage Kubernetes metadata
to restrict queries to Pods and Services within a namespace. For more information, read an
<a href=https://github.com/coredns/policy#kubernetes-metadata-multi-tenancy-policy>example</a> of
configuring this within the CoreDNS documentation.</p><p>When a <a href=#virtual-control-plane-per-tenant>Virtual Control Plane per tenant</a> model is used, a DNS
service must be configured per tenant or a multi-tenant DNS service must be used.
Here is an example of a <a href=https://github.com/kubernetes-sigs/cluster-api-provider-nested/blob/main/virtualcluster/doc/tenant-dns.md>customized version of CoreDNS</a>
that supports multiple tenants.</p><h3 id=operators>Operators</h3><p><a href=/docs/concepts/extend-kubernetes/operator/>Operators</a> are Kubernetes controllers that manage
applications. Operators can simplify the management of multiple instances of an application, like
a database service, which makes them a common building block in the multi-consumer (SaaS)
multi-tenancy use case.</p><p>Operators used in a multi-tenant environment should follow a stricter set of guidelines.
Specifically, the Operator should:</p><ul><li>Support creating resources within different tenant namespaces, rather than just in the namespace
in which the Operator is deployed.</li><li>Ensure that the Pods are configured with resource requests and limits, to ensure scheduling and fairness.</li><li>Support configuration of Pods for data-plane isolation techniques such as node isolation and
sandboxed containers.</li></ul><h2 id=implementations>Implementations</h2><div class="alert alert-secondary callout third-party-content" role=alert><strong>Note:</strong>
This section links to third party projects that provide functionality required by Kubernetes. The Kubernetes project authors aren't responsible for these projects, which are listed alphabetically. To add a project to this list, read the <a href=/docs/contribute/style/content-guide/#third-party-content>content guide</a> before submitting a change. <a href=#third-party-content-disclaimer>More information.</a></div><p>There are two primary ways to share a Kubernetes cluster for multi-tenancy: using Namespaces
(that is, a Namespace per tenant) or by virtualizing the control plane (that is, virtual control
plane per tenant).</p><p>In both cases, data plane isolation, and management of additional considerations such as API
Priority and Fairness, is also recommended.</p><p>Namespace isolation is well-supported by Kubernetes, has a negligible resource cost, and provides
mechanisms to allow tenants to interact appropriately, such as by allowing service-to-service
communication. However, it can be difficult to configure, and doesn't apply to Kubernetes
resources that can't be namespaced, such as Custom Resource Definitions, Storage Classes, and Webhooks.</p><p>Control plane virtualization allows for isolation of non-namespaced resources at the cost of
somewhat higher resource usage and more difficult cross-tenant sharing. It is a good option when
namespace isolation is insufficient but dedicated clusters are undesirable, due to the high cost
of maintaining them (especially on-prem) or due to their higher overhead and lack of resource
sharing. However, even within a virtualized control plane, you will likely see benefits by using
namespaces as well.</p><p>The two options are discussed in more detail in the following sections.</p><h3 id=namespace-per-tenant>Namespace per tenant</h3><p>As previously mentioned, you should consider isolating each workload in its own namespace, even if
you are using dedicated clusters or virtualized control planes. This ensures that each workload
only has access to its own resources, such as Config Maps and Secrets, and allows you to tailor
dedicated security policies for each workload. In addition, it is a best practice to give each
namespace names that are unique across your entire fleet (that is, even if they are in separate
clusters), as this gives you the flexibility to switch between dedicated and shared clusters in
the future, or to use multi-cluster tooling such as service meshes.</p><p>Conversely, there are also advantages to assigning namespaces at the tenant level, not just the
workload level, since there are often policies that apply to all workloads owned by a single
tenant. However, this raises its own problems. Firstly, this makes it difficult or impossible to
customize policies to individual workloads, and secondly, it may be challenging to come up with a
single level of "tenancy" that should be given a namespace. For example, an organization may have
divisions, teams, and subteams - which should be assigned a namespace?</p><p>To solve this, Kubernetes provides the <a href=https://github.com/kubernetes-sigs/hierarchical-namespaces>Hierarchical Namespace Controller (HNC)</a>,
which allows you to organize your namespaces into hierarchies, and share certain policies and
resources between them. It also helps you manage namespace labels, namespace lifecycles, and
delegated management, and share resource quotas across related namespaces. These capabilities can
be useful in both multi-team and multi-customer scenarios.</p><p>Other projects that provide similar capabilities and aid in managing namespaced resources are
listed below.</p><h4 id=multi-team-tenancy>Multi-team tenancy</h4><ul><li><a href=https://github.com/clastix/capsule>Capsule</a></li><li><a href=https://github.com/loft-sh/kiosk>Kiosk</a></li></ul><h4 id=multi-customer-tenancy>Multi-customer tenancy</h4><ul><li><a href=https://github.com/cloud-ark/kubeplus>Kubeplus</a></li></ul><h4 id=policy-engines>Policy engines</h4><p>Policy engines provide features to validate and generate tenant configurations:</p><ul><li><a href=https://kyverno.io/>Kyverno</a></li><li><a href=https://github.com/open-policy-agent/gatekeeper>OPA/Gatekeeper</a></li></ul><h3 id=virtual-control-plane-per-tenant>Virtual control plane per tenant</h3><p>Another form of control-plane isolation is to use Kubernetes extensions to provide each tenant a
virtual control-plane that enables segmentation of cluster-wide API resources.
<a href=#data-plane-isolation>Data plane isolation</a> techniques can be used with this model to securely
manage worker nodes across tenants.</p><p>The virtual control plane based multi-tenancy model extends namespace-based multi-tenancy by
providing each tenant with dedicated control plane components, and hence complete control over
cluster-wide resources and add-on services. Worker nodes are shared across all tenants, and are
managed by a Kubernetes cluster that is normally inaccessible to tenants.
This cluster is often referred to as a <em>super-cluster</em> (or sometimes as a <em>host-cluster</em>).
Since a tenant’s control-plane is not directly associated with underlying compute resources it is
referred to as a <em>virtual control plane</em>.</p><p>A virtual control plane typically consists of the Kubernetes API server, the controller manager,
and the etcd data store. It interacts with the super cluster via a metadata synchronization
controller which coordinates changes across tenant control planes and the control plane of the
super-cluster.</p><p>By using per-tenant dedicated control planes, most of the isolation problems due to sharing one
API server among all tenants are solved. Examples include noisy neighbors in the control plane,
uncontrollable blast radius of policy misconfigurations, and conflicts between cluster scope
objects such as webhooks and CRDs. Hence, the virtual control plane model is particularly
suitable for cases where each tenant requires access to a Kubernetes API server and expects the
full cluster manageability.</p><p>The improved isolation comes at the cost of running and maintaining an individual virtual control
plane per tenant. In addition, per-tenant control planes do not solve isolation problems in the
data plane, such as node-level noisy neighbors or security threats. These must still be addressed
separately.</p><p>The Kubernetes <a href=https://github.com/kubernetes-sigs/cluster-api-provider-nested/tree/main/virtualcluster>Cluster API - Nested (CAPN)</a>
project provides an implementation of virtual control planes.</p><h4 id=other-implementations>Other implementations</h4><ul><li><a href=https://github.com/clastix/kamaji>Kamaji</a></li><li><a href=https://github.com/loft-sh/vcluster>vcluster</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-265c06c3d1349382453ced9f2a7ecfde>10 - Kubernetes API Server Bypass Risks</h1><div class=lead>Security architecture information relating to the API server and other components</div><p>The Kubernetes API server is the main point of entry to a cluster for external parties
(users and services) interacting with it.</p><p>As part of this role, the API server has several key built-in security controls, such as
audit logging and <a class=glossary-tooltip title='A piece of code that intercepts requests to the Kubernetes API server prior to persistence of the object.' data-toggle=tooltip data-placement=top href=/docs/reference/access-authn-authz/admission-controllers/ target=_blank aria-label='admission controllers'>admission controllers</a>. However, there are ways to modify the configuration
or content of the cluster that bypass these controls.</p><p>This page describes the ways in which the security controls built into the
Kubernetes API server can be bypassed, so that cluster operators
and security architects can ensure that these bypasses are appropriately restricted.</p><h2 id=static-pods>Static Pods</h2><p>The <a class=glossary-tooltip title='An agent that runs on each node in the cluster. It makes sure that containers are running in a pod.' data-toggle=tooltip data-placement=top href=/docs/reference/generated/kubelet target=_blank aria-label=kubelet>kubelet</a> on each node loads and
directly manages any manifests that are stored in a named directory or fetched from
a specific URL as <a href=/docs/tasks/configure-pod-container/static-pod><em>static Pods</em></a> in
your cluster. The API server doesn't manage these static Pods. An attacker with write
access to this location could modify the configuration of static pods loaded from that
source, or could introduce new static Pods.</p><p>Static Pods are restricted from accessing other objects in the Kubernetes API. For example,
you can't configure a static Pod to mount a Secret from the cluster. However, these Pods can
take other security sensitive actions, such as using <code>hostPath</code> mounts from the underlying
node.</p><p>By default, the kubelet creates a <a class=glossary-tooltip title='An object in the API server that tracks a static pod on a kubelet.' data-toggle=tooltip data-placement=top href='/docs/reference/glossary/?all=true#term-mirror-pod' target=_blank aria-label='mirror pod'>mirror pod</a>
so that the static Pods are visible in the Kubernetes API. However, if the attacker uses an invalid
namespace name when creating the Pod, it will not be visible in the Kubernetes API and can only
be discovered by tooling that has access to the affected host(s).</p><p>If a static Pod fails admission control, the kubelet won't register the Pod with the
API server. However, the Pod still runs on the node. For more information, refer to
<a href=https://github.com/kubernetes/kubeadm/issues/1541#issuecomment-487331701>kubeadm issue #1541</a>.</p><h3 id=static-pods-mitigations>Mitigations</h3><ul><li>Only <a href=/docs/tasks/configure-pod-container/static-pod/#static-pod-creation>enable the kubelet static Pod manifest functionality</a>
if required by the node.</li><li>If a node uses the static Pod functionality, restrict filesystem access to the static Pod manifest directory
or URL to users who need the access.</li><li>Restrict access to kubelet configuration parameters and files to prevent an attacker setting
a static Pod path or URL.</li><li>Regularly audit and centrally report all access to directories or web storage locations that host
static Pod manifests and kubelet configuration files.</li></ul><h2 id=kubelet-api>The kubelet API</h2><p>The kubelet provides an HTTP API that is typically exposed on TCP port 10250 on cluster
worker nodes. The API might also be exposed on control plane nodes depending on the Kubernetes
distribution in use. Direct access to the API allows for disclosure of information about
the pods running on a node, the logs from those pods, and execution of commands in
every container running on the node.</p><p>When Kubernetes cluster users have RBAC access to <code>Node</code> object sub-resources, that access
serves as authorization to interact with the kubelet API. The exact access depends on
which sub-resource access has been granted, as detailed in <a href=https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authorization>kubelet authorization</a>.</p><p>Direct access to the kubelet API is not subject to admission control and is not logged
by Kubernetes audit logging. An attacker with direct access to this API may be able to
bypass controls that detect or prevent certain actions.</p><p>The kubelet API can be configured to authenticate requests in a number of ways.
By default, the kubelet configuration allows anonymous access. Most Kubernetes providers
change the default to use webhook and certificate authentication. This lets the control plane
ensure that the caller is authorized to access the <code>nodes</code> API resource or sub-resources.
The default anonymous access doesn't make this assertion with the control plane.</p><h3 id=mitigations>Mitigations</h3><ul><li>Restrict access to sub-resources of the <code>nodes</code> API object using mechanisms such as
<a href=/docs/reference/access-authn-authz/rbac/>RBAC</a>. Only grant this access when required,
such as by monitoring services.</li><li>Restrict access to the kubelet port. Only allow specified and trusted IP address
ranges to access the port.</li><li><a href=/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication>Ensure that kubelet authentication is set to webhook or certificate mode</a>.</li><li>Ensure that the unauthenticated "read-only" Kubelet port is not enabled on the cluster.</li></ul><h2 id=the-etcd-api>The etcd API</h2><p>Kubernetes clusters use etcd as a datastore. The <code>etcd</code> service listens on TCP port 2379.
The only clients that need access are the Kubernetes API server and any backup tooling
that you use. Direct access to this API allows for disclosure or modification of any
data held in the cluster.</p><p>Access to the etcd API is typically managed by client certificate authentication.
Any certificate issued by a certificate authority that etcd trusts allows full access
to the data stored inside etcd.</p><p>Direct access to etcd is not subject to Kubernetes admission control and is not logged
by Kubernetes audit logging. An attacker who has read access to the API server's
etcd client certificate private key (or can create a new trusted client certificate) can gain
cluster admin rights by accessing cluster secrets or modifying access rules. Even without
elevating their Kubernetes RBAC privileges, an attacker who can modify etcd can retrieve any API object
or create new workloads inside the cluster.</p><p>Many Kubernetes providers configure
etcd to use mutual TLS (both client and server verify each other's certificate for authentication).
There is no widely accepted implementation of authorization for the etcd API, although
the feature exists. Since there is no authorization model, any certificate
with client access to etcd can be used to gain full access to etcd. Typically, etcd client certificates
that are only used for health checking can also grant full read and write access.</p><h3 id=etcd-api-mitigations>Mitigations</h3><ul><li>Ensure that the certificate authority trusted by etcd is used only for the purposes of
authentication to that service.</li><li>Control access to the private key for the etcd server certificate, and to the API server's
client certificate and key.</li><li>Consider restricting access to the etcd port at a network level, to only allow access
from specified and trusted IP address ranges.</li></ul><h2 id=runtime-socket>Container runtime socket</h2><p>On each node in a Kubernetes cluster, access to interact with containers is controlled
by the container runtime (or runtimes, if you have configured more than one). Typically,
the container runtime exposes a Unix socket that the kubelet can access. An attacker with
access to this socket can launch new containers or interact with running containers.</p><p>At the cluster level, the impact of this access depends on whether the containers that
run on the compromised node have access to Secrets or other confidential
data that an attacker could use to escalate privileges to other worker nodes or to
control plane components.</p><h3 id=runtime-socket-mitigations>Mitigations</h3><ul><li>Ensure that you tightly control filesystem access to container runtime sockets.
When possible, restrict this access to the <code>root</code> user.</li><li>Isolate the kubelet from other components running on the node, using
mechanisms such as Linux kernel namespaces.</li><li>Ensure that you restrict or forbid the use of <a href=/docs/concepts/storage/volumes/#hostpath><code>hostPath</code> mounts</a>
that include the container runtime socket, either directly or by mounting a parent
directory. Also <code>hostPath</code> mounts must be set as read-only to mitigate risks
of attackers bypassing directory restrictions.</li><li>Restrict user access to nodes, and especially restrict superuser access to nodes.</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-6f8354561fd5286f997909e14b13110c>11 - Security Checklist</h1><div class=lead>Baseline checklist for ensuring security in Kubernetes clusters.</div><p>This checklist aims at providing a basic list of guidance with links to more
comprehensive documentation on each topic. It does not claim to be exhaustive
and is meant to evolve.</p><p>On how to read and use this document:</p><ul><li>The order of topics does not reflect an order of priority.</li><li>Some checklist items are detailed in the paragraph below the list of each section.</li></ul><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Checklists are <strong>not</strong> sufficient for attaining a good security posture on their
own. A good security posture requires constant attention and improvement, but a
checklist can be the first step on the never-ending journey towards security
preparedness. Some of the recommendations in this checklist may be too
restrictive or too lax for your specific security needs. Since Kubernetes
security is not "one size fits all", each category of checklist items should be
evaluated on its merits.</div><h2 id=authentication-authorization>Authentication & Authorization</h2><ul><li><input disabled type=checkbox> <code>system:masters</code> group is not used for user or component authentication after bootstrapping.</li><li><input disabled type=checkbox> The kube-controller-manager is running with <code>--use-service-account-credentials</code>
enabled.</li><li><input disabled type=checkbox> The root certificate is protected (either an offline CA, or a managed
online CA with effective access controls).</li><li><input disabled type=checkbox> Intermediate and leaf certificates have an expiry date no more than 3
years in the future.</li><li><input disabled type=checkbox> A process exists for periodic access review, and reviews occur no more
than 24 months apart.</li><li><input disabled type=checkbox> The <a href=/docs/concepts/security/rbac-good-practices/>Role Based Access Control Good Practices</a>
is followed for guidance related to authentication and authorization.</li></ul><p>After bootstrapping, neither users nor components should authenticate to the
Kubernetes API as <code>system:masters</code>. Similarly, running all of
kube-controller-manager as <code>system:masters</code> should be avoided. In fact,
<code>system:masters</code> should only be used as a break-glass mechanism, as opposed to
an admin user.</p><h2 id=network-security>Network security</h2><ul><li><input disabled type=checkbox> CNI plugins in-use supports network policies.</li><li><input disabled type=checkbox> Ingress and egress network policies are applied to all workloads in the
cluster.</li><li><input disabled type=checkbox> Default network policies within each namespace, selecting all pods, denying
everything, are in place.</li><li><input disabled type=checkbox> If appropriate, a service mesh is used to encrypt all communications inside of the cluster.</li><li><input disabled type=checkbox> The Kubernetes API, kubelet API and etcd are not exposed publicly on Internet.</li><li><input disabled type=checkbox> Access from the workloads to the cloud metadata API is filtered.</li><li><input disabled type=checkbox> Use of LoadBalancer and ExternalIPs is restricted.</li></ul><p>A number of <a href=/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/>Container Network Interface (CNI) plugins</a>
plugins provide the functionality to
restrict network resources that pods may communicate with. This is most commonly done
through <a href=/docs/concepts/services-networking/network-policies/>Network Policies</a>
which provide a namespaced resource to define rules. Default network policies
blocking everything egress and ingress, in each namespace, selecting all the
pods, can be useful to adopt an allow list approach, ensuring that no workloads
is missed.</p><p>Not all CNI plugins provide encryption in transit. If the chosen plugin lacks this
feature, an alternative solution could be to use a service mesh to provide that
functionality.</p><p>The etcd datastore of the control plane should have controls to limit access and
not be publicly exposed on the Internet. Furthermore, mutual TLS (mTLS) should
be used to communicate securely with it. The certificate authority for this
should be unique to etcd.</p><p>External Internet access to the Kubernetes API server should be restricted to
not expose the API publicly. Be careful as many managed Kubernetes distribution
are publicly exposing the API server by default. You can then use a bastion host
to access the server.</p><p>The <a href=/docs/reference/command-line-tools-reference/kubelet/>kubelet</a> API access
should be restricted and not publicly exposed, the defaults authentication and
authorization settings, when no configuration file specified with the <code>--config</code>
flag, are overly permissive.</p><p>If a cloud provider is used for hosting Kubernetes, the access from pods to the cloud
metadata API <code>169.254.169.254</code> should also be restricted or blocked if not needed
because it may leak information.</p><p>For restricted LoadBalancer and ExternalIPs use, see
<a href=https://github.com/kubernetes/kubernetes/issues/97076>CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs</a>
and the <a href=/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips>DenyServiceExternalIPs admission controller</a>
for further information.</p><h2 id=pod-security>Pod security</h2><ul><li><input disabled type=checkbox> RBAC rights to <code>create</code>, <code>update</code>, <code>patch</code>, <code>delete</code> workloads is only granted if necessary.</li><li><input disabled type=checkbox> Appropriate Pod Security Standards policy is applied for all namespaces and enforced.</li><li><input disabled type=checkbox> Memory limit is set for the workloads with a limit equal or inferior to the request.</li><li><input disabled type=checkbox> CPU limit might be set on sensitive workloads.</li><li><input disabled type=checkbox> For nodes that support it, Seccomp is enabled with appropriate syscalls
profile for programs.</li><li><input disabled type=checkbox> For nodes that support it, AppArmor or SELinux is enabled with appropriate
profile for programs.</li></ul><p>RBAC authorization is crucial but
<a href=/docs/concepts/security/rbac-good-practices/#workload-creation>cannot be granular enough to have authorization on the Pods' resources</a>
(or on any resource that manages Pods). The only granularity is the API verbs
on the resource itself, for example, <code>create</code> on Pods. Without
additional admission, the authorization to create these resources allows direct
unrestricted access to the schedulable nodes of a cluster.</p><p>The <a href=/docs/concepts/security/pod-security-standards/>Pod Security Standards</a>
define three different policies, privileged, baseline and restricted that limit
how fields can be set in the <code>PodSpec</code> regarding security.
These standards can be enforced at the namespace level with the new
<a href=/docs/concepts/security/pod-security-admission/>Pod Security</a> admission,
enabled by default, or by third-party admission webhook. Please note that,
contrary to the removed PodSecurityPolicy admission it replaces,
<a href=/docs/concepts/security/pod-security-admission/>Pod Security</a>
admission can be easily combined with admission webhooks and external services.</p><p>Pod Security admission <code>restricted</code> policy, the most restrictive policy of the
<a href=/docs/concepts/security/pod-security-standards/>Pod Security Standards</a> set,
<a href=/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces>can operate in several modes</a>,
<code>warn</code>, <code>audit</code> or <code>enforce</code> to gradually apply the most appropriate
<a href=/docs/tasks/configure-pod-container/security-context/>security context</a>
according to security best practices. Nevertheless, pods'
<a href=/docs/tasks/configure-pod-container/security-context/>security context</a>
should be separately investigated to limit the privileges and access pods may
have on top of the predefined security standards, for specific use cases.</p><p>For a hands-on tutorial on <a href=/docs/concepts/security/pod-security-admission/>Pod Security</a>,
see the blog post
<a href=/blog/2021/12/09/pod-security-admission-beta/>Kubernetes 1.23: Pod Security Graduates to Beta</a>.</p><p><a href=/docs/concepts/configuration/manage-resources-containers/>Memory and CPU limits</a>
should be set in order to restrict the memory and CPU resources a pod can
consume on a node, and therefore prevent potential DoS attacks from malicious or
breached workloads. Such policy can be enforced by an admission controller.
Please note that CPU limits will throttle usage and thus can have unintended
effects on auto-scaling features or efficiency i.e. running the process in best
effort with the CPU resource available.</p><div class="alert alert-warning caution callout" role=alert><strong>Caution:</strong> Memory limit superior to request can expose the whole node to OOM issues.</div><h3 id=enabling-seccomp>Enabling Seccomp</h3><p>Seccomp can improve the security of your workloads by reducing the Linux kernel
syscall attack surface available inside containers. The seccomp filter mode
leverages BPF to create an allow or deny list of specific syscalls, named
profiles. Those seccomp profiles can be enabled on individual workloads,
<a href=/docs/tutorials/security/seccomp/>a security tutorial is available</a>. In
addition, the <a href=https://github.com/kubernetes-sigs/security-profiles-operator>Kubernetes Security Profiles Operator</a>
is a project to facilitate the management and use of seccomp in clusters.</p><p>For historical context, please note that Docker has been using
<a href=https://docs.docker.com/engine/security/seccomp/>a default seccomp profile</a>
to only allow a restricted set of syscalls since 2016 from
<a href=https://www.docker.com/blog/docker-engine-1-10-security/>Docker Engine 1.10</a>,
but Kubernetes is still not confining workloads by default. The default seccomp
profile can be found <a href=https://github.com/containerd/containerd/blob/main/contrib/seccomp/seccomp_default.go>in containerd</a>
as well. Fortunately, <a href=/blog/2021/08/25/seccomp-default/>Seccomp Default</a>, a
new alpha feature to use a default seccomp profile for all workloads can now be
enabled and tested.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> Seccomp is only available on Linux nodes.</div><h3 id=enabling-apparmor-or-selinux>Enabling AppArmor or SELinux</h3><h4 id=apparmor>AppArmor</h4><p><a href=https://apparmor.net/>AppArmor</a> is a Linux kernel security module that can
provide an easy way to implement Mandatory Access Control (MAC) and better
auditing through system logs. To <a href=/docs/tutorials/security/apparmor/>enable AppArmor in Kubernetes</a>,
at least version 1.4 is required. Like seccomp, AppArmor is also configured
through profiles, where each profile is either running in enforcing mode, which
blocks access to disallowed resources or complain mode, which only reports
violations. AppArmor profiles are enforced on a per-container basis, with an
annotation, allowing for processes to gain just the right privileges.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> AppArmor is only available on Linux nodes, and enabled in
<a href=https://gitlab.com/apparmor/apparmor/-/wikis/home#distributions-and-ports>some Linux distributions</a>.</div><h4 id=selinux>SELinux</h4><p><a href=https://github.com/SELinuxProject/selinux-notebook/blob/main/src/selinux_overview.md>SELinux</a> is also a
Linux kernel security module that can provide a mechanism for supporting access
control security policies, including Mandatory Access Controls (MAC). SELinux
labels can be assigned to containers or pods
<a href=/docs/tasks/configure-pod-container/security-context/#assign-selinux-labels-to-a-container>via their <code>securityContext</code> section</a>.</p><div class="alert alert-info note callout" role=alert><strong>Note:</strong> SELinux is only available on Linux nodes, and enabled in
<a href=https://en.wikipedia.org/wiki/Security-Enhanced_Linux#Implementations>some Linux distributions</a>.</div><h2 id=pod-placement>Pod placement</h2><ul><li><input disabled type=checkbox> Pod placement is done in accordance with the tiers of sensitivity of the
application.</li><li><input disabled type=checkbox> Sensitive applications are running isolated on nodes or with specific
sandboxed runtimes.</li></ul><p>Pods that are on different tiers of sensitivity, for example, an application pod
and the Kubernetes API server, should be deployed onto separate nodes. The
purpose of node isolation is to prevent an application container breakout to
directly providing access to applications with higher level of sensitivity to easily
pivot within the cluster. This separation should be enforced to prevent pods
accidentally being deployed onto the same node. This could be enforced with the
following features:</p><dl><dt><a href=/docs/concepts/scheduling-eviction/assign-pod-node/>Node Selectors</a></dt><dd>Key-value pairs, as part of the pod specification, that specify which nodes to
deploy onto. These can be enforced at the namespace and cluster level with the
<a href=/docs/reference/access-authn-authz/admission-controllers/#podnodeselector>PodNodeSelector</a>
admission controller.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#podtolerationrestriction>PodTolerationRestriction</a></dt><dd>An admission controller that allows administrators to restrict permitted
<a href=/docs/concepts/scheduling-eviction/taint-and-toleration/>tolerations</a> within a
namespace. Pods within a namespace may only utilize the tolerations specified on
the namespace object annotation keys that provide a set of default and allowed
tolerations.</dd><dt><a href=/docs/concepts/containers/runtime-class/>RuntimeClass</a></dt><dd>RuntimeClass is a feature for selecting the container runtime configuration.
The container runtime configuration is used to run a Pod's containers and can
provide more or less isolation from the host at the cost of performance
overhead.</dd></dl><h2 id=secrets>Secrets</h2><ul><li><input disabled type=checkbox> ConfigMaps are not used to hold confidential data.</li><li><input disabled type=checkbox> Encryption at rest is configured for the Secret API.</li><li><input disabled type=checkbox> If appropriate, a mechanism to inject secrets stored in third-party storage
is deployed and available.</li><li><input disabled type=checkbox> Service account tokens are not mounted in pods that don't require them.</li><li><input disabled type=checkbox> <a href=/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume>Bound service account token volume</a>
is in-use instead of non-expiring tokens.</li></ul><p>Secrets required for pods should be stored within Kubernetes Secrets as opposed
to alternatives such as ConfigMap. Secret resources stored within etcd should
be <a href=/docs/tasks/administer-cluster/encrypt-data/>encrypted at rest</a>.</p><p>Pods needing secrets should have these automatically mounted through volumes,
preferably stored in memory like with the <a href=/docs/concepts/storage/volumes/#emptydir><code>emptyDir.medium</code> option</a>.
Mechanism can be used to also inject secrets from third-party storages as
volume, like the <a href=https://secrets-store-csi-driver.sigs.k8s.io/>Secrets Store CSI Driver</a>.
This should be done preferentially as compared to providing the pods service
account RBAC access to secrets. This would allow adding secrets into the pod as
environment variables or files. Please note that the environment variable method
might be more prone to leakage due to crash dumps in logs and the
non-confidential nature of environment variable in Linux, as opposed to the
permission mechanism on files.</p><p>Service account tokens should not be mounted into pods that do not require them. This can be configured by setting
<a href=/docs/tasks/configure-pod-container/configure-service-account/#use-the-default-service-account-to-access-the-api-server><code>automountServiceAccountToken</code></a>
to <code>false</code> either within the service account to apply throughout the namespace
or specifically for a pod. For Kubernetes v1.22 and above, use
<a href=/docs/reference/access-authn-authz/service-accounts-admin/#bound-service-account-token-volume>Bound Service Accounts</a>
for time-bound service account credentials.</p><h2 id=images>Images</h2><ul><li><input disabled type=checkbox> Minimize unnecessary content in container images.</li><li><input disabled type=checkbox> Container images are configured to be run as unprivileged user.</li><li><input disabled type=checkbox> References to container images are made by sha256 digests (rather than
tags) or the provenance of the image is validated by verifying the image's
digital signature at deploy time <a href=/docs/tasks/administer-cluster/verify-signed-images/#verifying-image-signatures-with-admission-controller>via admission control</a>.</li><li><input disabled type=checkbox> Container images are regularly scanned during creation and in deployment, and
known vulnerable software is patched.</li></ul><p>Container image should contain the bare minimum to run the program they
package. Preferably, only the program and its dependencies, building the image
from the minimal possible base. In particular, image used in production should not
contain shells or debugging utilities, as an
<a href=/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container>ephemeral debug container</a>
can be used for troubleshooting.</p><p>Build images to directly start with an unprivileged user by using the
<a href=https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#user><code>USER</code> instruction in Dockerfile</a>.
The <a href=/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod>Security Context</a>
allows a container image to be started with a specific user and group with
<code>runAsUser</code> and <code>runAsGroup</code>, even if not specified in the image manifest.
However, the file permissions in the image layers might make it impossible to just
start the process with a new unprivileged user without image modification.</p><p>Avoid using image tags to reference an image, especially the <code>latest</code> tag, the
image behind a tag can be easily modified in a registry. Prefer using the
complete <code>sha256</code> digest which is unique to the image manifest. This policy can be
enforced via an <a href=/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook>ImagePolicyWebhook</a>.
Image signatures can also be automatically <a href=/docs/tasks/administer-cluster/verify-signed-images/#verifying-image-signatures-with-admission-controller>verified with an admission controller</a>
at deploy time to validate their authenticity and integrity.</p><p>Scanning a container image can prevent critical vulnerabilities from being
deployed to the cluster alongside the container image. Image scanning should be
completed before deploying a container image to a cluster and is usually done
as part of the deployment process in a CI/CD pipeline. The purpose of an image
scan is to obtain information about possible vulnerabilities and their
prevention in the container image, such as a
<a href=https://www.first.org/cvss/>Common Vulnerability Scoring System (CVSS)</a>
score. If the result of the image scans is combined with the pipeline
compliance rules, only properly patched container images will end up in
Production.</p><h2 id=admission-controllers>Admission controllers</h2><ul><li><input disabled type=checkbox> An appropriate selection of admission controllers is enabled.</li><li><input disabled type=checkbox> A pod security policy is enforced by the Pod Security Admission or/and a
webhook admission controller.</li><li><input disabled type=checkbox> The admission chain plugins and webhooks are securely configured.</li></ul><p>Admission controllers can help to improve the security of the cluster. However,
they can present risks themselves as they extend the API server and
<a href=/blog/2022/01/19/secure-your-admission-controllers-and-webhooks/>should be properly secured</a>.</p><p>The following lists present a number of admission controllers that could be
considered to enhance the security posture of your cluster and application. It
includes controllers that may be referenced in other parts of this document.</p><p>This first group of admission controllers includes plugins
<a href=/docs/reference/access-authn-authz/admission-controllers/#which-plugins-are-enabled-by-default>enabled by default</a>,
consider to leave them enabled unless you know what you are doing:</p><dl><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#certificateapproval><code>CertificateApproval</code></a></dt><dd>Performs additional authorization checks to ensure the approving user has
permission to approve certificate request.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#certificatesigning><code>CertificateSigning</code></a></dt><dd>Performs additional authorization checks to ensure the signing user has
permission to sign certificate requests.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#certificatesubjectrestriction><code>CertificateSubjectRestriction</code></a></dt><dd>Rejects any certificate request that specifies a 'group' (or 'organization
attribute') of <code>system:masters</code>.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#limitranger><code>LimitRanger</code></a></dt><dd>Enforce the LimitRange API constraints.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook><code>MutatingAdmissionWebhook</code></a></dt><dd>Allows the use of custom controllers through webhooks, these controllers may
mutate requests that it reviews.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#podsecurity><code>PodSecurity</code></a></dt><dd>Replacement for Pod Security Policy, restricts security contexts of deployed
Pods.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#resourcequota><code>ResourceQuota</code></a></dt><dd>Enforces resource quotas to prevent over-usage of resources.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#validatingadmissionwebhook><code>ValidatingAdmissionWebhook</code></a></dt><dd>Allows the use of custom controllers through webhooks, these controllers do
not mutate requests that it reviews.</dd></dl><p>The second group includes plugin that are not enabled by default but in general
availability state and recommended to improve your security posture:</p><dl><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#denyserviceexternalips><code>DenyServiceExternalIPs</code></a></dt><dd>Rejects all net-new usage of the <code>Service.spec.externalIPs</code> field. This is a mitigation for
<a href=https://github.com/kubernetes/kubernetes/issues/97076>CVE-2020-8554: Man in the middle using LoadBalancer or ExternalIPs</a>.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#noderestriction><code>NodeRestriction</code></a></dt><dd>Restricts kubelet's permissions to only modify the pods API resources they own
or the node API ressource that represent themselves. It also prevents kubelet
from using the <code>node-restriction.kubernetes.io/</code> annotation, which can be used
by an attacker with access to the kubelet's credentials to influence pod
placement to the controlled node.</dd></dl><p>The third group includes plugins that are not enabled by default but could be
considered for certain use cases:</p><dl><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#alwayspullimages><code>AlwaysPullImages</code></a></dt><dd>Enforces the usage of the latest version of a tagged image and ensures that the deployer
has permissions to use the image.</dd><dt><a href=/docs/reference/access-authn-authz/admission-controllers/#imagepolicywebhook><code>ImagePolicyWebhook</code></a></dt><dd>Allows enforcing additional controls for images through webhooks.</dd></dl><h2 id=what-s-next>What's next</h2><ul><li><a href=/docs/concepts/security/rbac-good-practices/>RBAC Good Practices</a> for
further information on authorization.</li><li><a href=/docs/concepts/security/multi-tenancy/>Cluster Multi-tenancy guide</a> for
configuration options recommendations and best practices on multi-tenancy.</li><li><a href=/blog/2021/10/05/nsa-cisa-kubernetes-hardening-guidance/#building-secure-container-images>Blog post "A Closer Look at NSA/CISA Kubernetes Hardening Guidance"</a>
for complementary resource on hardening Kubernetes clusters.</li></ul></div></main></div></div><footer class=d-print-none><div class=footer__links><nav><a class=text-white href=/docs/home/>Home</a>
<a class=text-white href=/blog/>Blog</a>
<a class=text-white href=/training/>Training</a>
<a class=text-white href=/partners/>Partners</a>
<a class=text-white href=/community/>Community</a>
<a class=text-white href=/case-studies/>Case Studies</a></nav></div><div class=container-fluid><div class=row><div class="col-6 col-sm-2 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank href=https://discuss.kubernetes.io><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank href=https://twitter.com/kubernetesio><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Calendar aria-label=Calendar><a class=text-white target=_blank href="https://calendar.google.com/calendar/embed?src=calendar%40kubernetes.io"><i class="fas fa-calendar-alt"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Youtube aria-label=Youtube><a class=text-white target=_blank href=https://youtube.com/kubernetescommunity><i class="fab fa-youtube"></i></a></li></ul></div><div class="col-6 col-sm-2 text-right text-xs-center order-sm-3"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank href=https://github.com/kubernetes/kubernetes><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Slack aria-label=Slack><a class=text-white target=_blank href=https://slack.k8s.io><i class="fab fa-slack"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Contribute aria-label=Contribute><a class=text-white target=_blank href=https://git.k8s.io/community/contributors/guide><i class="fas fa-edit"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="Stack Overflow" aria-label="Stack Overflow"><a class=text-white target=_blank href=https://stackoverflow.com/questions/tagged/kubernetes><i class="fab fa-stack-overflow"></i></a></li></ul></div><div class="col-12 col-sm-8 text-center order-sm-2"><small class=text-white>&copy; 2023 The Kubernetes Authors | Documentation Distributed under <a href=https://git.k8s.io/website/LICENSE class=light-text>CC BY 4.0</a></small><br><small class=text-white>Copyright &copy; 2023 The Linux Foundation &reg;. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our <a href=https://www.linuxfoundation.org/trademark-usage class=light-text>Trademark Usage page</a></small><br><small class=text-white>ICP license: 京ICP备17074266号-3</small></div></div></div></footer></div><script src=/js/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script src=/js/popper-1.16.1.min.js intregrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=/js/bootstrap-4.6.1.min.js integrity=sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2 crossorigin=anonymous></script>
<script src=/js/script.js></script>
<script async src=/js/mermaid-8.13.4.min.js integrity=sha384-5hHNvPeMrNH14oM3IcQofDoBhiclNK3g2+hnEinKzQ07C4AliMeVpnvxuiwEGpaO crossorigin=anonymous></script>
<script src=/js/main.min.5c0bf7f21dc4f66485f74efbbeeff28a7e4f8cddaac1bae47043159c922ff3a3.js integrity="sha256-XAv38h3E9mSF9077vu/yin5PjN2qwbrkcEMVnJIv86M=" crossorigin=anonymous></script></body></html>